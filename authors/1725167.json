{
    "authorId": "1725167",
    "papers": [
        {
            "paperId": "258c791992a0d54dd29e4a187f2533a3b6542a01",
            "title": "ADecimo: Model Selection for Time Series Anomaly Detection",
            "abstract": "Anomaly detection is a fundamental task for time-series analytics with important implications for the downstream performance of many applications. Despite increasing academic interest and the large number of methods proposed in the literature, recent benchmark and evaluation studies demonstrated that there exists no single best anomaly detection method when applied to heterogeneous time series datasets. Therefore, the only scalable and viable solution to solve anomaly detection over very different time series collected from diverse domains is to propose a model selection method that will choose, based on time series characteristics, the best anomaly detection method to run. This paper describes ADecimo, a modular and extensible web application that helps users understand the performance of time series classification algorithms used as model selection methods for time series anomaly detection. Overall, our system enables users to compare 17 different classifiers over 1980 time series, and decide on the most suitable time series classification method for their own time series and use cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "2167783581",
                    "name": "Emmanouil Sylligardos"
                },
                {
                    "authorId": "2516699",
                    "name": "John Paparrizos"
                },
                {
                    "authorId": "8032437",
                    "name": "P. Trahanias"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "3c364fbc8e4d0fa2e49e7e396ab8e872993e1f5a",
            "title": "DET-LSH: A Locality-Sensitive Hashing Scheme with Dynamic Encoding Tree for Approximate Nearest Neighbor Search",
            "abstract": "Locality-sensitive hashing (LSH) is a well-known solution for approximate nearest neighbor (ANN) search in high-dimensional spaces due to its robust theoretical guarantee on query accuracy. Traditional LSH-based methods mainly focus on improving the efficiency and accuracy of the query phase by designing different query strategies, but pay little attention to improving the efficiency of the indexing phase. They typically fine-tune existing data-oriented partitioning trees to index data points and support their query strategies. However, their strategy to directly partition the multi-dimensional space is time-consuming, and performance degrades as the space dimensionality increases. In this paper, we design an encoding-based tree called Dynamic Encoding Tree (DE-Tree) to improve the indexing efficiency and support efficient range queries based on Euclidean distance. Based on DE-Tree, we propose a novel LSH scheme called DET-LSH. DET-LSH adopts a novel query strategy, which performs range queries in multiple independent index DE-Trees to reduce the probability of missing exact NN points, thereby improving the query accuracy. Our theoretical studies show that DET-LSH enjoys probabilistic guarantees on query accuracy. Extensive experiments on real-world datasets demonstrate the superiority of DET-LSH over the state-of-the-art LSH-based methods on both efficiency and accuracy. While achieving better query accuracy than competitors, DET-LSH achieves up to 6x speedup in indexing time and 2x speedup in query time over the state-of-the-art LSH-based methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2226477368",
                    "name": "Jiuqi Wei"
                },
                {
                    "authorId": "2270636286",
                    "name": "Botao Peng"
                },
                {
                    "authorId": "2300654810",
                    "name": "Xiaodong Lee"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "42f26ad1da872416c52b1d03883474f2481dfb04",
            "title": "Efficiently Mitigating the Impact of Data Drift on Machine Learning Pipelines",
            "abstract": "\n Despite the increasing success of Machine Learning (ML) techniques in real-world applications, their maintenance over time remains challenging. In particular, the prediction accuracy of deployed ML models can suffer due to significant changes between training and serving data over time, known as\n data drift.\n Traditional data drift solutions primarily focus on detecting drift, and then retraining the ML models, but do not discern whether the detected drift is harmful to model performance. In this paper, we observe that not all data drifts lead to degradation in prediction accuracy. We then introduce a novel approach for identifying portions of data distributions in serving data where drift can be potentially harmful to model performance, which we term Data Distributions with Low Accuracy (DDLA). Our approach, using decision trees, precisely pinpoints low-accuracy zones within ML models, especially Blackbox models. By focusing on these DDLAs, we effectively assess the impact of data drift on model performance and make informed decisions in the ML pipeline. In contrast to existing data drift techniques, we advocate for model retraining only in cases of harmful drifts that detrimentally affect model performance. Through extensive experimental evaluations on various datasets and models, our findings demonstrate that our approach significantly improves cost-efficiency over baselines, while achieving comparable accuracy.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2319440034",
                    "name": "Sijie Dong"
                },
                {
                    "authorId": "2116068917",
                    "name": "Qitong Wang"
                },
                {
                    "authorId": "70472730",
                    "name": "Sahri Soror"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2318864799",
                    "name": "Divesh Srivastava"
                }
            ]
        },
        {
            "paperId": "51596e1c39f692a2be4fe4cbfec01bd36386392f",
            "title": "Streaming Data Collection With a Private Sketch-Based Protocol",
            "abstract": "Data stream collection is critical to analyze service conditions and detect anomalies in time, especially in Internet of Things. However, it may undermine the individual privacy. Local differential privacy (LDP) has recently become a popular privacy-preserving technique protecting users\u2019 privacy. However, most of them are still limited to the assumption of one-item collection, resulting in poor utility when extended to the multi-item collection from a very large domain. This article proposes a private streaming data collection framework, private sketch-based framework (PSF), which takes advantage of sketches. Combining the proposed background information and a decode-first collection-side workflow, the framework improves the utility by reducing the errors introduced by the sketching algorithm and the privacy budget utilization when collecting multiple items. We analytically prove the superior accuracy and privacy characteristics of PSF. In order to support specific computing tasks, we build two private protocols based on PSF, PrivSketch and PrivSketch+, aiming at frequency estimation and mean estimation, respectively. We demonstrate the utility of PrivSketch and PrivSketch+ theoretically, and also evaluate them experimentally. Our evaluation, with several diverse synthetic and real data sets, demonstrates that PrivSketch is 1\u20133 orders of magnitude better than the competitors in terms of utility in both frequency estimation and frequent item estimation, while being up to ~100x faster. PrivSketch+ performs ~4 orders of magnitude better than advanced solutions, such as piecewise mechanism (PM) and hybrid mechanism (HM), under a limited privacy budget.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220577831",
                    "name": "Ying Li"
                },
                {
                    "authorId": "2300654810",
                    "name": "Xiaodong Lee"
                },
                {
                    "authorId": "2270636286",
                    "name": "Botao Peng"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2153235746",
                    "name": "Jing'an Xue"
                }
            ]
        },
        {
            "paperId": "5554bc406553f13bcb46ebe27dda0653139d56d3",
            "title": "$\\boldsymbol{Steiner}$-Hardness: A Query Hardness Measure for Graph-Based ANN Indexes",
            "abstract": "Graph-based indexes have been widely employed to accelerate approximate similarity search of high-dimensional vectors. However, the performance of graph indexes to answer different queries varies vastly, leading to an unstable quality of service for downstream applications. This necessitates an effective measure to test query hardness on graph indexes. Nonetheless, popular distance-based hardness measures like LID lose their effects due to the ignorance of the graph structure. In this paper, we propose $Steiner$-hardness, a novel connection-based graph-native query hardness measure. Specifically, we first propose a theoretical framework to analyze the minimum query effort on graph indexes and then define $Steiner$-hardness as the minimum effort on a representative graph. Moreover, we prove that our $Steiner$-hardness is highly relevant to the classical Directed $Steiner$ Tree (DST) problems. In this case, we design a novel algorithm to reduce our problem to DST problems and then leverage their solvers to help calculate $Steiner$-hardness efficiently. Compared with LID and other similar measures, $Steiner$-hardness shows a significantly better correlation with the actual query effort on various datasets. Additionally, an unbiased evaluation designed based on $Steiner$-hardness reveals new ranking results, indicating a meaningful direction for enhancing the robustness of graph indexes. This paper is accepted by PVLDB 2025.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268875699",
                    "name": "Zeyu Wang"
                },
                {
                    "authorId": "2116068917",
                    "name": "Qitong Wang"
                },
                {
                    "authorId": "2316954137",
                    "name": "Xiaoxing Cheng"
                },
                {
                    "authorId": "2268871272",
                    "name": "Peng Wang"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2314075475",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "780784e3d1db5dec7bc5462d0f843e2cb3f655a5",
            "title": "An Interactive Dive into Time-Series Anomaly Detection",
            "abstract": "Anomaly detection is an important problem in data analytics with applications in many domains. In recent years, there has been an increasing interest in anomaly detection tasks applied to time series. In this tutorial, we take a holistic view of anomaly detection in time series, starting from the core definitions and taxonomies related to time series and anomaly types, to an extensive description of the anomaly detection methods proposed by different communities in the literature. We explore the literature and the proposed methods by demonstrating systems that help users understand the core computational steps of some methods and navigate benchmark results. Finally, we describe the problem of model selection for anomaly detection and discuss recent experimental results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "2516699",
                    "name": "John Paparrizos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "83734272d78baa3517416ebc962bcbb5accc3346",
            "title": "Analysing Multi-Task Regression via Random Matrix Theory with Application to Time Series Forecasting",
            "abstract": "In this paper, we introduce a novel theoretical framework for multi-task regression, applying random matrix theory to provide precise performance estimations, under high-dimensional, non-Gaussian data distributions. We formulate a multi-task optimization problem as a regularization technique to enable single-task models to leverage multi-task learning information. We derive a closed-form solution for multi-task optimization in the context of linear models. Our analysis provides valuable insights by linking the multi-task learning performance to various model statistics such as raw data covariances, signal-generating hyperplanes, noise levels, as well as the size and number of datasets. We finally propose a consistent estimation of training and testing errors, thereby offering a robust foundation for hyperparameter optimization in multi-task regression scenarios. Experimental validations on both synthetic and real-world datasets in regression and multivariate time series forecasting demonstrate improvements on univariate models, incorporating our method into the training loss and thus leveraging multivariate information.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2270260148",
                    "name": "Romain Ilbert"
                },
                {
                    "authorId": "51461305",
                    "name": "Malik Tiomoko"
                },
                {
                    "authorId": "9536398",
                    "name": "Cosme Louart"
                },
                {
                    "authorId": "2261363007",
                    "name": "Ambroise Odonnat"
                },
                {
                    "authorId": "66383219",
                    "name": "Vasilii Feofanov"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "145898069",
                    "name": "I. Redko"
                }
            ]
        },
        {
            "paperId": "c8f5f6d981d96ebc84ba0a2665f00be3d1cfc27e",
            "title": "SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention",
            "abstract": "Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses current state-of-the-art methods and is on par with the biggest foundation model MOIRAI while having significantly fewer parameters. The code is available at https://github.com/romilbert/samformer.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2270260148",
                    "name": "Romain Ilbert"
                },
                {
                    "authorId": "2261363007",
                    "name": "Ambroise Odonnat"
                },
                {
                    "authorId": "66383219",
                    "name": "Vasilii Feofanov"
                },
                {
                    "authorId": "8229598",
                    "name": "Aladin Virmaux"
                },
                {
                    "authorId": "2284224860",
                    "name": "Giuseppe Paolo"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "145898069",
                    "name": "I. Redko"
                }
            ]
        },
        {
            "paperId": "ca88eea9b900751f8477b7b1caf18f000d07cee3",
            "title": "A Critical Re-evaluation of Record Linkage Benchmarks for Learning-Based Matching Algorithms",
            "abstract": "Entity resolution (ER) is the process of identifying records that refer to the same entities within one or across multiple databases. Numerous techniques have been developed to tackle ER challenges over the years, with recent emphasis placed on machine and deep learning methods for the matching phase. However, the quality of the benchmark datasets typically used in the experimental evaluations of learning-based matching algorithms has not been examined in the literature. To cover this gap, we propose four complementary approaches to assessing the difficulty and appropriateness of 13 commonly used datasets: two theoretical ones, which involve new measures of linearity and existing measures of complexity, and two practical ones - the difference between the best non-linear and linear matchers, as well as the difference between the best learning-based matcher and the perfect oracle. Our analysis demonstrates that most existing benchmark datasets pose rather easy classification tasks. As a result, they are not suitable for properly evaluating learning-based matching algorithms. To address this issue, we propose a new methodology for yielding benchmark datasets. We put it into practice by creating four new matching tasks, and we verify that these new benchmarks are more challenging and therefore more suitable for further advancements in the field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2239519749",
                    "name": "George Papadakis"
                },
                {
                    "authorId": "1431726968",
                    "name": "Nishadi Kirielle"
                },
                {
                    "authorId": "2312756808",
                    "name": "Peter Christen"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "ecaddbcc7c71ad9d63543a15ed9bd399b92e1995",
            "title": "User-friendly Foundation Model Adapters for Multivariate Time Series Classification",
            "abstract": "Foundation models, while highly effective, are often resource-intensive, requiring substantial inference time and memory. This paper addresses the challenge of making these models more accessible with limited computational resources by exploring dimensionality reduction techniques. Our goal is to enable users to run large pre-trained foundation models on standard GPUs without sacrificing performance. We investigate classical methods such as Principal Component Analysis alongside neural network-based adapters, aiming to reduce the dimensionality of multivariate time series data while preserving key features. Our experiments show up to a 10x speedup compared to the baseline model, without performance degradation, and enable up to 4.5x more datasets to fit on a single GPU, paving the way for more user-friendly and scalable foundation models.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "66383219",
                    "name": "Vasilii Feofanov"
                },
                {
                    "authorId": "2270260148",
                    "name": "Romain Ilbert"
                },
                {
                    "authorId": "51461305",
                    "name": "Malik Tiomoko"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "145898069",
                    "name": "I. Redko"
                }
            ]
        },
        {
            "paperId": "163c06d8d21331edc5aaf68dd9b25f37a58ca980",
            "title": "Odyssey: A Journey in the Land of Distributed Data Series Similarity Search",
            "abstract": "\n This paper presents Odyssey, a novel\n distributed\n data-series processing framework that efficiently addresses the critical challenges of exhibiting good speedup and ensuring high scalability in data series processing by taking advantage of the full computational capacity of modern distributed systems comprised of multi-core servers. Odyssey addresses a number of challenges in designing efficient and highly-scalable\n distributed\n data series index, including efficient scheduling, and load-balancing without paying the prohibitive cost of moving data around. It also supports a flexible partial replication scheme, which enables Odyssey to navigate through a fundamental trade-off between data scalability and good performance during query answering. Through a wide range of configurations and using several real and synthetic datasets, our experimental analysis demonstrates that Odyssey achieves its challenging goals.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2133084924",
                    "name": "Manos Chatzakis"
                },
                {
                    "authorId": "1745030",
                    "name": "P. Fatourou"
                },
                {
                    "authorId": "9845452",
                    "name": "Eleftherios Kosmas"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2054590163",
                    "name": "Botao Peng"
                }
            ]
        },
        {
            "paperId": "256c9978e76871f9cc42d695c9d0ea0acfb90166",
            "title": "Elpis: Graph-Based Similarity Search for Scalable Data Science",
            "abstract": "\n The recent popularity of learned embeddings has fueled the growth of massive collections of high-dimensional (high-d) vectors that model complex data. Finding similar vectors in these collections is at the core of many important and practical data science applications. The data series community has developed tree-based similarity search techniques that outperform state-of-the-art methods on large collections of both data series and generic high-d vectors, on all scenarios except for no-guarantees\n ng\n -approximate search, where graph-based approaches designed by the high-d vector community achieve the best performance. However, building graph-based indexes is extremely expensive both in time and space. In this paper, we bring these two worlds together, study the corresponding solutions and their performance behavior, and propose ELPIS, a new strong baseline that takes advantage of the best features of both to achieve a superior performance in terms of indexing and ng-approximate search in-memory. ELPIS builds the index 3x-8x faster than competitors, using 40% less memory. It also achieves a high recall of 0.99, up to 2x faster than the state-of-the-art methods, and answers 1-NN queries up to one order of magnitude faster.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2214656158",
                    "name": "Ilias Azizi"
                },
                {
                    "authorId": "51935231",
                    "name": "Karima Echihabi"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "393b8a0894c333a2e7921129ceb27854bd230dd4",
            "title": "Towards a Generic Framework for Mechanism-guided Deep Learning for Manufacturing Applications",
            "abstract": "Manufacturing data analytics tasks are traditionally undertaken with Mechanism Models (MMs), which are domain-specific mathematical equations modeling the underlying physical or chemical processes of the tasks. Recently, Deep Learning (DL) has been increasingly applied to manufacturing. MMs and DL have their individual pros and cons, motivating the development of Mechanism-guided Deep Learning Models (MDLMs) that combine the two. Existing MDLMs are often tailored to specific tasks or types of MMs, and can fail to effectively 1) utilize interconnections of multiple input examples, 2) adaptively self-correct prediction errors with error bounding, and 3) ensemble multiple MMs. In this work, we propose a generic, task-agnostic MDLM framework that can embed one or more MMs in deep networks, and address the 3 aforementioned issues. We present 2 diverse use cases where we experimentally demonstrate the effectiveness and efficiency of our models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119079859",
                    "name": "Hanbo Zhang"
                },
                {
                    "authorId": "2156137992",
                    "name": "Jiangxin Li"
                },
                {
                    "authorId": "2198887752",
                    "name": "Shen Liang"
                },
                {
                    "authorId": "46808587",
                    "name": "Peng Wang"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2146563002",
                    "name": "Chen Wang"
                },
                {
                    "authorId": "145200778",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "2041317462",
                    "name": "Haoxuan Zhou"
                },
                {
                    "authorId": "2227535256",
                    "name": "Jianwei Song"
                },
                {
                    "authorId": "2227762768",
                    "name": "Wen Lu"
                }
            ]
        },
        {
            "paperId": "41b36ae4c127cd28d4c179bb90204eb121483b81",
            "title": "A Hierarchical Transformer Encoder to Improve Entire Neoplasm Segmentation on Whole Slide Images of Hepatocellular Carcinoma",
            "abstract": "In digital histopathology, entire neoplasm segmentation on Whole Slide Image (WSI) of Hepatocellular Carcinoma (HCC) plays an important role, especially as a preprocessing filter to automatically exclude healthy tissue, in histological molecular correlations mining and other downstream histopathological tasks. The segmentation task remains challenging due to HCC\u2019s inherent high-heterogeneity and the lack of dependency learning in large field of view. In this article, we propose a novel deep learning architecture with a hierarchical Transformer encoder, HiTrans, to learn the global dependencies within expanded 4096\u00d74096 WSI patches. Hi-Trans is designed to encode and decode the patches with larger reception fields and the learned global dependencies, compared to the state-of-the-art Fully Convolutional Neural networks (FCNN). Empirical evaluations verified that Hi-Trans leads to better segmentation performance by taking into account regional and global dependency information.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223180400",
                    "name": "Zhuxian Guo"
                },
                {
                    "authorId": null,
                    "name": "Qitong Wang"
                },
                {
                    "authorId": "2151194032",
                    "name": "H. M\u00fcller"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2310815",
                    "name": "N. Lom\u00e9nie"
                },
                {
                    "authorId": "2255097",
                    "name": "Camille Kurtz"
                }
            ]
        },
        {
            "paperId": "51b4a5ffbfa18140217b7b5f124488d19626afba",
            "title": "Appliance Detection Using Very Low-Frequency Smart Meter Time Series",
            "abstract": "In recent years, smart meters have been widely adopted by electricity suppliers to improve the management of the smart grid system. These meters usually collect energy consumption data at a very low frequency (every 30min), enabling utilities to bill customers more accurately. To provide more personalized recommendations, the next step is to detect the appliances owned by customers, which is a challenging problem, due to the very-low meter reading frequency. Even though the appliance detection problem can be cast as a time series classification problem, with many such classifiers having been proposed in the literature, no study has applied and compared them on this specific problem. This paper presents an in-depth evaluation and comparison of state-of-the-art time series classifiers applied to detecting the presence/absence of diverse appliances in very low-frequency smart meter data. We report results with five real datasets. We first study the impact of the detection quality of 13 different appliances using 30min sampled data, and we subsequently propose an analysis of the possible detection performance gain by using a higher meter reading frequency. The results indicate that the performance of current time series classifiers varies significantly. Some of them, namely deep learning-based classifiers, provide promising results in terms of accuracy (especially for certain appliances), even using 30min sampled data, and are scalable to the large smart meter time series collections of energy consumption data currently available to electricity suppliers. Nevertheless, our study shows that more work is needed in this area to further improve the accuracy of the proposed solutions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2217691205",
                    "name": "Adrien Petralia"
                },
                {
                    "authorId": "4687593",
                    "name": "P. Charpentier"
                },
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "5808760f356feb31ee06f7494d924171cf4eabd5",
            "title": "ADF & TransApp: A Transformer-Based Framework for Appliance Detection Using Smart Meter Consumption Series",
            "abstract": "Over the past decade, millions of smart meters have been installed by electricity suppliers worldwide, allowing them to collect a large amount of electricity consumption data, albeit sampled at a low frequency (one point every 30min). One of the important challenges these suppliers face is how to utilize these data to detect the presence/absence of different appliances in the customers' households. This valuable information can help them provide personalized offers and recommendations to help customers towards the energy transition. Appliance detection can be cast as a time series classification problem. However, the large amount of data combined with the long and variable length of the consumption series pose challenges when training a classifier. In this paper, we propose ADF, a framework that uses subsequences of a client consumption series to detect the presence/absence of appliances. We also introduce TransApp, a Transformer-based time series classifier that is first pretrained in a self-supervised way to enhance its performance on appliance detection tasks. We test our approach on two real datasets, including a publicly available one. The experimental results with two large real datasets show that the proposed approach outperforms current solutions, including state-of-the-art time series classifiers applied to appliance detection.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2217691205",
                    "name": "Adrien Petralia"
                },
                {
                    "authorId": "2279024441",
                    "name": "Philippe Charpentier"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "5f5151ff7a618f9c0ed236a96c760808ed259fe3",
            "title": "Graph- and Tree-based Indexes for High-dimensional Vector Similarity Search: Analyses, Comparisons, and Future Directions",
            "abstract": "Approximate nearest neighbor search on high-dimensional vectors is a crucial component for numerous applications in various \ufb01elds. To solve this problem ef\ufb01ciently, dozens of indexes have been proposed in the past decades. Among them, graph-based indexes show superior query performance in memory while tree-based indexes achieve the best scalability and building ef\ufb01ciency. In this paper, we systematically study the evolution of these two kinds of indexes and the recent progress with ablation studies and analyses, which help understand where the performance improvement comes from and the difference between different index families. Moreover, we conduct a comparative study over these two index families and discuss the existing and potential combinations of them. We believe this study can serve as a guide to the most promising directions for addressing the open problems in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268875699",
                    "name": "Zeyu Wang"
                },
                {
                    "authorId": "2268871272",
                    "name": "Peng Wang"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2273964317",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "6d13e221dad0497af2fb994027739f57bacda46c",
            "title": "A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms",
            "abstract": "Entity resolution (ER) is the process of identifying records that refer to the same entities within one or across multiple databases. Numerous techniques have been developed to tackle ER challenges over the years, with recent emphasis placed on machine and deep learning methods for the matching phase. However, the quality of the benchmark datasets typically used in the experimental evaluations of learning-based matching algorithms has not been examined in the literature. To cover this gap, we propose four different approaches to assessing the difficulty and appropriateness of 13 established datasets: two theoretical approaches, which involve new measures of linearity and existing measures of complexity, and two practical approaches: the difference between the best non-linear and linear matchers, as well as the difference between the best learning-based matcher and the perfect oracle. Our analysis demonstrates that most of the popular datasets pose rather easy classification tasks. As a result, they are not suitable for properly evaluating learning-based matching algorithms. To address this issue, we propose a new methodology for yielding benchmark datasets. We put it into practice by creating four new matching tasks, and we verify that these new benchmarks are more challenging and therefore more suitable for further advancements in the field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "1431726968",
                    "name": "Nishadi Kirielle"
                },
                {
                    "authorId": "145188508",
                    "name": "P. Christen"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "7f4efe3c7d3a2b7768890337ad1a29b1493f0617",
            "title": "A general approach to Supervised Meta-Blocking",
            "abstract": "Entity Resolution is a core data integration task that relies on Blocking to scale to large datasets. Schema-agnostic blocking achieves very high recall, requires no domain knowledge, and works on data with any structure. The main drawback of this approach is the number of generated superfluous comparisons (i.e. non-matching), which can be reduced by Meta-blocking techniques that aim to discard most of them. Unsupervised Meta-blocking performs this process by scoring each comparison with a single metric and then applying a pruning algorithm, so choosing the right metric among the existing ones is fundamental to achieving good results. Supervised Meta-blocking improves this approach by combining multiple scores per comparison into a feature vector that is fed to a binary classifier used to decide if a comparison is a match or not. In this work, we generalize the Supervised Meta-blocking approach by using a probabilistic classifier that pairs each comparison with a score that represents its likelihood to be a match, allowing to use of any pruning algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3000851",
                    "name": "Luca Gagliardelli"
                },
                {
                    "authorId": "2239519749",
                    "name": "George Papadakis"
                },
                {
                    "authorId": "2301232",
                    "name": "Giovanni Simonini"
                },
                {
                    "authorId": "2239502655",
                    "name": "Sonia Bergamaschi"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "87432f6ad260cd940cf8de11ce40edf4c0f36dc0",
            "title": "Dumpy: A Compact and Adaptive Index for Large Data Series Collections",
            "abstract": "Data series indexes are necessary for managing and analyzing the increasing amounts of data series collections that are nowadays available. These indexes support both exact and approximate similarity search, with approximate search providing high-quality results within milliseconds, which makes it very attractive for certain modern applications. Reducing the pre-processing (i.e., index building) time and improving the accuracy of search results are two major challenges. DSTree and the iSAX index family are state-of-the-art solutions for this problem. However, DSTree suffers from long index building times, while iSAX suffers from low search accuracy. In this paper, we identify two problems of the iSAX index family that adversely affect the overall performance. First, we observe the presence of a proximity-compactness trade-off related to the index structure design (i.e., the node fanout degree), significantly limiting the efficiency and accuracy of the resulting index. Second, a skewed data distribution will negatively affect the performance of iSAX. To overcome these problems, we propose Dumpy, an index that employs a novel multi-ary data structure with an adaptive node splitting algorithm and an efficient building workflow. Furthermore, we devise Dumpy-Fuzzy as a variant of Dumpy which further improves search accuracy by proper duplication of series. Experiments with a variety of large, real datasets demonstrate that the Dumpy solutions achieve considerably better efficiency, scalability and search accuracy than its competitors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108726146",
                    "name": "Zeyu Wang"
                },
                {
                    "authorId": null,
                    "name": "Qitong Wang"
                },
                {
                    "authorId": "46808587",
                    "name": "Peng Wang"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "145200778",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "95d38e9394c4a3c63768cd0ee0f2a6152af1f70e",
            "title": "Breaking Boundaries: Balancing Performance and Robustness in Deep Wireless Traffic Forecasting",
            "abstract": "Balancing the trade-off between accuracy and robustness is a long-standing challenge in time series forecasting. While most of existing robust algorithms have achieved certain suboptimal performance on clean data, sustaining the same performance level in the presence of data perturbations remains extremely hard. In this paper, we study a wide array of perturbation scenarios and propose novel defense mechanisms against adversarial attacks using real-world telecom data. We compare our strategy against two existing adversarial training algorithms under a range of maximal allowed perturbations, defined using \\ell_\\infty -norm, \\in [0.1,0.4]. Our findings reveal that our hybrid strategy, which is composed of a classifier to detect adversarial examples, a denoiser to eliminate noise from the perturbed data samples, and a standard forecaster, achieves the best performance on both clean and perturbed data. Our optimal model can retain up to 92.02% the performance of the original forecasting model in terms of Mean Squared Error (MSE) on clean data, while being more robust than the standard adversarially trained models on perturbed data. Its MSE is 2.71\u00d7 and 2.51\u00d7 lower than those of comparing methods on normal and perturbed data, respectively. In addition, the components of our models can be trained in parallel, resulting in better computational efficiency. Our results indicate that we can optimally balance the trade-off between the performance and robustness of forecasting models by improving the classifier and denoiser, even in the presence of sophisticated and destructive poisoning attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2270260148",
                    "name": "Romain Ilbert"
                },
                {
                    "authorId": "2270159504",
                    "name": "Thai V. Hoang"
                },
                {
                    "authorId": "2268020626",
                    "name": "Zonghua Zhang"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "9c35042b6211dc4ab4d29d6083bf71340313205d",
            "title": "PrivSketch: A Private Sketch-based Frequency Estimation Protocol for Data Streams",
            "abstract": "Local differential privacy (LDP) has recently become a popular privacy-preserving data collection technique protecting users' privacy. The main problem of data stream collection under LDP is the poor utility due to multi-item collection from a very large domain. This paper proposes PrivSketch, a high-utility frequency estimation protocol taking advantage of sketches, suitable for private data stream collection. Combining the proposed background information and a decode-first collection-side workflow, PrivSketch improves the utility by reducing the errors introduced by the sketching algorithm and the privacy budget utilization when collecting multiple items. We analytically prove the superior accuracy and privacy characteristics of PrivSketch, and also evaluate them experimentally. Our evaluation, with several diverse synthetic and real datasets, demonstrates that PrivSketch is 1-3 orders of magnitude better than the competitors in terms of utility in both frequency estimation and frequent item estimation, while being up to ~100x faster.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220577831",
                    "name": "Ying Li"
                },
                {
                    "authorId": "2060143933",
                    "name": "Xiaodong Lee"
                },
                {
                    "authorId": "2054590163",
                    "name": "Botao Peng"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2153235746",
                    "name": "Jing'an Xue"
                }
            ]
        },
        {
            "paperId": "abf373555d95d1fa047a4ba7def2abe49b88ef59",
            "title": "Efficient Range and kNN Twin Subsequence Search in Time Series",
            "abstract": "Analyzing time series data is crucial for many applications. In particular, subsequence search refers to finding subsequences within an input time series <inline-formula><tex-math notation=\"LaTeX\">$T$</tex-math><alternatives><mml:math><mml:mi>T</mml:mi></mml:math><inline-graphic xlink:href=\"chatzigeorgakidis-ieq1-3167257.gif\"/></alternatives></inline-formula> that are similar to a query sequence <inline-formula><tex-math notation=\"LaTeX\">$Q$</tex-math><alternatives><mml:math><mml:mi>Q</mml:mi></mml:math><inline-graphic xlink:href=\"chatzigeorgakidis-ieq2-3167257.gif\"/></alternatives></inline-formula>. Existing subsequence search approaches typically employ Euclidean distance or Dynamic Time Warping as similarity measures and address range queries. In this paper, we focus on Chebyshev distance, which is the largest difference between each individual pair of points across the entire length of two compared subsequences. We call such similar pairs <italic>twins</italic>. We first show how existing time series indices can be extended to perform twin subsequence search. Then, we introduce TS-Index, a novel index tailored to the computation of twin subsequence search queries. Moreover, given that specifying a distance threshold is often not straightforward, we show how TS-Index can also be used to evaluate <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"chatzigeorgakidis-ieq3-3167257.gif\"/></alternatives></inline-formula>NN queries. Our extensive experimental evaluation compares these approaches using real time series datasets. The results demonstrate that TS-Index can retrieve twin subsequences faster than all other methods under various conditions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2842794",
                    "name": "Georgios Chatzigeorgakidis"
                },
                {
                    "authorId": "1807115",
                    "name": "Dimitrios Skoutas"
                },
                {
                    "authorId": "1762083",
                    "name": "Kostas Patroumpas"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2587884",
                    "name": "Spiros Athanasiou"
                },
                {
                    "authorId": "1688897",
                    "name": "Spiros Skiadopoulos"
                }
            ]
        },
        {
            "paperId": "b89d4f8a99f1695dc5b819d01752bb536137dd4e",
            "title": "FreSh: A Lock-Free Data Series Index",
            "abstract": "We present FreSh, a lock-free data series index that exhibits good performance (while being robust). FreSh is based on Refresh, which is a generic approach we have developed for supporting lock-freedom in an efficient way on top of any locality-aware data series index. We believe Refresh is of independent interest and can be used to get well-performed lock-free versions of other locality-aware blocking data structures. For developing FreSh, we first studied in depth the design decisions of current state-of-the-art data series indexes, and the principles governing their performance. This led to a theoretical framework, which enables the development and analysis of data series indexes in a modular way. The framework allowed us to apply Refresh, repeatedly, to get lock-free versions of the different phases of a family of data series indexes. Experiments with several synthetic and real datasets illustrate that FreSh achieves performance that is as good as that of the state-of-the-art blocking in-memory data series index. This shows that the helping mechanisms of FreSh are light-weight, respecting certain principles that are crucial for performance in locality-aware data structures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1745030",
                    "name": "P. Fatourou"
                },
                {
                    "authorId": "9845452",
                    "name": "Eleftherios Kosmas"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "6247282",
                    "name": "G. Paterakis"
                }
            ]
        },
        {
            "paperId": "c8065725bb05ede0280f54c6a0dd67c1c92da41e",
            "title": "Choose Wisely: An Extensive Evaluation of Model Selection for Anomaly Detection in Time Series",
            "abstract": "Anomaly detection is a fundamental task for time-series analytics with important implications for the downstream performance of many applications. Despite increasing academic interest and the large number of methods proposed in the literature, recent benchmark and evaluation studies demonstrated that no overall best anomaly detection methods exist when applied to very heterogeneous time series datasets. Therefore, the only scalable and viable solution to solve anomaly detection over very different time series collected from diverse domains is to propose a model selection method that will select, based on time series characteristics, the best anomaly detection method to run. Existing AutoML solutions are, unfortunately, not directly applicable to time series anomaly detection, and no evaluation of time series-based approaches for model selection exists. Towards that direction, this paper studies the performance of time series classification methods used as model selection for anomaly detection. Overall, we compare 17 different classifiers over 1800 time series, and we propose the first extensive experimental evaluation of time series classification as model selection for anomaly detection. Our results demonstrate that model selection methods outperform every single anomaly detection method while being in the same order of magnitude regarding execution time. This evaluation is the first step to demonstrate the accuracy and efficiency of time series classification algorithms for anomaly detection, and represents a strong baseline that can then be used to guide the model selection step in general AutoML pipelines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167783581",
                    "name": "Emmanouil Sylligardos"
                },
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "2516699",
                    "name": "John Paparrizos"
                },
                {
                    "authorId": "8032437",
                    "name": "P. Trahanias"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "cd7a08ecee3ffe8f67344a4d77284322fd5ab082",
            "title": "SEAnet: A Deep Learning Architecture for Data Series Similarity Search",
            "abstract": "A key operation for massive data series collection analysis is similarity search. According to recent studies, SAX-based indexes offer state-of-the-art performance for similarity search tasks. However, their performance lags under high-frequency, weakly correlated, excessively noisy, or other dataset-specific properties. In this work, we propose Deep Embedding Approximation (DEA), a novel family of data series summarization techniques based on deep neural networks. Moreover, we describe SEAnet, a novel architecture especially designed for learning DEA, that introduces the Sum of Squares preservation property into the deep network design. We further enhance SEAnet with SEAtrans encoder. Finally, we propose novel sampling strategies, SEAsam and SEAsamE, that allow SEAnet to effectively train on massive datasets. Comprehensive experiments on 7 diverse synthetic and real datasets verify the advantages of DEA learned using SEAnet in providing high-quality data series summarizations and similarity search results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Qitong Wang"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "ee123f5ade727d02844593fe4d4eeb50739a6243",
            "title": "New Trends in Time Series Anomaly Detection",
            "abstract": "Anomaly detection is an important problem in data analytics with applications in many domains. In recent years, there has been an increasing interest in anomaly detection tasks applied to time series. In this tutorial, we take a holistic view on anomaly detection in time series, starting from the core definitions and tax-onomies related to time series and anomaly types, to an extensive description of the anomaly detection methods proposed by different communities in the literature. Then, we discuss shortcomings in traditional evaluation measures. Finally, we present new solutions to assess the quality of anomaly detection approaches and new benchmarks capturing diverse domains and applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "2214338691",
                    "name": "John Paparizzos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "1add007a5e7938524605f15fef60c073e259dea4",
            "title": "Scalable Analytics on Large Sequence Collections",
            "abstract": "Data series are a prevalent data type that has attracted lots of interest in recent years. Specifically, there has been an explosive interest towards the analysis of large volumes of data series in many different domains, and in particular, in the Internet of Things (IoT). In this tutorial, we focus on applications that produce massive collections of data series, and we provide the necessary background on data series management and analytics. Moreover, we discuss the need for fast similarity search for supporting machine learning applications, and describe efficient similarity search techniques, indexes and query processing algorithms. Finally, we discuss the role that deep learning techniques can play in this context. We conclude with the challenges and open research problems in this domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51935231",
                    "name": "Karima Echihabi"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "3bff26a559cd98837febc9b3f87553d0565c5e3b",
            "title": "Generalized Supervised Meta-blocking (technical report)",
            "abstract": "Entity Resolution constitutes a core data integration task that relies on Blocking in order to tame its quadratic time complexity. Schema-agnostic blocking achieves very high recall, requires no domain knowledge and applies to data of any structuredness and schema heterogeneity. This comes at the cost of many irrelevant candidate pairs (i.e., comparisons), which can be significantly reduced through Meta-blocking techniques, i.e., techniques that leverage the co-occurrence patterns of entities inside the blocks: first, a weighting scheme assigns a score to every pair of candidate entities in proportion to the likelihood that they are matching and then, a pruning algorithm discards the pairs with the lowest scores. Supervised Meta-blocking goes beyond this approach by combining multiple scores per comparison into a feature vector that is fed to a binary classifier. By using probabilistic classifiers, Generalized Supervised Meta-blocking associates every pair of candidates with a score that can be used by any pruning algorithm. For higher effectiveness, new weighting schemes are examined as features. Through an extensive experimental analysis, we identify the best pruning algorithms, their optimal sets of features as well as the minimum possible size of the training set. The resulting approaches achieve excellent performance across several established benchmark datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3000851",
                    "name": "Luca Gagliardelli"
                },
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "2301232",
                    "name": "Giovanni Simonini"
                },
                {
                    "authorId": "2724566",
                    "name": "S. Bergamaschi"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "4389b25c1a26fe22d76ce3cf5c8c8e24b5a714a9",
            "title": "Hercules Against Data Series Similarity Search",
            "abstract": "We propose Hercules, a parallel tree-based technique for exact similarity search on massive disk-based data series collections. We present novel index construction and query answering algorithms that leverage different summarization techniques, carefully schedule costly operations, optimize memory and disk accesses, and exploit the multi-threading and SIMD capabilities of modern hardware to perform CPU-intensive calculations. We demonstrate the superiority and robustness of Hercules with an extensive experimental evaluation against state-of-the-art techniques, using many synthetic and real datasets, and query workloads of varying difficulty. The results show that Hercules performs up to one order of magnitude faster than the best competitor (which is not always the same). Moreover, Hercules is the only index that outperforms the optimized scan on all scenarios, including the hard query workloads on disk-based datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51935231",
                    "name": "Karima Echihabi"
                },
                {
                    "authorId": "1745030",
                    "name": "P. Fatourou"
                },
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1795133",
                    "name": "H. Benbrahim"
                }
            ]
        },
        {
            "paperId": "72b579f4d34c5be86722ead5b8a14645ae092105",
            "title": "Theseus: Navigating the Labyrinth of Time-Series Anomaly Detection",
            "abstract": "The detection of anomalies in time series has gained ample academic and industrial attention, yet, no comprehensive benchmark exists to evaluate time-series anomaly detection methods. Therefore, there is no final verdict on which method performs the best (and under what conditions). Consequently, we often observe methods performing exceptionally well on one dataset but surprisingly poorly on another, creating an illusion of progress. To address these issues, we thoroughly studied over one hundred papers, and summarized our effort in TSB-UAD, a new benchmark to evaluate univariate time series anomaly detection methods. In this paper, we describe Theseus, a modular and extensible web application that helps users navigate through the benchmark, and reason about the merits and drawbacks of both anomaly detection methods and accuracy measures, under different conditions. Overall, our system enables users to compare 12 anomaly detection methods on 1980 time series, using 13 accuracy measures, and decide on the most suitable method and measure for some application.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "2516699",
                    "name": "John Paparrizos"
                },
                {
                    "authorId": "8332236",
                    "name": "Yuhao Kang"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "32360997",
                    "name": "R. Tsay"
                },
                {
                    "authorId": "114189441",
                    "name": "Aaron J. Elmore"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                }
            ]
        },
        {
            "paperId": "75f84819bef728ddf2a19884e3bf4d3b66e07855",
            "title": "dCAM: Dimension-wise Class Activation Map for Explaining Multivariate Data Series Classification",
            "abstract": "Data series classification is an important and challenging problem in data science. Explaining the classification decisions by finding the discriminant parts of the input that led the algorithm to some decision is a real need in many applications. Convolutional neural networks perform well for the data series classification task; though, the explanations provided by this type of algorithms are poor for the specific case of multivariate data series. Addressing this important limitation is a significant challenge. In this paper, we propose a novel method that solves this problem by highlighting both the temporal and dimensional discriminant information. Our contribution is two-fold: we first describe a convolutional architecture that enables the comparison of dimensions; then, we propose a method that returns dCAM, a Dimension-wise Class Activation Map specifically designed for multivariate time series (and CNN-based models). Experiments with several synthetic and real datasets demonstrate that dCAM is not only more accurate than previous approaches, but the only viable solution for discriminant feature discovery and classification explanation in multivariate time series.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "143805710",
                    "name": "M. Meftah"
                },
                {
                    "authorId": "34082089",
                    "name": "E. Remy"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "994b5072e247006738bad060fcd1ac09e39dbdef",
            "title": "SA-Q: Observing, Evaluating, and Enhancing the Quality of the Results of Sentiment Analysis Tools",
            "abstract": "Sentiment analysis has received constant research attention due to its usefulness and importance in different applications. However, despite the research advances in this field, most current tools suffer in prediction quality due to the inconsistencies in their results, i.e., intra- and inter-tool inconsistencies. This demonstration proposes a system for the evaluation of sentiment analysis quality namely SA-Q. The system allows the evaluation of inconsistency in sentiment analysis tools, the resolution of the inconsistency using state-of-the-art methods and the recommendation of relevant sentiment analysis tool for any type of data set provided by the attendees. It allows the attendees to compare the tools. Moreover, we demonstrate that SA-Q evaluates the consistency of tools on two levels (intra-tool and inter-tool). Through various scenarios, we showcase the challenges of inconsistency resolution, demonstrate the usefulness of the proposed system and the recommendations that can be given to the attendees for their datasets. We demonstrate that SA-Q system has practical utility in many areas of industrial applications for better decision making. This demonstration shows promising research areas for data management, NLP, and machine learning communities by adopting and drawing inspiration from truth inference methods to create more robust tools and improve the tool's scalability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057858196",
                    "name": "Wissam Mammar Kouadri"
                },
                {
                    "authorId": "1795027",
                    "name": "S. Benbernou"
                },
                {
                    "authorId": "1803906",
                    "name": "Mourad Ouziri"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "40488988",
                    "name": "Iheb Ben Amor"
                }
            ]
        },
        {
            "paperId": "a97d40de9b06bec13dae6105305673c98275b51d",
            "title": "iEDeaL: A Deep Learning Framework for Detecting Highly Imbalanced Interictal Epileptiform Discharges",
            "abstract": "\n Epilepsy is a chronic neurological disease, ranked as the second most burdensome neurological disorder worldwide. Detecting Interictal Epileptiform Discharges (IEDs) is among the most important clinician operations to support epilepsy diagnosis, rendering automatic IED detection based on electroencephalography (EEG) signals an important topic. However, most existing solutions were designed and evaluated upon artificially balanced IED datasets, which do not conform to the real-world highly imbalanced scenarios. In this work, we propose the iEDeaL framework for automatic IED detection in challenging real-world use cases. The main components of iEDeaL are the new SC neural network architecture, to efficiently detect IEDs on raw EEG series instead of extracted features, and SaSu, a novel loss function to train SC by optimizing the\n F\n \u03b2\n -score. Experiments on two real-world imbalanced IED datasets verify the advantages of iEDeaL in offering more accurate and efficient IED detection when compared with other state-of-the-art deep learning-based and spectrogram feature-based solutions.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Qitong Wang"
                },
                {
                    "authorId": "49139103",
                    "name": "S. Whitmarsh"
                },
                {
                    "authorId": "2104445675",
                    "name": "Vincent Navarro"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "dfed1188c2bdff8a823454aa99d53462cd8e04cc",
            "title": "Generalized Supervised Meta-blocking",
            "abstract": "Entity Resolution is a core data integration task that relies on Blocking to scale to large datasets. Schema-agnostic blocking achieves very high recall, requires no domain knowledge and applies to data of any structuredness and schema heterogeneity. This comes at the cost of many irrelevant candidate pairs (i.e., comparisons), which can be significantly reduced by Meta-blocking techniques that leverage the entity co-occurrence patterns inside blocks: first, pairs of candidate entities are weighted in proportion to their matching likelihood, and then, pruning discards the pairs with the lowest scores. Supervised Meta-blocking goes beyond this approach by combining multiple scores per comparison into a feature vector that is fed to a binary classifier. By using probabilistic classifiers, Generalized Supervised Meta-blocking associates every pair of candidates with a score that can be used by any pruning algorithm. For higher effectiveness, new weighting schemes are examined as features. Through extensive experiments, we identify the best pruning algorithms, their optimal sets of features, as well as the minimum possible size of the training set.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3000851",
                    "name": "Luca Gagliardelli"
                },
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "2301232",
                    "name": "Giovanni Simonini"
                },
                {
                    "authorId": "2724566",
                    "name": "S. Bergamaschi"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "f354feb334584568d0aea9aa8a7697e318ebb7ac",
            "title": "TSB-UAD: An End-to-End Benchmark Suite for Univariate Time-Series Anomaly Detection",
            "abstract": "The detection of anomalies in time series has gained ample academic and industrial attention. However, no comprehensive benchmark exists to evaluate time-series anomaly detection methods. It is common to use (i) proprietary or synthetic data, often biased to support particular claims; or (ii) a limited collection of publicly available datasets. Consequently, we often observe methods performing exceptionally well in one dataset but surprisingly poorly in another, creating an illusion of progress. To address the issues above, we thoroughly studied over one hundred papers to identify, collect, process, and systematically format datasets proposed in the past decades. We summarize our effort in TSB-UAD, a new benchmark to ease the evaluation of univariate time-series anomaly detection methods. Overall, TSB-UAD contains 13766 time series with labeled anomalies spanning different domains with high variability of anomaly types, ratios, and sizes. TSB-UAD includes 18 previously proposed datasets containing 1980 time series and we contribute two collections of datasets. Specifically, we generate 958 time series using a principled methodology for transforming 126 time-series classification datasets into time series with labeled anomalies. In addition, we present data transformations with which we introduce new anomalies, resulting in 10828 time series with varying complexity for anomaly detection. Finally, we evaluate 12 representative methods demonstrating that TSB-UAD is a robust resource for assessing anomaly detection methods. We make our data and code available at www.timeseries.org/TSB-UAD. TSB-UAD provides a valuable, reproducible, and frequently updated resource to establish a leaderboard of univariate time-series anomaly detection methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2516699",
                    "name": "John Paparrizos"
                },
                {
                    "authorId": "8332236",
                    "name": "Yuhao Kang"
                },
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "2173928492",
                    "name": "Ruey Tsay"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                }
            ]
        },
        {
            "paperId": "f93004d558d9f9cbc98d8667c9ae606e5b3ef9b7",
            "title": "Volume Under the Surface: A New Accuracy Evaluation Measure for Time-Series Anomaly Detection",
            "abstract": "Anomaly detection (AD) is a fundamental task for time-series analytics with important implications for the downstream performance of many applications. In contrast to other domains where AD mainly focuses on point-based anomalies (i.e., outliers in standalone observations), AD for time series is also concerned with range-based anomalies (i.e., outliers spanning multiple observations). Nevertheless, it is common to use traditional point-based information retrieval measures, such as Precision, Recall, and F-score, to assess the quality of methods by thresholding the anomaly score to mark each point as an anomaly or not. However, mapping discrete labels into continuous data introduces unavoidable shortcomings, complicating the evaluation of range-based anomalies. Notably, the choice of evaluation measure may significantly bias the experimental outcome. Despite over six decades of attention, there has never been a large-scale systematic quantitative and qualitative analysis of time-series AD evaluation measures. This paper extensively evaluates quality measures for time-series AD to assess their robustness under noise, misalignments, and different anomaly cardinality ratios. Our results indicate that measures producing quality values independently of a threshold (i.e., AUC-ROC and AUC-PR) are more suitable for time-series AD. Motivated by this observation, we first extend the AUC-based measures to account for range-based anomalies. Then, we introduce a new family of parameter-free and threshold-independent measures, VUS (Volume Under the Surface), to evaluate methods while varying parameters. Our findings demonstrate that our four measures are significantly more robust in assessing the quality of time-series AD methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2516699",
                    "name": "John Paparrizos"
                },
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2173928492",
                    "name": "Ruey Tsay"
                },
                {
                    "authorId": "114189441",
                    "name": "Aaron J. Elmore"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                }
            ]
        },
        {
            "paperId": "001e999b990a73428bf94bf758a5c510891ff8c0",
            "title": "SING: Sequence Indexing Using GPUs",
            "abstract": "Data series similarity search is a core operation for several data series analysis applications across many domains. This has attracted lots of interest that led to the development of several indexing techniques. Nevertheless, these techniques fail to deliver the similarity search time performance that is needed for interactive exploration, or analysis of large data series collections. We propose SING, the first data series index designed to take advantage of Graphics Processing Units (GPUs). SING is an in-memory index that uses CPU+GPU co-processing (as well as SIMD, multi-core and multi-socket architectures), in order to accelerate similarity search. Our experimental evaluation with synthetic and real datasets shows that SING is up to 5.1x faster than the state-of-the-art parallel in-memory approach, and up to 62x faster than the state-of-the-art parallel serial scan algorithm. SING achieves exact similarity search query times as low as 32msec on 100GB datasets, which enables interactive data exploration on very large data series collections.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054590163",
                    "name": "Botao Peng"
                },
                {
                    "authorId": "1745030",
                    "name": "P. Fatourou"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "249187d9ab287c8d08c3c993dccf80f7ca4ba3ea",
            "title": "SAND in Action: Subsequence Anomaly Detection for Streams",
            "abstract": "Subsequence anomaly detection in long data series is a significant problem. While the demand for real-time analytics and decision making increases, anomaly detection methods have to operate over streams and handle drifts in data distribution. Nevertheless, existing approaches either require prior domain knowledge or become cumbersome and expensive to use in situations with recurrent anomalies of the same type. Moreover, subsequence anomaly detection methods usually require access to the entire dataset and are not able to learn and detect anomalies in streaming settings. To address these limitations, we propose SAND, a novel online system suitable for domain-agnostic anomaly detection. SAND relies on a novel steaming methodology to incrementally update a model that adapts to distribution drifts and omits obsolete data. We demonstrate our system over different streaming scenarios and compare SAND with other subsequence anomaly detection methods. PVLDB Reference Format: Paul Boniol, John Paparrizos, Themis Palpanas, and Michael J. Franklin. SAND in Action: Subsequence Anomaly Detection for Streams. PVLDB, 14(12): 2867 2870, 2021. doi:10.14778/3476311.3476365",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "2516699",
                    "name": "John Paparrizos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                }
            ]
        },
        {
            "paperId": "349349fee21cd4630bac61e89e1998d71e7756cb",
            "title": "High-Dimensional Similarity Search for Scalable Data Science",
            "abstract": "Similarity search is a core operation of many critical data science applications, involving massive collections of high-dimensional objects. Similarity search finds objects in a collection close to a given query according to some definition of sameness. Objects can be data series, text, multimedia, graphs, database tables or deep network embeddings. In this tutorial, we revisit the similarity search problem in light of the recent advances in the field and the new big data landscape. We discuss key data science applications that require efficient high-dimensional similarity search, we survey the state-of-the-art high-dimensional similarity search approaches and share surprising insights about their strengths and weaknesses, and we discuss the challenges and open research problems in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51935231",
                    "name": "Karima Echihabi"
                },
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "49b45a191d4e4301d851fd2c894831abaad28135",
            "title": "Deep Learning Embeddings for Data Series Similarity Search",
            "abstract": "A key operation for the (increasingly large) data series collection analysis is similarity search. According to recent studies, SAX-based indexes offer state-of-the-art performance for similarity search tasks. However, their performance lags under high-frequency, weakly correlated, excessively noisy, or other dataset-specific properties. In this work, we propose Deep Embedding Approximation (DEA), a novel family of data series summarization techniques based on deep neural networks. Moreover, we describe SEAnet, a novel architecture especially designed for learning DEA, that introduces the Sum of Squares preservation property into the deep network design. Finally, we propose a new sampling strategy, SEASam, that allows SEAnet to effectively train on massive datasets. Comprehensive experiments on 7 diverse synthetic and real datasets verify the advantages of DEA learned using SEAnet, when compared to other state-of-the-art traditional and DEA solutions, in providing high-quality data series summarizations and similarity search results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Qitong Wang"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "5d72bd4f2bb102111fcbca4b9877e90416fd5278",
            "title": "Big Sequence Management: Scaling up and Out",
            "abstract": "Data series are a prevalent data type that has attracted lots of interest in recent years. Specifically, there has been an explosive interest towards the analysis of large volumes of data series in many different domains. This is both in businesses (e.g., in mobile applications) and in sciences (e.g., in biology). In this tutorial, we focus on applications that produce massive collections of data series, and we provide the necessary background on data series storage, retrieval and analytics. We look at systems historically used to handle and mine data in the form of data series, as well as at the state of the art data series management systems that were recently proposed. Moreover, we discuss the need for fast similarity search for supporting data mining applications, and describe efficient similarity search techniques, indexes and query processing algorithms. Finally, we look at the gap of modern data series management systems in regards to support for efficient complex analytics, and we argue in favor of the integration of summarizations and indexes in modern data series management systems. We conclude with the challenges and open research problems in this domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51935231",
                    "name": "Karima Echihabi"
                },
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "61c1dd89dd999eb6056a8b2bec595425d92d7b99",
            "title": "The Four Generations of Entity Resolution",
            "abstract": "Entity Resolution (ER) lies at the core of data integration and cleaning and, thus, a bulk of the research examines ways for improving its e\ufb00ectiveness and time e\ufb03ciency. The initial ER methods primarily target Veracity in the context of structured (relational) data that are described by a schema of well-known quality and meaning. To achieve high e\ufb00ectiveness, they leverage schema, expert, and/or external knowledge. Part of these methods are extended to address Volume, processing large datasets through multi-core or massive parallelization approaches, such as the MapReduce paradigm. However, these early schema-based approaches are inapplicable to Web Data, which abound in voluminous, noisy, semi-structured, and highly heterogeneous information. To address the additional challenge of Variety, recent works on ER adopt a novel, loosely schema-aware functionality that emphasizes scalability and robustness to noise. Another line of present research focuses on the additional challenge of Velocity, aiming to process data collections of a continuously increasing volume. The latest works, though, take advantage of the signi\ufb01cant breakthroughs in Deep Learning and Crowdsourcing, incorporating external knowledge to enhance the existing words to a signi\ufb01cant extent. This synthesis lecture organizes ER methods into four generations based on the challenges posed by these four Vs. For each generation, we outline the corresponding ER work\ufb02ow, discuss the state-of-the-art methods per work\ufb02ow step, and present current research directions. The discussion of these methods takes into account a historical perspective, explaining the evolution of the methods over time along with their similarities and di\ufb00erences. The lecture also discusses the available ER tools and benchmark datasets that allow expert as well as novice users to make use of the available solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "35176340",
                    "name": "Ekaterini Ioannou"
                },
                {
                    "authorId": "2068368537",
                    "name": "Emanouil Thanos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "a37f9d7bd61388888b2c73c92fbaf4f61c19bcd9",
            "title": "Twin Subsequence Search in Time Series",
            "abstract": "We address the problem of subsequence search in time series using Chebyshev distance, to which we refer as twin subsequence search. We first show how existing time series indices can be extended to perform twin subsequence search. Then, we introduce TS-Index, a novel index tailored to this problem. Our experimental evaluation compares these approaches against real time series datasets, and demonstrates that TS-Index can retrieve twin subsequences much faster under various query conditions. This paper has been published in the 24th International Conference on Extending Database Technology (EDBT 2021).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2842794",
                    "name": "Georgios Chatzigeorgakidis"
                },
                {
                    "authorId": "1807115",
                    "name": "Dimitrios Skoutas"
                },
                {
                    "authorId": "1762083",
                    "name": "Kostas Patroumpas"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2587884",
                    "name": "Spiros Athanasiou"
                },
                {
                    "authorId": "1688897",
                    "name": "Spiros Skiadopoulos"
                }
            ]
        },
        {
            "paperId": "bede74dcbd724e0fce6fb5c50d8eebdb71cc564d",
            "title": "SAND: Streaming Subsequence Anomaly Detection",
            "abstract": "With the increasing demand for real-time analytics and decision making, anomaly detection methods need to operate over streams of values and handle drifts in data distribution. Unfortunately, existing approaches have severe limitations: they either require prior domain knowledge or become cumbersome and expensive to use in situations with recurrent anomalies of the same type. In addition, subsequence anomaly detection methods usually require access to the entire dataset and are not able to learn and detect anomalies in streaming settings. To address these problems, we propose SAND, a novel online method suitable for domain-agnostic anomaly detection. SAND aims to detect anomalies based on their distance to a model that represents normal behavior. SAND relies on a novel steaming methodology to incrementally update such model, which adapts to distribution drifts and omits obsolete data. The experimental results on several real-world datasets demonstrate that SAND correctly identifies single and recurrent anomalies without prior knowledge of the characteristics of these anomalies. SAND outperforms by a large margin the current state-of-the-art algorithms in terms of accuracy while achieving orders of magnitude speedups.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "2516699",
                    "name": "John Paparrizos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                }
            ]
        },
        {
            "paperId": "e909a45946965274140ae60d5303cd06944fe582",
            "title": "Electricity Demand Activation Extraction: From Known to Unknown Signatures, Using Similarity Search",
            "abstract": "A powerful tool for reducing energy consumption is energy disaggregation (also called NILM Non-Intrusive Load Monitoring), where the goal is to disaggregate the smart meter readings of a household's total electricity consumption to the consumption of that household's individual appliances. State-of-the-art machine learning methods are widely used to solve the NILM problem, but in order to generalize well they require a large amount of data, which are not readily available. We thus need labeled electricity consumption readings from individual appliance activations. Though, manually annotating the start and end of single-appliance activations is extremely laborious and time consuming. Therefore, automated activation extraction methods are needed. Earlier approaches to solve this problem suffer from limitations, such as incomplete signatures, double signatures, and outliers. In this work, we introduce three scalable methods based on techniques that use time series similarity search. The first method is Cartesio that (improves on earlier work that relies on known features of the appliance) and separately detects the start and end times of an appliance activation. The second method is ValmA, a method for identifying previously unknown candidate signatures of variable length, which is essentially parameter-free. The third method is SimBA, a similarity search based method for efficient detection of known signatures in large datasets. These signatures can be computed from the activations extracted using the previous methods. Our experimental results with real 6 and 10 seconds-sampling data demonstrate that, compared to a state-of-the-art solution, our methods improve the accuracy and robustness of appliance activation extraction in very large time series collections. To compare these methods, we also describe a new accuracy measure that takes into account the special characteristics of subsequences, leading to more precise performance evaluation results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "16992371",
                    "name": "P. Laviron"
                },
                {
                    "authorId": "2114722368",
                    "name": "Xueqi Dai"
                },
                {
                    "authorId": "4770311",
                    "name": "B. Huquet"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "01511f188344e180b3b8af7af99afceb646a4879",
            "title": "Quality of Sentiment Analysis Tools: The Reasons of Inconsistency",
            "abstract": "\n In this paper, we present a comprehensive study that evaluates six state-of-the-art sentiment analysis tools on five public datasets, based on the quality of predictive results in the presence of semantically equivalent documents, i.e., how consistent existing tools are in predicting the polarity of documents based on paraphrased text. We observe that sentiment analysis tools exhibit\n intra-tool inconsistency\n , which is the prediction of different polarity for semantically equivalent documents by the same tool, and\n inter-tool inconsistency\n , which is the prediction of different polarity for semantically equivalent documents across different tools. We introduce a heuristic to assess the data quality of an augmented dataset and a new set of metrics to evaluate tool inconsistencies. Our results indicate that tool inconsistencies is still an open problem, and they point towards promising research directions and accuracy improvements that can be obtained if such inconsistencies are resolved.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057858196",
                    "name": "Wissam Mammar Kouadri"
                },
                {
                    "authorId": "1803906",
                    "name": "Mourad Ouziri"
                },
                {
                    "authorId": "1795027",
                    "name": "S. Benbernou"
                },
                {
                    "authorId": "51935231",
                    "name": "Karima Echihabi"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "40488988",
                    "name": "Iheb Ben Amor"
                }
            ]
        },
        {
            "paperId": "0f834debb85f3adb94a910f2127acf92848715b4",
            "title": "JedAI3 : beyond batch, blocking-based Entity Resolution",
            "abstract": "JedAI is an open-source toolkit that allows for building and benchmarking thousands of schema-agnostic Entity Resolution (ER) pipelines through a non-learning, blocking-based end-to-end workflow. In this paper, we present its latest release, JedAI 3 , which conveys two new end-to-end workflows: one for budget-agnostic ER that is based on similarity joins, and one for budget-aware (i",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "29012102",
                    "name": "Leonidas Tsekouras"
                },
                {
                    "authorId": "1586607286",
                    "name": "Manos Thanos"
                },
                {
                    "authorId": "2138174",
                    "name": "Nikiforos Pittaras"
                },
                {
                    "authorId": "2084273548",
                    "name": "Giovanni"
                },
                {
                    "authorId": "2098487640",
                    "name": "Simonini"
                },
                {
                    "authorId": "1807115",
                    "name": "Dimitrios Skoutas"
                },
                {
                    "authorId": "1586615044",
                    "name": "Paul Isaris"
                },
                {
                    "authorId": "144274464",
                    "name": "George Giannakopoulos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1746733",
                    "name": "Manolis Koubarakis"
                }
            ]
        },
        {
            "paperId": "1a37223175138bc1aa53b425ea2fdd0b382405a5",
            "title": "Massively Distributed Time Series Indexing and Querying",
            "abstract": "Indexing is crucial for many data mining tasks that rely on efficient and effective similarity query processing. Consequently, indexing large volumes of time series, along with high performance similarity query processing, have became topics of high interest. For many applications across diverse domains though, the amount of data to be processed might be intractable for a single machine, making existing centralized indexing solutions inefficient. We propose a parallel indexing solution that gracefully scales to billions of time series, and a parallel query processing strategy that, given a batch of queries, efficiently exploits the index. Our experiments, on both synthetic and real world data, illustrate that our index creation algorithm works on four billion time series in less than five hours, while the state of the art centralized algorithms do not scale and have their limit on 1 billion time series, where they need more than five days. Also, our distributed querying algorithm is able to efficiently process millions of queries over collections of billions of time series, thanks to an effective load balancing mechanism.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32212787",
                    "name": "D. Yagoubi"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "2028901",
                    "name": "F. Masseglia"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "64dce74b6914efc4a40c11fae8a929499e004340",
            "title": "MESSI: In-Memory Data Series Indexing",
            "abstract": "Data series similarity search is a core operation for several data series analysis applications across many different domains. However, the state-of-the-art techniques fail to deliver the time performance required for interactive exploration, or analysis of large data series collections. In this work, we propose MESSI, the first data series index designed for in-memory operation on modern hardware. Our index takes advantage of the modern hardware parallelization opportunities (i.e., SIMD instructions, multi-core and multi-socket architectures), in order to accelerate both index construction and similarity search processing times. Moreover, it benefits from a careful design in the setup and coordination of the parallel workers and data structures, so that it maximizes its performance for in-memory operations. Our experiments with synthetic and real datasets demonstrate that overall MESSI is up to 4x faster at index construction, and up to 11x faster at query answering than the state-of-the-art parallel approach. MESSI is the first to answer exact similarity search queries on 100GB datasets in ~50msec (30-75msec across diverse datasets), which enables real-time, interactive data exploration on very large data series collections.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054590163",
                    "name": "Botao Peng"
                },
                {
                    "authorId": "1745030",
                    "name": "P. Fatourou"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "6647d420b42467184fa0adb4737b64a57f6b1417",
            "title": "GraphAn",
            "abstract": "Subsequence anomaly detection in long sequences is an important problem with applications in a wide range of domains. However, the state-of-the-art approaches have severe limitations: they either require prior domain knowledge, or become cumbersome and inefficient/ineffective in situations with recurrent anomalies of the same type. We recently proposed Series2Graph, a novel method based on a graph representation of a low-dimensionality embedding of subsequences, that detects anomalous subsequences. The experimental results, on the largest set of synthetic and real datasets used to date, demonstrate that the proposed approach correctly identifies single and recurrent anomalies of various types without any prior knowledge of the characteristics of these anomalies, outperforming by a large margin several competing approaches in accuracy, while being up to orders of magnitude faster. In this demonstration, we present GraphAn, a system based on Series2Graph, show-case the challenges of the problem, and demonstrate the advantages of the proposed system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "143805710",
                    "name": "M. Meftah"
                },
                {
                    "authorId": "34082089",
                    "name": "E. Remy"
                }
            ]
        },
        {
            "paperId": "6d048cd1ca508f0aef66ce1c7692acc8a0d9944c",
            "title": "Graph-Query Suggestions for Knowledge Graph Exploration",
            "abstract": "We consider the task of exploratory search through graph queries on knowledge graphs. We propose to assist the user by expanding the query with intuitive suggestions to provide a more informative (full) query that can retrieve more detailed and relevant answers. To achieve this result, we propose a model that can bridge graph search paradigms with well-established techniques for information-retrieval. Our approach does not require any additional knowledge from the user and builds on principled language modelling approaches. We empirically show the effectiveness and efficiency of our approach on a large knowledge graph and how our suggestions are able to help build more complete and informative queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2574504",
                    "name": "Matteo Lissandrini"
                },
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "772352c607ebeb81483267135da9e4443e198cdc",
            "title": "Domain- and Structure-Agnostic End-to-End Entity Resolution with JedAI",
            "abstract": "We present JedAI, a new open-source toolkit for endto- end Entity Resolution. JedAI is domain-agnostic in the sense that it does not depend on background expert knowledge, applying seamlessly to data of any domain with minimal human intervention. JedAI is also structure-agnostic, as it can process any type of data, ranging from structured (relational) to semi-structured (RDF) and un-structured (free-text) entity descriptions. JedAI consists of two parts: (i) JedAI-core is a library of numerous state-of-the-art methods that can be mixed and matched to form (thousands of) end-to-end workflows, allowing for easily benchmarking their relative performance. (ii) JedAI-gui is a user-friendly desktop application that facilitates the composition of complex workflows via a wizard-like interface. It is suitable for both lay and power users, offering concrete guidelines and automatic configuration, as well as manual configuration options, visual exploration, and detailed statistics for each method's performance. In this paper, we also delve into the new features of JedAI's latest version (2.1), and demonstrate its performance experimentally.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "29012102",
                    "name": "Leonidas Tsekouras"
                },
                {
                    "authorId": "28947806",
                    "name": "Emmanouil Thanos"
                },
                {
                    "authorId": "144274464",
                    "name": "George Giannakopoulos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1746733",
                    "name": "Manolis Koubarakis"
                }
            ]
        },
        {
            "paperId": "7ad6a358fac1c7e9ca07f7d8a2b79c7915fd0f52",
            "title": "Automated Anomaly Detection in Large Sequences",
            "abstract": "Subsequence anomaly (or outlier) detection in long sequences is an important problem with applications in a wide range of domains. However, current approaches have severe limitations: they either require prior domain knowledge, or become cumbersome and expensive to use in situations with recurrent anomalies of the same type. In this work, we address these problems, and propose NorM, a novel approach, suitable for domain-agnostic anomaly detection. NorM is based on a new data series primitive, which permits to detect anomalies based on their (dis)similarity to a model that represents normal behavior. The experimental results on several real datasets demonstrate that the proposed approach outperforms by a large margin the current state-of-the art algorithms in terms of accuracy, while being orders of magnitude faster.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "34950861",
                    "name": "Michele Linardi"
                },
                {
                    "authorId": "1720877737",
                    "name": "Federico Roncallo"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "81acc645a7c394f6a53ae8cffc06aa63a596fb8f",
            "title": "Series2Graph",
            "abstract": "Subsequence anomaly detection in long sequences is an important problem with applications in a wide range of domains. However, the approaches that have been proposed so far in the literature have severe limitations: they either require prior domain knowledge that is used to design the anomaly discovery algorithms, or become cumbersome and expensive to use in situations with recurrent anomalies of the same type. In this work, we address these problems, and propose an unsupervised method suitable for domain agnostic subsequence anomaly detection. Our method, Series2Graph, is based on a graph representation of a novel low-dimensionality embedding of subsequences. Series2Graph needs neither labeled instances (like supervised techniques), nor anomaly-free data (like zero-positive learning techniques), and identifies anomalies of varying lengths. The experimental results, on the largest set of synthetic and real datasets used to date, demonstrate that the proposed approach correctly identifies single and recurrent anomalies without any prior knowledge of their characteristics, outperforming by a large margin several competing approaches in accuracy, while being up to orders of magnitude faster.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "98f76de8d40728768b08ce85bb841250741c9f56",
            "title": "ParIS+: Data Series Indexing on Multi-Core Architectures",
            "abstract": "Data series similarity search is a core operation for several data series analysis applications across many different domains. Nevertheless, even state-of-the-art techniques cannot provide the time performance required for large data series collections. We propose ParIS and ParIS+, the first disk-based data series indices carefully designed to inherently take advantage of multi-core architectures, in order to accelerate similarity search processing times. Our experiments demonstrate that ParIS+ completely removes the CPU latency during index construction for disk-resident data, and for exact query answering is up to 1 order of magnitude faster than the current state of the art index scan method, and up to 3 orders of magnitude faster than the optimized serial scan method. ParIS+ (which is an evolution of the ADS+ index) owes its efficiency to the effective use of multi-core and multi-socket architectures, in order to distribute and execute in parallel both index construction and query answering, and to the exploitation of the Single Instruction Multiple Data (SIMD) capabilities of modern CPUs, in order to further parallelize the execution of instructions inside each core.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054590163",
                    "name": "Botao Peng"
                },
                {
                    "authorId": "1745030",
                    "name": "P. Fatourou"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "c785118e32d60ec9166e9d863fae1a901e02c863",
            "title": "Data Series Progressive Similarity Search with Probabilistic Quality Guarantees",
            "abstract": "Existing systems dealing with the increasing volume of data series cannot guarantee interactive response times, even for fundamental tasks such as similarity search. Therefore, it is necessary to develop analytic approaches that support exploration and decision making by providing progressive results, before the final and exact ones have been computed. Prior works lack both efficiency and accuracy when applied to large-scale data series collections. We present and experimentally evaluate a new probabilistic learning-based method that provides quality guarantees for progressive Nearest Neighbor (NN) query answering. We provide both initial and progressive estimates of the final answer that are getting better during the similarity search, as well suitable stopping criteria for the progressive queries. Experiments with synthetic and diverse real datasets demonstrate that our prediction methods constitute the first practical solution to the problem, significantly outperforming competing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3450023",
                    "name": "Anna Gogolou"
                },
                {
                    "authorId": "2110778",
                    "name": "Theophanis Tsandilas"
                },
                {
                    "authorId": "51935231",
                    "name": "Karima Echihabi"
                },
                {
                    "authorId": "34790289",
                    "name": "A. Bezerianos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "c86ebb69053348f7dce9cd155cbf84e4067dda45",
            "title": "Entity Resolution: Past, Present and Yet-to-Come",
            "abstract": "Entity Resolution (ER) lies at the core of data integration, with a bulk of research focusing on its effectiveness and its time efficiency. Most past relevant works were crafted for addressing Veracity over structured (relational) data. They typically rely on schema, expert and external knowledge to maximize accuracy. Part of these methods have been recently extended to process large volumes of data through massive parallelization techniques, such as the MapReduce paradigm. With the present advent of Big Web Data, the scope moved towards Variety, aiming to handle semi-structured data collections, with noisy and highly heterogeneous information. Relevant works adopt a novel, loosely schema-aware functionality that emphasizes scalability and robustness to noise. Another line of present research focuses on Velocity, i.e., processing data collections of a continuously increasing volume. In this tutorial, we present the ER generations by discussing past, present, and yet-to-come mechanisms. For each generation, we outline the corresponding ER workflow along with the state-of-the-art methods per workflow step. Thus, we provide the participants with a deep understanding of the broad field of ER, highlighting the recent advances in crowd-sourcing and deep learning applications in this active research domain. We also equip them with practical skills in applying ER workflows through a hands-on session that involves our publicly available ER toolbox and data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "35176340",
                    "name": "Ekaterini Ioannou"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "cc6f6f87fe580cc0e26af30b1ae2b91e2f66ff08",
            "title": "Scalable Machine Learning on High-Dimensional Vectors: From Data Series to Deep Network Embeddings",
            "abstract": "There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to analyze very large collections of static and streaming sequences (a.k.a. data series), predominantly in real-time. Examples of such applications come from Internet of Things installations, neuroscience, astrophysics, and a multitude of other scientific and application domains that need to apply machine learning techniques for knowledge extraction. It is not unusual for these applications, for which similarity search is a core operation, to involve numbers of data series in the order of hundreds of millions to billions, which are seldom analyzed in their full detail due to their sheer size. Such application requirements have driven the development of novel similarity search methods that can facilitate scalable analytics in this context. At the same time, a host of other methods have been developed for similarity search of high-dimensional vectors in general. All these methods are now becoming increasingly important, because of the growing popularity and size of sequence collections, as well as the growing use of high-dimensional vector representations of a large variety of objects (such as text, multimedia, images, audio and video recordings, graphs, database tables, and others) thanks to deep network embeddings. In this work, we review recent efforts in designing techniques for indexing and analyzing massive collections of data series, and argue that they are the methods of choice even for general high-dimensional vectors. Finally, we discuss the challenges and open research problems in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51935231",
                    "name": "Karima Echihabi"
                },
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "d44b386ce0e56b9ca77d1f02e0c882661f926aa7",
            "title": "SAD: An Unsupervised System for Subsequence Anomaly Detection",
            "abstract": "Subsequence anomaly (or outlier) detection in long sequences is an important problem with applications in a wide range of domains. However, current approaches have severe limitations: they either require prior domain knowledge, or become cumbersome and expensive to use in situations with recurrent anomalies of the same type. We recently proposed NorM, a novel approach suitable for domain-agnostic anomaly detection, which addresses the aforementioned problems by detecting anomalies based on their (dis)similarity to a model that represents normal behavior. The experimental results on several real datasets demonstrate that the proposed approach outperforms the current state-of-the art in terms of both accuracy and execution time. In this demonstration, we present a system for unsupervised Subsequence Anomaly Detection (SAD) that uses the NorM method. Through various scenarios with real datasets, we showcase the challenges of the problem, and we demonstrate the advantages of the proposed system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720881471",
                    "name": "Paul Boniol"
                },
                {
                    "authorId": "34950861",
                    "name": "Michele Linardi"
                },
                {
                    "authorId": "1720877737",
                    "name": "Federico Roncallo"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "dea9b9e55f5d15529f5accbc06c9f6f4697b75e5",
            "title": "SentiQ: A Probabilistic Logic Approach to Enhance Sentiment Analysis Tool Quality",
            "abstract": "The opinion expressed in various Web sites and social-media is an essential contributor to the decision making process of several organizations. Existing sentiment analysis tools aim to extract the polarity (i.e., positive, negative, neutral) from these opinionated contents. Despite the advance of the research in the field, sentiment analysis tools give \\textit{inconsistent} polarities, which is harmful to business decisions. In this paper, we propose SentiQ, an unsupervised Markov logic Network-based approach that injects the semantic dimension in the tools through rules. It allows to detect and solve inconsistencies and then improves the overall accuracy of the tools. Preliminary experimental results demonstrate the usefulness of SentiQ.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057858196",
                    "name": "Wissam Mammar Kouadri"
                },
                {
                    "authorId": "1795027",
                    "name": "S. Benbernou"
                },
                {
                    "authorId": "1803906",
                    "name": "Mourad Ouziri"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "40488988",
                    "name": "Iheb Ben Amor"
                }
            ]
        },
        {
            "paperId": "e13c320fc31a21c5ede04fffd1d7ba10ee284bc8",
            "title": "Interdisciplinary Research in Artificial Intelligence: Challenges and Opportunities",
            "abstract": "The use of artificial intelligence (AI) in a variety of research fields is speeding up multiple digital revolutions, from shifting paradigms in healthcare, precision medicine and wearable sensing, to public services and education offered to the masses around the world, to future cities made optimally efficient by autonomous driving. When a revolution happens, the consequences are not obvious straight away, and to date, there is no uniformly adapted framework to guide AI research to ensure a sustainable societal transition. To answer this need, here we analyze three key challenges to interdisciplinary AI research, and deliver three broad conclusions: 1) future development of AI should not only impact other scientific domains but should also take inspiration and benefit from other fields of science, 2) AI research must be accompanied by decision explainability, dataset bias transparency as well as development of evaluation methodologies and creation of regulatory agencies to ensure responsibility, and 3) AI education should receive more attention, efforts and innovation from the educational and scientific communities. Our analysis is of interest not only to AI practitioners but also to other researchers and the general public as it offers ways to guide the emerging collaborations and interactions toward the most fruitful outcomes.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2007729358",
                    "name": "R. Kusters"
                },
                {
                    "authorId": "1707548",
                    "name": "D. Misevic"
                },
                {
                    "authorId": "2055413987",
                    "name": "H. Berry"
                },
                {
                    "authorId": "1934171",
                    "name": "Antoine Cully"
                },
                {
                    "authorId": "2394997",
                    "name": "Y. Le Cunff"
                },
                {
                    "authorId": "2027646754",
                    "name": "Loic Dandoy"
                },
                {
                    "authorId": "1423321194",
                    "name": "N. D\u00edaz-Rodr\u00edguez"
                },
                {
                    "authorId": "2027648867",
                    "name": "Marion Ficher"
                },
                {
                    "authorId": "2325078",
                    "name": "Jonathan Grizou"
                },
                {
                    "authorId": "2739515",
                    "name": "Alice Othmani"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "6204450",
                    "name": "M. Komorowski"
                },
                {
                    "authorId": "143710789",
                    "name": "P. Loiseau"
                },
                {
                    "authorId": "2027647857",
                    "name": "Cl\u00e9ment Moulin Frier"
                },
                {
                    "authorId": "2027647472",
                    "name": "Santino Nanini"
                },
                {
                    "authorId": "144041798",
                    "name": "D. Quercia"
                },
                {
                    "authorId": "69343681",
                    "name": "M. Sebag"
                },
                {
                    "authorId": "2027648809",
                    "name": "Fran\u00e7oise Souli\u00e9 Fogelman"
                },
                {
                    "authorId": "2103853586",
                    "name": "Sofiane Taleb"
                },
                {
                    "authorId": "4922715",
                    "name": "L. Tupikina"
                },
                {
                    "authorId": "2068226731",
                    "name": "Vaibhav Sahu"
                },
                {
                    "authorId": "49108749",
                    "name": "Jill-J\u00eann Vie"
                },
                {
                    "authorId": "2027647683",
                    "name": "Fatima Wehbi"
                }
            ]
        },
        {
            "paperId": "f11cba099b8cf14815f7b3d85f55ecfddbf9f04d",
            "title": "An Overview of End-to-End Entity Resolution for Big Data",
            "abstract": "One of the most critical tasks for improving data quality and increasing the reliability of data analytics is Entity Resolution (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should satisfy more than one of the Big Data characteristics simultaneously (i.e., Volume and Velocity with Variety). We present the basic concepts, processing steps, and execution strategies that have been proposed by database, semantic Web, and machine learning communities in order to cope with the loose structuredness, extreme diversity, high speed, and large scale of entity descriptions used by real-world applications. We provide an end-to-end view of ER workflows for Big Data, critically review the pros and cons of existing methods, and conclude with the main open research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                }
            ]
        },
        {
            "paperId": "0673c8782cd5c6a484db6c1e1febdfc14b2d0f06",
            "title": "Return of the Lernaean Hydra: Experimental Evaluation of Data Series Approximate Similarity Search",
            "abstract": "Data series are a special type of multidimensional data present in numerous domains, where similarity search is a key operation that has been extensively studied in the data series literature. In parallel, the multidimensional community has studied approximate similarity search techniques. We propose a taxonomy of similarity search techniques that reconciles the terminology used in these two domains, we describe modifications to data series indexing techniques enabling them to answer approximate similarity queries with quality guarantees, and we conduct a thorough experimental evaluation to compare approximate similarity search techniques under a unified framework, on synthetic and real datasets in memory and on disk. Although data series differ from generic multidimensional vectors (series usually exhibit correlation between neighboring values), our results show that data series techniques answer approximate queries with strong guarantees and an excellent empirical performance, on data series and vectors alike. These techniques outperform the state-of-the-art approximate techniques for vectors when operating on disk, and remain competitive in memory.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51935231",
                    "name": "Karima Echihabi"
                },
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1795133",
                    "name": "H. Benbrahim"
                }
            ]
        },
        {
            "paperId": "3a3a7d335aac34b971843b0d433ea54798018efb",
            "title": "A Survey of Blocking and Filtering Techniques for Entity Resolution",
            "abstract": "Efficiency techniques are an integral part of Entity Resolution, since its infancy. In this survey, we organized the bulk of works in the field into Blocking, Filtering and hybrid techniques, facilitating their understanding and use. We also provided an in-dept coverage of each category, further classifying the corresponding works into novel sub-categories. Lately, the efficiency techniques have received more attention, due to the rise of Big Data. This includes large volumes of semi-structured data, which pose challenges not only to the scalability of efficiency techniques, but also to their core assumptions: the requirement of Blocking for schema knowledge and of Filtering for high similarity thresholds. The former led to the introduction of schema-agnostic Blocking in conjunction with Block Processing techniques, while the latter led to more relaxed criteria of similarity. Our survey covers these new fields in detail, putting in context all relevant works.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "1807115",
                    "name": "Dimitrios Skoutas"
                },
                {
                    "authorId": "28947806",
                    "name": "Emmanouil Thanos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "463c2184cac573fa089d77695ce735cfb2f8e3a6",
            "title": "Local Pair and Bundle Discovery over Co-Evolving Time Series",
            "abstract": "Time series exploration and mining has many applications across several industrial and scientific domains. In this paper, we consider the problem of detecting locally similar pairs and groups, called bundles, over co-evolving time series. These are pairs or groups of subsequences whose values do not differ by more than \u03b5 for at least \u03b4 consecutive timestamps, thus indicating common local patterns and trends. We first present a baseline algorithm that performs a sweep line scan across all timestamps to identify matches. Then, we propose a filter-verification technique that only examines candidate matches at judiciously chosen checkpoints across time. Specifically, we introduce two block scanning algorithms for discovering local pairs and bundles respectively, which leverage the potential of checkpoints to aggressively prune the search space. We experimentally evaluate our methods against real-world and synthetic datasets, demonstrating a speed-up in execution time by an order of magnitude over the baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2842794",
                    "name": "Georgios Chatzigeorgakidis"
                },
                {
                    "authorId": "1807115",
                    "name": "Dimitrios Skoutas"
                },
                {
                    "authorId": "1762083",
                    "name": "Kostas Patroumpas"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2587884",
                    "name": "Spiros Athanasiou"
                },
                {
                    "authorId": "1688897",
                    "name": "Spiros Skiadopoulos"
                }
            ]
        },
        {
            "paperId": "6cfa9db51c7e525c9a7de7a669361ada15917e83",
            "title": "Blocking and Filtering Techniques for Entity Resolution",
            "abstract": "Entity Resolution (ER), a core task of Data Integration, detects different entity profiles that correspond to the same real-world object. Due to its inherently quadratic complexity, a series of techniques accelerate it so that it scales to voluminous data. In this survey, we review a large number of relevant works under two different but related frameworks: Blocking and Filtering. The former restricts comparisons to entity pairs that are more likely to match, while the latter identifies quickly entity pairs that are likely to satisfy predetermined similarity thresholds. We also elaborate on hybrid approaches that combine different characteristics. For each framework we provide a comprehensive list of the relevant works, discussing them in the greater context. We conclude with the most promising directions for future work in the field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "1807115",
                    "name": "Dimitrios Skoutas"
                },
                {
                    "authorId": "28947806",
                    "name": "Emmanouil Thanos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "7395570cbed63777cce6bdc2b2e2a07811d5e04b",
            "title": "End-to-End Entity Resolution for Big Data: A Survey",
            "abstract": "One of the most important tasks for improving data quality and the reliability of data analytics results is Entity Resolution (ER). ER aims to identify different descriptions that refer to the same real-world entity, and remains a challenging problem. While previous works have studied specific aspects of ER (and mostly in traditional settings), in this survey, we provide for the first time an end-to-end view of modern ER workflows, and of the novel aspects of entity indexing and matching methods in order to cope with more than one of the Big Data characteristics simultaneously. We present the basic concepts, processing steps and execution strategies that have been proposed by different communities, i.e., database, semantic Web and machine learning, in order to cope with the loose structuredness, extreme diversity, high speed and large scale of entity descriptions used by real-world applications. Finally, we provide a synthetic discussion of the existing approaches, and conclude with a detailed presentation of open research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                }
            ]
        },
        {
            "paperId": "73c804b1343b958af9a62336238a93fd97eb460f",
            "title": "Progressive Similarity Search on Time Series Data",
            "abstract": "Time series data are increasing at a dramatic rate, yet their analysis remains highly relevant in a wide range of human activities. Due to their volume, existing systems dealing with time series data cannot guarantee interactive response times, even for fundamental tasks such as similarity search. Therefore , in this paper, we present our vision to develop analytic approaches that support exploration and decision making by providing progressive results, before the final and exact ones have been computed. We demonstrate through experiments that providing first approximate and then progressive answers is useful (and necessary) for similarity search queries on very large time series data. Our findings indicate that there is a gap between the time the most similar answer is found and the time when the search algorithm terminates, resulting in inflated waiting times without any improvement. We present preliminary ideas on computing probabilistic estimates of the final results that could help users decide when to stop the search process, i.e., deciding when improvement in the final answer is unlikely, thus eliminating waiting time. Finally, we discuss two additional challenges: how to compute efficiently these probabilistic estimates, and how to communicate them to users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3450023",
                    "name": "Anna Gogolou"
                },
                {
                    "authorId": "2110778",
                    "name": "Theophanis Tsandilas"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "34790289",
                    "name": "A. Bezerianos"
                }
            ]
        },
        {
            "paperId": "81069e5a62e01dcbe3d38ce43b26d74e153237a0",
            "title": "Coconut Palm: Static and Streaming Data Series Exploration Now in your Palm",
            "abstract": "Many modern applications produce massive streams of data series and maintain them in indexes to be able to explore them through nearest neighbor search. Existing data series indexes, however, are expensive to operate as they issue many random I/Os to storage. To address this problem, we recently proposed Coconut, a new infrastructure that organizes data series based on a new sortable format. In this way, Coconut is able to leverage state-of-the-art indexing techniques that rely on sorting for the first time to build, maintain and query data series indexes using fast sequential I/Os. In this demonstration, we present Coconut Palm, a new exploration tool that allows to interactively combine different indexing techniques from within the Coconut infrastructure and to thereby seamlessly explore data series from across various scientific domains. We highlight the rich indexing design choices that Coconut opens up, and we present a new recommender tool that allows users to intelligently navigate them for both static and streaming data exploration scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1821005",
                    "name": "H. Kondylakis"
                },
                {
                    "authorId": "3242478",
                    "name": "Niv Dayan"
                },
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "8ebfd48ee6383f29c48f9dfaea522526f0dc892b",
            "title": "Exploring the Data Wilderness through Examples",
            "abstract": "Exploration is one of the primordial ways to accrue knowledge about the world and its nature. As we accumulate, mostly automatically, data at unprecedented volumes and speed, our datasets have become complex and hard to understand. In this context exploratory search provides a handy tool for progressively gather the necessary knowledge by starting from a tentative query that hopefully leads to answers at least partially relevant and that can provide cues about the next queries to issue. Recently, we have witnessed a rediscovery of the so-called example-based methods, in which the user or the analyst circumvent query languages by using examples as input. This shift in semantics has led to a number of methods receiving as query a set of example members of the answer set. The search system then infers the entire answer set based on the given examples and any additional information provided by the underlying database. In this tutorial, we present an excursus over the main example-based methods for exploratory analysis, show techniques tailored to different data types, and provide a unifying view of the problem. We show how different data types require different techniques, and present algorithms that are specifically designed for relational, textual, and graph data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "2574504",
                    "name": "Matteo Lissandrini"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "990d0c63e2e6c6f288a8dd959150dde78cb5d870",
            "title": "Local Similarity Search on Geolocated Time Series Using Hybrid Indexing",
            "abstract": "Geolocated time series, i.e., time series associated with certain locations, abound in many modern applications. In this paper, we consider hybrid queries for retrieving geolocated time series based on filters that combine spatial distance and time series similarity. For the latter, unlike existing work, we allow filtering based on local similarity, which is computed based on subsequences rather than the entire length of each series, thus allowing the discovery of more fine-grained trends and patterns. To efficiently support such queries, we first leverage the state-of-the-art BTSR-tree index, which utilizes bounds over both the locations and the shapes of time series to prune the search space. Moreover, we propose optimizations that check at specific timestamps to identify candidate time series that may exceed the required local similarity threshold. To further increase pruning power, we introduce the SBTSR-tree index, an extension to BTSR-tree, which additionally segments the time series temporally, allowing the construction of tighter bounds. Our experimental results on several real-world datasets demonstrate that SBTSR-tree can provide answers much faster for all examined query types.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2842794",
                    "name": "Georgios Chatzigeorgakidis"
                },
                {
                    "authorId": "1807115",
                    "name": "Dimitrios Skoutas"
                },
                {
                    "authorId": "1762083",
                    "name": "Kostas Patroumpas"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2587884",
                    "name": "Spiros Athanasiou"
                },
                {
                    "authorId": "1688897",
                    "name": "Spiros Skiadopoulos"
                }
            ]
        },
        {
            "paperId": "a7bb9a4372dbb3a384330ed3a73d1099bbd36a23",
            "title": "Report on the First and Second Interdisciplinary Time Series Analysis Workshop (ITISA)",
            "abstract": "The analysis of time-series data associated with modernday industrial operations and scientific experiments is now pushing both computational power and resources to their limits. In order to analyze the existing and (more importantly) future very large time series collections, new technologies and the development of more efficient and smarter algorithms are required. The two editions of the Interdisciplinary Time Series Analysis Workshop brought together data analysts from the fields of computer science, astrophysics, neuroscience, engineering, electricity networks, and music. The focus of these workshops was on the requirements of different applications in the various domains, and also on the advances in both academia and industry, in the areas of time-series management and analysis. In this paper, we summarize the experiences presented in and the results obtained from the two workshops, highlighting the relevant state-ofthe- art-techniques and open research problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "8149286",
                    "name": "V. Beckmann"
                }
            ]
        },
        {
            "paperId": "db0a57c8ecbb9c112f4f39d1d728b09a9d711e7d",
            "title": "Comparing Similarity Perception in Time Series Visualizations",
            "abstract": "A common challenge faced by many domain experts working with time series data is how to identify and compare similar patterns. This operation is fundamental in high-level tasks, such as detecting recurring phenomena or creating clusters of similar temporal sequences. While automatic measures exist to compute time series similarity, human intervention is often required to visually inspect these automatically generated results. The visualization literature has examined similarity perception and its relation to automatic similarity measures for line charts, but has not yet considered if alternative visual representations, such as horizon graphs and colorfields, alter this perception. Motivated by how neuroscientists evaluate epileptiform patterns, we conducted two experiments that study how these three visualization techniques affect similarity perception in EEG signals. We seek to understand if the time series results returned from automatic similarity measures are perceived in a similar manner, irrespective of the visualization technique; and if what people perceive as similar with each visualization aligns with different automatic measures and their similarity constraints. Our findings indicate that horizon graphs align with similarity measures that allow local variations in temporal position or speed (i.e., dynamic time warping) more than the two other techniques. On the other hand, horizon graphs do not align with measures that are insensitive to amplitude and y-offset scaling (i.e., measures based on z-normalization), but the inverse seems to be the case for line charts and colorfields. Overall, our work indicates that the choice of visualization affects what temporal patterns we consider as similar, i.e., the notion of similarity in time series is not visualization independent.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "3450023",
                    "name": "Anna Gogolou"
                },
                {
                    "authorId": "2110778",
                    "name": "Theophanis Tsandilas"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "34790289",
                    "name": "A. Bezerianos"
                }
            ]
        },
        {
            "paperId": "e6e0400b4bd585b043fd3b2adcbc5cca9cfe1b5b",
            "title": "Example-based Search: a New Frontier for Exploratory Search",
            "abstract": "Exploration is one of the primordial ways to accrue knowledge about the world and its nature. As we accumulate, mostly automatically, data at unprecedented volumes and speed, our datasets have become complex and hard to understand. In this context, exploratory search provides a handy tool for progressively gather the necessary knowledge by starting from a tentative query that can provide cues about the next queries to issue. An exploratory query should be simple enough to avoid complicate declarative languages (such as SQL) and convoluted mechanism, and at the same time retain the flexibility and expressiveness required to express complex information needs. Recently, we have witnessed a rediscovery of the so called example-based methods, in which the user, or the analyst circumvent query languages by using examples as input. This shift in semantics has led to a number of methods receiving as query a set of example members of the answer set. The search system then infers the entire answer set based on the given examples and any additional information provided by the underlying database. In this tutorial, we present an excursus over the main example-based methods for exploratory analysis. We show how different data types require different techniques, and present algorithms that are specifically designed for relational, textual, and graph data. We conclude by providing a unifying view of this query-paradigm and identify new exciting research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2574504",
                    "name": "Matteo Lissandrini"
                },
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "fa4018da91777678dd0887be1e955a5a6845588c",
            "title": "Schema-Agnostic Progressive Entity Resolution",
            "abstract": "Entity Resolution (ER) is the task of finding entity profiles that correspond to the same real-world entity. Progressive ER aims to efficiently resolve large datasets when limited time and/or computational resources are available. In practice, its goal is to provide the best possible partial solution by approximating the optimal comparison order of the entity profiles. So far, Progressive ER has only been examined in the context of structured (relational) data sources, as the existing methods rely on schema knowledge to save unnecessary comparisons: they restrict their search space to similar entities with the help of schema-based blocking keys (i.e., signatures that represent the entity profiles). As a result, these solutions are not applicable in Big Data integration applications, which involve large and heterogeneous datasets, such as relational and RDF databases, JSON files, Web corpus etc. To cover this gap, we propose a family of schema-agnostic Progressive ER methods, which do not require schema information, thus applying to heterogeneous data sources of any schema variety. First, we introduce two na\u00efve schema-agnostic methods, showing that straightforward solutions exhibit a poor performance that does not scale well to large volumes of data. Then, we propose four different advanced methods. Through an extensive experimental evaluation over 7 real-world, established datasets, we show that all the advanced methods outperform to a significant extent both the na\u00efve and the state-of-the-art schema-based ones. We also investigate the relative performance of the advanced methods, providing guidelines on the method selection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2301232",
                    "name": "Giovanni Simonini"
                },
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2724566",
                    "name": "S. Bergamaschi"
                }
            ]
        },
        {
            "paperId": "159779334e1fed91c7ee9bf2eaa69b1188519fc0",
            "title": "The return of JedAI: End-to-End Entity Resolution for Structured and Semi-Structured Data",
            "abstract": "\n JedAI is an Entity Resolution toolkit that can be used in three ways:\n (i)\n as an open-source library that combines state-of-the-art methods into a plethora of end-to-end workflows,\n (ii)\n as a user-friendly desktop application with a wizardlike interface that provides complex, out-of-the-box solutions even to lay users, and\n (iii)\n as a workbench for comparing the performance of numerous workflows over both structured and semi-structured data. Here, we present its significant upgrade, JedAI 2.0, which enhances the original version in three important respects:\n (i) time efficiency\n , as the running time has been drastically reduced with the use of high performance data structures and multi-core processing,\n (ii) effectiveness\n , since we enriched its library with more established methods, a new layer that exploits loose schema binding as well as the automatic, data-driven configuration of individual methods or entire workflows, and\n (iii)\n usability\n , as the GUI now enables users to manually configure any method based on concrete guidelines, to store the matching results into any of the supported data formats and to visually explore both input and output data.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "29012102",
                    "name": "Leonidas Tsekouras"
                },
                {
                    "authorId": "28947806",
                    "name": "Emmanouil Thanos"
                },
                {
                    "authorId": "144274464",
                    "name": "George Giannakopoulos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1746733",
                    "name": "Manolis Koubarakis"
                }
            ]
        },
        {
            "paperId": "33b44d897e73fb50b9ab7bc3ebdc2986e3c77736",
            "title": "Coconut: A Scalable Bottom-Up Approach for Building Data Series Indexes",
            "abstract": "Many modern applications produce massive amounts of data series that need to be analyzed, requiring efficient similarity search operations. However, the state-of-the-art data series indexes that are used for this purpose do not scale well for massive datasets in terms of performance, or storage costs. We pinpoint the problem to the fact that existing summarizations of data series used for indexing cannot be sorted while keeping similar data series close to each other in the sorted order. This leads to two design problems. First, traditional bulk-loading algorithms based on sorting cannot be used. Instead, index construction takes place through slow top-down insertions, which create a non-contiguous index that results in many random I/Os. Second, data series cannot be sorted and split across nodes evenly based on their median value; thus, most leaf nodes are in practice nearly empty. This further slows down query speed and amplifies storage costs. To address these problems, we present Coconut. The first innovation in Coconut is an inverted, sortable data series summarization that organizes data series based on a z-order curve, keeping similar series close to each other in the sorted order. As a result, Coconut is able to use bulk-loading techniques that rely on sorting to quickly build a contiguous index using large sequential disk I/Os. We then explore prefix-based and median-based splitting policies for bottom-up bulk-loading, showing that median-based splitting outperforms the state of the art, ensuring that all nodes are densely populated. Overall, we show analytically and empirically that Coconut dominates the state-of-the-art data series indexes in terms of construction speed, query speed, and storage costs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1821005",
                    "name": "H. Kondylakis"
                },
                {
                    "authorId": "3242478",
                    "name": "Niv Dayan"
                },
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "44acfdcfda984124c3861a6a1040b0944ded6212",
            "title": "An Automated System for Internet Pharmacy Verification",
            "abstract": "In the past years, we have witnessed an explosion of web applications, and in particular of electronic commerce websites. This has led to unquestionable benefits for both producers and consumers of goods. On the other hand, however, untrusted companies have the opportunity to bypass checks and regulations imposed by relevant bodies. This problem is prevalent in the context of online commerce of pharmaceutical products, where it is essential that such products are safe, of good quality, and only used with a proper prescription. In this work, we study the problem of internet pharmacy verification. To this effect, we build a classifier, able to find patterns and predict the class of unseen data. Moreover, we devise algorithms that give a trust score to each pharmacy, in order to have a legitimacy indicator usable by human reviewers. We experimentally evaluate the proposed approach with real data coming from two different time periods. The results demonstrate the effectiveness of our approach, as well as the potential of using similar techniques for automatically checking regulation compliance in electronic commerce.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39156510",
                    "name": "A. Cordioli"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "47b21958cc2e95066d53b522d7d3b54d704ff67f",
            "title": "VALMOD: A Suite for Easy and Exact Detection of Variable Length Motifs in Data Series",
            "abstract": "Data series motif discovery represents one of the most useful primitives for data series mining, with applications to many domains, such as robotics, entomology, seismology, medicine, and climatology, and others. The state-of-the-art motif discovery tools still require the user to provide the motif length. Yet, in several cases, the choice of motif length is critical for their detection. Unfortunately, the obvious brute-force solution, which tests all lengths within a given range, is computationally untenable, and does not provide any support for ranking motifs at different resolutions (i.e., lengths). We demonstrate VALMOD, our scalable motif discovery algorithm that efficiently finds all motifs in a given range of lengths, and outputs a length-invariant ranking of motifs. Furthermore, we support the analysis process by means of a newly proposed meta-data structure that helps the user to select the most promising pattern length. This demo aims at illustrating in detail the steps of the proposed approach, showcasing how our algorithm and corresponding graphical insights enable users to efficiently identify the correct motifs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34950861",
                    "name": "Michele Linardi"
                },
                {
                    "authorId": "48269872",
                    "name": "Yan Zhu"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "50543766",
                    "name": "Eamonn J. Keogh"
                }
            ]
        },
        {
            "paperId": "611f7002186e94a36eda93ce2f555ff1cd669a45",
            "title": "Matrix Profile X: VALMOD - Scalable Discovery of Variable-Length Motifs in Data Series",
            "abstract": "In the last fifteen years, data series motif discovery has emerged as one of the most useful primitives for data series mining, with applications to many domains, including robotics, entomology, seismology, medicine, and climatology. Nevertheless, the state-of-the-art motif discovery tools still require the user to provide the motif length. Yet, in at least some cases, the choice of motif length is critical and unforgiving. Unfortunately, the obvious brute-force solution, which tests all lengths within a given range, is computationally untenable. In this work, we introduce VALMOD, an exact and scalable motif discovery algorithm that efficiently finds all motifs in a given range of lengths. We evaluate our approach with five diverse real datasets, and demonstrate that it is up to 20 times faster than the state-of-the-art. Our results also show that removing the unrealistic assumption that the user knows the correct length, can often produce more intuitive and actionable results, which could have been missed otherwise.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34950861",
                    "name": "Michele Linardi"
                },
                {
                    "authorId": "48269872",
                    "name": "Yan Zhu"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "50543766",
                    "name": "Eamonn J. Keogh"
                }
            ]
        },
        {
            "paperId": "6a4d94159beec4fa00bc361e003bc5c86585cd73",
            "title": "Data Exploration Using Example-Based Methods",
            "abstract": "Abstract Data usually comes in a plethora of formats and dimensions, rendering the information extraction and exploration processes challenging. Thus, being able to perform exploratory analyses of ...",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2574504",
                    "name": "Matteo Lissandrini"
                },
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "71265f512624c8611748b760a5c76542f3b37c16",
            "title": "The Lernaean Hydra of Data Series Similarity Search: An Experimental Evaluation of the State of the Art",
            "abstract": "Increasingly large data series collections are becoming commonplace across many different domains and applications. A key operation in the analysis of data series collections is similarity search, which has attracted lots of attention and effort over the past two decades. Even though several relevant approaches have been proposed in the literature, none of the existing studies provides a detailed evaluation against the available alternatives. The lack of comparative results is further exacerbated by the non-standard use of terminology, which has led to confusion and misconceptions. In this paper, we provide definitions for the different flavors of similarity search that have been studied in the past, and present the first systematic experimental evaluation of the efficiency of data series similarity search techniques. Based on the experimental results, we describe the strengths and weaknesses of each approach and give recommendations for the best approach to use under typical use cases. Finally, by identifying the shortcomings of each method, our findings lay the ground for solid further developments in the field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51935231",
                    "name": "Karima Echihabi"
                },
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1795133",
                    "name": "H. Benbrahim"
                }
            ]
        },
        {
            "paperId": "843793928e308b5414d2883ac869e813ec16f65d",
            "title": "Progressive Data Science: Potential and Challenges",
            "abstract": "Data science requires time-consuming iterative manual activities. In particular, activities such as data selection, preprocessing, transformation, and mining, highly depend on iterative trial-and-error processes that could be sped up significantly by providing quick feedback on the impact of changes. The idea of progressive data science is to compute the results of changes in a progressive manner, returning a first approximation of results quickly and allow iterative refinements until converging to a final result. Enabling the user to interact with the intermediate results allows an early detection of erroneous or suboptimal choices, the guided definition of modifications to the pipeline and their quick assessment. In this paper, we discuss the progressiveness challenges arising in different steps of the data science pipeline. We describe how changes in each step of the pipeline impact the subsequent steps and outline why progressive data science will help to make the process more effective. Computing progressive approximations of outcomes resulting from changes creates numerous research challenges, especially if the changes are made in the early steps of the pipeline. We discuss these challenges and outline first steps towards progressiveness, which, we argue, will ultimately help to significantly speed-up the overall data science process.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2427554",
                    "name": "C. Turkay"
                },
                {
                    "authorId": "2343408",
                    "name": "Nicola Pezzotti"
                },
                {
                    "authorId": "2691974",
                    "name": "Carsten Binnig"
                },
                {
                    "authorId": "2879705",
                    "name": "Hendrik Strobelt"
                },
                {
                    "authorId": "143684647",
                    "name": "B. Hammer"
                },
                {
                    "authorId": "9106757",
                    "name": "D. Keim"
                },
                {
                    "authorId": "144381703",
                    "name": "Jean-Daniel Fekete"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2143404317",
                    "name": "Yunhai Wang"
                },
                {
                    "authorId": "2289824",
                    "name": "Florin Rusu"
                }
            ]
        },
        {
            "paperId": "846b945e8af5340a803708a2971268216decaddf",
            "title": "Data Series Management: Fulfilling the Need for Big Sequence Analytics",
            "abstract": "Massive data sequence collections exist in virtually every scientific and social domain, and have to be analyzed to extract useful knowledge. However, no existing data management solution (such as relational databases, column stores, array databases, and time series management systems) can offer native support for sequences and the corresponding operators necessary for complex analytics. We argue for the need to study the theory and foundations for sequence management of big data sequences, and to build corresponding systems that will enable scalable management and analysis of very large sequence collections. To this effect, we need to develop novel techniques to efficiently support a wide range of sequence queries and mining operations, while leveraging modern hardware. The overall goal is to allow analysts across domains to tap in the goldmine of the massive and ever-growing sequence collections they (already) have.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "a3f5b2656cb05d46d39432f5a682bee169027506",
            "title": "Scalable, Variable-Length Similarity Search in Data Series: The ULISSE Approach",
            "abstract": "Data series similarity search is an important operation and at the core of several analysis tasks and applications related to data series collections. Despite the fact that data series indexes enable fast similarity search, all existing indexes can only answer queries of a single length (fixed at index construction time), which is a severe limitation. In this work, we propose ULISSE, the first data series index structure designed for answering similarity search queries of variable length. Our contribution is two-fold. First, we introduce a novel representation technique, which effectively and succinctly summarizes multiple sequences of different length (irrespective of Z-normalization). Based on the proposed index, we describe efficient algorithms for approximate and exact similarity search, combining disk based index visits and in-memory sequential scans. We experimentally evaluate our approach using several synthetic and real datasets. The results show that ULISSE is several times (and up to orders of magnitude) more efficient in terms of both space and time cost, when compared to competing approaches. PVLDB Reference Format: Michele Linardi, Themis Palpanas. Scalable, Variable-Length Similarity Search in Data Series: The ULISSE Approach. PVLDB, 11 (13): 22362248, 2018. DOI: https://doi.org/10.14778/3275366.3275372",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34950861",
                    "name": "Michele Linardi"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "ad3930b6d24e2e27eeac3f80abfd20c281adbb67",
            "title": "ULISSE: ULtra Compact Index for Variable-Length Similarity Search in Data Series",
            "abstract": "Data series similarity search is an important operation and at the core of several analysis tasks and applications related to data series collections. Despite the fact that data series indexes enable fast similarity search, all existing indexes can only answer queries of a single length (fixed at index construction time), which is a severe limitation. In this work, we propose ULISSE, the first data series index structure designed for answering similarity search queries of variable length. Our contribution is two-fold. First, we introduce a novel representation technique, which effectively and succinctly summarizes multiple sequences of different length. Based on the proposed index, we describe efficient algorithms for approximate and exact similarity search, combining disk based index visits and in-memory sequential scans. We experimentally evaluate our approach using several synthetic and real datasets. The results show that ULISSE is several times (and up to orders of magnitude) more efficient in terms of both space and time cost, when compared to competing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34950861",
                    "name": "Michele Linardi"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "bcdaacda23a3426ca5b50ad30533d3f160b046b3",
            "title": "Schema-Agnostic Progressive Entity Resolution",
            "abstract": "Entity Resolution (ER) is the task of finding entity profiles that correspond to the same real-world entity. Progressive ER aims to efficiently resolve large datasets when limited time and/or computational resources are available. In practice, its goal is to provide the best possible partial solution by approximating the optimal comparison order of the entity profiles. So far, Progressive ER has only been examined in the context of structured (relational) data sources, as the existing methods rely on schema knowledge to save unnecessary comparisons: they restrict their search space to similar entities with the help of schema-based blocking keys (i.e., signatures that represent the entity profiles). As a result, these solutions are not applicable in Big Data integration applications, which involve large and heterogeneous datasets, such as relational and RDF databases, JSON files, Web corpus etc. To cover this gap, we propose a family of schema-agnostic Progressive ER methods, which do not require schema information, thus applying to heterogeneous data sources of any schema variety. First, we introduce a na?ve schema-agnostic method, showing that the straightforward solution exhibits a poor performance that does not scale well to large volumes of data. Then, we propose three different advanced methods. Through an extensive experimental evaluation over 7 real-world, established datasets, we show that all the advanced methods outperform to a significant extent both the na?ve and the state-of-the-art schema-based ones. We also investigate the relative performance of the advanced methods, providing guidelines on the method selection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2301232",
                    "name": "Giovanni Simonini"
                },
                {
                    "authorId": "2239519749",
                    "name": "George Papadakis"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2239502655",
                    "name": "Sonia Bergamaschi"
                }
            ]
        },
        {
            "paperId": "c1c16e63e4acc48d2d9f2cece6286c5bef71f971",
            "title": "X2Q: Your Personal Example-based Graph Explorer",
            "abstract": "\n Exploring knowledge graphs can be a daunting task for any user, expert or novice. This is due to the complexity of the schema or because they are unfamiliar with the contents of the data, or even because they do not know precisely what they are looking for. For the same reason there is a significant demand for exploratory methods for this kind of data. We propose X\n 2\n Q, a system that facilitates the exploration of knowledge graphs with a hands-on approach. X\n 2\n Q embodies the flexible multi-exemplar query paradigm, in which easy to express examples serve as the basis for formulating sophisticated, and hard to express queries. Our system helps building examples in an interactive fashion, by showing results of the partial exemplar query as well as suggestions for improving the current examples. Then, the user feedback is incorporated in our scores to filter the irrelevant suggestions upfront. X\n 2\n Q returns answers in real-time on Freebase, one of the largest available knowledge graphs.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2574504",
                    "name": "Matteo Lissandrini"
                },
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "d3f1e7dbe739ba1740bbc524d7cca78a76699790",
            "title": "ParIS: The Next Destination for Fast Data Series Indexing and Query Answering",
            "abstract": "We propose ParIS, the first disk-based data series index that inherently takes advantage of modern hardware parallelization, in order to accelerate processing times. Our experimental results demonstrate that ParIS completely removes the CPU latency during index construction for disk-resident data. In terms of exact query answering, ParIS is more than 2 orders of magnitude faster than the current state of the art index scan method, and more than 3 orders of magnitude faster than the optimized serial scan method. ParIS owes its efficiency not only to the effective use of multi-core and multi-socket architectures, in order to distribute and execute in parallel both index construction and query answering, but also to the exploitation of the Single Instruction Multiple Data (SIMD) capabilities of modern CPUs, in order to further parallelize the execution of individual instructions inside each core.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054590163",
                    "name": "Botao Peng"
                },
                {
                    "authorId": "1745030",
                    "name": "P. Fatourou"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "f7eaa74c027ca2398972574542a24598a6320fd0",
            "title": "Comparing Time Series Similarity Perception under Different Color Interpolations",
            "abstract": "In previous work [1] we compared three time series visualization techniques (colorfields, horizon graphs, and line charts) in small multiples [2], in order to determine if the time series results returned from automatic similarity measures are perceived in a similar manner, irrespective of the visualization technique. Our results indicated that the notion of similarity is visualization dependent. In that first study, our colorfields implementation used a naive RGB color interpolation between red and blue hues. In this research report we describe a follow-up experiment, comparing this simple RGB interpolation to one that is perceptually uniform (CIE L*a*b*), in order to understand if the choice of color interpolation plays a role in the perception of similarity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3450023",
                    "name": "Anna Gogolou"
                },
                {
                    "authorId": "2110778",
                    "name": "Theophanis Tsandilas"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "34790289",
                    "name": "A. Bezerianos"
                }
            ]
        },
        {
            "paperId": "ff1d260e41aad58e0f2fe68d4b61191b0c6a1631",
            "title": "Multi-Example Search in Rich Information Graphs",
            "abstract": "In rich information spaces, it is often hard for users to formally specify the characteristics of the desired answers, either due to the complexity of the schema or of the query language, or even because they do not know exactly what they are looking for. Exemplar queries constitute a query paradigm that overcomes those problems, by allowing users to provide examples of the elements of interest in place of the query specification. In this paper, we propose a general approach where the user-provided example can comprise several partial specification fragments, where each fragment describes only one part of the desired result. We provide a formal definition of the problem, which generalizes existing formulations for both the relational and the graph model. We then describe exact algorithms for its solution for the case of information graphs, as well as top-k algorithms. Experiments on large real datasets demonstrate the effectiveness and efficiency of the proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2574504",
                    "name": "Matteo Lissandrini"
                },
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "258b7b6c13268c14911f1bb65df21ce30850ad2f",
            "title": "TweeLoc: A System for Geolocalizing Tweets at Fine-Grain",
            "abstract": "The recent rise in the use of social networks has resulted in an abundance of information on different aspects of everyday social activities that is available online. In the process of analysis of identifying the information originating from social networks, and especially Twitter, an important aspect is that of the geographic coordinates, i.e., geolocalisation, of the relevant information. Geolocalized information can be used by a variety of applications in order to offer better, or new services. However, only a small percentage of the twitter posts are geotagged, which restricts the applicability of location-based applications. In this work, we describe TweeLoc, our prototype system for geolocalizing tweets that are not geotagged, which can effectively estimate the tweet location at the level of a city neighborhood. TweeLoc employs a dashboard that visualizes the social activity of the geographic regions specified by the user, and provides relevant easy-to-access statistics. Moreover, it displays information on the way that these statistics evolve over time. Our system can help end-users and large-scale event organizers to better plan and manage their activities, and can complete this task fast and more accurately than alternative solutions that we compare to.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143777681",
                    "name": "P. Paraskevopoulos"
                },
                {
                    "authorId": "2053143083",
                    "name": "Giovanni Pellegrini"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "69f53559815f4645ec3e358b40e611ad3ad36a8c",
            "title": "New Trends on Exploratory Methods for Data Analytics",
            "abstract": "Data usually comes in a plethora of formats and dimensions, rendering the exploration and information extraction processes cumbersome. Thus, being able to cast exploratory queries in the data with the intent of having an immediate glimpse on some of the data properties is becoming crucial. An exploratory query should be simple enough to avoid complicate declarative languages (such as SQL) and mechanisms, and at the same time retain the flexibility and expressiveness of such languages. Recently, we have witnessed a rediscovery of the so called example-based methods, in which the user, or the analyst circumvent query languages by using examples as input. An example is a representative of the intended results, or in other words, an item from the result set. Example-based methods exploit inherent characteristics of the data to infer the results that the user has in mind, but may not able to (easily) express. They can be useful both in cases where a user is looking for information in an unfamiliar dataset, or simply when she is exploring the data without knowing what to find in there. In this tutorial, we present an excursus over the main methods for exploratory analysis, with a particular focus on example-based methods. We show how different data types require different techniques, and present algorithms that are specifically designed for relational, textual, and graph data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "2574504",
                    "name": "Matteo Lissandrini"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "ad7cdb465272b697ba4e1b295bce467b26be2dd5",
            "title": "DPiSAX: Massively Distributed Partitioned iSAX",
            "abstract": "Indexing is crucial for many data mining tasks that rely on efficient and effective similarity query processing. Consequently, indexing large volumes of time series, along with high performance similarity query processing, have became topics of high interest. For many applications across diverse domains though, the amount of data to be processed might be intractable for a single machine, making existing centralized indexing solutions inefficient. We propose a parallel indexing solution that gracefully scales to billions of time series, and a parallel query processing strategy that, given a batch of queries, efficiently exploits the index. Our experiments, on both synthetic and real world data, illustrate that our index creation algorithm works on 1 billion time series in less than 2 hours, while the state of the art centralized algorithms need more than 5 days. Also, our distributed querying algorithm is able to efficiently process millions of queries over collections of billions of time series, thanks to an effective load balancing mechanism.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32212787",
                    "name": "D. Yagoubi"
                },
                {
                    "authorId": "1709023",
                    "name": "Reza Akbarinia"
                },
                {
                    "authorId": "2028901",
                    "name": "F. Masseglia"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "b19d103079d00d04d6d537bfecc911406704ac11",
            "title": "Correlation-Aware Distance Measures for Data Series",
            "abstract": "The \ufb01eld of data series processing has attracted lots of attention thanks to the increased availability of unprecedented amounts of sequential data. These data are then processed and analyzed using a large variety of techniques, most of which are based on the computation of some distance function. In this study, we evaluate the bene\ufb01ts of incorporating into the distance functions correlation measures, which enable us to capture the associations among neighboring values in the sequence. We propose three such measures, inspired by statistical and probabilistic approaches. We analytically and experimentally demonstrate the bene\ufb01ts of the new measures using the 1NN classi\ufb01cation task, and discuss the lessons learned.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1701357",
                    "name": "Katsiaryna Mirylenka"
                },
                {
                    "authorId": "1905966",
                    "name": "Michele Dallachiesa"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "cb3fe0eb284ce80b795cf35e525fa2e8a3ed748d",
            "title": "Data Series Similarity Using Correlation-Aware Measures",
            "abstract": "The increased availability of unprecedented amounts of sequential data (generated by Internet-of-Things, as well as scientific applications) has led in the past few years to a renewed interest and attention to the field of data series processing and analysis. Data series collections are processed and analyzed using a large variety of techniques, most of which are based on the computation of some distance function. In this study, we revisit this basic operation of data series distance calculation. We observe that the popular distance measures are oblivious to the correlations inherent in neighboring values in a data series. Therefore, we evaluate the plausibility and benefit of incorporating into the distance function measures of correlation, which enable us to capture the associations among neighboring values in the sequence. We propose four such measures, inspired by statistical and probabilistic approaches, which can effectively model these correlations. We analytically and experimentally demonstrate the benefits of the new measures using the 1NN classification task, and discuss the lessons learned. Finally, we propose future research directions for enabling the proposed measures to be used in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1701357",
                    "name": "Katsiaryna Mirylenka"
                },
                {
                    "authorId": "1905966",
                    "name": "Michele Dallachiesa"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "cf662e70ee1405edc0a8e0003f962097e7180840",
            "title": "The Parallel and Distributed Future of Data Series Mining",
            "abstract": "There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to index and mine very large collections of sequences, or data series. Examples of such applications come from biology, astronomy, entomology, the web, and other domains. It is not unusual for these applications to involve numbers of data series in the order of hundreds of millions to billions, which are often times not analyzed in their full detail due to their sheer size. In this work, we describe past efforts in designing techniques for indexing and mining truly massive collections of data series, based on indexing techniques for fast similarity search, an operation that lies at the core of many mining algorithms. We show that there are two bottlenecks in mining such massive datasets, namely, the time taken to build the index, and the time required to answer exactly similarity queries. In response to these challenges, we discuss novel techniques that adaptively create data series indexes, allowing users to correctly answer queries before the indexing task is finished. We also show how our methods allow mining on datasets that would otherwise be completely untenable, including the first published experiments using one billion data series. Moreover, we present our vision for the future in big sequence management and mining research: we argue that more efforts should concentrate on parallel (including modern hardware optimization opportunities) and distributed solutions, which have until now been largely unexploited.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "f488c1d8409dc2c43c739abdf33fa9f98357581d",
            "title": "Multi-core Meta-blocking for Big Linked Data",
            "abstract": "Discovering matching entities in different Knowledge Bases constitutes a core task in the Linked Data paradigm. Due to its quadratic time complexity, Entity Resolution typically scales to large datasets through blocking, which restricts comparisons to similar entities. For Big Linked Data, Meta-blocking is also needed to restructure the blocks in a way that boosts precision, while maintaining high recall. Based on blocking and Meta-blocking, JedAI Toolkit implements an end-to-end ER workflow for both relational and RDF data. However, its bottleneck is the time-consuming procedure of Meta-blocking, which iterates over all comparisons in each block. To accelerate it, we present a suite of parallelization techniques that are suitable for multi-core processors. We present 2 categories of parallelization strategies, with each one comprising 4 different approaches that are orthogonal to Meta-blocking algorithms. We perform extensive experiments over a real dataset with 3.4 million entities and 13 billion comparisons, demonstrating that our methods can process it within few minutes, achieving high speedup.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "3302972",
                    "name": "K. Bereta"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1746733",
                    "name": "Manolis Koubarakis"
                }
            ]
        },
        {
            "paperId": "223796a46137095cde6c363652a6fbb8b0d3fce5",
            "title": "Efficient Error-tolerant Search on Knowledge Graphs",
            "abstract": "Edge-labeled graphs are widely used to describe relationships between entities in a database. Given a query subgraph that represents an example of what the user is searching for, we study the problem of efficiently searching for similar subgraphs in a large data graph, where the similarity is defined in terms of the well-known graph edit distance. We call these queries \"error-tolerant exemplar queries\" since matches are allowed despite small variations in the graph structure and the labels. The problem in its general case is computationally intractable, but efficient solutions are reachable for labeled graphs under well-behaved distribution of the labels, commonly found in knowledge graphs. We propose two efficient exact algorithms, based on a filtering-and-verification framework, for finding subgraphs in a large data graph that are isomorphic to a query graph under some edit operations. Our filtering scheme, which uses the neighbourhood structure around a node and the presence or absence of paths, significantly reduces the number of candidates that are passed to the verification stage. Moreover, we analyze the costs of our algorithms and the conditions under which one algorithm is expected to outperform the other. Our analysis identifies some of the variables that affect the cost, including the number and the selectivity of query edge labels and the degree of nodes in the data graph, and characterizes their relationships. We empirically evaluate the effectiveness of our filtering schemes and queries, the efficiency of our algorithms and the reliability of our cost models on real datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2071817954",
                    "name": "Z. Shao"
                },
                {
                    "authorId": "2144238",
                    "name": "Davood Rafiei"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "2bf554c4f1caf2897bbf93f3635654cd8e0d31e8",
            "title": "Managing Diverse Sentiments at Large Scale",
            "abstract": "The large-scale aggregation and analysis of user opinions is becoming increasingly relevant to a variety of applications, from detecting social mood on some political topics to tracking their sentiment changes related to events. The analysis of diverse sentiments is another important application, which becomes possible based on the ability of modern methods to capture sentiment polarity on various topics with high precision and on the ever-growing scale. Therefore, there is a need for a scalable way of sentiment aggregation with respect to the time dimension, which stores enough information to preserve diversity, and which allows statistically accurate analysis of sentiment trends and opinion shifts. In this paper, we are focusing on the novel problem of aggregating diverse sentiments at a large scale, based on data sources that are continuously updated. First, we develop a theoretical framework that models sentiment diversity (contradiction) and defines two types of contradictions, depending on the distribution of sentiments over time. Second, we introduce novel measures that capture sentiment diversity from aggregated sentiment statistics. Third, we develop robust and scalable indexing and storage methods for diverse sentiments. Finally, we propose an adaptive approach for identifying contradictions at different time scales. The experimental evaluation demonstrates the effectiveness of the proposed method of capturing contradictions and its superiority over relational databases in real-world scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2245199",
                    "name": "Mikalai Tsytsarau"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "42f5a4ae05ec47296da0995fadc611a0a9553e34",
            "title": "Error-tolerant Exemplar Queries on RDF Graphs",
            "abstract": "Edge-labeled graphs are widely used to describe relationships between entities in a database. We study a class of queries, referred to as exemplar queries, on edge-labeled graphs where each query gives an example of what the user is searching for. Given an exemplar query, we study the problem of efficiently searching for similar subgraphs in a large data graph, where the similarity is defined in terms of the well-known graph edit distance. We call these queries error-tolerant exemplar queries since matches are allowed despite small variations in the graph structure and the labels. The problem in its general case is computationally intractable but efficient solutions are reachable for labeled graphs under well-behaved distribution of the labels, commonly found in knowledge graphs and RDF databases. In this paper, we propose two efficient exact algorithms, based on a filtering-and-verification framework, for finding subgraphs in a large data graph that are isomorphic to a query graph under some edit operations. Our filtering scheme, which uses the neighbourhood structure around a node and the presence or absence of paths, significantly reduces the number of candidates that are passed to the verification stage. We analyze the costs of our algorithms and the conditions under which one algorithm is expected to outperform the other. Our cost analysis identifies some of the variables that affect the cost, including the number and the selectivity of the edge labels in the query and the degree of nodes in the data graph, and characterizes the relationships. We empirically evaluate the effectiveness of our filtering schemes and queries, the efficiency of our algorithms and the reliability of our cost models on real datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2071817954",
                    "name": "Z. Shao"
                },
                {
                    "authorId": "2144238",
                    "name": "Davood Rafiei"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "4c63aee3ac3d136560c5a333520a9851333a9ded",
            "title": "Scaling Entity Resolution to Large, Heterogeneous Data with Enhanced Meta-blocking",
            "abstract": "Entity Resolution constitutes a quadratic task that typically scales to large entity collections through blocking. The resulting blocks can be restructured by Meta-blocking in order to significantly increase precision at a limited cost in recall. Yet, its processing can be time-consuming, while its precision remains poor for configurations with high recall. In this work, we propose new meta-blocking methods that improve precision by up to an order of magnitude at a negligible cost to recall. We also introduce two efficiency techniques that, when combined, reduce the overhead time of Metablocking by more than an order of magnitude. We evaluate our approaches through an extensive experimental study over 6 realworld, heterogeneous datasets. The outcomes indicate that our new algorithms outperform all meta-blocking techniques as well as the state-of-the-art methods for block processing in all respects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "2325154",
                    "name": "George Papastefanatos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1746733",
                    "name": "Manolis Koubarakis"
                }
            ]
        },
        {
            "paperId": "570a49754b44fa76e9fc40e0e081199d9b2234a4",
            "title": "Characterizing Home Device Usage From Wireless Traffic Time Series",
            "abstract": "The analysis of temporal behavioral patterns of home network users can reveal important information to Internet Service Providers (ISPs) and help them to optimize their networks and offer new services (e.g., remote software upgrades, troubleshooting, energy savings). This study uses time series analysis of continuous traffic data from wireless home networks , to extract traffic patterns recurring within, or across homes, and assess the impact of different device types (fixed or portable) on home traffic. Traditional techniques for time series analysis are not suited in this respect, due to the limited stationary and evolving distribution properties of wireless home traffic data. We propose a novel framework that relies on a correlation-based similarity measure of time series , as well as a notion of strong stationarity to define motifs and dominant devices. Using this framework, we analyze the wireless traffic collected from 196 home gateways over two months. The proposed approach goes beyond existing application-specific analysis techniques, such as analysis of wireless traffic, which mainly rely on data aggregated across hundreds, or thousands of users. Our framework, enables the extraction of recurring patterns from traffic time series of individual homes, leading to a much more fine-grained analysis of the behavior patterns of the users. We also determine the best time aggregation policy w.r.t. to the number and statistical importance of the extracted motifs, as well as the device types dominating these motifs and the overall gateway traffic. Our results show that ISPs can exceed the simple observation of the aggregated gateway traffic and better understand their networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1701357",
                    "name": "Katsiaryna Mirylenka"
                },
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2285687",
                    "name": "Ioannis Pefkianakis"
                },
                {
                    "authorId": "46518232",
                    "name": "M. May"
                }
            ]
        },
        {
            "paperId": "5b05c166917c3c802ea9438b794576c20a96b31a",
            "title": "When a Tweet Finds its Place: Fine-Grained Tweet Geolocalisation",
            "abstract": ". The recent rise in the use of social networks has resulted in an abundance of information on di\ufb00erent aspects of everyday social activities that is available online. In the process of analysis of the information originating from social networks, and especially Twitter, an important aspect is that of the geographic coordinates, i.e., geolocalisa-tion, of the relevant information. This information is used by a variety of applications for the better understanding of an urban area, the tracking of the way a virus spreads, the identi\ufb01cation of people that need help in case of a disaster (e.g., an earthquake), or just for the better understanding of the dynamics of a major event (e.g., a concert). However, only a tiny percentage of the twitter posts are geotagged, which restricts the applicability of location-based applications. In this work, we extend our framework for geolocating tweets that are not geotagged, and describe a general solution for estimating the city and neighborhood in the city, from which a post was generated. In addition, we study the spe-ci\ufb01c problem of geolocalising tweets deriving from targeted locations of interest (i.e., cities and neighborhoods in these cities), and present the visualizations of the prototype dashboard application we have developed, which can help end-users and large-scale event organizers to better plan and manage their activities. The experimental evaluation with real data demonstrates the e\ufb03ciency and e\ufb00ectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143777681",
                    "name": "P. Paraskevopoulos"
                },
                {
                    "authorId": "2053143083",
                    "name": "Giovanni Pellegrini"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "ca10c77633b52584503f2f9ab05cc3837465ed3e",
            "title": "Blocking for large-scale Entity Resolution: Challenges, algorithms, and practical examples",
            "abstract": "Entity Resolution constitutes one of the cornerstone tasks for the integration of overlapping information sources. Due to its quadratic complexity, a large amount of research has focused on improving its efficiency so that it scales to Web Data collections, which are inherently voluminous and highly heterogeneous. The most common approach for this purpose is blocking, which clusters similar entities into blocks so that the pair-wise comparisons are restricted to the entities contained within each block. In this tutorial, we take a close look on blocking-based Entity Resolution, starting from the early blocking methods that were crafted for database integration. We highlight the challenges posed by contemporary heterogeneous, noisy, voluminous Web Data and explain why they render inapplicable these schema-based techniques. We continue with the presentation of blocking methods that have been developed for large-scale and heterogeneous information and are suitable for Web Data collections. We also explain how their efficiency can be further improved by meta-blocking and parallelization techniques. We conclude with a hands-on session that demonstrates the relative performance of several, state-of-the-art techniques. The participants of the tutorial will put in practice all the topics discussed in the theory part, and will get familiar with a reference toolbox, which includes the most prominent techniques in the area and can be readily used to tackle Entity Resolution problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "cda10bf8e5720af4651908d79f1a7f6f80979b54",
            "title": "Data series management: The next challenge",
            "abstract": "There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to index and mine very large collections of sequences, or data series. Examples of such applications come from biology, astronomy, entomology, the web, and other domains. It is not unusual for these applications to involve numbers of data series in the order of hundreds of millions to billions, which are often times not analyzed in their full detail due to their sheer size. In this study, we describe recent efforts in designing techniques for indexing and mining truly massive collections of data series that will enable scientists to easily analyze their data. We argue that the main bottleneck in mining such massive datasets is the time taken to build the index. Therefore, we discuss solutions to this problem, including novel techniques that adaptively create data series indexes, allowing users to correctly answer queries before the indexing task is finished. Finally, we present our vision for the future of the very promising data series management research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "e9cdd788cb4a455380131b2ead17a7e8083b59ae",
            "title": "Comparative Analysis of Approximate Blocking Techniques for Entity Resolution",
            "abstract": "Entity Resolution is a core task for merging data collections. Due to its quadratic complexity, it typically scales to large volumes of data through blocking: similar entities are clustered into blocks and pair-wise comparisons are executed only between co-occurring entities, at the cost of some missed matches. There are numerous blocking methods, and the aim of this work is to offer a comprehensive empirical survey, extending the dimensions of comparison beyond what is commonly available in the literature. We consider 17 state-of-the-art blocking methods and use 6 popular real datasets to examine the robustness of their internal configurations and their relative balance between effectiveness and time efficiency. We also investigate their scalability over a corpus of 7 established synthetic datasets that range from 10,000 to 2 million entities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "40221862",
                    "name": "Jonathan Svirsky"
                },
                {
                    "authorId": "70189402",
                    "name": "A. Gal"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "00c998560cc223450eaff0234379e1982976d06c",
            "title": "Data Series Management: The Road to Big Sequence Analytics",
            "abstract": "Massive data series collections are becoming a reality for virtually every scientific and social domain, and have to be processed and analyzed, in order to extract useful knowledge. Current data series management solutions are ad hoc, requiring huge investments in time and effort, and duplication of effort across different teams. Systems like relational databases, Column Stores, and Array Databases are not a suitable solution either, since none of these systems offers native support for data series. Our vision is to design and develop a generalpurpose Data Series Management System, able to copewith big data series, that is, very large and fast-changing collections of data series, which can be heterogeneous (i.e., originate from disparate domains and thus exhibit very different characteristics), and which can have uncertainty in their values (e.g., due to inherent errors in the measurements). Just like databases abstracted the relational data management problem and offered a black box solution that is now omnipresent, the proposed system will allow analysts that are not experts in data series management, as well as common users, to tap in the goldmine of the massive and ever-growing data series collections they (already) have",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "45e4224890226c0d0023c56c46f37c07b830682a",
            "title": "RINSE: Interactive Data Series Exploration with ADS+",
            "abstract": "Numerous applications continuously produce big amounts of data series, and in several time critical scenarios analysts need to be able to query these data as soon as they become available. An adaptive index data structure, ADS+, which is specifically tailored to solve the problem of indexing and querying very large data series collections has been recently proposed as a solution to this problem. The main idea is that instead of building the complete index over the complete data set up-front and querying only later, we interactively and adaptively build parts of the index, only for the parts of the data on which the users pose queries. The net effect is that instead of waiting for extended periods of time for the index creation, users can immediately start exploring the data series. In this work, we present a demonstration of ADS+; we introduce RINSE, a system that allows users to experience the benefits of the ADS+ adaptive index through an intuitive web interface. Users can explore large datasets and find patterns of interest, using nearest neighbor search. They can draw queries (data series) using a mouse, or touch screen, or they can select from a predefined list of data series. RINSE can scale to large data sizes, while drastically reducing the data to query delay: by the time state-of-the-art indexing techniques finish indexing 1 billion data series (and before answering even a single query), adaptive data series indexing can already answer 3 * 105 queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "4be7bddf56bd067f01631df91023c4d3632dcfa5",
            "title": "Report on the First International Workshop on Personal Data Analytics in the Internet of Things (PDA@IOT 2014)",
            "abstract": "The 1st International Workshop on Personal Data Analytics in the Internet of Things (PDA@IOT), held in conjunction with VLDB 2014, aims at sparking research on data analytics, shifting the focus from business to consumers services. While much of the public and academic discourse about personal data has been dominated by a focus on the privacy concerns and the risks they raise to the individual, especially when they are seen as the new oil of the global economy. PDA@IOT focus on how persons could effectively exploit the data they massively create in CyberPhysicalworlds. We believe that the full potential of the IoT goes far beyond connecting \u201cthings\u201d to the Internet: it is about using data to create new value for people. In a People-centric computing paradigm, both small scale \npersonal data and large scale aggregated data should be exploited to identify unmet needs and proactively offer \nthem to users. PDA@IOT seeks to address current technology barriers that impede existing personal data \nprocessing and analytics solutions to empower people in personal decision making. \nThe PDA@IOT ambition is to provide a unique forum for researchers and practitioners that approach personal data from different angles, ranging from data management and processing, to data mining and human-data interaction, as well as to nourish the interdisciplinary synergies required to tackle the challenges and problems emerging in People-centric Computing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3009404",
                    "name": "V. Christophides"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "7bc5e354032bb5de5ff32e70de37bc610b54e55c",
            "title": "Unleashing the Power of Information Graphs",
            "abstract": "Information graphs are generic graphs that model different types of information through nodes and edges. Knowledge graphs are the most common type of information graphs in which nodes represent entities and edges represent relationships among them. In this paper, we argue that exploitation of information graphs can lead into novel query answering capabilities that go beyond the existing capabilities of keyword search, and focus on one of them, namely, exemplar queries. Exemplar queries is a recently introduced paradigm that treats a user query as an example from the desired result set. In this paper, we describe the foundations of exemplar queries and the significant role of information graphs, and we present several applications and relevant research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2574504",
                    "name": "Matteo Lissandrini"
                },
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "152913534",
                    "name": "Dimitra Papadimitriou"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "ad5255bfe9c6f2b55814e8a1e2a2a0f9813d4e5a",
            "title": "Parallel meta-blocking: Realizing scalable entity resolution over large, heterogeneous data",
            "abstract": "Entity resolution constitutes a crucial task for many applications, but has an inherently quadratic complexity. Typically, it scales to large volumes of data through blocking: similar entities are clustered into blocks so that it suffices to perform comparisons only within each block. Meta-blocking further increases efficiency by cleaning the overlapping blocks from unnecessary comparisons. However, even Meta-blocking can be time-consuming: applying it to blocks with 7.4 million entities and 2.21011 comparisons takes almost 8 days on a modern high-end server. In this paper, we parallelize Meta-blocking based on MapReduce. We propose a simple strategy that explicitly creates the core concept of Meta-blocking, the blocking graph. We then describe an advanced strategy that creates the blocking graph implicitly, reducing the overhead of data exchange. We also introduce a load balancing algorithm that distributes the computationally intensive workload evenly among the available compute nodes. Our experimental analysis verifies the superiority of our advanced strategy and demonstrates an almost linear speedup for all meta-blocking techniques with respect to the number of available nodes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3226264",
                    "name": "Vasilis Efthymiou"
                },
                {
                    "authorId": "2239519749",
                    "name": "George Papadakis"
                },
                {
                    "authorId": "2325154",
                    "name": "George Papastefanatos"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "c788d3b56ce79e032c5d4f02ce9869ed2737673a",
            "title": "Query Workloads for Data Series Indexes",
            "abstract": "Data series are a prevalent data type that has attracted lots of interest in recent years. Most of the research has focused on how to efficiently support similarity or nearest neighbor queries over large data series collections (an important data mining task), and several data series summarization and indexing methods have been proposed in order to solve this problem. Nevertheless, up to this point very little attention has been paid to properly evaluating such index structures, with most previous work relying solely on randomly selected data series to use as queries (with/without adding noise). In this work, we show that random workloads are inherently not suitable for the task at hand and we argue that there is a need for carefully generating a query workload. We define measures that capture the characteristics of queries, and we propose a method for generating workloads with the desired properties, that is, effectively evaluating and comparing data series summarizations and indexes. In our experimental evaluation, with carefully controlled query workloads, we shed light on key factors affecting the performance of nearest neighbor search in large data series collections.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "91993942",
                    "name": "Yin Lou"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "143614516",
                    "name": "J. Gehrke"
                }
            ]
        },
        {
            "paperId": "cdec92166e9caaf3248168b3fe6b50933e0f09fd",
            "title": "Fine-grained geolocalisation of non-geotagged tweets",
            "abstract": "The rise in the use of social networks in the recent years has resulted in an abundance of information on different aspects of everyday social activities that is available online, with the most prominent and timely source of such information being Twitter. This has resulted in a proliferation of tools and applications that can help end-users and large-scale event organizers to better plan and manage their activities. In this process of analysis of the information originating from social networks, an important aspect is that of the geographic coordinates, i.e., geolocalisation, of the relevant information, which is necessary for several applications (e.g., on trending venues, traffic jams, etc.). Unfortunately, only a very small percentage of the Twitter posts are geotagged, which significantly restricts the applicability and utility of such applications. In this work, we address this problem by proposing a framework for geolocating tweets that are not geotagged. Our solution is general, and estimates the location from which a post was generated by exploiting the similarities in the content between this post and a set of geotagged tweets, as well as their time-evolution characteristics. Contrary to previous approaches, our framework aims at providing accurate geolocation estimates at fine grain (i.e., within a city). The experimental evaluation with real data demonstrates the efficiency and effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143777681",
                    "name": "P. Paraskevopoulos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "de58da3e31bce25ef787b29a6d000496f3c90f6f",
            "title": "Practical Data Prediction for Real-World Wireless Sensor Networks",
            "abstract": "Data prediction is proposed in wireless sensor networks (WSNs) to extend the system lifetime by enabling the sink to determine the data sampled, within some accuracy bounds, with only minimal communication from source nodes. Several theoretical studies clearly demonstrate the tremendous potential of this approach, able to suppress the vast majority of data reports at the source nodes. Nevertheless, the techniques employed are relatively complex, and their feasibility on resource-scarce WSN devices is often not ascertained. More generally, the literature lacks reports from real-world deployments, quantifying the overall system-wide lifetime improvements determined by the interplay of data prediction with the underlying network. These two aspects, feasibility and system-wide gains, are key in determining the practical usefulness of data prediction in real-world WSN applications. In this paper, we describe derivative-based prediction (DBP), a novel data prediction technique much simpler than those found in the literature. Evaluation with real data sets from diverse WSN deployments shows that DBP often performs better than the competition, with data suppression rates up to 99 percent and good prediction accuracy. However, experiments with a real WSN in a road tunnel show that, when the network stack is taken into consideration, DBP only triples lifetime-a remarkable result per se, but a far cry from the data suppression rates above. To fully achieve the energy savings enabled by data prediction, the data and network layers must be jointly optimized. In our testbed experiments, a simple tuning of the MAC and routing stack, taking into account the operation of DBP, yields a remarkable seven-fold lifetime improvement w.r.t. the mainstream periodic reporting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3348930",
                    "name": "Usman Raza"
                },
                {
                    "authorId": "3122175",
                    "name": "A. Camerra"
                },
                {
                    "authorId": "145093491",
                    "name": "A. Murphy"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1721982",
                    "name": "G. Picco"
                }
            ]
        },
        {
            "paperId": "0d9169f9f47e5913b08f31f67b3049df2ccfec56",
            "title": "Data Quality and its Staleness dimension",
            "abstract": "By its nature, term \u201cdata quality\u201d with its generic meaning \u201cfitness for use\u201d has both subjective and objective aspects. To demonstrate how one can benefit from measuring and controlling quality of one\u2019s data, in this book we presented three real world use cases which demonstrate a top-down research approach of the data quality scope in three different real world applications. In particular, we study the following problems: 1) how quality of data can be defined and propagated to customers in a business intelligence application for quality-aware decision making; 2) how data quality can be defined, measured and used in a web-based system operating with semi-structured data from and designated to both humans and machines; 3) how a data-driven (vs. system-driven) time-related data quality notion of staleness can be defined, efficiently measured and monitored in a generic information system. The work should help researchers and professionals working on both generic data quality problems as its understanding in a given context, and on data quality\u2019s specific applications as measurement its dimensions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3025805",
                    "name": "O. Chayka"
                },
                {
                    "authorId": "1755699",
                    "name": "P. Bouquet"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "108a64af24cfd69ed7eebfbdcd393efc0c3dcfce",
            "title": "A Case Study of Active, Continuous and Predictive Social Media Analytics for Smart City",
            "abstract": "Imagine you are in Milano for the Design Week. You have just spent a couple of days attending few nice events in Brera district. Which of the other hundreds of events spread around in Milano shall you attend now? This paper presents a system able to recommend venues to the visitors of such a city-scale event based on the digital footprints they left on Social Media. By combining deductive and inductive stream reasoning techniques with visitor-modeling functionality, this system semantically analyses and links visitors' social network activities to produce high-quality recommendations even when information about visitors' preferences for venues and events is sparse.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "50535911",
                    "name": "Marco Balduini"
                },
                {
                    "authorId": "1741582",
                    "name": "S. Bocconi"
                },
                {
                    "authorId": "1710630",
                    "name": "A. Bozzon"
                },
                {
                    "authorId": "2539248",
                    "name": "Emanuele Della Valle"
                },
                {
                    "authorId": "145905495",
                    "name": "Yi Huang"
                },
                {
                    "authorId": "2828604",
                    "name": "Jasper Oosterman"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2245199",
                    "name": "Mikalai Tsytsarau"
                }
            ]
        },
        {
            "paperId": "53a0dbdc1443c5ef4849b6cf972dbc96f1687f9f",
            "title": "IQR: an interactive query relaxation system for the empty-answer problem",
            "abstract": "We present IQR, a system that demonstrates optimization based interactive relaxations for queries that return an empty answer. Given an empty answer, IQR dynamically suggests one relaxation of the original query conditions at a time to the user, based on certain optimization objectives, and the user responds by either accepting or declining the relaxation, until the user arrives at a non-empty answer, or a non-empty answer is impossible to achieve with any further relaxations. The relaxation suggestions hinge on a proba- bilistic framework that takes into account the probability of the user accepting a suggested relaxation, as well as how much that relaxation serves towards the optimization objec- tive. IQR accepts a wide variety of optimization objectives - user centric objectives, such as, minimizing the number of user interactions (i.e., effort) or returning relevant results, as well as seller centric objectives, such as, maximizing profit. IQR offers principled exact and approximate solutions for gen- erating relaxations that are demonstrated using multiple, large real datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "2621544",
                    "name": "Alice Marascu"
                },
                {
                    "authorId": "1702973",
                    "name": "Senjuti Basu Roy"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "6036373ca7cb6546cda7e87fb91cb311d16ea4d1",
            "title": "Dynamics of news events and social media reaction",
            "abstract": "The analysis of social sentiment expressed on the Web is becoming increasingly relevant to a variety of applications, and it is important to understand the underlying mechanisms which drive the evolution of sentiments in one way or another, in order to be able to predict these changes in the future. In this paper, we study the dynamics of news events and their relation to changes of sentiment expressed on relevant topics. We propose a novel framework, which models the behavior of news and social media in response to events as a convolution between event's importance and media response function, specific to media and event type. This framework is suitable for detecting time and duration of events, as well as their impact and dynamics, from time series of publication volume. These data can greatly enhance events analysis; for instance, they can help distinguish important events from unimportant, or predict sentiment and stock market shifts. As an example of such application, we extracted news events for a variety of topics and then correlated this data with the corresponding sentiment time series, revealing the connection between sentiment shifts and event dynamics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2245199",
                    "name": "Mikalai Tsytsarau"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "144016128",
                    "name": "M. Castellanos"
                }
            ]
        },
        {
            "paperId": "620bfadd90c6173b60af036a6c5aca6cb94c7eb4",
            "title": "Node classification in uncertain graphs",
            "abstract": "In many real applications that use and analyze networked data, the links in the network graph may be erroneous, or derived from probabilistic techniques. In such cases, the node classification problem can be challenging, since the unreliability of the links may affect the final results of the classification process. In this paper, we focus on situations that require the analysis of the uncertainty that is present in the graph structure. We study the novel problem of node classification in uncertain graphs, by treating uncertainty as a first-class citizen. We propose two techniques based on a Bayes model, and show the benefits of incorporating uncertainty in the classification process as a first-class citizen. The experimental results demonstrate the effectiveness of our approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1905966",
                    "name": "Michele Dallachiesa"
                },
                {
                    "authorId": "1682418",
                    "name": "C. Aggarwal"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "99927d68c20222d2c50382d1618da830471b651a",
            "title": "Exemplar Queries: Give me an Example of What You Need",
            "abstract": "Search engines are continuously employing advanced techniques that aim to capture user intentions and provide results that go beyond the data that simply satisfy the query conditions. Examples include the personalized results, related searches, similarity search, popular and relaxed queries. In this work we introduce a novel query paradigm that considers a user query as an example of the data in which the user is interested. We call these queries exemplar queries and claim that they can play an important role in dealing with the information deluge. We provide a formal specification of the semantics of such queries and show that they are fundamentally different from notions like queries by example, approximate and related queries. We provide an implementation of these semantics for graph-based data and present an exact solution with a number of optimizations that improve performance without compromising the quality of the answers. We also provide an approximate solution that prunes the search space and achieves considerably better time-performance with minimal or no impact on effectiveness. We experimentally evaluate the effectiveness and efficiency of these solutions with synthetic and real datasets, and illustrate the usefulness of exemplar queries in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "2574504",
                    "name": "Matteo Lissandrini"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "aea9a087a8fe187550c6910c8b45205fd76e316f",
            "title": "Indexing for interactive exploration of big data series",
            "abstract": "Numerous applications continuously produce big amounts of data series, and in several time critical scenarios analysts need to be able to query these data as soon as they become available, which is not currently possible with the state-of-the-art indexing methods and for very large data series collections. In this paper, we present the first adaptive indexing mechanism, specifically tailored to solve the problem of indexing and querying very large data series collections. The main idea is that instead of building the complete index over the complete data set up-front and querying only later, we interactively and adaptively build parts of the index, only for the parts of the data on which the users pose queries. The net effect is that instead of waiting for extended periods of time for the index creation, users can immediately start exploring the data series. We present a detailed design and evaluation of adaptive data series indexing over both synthetic data and real-world workloads. The results show that our approach can gracefully handle large data series collections, while drastically reducing the data to query delay: by the time state-of-the-art indexing techniques finish indexing 1 billion data series (and before answering even a single query), adaptive data series indexing has already answered $3*10^5$ queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "cbf0f71607f62546ab21122a5e7850cf4bdbd760",
            "title": "Top-k Nearest Neighbor Search In Uncertain Data Series",
            "abstract": "Many real applications consume data that is intrinsically uncertain, noisy and error-prone. In this study, we investigate the problem of finding the top-k nearest neighbors in uncertain data series, which occur in several different domains. We formalize the top-k nearest neighbor problem for uncertain data series, and describe a model for uncertain data series that captures both uncertainty and correlation. This distinguishes our approach from prior work that compromises the accuracy of the model by assuming independence of the value distribution at neighboring time-stamps. We introduce the Holistic-PkNN algorithm, which uses novel metric bounds for uncertain series and an efficient refinement strategy to reduce the overall number of required probability estimates. We evaluate our proposal under a variety of settings using a combination of synthetic and 45 real datasets from diverse domains. The results demonstrate the significant advantages of the proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1905966",
                    "name": "Michele Dallachiesa"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1743316",
                    "name": "Ihab F. Ilyas"
                }
            ]
        },
        {
            "paperId": "d8d8192a5b2452736dd58831effee335b1c21b83",
            "title": "A systematic approach for dynamic targeted monitoring of KPIs",
            "abstract": "There has been growing interest for more than a decade in Business Analytics as a means for improving business performance. One of the most popular Business Analytics technique involves monitoring performance by means of Key Performance Indicators (KPIs). A KPI is a powerful tool that relates enterprise data to business goals, thereby enabling managers to guide the analytic process and identify deviations in their strategic plan. Nevertheless, monitoring KPIs requires that they are evaluated at multiple levels of detail, in order to identify potential problems earlier instead of being noted after the fact. Unfortunately, there are obstacles to the generation and enactment of such monitoring processes. In particular, there is no systematic, tool-supported process for defining what is to be monitored given a strategic plan, nor are there tools for automatically generating monitoring queries. As a result, monitoring consists of a manual process whereby queries are generated for high level indicators across a few scorecards and dashboards. In this paper we present a systematic semi-automatic approach that covers the entire monitoring process. Our approach performs a partial search guided by the KPIs of the company, generating queries required during the monitoring process. Thanks to our approach, users become aware of the existence of problems and where they are located, without requiring a priori information about the nature of the problem being searched. Moreover, we have implemented our approach in our CASE tool HERMES and evaluated the results on a case study using real data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145782886",
                    "name": "A. Mat\u00e9"
                },
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "145340803",
                    "name": "J. Trujillo"
                },
                {
                    "authorId": "1750566",
                    "name": "J. Mylopoulos"
                },
                {
                    "authorId": "1757263",
                    "name": "Elvis Koci"
                }
            ]
        },
        {
            "paperId": "f7e52d09cd3b718d029d9c7c9104b51dce846102",
            "title": "Meta-Blocking: Taking Entity Resolutionto the Next Level",
            "abstract": "Entity Resolution is an inherently quadratic task that typically scales to large data collections through blocking. In the context of highly heterogeneous information spaces, blocking methods rely on redundancy in order to ensure high effectiveness at the cost of lower efficiency (i.e., more comparisons). This effect is partially ameliorated by coarse-grained block processing techniques that discard entire blocks either a-priori or during the resolution process. In this paper, we introduce meta-blocking as a generic procedure that intervenes between the creation and the processing of blocks, transforming an initial set of blocks into a new one with substantially fewer comparisons and equally high effectiveness. In essence, meta-blocking aims at extracting the most similar pairs of entities by leveraging the information that is encapsulated in the block-to-entity relationships. To this end, it first builds an abstract graph representation of the original set of blocks, with the nodes corresponding to entity profiles and the edges connecting the co-occurring ones. During the creation of this structure all redundant comparisons are discarded, while the superfluous ones can be removed by pruning of the edges with the lowest weight. We analytically examine both procedures, proposing a multitude of edge weighting schemes, graph pruning algorithms as well as pruning criteria. Our approaches are schema-agnostic, thus accommodating any type of blocks. We evaluate their performance through a thorough experimental study over three large-scale, real-world data sets, with the outcomes verifying significant efficiency enhancements at a negligible cost in effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "1680709",
                    "name": "Georgia Koutrika"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1744808",
                    "name": "W. Nejdl"
                }
            ]
        },
        {
            "paperId": "fab7ad8b199610427ecf56da3e1b6089c37a5e81",
            "title": "New Trends in Database and Information Systems II: Selected papers of the 18th East European Conference on Advances in Databases and Information Systems and Associated Satellite Events",
            "abstract": "This volume contains the papers of 3 workshops and the doctoral consortium, which are organized in the framework of the 18th East-European Conference on Advances in Databases and Information Systems (ADBIS2014). The 3rd International Workshop on GPUs in Databases (GID2014) is devoted to subjects related to utilization of Graphics Processing Units in database environments. The use of GPUs in databases has not yet received enough attention from the database community. The intention of the GID workshop is to provide a discussion on popularizing the GPUs and providing a forum for discussion with respect to the GIDs research ideas and their potential to achieve high speedups in many database applications.The 3rd International Workshop on Ontologies Meet Advanced Information Systems (OAIS2014) has a twofold objective to present: new and challenging issues in the contribution of ontologies for designing high quality information systems, and new research and technological developments which use ontologies all over the life cycle of information systems.The 1st International Workshop on Technologies for Quality Management in Challenging Applications (TQMCA2014) focuses on quality management and its importance in new fields such as big data, crowd-sourcing, and stream databases. The Workshop has addressed the need to develop novel approaches and technologies, and to entirely integrate quality management into information system management.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143804820",
                    "name": "Nick Bassiliades"
                },
                {
                    "authorId": "2197884149",
                    "name": "Mirjana Ivanovic"
                },
                {
                    "authorId": "1403842969",
                    "name": "M. Kon-Popovska"
                },
                {
                    "authorId": "1796253",
                    "name": "Y. Manolopoulos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1776969",
                    "name": "Goce Trajcevski"
                },
                {
                    "authorId": "1741423",
                    "name": "A. Vakali"
                }
            ]
        },
        {
            "paperId": "fad8f585c4f67ba6b67e68680fcd29593b76ef61",
            "title": "Searching with XQ: the exemplar query search engine",
            "abstract": "We demonstrate XQ, a query engine that implements a novel technique for searching relevant information on the web and in various data sources, called Exemplar Queries. While the traditional query model expects the user to provide a set of specifications that the elements of interest need to satisfy, XQ expects the user to provide only an element of interest and we infer the desired answer set based on that element. Through the various examples we demonstrate the functionality of the system and its applicability in various cases. At the same time, we highlight the technical challenges for this type of query answering and illustrate the implementation approach we have materialized. The demo is intended for both researchers and practitioners and aims at illustrating the benefits of the adoption of this new form of query answering in practical applications and the further study and advancement of its technical solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "2574504",
                    "name": "Matteo Lissandrini"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "27fc4ccdffa3daeeec9a6a650d929cfd2ed1f7ae",
            "title": "Monitoring and diagnosing indicators for business analytics",
            "abstract": "Modeling the strategic objectives has been shown to be useful both for understanding a business as well as planning and guiding the overall activities within an enterprise. Business strategy is modeled according to human expertise, setting up the goals as well as the indicators that monitor activities and goals. However, usually indicators provide high-level aggregated views of data, making it difficult to pinpoint problems within specific sub-areas until they have a significant impact into the aggregated value. By the time these problems become evident, they have already hindered the performance of the organization. However, performing a detailed analysis manually can be a daunting task, due to the size of the data space. In order to solve this problem, we propose a user-driven method to analyze the data related to each business indicator by means of data mining. We illustrate our approach with a real world example based on the Europe 2020 framework. Our approach allows us not only to identify latent problems, but also to highlight deviations from anticipated trends that may represent opportunities and exceptional situations, thereby enabling an organization to take advantage of them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2614982",
                    "name": "Konstantinos Zoumpatianos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1750566",
                    "name": "J. Mylopoulos"
                },
                {
                    "authorId": "145782886",
                    "name": "A. Mat\u00e9"
                },
                {
                    "authorId": "145340803",
                    "name": "J. Trujillo"
                }
            ]
        },
        {
            "paperId": "3d0ddaadc9e97c1cc98762bf3b477e2860511c39",
            "title": "A Probabilistic Optimization Framework for the Empty-Answer Problem",
            "abstract": "We propose a principled optimization-based interactive query relaxation framework for queries that return no answers. Given an initial query that returns an empty answer set, our framework dynamically computes and suggests alternative queries with less conditions than those the user has initially requested, in order to help the user arrive at a query with a non-empty answer, or at a query for which no matter how many additional conditions are ignored, the answer will still be empty. Our proposed approach for suggesting query relaxations is driven by a novel probabilistic framework based on optimizing a wide variety of application-dependent objective functions. We describe optimal and approximate solutions of different optimization problems using the framework. We analyze these solutions, experimentally verify their efficiency and effectiveness, and illustrate their advantage over the existing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "2621544",
                    "name": "Alice Marascu"
                },
                {
                    "authorId": "1702973",
                    "name": "Senjuti Basu Roy"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "3ec6641d9e6525ab15714d7a6eed570b1687cdf7",
            "title": "New Trends in Databases and Information Systems: 17th East European Conference on Advances in Databases and Information Systems",
            "abstract": "This book reports on state-of-art research and applications in the field of databases and information systems. It includes both fourteen selected short contributions, presented at the East-European Conference on Advances in Databases and Information Systems (ADBIS 2013, September 1-4, Genova, Italy), and twenty-six papers from ADBIS 2013 satellite events. The short contributions from the main conference are collected in the first part of the book, which covers a wide range of topics, like data management, similarity searches, spatio-temporal and social network data, data mining, data warehousing, and data management on novel architectures, such as graphics processing units, parallel database management systems, cloud and MapReduce environments. In contrast, the contributions from the satellite events are organized in five different parts, according to their respective ADBIS satellite event: BiDaTA 2013 - Special Session on Big Data: New Trends and Applications); GID 2013 The Second International Workshop on GPUs in Databases; OAIS 2013 The Second International Workshop on Ontologies Meet Advanced Information Systems; SoBI 2013 The First International Workshop on Social Business Intelligence: Integrating Social Content in Decision Making; and last but not least, the Doctoral Consortium, a forum for Ph.D. students. The book, which addresses academics and professionals alike, provides the readers with a comprehensive and timely overview of new trends in database and information systems research, and promotes new ideas and collaborations among the different research communities of the eastern European countries and the rest of the world.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1726425",
                    "name": "B. Catania"
                },
                {
                    "authorId": "1684646",
                    "name": "T. Cerquitelli"
                },
                {
                    "authorId": "1694792",
                    "name": "S. Chiusano"
                },
                {
                    "authorId": "143634280",
                    "name": "G. Guerrini"
                },
                {
                    "authorId": "2227367049",
                    "name": "Mirko Kmpf"
                },
                {
                    "authorId": "144122431",
                    "name": "A. Kemper"
                },
                {
                    "authorId": "49898261",
                    "name": "B. Novikov"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "72929942",
                    "name": "Jaroslav Pokorn"
                },
                {
                    "authorId": "1741423",
                    "name": "A. Vakali"
                }
            ]
        },
        {
            "paperId": "6e7adb4a35f8c7d961f58e1811ed7a8cb826456a",
            "title": "Efficient sentiment correlation for large-scale demographics",
            "abstract": "Analyzing sentiments of demographic groups is becoming important for the Social Web, where millions of users provide opinions on a wide variety of content. While several approaches exist for mining sentiments from product reviews or micro-blogs, little attention has been devoted to aggregating and comparing extracted sentiments for different demographic groups over time, such as 'Students in Italy' or 'Teenagers in Europe'. This problem demands efficient and scalable methods for sentiment aggregation and correlation, which account for the evolution of sentiment values, sentiment bias, and other factors associated with the special characteristics of web data. We propose a scalable approach for sentiment indexing and aggregation that works on multiple time granularities and uses incrementally updateable data structures for online operation. Furthermore, we describe efficient methods for computing meaningful sentiment correlations, which exploit pruning based on demographics and use top-k correlations compression techniques. We present an extensive experimental evaluation with both synthetic and real datasets, demonstrating the effectiveness of our pruning techniques and the efficiency of our solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2245199",
                    "name": "Mikalai Tsytsarau"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "9e781b09dbd4aaf1d6060aaed3afaf65bb1b07e3",
            "title": "Entity ranking using click-log information",
            "abstract": "Log information describing the items the users have selected from the set of answers a query engine returns to their queries constitute an excellent form of indirect user feedback that has been extensively used in the web to improve the effectiveness of search engines. In this work we study how the logs can be exploited to improve the ranking of the results returned by an entity search engine. Entity search engines are becoming more and more popular as the web is changing from a web of documents into a \"web of things\". We show that entity search engines pose new challenges since their model is different than the one documents are based on. We present a novel framework for feature extraction that is based on the notions of entity matching and attribute frequencies. The extracted features are then used to train a ranking classifier. We introduce different methods and metrics for ranking, we combine them with existing traditional techniques and we study their performance using real and synthetic data. The experiments show that our technique provides better results in terms of accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3094226",
                    "name": "D. Mottin"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "a6647da1bd947c31ad1093788a4fb4d90c6d82fc",
            "title": "Finding interesting correlations with conditional heavy hitters",
            "abstract": "The notion of heavy hitters-items that make up a large fraction of the population - has been successfully used in a variety of applications across sensor and RFID monitoring, network data analysis, event mining, and more. Yet this notion often fails to capture the semantics we desire when we observe data in the form of correlated pairs. Here, we are interested in items that are conditionally frequent: when a particular item is frequent within the context of its parent item. In this work, we introduce and formalize the notion of Conditional Heavy Hitters to identify such items, with applications in network monitoring, and Markov chain modeling. We introduce several streaming algorithms that allow us to find conditional heavy hitters efficiently, and provide analytical results. Different algorithms are successful for different input characteristics. We perform experimental evaluations to demonstrate the efficacy of our methods, and to study which algorithms are most suited for different types of data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1701357",
                    "name": "Katsiaryna Mirylenka"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "ffff3caadc5e8a5440541ecae870c705806426ab",
            "title": "A Blocking Framework for Entity Resolution in Highly Heterogeneous Information Spaces",
            "abstract": "In the context of entity resolution (ER) in highly heterogeneous, noisy, user-generated entity collections, practically all block building methods employ redundancy to achieve high effectiveness. This practice, however, results in a high number of pairwise comparisons, with a negative impact on efficiency. Existing block processing strategies aim at discarding unnecessary comparisons at no cost in effectiveness. In this paper, we systemize blocking methods for clean-clean ER (an inherently quadratic task) over highly heterogeneous information spaces (HHIS) through a novel framework that consists of two orthogonal layers: the effectiveness layer encompasses methods for building overlapping blocks with small likelihood of missed matches; the efficiency layer comprises a rich variety of techniques that significantly restrict the required number of pairwise comparisons, having a controllable impact on the number of detected duplicates. We map to our framework all relevant existing methods for creating and processing blocks in the context of HHIS, and additionally propose two novel techniques: attribute clustering blocking and comparison scheduling. We evaluate the performance of each layer and method on two large-scale, real-world data sets and validate the excellent balance between efficiency and effectiveness that they achieve.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "35176340",
                    "name": "Ekaterini Ioannou"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1718268",
                    "name": "C. Nieder\u00e9e"
                },
                {
                    "authorId": "1744808",
                    "name": "W. Nejdl"
                }
            ]
        },
        {
            "paperId": "1309305664872fd57247614ebf5d53a0a333e6e4",
            "title": "A knowledge mining framework for business analysts",
            "abstract": "Several studies have focused on problems related to data mining techniques, including several applications of these techniques in the e-commerce setting. In this work, we describe how data mining technology can be effectively applied in an e-commerce environment, delivering significant benefits to the business analyst. We propose a framework that takes the results of the data mining process as input, and converts these results into actionable knowledge, by enriching them with information that can be readily interpreted by the business analyst. The framework can accommodate various data mining algorithms, and provides a customizable user interface.\n We experimentally evaluate the proposed approach by using a real-world case study that demonstrates the added benefit of the proposed method. The same study validates the claim that the produced results represent actionable knowledge that can help the business analyst improve the business performance, since it significantly reduces the time needed for data analysis, which results in substantial financial savings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "338ac533bf2e5198d0d147f3561f441978f00e0c",
            "title": "Defining and Measuring Data-Driven Quality Dimension of Staleness",
            "abstract": "With the growing complexity of data acquisition and processing methods, there is an increasing demand in understanding which data is outdated and how to have it as fresh as possible. Staleness is one of the key, time-related, data quality characteristics, that represents a degree of synchronization between data originators and information systems possessing the data. However, nowadays there is no common and pervasive notion of data staleness, as well as methods for its measurement in a wide scope of applications. \nOur work provides a definition of a data-driven notion of staleness for information systems with frequently updatable data. For such a data, we demonstrate an efficient exponential smoothing method of staleness measurement, compared to naive approaches, using the same limited amount of memory, based on averaging of frequency of updates. \nWe present experimental results of staleness measurement algorithms that we run on history of updates of articles from Wikipedia.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3025805",
                    "name": "O. Chayka"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1755699",
                    "name": "P. Bouquet"
                }
            ]
        },
        {
            "paperId": "709cb615533302375842e11885fd32445f6757bc",
            "title": "What does model-driven data acquisition really achieve in wireless sensor networks?",
            "abstract": "Model-driven data acquisition techniques aim at reducing the amount of data reported, and therefore the energy consumed, in wireless sensor networks (WSNs). At each node, a model predicts the sampled data; when the latter deviate from the current model, a new model is generated and sent to the data sink. However, experiences in real-world deployments have not been reported in the literature. Evaluation typically focuses solely on the quantity of data reports suppressed at source nodes: the interplay between data modeling and the underlying network protocols is not analyzed. In contrast, this paper investigates in practice whether i) model-driven data acquisition works in a real application; ii) the energy savings it enables in theory are still worthwhile once the network stack is taken into account. We do so in the concrete setting of a WSN-based system for adaptive lighting in road tunnels. Our novel modeling technique, Derivative-Based Prediction (DBP), suppresses up to 99% of the data reports, while meeting the error tolerance of our application. DBP is considerably simpler than competing techniques, yet performs better in our real setting. Experiments in both an indoor testbed and an operational road tunnel show also that, once the network stack is taken into consideration, DBP triples the WSN lifetime-a remarkable result per se, but a far cry from the aforementioned 99% data suppression. This suggests that, to fully exploit the energy savings enabled by data modeling techniques, a coordinated operation of the data and network layers is necessary.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3348930",
                    "name": "Usman Raza"
                },
                {
                    "authorId": "3122175",
                    "name": "A. Camerra"
                },
                {
                    "authorId": "145093491",
                    "name": "A. Murphy"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1721982",
                    "name": "G. Picco"
                }
            ]
        },
        {
            "paperId": "7488bd7e6f6851baaff23600f0b0628050de565c",
            "title": "dbTrento: the data and information management group at the University of Trento",
            "abstract": "The dbTrento group was established in 2006 by Profs Themis Palpanas and Yannis Velegrakis. Since then it has steadily grown into a fully functioning group with (currently) 17 members. It is located in Trento, a beautifully preserved historic town in the Dolomite mountains, which hosts one of the 6 ICT Labs of the European Institute of Innovation and Technology (EIT), and aims to become a reference research and technological center in Europe. The VLDB 2013 conference is being organized by dbTrento, its founders serving as the General Chairs. The mission of the group is to conduct high quality research on different aspects of large scale data and information management. The following sections provide a high level description of the work in these areas, which has also led to 3 Best Paper awards.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "7ec000a6cc721c570a195c7110bf4137907c4a11",
            "title": "Beyond 100 million entities: large-scale blocking-based resolution for heterogeneous data",
            "abstract": "A prerequisite for leveraging the vast amount of data available on the Web is Entity Resolution, i.e., the process of identifying and linking data that describe the same real-world objects. To make this inherently quadratic process applicable to large data sets, blocking is typically employed: entities (records) are grouped into clusters - the blocks - of matching candidates and only entities of the same block are compared. However, novel blocking techniques are required for dealing with the noisy, heterogeneous, semi-structured, user-generateddata in the Web, as traditional blocking techniques are inapplicable due to their reliance on schema information. The introduction of redundancy, improves the robustness of blocking methods but comes at the price of additional computational cost.\n In this paper, we present methods for enhancing the efficiency of redundancy-bearing blocking methods, such as our attribute-agnostic blocking approach. We introduce novel blocking schemes that build blocks based on a variety of evidences, including entity identifiers and relationships between entities; they significantly reduce the required number of comparisons, while maintaining blocking effectiveness at very high levels. We also introduce two theoretical measures that provide a reliable estimation of the performance of a blocking method, without requiring the analytical processing of its blocks. Based on these measures, we develop two techniques for improving the performance of blocking: combining individual, complementary blocking schemes, and purging blocks until given criteria are satisfied. We test our methods through an extensive experimental evaluation, using a voluminous data set with 182 million heterogeneous entities. The outcomes of our study show the applicability and the high performance of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "35176340",
                    "name": "Ekaterini Ioannou"
                },
                {
                    "authorId": "1718268",
                    "name": "C. Nieder\u00e9e"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1744808",
                    "name": "W. Nejdl"
                }
            ]
        },
        {
            "paperId": "8c434e29357fb265cab04d32c355bebc40c05063",
            "title": "Density-based Projected Clustering over High Dimensional Data Streams",
            "abstract": "Clustering of high dimensional data streams is an important problem in many application domains, a prominent example being network monitoring. Several approaches have been lately proposed for solving independently the different aspects of the problem. There exist methods for clustering over full dimensional streams and methods for finding clusters in subspaces of high dimensional static data. Yet only a few approaches have been proposed so far which tackle both the stream and the high dimensionality aspects of the problem simultaneously. In this work, we propose a new density-based projected clustering algorithm, HDDStream, for high dimensional data streams. Our algorithm summarizes both the data points and the dimensions where these points are grouped together and maintains these summaries online, as new points arrive over time and old points expire due to ageing. Our experimental results illustrate the effectiveness and the efficiency of HDDStream and also demonstrate that it could serve as a trigger for detecting drastic changes in the underlying stream population, like bursts of network attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                },
                {
                    "authorId": "3310376",
                    "name": "Arthur Zimek"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1782292",
                    "name": "Peer Kr\u00f6ger"
                },
                {
                    "authorId": "1688561",
                    "name": "H. Kriegel"
                }
            ]
        },
        {
            "paperId": "af0f885334aff2f67aef311080960f5d27eeaa0d",
            "title": "CloudAlloc: a monitoring and reservation system for compute clusters",
            "abstract": "Cloud computing has emerged as a promising environment capable of providing flexibility, scalability, elasticity, fail-over mechanisms, high availability, and other important features to applications. Compute clusters are relatively easy to create and use, but tools to effectively share cluster resources are lacking. CloudAlloc addresses this problem and schedules workloads to cluster resources using allocation algorithms that can be easily changed according to the objectives of the enterprise. It also monitors resource utilization and thus, provides accountability for actual usage. CloudAlloc is a lightweight, flexible, easy-to-use tool for cluster resource allocation that has also proved useful as a research platform. We demonstrate its features and also discuss its allocation algorithms that minimize power usage. CloudAlloc was implemented and is in use at HP Labs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2082898827",
                    "name": "Enrico Iori"
                },
                {
                    "authorId": "1695573",
                    "name": "A. Simitsis"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "143796187",
                    "name": "K. Wilkinson"
                },
                {
                    "authorId": "2367532",
                    "name": "S. Harizopoulos"
                }
            ]
        },
        {
            "paperId": "c582360efacf02d5469f4b4e80438c7cb7a16e33",
            "title": "Uncertain Time-Series Similarity: Return to the Basics",
            "abstract": "In the last years there has been a considerable increase in the availability of continuous sensor measurements in a wide range of application domains, such as Location-Based Services (LBS), medical monitoring systems, manufacturing plants and engineering facilities to ensure efficiency, product quality and safety, hydrologic and geologic observing systems, pollution management, and others. \n \nDue to the inherent imprecision of sensor observations, many investigations have recently turned into querying, mining and storing uncertain data. Uncertainty can also be due to data aggregation, privacy-preserving transforms, and error-prone mining algorithms. \n \nIn this study, we survey the techniques that have been proposed specifically for modeling and processing uncertain time series, an important model for temporal data. We provide an analytical evaluation of the alternatives that have been proposed in the literature, highlighting the advantages and disadvantages of each approach, and further compare these alternatives with two additional techniques that were carefully studied before. We conduct an extensive experimental evaluation with 17 real datasets, and discuss some surprising results, which suggest that a fruitful research direction is to take into account the temporal correlations in the time series. Based on our evaluations, we also provide guidelines useful for the practitioners in the field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1905966",
                    "name": "Michele Dallachiesa"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "1701357",
                    "name": "Katsiaryna Mirylenka"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "0ffef3e1ecba4dc6995fbe38a62f56eb1e7ba6f3",
            "title": "Eliminating the redundancy in blocking-based entity resolution methods",
            "abstract": "Entity resolution is the task of identifying entities that refer to the same real-world object. It has important applications in the context of digital libraries, such as citation matching and author disambiguation. Blocking is an established methodology for efficiently addressing this problem; it clusters similar entities together, and compares solely entities inside each cluster. In order to effectively deal with the current large, noisy and heterogeneous data collections, novel blocking methods that rely on redundancy have been introduced: they associate each entity with multiple blocks in order to increase recall, thus increasing the computational cost, as well.\n In this paper, we introduce novel techniques that remove the superfluous comparisons from any redundancy-based blocking method. They improve the time-efficiency of the latter without any impact on the end result. We present the optimal solution to this problem that discards all redundant comparisons at the cost of quadratic space complexity. For applications with space limitations, we also present an alternative, lightweight solution that operates at the abstract level of blocks in order to discard a significant part of the redundant comparisons. We evaluate our techniques on two large, real-world data sets and verify the significant improvements they convey when integrated into existing blocking methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "35176340",
                    "name": "Ekaterini Ioannou"
                },
                {
                    "authorId": "1718268",
                    "name": "C. Nieder\u00e9e"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1744808",
                    "name": "W. Nejdl"
                }
            ]
        },
        {
            "paperId": "1191d6a025e62eda79a9b52a682d538194477e4c",
            "title": "Diverse Dimension Decomposition of an Itemset Space",
            "abstract": "We introduce the problem of diverse dimension decomposition in transactional databases. A dimension is a set of mutually-exclusive item sets, and our problem is to find a decomposition of the item set space into dimensions, which are orthogonal to each other, and that provide high coverage of the input database. The mining framework we propose effectively represents a dimensionality-reducing transformation from the space of all items to the space of orthogonal dimensions. Our approach relies on information-theoretic concepts, and we are able to formulate the dimension-finding problem with a single objective function that simultaneously captures constraints on coverage, exclusivity and orthogonality. We describe an efficient greedy method for finding diverse dimensions from transactional databases. The experimental evaluation of the proposed approach using two real datasets, flickr and delicious, demonstrates the effectiveness of our solution. Although we are motivated by the applications in the collaborative tagging domain, we believe that the mining task we introduce in this paper is general enough to be useful in other application domains.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2245199",
                    "name": "Mikalai Tsytsarau"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "241cb885a4d22b84cad095a7a18b28b94185d27f",
            "title": "Efficient Top-k Approximate Subtree Matching in Small Memory",
            "abstract": "We consider the Top-k Approximate Subtree Matching (TASM) problem: finding the k best matches of a small query tree within a large document tree using the canonical tree edit distance as a similarity measure between subtrees. Evaluating the tree edit distance for large XML trees is difficult: the best known algorithms have cubic runtime and quadratic space complexity, and, thus, do not scale. Our solution is TASM-postorder, a memory-efficient and scalable TASM algorithm. We prove an upper bound for the maximum subtree size for which the tree edit distance needs to be evaluated. The upper bound depends on the query and is independent of the document size and structure. A core problem is to efficiently prune subtrees that are above this size threshold. We develop an algorithm based on the prefix ring buffer that allows us to prune all subtrees above the threshold in a single postorder scan of the document. The size of the prefix ring buffer is linear in the threshold. As a result, the space complexity of TASM-postorder depends only on k and the query size, and the runtime of TASM-postorder is linear in the size of the document. Our experimental evaluation on large synthetic and real XML documents confirms our analytic results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2893196",
                    "name": "Nikolaus Augsten"
                },
                {
                    "authorId": "145690522",
                    "name": "Denilson Barbosa"
                },
                {
                    "authorId": "145903871",
                    "name": "Michael H. B\u00f6hlen"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "77b0bf2c7731b4c16bd19640f23cf6efada06e84",
            "title": "Detecting and exploiting stability in evolving heterogeneous information spaces",
            "abstract": "Individuals contribute content on the Web at an unprecedented rate, accumulating immense quantities of (semi-)structured data. Wisdom of the Crowds theory advocates that such information (or parts of it) is constantly overwritten, updated, or even deleted by other users, with the goal of rendering it more accurate, or up-to-date. This is particularly true for the collaboratively edited, semi-structured data of entity repositories, whose entity profiles are consistently kept fresh. Therefore, their core information that remain stable with the passage of time, despite being reviewed by numerous users, are particularly useful for the description of an entity.\n Based on the above hypothesis, we introduce a classification scheme that predicts, on the basis of statistical and content patterns, whether an attribute (i.e., name-value pair) is going to be modified in the future. We apply our scheme on a large, real-world, versioned dataset and verify its effectiveness. Our thorough experimental study also suggests that reducing entity profiles to their stable parts conveys significant benefits to two common tasks in computer science: information retrieval and information integration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "144274464",
                    "name": "George Giannakopoulos"
                },
                {
                    "authorId": "1718268",
                    "name": "C. Nieder\u00e9e"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1744808",
                    "name": "W. Nejdl"
                }
            ]
        },
        {
            "paperId": "9f58f9bb4e1369cfad736b2e6e732123fdbbe19e",
            "title": "Decentralised social network management",
            "abstract": "Social networking sites have gained much popularity in the recent years because of the opportunities they give to people to connect to each other in an easy and timely manner, and to exchange and share various kinds of information. However, these sites are based on a centralised paradigm, which limits the mobility of their users, and ultimately, their chances to establish new relationships and benefit from diverse networking services. In this paper, we argue for a decentralised paradigm for social networking, in which users retain control of their profiles, and social networking sites focus on the delivery of innovative and competitive services. In this environment, both the social networking sites and their users will be able to develop to their full potential. This goal can be achieved by using a combination of semantic web technologies and tools, loosening the bind between the social network management and social networking web applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48643170",
                    "name": "S. Bortoli"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1755699",
                    "name": "P. Bouquet"
                }
            ]
        },
        {
            "paperId": "caa92e58538bd1c8e1d4161380fe62f4faa4db6a",
            "title": "To compare or not to compare: making entity resolution more efficient",
            "abstract": "Blocking methods are crucial for making the inherently quadratic task of Entity Resolution more efficient. The blocking methods proposed in the literature rely on the homogeneity of data and the availability of binding schema information; thus, they are inapplicable to the voluminous, noisy, and highly heterogeneous data of the Web 2.0 user-generated content. To deal with such data, attribute-agnostic blocking has been recently introduced, following a two-fold strategy: the first layer places entities into overlapping blocks in order to achieve high effectiveness, while the second layer reduces the number of unnecessary comparisons in order to enhance efficiency.\n In this paper, we present a set of techniques that can be plugged into the second strategy layer of attribute-agnostic blocking to further improve its efficiency. We introduce a technique that eliminates redundant comparisons, and, based on this, we incorporate an approximate method for pruning comparisons that are highly likely to involve non-matching entities. We also introduce a novel measure for quantifying the redundancy a blocking method entails and explain how it can be used to a-priori tune the process of comparisons pruning. We apply our blocking techniques on two large, real-world data sets and report results that demonstrate a substantial increase in efficiency at a negligible (if any) cost in effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144774666",
                    "name": "G. Papadakis"
                },
                {
                    "authorId": "35176340",
                    "name": "Ekaterini Ioannou"
                },
                {
                    "authorId": "1718268",
                    "name": "C. Nieder\u00e9e"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1744808",
                    "name": "W. Nejdl"
                }
            ]
        },
        {
            "paperId": "cf710875adaf49dca4e8ce1d484b166c899ef324",
            "title": "Similarity matching for uncertain time series: analytical and experimental comparison",
            "abstract": "In the last years there has been a considerable increase in the availability of continuous sensor measurements in a wide range of application domains, such as Location-Based Services (LBS), medical monitoring systems, manufacturing plants and engineering facilities to ensure efficiency, product quality and safety, hydrologic and geologic observing systems, pollution management, and others.\n Due to the inherent imprecision of sensor observations, many investigations have recently turned into querying, mining and storing uncertain data. Uncertainty can also be due to data aggregation, privacy-preserving transforms, and error-prone mining algorithms.\n In this study, we survey the techniques that have been proposed specifically for modeling and processing uncertain time series, an important model for temporal data. We provide both an analytical evaluation of the alternatives that have been proposed in the literature, highlighting the advantages and disadvantages of each approach. We additionally conduct an extensive experimental evaluation with 17 real datasets, and discuss some surprising results. Based on our evaluations, we also provide guidelines useful for practitioners in the field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1905966",
                    "name": "Michele Dallachiesa"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "1701357",
                    "name": "Katsiaryna Mirylenka"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "f62949d0a3b5317d17ab7ecd837b1312d3bfc9eb",
            "title": "Towards a Framework for Detecting and Managing Opinion Contradictions",
            "abstract": "Sentiment Analysis gains in interest due to the large amount of potential applications and the increasing number of opinions expressed in particular in the Web. The focus of this paper is the development of a framework on top of sentiment analysis for detecting contradictions. First, we introduce a statistical model of contradictions based on a mean value and the variance of sentiments among different posts. It can be used to analyze and track sentiment evolution over time, to identify interesting trends and patterns or even to enable argument extraction. Using synthetic datasets, we demonstrate the effectiveness of our method in capturing contradictions on noisy data. Inspired by this model, which has proven to be effective and efficient for numeric sentiments, we are trying to generalize it for arbitrary opinion data and outline a universal framework which can be efficiently used on a large scale. We discuss various problems and challenges of such a formulation and outline the scope of our future work in this direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2245199",
                    "name": "Mikalai Tsytsarau"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "4d3a47a7f95eb3814aa4a81c0944994f2414a911",
            "title": "TASM: Top-k Approximate Subtree Matching",
            "abstract": "We consider the Top-k Approximate Subtree Matching (TASM) problem: finding the k best matches of a small query tree, e.g., a DBLP article with 15 nodes, in a large document tree, e.g., DBLP with 26M nodes, using the canonical tree edit distance as a similarity measure between subtrees. Evaluating the tree edit distance for large XML trees is difficult: the best known algorithms have cubic runtime and quadratic space complexity, and, thus, do not scale. Our solution is TASM-postorder, a memory-efficient and scalable TASM algorithm. We prove an upper-bound for the maximum subtree size for which the tree edit distance needs to be evaluated. The upper bound depends on the query and is independent of the document size and structure. A core problem is to efficiently prune subtrees that are above this size threshold. We develop an algorithm based on the prefix ring buffer that allows us to prune all subtrees above the threshold in a single postorder scan of the document. The size of the prefix ring buffer is linear in the threshold. As a result, the space complexity of TASM-postorder depends only on k and the query size, and the runtime of TASM-postorder is linear in the size of the document. Our experimental evaluation on large synthetic and real XML documents confirms our analytic results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2893196",
                    "name": "Nikolaus Augsten"
                },
                {
                    "authorId": "145690522",
                    "name": "Denilson Barbosa"
                },
                {
                    "authorId": "145903871",
                    "name": "Michael H. B\u00f6hlen"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "6656390846c077abb43fe8f428d9af64def290f3",
            "title": "Scalable discovery of contradictions on the web",
            "abstract": "Our study addresses the problem of large-scale contradiction detection and management, from data extracted from the Web. We describe the first systematic solution to the problem, based on a novel statistical measure for contradictions, which exploits first- and second-order moments of sentiments. Our approach enables the interactive analysis and online identification of contradictions under multiple levels of time granularity. The proposed algorithm can be used to analyze and track opinion evolution over time and to identify interesting trends and patterns. It uses an incrementally updatable data structure to achieve computational efficiency and scalability. Experiments with real datasets show promising time performance and accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2245199",
                    "name": "Mikalai Tsytsarau"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2251210",
                    "name": "K. Denecke"
                }
            ]
        },
        {
            "paperId": "70d972e44132ff28b97bc1635ddf0dad7b4e20a3",
            "title": "iSAX 2.0: Indexing and Mining One Billion Time Series",
            "abstract": "There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to index and mine very large collections of time series. Examples of such applications come from astronomy, biology, the web, and other domains. It is not unusual for these applications to involve numbers of time series in the order of hundreds of millions to billions. However, all relevant techniques that have been proposed in the literature so far have not considered any data collections much larger than one-million time series. In this paper, we describe iSAX 2.0, a data structure designed for indexing and mining truly massive collections of time series. We show that the main bottleneck in mining such massive datasets is the time taken to build the index, and we thus introduce a novel bulk loading mechanism, the first of this kind specifically tailored to a time series index. We show how our method allows mining on datasets that would otherwise be completely untenable, including the first published experiments to index one billion time series, and experiments in mining massive data from domains as diverse as entomology, DNA and web-scale image collections.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3122175",
                    "name": "A. Camerra"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1887371",
                    "name": "Jin Shieh"
                },
                {
                    "authorId": "50543766",
                    "name": "Eamonn J. Keogh"
                }
            ]
        },
        {
            "paperId": "7b775f013494824472b1703ce92f4a620566ca1e",
            "title": "The Entity Name System: Enabling the web of entities",
            "abstract": "We are currently witnessing an increasing interest in the use of the web as an information and knowledge source. Much of the information sought after in the web is in this case relevant to named entities (i.e., persons, locations, organizations, etc.). An important observation is that the entity identification problem lies at the core of many applications in this context. In order to deal with this problem, we propose the Entity Name System (ENS), a large scale, distributed infrastructure for assigning and managing unique identifiers for entities in the web. In this paper, we examine the special requirements for storage and management of entities, in the context of the ENS. We present a conceptual model for the representation of entities, and discuss problems related to data quality, as well as the management of the entity lifecycle. Finally, we describe the architecture of the current prototype of the system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802012",
                    "name": "Heiko Stoermer"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "144274464",
                    "name": "George Giannakopoulos"
                }
            ]
        },
        {
            "paperId": "7e23875219772196ec76b85ab4d30905b6f831b4",
            "title": "The Effect of History on Modeling Systems' Performance: The Problem of the Demanding Lord",
            "abstract": "In several concept attainment systems, ranging from recommendation systems to information filtering, a sliding window of learning instances has been used in the learning process to allow the learner to follow concepts that change over time. However, no analytic study has been performed on the relation between the size of the sliding window and the performance of a learning system. In this work, we present such an analytic model that describes the effect of the sliding window size on the prediction performance of a learning system based on iterative feedback. Using a signal-to-noise approach to model the learning ability of the underlying machine learning algorithms, we can provide good estimates of the average performance of a modeling system independently of the supervised machine learning algorithm employed. We experimentally validate the effectiveness of the proposed methodology with detailed experiments using synthetic and real datasets, and a variety of learning algorithms, including Support Vector Machines, Naive Bayes, Nearest Neighbor and Decision Trees. The results validate the analysis and indicate very good estimation performance in different settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144274464",
                    "name": "George Giannakopoulos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "13e56ec53497324d193b1a7bb0d10042efb331d8",
            "title": "Topic-related Sentiment Analysis for Discovering Contradicting Opinions in Weblogs",
            "abstract": "This work addresses the problem of analyzing the evolution of community opinions across time. First, a two-step approach is introduced to determine a continuous sentiment value for each topic discussed in a text based on SentiWordNet as lexical resource. Sentences are clustered according to their topic using Latent Dirichlet Allocation. Both steps are extensively evaluated and tested. The output is then exploited for studying contradictions among weblog posts and comments. We introduce a novel measure for contradictions based on a mean value and the variance of opinions among different posts. In addition, a method is proposed, which identifies posts with contradicting opinions on certain topics on a basis of such a measure. It can be used to analyze and track opinion evolution over time and to identify interesting trends and patterns. The developed algorithm is applied to a dataset of medical blogs and comments on political news with promising performance and accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2251210",
                    "name": "K. Denecke"
                },
                {
                    "authorId": "2245199",
                    "name": "Mikalai Tsytsarau"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2972143",
                    "name": "Marko Brosowski"
                }
            ]
        },
        {
            "paperId": "a605df4eefe9c468aa124b949b8b4314e286765b",
            "title": "Scalable Discovery of Contradicting Opinions in Weblogs",
            "abstract": "Weblogs are a popular means of information communication, where people discuss a variety of topics, and often times also express their opinions on these topics. In this work, we address the problem of analyzing the evolution of community opinions across time, as these are represented in the weblogs. In particular, we are interested in identifying topics and time windows, for which contradictory opinions have been expressed. We describe an approach for solving the above problem, which consists of the following steps. We first introduce a technique for topic and opinion extraction that operates at the sentence level. Then, we propose a novel measure for contradictions that can effectively aggregate the relevant information from the weblog posts. We discuss its properties, and show how it can be used to detect two different types of contradictions, namely, simultaneous contradictions, and change of sentiment. Finally, we describe an efficient data structure for answering queries related to contradiction detection, and show that it has the additional property of being incrementally maintainable. A detailed experimental evaluation of our approach with synthetic and real datasets demonstrates the applicability and efficiency of our techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2245199",
                    "name": "Mikalai Tsytsarau"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "2251210",
                    "name": "K. Denecke"
                },
                {
                    "authorId": "2972143",
                    "name": "Marko Brosowski"
                }
            ]
        },
        {
            "paperId": "b8e6e765f683b0b98e9c47ba62915daf047bd5ff",
            "title": "Towards a general entity representation model",
            "abstract": "In this paper, we argue for an infrastructure responsible for assigning and managing unique identifiers for entities in the semantic web, and we propose a conceptual model for the storage and management of these entities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1780315",
                    "name": "B. Bazzanella"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1802012",
                    "name": "Heiko Stoermer"
                }
            ]
        },
        {
            "paperId": "f3ecf7b39befb8da1289c13742f0d6c43a864cec",
            "title": "Adaptivity in Entity Subscription Services",
            "abstract": "Real-word entities can be mapped to unique entity identifiers through an Entity Name System (ENS), to systematically support the re-use of these identifiers and disambiguate references to real world entities in the Web. An entity subscription service informs subscribed users of changes in the descriptive data of an entity, which is a set of attribute name-value pairs. We study the design, implementation and application of an adaptable push-policy subscription service, within a large-scale ENS. The subscription system aims to deliver ranked descriptions of the changes on entities, following user preferences through a feedback-driven adaptation process. The adaptation is based on both the content and the type of each entity change. We evaluate the learning curve of the system and the utility of the content-type discrimination. The experiments demonstrate good results, especially in the system's content-aware adaptation aspect.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144274464",
                    "name": "George Giannakopoulos"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "33a8f84fc96991e5ae269e79e100ad766886b161",
            "title": "Entity Data Management in OKKAM",
            "abstract": "In the past years, we are witnessing an increasing interest in the semantic Web and the relevant technologies, which can have a significant impact in the enterprise environment of information and knowledge management. An important observation is that the entity identification problem lies at the core of many semantic Web applications. In this paper, we examine the special requirements of storage and management for entities, in the context of an entity management system for the semantic Web. We study the requirements with respect to creating and modifying these entities, as well as to managing their evolution over time. Finally, we propose a conceptual model for there presentation of entities, and discuss related research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "50027267",
                    "name": "J. Chaudhry"
                },
                {
                    "authorId": "1739619",
                    "name": "Periklis Andritsos"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "8dd110d19606a16feaa002e31e2304e90574df62",
            "title": "Managing Data Quality in Business Intelligence Applications",
            "abstract": "Business Intelligence (BI) solutions commonly aim at assisting decision-making processes by providing a comprehensive view over a company\u2019s core business data and suitable abstractions thereof. Decision-making based on BI solutions therefore builds on the assumption that providing users with targeted, problemspecific fact data enables them to make informed and, hence, better decisions in their everyday businesses. In order to really provide users with all the necessary details to make informed decisions, we however believe that \u2013 in addition to conventional reports \u2013 it is essential to also provide users with information about the quality, i.e. with quality metadata, regarding the data from which reports are generated. Identifying a lack of support for quality metadata management in conventional BI solutions, in this paper we propose the idea of quality-aware reports and a possible architecture for quality-aware BI, able to involve the users themselves into the quality metadata management process, by explicitly soliciting and exploiting user feedback.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144896044",
                    "name": "F. Daniel"
                },
                {
                    "authorId": "145866446",
                    "name": "F. Casati"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "3025805",
                    "name": "O. Chayka"
                }
            ]
        },
        {
            "paperId": "bfb4f40d594cf2d746aaace8fd7a5a2a5b2ca713",
            "title": "Entity Lifecycle Management for OKKAM",
            "abstract": "In this paper, we examine the special requirements of lifecycle management for entities in the context of an entity management system for the semantic web. We study the requirements with respect to creating and modifying these entities, as well as to managing their evolution over time. Furthermore, we present the issues arising from the access control models needed for the management of a large, distributed repository of entities. Finally, we discuss the research directions that can offer solutions to the above problems, and give a brief overview of techniques and methods relevant to these solution directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50027267",
                    "name": "J. Chaudhry"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1739619",
                    "name": "Periklis Andritsos"
                },
                {
                    "authorId": "1790878",
                    "name": "A. Ma\u00f1a"
                }
            ]
        },
        {
            "paperId": "df4c48faea2fd312f29a32c1d1ecc8cdbd11f895",
            "title": "Streaming Time Series Summarization Using User-Defined Amnesic Functions",
            "abstract": "The past decade has seen a wealth of research on time series representations. The vast majority of research has concentrated on representations that are calculated in batch mode and represent each value with approximately equal fidelity. However, the increasing deployment of mobile devices and real time sensors has brought home the need for representations that can be incrementally updated, and can approximate the data with fidelity proportional to its age. The latter property allows us to answer queries about the recent past with greater precision, since in many domains recent information is more useful than older information. We call such representations amnesic. While there has been previous work on amnesic representations, the class of amnesic functions possible was dictated by the representation itself. In this work, we introduce a novel representation of time series that can represent arbitrary, user-specified amnesic functions. We propose online algorithms for our representation, and discuss their properties. Finally, we perform an extensive empirical evaluation on 40 datasets, and show that our approach can efficiently maintain a high quality amnesic approximation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1699100",
                    "name": "M. Vlachos"
                },
                {
                    "authorId": "50543766",
                    "name": "Eamonn J. Keogh"
                },
                {
                    "authorId": "1736832",
                    "name": "D. Gunopulos"
                }
            ]
        },
        {
            "paperId": "7054cd9eb0950e822c17246d0eae4f17b98cb00e",
            "title": "Online Distribution Estimation for Streaming Data: Framework and Applications",
            "abstract": "In the last few years, we have been witnessing an evergrowing need for continuous observation and monitoring applications. This need is driven by recent technological advances that have made streaming applications possible, and by the fact that analysts in various domains have realized the value that such applications can provide. In this paper, we propose a general framework for computing efficiently an approximation of multi-dimensional distributions of streaming data. This framework enables the development of a wide variety of complex streaming applications. In addition, we demonstrate how our framework can operate in a distributed fashion, thus, making better use of the available resources. We motivate our techniques using two concrete problems, both in the challening context of resource-constrained sensor networks. The first problem is outlier detection, while the second is detection and tracking of homogeneous regions. Experiments with synthetic and real data show that our method is efficient and accurate, and compares favorably to other proposed techniques for both the problems that we studied.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1685532",
                    "name": "V. Kalogeraki"
                },
                {
                    "authorId": "1736832",
                    "name": "D. Gunopulos"
                }
            ]
        },
        {
            "paperId": "74513b68335a222894fea6ec1df0d00f838f3588",
            "title": "WhiteWater: Distributed Processing of Fast Streams",
            "abstract": "Monitoring systems today often involve continuous queries over streaming data in a distributed collaborative fashion. The distribution of query operators over a network of processors, as well as their processing sequence, form a query configuration with inherent constraints on the throughput that it can support. In this paper, we discuss the implications of measuring and optimizing for output throughput, as well as its limitations. We propose to use instead the more granular input throughput and a version of throughput measure, the profiled input throughput, that is focused on matching the expected behavior of the input streams. We show how we can evaluate a query configuration based on profiled input throughput and that the problem of finding the optimal configuration is NP-hard. Furthermore, we describe how we can overcome the complexity limitation by adapting hill-climbing heuristics to reduce the search space of configurations. We show experimentally that the approach used is not only efficient but also effective.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2915394",
                    "name": "I. Stanoi"
                },
                {
                    "authorId": "1678202",
                    "name": "G. Mihaila"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "35250359",
                    "name": "Christian A. Lang"
                }
            ]
        },
        {
            "paperId": "24fc7f37abf07f25513a159a05a2c5e939a4f6b6",
            "title": "Distributed Real-Time Detection and Tracking of Homogeneous Regions in Sensor Networks",
            "abstract": "In many applications we can deploy large number of sensors spanning wide geographical areas, to monitor environmental phenomena. The analysis of the data collected by such sensor network can help us to understand the field dynamics, and optimize the deployment of other solutions. We define a group of sensors having similar underlying distribution over a period of time as a homogeneous region. In this paper we propose distributed algorithms to detect such regions, approximate their boundary with a piece-wise linear curve and track the boundary in real-time. Experimental results show the accuracy and efficiency of our detection and tracking algorithms",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053933772",
                    "name": "Sharmila Subramaniam"
                },
                {
                    "authorId": "1685532",
                    "name": "V. Kalogeraki"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "29cab8c76466a3d29357552f82cd30f9baf383f3",
            "title": "Maximizing the sustained throughput of distributed continuous queries",
            "abstract": "Monitoring systems today often involve continuous queries over streaming data, in a distributed collaborative system. The distribution of query operators over a network of processors, and their processing sequence, form a query configuration with inherent constraints on the throughput it can support. In this paper we propose to optimize stream queries with respect to a version of throughput measure, the profiled input throughput. This measure is focused on matching the expected behavior of the input streams. To prune the search space we used hill-climbing techniques that proved to be efficient and effective.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2915394",
                    "name": "I. Stanoi"
                },
                {
                    "authorId": "1678202",
                    "name": "G. Mihaila"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "35250359",
                    "name": "Christian A. Lang"
                }
            ]
        },
        {
            "paperId": "93be9463983a6a3adb96c42a5dc59c6b5537cec0",
            "title": "Model-Driven Dashboards for Business Performance Reporting",
            "abstract": "Business performance modeling and model-driven business transformation are two research directions that are attracting much attention lately. In this study, we propose an approach for dashboard development that is model-driven and can be integrated with the business performance models. We adopt the business performance modeling framework, and we extend it in order to capture the reporting aspect of the business operation. We describe models that can effectively represent all the elements necessary for the business performance reporting process, and the interactions among them. We also demonstrate how all these models can be combined and automatically generate the final solution. Finally, we discuss our experience from the application of our technique in a real-world scenario. This case study shows that our technique can be efficiently applied to and handle changes in the underlying business models, delivering significant benefits in terms of both development time and flexibility",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1808548",
                    "name": "Pawan Chowdhary"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1829623",
                    "name": "F. Pinel"
                },
                {
                    "authorId": "144648339",
                    "name": "Shyh-Kwei Chen"
                },
                {
                    "authorId": "49605055",
                    "name": "Frederick Y. Wu"
                }
            ]
        },
        {
            "paperId": "afa2c668482af7e5c23bb55f64111ae5a413a99c",
            "title": "Online outlier detection in sensor data using non-parametric models",
            "abstract": "Sensor networks have recently found many popular applications in a number of different settings. Sensors at different locations can generate streaming data, which can be analyzed in real-time to identify events of interest. In this paper, we propose a framework that computes in a distributed fashion an approximation of multi-dimensional data distributions in order to enable complex applications in resource-constrained sensor networks.We motivate our technique in the context of the problem of outlier detection. We demonstrate how our framework can be extended in order to identify either distance- or density-based outliers in a single pass over the data, and with limited memory requirements. Experiments with synthetic and real data show that our method is efficient and accurate, and compares favorably to other proposed techniques. We also demonstrate the applicability of our technique to other related problems in sensor networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053933772",
                    "name": "Sharmila Subramaniam"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "144135820",
                    "name": "Dimitris Papadopoulos"
                },
                {
                    "authorId": "1685532",
                    "name": "V. Kalogeraki"
                },
                {
                    "authorId": "1736832",
                    "name": "D. Gunopulos"
                }
            ]
        },
        {
            "paperId": "65095e08b82e04fc3d355040c0df31056a4bea91",
            "title": "Using datacube aggregates for approximate querying and deviation detection",
            "abstract": "Much research has been devoted to the efficient computation of relational aggregations and, specifically, the efficient execution of the datacube operation. In this paper, we consider the inverse problem, that of deriving (approximately) the original data from the aggregates. We motivate this problem in the context of two specific application areas, approximate query answering and data analysis. We propose a framework based on the notion of information entropy that enables us to estimate the original values in a data set, given only aggregated information about it. We then show how approximate queries on the data from which the aggregates were derived can be performed using our framework. We also describe an alternate use of the proposed framework that enables us to identify values that deviate from the underlying data distribution, suitable for data mining purposes. We present a detailed performance study of the algorithms using both real and synthetic data, highlighting the benefits of our approach as well as the efficiency of the proposed solutions. Finally, we evaluate our techniques with a case study on a real data set, which illustrates the applicability of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "1682406",
                    "name": "A. Mendelzon"
                }
            ]
        },
        {
            "paperId": "28a207174e2b6219a0d9f6c0f981ab2f1edfa478",
            "title": "Online amnesic approximation of streaming time series",
            "abstract": "The past decade has seen a wealth of research on time series representations, because the manipulation, storage, and indexing of large volumes of raw time series data is impractical. The vast majority of research has concentrated on representations that are calculated in batch mode and represent each value with approximately equal fidelity. However, the increasing deployment of mobile devices and real time sensors has brought home the need for representations that can be incrementally updated, and can approximate the data with fidelity proportional to its age. The latter property allows us to answer queries about the recent past with greater precision, since in many domains recent information is more useful than older information. We call such representations amnesic. While there has been previous work on amnesic representations, the class of amnesic functions possible was dictated by the representation itself. We introduce a novel representation of time series that can represent arbitrary, user-specified amnesic functions. For example, a meteorologist may decide that data that is twice as old can tolerate twice as much error, and thus, specify a linear amnesic function. In contrast, an econometrist might opt for an exponential amnesic function. We propose online algorithms for our representation, and discuss their properties. Finally, we perform an extensive empirical evaluation on 40 datasets, and show that our approach can efficiently maintain a high quality amnesic approximation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1699100",
                    "name": "M. Vlachos"
                },
                {
                    "authorId": "50543766",
                    "name": "Eamonn J. Keogh"
                },
                {
                    "authorId": "1736832",
                    "name": "D. Gunopulos"
                },
                {
                    "authorId": "1929595",
                    "name": "Wagner Truppel"
                }
            ]
        },
        {
            "paperId": "25fecacc2215df184bc5a7466890302694a17a81",
            "title": "Space constrained selection problems for data warehouses and pervasive computing",
            "abstract": "Space constrained optimization problems arise in a multitude of important applications such as data warehouses and pervasive computing. A typical instance of such problems is to select a set of items of interest, subject to a constraint on the total space occupied by these items. Assuming that each item is associated with a benefit, for a suitably defined notion of benefit, one wishes to optimize the total benefit for the selected items. We show that in many important applications, one faces variants of this basic problem in which the individual items are sets themselves, and each set is associated with a benefit value. We present instances of such problems in the context of data warehouse management and pervasive computing, derive their complexity, and propose several techniques for solving them. Since there are no known approximation algorithms for these problems, we explore the use of greedy and randomized techniques. We present a detailed performance study of the algorithms, highlighting the efficiency of the proposed solutions and the benefits of each approach. Finally, we present a worst-case analysis of the algorithms, which can be useful in practice for choosing among the alternatives. The solutions proposed in the paper are generic and likely to find applications in many more problems of interest than those mentioned above.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "1682406",
                    "name": "A. Mendelzon"
                }
            ]
        },
        {
            "paperId": "a0f65d93d6cb65ebd2cb883ded4349fdc65a7439",
            "title": "Distributed deviation detection in sensor networks",
            "abstract": "Sensor networks have recently attracted much attention, because of their potential applications in a number of different settings. The sensors can be deployed in large numbers in wide geographical areas, and can be used to monitor physical phenomena, or to detect certain events.An interesting problem which has not been adequately addressed so far is that of distributed online deviation detection in streaming data. The identification of deviating values provides an efficient way to focus on the interesting events in the sensor network.In this work, we propose a technique for online deviation detection in streaming data. We discuss how these techniques can operate efficiently in the distributed environment of a sensor network, and discuss the tradeoffs that arise in this setting. Our techniques process as much of the data as possible in a decentralized fashion, so as to avoid unnecessary communication and computational effort.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "144135820",
                    "name": "Dimitris Papadopoulos"
                },
                {
                    "authorId": "1685532",
                    "name": "V. Kalogeraki"
                },
                {
                    "authorId": "1736832",
                    "name": "D. Gunopulos"
                }
            ]
        },
        {
            "paperId": "cd72ba040ae1022be5d089af27500c3c620710ad",
            "title": "Data reduction in data warehouses",
            "abstract": "Much research has been devoted to the efficient computation of relational aggregations. In this paper we consider the inverse problem, that of deriving (approximately) the original data from the aggregates. \nWe motivate this problem in the context of two specific application areas, approximate query answering and data analysis. We propose a framework based on the notion of information entropy that enables us to estimate the original values in a data set, given only aggregated information about it. We then show how approximate queries on the data from which the aggregates were derived can be performed using our framework. We also describe an alternate use of the proposed framework that enables us to identify values that deviate from the underlying data distribution, suitable for data ruining purposes. \nWe present a detailed performance study of the algorithms using both real and synthetic data, highlighting the benefits of our approach as well as the efficiency of the proposed solutions. \nSubsequently, we consider the above problem in a space constrained environment, where only a subset of the required aggregates can be stored. More specifically, we wish to select a set of aggregates of interest, subject to a constraint on the total space occupied by these aggregates. The objective is to maximize the total benefit, which is a function of the number and importance of the queries that we can estimate given the selected aggregates. \nFor this problem, which as we show is NP-hard, there are no known polynomial approximation schemes, and we propose several algorithms for solving it. We explore the use of greedy and randomized techniques as well as clustering based approaches. The solutions presented herein are generic and can be applied to other problem domains as well. \nWe explore the properties and special characteristics of the above techniques with an experimental evaluation. Our results illustrate the behavior of the algorithms under different settings, and highlight the benefits of each approach. Based on our analysis, we present worst case scenarios for the algorithms. This offers insight into the operation of the algorithms, and provides a practical guide for selecting among the proposed techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "f996f5ed468126a655b3c49b8f3b83ba511dcddb",
            "title": "Ranked join indices",
            "abstract": "A plethora of data sources contain data entities that could be ordered according to a variety of attributes associated with the entities. Such orderings result effectively in a ranking of the entities according to the values in the attribute domain. Commonly, users correlate such sources for query processing purposes through join operations. In query processing, it is desirable to incorporate user preferences towards specific attributes or their values. A way to incorporate such preferences is by utilizing scoring functions that combine user preferences and attribute values and return a numerical score for each tuple in the join result. Then, a target query, which we refer to as top-k join query, seeks to identify the k tuples in the join result with the highest scores. We propose a novel technique, which we refer to as ranked join index, to efficiently answer top-k join queries for arbitrary, user specified, preferences and a large class of scoring functions. Our rank join index requires small space (compared to the entire join result) and provides guarantees for its performance. Moreover, our proposal provides a graceful tradeoff between its space requirements and worst case search performance. We supplement our analytical results with a thorough experimental evaluation using a variety of real and synthetic data sets, demonstrating that, in comparison to other viable approaches, our technique offers significant performance benefits.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1701195",
                    "name": "Panayiotis Tsaparas"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1751420",
                    "name": "Y. Kotidis"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "c8b593196ccca4242f23a568e8bec638393357ce",
            "title": "Entropy based approximate querying and exploration of datacubes",
            "abstract": "Much research has been devoted to the efficient computation of relational aggregations and specifically the efficient execution of the datacube operation. We consider the inverse problem, that of deriving (approximately) the original data from the aggregates. We motivate this problem in the context of two specific application areas, that of approximate query answering and data analysis. We propose a framework based on the notion of information entropy that enables us to estimate the original values in a data set, given only aggregated information about it. We also describe an alternate utility of the proposed framework, that enables us to identify values that deviate from the underlying data distribution, suitable for data mining purposes. Finally, we present a detailed performance study of the algorithms using both real and synthetic data, highlighting the benefits of our approach as well as the efficiency of the proposed solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                }
            ]
        },
        {
            "paperId": "2320cc76c219727ce6ebc11a5a5b4eac15864200",
            "title": "Knowledge discovery in data warehouses",
            "abstract": "As the size of data warehouses increase to several hundreds of gigabytes or terabytes, the need for methods and tools that will automate the process of knowledge extraction, or guide the user to subsets of the dataset that are of particular interest, is becoming prominent. In this survey paper we explore the problem of identifying and extracting interesting knowledge from large collections of data residing in data warehouses, by using data mining techniques. Such techniques have the ability to identify patterns and build succinct models to describe the data. These models can also be used to achieve summarization and approximation. We review the associated work in the OLAP, data mining, and approximate query answering literature. We discuss the need for the traditional data mining techniques to adapt, and accommodate the specific characteristics of OLAP systems. We also examine the notion of interestingness of data, as a tool to guide the analysis process. We describe methods that have been proposed in the literature for determining what is interesting to the user and what is not, and how these approaches can be incorporated in the data mining algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        },
        {
            "paperId": "070422fa3cc9321e3f7cc51c3d45f6037f0034d9",
            "title": "Reducing Retrieval Latencies in the Web: the Past, the Present, and the Future",
            "abstract": "One of the main directions of research in the Web is to reduce the time latencies users experience when navigating through Web sites. The work in this direction originated from relevant research in operating systems (e.g. caching), where the aim is to reduce le system latencies. Caching is already being used in the Web domain. However, recent studies indicate that the beneets from this technique are rather limited. Thus, another method that was rst applied in operating systems, prefetching, is now being studied in the Web context. In addition to prefetching, there is a number of other techniques that are being examined in the Web domain and are speciically targeted to this environment. These techniques include compression, and piggybacking of information between the servers and the clients. The main contributions of this survey paper are: A taxonomy of the caching and prefetching techniques, according to the underlying principles, and the algorithms used; a critical survey of the methods that have been proposed so far for dealing with the retrieval latencies problem, and how each one of those ts in the taxonomy; a discussion of the results of the past and ongoing work, and the identiication of future research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "145846952",
                    "name": "B. Krishnamurthy"
                }
            ]
        },
        {
            "paperId": "5ff0a7c8329159760b13496486c466d73f9404da",
            "title": "Web prefetching using partial match prediction",
            "abstract": "Web traffic is now one of the major components of Internet traffic, which corresponds t o the explosive growth that this medium is experiencing. One of the main directions of research in this area is t o reduce the time latencies users experience when navigating through Web sites. Caching is already being used in that direction, yet, the characteristics of the Web cause caching in this medium to have poor performance. Therefore, peletching is now being studied in the Web context. When prefetching is employed, Web pages that the user is likely to access in the near future are transfered without being requested. Thus, when prefetching is appropriately employed, the performance of the cache can be significantly improved, which will in turn have a direct impact on the delays perceived by the users. This study investigates the use of partial match prediction, a technique taken from the d a t a compression literature, for prefetching in the Web. The main concern when employing prefetching is t o predict as many future requests as possible, while limiting the false predictions to a minimum. The simulation results suggest that a high fraction of the predictions are accurate, so tha t additional network traffic is kept low. Furthermore, the simulations show that prefetching can substantially increase cache hi t rates.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                }
            ]
        }
    ]
}