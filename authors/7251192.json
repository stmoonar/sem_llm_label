{
    "authorId": "7251192",
    "papers": [
        {
            "paperId": "5b03ba1287288498f465150d4024350bbb0a0842",
            "title": "A Method for Completing Missing 3D Point Cloud Reconstructed from Aerial Multi-View Images Using Self-Attention Mechanism",
            "abstract": "This paper proposes a method to complete the missing 3D point cloud reconstructed from aerial multi-view images by using a deep learning method with self-attention. The advancement of drone technology has made it easier to acquire aerial multi-view images. While it is possible to generate 3D point clouds of the terrain by applying 3D photogrammetric techniques to these images, when capturing multi-view aerial images with a drone, high-altitude vertical shooting is often necessary for privacy protection. For example, some portions of the generated 3D point clouds are lost due to shadowed areas caused by roofs and eaves. To address this issue, this research proposes a method to complete the missing 3D point cloud by using a deep learning. In order to obtain accurate and sufficient amount of training data, 3D CG building models are used for generating sets of missing 3D point cloud data and their corresponding Ground Truth. In the experiment, we applied our method to a 3D point cloud generated from actual captured aerial multi-view images and confirmed that the point cloud with a reasonable shape for the missing parts are successfully completed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2142741020",
                    "name": "Takenobu Kiyama"
                },
                {
                    "authorId": "143975466",
                    "name": "Chun Xie"
                },
                {
                    "authorId": "2438122",
                    "name": "Hidehiko Shishido"
                },
                {
                    "authorId": "1906003",
                    "name": "H. Toriya"
                },
                {
                    "authorId": "7251192",
                    "name": "I. Kitahara"
                }
            ]
        },
        {
            "paperId": "6d67fc49715ffc68ecd655eb66f26e021a24d1e1",
            "title": "Detection of Shot Information Using Footwork Trajectory and Skeletal Information of Badminton Players",
            "abstract": ": As video analysis has become important for sports science, various research has been conducted. In badminton, while shot information is essential primary data for performance analysis, it has been input manually, which makes it difficult to give instant feedback onsite. Our research aims to automatically detect shot information from videos of badminton game. By applying video tracking, the player's footwork trajectory and skeletal information are estimated. Based on the estimated information, the hit timing is detected using deep learning classification. The horizontal position of the hit point, which is useful for game analysis, is also detected from the player's footwork trajectory around the hit timing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2201972212",
                    "name": "Naoki Tanaka"
                },
                {
                    "authorId": "2438122",
                    "name": "Hidehiko Shishido"
                },
                {
                    "authorId": "114085128",
                    "name": "Masashi Suita"
                },
                {
                    "authorId": "2202018873",
                    "name": "Takeshi Nishijima"
                },
                {
                    "authorId": "1679232",
                    "name": "Y. Kameda"
                },
                {
                    "authorId": "7251192",
                    "name": "I. Kitahara"
                }
            ]
        },
        {
            "paperId": "a1cfaecc04d42365783929493fad7d2886b8c284",
            "title": "Reaction Time Estimation Based on Recursive Short-Term Principal Component Analysis for Skeletal Information of Badminton Players",
            "abstract": ": The aim of this paper is to measure the shot-reaction intervals of badminton players based on time-series 3D skeletal information. In competitions where game dominance changes, effective plays and tactics in situations can be investigated by analyzing the measured reaction intervals. In our proposed method, we estimated shot-reaction intervals using a badminton player\u2019s motion information and applied a short-term principal component analysis to the sequential 3D skeletal information of athletes to extract features useful for motion analysis. Hit and reaction times were detected by identifying the extrema in the first and second principal component scores. We estimated a shot\u2019s reaction interval from the hit time to the reaction time at which the player starts moving in response. We applied the proposed method to the 3D skeletal information of a badminton player and confirmed that reaction intervals can be estimated. By using the results of this study to provide feedback to badminton players on the analysis of reaction intervals, players can learn and improve their effective and ineffective tactics and plays.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267814152",
                    "name": "Kana Sagawa"
                },
                {
                    "authorId": "2438122",
                    "name": "Hidehiko Shishido"
                },
                {
                    "authorId": "114085128",
                    "name": "Masashi Suita"
                },
                {
                    "authorId": "7251192",
                    "name": "I. Kitahara"
                }
            ]
        },
        {
            "paperId": "d62c0e8d57767dd12a3c8e939c93b83fb0f5fe37",
            "title": "Vehicle Localization in a Completed City-Scale 3D Scene Using Aerial Images and an On-Board Stereo Camera",
            "abstract": "Simultaneous Localization and Mapping (SLAM) forms the foundation of vehicle localization in autonomous driving. Utilizing high-precision 3D scene maps as prior information in vehicle localization greatly assists in the navigation of autonomous vehicles within large-scale 3D scene models. However, generating high-precision maps is complex and costly, posing challenges to commercialization. As a result, a global localization system that employs low-precision, city-scale 3D scene maps reconstructed by unmanned aerial vehicles (UAVs) is proposed to optimize visual positioning for vehicles. To address the discrepancies in image information caused by differing aerial and ground perspectives, this paper introduces a wall complementarity algorithm based on the geometric structure of buildings to refine the city-scale 3D scene. A 3D-to-3D feature registration algorithm is developed to determine vehicle location by integrating the optimized city-scale 3D scene with the local scene generated by an onboard stereo camera. Through simulation experiments conducted in a computer graphics (CG) simulator, the results indicate that utilizing a completed low-precision scene model enables achieving a vehicle localization accuracy with an average error of 3.91 m, which is close to the 3.27 m error obtained using the high-precision map. This validates the effectiveness of the proposed algorithm. The system demonstrates the feasibility of utilizing low-precision city-scale 3D scene maps generated by unmanned aerial vehicles (UAVs) for vehicle localization in large-scale scenes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108912574",
                    "name": "Haihan Zhang"
                },
                {
                    "authorId": "143975466",
                    "name": "Chun Xie"
                },
                {
                    "authorId": "1906003",
                    "name": "H. Toriya"
                },
                {
                    "authorId": "2438122",
                    "name": "Hidehiko Shishido"
                },
                {
                    "authorId": "7251192",
                    "name": "I. Kitahara"
                }
            ]
        },
        {
            "paperId": "efb4bb65663daf96ee40025f6f4517988ff54223",
            "title": "Camera Motion Generation Method Based on Performer's Position for Performance Filming",
            "abstract": "The role of camera techniques in video quality, a crucial component of visual expression, cannot be overstated. Unlike film or drama productions, live performances offer limited opportunities for reshoots, underscoring the need for meticulous preplanning of camera movements. The proposed method leverages deep neural networks to learn and replicate the camera's positions and postures in response to performers' placements and orientations on stage, thereby mimicking the tacit knowledge of professional camerawork. The method unfolds in two phases: Initially, a network is used to determine camera placements and postures based on the performers' positions and orientations as indicated in the stage script. Subsequently, a second network generates the live camera movements during the performance, factoring in both the performers' placements and orientations and the preliminary camera placements and postures determined by the first network. The architecture of the network incorporates a Transformer that infuses a relative position representation into the input data, proving its ability to more accurately learn camera motion features compared to the standard Transformer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143975466",
                    "name": "Chun Xie"
                },
                {
                    "authorId": "2267048766",
                    "name": "Isao Hemmi"
                },
                {
                    "authorId": "2438122",
                    "name": "Hidehiko Shishido"
                },
                {
                    "authorId": "7251192",
                    "name": "I. Kitahara"
                }
            ]
        },
        {
            "paperId": "03393134a0708bba5833ec2161b6b7586f60c40c",
            "title": "OmniVoxel: A Fast and Precise Reconstruction Method of Omnidirectional Neural Radiance Field",
            "abstract": "This paper proposes a method to reconstruct the neural radiance field with equirectangular omnidirectional images. Implicit neural scene representation with a radiance field can reconstruct the 3D shape of a scene continuously within a limited spatial area. However, training a fully implicit representation on commercial PC hardware requires a lot of time and computing resources (15~20 hours per scene). Therefore, we propose a method to accelerate this process significantly (20~40 minutes per scene). Instead of using a fully implicit representation of rays for radiance field reconstruction, we adopt feature voxels that contain density and color features in tensors. Considering omnidirectional equirectangular input and the camera layout, we use spherical voxelization for representation instead of cubic representation. Our voxelization method could balance the reconstruction quality of the inner scene and outer scene. In addition, we adopt the axis-aligned positional encoding method on the color features to increase the total image quality. Our method achieves satisfying empirical performance on synthetic datasets with random camera poses. Moreover, we test our method with real scenes which contain complex geometries and also achieve state-of-the-art performance. Our code and complete dataset will be released at the same time as the paper publication.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108511359",
                    "name": "Qiao Li"
                },
                {
                    "authorId": "2162978289",
                    "name": "Itsuki Ueda"
                },
                {
                    "authorId": "143975466",
                    "name": "Chun Xie"
                },
                {
                    "authorId": "2438122",
                    "name": "Hidehiko Shishido"
                },
                {
                    "authorId": "7251192",
                    "name": "I. Kitahara"
                }
            ]
        },
        {
            "paperId": "045624006b70c83c0e15272373f314cbd5773078",
            "title": "Neural Density-Distance Fields",
            "abstract": "The success of neural fields for 3D vision tasks is now indisputable. Following this trend, several methods aiming for visual localization (e.g., SLAM) have been proposed to estimate distance or density fields using neural fields. However, it is difficult to achieve high localization performance by only density fields-based methods such as Neural Radiance Field (NeRF) since they do not provide density gradient in most empty regions. On the other hand, distance field-based methods such as Neural Implicit Surface (NeuS) have limitations in objects' surface shapes. This paper proposes Neural Density-Distance Field (NeDDF), a novel 3D representation that reciprocally constrains the distance and density fields. We extend distance field formulation to shapes with no explicit boundary surface, such as fur or smoke, which enable explicit conversion from distance field to density field. Consistent distance and density fields realized by explicit conversion enable both robustness to initial values and high-quality registration. Furthermore, the consistency between fields allows fast convergence from sparse point clouds. Experiments show that NeDDF can achieve high localization performance while providing comparable results to NeRF on novel view synthesis. The code is available at https://github.com/ueda0319/neddf.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2162978289",
                    "name": "Itsuki Ueda"
                },
                {
                    "authorId": "32772871",
                    "name": "Yoshihiro Fukuhara"
                },
                {
                    "authorId": "1730200",
                    "name": "Hirokatsu Kataoka"
                },
                {
                    "authorId": "134044998",
                    "name": "Hiroaki Aizawa"
                },
                {
                    "authorId": "2438122",
                    "name": "Hidehiko Shishido"
                },
                {
                    "authorId": "7251192",
                    "name": "I. Kitahara"
                }
            ]
        },
        {
            "paperId": "132b8c5deb9e86ac6ad45a55a8d569abfac4bb49",
            "title": "Omnidirectional Neural Radiance Field for Immersive Experience",
            "abstract": "This paper proposes a method using only RGB information from multiple captured panoramas to provide an immersive observing experience for real scenes. We generated an omnidirectional neural radiance field by adopting the Fibonacci sphere model for sampling rays and several optimized positional encoding approaches. We tested our method on synthetic and real scenes and achieved satisfying empirical performance. Our result makes the immersive continuous free-viewpoint experience possible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2163078476",
                    "name": "Qiaoge Li"
                },
                {
                    "authorId": "2162978289",
                    "name": "Itsuki Ueda"
                },
                {
                    "authorId": "143975466",
                    "name": "Chun Xie"
                },
                {
                    "authorId": "2438122",
                    "name": "Hidehiko Shishido"
                },
                {
                    "authorId": "7251192",
                    "name": "I. Kitahara"
                }
            ]
        },
        {
            "paperId": "32ed9e7aae0f679ae6111d5c926bfd5bf1400167",
            "title": "Precise Gymnastic Scoring from TV Playback",
            "abstract": "This paper proposes a method to rate the quality and completion of movements through the television playback of gymnastic competitions. For Action Quality Assessment (AQA) of gymnastics competitions, the position, height, and angle of the athlete\u2019s joints and bones relative to the ground are generally extracted and evaluated by professional referees. This method requires sophisticated movement tracking devices and special labs. Therefore, this paper propose a system that accurately assesses difficulty, execution, and penalty scores directly from TV playback in gymnastics competitions. We designed multiple models and loss functions based on the envisioned task and verified their effectiveness in score evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2163078476",
                    "name": "Qiaoge Li"
                },
                {
                    "authorId": "2201646124",
                    "name": "Zhenghang Cui"
                },
                {
                    "authorId": "7251192",
                    "name": "I. Kitahara"
                },
                {
                    "authorId": "1706605",
                    "name": "R. Sagawa"
                }
            ]
        },
        {
            "paperId": "4b343336cfe307d7214b5fe487ad8edec6c5b8e8",
            "title": "Transparent Ink Wash Style for Free-Viewpoint Video Generation",
            "abstract": "We propose a framework that combines free-viewpoint video generation based on Neural Radiance Field (NeRF) with 2D ink-wash style transfer. In this work, we focus on 1) tackling the inconsistency issue caused by image style transfer and 2) synthesizing novel as well as stylized views of arbitrary given objects using NeRF, and 3) adding the transparent effect to ensure generated video looks more vivid.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2198912927",
                    "name": "Zhizheng Xiang"
                },
                {
                    "authorId": "2438122",
                    "name": "Hidehiko Shishido"
                },
                {
                    "authorId": "7251192",
                    "name": "I. Kitahara"
                }
            ]
        }
    ]
}