{
    "authorId": "2293820617",
    "papers": [
        {
            "paperId": "125ab16dd7de95aa897230e222ef354d6637e7ef",
            "title": "Bridging Text Data and Graph Data: Towards Semantics and Structure-aware Knowledge Discovery",
            "abstract": "Graphs and texts are two key modalities in data mining. In many cases, the data presents a mixture of the two modalities and the information is often complementary: in e-commerce data, the product-user graph and product descriptions capture different aspects of product features; in scientific literature, the citation graph, author metadata, and the paper content all contribute to modeling the paper impact.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "2293820617",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "2293818809",
                    "name": "Sha Li"
                },
                {
                    "authorId": "2257136881",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "3dd5ad34012164c4ec9c571a12cc6a7561683dea",
            "title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
            "abstract": "In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one or two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this end, we comprehensively survey over 260 scientific LLMs, discuss their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality. Moreover, we investigate how LLMs have been deployed to benefit scientific discovery. Resources related to this survey are available at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293820617",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "29963551",
                    "name": "Xiusi Chen"
                },
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "2307033488",
                    "name": "Sheng Wang"
                },
                {
                    "authorId": "2307565578",
                    "name": "Shuiwang Ji"
                },
                {
                    "authorId": "2283212563",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "2257136881",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "a30bd328bc36a3f75aa18f653919611b1a8ea23d",
            "title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs",
            "abstract": "Large language models (LLMs), while exhibiting exceptional performance, suffer from hallucinations, especially on knowledge-intensive tasks. Existing works propose to augment LLMs with individual text units retrieved from external knowledge corpora to alleviate the issue. However, in many domains, texts are interconnected (e.g., academic papers in a bibliographic graph are linked by citations and co-authorships) which form a (text-attributed) graph. The knowledge in such graphs is encoded not only in single texts/nodes but also in their associated connections. To facilitate the research of augmenting LLMs with graphs, we manually construct a Graph Reasoning Benchmark dataset called GRBench, containing 1,740 questions that can be answered with the knowledge from 10 domain graphs. Then, we propose a simple and effective framework called Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging LLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of three sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We conduct systematic experiments with three LLM backbones on GRBench, where Graph-CoT outperforms the baselines consistently. The code is available at https://github.com/PeterGriffinJin/Graph-CoT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "150961077",
                    "name": "Chulin Xie"
                },
                {
                    "authorId": "2295927976",
                    "name": "Jiawei Zhang"
                },
                {
                    "authorId": "2061977359",
                    "name": "Kashob Kumar Roy"
                },
                {
                    "authorId": "2293820617",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "2286977473",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "2257136881",
                    "name": "Jiawei Han"
                }
            ]
        }
    ]
}