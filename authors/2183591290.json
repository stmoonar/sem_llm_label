{
    "authorId": "2183591290",
    "papers": [
        {
            "paperId": "4301175198235888abfff048d5a430c832d9403b",
            "title": "Unifying Visual and Semantic Feature Spaces with Diffusion Models for Enhanced Cross-Modal Alignment",
            "abstract": "Image classification models often demonstrate unstable performance in real-world applications due to variations in image information, driven by differing visual perspectives of subject objects and lighting discrepancies. To mitigate these challenges, existing studies commonly incorporate additional modal information matching the visual data to regularize the model's learning process, enabling the extraction of high-quality visual features from complex image regions. Specifically, in the realm of multimodal learning, cross-modal alignment is recognized as an effective strategy, harmonizing different modal information by learning a domain-consistent latent feature space for visual and semantic features. However, this approach may face limitations due to the heterogeneity between multimodal information, such as differences in feature distribution and structure. To address this issue, we introduce a Multimodal Alignment and Reconstruction Network (MARNet), designed to enhance the model's resistance to visual noise. Importantly, MARNet includes a cross-modal diffusion reconstruction module for smoothly and stably blending information across different domains. Experiments conducted on two benchmark datasets, Vireo-Food172 and Ingredient-101, demonstrate that MARNet effectively improves the quality of image information extracted by the model. It is a plug-and-play framework that can be rapidly integrated into various image classification frameworks, boosting model performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2183591290",
                    "name": "Yuze Zheng"
                },
                {
                    "authorId": "2313697725",
                    "name": "Zixuan Li"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2313586899",
                    "name": "Jinxing Liu"
                },
                {
                    "authorId": "2226670159",
                    "name": "Yuqing Wang"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2240864213",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "385dd16d1859525e14020987e451acce1e5511d6",
            "title": "Cross-Modal Content Inference and Feature Enrichment for Cold-Start Recommendation",
            "abstract": "Multimedia recommendation aims to fuse the multi-modal information of items for feature enrichment to improve the recommendation performance. However, existing methods typically introduce multi-modal information based on collaborative information to improve the overall recommendation precision, while failing to explore its cold-start recommendation performance. Meanwhile, these above methods are only applicable when such multi-modal data is available. To address this problem, this paper proposes a recommendation framework, named Cross-modal Content Inference and Feature Enrichment Recommendation (CIERec), which exploits the multi-modal information to improve its cold-start recommendation performance. Specifically, CIERec first introduces image annotation as the privileged information to help guide the mapping of unified features from the visual space to the semantic space in the training phase. And then CIERec enriches the content representation with the fusion of collaborative, visual, and cross-modal inferred representations, so as to improve its cold-start recommendation performance. Experimental results on two real-world datasets show that the content representations learned by CIERec are able to achieve superior cold-start recommendation performance over existing visually-aware recommendation algorithms. More importantly, CIERec can consistently achieve significant improvements with different conventional visually-aware backbones, which verifies its universality and effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152191425",
                    "name": "Haokai Ma"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2143918656",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2183591290",
                    "name": "Yuze Zheng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "bd2bb353afad3fa5332354a9d2694e788520b221",
            "title": "Improving the Generalization of Visual Classification Models Across IoT Cameras via Cross-Modal Inference and Fusion",
            "abstract": "The performance of visual classification models across Internet of Things devices is usually limited by the changes in local environments, resulted from the diverse appearances of the target objects and differences in light conditions and background scenes. To alleviate these problems, existing studies usually introduce the multimodal information to guide the learning process of the visual classification models, making the models extract the visual features from the discriminative image regions. Especially, cross-modal alignment between visual and textual features has been considered as an effective way for this task by learning a domain-consistent latent feature space for the visual and semantic features. However, this approach may suffer from the heterogeneity between multiple modalities, such as the multimodal features and the differences in the learned feature values. To alleviate this problem, this article first presents a comparative analysis of the functionality of various alignment strategies and their impacts on improving visual classification. Subsequently, a cross-modal inference and fusion framework (termed as CRIF) is proposed to align the heterogeneous features in both the feature distributions and values. More importantly, CRIF includes a cross-modal information enrichment module to improve the final classification and learn the mappings from the visual to the semantic space. We conduct experiments on four benchmarking data sets, i.e., the Vireo-Food172, NUS-WIDE, MSR-VTT, and ActivityNet Captions data sets. We report state-of-the-art results for basic classification tasks on the four data sets and conduct subsequent experiments on feature alignment and fusion. The experimental results verify that CRIF can effectively improve the learning ability of the visual classification models, and it is a model-agnostic framework that consistently improves the performance of state-of-the-art visual classification models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2213712365",
                    "name": "Qing-Ling Guan"
                },
                {
                    "authorId": "2183591290",
                    "name": "Yuze Zheng"
                },
                {
                    "authorId": "46643379",
                    "name": "L. Meng"
                },
                {
                    "authorId": "2213666843",
                    "name": "Liquan Dong"
                },
                {
                    "authorId": "144612872",
                    "name": "Q. Hao"
                }
            ]
        },
        {
            "paperId": "b02d4af3960330d206c75186cc849b61b4f93781",
            "title": "A Visually-Aware Food Analysis System for Diet Management",
            "abstract": "This demo illustrates a visually-aware food analysis (VAFA) system for socially-engaged diet management. VAFA is able to receive multimedia inputs, such as the images of food with/without a description to record a user\u2019s daily diet. Such information will be passed to AI algorithms for food classification, ingredient recognition, and nutrition analysis, to produce a nutrition report for the user. Moreover, VAFA profiles the users\u2019 eating habits to make personalized recipe recommendation and identify the social communities with similar eating preferences. VAFA is empowered by state-of-the-art AI algorithms and a large-scale dataset with 300K users, 400K recipes, and over 10M user-recipe interactions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2183601740",
                    "name": "Hang Wu"
                },
                {
                    "authorId": "2145307865",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "2184029053",
                    "name": "Xuelong Li"
                },
                {
                    "authorId": "2152191425",
                    "name": "Haokai Ma"
                },
                {
                    "authorId": "2183591290",
                    "name": "Yuze Zheng"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        }
    ]
}