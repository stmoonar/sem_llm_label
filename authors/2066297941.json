{
    "authorId": "2066297941",
    "papers": [
        {
            "paperId": "08c0353aa13f2ee410dad2accf671f02791b9e47",
            "title": "A Simple and Practical Method for Reducing the Disparate Impact of Differential Privacy",
            "abstract": "Differentially private (DP) mechanisms have been deployed in a variety of high-impact social settings (perhaps most notably by the U.S. Census). Since all DP mechanisms involve adding noise to results of statistical queries, they are expected to impact our ability to accurately analyze and learn from data, in effect trading off privacy with utility. Alarmingly, the impact of DP on utility can vary significantly among different sub-populations. A simple way to reduce this disparity is with stratification. First compute an independent private estimate for each group in the data set (which may be the intersection of several protected classes), then, to compute estimates of global statistics, appropriately recombine these group estimates. Our main observation is that naive stratification often yields high-accuracy estimates of population-level statistics, without the need for additional privacy budget. We support this observation theoretically and empirically. Our theoretical results center on the private mean estimation problem, while our empirical results center on extensive experiments on private data synthesis to demonstrate the effectiveness of stratification on a variety of private mechanisms. Overall, we argue that this straightforward approach provides a strong baseline against which future work on reducing utility disparities of DP mechanisms should be compared.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066297941",
                    "name": "Lucas Rosenblatt"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "2275240652",
                    "name": "Christopher Musco"
                }
            ]
        },
        {
            "paperId": "1af6cefb83c92b8bd99e86a0aaaddc70f31c2524",
            "title": "I Open at the Close: A Deep Reinforcement Learning Evaluation of Open Streets Initiatives",
            "abstract": "The open streets initiative \"opens\" streets to pedestrians and bicyclists by closing them to cars and trucks. The initiative, adopted by many cities across North America, increases community space in urban environments. But could open streets also make cities safer and less congested? We study this question by framing the choice of which streets to open as a reinforcement learning problem. In order to simulate the impact of opening streets, we first compare models for predicting vehicle collisions given network and temporal data. We find that a recurrent graph neural network, leveraging the graph structure and the short-term temporal dependence of the data, gives the best predictive performance. Then, with the ability to simulate collisions and traffic, we frame a reinforcement learning problem to find which streets to open. We compare the streets in the open streets initiative to those proposed by a Q-learning algorithm. We find that the streets proposed by the Q-learning algorithm have reliably better outcomes, while streets already selected by the open streets initiative have similar outcomes to randomly selected streets. We present our work as a step toward principally choosing which streets to open for safer and less congested cities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2062796571",
                    "name": "R. T. Witter"
                },
                {
                    "authorId": "2066297941",
                    "name": "Lucas Rosenblatt"
                }
            ]
        },
        {
            "paperId": "692d0b221057485908c2524d6ebc96763daa7fd6",
            "title": "The Possibility of Fairness: Revisiting the Impossibility Theorem in Practice",
            "abstract": "The \u201cimpossibility theorem\u201d \u2014 which is considered foundational in algorithmic fairness literature \u2014 asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a practitioner\u2019s perspective of fairness), it becomes possible to identify abundant sets of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when \u2014 and to what degree \u2014 fairness along multiple criteria can be achieved. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067721671",
                    "name": "A. Bell"
                },
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "2205540262",
                    "name": "Nazarii Drushchak"
                },
                {
                    "authorId": "2205547383",
                    "name": "Tetiana Herasymova"
                },
                {
                    "authorId": "2066297941",
                    "name": "Lucas Rosenblatt"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "b4d9cba448229a11fb07a9e7d1451015a4ec6612",
            "title": "Critical Perspectives: A Benchmark Revealing Pitfalls in PerspectiveAPI",
            "abstract": "Detecting \u201ctoxic\u201d language in internet content is a pressing social and technical challenge. In this work, we focus on Perspective API from Jigsaw, a state-of-the-art tool that promises to score the \u201ctoxicity\u201d of text, with a recent model update that claims impressive results (Lees et al., 2022). We seek to challenge certain normative claims about toxic language by proposing a new benchmark, Selected Adversarial SemanticS, or SASS. We evaluate Perspective on SASS, and compare to low-effort alternatives, like zero-shot and few-shot GPT-3 prompt models, in binary classification settings. We find that Perspective exhibits troubling shortcomings across a number of our toxicity categories. SASS provides a new tool for evaluating performance on previously undetected toxic language that avoids common normative pitfalls. Our work leads us to emphasize the importance of questioning assumptions made by tools already in deployment for toxicity detection in order to anticipate and prevent disparate harms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2199317163",
                    "name": "Lorena Piedras"
                },
                {
                    "authorId": "2066297941",
                    "name": "Lucas Rosenblatt"
                },
                {
                    "authorId": "38458166",
                    "name": "Julia Wilkins"
                }
            ]
        },
        {
            "paperId": "45c372f16c077fd47fcad2a67d0e6c6efdfe0bbd",
            "title": "Spending Privacy Budget Fairly and Wisely",
            "abstract": "Differentially private (DP) synthetic data generation is a practical method for improving access to data as a means to encourage productive partner-ships. One issue inherent to DP is that the \u201cprivacy budget\u201d is generally \u201cspent\u201d evenly across features in the data set. This leads to good statistical parity with the real data, but can undervalue the conditional probabilities and marginals that are critical for predictive quality of synthetic data. Further, loss of predictive quality may be non-uniform across the data set, with subsets that correspond to minority groups potentially suffering a higher loss. In this paper, we develop ensemble methods that distribute the privacy budget \u201cwisely\u201d to maximize predictive accuracy of models trained on DP data, and \u201cfairly\u201d to bound potential disparities in accuracy across groups and reduce inequality. Our methods are based on the insights that feature importance can inform how privacy budget is allocated, and, further, that per-group feature importance and fairness-related performance objectives can be incorporated in the allocation. These insights make our methods tunable to social contexts, allowing data owners to produce balanced synthetic data for predictive analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066297941",
                    "name": "Lucas Rosenblatt"
                },
                {
                    "authorId": "2115224286",
                    "name": "Joshua Allen"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "564c0f5dda77fd8a6a9cd41a0ec88989d92181ad",
            "title": "Counterfactual Fairness Is Basically Demographic Parity",
            "abstract": "Making fair decisions is crucial to ethically implementing machine learning algorithms in social settings. In this work, we consider the celebrated definition of counterfactual fairness. We begin by showing that an algorithm which satisfies counterfactual fairness also satisfies demographic parity, a far simpler fairness constraint. Similarly, we show that all algorithms satisfying demographic parity can be trivially modified to satisfy counterfactual fairness. Together, our results indicate that counterfactual fairness is basically equivalent to demographic parity, which has important implications for the growing body of work on counterfactual fairness. We then validate our theoretical findings empirically, analyzing three existing algorithms for counterfactual fairness against three simple benchmarks. We find that two simple benchmark algorithms outperform all three existing algorithms---in terms of fairness, accuracy, and efficiency---on several data sets. Our analysis leads us to formalize a concrete fairness goal: to preserve the order of individuals within protected groups. We believe transparency around the ordering of individuals within protected groups makes fair algorithms more trustworthy. By design, the two simple benchmark algorithms satisfy this goal while the existing algorithms do not.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066297941",
                    "name": "Lucas Rosenblatt"
                },
                {
                    "authorId": "2062796571",
                    "name": "R. T. Witter"
                }
            ]
        },
        {
            "paperId": "5d467de7406c505bac2aba12c8d29ae861bbe985",
            "title": "Epistemic Parity: Reproducibility as an Evaluation Metric for Differential Privacy",
            "abstract": "Differential privacy (DP) data synthesizers are increasingly proposed to afford public release of sensitive information, offering theoretical guarantees for privacy (and, in some cases, utility), but limited empirical evidence of utility in practical settings. Utility is typically measured as the error on representative proxy tasks, such as descriptive statistics, multivariate correlations, the accuracy of trained classifiers, or performance over a query workload. The ability for these results to generalize to practitioners' experience has been questioned in a number of settings, including the U.S. Census. In this paper, we propose an evaluation methodology for synthetic data that avoids assumptions about the representativeness of proxy tasks, instead measuring the likelihood that published conclusions would change had the authors used synthetic data, a condition we call epistemic parity. Our methodology consists of reproducing empirical conclusions of peer-reviewed papers on real, publicly available data, then re-running these experiments a second time on DP synthetic data and comparing the results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066297941",
                    "name": "Lucas Rosenblatt"
                },
                {
                    "authorId": "2183082363",
                    "name": "Anastasia Holovenko"
                },
                {
                    "authorId": "2141567332",
                    "name": "Taras Rumezhak"
                },
                {
                    "authorId": "2183082384",
                    "name": "Andrii Stadnik"
                },
                {
                    "authorId": "11484361",
                    "name": "Bernease Herman"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "1686294",
                    "name": "Bill Howe"
                }
            ]
        },
        {
            "paperId": "689f70ea07215b58e39dfc624480b2b8e8c5848a",
            "title": "PerfGuard: Deploying ML-for-Systems without Performance Regressions, Almost!",
            "abstract": "\n Modern data processing systems require optimization at massive scale, and using machine learning to optimize these systems (ML-for-systems) has shown promising results. Unfortunately, ML-for-systems is subject to over generalizations that do not capture the large variety of workload patterns, and tend to augment the performance of certain subsets in the workload while regressing performance for others. In this paper, we introduce a performance safeguard system, called\n PerfGuard\n , that designs pre-production experiments for deploying ML-for-systems. Instead of searching the entire space of query plans (a well-known, intractable problem), we focus on query plan deltas (a significantly smaller space). PerfGuard formalizes these differences, and correlates plan deltas to important feedback signals, like execution cost. We describe the deep learning architecture and the end-to-end pipeline in PerfGuard that could be used with general relational databases. We show that this architecture improves on baseline models, and that our pipeline identifies key query plan components as major contributors to plan disparity. Offline experimentation shows PerfGuard as a promising approach, with many opportunities for future improvement.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145189529",
                    "name": "H. M. S. Hossain"
                },
                {
                    "authorId": "143634544",
                    "name": "Marc T. Friedman"
                },
                {
                    "authorId": "1471618633",
                    "name": "Hiren Patel"
                },
                {
                    "authorId": "49830907",
                    "name": "S. Qiao"
                },
                {
                    "authorId": "153309692",
                    "name": "Soundar Srinivasan"
                },
                {
                    "authorId": "2965406",
                    "name": "Markus Weimer"
                },
                {
                    "authorId": "2102463296",
                    "name": "Remmelt Ammerlaan"
                },
                {
                    "authorId": "2066297941",
                    "name": "Lucas Rosenblatt"
                },
                {
                    "authorId": "2083999540",
                    "name": "Gilbert Antonius"
                },
                {
                    "authorId": "98228555",
                    "name": "P. Orenberg"
                },
                {
                    "authorId": "2067082298",
                    "name": "Vijay Ramani"
                },
                {
                    "authorId": "2153931746",
                    "name": "Abhishek Roy"
                },
                {
                    "authorId": "2065950584",
                    "name": "Irene Shaffer"
                },
                {
                    "authorId": "2153832",
                    "name": "Alekh Jindal"
                }
            ]
        },
        {
            "paperId": "7f5f8ff19f6890ced904bd273ed00c1932299261",
            "title": "Differentially Private Synthetic Data: Applied Evaluations and Enhancements",
            "abstract": "Machine learning practitioners frequently seek to leverage the most informative available data, without violating the data owner's privacy, when building predictive models. Differentially private data synthesis protects personal details from exposure, and allows for the training of differentially private machine learning models on privately generated datasets. But how can we effectively assess the efficacy of differentially private synthetic data? In this paper, we survey four differentially private generative adversarial networks for data synthesis. We evaluate each of them at scale on five standard tabular datasets, and in two applied industry scenarios. We benchmark with novel metrics from recent literature and other standard machine learning tools. Our results suggest some synthesizers are more applicable for different privacy budgets, and we further demonstrate complicating domain-based tradeoffs in selecting an approach. We offer experimental learning on applied machine learning scenarios with private internal data to researchers and practioners alike. In addition, we propose QUAIL, an ensemble-based modeling approach to generating synthetic data. We examine QUAIL's tradeoffs, and note circumstances in which it outperforms baseline differentially private supervised learning models under the same budget constraint.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066297941",
                    "name": "Lucas Rosenblatt"
                },
                {
                    "authorId": "2111039012",
                    "name": "Xiao-Yang Liu"
                },
                {
                    "authorId": "1997924",
                    "name": "Samira Pouyanfar"
                },
                {
                    "authorId": "145711762",
                    "name": "Eduardo de Leon"
                },
                {
                    "authorId": "2053097301",
                    "name": "Anuj M. Desai"
                },
                {
                    "authorId": "2115224286",
                    "name": "Joshua Allen"
                }
            ]
        },
        {
            "paperId": "a75ed733c0e83b3375c6fe3f78bbab8533ec10eb",
            "title": "Vocal Programming for People with Upper-Body Motor Impairments",
            "abstract": "Programming heavily relies on entering text using traditional QWERTY keyboards, which poses challenges for people with limited upper-body movement. Developing tools using a publicly available speech recognition API could provide a basis for keyboard free programming. In this paper, we describe our efforts in design, development, and evaluation of a voice-based IDE to support people with limited dexterity. We report on a formative Wizard of Oz (WOz) based design process to gain an understanding of how people would use and what they expect from a speech-based programming environment. Informed by the findings from the WOz, we developed VocalIDE, a prototype speech-based IDE with features such as Context Color Editing that facilitates vocal programming. Finally, we evaluate the utility of VocalIDE with 8 participants who have upper limb motor impairments. The study showed that VocalIDE significantly improves the participants' ability to make navigational edits and select text while programming.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066297941",
                    "name": "Lucas Rosenblatt"
                },
                {
                    "authorId": "144451887",
                    "name": "Patrick Carrington"
                },
                {
                    "authorId": "39496240",
                    "name": "Kotaro Hara"
                },
                {
                    "authorId": "1744846",
                    "name": "Jeffrey P. Bigham"
                }
            ]
        }
    ]
}