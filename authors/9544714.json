{
    "authorId": "9544714",
    "papers": [
        {
            "paperId": "92f82b8752712c12253b64d53b79b4a4833670a8",
            "title": "Tab-Cleaner: Weakly Supervised Tabular Data Cleaning via Pre-training for E-commerce Catalog",
            "abstract": "Product catalogs, conceptually in the form of text-rich tables, are self-reported by individual retailers and thus inevitably contain noisy facts. Verifying such textual attributes in product catalogs is essential to improve their reliability. However, popular methods for processing free-text content, such as pre-trained language models, are not particularly effective on structured tabular data since they are typically trained on free-form natural language texts. In this paper, we present Tab-Cleaner, a model designed to handle error detection over text-rich tabular data following a pre-training / fine-tuning paradigm. We train Tab-Cleaner on a real-world Amazon Product Catalog table w.r.t millions of products and show improvements over state-of-the-art methods by 16\\% on PR AUC over attribute applicability classification task and by 11\\% on PR AUC over attribute value validation task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3161399",
                    "name": "Kewei Cheng"
                },
                {
                    "authorId": "2157096355",
                    "name": "Xian Li"
                },
                {
                    "authorId": "1879297505",
                    "name": "Zhengyang Wang"
                },
                {
                    "authorId": "2047145237",
                    "name": "Chenwei Zhang"
                },
                {
                    "authorId": "9544714",
                    "name": "Binxuan Huang"
                },
                {
                    "authorId": "15574937",
                    "name": "Y. Xu"
                },
                {
                    "authorId": "2143918656",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2109461904",
                    "name": "Yizhou Sun"
                }
            ]
        },
        {
            "paperId": "a0c2052ea02e1916263841db5b9ca3b13e10ccd1",
            "title": "Concept2Box: Joint Geometric Embeddings for Learning Two-View Knowledge Graphs",
            "abstract": "Knowledge graph embeddings (KGE) have been extensively studied to embed large-scale relational data for many real-world applications. Existing methods have long ignored the fact many KGs contain two fundamentally different views: high-level ontology-view concepts and fine-grained instance-view entities. They usually embed all nodes as vectors in one latent space. However, a single geometric representation fails to capture the structural differences between two views and lacks probabilistic semantics towards concepts' granularity. We propose Concept2Box, a novel approach that jointly embeds the two views of a KG using dual geometric representations. We model concepts with box embeddings, which learn the hierarchy structure and complex relations such as overlap and disjoint among them. Box volumes can be interpreted as concepts' granularity. Different from concepts, we model entities as vectors. To bridge the gap between concept box embeddings and entity vector embeddings, we propose a novel vector-to-box distance metric and learn both embeddings jointly. Experiments on both the public DBpedia KG and a newly-created industrial KG showed the effectiveness of Concept2Box.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12318198",
                    "name": "Zijie Huang"
                },
                {
                    "authorId": "2111191142",
                    "name": "Daheng Wang"
                },
                {
                    "authorId": "9544714",
                    "name": "Binxuan Huang"
                },
                {
                    "authorId": "2047145237",
                    "name": "Chenwei Zhang"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "2987847",
                    "name": "Yangkexin Liang"
                },
                {
                    "authorId": "1879297505",
                    "name": "Zhengyang Wang"
                },
                {
                    "authorId": "2157096355",
                    "name": "Xian Li"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "2109461904",
                    "name": "Yizhou Sun"
                },
                {
                    "authorId": "2158624285",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "f806b3ac12502ecf9305f27f77fddbbaf0554bed",
            "title": "Self-Training for Label-Efficient Information Extraction from Semi-Structured Web-Pages",
            "abstract": "\n Information Extraction (IE) from semi-structured web-pages is a long studied problem. Training a model for this extraction task requires a large number of human-labeled samples. Prior works have proposed transferable models to improve the label-efficiency of this training process. Extraction performance of transferable models however, depends on the size of their fine-tuning corpus. This holds true for large language models (LLM) such as GPT-3 as well. Generalist models like LLMs need to be fine-tuned on in-domain, human-labeled samples for competitive performance on this extraction task. Constructing a large-scale fine-tuning corpus with human-labeled samples, however, requires significant effort. In this paper, we develop a\n Label-Efficient Self-Training Algorithm\n (LEAST) to improve the label-efficiency of this fine-tuning process. Our contributions are two-fold.\n First\n , we develop a generative model that facilitates the construction of a large-scale fine-tuning corpus with minimal human-effort.\n Second\n , to ensure that the extraction performance does not suffer due to noisy training samples in our fine-tuning corpus, we develop an uncertainty-aware training strategy. Experiments on two publicly available datasets show that LEAST generalizes to multiple verticals and backbone models. Using LEAST, we can train models with less than ten human-labeled pages from each website, outperforming strong baselines while reducing the number of human-labeled training samples needed for comparable performance by up to 11\n x.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3245784",
                    "name": "Ritesh Sarkhel"
                },
                {
                    "authorId": "9544714",
                    "name": "Binxuan Huang"
                },
                {
                    "authorId": "144182018",
                    "name": "Colin Lockard"
                },
                {
                    "authorId": "3310534",
                    "name": "Prashant Shiralkar"
                }
            ]
        },
        {
            "paperId": "0890fc89baf2e38291097d96b40844ec4114ac55",
            "title": "DOM-LM: Learning Generalizable Representations for HTML Documents",
            "abstract": "HTML documents are an important medium for disseminating information on the Web for human consumption. An HTML document presents information in multiple text formats including unstructured text, structured key-value pairs, and tables. Effective representation of these documents is essential for machine understanding to enable a wide range of applications, such as Question Answering, Web Search, and Personalization. Existing work has either represented these documents using visual features extracted by rendering them in a browser, which is typically computationally expensive, or has simply treated them as plain text documents, thereby failing to capture useful information presented in their HTML structure. We argue that the text and HTML structure together convey important semantics of the content and therefore warrant a special treatment for their representation learning. In this paper, we introduce a novel representation learning approach for web pages, dubbed DOM-LM, which addresses the limitations of existing approaches by encoding both text and DOM tree structure with a transformer-based encoder and learning generalizable representations for HTML documents via self-supervised pre-training. We evaluate DOM-LM on a variety of webpage understanding tasks, including Attribute Extraction, Open Information Extraction, and Question Answering. Our extensive experiments show that DOM-LM consistently outperforms all baselines designed for these tasks. In particular, DOM-LM demonstrates better generalization performance both in few-shot and zero-shot settings, making it attractive for making it suitable for real-world application settings with limited labeled data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145924070",
                    "name": "Xiang Deng"
                },
                {
                    "authorId": "3310534",
                    "name": "Prashant Shiralkar"
                },
                {
                    "authorId": "144182018",
                    "name": "Colin Lockard"
                },
                {
                    "authorId": "9544714",
                    "name": "Binxuan Huang"
                },
                {
                    "authorId": "1515546612",
                    "name": "Huan Sun"
                }
            ]
        },
        {
            "paperId": "eb69d260b5ff444e570fa1b01e04db37dc95f6ae",
            "title": "Label-Efficient Self-Training for Attribute Extraction from Semi-Structured Web Documents",
            "abstract": "Extracting structured information from HTML documents is a long-studied problem with a broad range of applications, including knowledge base construction, faceted search, and personalized recommendation. Prior works rely on a few human-labeled web pages from each target website or thousands of human-labeled web pages from some seed websites to train a transferable extraction model that generalizes on unseen target websites. Noisy content, low site-level consistency, and lack of inter-annotator agreement make labeling web pages a time-consuming and expensive ordeal. We develop LEAST \u2013 a La bel- E fficient S elf- T raining method for Semi-Structured Web Documents to overcome these limitations. LEAST utilizes a few human-labeled pages to pseudo-annotate a large number of unlabeled web pages from the target vertical. It trains a transferable web-extraction model on both human-labeled and pseudo-labeled samples using self-training. To mitigate error propagation due to noisy training samples, LEAST re-weights each training sample based on its estimated label accuracy and incorporates it in training. To the best of our knowledge, this is the first work to propose end-to-end training for transferable web extraction models utilizing only a few human-labeled pages. Experiments on a large-scale public dataset show that using less than ten human-labeled pages from each seed website for training, a LEAST -trained model outperforms previous state-of-the-art by more than 26 average F1 points on unseen websites, reducing the number of human-labeled pages to achieve similar performance by more than 10x.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3245784",
                    "name": "Ritesh Sarkhel"
                },
                {
                    "authorId": "9544714",
                    "name": "Binxuan Huang"
                },
                {
                    "authorId": "144182018",
                    "name": "Colin Lockard"
                },
                {
                    "authorId": "3310534",
                    "name": "Prashant Shiralkar"
                }
            ]
        },
        {
            "paperId": "153a4c7272ebf4883746d2629f98749a857c8018",
            "title": "TCN: Table Convolutional Network for Web Table Interpretation",
            "abstract": "Information extraction from semi-structured webpages provides valuable long-tailed facts for augmenting knowledge graph. Relational Web tables are a critical component containing additional entities and attributes of rich and diverse knowledge. However, extracting knowledge from relational tables is challenging because of sparse contextual information. Existing work linearize table cells and heavily rely on modifying deep language models such as BERT which only captures related cells information in the same table. In this work, we propose a novel relational table representation learning approach considering both the intra- and inter-table contextual information. On one hand, the proposed Table Convolutional Network model employs the attention mechanism to adaptively focus on the most informative intra-table cells of the same row or column; and, on the other hand, it aggregates inter-table contextual information from various types of implicit connections between cells across different tables. Specifically, we propose three novel aggregation modules for (i) cells of the same value, (ii) cells of the same schema position, and (iii) cells linked to the same page topic. We further devise a supervised multi-task training objective for jointly predicting column type and pairwise column relation, as well as a table cell recovery objective for pre-training. Experiments on real Web table datasets demonstrate our method can outperform competitive baselines by of F1 for column type prediction and by of F1 for pairwise column relation prediction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111191142",
                    "name": "Daheng Wang"
                },
                {
                    "authorId": "3310534",
                    "name": "Prashant Shiralkar"
                },
                {
                    "authorId": "144182018",
                    "name": "Colin Lockard"
                },
                {
                    "authorId": "9544714",
                    "name": "Binxuan Huang"
                },
                {
                    "authorId": "2143917898",
                    "name": "Xin Dong"
                },
                {
                    "authorId": "144812586",
                    "name": "Meng Jiang"
                }
            ]
        },
        {
            "paperId": "069913a80594266bf87b79eba5c47d7db9c87627",
            "title": "Entity Linking for Short Text Using Structured Knowledge Graph via Multi-Grained Text Matching",
            "abstract": "Entity Linking (EL) recognizes textual mentions of entities and maps them to the corresponding entities in a Knowledge Graph (KG). In this paper, we propose a novel method for EL on short text using entity representations base on their name labels, descriptions, and other related entities in the KG. We then leverage a pre-trained BERT model to calculate the semantic similarity between the entity and the text. This method does not require a large volume of data to jointly train word and entity representations, and is easily portable to a new domain with a KG. We demonstrate that our approach outperforms previous methods on a public benchmark dataset with a large margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9544714",
                    "name": "Binxuan Huang"
                },
                {
                    "authorId": "40465036",
                    "name": "H. Wang"
                },
                {
                    "authorId": "2113289382",
                    "name": "Han Wang"
                },
                {
                    "authorId": "2116678322",
                    "name": "Tong Wang"
                },
                {
                    "authorId": "100576986",
                    "name": "Yue Liu"
                },
                {
                    "authorId": "2152798037",
                    "name": "Yang Liu"
                }
            ]
        },
        {
            "paperId": "1d07723000f86ba1331cdc9d6e42ec222a7d1bea",
            "title": "Learning User Latent Attributes on Social Media",
            "abstract": "In recent years, there is a growing interest in using social media to understand social phenomena. Researchers have demonstrated many important applications of usingonline social media to understand real world events, such as presidential election prediction, earthquake early detection, and disaster management. A social media site is mixed with different types of users, in terms of gender, location, ideology, and etc. Different types of users may have different motivations, different opinions towards certain topics, different resources at their disposal, different behaviors in events. If researchers want to understand what is happening on a social media site, it is important to know where a post comes from, who wrote this post, and which party the authorbelongs to. However, this information is not explicitly provided by users. In this thesis, the goal is to predict users\u2019 latent attributes such as their locations, social identities, and political orientations. Thanks to the massive text data on social media, we can learn rich knowledge from text to predict users\u2019 attributes. In the meanwhile, text data from social media often comes with a significant amount of metadata. Furthermore, data from social networks also contains rich connection information, eg. mentioning, following. It is still a challenge task to combine text, meta data, usernetwork together for user attributes prediction.In this thesis, I approach user attributes prediction at three levels \u2014 single post, user timeline, graph-level classification. I start with a global location prediction system that uses one single tweet as input to learn one user\u2019s location. It utilizeslocation-related features in a tweet, such as text and user profile metadata. I extend the tweet-level prediction system to user-level, which combines multiple posts in oneuser\u2019s timeline. I demonstrate the effectiveness of this model on the task of user social identity classification. An improved user-level hierarchical location prediction model is also presented. In these described models, I mainly focus on learning user attributes from users themselves. In the next step, I consider social graph as additional information to improve performance. Users connected in a social network often show similarities in certain aspects, which is a well-known phenomenon called social homophily. Experiments demonstrate that combining a social graph dramatically improves the performance of our prediction system compared to the previous user-level method. As a case study of the attributes prediction system, I apply the method on a real world emergency event \u2014 the novel coronavirus outbreak starting from 2019. I demonstrate that we gain better understanding of the public conversation during thisglobal emergency event.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9544714",
                    "name": "Binxuan Huang"
                }
            ]
        },
        {
            "paperId": "c92152f1352d6f56d785d5c2360278efee0f8418",
            "title": "Disinformation and Misinformation on Twitter during the Novel Coronavirus Outbreak",
            "abstract": "As the novel coronavirus spread globally, a growing public panic was expressed over the internet. We examine the public discussion concerning COVID-19 on Twitter. We use a dataset of 67 million tweets from 12 million users collected between January 29, 2020 and March 4, 2020. We categorize users based on their home countries, social identities, and political orientation. We find that news media, government officials, and individual news reporters posted a majority of influential tweets, while the most influential ones are still written by regular users. Tweets mentioning \"fake news\" URLs and disinformation story-lines are also more likely to be spread by regular users. Unlike real news and normal tweets, tweets containing URLs pointing to \"fake news\" sites are most likely to be retweeted within the source country and so are less likely to spread internationally.",
            "fieldsOfStudy": [
                "Computer Science",
                "Political Science"
            ],
            "authors": [
                {
                    "authorId": "9544714",
                    "name": "Binxuan Huang"
                },
                {
                    "authorId": "1702030",
                    "name": "Kathleen M. Carley"
                }
            ]
        },
        {
            "paperId": "4975a63dfa74b25de1f7b8ce1d16f56a7f4c811c",
            "title": "A Hierarchical Location Prediction Neural Network for Twitter User Geolocation",
            "abstract": "Accurate estimation of user location is important for many online services. Previous neural network based methods largely ignore the hierarchical structure among locations. In this paper, we propose a hierarchical location prediction neural network for Twitter user geolocation. Our model first predicts the home country for a user, then uses the country result to guide the city-level prediction. In addition, we employ a character-aware word embedding layer to overcome the noisy information in tweets. With the feature fusion layer, our model can accommodate various feature combinations and achieves state-of-the-art results over three commonly used benchmarks under different feature settings. It not only improves the prediction accuracy but also greatly reduces the mean error distance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9544714",
                    "name": "Binxuan Huang"
                },
                {
                    "authorId": "1702030",
                    "name": "Kathleen M. Carley"
                }
            ]
        }
    ]
}