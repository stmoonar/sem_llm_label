{
    "authorId": "2237783897",
    "papers": [
        {
            "paperId": "08b877e527f8cc55e6527fa7a1b1f7265783a72c",
            "title": "MAINDZ at SemEval-2024 Task 5: CLUEDO - Choosing Legal oUtcome by Explaining Decision through Oversight",
            "abstract": "Large language models (LLMs) have recently obtained strong performance on complex reasoning tasks. However, their capabilities in specialized domains like law remain relatively unexplored. We present CLUEDO, a system to tackle a novel legal reasoning task that involves determining if a provided answer correctly addresses a legal question derived from U.S. civil procedure cases. CLUEDO utilizes multiple collaborator models that are trained using multiple-choice prompting to choose the right label and generate explanations. These collaborators are overseen by a final \u201cdetective\u201d model that identifies the most accurate answer in a zero-shot manner. Our approach achieves an F1 macro score of 0.74 on the development set and 0.76 on the test set, outperforming individual models. Unlike the powerful GPT-4, CLUEDO provides more stable predictions thanks to the ensemble approach. Our results showcase the promise of tailored frameworks to enhance legal reasoning capabilities in LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2181131869",
                    "name": "Irene Benedetto"
                },
                {
                    "authorId": "1909140421",
                    "name": "Alkis Koudounas"
                },
                {
                    "authorId": "30079913",
                    "name": "Lorenzo Vaiani"
                },
                {
                    "authorId": "51209710",
                    "name": "Eliana Pastor"
                },
                {
                    "authorId": "2237783897",
                    "name": "Luca Cagliero"
                },
                {
                    "authorId": "29829627",
                    "name": "Francesco Tarasconi"
                }
            ]
        },
        {
            "paperId": "0f54fab8f05f8bea99ac5edb3572717fa591715f",
            "title": "On Leveraging Multi-Page Element Relations in Visually-Rich Documents",
            "abstract": "Thanks to the rapid progress of the digitalization process, Visually-Rich Documents (VRDs) such as PDF files or scanned documents have become among the most widespread sources of knowledge. However, Question Answering on VRDs is challenged by the presence of multi-page relationships between document elements such as tables, figures, sections. This paper addresses a specific Visual Question Answering subtask from VDRs where answer generation leverages pairwise element relations in multi-page documents. We explore the performance of text-only and multimodal Transformer-based architectures as well as open-source Large Language Models. The results show that multimodal Transformers outperform the other tested methods, particularly when training samples contain explicit textual references to the elements in the document layout.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237785478",
                    "name": "Davide Napolitano"
                },
                {
                    "authorId": "30079913",
                    "name": "Lorenzo Vaiani"
                },
                {
                    "authorId": "2237783897",
                    "name": "Luca Cagliero"
                }
            ]
        },
        {
            "paperId": "21cf7504bd8d9086af220939c2526c1e65b65135",
            "title": "Beyond Accuracy Optimization: Computer Vision Losses for Large Language Model Fine-Tuning",
            "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across various tasks. However, current training approaches combine standard cross-entropy loss with extensive data, human feedback, or ad hoc methods to enhance performance. These solutions are often not scalable or feasible due to their associated costs, complexity, or resource requirements. This study investigates the use of established semantic segmentation loss functions in natural language generation to create a versatile, practical, and scalable solution for fine-tuning different architectures. We evaluate their effectiveness in solving Math Word Problems and question answering across different models of varying sizes. For the analyzed tasks, we found that the traditional Cross-Entropy loss represents a sub-optimal choice, while models trained to minimize alternative (task-dependent) losses, such as Focal or Lov\\'asz, achieve a mean improvement of +42% on exact match without requiring additional data or human feedback. These findings suggest a promising pathway for more efficient and accessible training processes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2205886540",
                    "name": "Daniele Rege Cambrin"
                },
                {
                    "authorId": "2237787732",
                    "name": "Giuseppe Gallipoli"
                },
                {
                    "authorId": "2181131869",
                    "name": "Irene Benedetto"
                },
                {
                    "authorId": "2237783897",
                    "name": "Luca Cagliero"
                },
                {
                    "authorId": "2266504755",
                    "name": "Paolo Garza"
                }
            ]
        },
        {
            "paperId": "3a55568a1d1a7e37faad1ca4e259a98e3f97027f",
            "title": "3MVRD: Multimodal Multi-task Multi-teacher Visually-Rich Form Document Understanding",
            "abstract": "This paper presents a groundbreaking multimodal, multi-task, multi-teacher joint-grained knowledge distillation model for visually-rich form document understanding. The model is designed to leverage insights from both fine-grained and coarse-grained levels by facilitating a nuanced correlation between token and entity representations, addressing the complexities inherent in form documents. Additionally, we introduce new inter-grained and cross-grained loss functions to further refine diverse multi-teacher knowledge distillation transfer process, presenting distribution gaps and a harmonised understanding of form documents. Through a comprehensive evaluation across publicly available form document understanding datasets, our proposed model consistently outperforms existing baselines, showcasing its efficacy in handling the intricate structures and content of visually complex form documents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2166956722",
                    "name": "Yihao Ding"
                },
                {
                    "authorId": "30079913",
                    "name": "Lorenzo Vaiani"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "2282598754",
                    "name": "Jean Lee"
                },
                {
                    "authorId": "1712027",
                    "name": "P. Garza"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                },
                {
                    "authorId": "2237783897",
                    "name": "Luca Cagliero"
                }
            ]
        },
        {
            "paperId": "5980dc1bc1912daa81c1b792fbb1792ee00279c4",
            "title": "ChatGPT, be my Teaching Assistant! Automatic Correction of SQL Exercises",
            "abstract": "The use of Large Language Models (LLMs) such as OpenAI ChatGPT to enhance teachers' and learners' experience has become established. The impressive capabilities of ChatGPT in solving Text2SQL problems prompts their use in database courses to solve SQL exercises. In this paper, we dig deep into ChatGPT abilities applied to SQL exercises. We quantitatively and qualitatively evaluate the performance of a ChatGPT-as-a-SQL-assistant on benchmark data, with particular attention paid to its ability to correctly detect syntactic and semantic errors, provide insightful judgment explanations, and assign grades comparable to those of human teachers. Furthermore, we also analyze the benefits of leveraging few-shot learning to adapt LLM responses to the expectation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237783897",
                    "name": "Luca Cagliero"
                },
                {
                    "authorId": "3141797",
                    "name": "L. Farinetti"
                },
                {
                    "authorId": "1750925166",
                    "name": "Jacopo Fior"
                },
                {
                    "authorId": "2317109178",
                    "name": "Andrea Ignazio Manenti"
                }
            ]
        },
        {
            "paperId": "9c7e9acca0ad8aba3a284c997fbbc7cbe9319eff",
            "title": "Emotion Recognition from Videos Using Multimodal Large Language Models",
            "abstract": "The diffusion of Multimodal Large Language Models (MLLMs) has opened new research directions in the context of video content understanding and classification. Emotion recognition from videos aims to automatically detect human emotions such as anxiety and fear. It requires deeply elaborating multiple data modalities, including acoustic and visual streams. State-of-the-art approaches leverage transformer-based architectures to combine multimodal sources. However, the impressive performance of MLLMs in content retrieval and generation offers new opportunities to extend the capabilities of existing emotion recognizers. This paper explores the performance of MLLMs in the emotion recognition task in a zero-shot learning setting. Furthermore, it presents a state-of-the-art architecture extension based on MLLM content reformulation. The performance achieved on the Hume-Reaction benchmark shows that MLLMs are still unable to outperform the state-of-the-art average performance but, notably, are more effective than traditional transformers in recognizing emotions with an intensity that deviates from the average of the samples.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30079913",
                    "name": "Lorenzo Vaiani"
                },
                {
                    "authorId": "2237783897",
                    "name": "Luca Cagliero"
                },
                {
                    "authorId": "2292447799",
                    "name": "Paolo Garza"
                }
            ]
        },
        {
            "paperId": "a7589daf988106718f31e07bfdd043b7f24b1232",
            "title": "Extreme Classification of European Union Law Documents Driven by Entity Embeddings",
            "abstract": "Extreme Multi-label Classification (XMC) is the task of labeling documents with one or more labels from a large set of classes. In the context of Legal Artificial Intelligence, XMC is relevant to the automatic categorization of documents as they commonly address several orthogonal categorization schemes. Since retrieving a sufficient number of training document examples per class is challenging, XMC models are expected to be particularly effective in zero-shot learning scenarios. Existing approaches rely on transformer-based classification models, which leverage the attention mechanism to attend to specific textual units. However, classical attention scores are not able to differentiate between domain-specific and generic textual units. In this paper, we propose to use a legal entity-aware approach to zero-shot XMC of European Union law documents. By integrating information about domain-specific legal entities we ease the detection of label-sensitive information and prevent XMC models from attending to irrelevant or wrong text spans. The results achieved on the law documents available in the EURLex benchmark show that our approach is superior to both previous transformer-based approaches and opensource Large Language Models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2181131869",
                    "name": "Irene Benedetto"
                },
                {
                    "authorId": "2237783897",
                    "name": "Luca Cagliero"
                },
                {
                    "authorId": "29829627",
                    "name": "Francesco Tarasconi"
                }
            ]
        },
        {
            "paperId": "cefe6ed44f99b580e7aa8acd647c176685350d9b",
            "title": "BONES: a Benchmark fOr Neural Estimation of Shapley values",
            "abstract": "Shapley Values are concepts established for eXplainable AI. They are used to explain black-box predictive models by quantifying the features' contributions to the model's outcomes. Since computing the exact Shapley Values is known to be computationally intractable on real-world datasets, neural estimators have emerged as alternative, more scalable approaches to get approximated Shapley Values estimates. However, experiments with neural estimators are currently hard to replicate as algorithm implementations, explainer evaluators, and results visualizations are neither standardized nor promptly usable. To bridge this gap, we present BONES, a new benchmark focused on neural estimation of Shapley Value. It provides researchers with a suite of state-of-the-art neural and traditional estimators, a set of commonly used benchmark datasets, ad hoc modules for training black-box models, as well as specific functions to easily compute the most popular evaluation metrics and visualize results. The purpose is to simplify XAI model usage, evaluation, and comparison. In this paper, we showcase BONES results and visualizations for XAI model benchmarking on both tabular and image data. The open-source library is available at the following link: https://github.com/DavideNapolitano/BONES.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237785478",
                    "name": "Davide Napolitano"
                },
                {
                    "authorId": "2237783897",
                    "name": "Luca Cagliero"
                }
            ]
        },
        {
            "paperId": "462855d1fbb474ce0d6a8bd1177a875a64714041",
            "title": "Enhancing BERT-Based Visual Question Answering through Keyword-Driven Sentence Selection",
            "abstract": "The Document-based Visual Question Answering competition addresses the automatic detection of parent-child relationships between elements in multi-page documents. The goal is to identify the document elements that answer a specific question posed in natural language. This paper describes the PoliTo's approach to addressing this task, in particular, our best solution explores a text-only approach, leveraging an ad hoc sampling strategy. Specifically, our approach leverages the Masked Language Modeling technique to fine-tune a BERT model, focusing on sentences containing sensitive keywords that also occur in the questions, such as references to tables or images. Thanks to the effectiveness of this approach, we are able to achieve high performance compared to baselines, demonstrating how our solution contributes positively to this task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237785478",
                    "name": "Davide Napolitano"
                },
                {
                    "authorId": "30079913",
                    "name": "Lorenzo Vaiani"
                },
                {
                    "authorId": "2237783897",
                    "name": "Luca Cagliero"
                }
            ]
        },
        {
            "paperId": "afdc659c7a58cfa9f0513ac274da797d7881953e",
            "title": "PoliTo at MULTI-Fake-DetectiVE: Improving FND-CLIP for Multimodal Italian Fake News Detection",
            "abstract": "The MULTI-Fake-DetectiVE challenge addresses the automatic detection of Italian fake news in a multimodal setting, where both textual and visual components contribute as potential sources of fake content. This paper describes the PoliTO approach to the tasks of fake news detection and analysis of the modality contributions. Our solution turns out to be the best performer on both tasks. It leverages the established FND-CLIP multimodal architecture and proposes ad hoc extensions including sentiment-based text encoding, image transformation in the frequency domain, and data augmentation via back-translation. Thanks to its effectiveness in combining visual and textual content, our solution contributes to fighting the spread of disinformation in the Italian news flow.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237790629",
                    "name": "Lorenzo D'Amico"
                },
                {
                    "authorId": "2237785478",
                    "name": "Davide Napolitano"
                },
                {
                    "authorId": "30079913",
                    "name": "Lorenzo Vaiani"
                },
                {
                    "authorId": "2237783897",
                    "name": "Luca Cagliero"
                }
            ]
        }
    ]
}