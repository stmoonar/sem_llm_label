{
    "authorId": "2319137716",
    "papers": [
        {
            "paperId": "21b4777948797377deedf4a9f1f58ad13f6b8b5d",
            "title": "Overview of the Tenth Dialog System Technology Challenge: DSTC10",
            "abstract": "This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                },
                {
                    "authorId": "2150275",
                    "name": "Satwik Kottur"
                },
                {
                    "authorId": "2887412",
                    "name": "Jinchao Li"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2066415714",
                    "name": "Zhengcong Fei"
                },
                {
                    "authorId": "2109965103",
                    "name": "Zekang Li"
                },
                {
                    "authorId": "27672597",
                    "name": "Jinchao Zhang"
                },
                {
                    "authorId": "2257374643",
                    "name": "Yang Feng"
                },
                {
                    "authorId": "2116575668",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "145916630",
                    "name": "Karthik Gopalakrishnan"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "1979505",
                    "name": "A. Geramifard"
                },
                {
                    "authorId": "1765212",
                    "name": "Chiori Hori"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1694652",
                    "name": "Rafael E. Banchs"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ]
        },
        {
            "paperId": "22aba632fd56545ca9dc8ac055277f237e428e88",
            "title": "Baby Bear: Seeking a Just Right Rating Scale for Scalar Annotations",
            "abstract": "Our goal is a mechanism for efficiently assigning scalar ratings to each of a large set of elements. For example,\"what percent positive or negative is this product review?\"When sample sizes are small, prior work has advocated for methods such as Best Worst Scaling (BWS) as being more robust than direct ordinal annotation (\"Likert scales\"). Here we first introduce IBWS, which iteratively collects annotations through Best-Worst Scaling, resulting in robustly ranked crowd-sourced data. While effective, IBWS is too expensive for large-scale tasks. Using the results of IBWS as a best-desired outcome, we evaluate various direct assessment methods to determine what is both cost-efficient and best correlating to a large scale BWS annotation strategy. Finally, we illustrate in the domains of dialogue and sentiment how these annotations can support robust learning-to-rank models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2316521810",
                    "name": "Xu Han"
                },
                {
                    "authorId": "2316479270",
                    "name": "Felix Yu"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "2292194313",
                    "name": "Benjamin Van Durme"
                }
            ]
        },
        {
            "paperId": "29796ab9919d4aa68f1d81e2316803f7eb4869dd",
            "title": "On the Role of Summary Content Units in Text Summarization Evaluation",
            "abstract": "At the heart of the Pyramid evaluation method for text summarization lie human written summary content units (SCUs). These SCUs areconcise sentences that decompose a summary into small facts. Such SCUs can be used to judge the quality of a candidate summary, possibly partially automated via natural language inference (NLI) systems. Interestingly, with the aim to fully automate the Pyramid evaluation, Zhang and Bansal (2021) show that SCUs can be approximated by automatically generated semantic role triplets (STUs). However, several questions currently lack answers, in particular: i) Are there other ways of approximating SCUs that can offer advantages?ii) Under which conditions are SCUs (or their approximations) offering the most value? In this work, we examine two novel strategiesto approximate SCUs: generating SCU approximations from AMR meaning representations (SMUs) and from large language models (SGUs), respectively. We find that while STUs and SMUs are competitive, the best approximation quality is achieved by SGUs. We also show through a simple sentence-decomposition baseline (SSUs) that SCUs (and their approximations) offer the most value when rankingshort summaries, but may not help as much when ranking systems or longer summaries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2294574109",
                    "name": "Marcel Nawrath"
                },
                {
                    "authorId": "2294572396",
                    "name": "Agnieszka Nowak"
                },
                {
                    "authorId": "2294572422",
                    "name": "Tristan Ratz"
                },
                {
                    "authorId": "2294568442",
                    "name": "Danilo C. Walenta"
                },
                {
                    "authorId": "2263464894",
                    "name": "Juri Opitz"
                },
                {
                    "authorId": "2294595510",
                    "name": "Leonardo F. R. Ribeiro"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "2264072172",
                    "name": "Daniel Deutsch"
                },
                {
                    "authorId": "2253595522",
                    "name": "Simon Mille"
                },
                {
                    "authorId": "2294581001",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "48570960",
                    "name": "Lining Zhang"
                },
                {
                    "authorId": "2265058484",
                    "name": "Sebastian Gehrmann"
                },
                {
                    "authorId": "2221260",
                    "name": "Saad Mahamood"
                },
                {
                    "authorId": "2029314697",
                    "name": "Miruna Clinciu"
                },
                {
                    "authorId": "37619618",
                    "name": "Khyathi Raghavi Chandu"
                },
                {
                    "authorId": "2294689673",
                    "name": "Yufang Hou"
                }
            ]
        },
        {
            "paperId": "026cadc98de8b574919bc72bbd220e5a1d4da31b",
            "title": "How to Choose How to Choose Your Chatbot: A Massively Multi-System MultiReference Data Set for Dialog Metric Evaluation",
            "abstract": "We release MMSMR, a Massively Multi-System MultiReference dataset to enable future work on metrics and evaluation for dialog. Automatic metrics for dialogue evaluation should be robust proxies for human judgments; however, the verification of robustness is currently far from satisfactory. To quantify the robustness correlation and understand what is necessary in a test set, we create and release an 8-reference dialog dataset by extending single-reference evaluation sets and introduce this new language learning conversation dataset. We then train 1750 systems and evaluate them on our novel test set and the DailyDialog dataset. We release the novel test set, and model hyper parameters, inference outputs, and metric scores for each system on a variety of datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3115181",
                    "name": "Huda Khayrallah"
                },
                {
                    "authorId": "1491643573",
                    "name": "Zuhaib Akhtar"
                },
                {
                    "authorId": "2218123822",
                    "name": "Edward Cohen"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                }
            ]
        },
        {
            "paperId": "9799c17fd287bb9e8d231fe032c6dbf9c0c9d675",
            "title": "Overview of Robust and Multilingual Automatic Evaluation Metrics\n\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4",
            "abstract": "The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220406508",
                    "name": "Mario Rodr'iguez-Cantelar"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1672552269",
                    "name": "Chengguang Tang"
                },
                {
                    "authorId": "2026468506",
                    "name": "Ke Shi"
                },
                {
                    "authorId": "3022427",
                    "name": "Sarik Ghazarian"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ]
        },
        {
            "paperId": "434e375add81eaf82a49c6a0b20683ac20dfdcc1",
            "title": "A Needle in a Haystack: An Analysis of High-Agreement Workers on MTurk for Summarization",
            "abstract": "To prevent the costly and inefficient use of resources on low-quality annotations, we want a method for creating a pool of dependable annotators who can effectively complete difficult tasks, such as evaluating automatic summarization. Thus, we investigate the recruitment of high-quality Amazon Mechanical Turk workers via a two-step pipeline. We show that we can successfully filter out subpar workers before they carry out the evaluations and obtain high-agreement annotations with similar constraints on resources. Although our workers demonstrate a strong consensus among themselves and CloudResearch workers, their alignment with expert judgments on a subset of the data is not as expected and needs further training in correctness. This paper still serves as a best practice for the recruitment of qualified annotators in other challenging annotation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48570960",
                    "name": "Lining Zhang"
                },
                {
                    "authorId": "2738095",
                    "name": "Simon Mille"
                },
                {
                    "authorId": "39517968",
                    "name": "Yufang Hou"
                },
                {
                    "authorId": "145346875",
                    "name": "Daniel Deutsch"
                },
                {
                    "authorId": "40684993",
                    "name": "Elizabeth Clark"
                },
                {
                    "authorId": "2108176413",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "2221260",
                    "name": "Saad Mahamood"
                },
                {
                    "authorId": "3159346",
                    "name": "Sebastian Gehrmann"
                },
                {
                    "authorId": "2029314697",
                    "name": "Miruna Clinciu"
                },
                {
                    "authorId": "37619618",
                    "name": "Khyathi Raghavi Chandu"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                }
            ]
        },
        {
            "paperId": "4bef9d46209ac8988ea5ab83547149760d4af65e",
            "title": "Automatic Document Selection for Efficient Encoder Pretraining",
            "abstract": "Building pretrained language models is considered expensive and data-intensive, but must we increase dataset size to achieve better performance? We propose an alternative to larger training sets by automatically identifying smaller yet domain-representative subsets. We extend Cynical Data Selection, a statistical sentence scoring method that conditions on a representative target domain corpus. As an example, we treat the OntoNotes corpus as a target domain and pretrain a RoBERTa-like encoder from a cynically selected subset of the Pile. On both perplexity and across several downstream tasks in the target domain, it consistently outperforms random selection with 20x less data, 3x fewer training iterations, and 2x less estimated cloud compute cost, validating the recipe of automatic document selection for LM pretraining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48260295",
                    "name": "Yukun Feng"
                },
                {
                    "authorId": "2465658",
                    "name": "Patrick Xia"
                },
                {
                    "authorId": "7536576",
                    "name": "Benjamin Van Durme"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                }
            ]
        },
        {
            "paperId": "4ebac708d34577c73dbdb392a6531e82d4d5145c",
            "title": "Linear Connectivity Reveals Generalization Strategies",
            "abstract": "It is widely accepted in the mode connectivity literature that when two neural networks are trained similarly on the same data, they are connected by a path through parameter space over which test set accuracy is maintained. Under some circumstances, including transfer learning from pretrained models, these paths are presumed to be linear. In contrast to existing results, we find that among text classifiers (trained on MNLI, QQP, and CoLA), some pairs of finetuned models have large barriers of increasing loss on the linear paths between them. On each task, we find distinct clusters of models which are linearly connected on the test loss surface, but are disconnected from models outside the cluster -- models that occupy separate basins on the surface. By measuring performance on specially-crafted diagnostic datasets, we find that these clusters correspond to different generalization strategies: one cluster behaves like a bag of words model under domain shift, while another cluster uses syntactic heuristics. Our work demonstrates how the geometry of the loss surface can guide models towards different heuristic functions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2159712169",
                    "name": "Jeevesh Juneja"
                },
                {
                    "authorId": "80494139",
                    "name": "Rachit Bansal"
                },
                {
                    "authorId": "2111049203",
                    "name": "Kyunghyun Cho"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "2362960",
                    "name": "Naomi Saphra"
                }
            ]
        },
        {
            "paperId": "9ed047bb4ef6ddc296d473bf4f3b55488aeba350",
            "title": "GEMv2: Multilingual NLG Benchmarking in a Single Line of Code",
            "abstract": "Evaluation in machine learning is usually informed by past choices, for example which datasets or metrics to use. This standardization enables the comparison on equal footing using leaderboards, but the evaluation choices become sub-optimal as better alternatives arise. This problem is especially pertinent in natural language generation which requires ever-improving suites of datasets, metrics, and human evaluation to make definitive claims. To make following best model evaluation practices easier, we introduce GEMv2. The new version of the Generation, Evaluation, and Metrics Benchmark introduces a modular infrastructure for dataset, model, and metric developers to benefit from each others work. GEMv2 supports 40 documented datasets in 51 languages. Models for all datasets can be evaluated online and our interactive data card creation and rendering tools make it easier to add new datasets to the living benchmark.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3159346",
                    "name": "Sebastian Gehrmann"
                },
                {
                    "authorId": "49785688",
                    "name": "Abhik Bhattacharjee"
                },
                {
                    "authorId": "2143239611",
                    "name": "Abinaya Mahendiran"
                },
                {
                    "authorId": "2115765629",
                    "name": "Alex Wang"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "21626987",
                    "name": "Aman Madaan"
                },
                {
                    "authorId": "1584940075",
                    "name": "Angelina McMillan-Major"
                },
                {
                    "authorId": "46270987",
                    "name": "Anna Shvets"
                },
                {
                    "authorId": "84414257",
                    "name": "Ashish Upadhyay"
                },
                {
                    "authorId": "1490485182",
                    "name": "Bingsheng Yao"
                },
                {
                    "authorId": "150048491",
                    "name": "Bryan Wilie"
                },
                {
                    "authorId": "1857797",
                    "name": "Chandra Bhagavatula"
                },
                {
                    "authorId": "2172174590",
                    "name": "Chaobin You"
                },
                {
                    "authorId": "2122535749",
                    "name": "Craig Thomson"
                },
                {
                    "authorId": "3360992",
                    "name": "Cristina Garbacea"
                },
                {
                    "authorId": "36743361",
                    "name": "Dakuo Wang"
                },
                {
                    "authorId": "145346875",
                    "name": "Daniel Deutsch"
                },
                {
                    "authorId": "2694222",
                    "name": "Deyi Xiong"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2921637",
                    "name": "Dimitra Gkatzia"
                },
                {
                    "authorId": "9215251",
                    "name": "Dragomir R. Radev"
                },
                {
                    "authorId": "144354055",
                    "name": "Elizabeth Clark"
                },
                {
                    "authorId": "41152329",
                    "name": "Esin Durmus"
                },
                {
                    "authorId": "8759332",
                    "name": "Faisal Ladhak"
                },
                {
                    "authorId": "1694491",
                    "name": "Filip Ginter"
                },
                {
                    "authorId": "9162688",
                    "name": "Genta Indra Winata"
                },
                {
                    "authorId": "2879705",
                    "name": "Hendrik Strobelt"
                },
                {
                    "authorId": "50376014",
                    "name": "Hiroaki Hayashi"
                },
                {
                    "authorId": "2848048",
                    "name": "Jekaterina Novikova"
                },
                {
                    "authorId": "1776599",
                    "name": "Jenna Kanerva"
                },
                {
                    "authorId": "2164872258",
                    "name": "Jenny Chim"
                },
                {
                    "authorId": "2112249795",
                    "name": "Jiawei Zhou"
                },
                {
                    "authorId": "2133312617",
                    "name": "Jordan Clive"
                },
                {
                    "authorId": "2124977868",
                    "name": "Joshua Maynez"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "47080255",
                    "name": "Juraj Juraska"
                },
                {
                    "authorId": "4834571",
                    "name": "Kaustubh D. Dhole"
                },
                {
                    "authorId": "37619618",
                    "name": "Khyathi Raghavi Chandu"
                },
                {
                    "authorId": "10430740",
                    "name": "Leonardo F. R. Ribeiro"
                },
                {
                    "authorId": "2072250824",
                    "name": "Lewis Tunstall"
                },
                {
                    "authorId": "72436283",
                    "name": "Li Zhang"
                },
                {
                    "authorId": "51478016",
                    "name": "Mahima Pushkarna"
                },
                {
                    "authorId": "2219854",
                    "name": "Mathias Creutz"
                },
                {
                    "authorId": "145713719",
                    "name": "Michael White"
                },
                {
                    "authorId": "26688118",
                    "name": "Mihir Kale"
                },
                {
                    "authorId": "2065867306",
                    "name": "Moussa Kamal Eddine"
                },
                {
                    "authorId": "2048028927",
                    "name": "Nico Daheim"
                },
                {
                    "authorId": "34202134",
                    "name": "Nishant Subramani"
                },
                {
                    "authorId": "2544049",
                    "name": "Ondrej Dusek"
                },
                {
                    "authorId": "28130078",
                    "name": "P. Liang"
                },
                {
                    "authorId": "1451644426",
                    "name": "Pawan Sasanka Ammanamanchi"
                },
                {
                    "authorId": "2152208184",
                    "name": "Qinqin Zhu"
                },
                {
                    "authorId": "2990638",
                    "name": "Ratish Puduppully"
                },
                {
                    "authorId": "46218926",
                    "name": "Reno Kriz"
                },
                {
                    "authorId": "2046603",
                    "name": "Rifat Shahriyar"
                },
                {
                    "authorId": "144969436",
                    "name": "Ronald Cardenas"
                },
                {
                    "authorId": "2221260",
                    "name": "Saad Mahamood"
                },
                {
                    "authorId": "1486204986",
                    "name": "Salomey Osei"
                },
                {
                    "authorId": "2220548276",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "102379512",
                    "name": "S. vStajner"
                },
                {
                    "authorId": "46185285",
                    "name": "S\u00e9bastien Montella"
                },
                {
                    "authorId": "89189891",
                    "name": "Shailza"
                },
                {
                    "authorId": "51228129",
                    "name": "Shailza Jolly"
                },
                {
                    "authorId": "2738095",
                    "name": "Simon Mille"
                },
                {
                    "authorId": "1400373232",
                    "name": "Tahmid Hasan"
                },
                {
                    "authorId": "2057973326",
                    "name": "Tianhao Shen"
                },
                {
                    "authorId": "51221489",
                    "name": "Tosin P. Adewumi"
                },
                {
                    "authorId": "24025563",
                    "name": "Vikas Raunak"
                },
                {
                    "authorId": "2831377",
                    "name": "Vipul Raheja"
                },
                {
                    "authorId": "48942032",
                    "name": "Vitaly Nikolaev"
                },
                {
                    "authorId": "40052131",
                    "name": "V. Tsai"
                },
                {
                    "authorId": "2262249",
                    "name": "Yacine Jernite"
                },
                {
                    "authorId": "2110290513",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "31470750",
                    "name": "Yisi Sang"
                },
                {
                    "authorId": "2108176413",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "39517968",
                    "name": "Yufang Hou"
                }
            ]
        },
        {
            "paperId": "5cf42d26583d2b083262451e9005e6ed273badca",
            "title": "Automatic Evaluation and Moderation of Open-domain Dialogue Systems",
            "abstract": "The development of Open-Domain Dialogue Systems (ODS)is a trending topic due to the large number of research challenges, large societal and business impact, and advances in the underlying technology. However, the development of these kinds of systems requires two important characteristics:1) automatic evaluation mechanisms that show high correlations with human judgements across multiple dialogue evaluation aspects (with explainable features for providing constructive and explicit feedback on the quality of generative models' responses for quick development and deployment)and 2) mechanisms that can help to control chatbot responses,while avoiding toxicity and employing intelligent ways to handle toxic user comments and keeping interaction flow and engagement. This track at the 10th Dialogue System Technology Challenge (DSTC10) is part of the ongoing effort to promote scalable and toxic-free ODS. This paper describes the datasets and baselines provided to participants, as well as submission evaluation results for each of the two proposed subtasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1694652",
                    "name": "Rafael E. Banchs"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ]
        }
    ]
}