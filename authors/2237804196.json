{
    "authorId": "2237804196",
    "papers": [
        {
            "paperId": "20fae5b3b9f34a7f1f44983fd2a4c5381016f6d9",
            "title": "What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents",
            "abstract": "In this study, we introduce\"CosmoAgent,\"an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-centric bias inherent in current LLM designs, we propose the novel concept of using LLMs with diverse ethical paradigms and simulating interactions between entities with distinct moral principles. This innovative research provides a new way to understand complex inter-civilizational dynamics, expanding our perspective while pioneering novel strategies for conflict resolution, which are crucial for preventing interstellar conflicts. We have also released the code and datasets to enable further academic investigation into this interesting area of research. The code is available at https://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267333980",
                    "name": "Mingyu Jin"
                },
                {
                    "authorId": "2279758205",
                    "name": "Beichen Wang"
                },
                {
                    "authorId": "2284766516",
                    "name": "Zhaoqian Xue"
                },
                {
                    "authorId": "2279758147",
                    "name": "Suiyuan Zhu"
                },
                {
                    "authorId": "2007245028",
                    "name": "Wenyue Hua"
                },
                {
                    "authorId": "2285226182",
                    "name": "Hua Tang"
                },
                {
                    "authorId": "2261740874",
                    "name": "Kai Mei"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2279766837",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "2f385f5e04b58b962278efdd61e48f6af9196e0d",
            "title": "Knowledge Graph Large Language Model (KG-LLM) for Link Prediction",
            "abstract": "The task of multi-hop link prediction within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, as it requires the model to reason through and understand all intermediate connections before making a prediction. In this paper, we introduce the Knowledge Graph Large Language Model (KG-LLM), a novel framework that leverages large language models (LLMs) for knowledge graph tasks. We first convert structured knowledge graph data into natural language and then use these natural language prompts to fine-tune LLMs to enhance multi-hop link prediction in KGs. By converting the KG to natural language prompts, our framework is designed to learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading LLMs within this framework, including Flan-T5, LLaMa2 and Gemma. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities for handling previously unseen prompts. Experimental results show that KG-LLM significantly improves the models' generalization capabilities, leading to more accurate predictions in unfamiliar scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267332168",
                    "name": "Dong Shu"
                },
                {
                    "authorId": "2291006059",
                    "name": "Tianle Chen"
                },
                {
                    "authorId": "2267333980",
                    "name": "Mingyu Jin"
                },
                {
                    "authorId": "2292210436",
                    "name": "Yiting Zhang"
                },
                {
                    "authorId": "2279759627",
                    "name": "Chong Zhang"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2279766837",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "301e4ba731b703313ef24c1b6e95e9c0bc05a7dd",
            "title": "Health-LLM: Personalized Retrieval-Augmented Disease Prediction System",
            "abstract": "Recent advancements in artificial intelligence (AI), especially large language models (LLMs), have significantly advanced healthcare applications and demonstrated potentials in intelligent medical treatment. However, there are conspicuous challenges such as vast data volumes and inconsistent symptom characterization standards, preventing full integration of healthcare AI systems with individual patients' needs. To promote professional and personalized healthcare, we propose an innovative framework, Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management applications, our system has three main advantages: (1) It integrates health reports and medical knowledge into a large model to ask relevant questions to large language model for disease prediction; (2) It leverages a retrieval augmented generation (RAG) mechanism to enhance feature extraction; (3) It incorporates a semi-automated feature updating framework that can merge and delete features to improve accuracy of disease prediction. We experiment on a large number of health reports to assess the effectiveness of Health-LLM system. The results indicate that the proposed system surpasses the existing ones and has the potential to significantly advance disease prediction and personalized health management.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220539385",
                    "name": "Mingyu Jin"
                },
                {
                    "authorId": "2220796036",
                    "name": "Qinkai Yu"
                },
                {
                    "authorId": "2267332168",
                    "name": "Dong Shu"
                },
                {
                    "authorId": "2279759627",
                    "name": "Chong Zhang"
                },
                {
                    "authorId": "2268855367",
                    "name": "Lizhou Fan"
                },
                {
                    "authorId": "2007245028",
                    "name": "Wenyue Hua"
                },
                {
                    "authorId": "2279758147",
                    "name": "Suiyuan Zhu"
                },
                {
                    "authorId": "2278984372",
                    "name": "Yanda Meng"
                },
                {
                    "authorId": "2292292249",
                    "name": "Zhenting Wang"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2279766837",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "365dff5e201eda4e58c49e12a0dab4b12d191185",
            "title": "Data-centric NLP Backdoor Defense from the Lens of Memorization",
            "abstract": "Backdoor attack is a severe threat to the trustworthiness of DNN-based language models. In this paper, we first extend the definition of memorization of language models from sample-wise to more fine-grained sentence element-wise (e.g., word, phrase, structure, and style), and then point out that language model backdoors are a type of element-wise memorization. Through further analysis, we find that the strength of such memorization is positively correlated to the frequency of duplicated elements in the training dataset. In conclusion, duplicated sentence elements are necessary for successful backdoor attacks. Based on this, we propose a data-centric defense. We first detect trigger candidates in training data by finding memorizable elements, i.e., duplicated elements, and then confirm real triggers by testing if the candidates can activate backdoor behaviors (i.e., malicious elements). Results show that our method outperforms state-of-the-art defenses in defending against different types of NLP backdoors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2154723145",
                    "name": "Zhenting Wang"
                },
                {
                    "authorId": "2322451729",
                    "name": "Zhizhi Wang"
                },
                {
                    "authorId": "2323911406",
                    "name": "Mingyu Jin"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2008017411",
                    "name": "Juan Zhai"
                },
                {
                    "authorId": "2298338546",
                    "name": "Shiqing Ma"
                }
            ]
        },
        {
            "paperId": "3a51d62c874d672b8ecb530104e4bf4c06358ea7",
            "title": "LawLLM: Law Large Language Model for the US Legal System",
            "abstract": "In the rapidly evolving field of legal analytics, finding relevant cases and accurately predicting judicial outcomes are challenging because of the complexity of legal language, which often includes specialized terminology, complex syntax, and historical context. Moreover, the subtle distinctions between similar and precedent cases require a deep understanding of legal knowledge. Researchers often conflate these concepts, making it difficult to develop specialized techniques to effectively address these nuanced tasks. In this paper, we introduce the Law Large Language Model (LawLLM), a multi-task model specifically designed for the US legal domain to address these challenges. LawLLM excels at Similar Case Retrieval (SCR), Precedent Case Recommendation (PCR), and Legal Judgment Prediction (LJP). By clearly distinguishing between precedent and similar cases, we provide essential clarity, guiding future research in developing specialized strategies for these tasks. We propose customized data preprocessing techniques for each task that transform raw legal data into a trainable format. Furthermore, we also use techniques such as in-context learning (ICL) and advanced information retrieval methods in LawLLM. The evaluation results demonstrate that LawLLM consistently outperforms existing baselines in both zero-shot and few-shot scenarios, offering unparalleled multi-task capabilities and filling critical gaps in the legal domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267332168",
                    "name": "Dong Shu"
                },
                {
                    "authorId": "2314366999",
                    "name": "Haoran Zhao"
                },
                {
                    "authorId": "2314066416",
                    "name": "Xukun Liu"
                },
                {
                    "authorId": "2313922259",
                    "name": "David Demeter"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2279766837",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "40c0d1f38ab081e21cc3b1e2e5334a9b54b6ff08",
            "title": "The Impact of Reasoning Step Length on Large Language Models",
            "abstract": "Chain of Thought (CoT) is significant in improving the reasoning abilities of large language models (LLMs). However, the correlation between the effectiveness of CoT and the length of reasoning steps in prompts remains largely unknown. To shed light on this, we have conducted several empirical experiments to explore the relations. Specifically, we design experiments that expand and compress the rationale reasoning steps within CoT demonstrations while keeping all other factors constant. We have the following key findings. First, the results indicate that lengthening the reasoning steps in prompts, even without adding new information into the prompt, considerably enhances LLMs' reasoning abilities across multiple datasets. Alternatively, shortening the reasoning steps, even while preserving the key information, significantly diminishes the reasoning abilities of models. This finding highlights the importance of the number of steps in CoT prompts and provides practical guidance to make better use of LLMs' potential in complex problem-solving scenarios. Second, we also investigated the relationship between the performance of CoT and the rationales used in demonstrations. Surprisingly, the result shows that even incorrect rationales can yield favorable outcomes if they maintain the requisite length of inference. Third, we observed that the advantages of increasing reasoning steps are task-dependent: simpler tasks require fewer steps, whereas complex tasks gain significantly from longer inference sequences. The code is available at https://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267333980",
                    "name": "Mingyu Jin"
                },
                {
                    "authorId": "2220796036",
                    "name": "Qinkai Yu"
                },
                {
                    "authorId": "2267332168",
                    "name": "Dong Shu"
                },
                {
                    "authorId": "2237987232",
                    "name": "Haiyan Zhao"
                },
                {
                    "authorId": "2007245028",
                    "name": "Wenyue Hua"
                },
                {
                    "authorId": "2278984372",
                    "name": "Yanda Meng"
                },
                {
                    "authorId": "2239061409",
                    "name": "Yongfeng Zhang"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                }
            ]
        },
        {
            "paperId": "44c5c804442b635a745390d7d17b1ecb5e3ca89a",
            "title": "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era",
            "abstract": "Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency. This extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the\"black box\"to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems, and (2) how LLMs can contribute to the advancement of XAI. We introduce 10 strategies, introducing the key techniques for each and discussing their associated challenges. We also provide case studies to demonstrate how to obtain and leverage explanations. The code used in this paper can be found at: https://github.com/JacksonWuxs/UsableXAI_LLM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145346360",
                    "name": "Xuansheng Wu"
                },
                {
                    "authorId": "2237987232",
                    "name": "Haiyan Zhao"
                },
                {
                    "authorId": "2261804201",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "2249001715",
                    "name": "Yucheng Shi"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2291322576",
                    "name": "Tianming Liu"
                },
                {
                    "authorId": "2262445470",
                    "name": "Xiaoming Zhai"
                },
                {
                    "authorId": "2291141500",
                    "name": "Wenlin Yao"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2256183798",
                    "name": "Ninghao Liu"
                }
            ]
        },
        {
            "paperId": "4f60009234e76f9f8969f6cca23b3b07e944e984",
            "title": "Towards Uncovering How Large Language Model Works: An Explainability Perspective",
            "abstract": "Large language models (LLMs) have led to breakthroughs in language tasks, yet the internal mechanisms that enable their remarkable generalization and reasoning abilities remain opaque. This lack of transparency presents challenges such as hallucinations, toxicity, and misalignment with human values, hindering the safe and beneficial deployment of LLMs. This paper aims to uncover the mechanisms underlying LLM functionality through the lens of explainability. First, we review how knowledge is architecturally composed within LLMs and encoded in their internal parameters via mechanistic interpretability techniques. Then, we summarize how knowledge is embedded in LLM representations by leveraging probing techniques and representation engineering. Additionally, we investigate the training dynamics through a mechanistic perspective to explain phenomena such as grokking and memorization. Lastly, we explore how the insights gained from these explanations can enhance LLM performance through model editing, improve efficiency through pruning, and better align with human values.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237987232",
                    "name": "Haiyan Zhao"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2296792230",
                    "name": "Bo Shen"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                }
            ]
        },
        {
            "paperId": "56019756e85646883855e3583523317de465af42",
            "title": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?",
            "abstract": "Large language models (LLMs) have shown remarkable performances across a wide range of tasks. However, the mechanisms by which these models encode tasks of varying complexities remain poorly understood. In this paper, we explore the hypothesis that LLMs process concepts of varying complexities in different layers, introducing the idea of ``Concept Depth'' to suggest that more complex concepts are typically acquired in deeper layers. Specifically, we categorize concepts based on their level of abstraction, defining them in the order of increasing complexity within factual, emotional, and inferential tasks. We conduct extensive probing experiments using layer-wise representations across various LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the three domains of tasks. Our findings reveal that models could efficiently conduct probing for simpler tasks in shallow layers, and more complex tasks typically necessitate deeper layers for accurate understanding. Additionally, we examine how external factors, such as adding noise to the input and quantizing the model weights, might affect layer-wise representations. Our findings suggest that these factors can impede the development of a conceptual understanding of LLMs until deeper layers are explored. We hope that our proposed concept and experimental insights will enhance the understanding of the mechanisms underlying LLMs. Our codes are available at \\url{https://github.com/Luckfort/CD}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220539385",
                    "name": "Mingyu Jin"
                },
                {
                    "authorId": "2220796036",
                    "name": "Qinkai Yu"
                },
                {
                    "authorId": "2295904071",
                    "name": "Jingyuan Huang"
                },
                {
                    "authorId": "2153554138",
                    "name": "Qingcheng Zeng"
                },
                {
                    "authorId": "2292292249",
                    "name": "Zhenting Wang"
                },
                {
                    "authorId": "2007245028",
                    "name": "Wenyue Hua"
                },
                {
                    "authorId": "2237987232",
                    "name": "Haiyan Zhao"
                },
                {
                    "authorId": "2261740874",
                    "name": "Kai Mei"
                },
                {
                    "authorId": "2278984372",
                    "name": "Yanda Meng"
                },
                {
                    "authorId": "2295886392",
                    "name": "Kaize Ding"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2279766837",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "7adf8d98e4f4e7663f1b48ed07e94d6b396213e8",
            "title": "Exploring Multilingual Probing in Large Language Models: A Cross-Language Analysis",
            "abstract": "Probing techniques for large language models (LLMs) have primarily focused on English, overlooking the vast majority of the world's languages. In this paper, we extend these probing methods to a multilingual context, investigating the behaviors of LLMs across diverse languages. We conduct experiments on several open-source LLM models, analyzing probing accuracy, trends across layers, and similarities between probing vectors for multiple languages. Our key findings reveal: (1) a consistent performance gap between high-resource and low-resource languages, with high-resource languages achieving significantly higher probing accuracy; (2) divergent layer-wise accuracy trends, where high-resource languages show substantial improvement in deeper layers similar to English; and (3) higher representational similarities among high-resource languages, with low-resource languages demonstrating lower similarities both among themselves and with high-resource languages. These results highlight significant disparities in LLMs' multilingual capabilities and emphasize the need for improved modeling of low-resource languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2322452231",
                    "name": "Daoyang Li"
                },
                {
                    "authorId": "2323911406",
                    "name": "Mingyu Jin"
                },
                {
                    "authorId": "2312269664",
                    "name": "Qingcheng Zeng"
                },
                {
                    "authorId": "2237987232",
                    "name": "Haiyan Zhao"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                }
            ]
        }
    ]
}