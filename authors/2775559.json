{
    "authorId": "2775559",
    "papers": [
        {
            "paperId": "0579266e50e05ceb5ddb33ecd1469e2759b4564c",
            "title": "The Pulse of Mood Online: Unveiling Emotional Reactions in a Dynamic Social Media Landscape",
            "abstract": "The rich and dynamic information environment of social media provides researchers, policy makers, and entrepreneurs with opportunities to learn about social phenomena in a timely manner. However, using these data to understand social behavior is difficult due to heterogeneity of topics and events discussed in the highly dynamic online information environment. To address these challenges, we present a method for systematically detecting and measuring emotional reactions to offline events using change point detection on the time series of collective affect, and further explaining these reactions using a transformer-based topic model. We demonstrate the utility of the method by successfully detecting major and smaller events on three different datasets, including (1) a Los Angeles Tweet dataset between Jan. and Aug. 2020, in which we revealed the complex psychological impact of the BlackLivesMatter movement and the COVID-19 pandemic, (2) a dataset related to abortion rights discussions in USA, in which we uncovered the strong emotional reactions to the overturn of Roe v. Wade and state abortion bans, and (3) a dataset about the 2022 French presidential election, in which we discovered the emotional and moral shift from positive before voting to fear and criticism after voting. The capability of our method allows for better sensing and monitoring of population's reactions during crises using online data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110835750",
                    "name": "Siyi Guo"
                },
                {
                    "authorId": "2279405154",
                    "name": "Zihao He"
                },
                {
                    "authorId": "2088899596",
                    "name": "Ashwin Rao"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "104832606",
                    "name": "J. Brantingham"
                },
                {
                    "authorId": "2073018730",
                    "name": "Kristina Lerman"
                }
            ]
        },
        {
            "paperId": "30873bb90a189c0f35365449a32ec5e101509696",
            "title": "Policy Learning for Localized Interventions from Observational Data",
            "abstract": "A largely unaddressed problem in causal inference is that of learning reliable policies in continuous, high-dimensional treatment variables from observational data. Especially in the presence of strong confounding, it can be infeasible to learn the entire heterogeneous response surface from treatment to outcome. It is also not particularly useful, when there are practical constraints on the size of the interventions altering the observational treatments. Since it tends to be easier to learn the outcome for treatments near existing observations, we propose a new framework for evaluating and optimizing the effect of small, tailored, and localized interventions that nudge the observed treatment assignments. Our doubly robust effect estimator plugs into a policy learner that stays within the interventional scope by optimal transport. Consequently, the error of the total policy effect is restricted to prediction errors nearby the observational distribution, rather than the whole response surface.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10195617",
                    "name": "Myrl G. Marmarelis"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "1719898",
                    "name": "G. V. Steeg"
                }
            ]
        },
        {
            "paperId": "e38402c5aec9e3aa64237bf29c900f01cbb25d75",
            "title": "Don\u2019t Blame the Data, Blame the Model: Understanding Noise and Bias When Learning from Subjective Annotations",
            "abstract": "Researchers have raised awareness about the harms of aggregating labels especially in subjective tasks that naturally contain disagreements among human annotators. In this work we show that models that are only provided aggregated labels show low confidence on high-disagreement data instances. While previous studies consider such instances as mislabeled, we argue that the reason the high-disagreement text instances have been hard-to-learn is that the conventional aggregated models underperform in extracting useful signals from subjective tasks. Inspired by recent studies demonstrating the effectiveness of learning from raw annotations, we investigate classifying using Multiple Ground Truth (Multi-GT) approaches. Our experiments show an improvement of confidence for the high-disagreement instances.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2290185705",
                    "name": "Abhishek Anand"
                },
                {
                    "authorId": "1620657808",
                    "name": "Negar Mokhberian"
                },
                {
                    "authorId": "2290246671",
                    "name": "Prathyusha Naresh Kumar"
                },
                {
                    "authorId": "2218179145",
                    "name": "Anweasha Saha"
                },
                {
                    "authorId": "2279405154",
                    "name": "Zihao He"
                },
                {
                    "authorId": "2088899596",
                    "name": "Ashwin Rao"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "2073018730",
                    "name": "Kristina Lerman"
                }
            ]
        },
        {
            "paperId": "02541190b4e31b5f2871ab94a4ac1ab63d971bcc",
            "title": "Gender and Prestige Bias in Coronavirus News Reporting",
            "abstract": "Journalists play a vital role in surfacing issues of societal importance, but their choices of what to highlight and who to interview are influenced by societal biases. In this work, we use natural language processing tools to measure these biases in a large corpus of news articles about the Covid-19 pandemic. Specifically, we identify when experts are quoted in news and extract their names and institutional affiliations. We enrich the data by classifying each expert's gender, the type of organization they belong to, and for academic institutions, their ranking. Our analysis reveals disparities in the representation of experts in news. We find a substantial gender gap, where men are quoted three times more than women. The gender gap varies by partisanship of the news source, with conservative media exhibiting greater gender bias. We also identify academic prestige bias, where journalists turn to experts from highly-ranked academic institutions more than experts from less prestigious institutions, even if the latter group has more public health expertise. Liberal news sources exhibit slightly more prestige bias than conservative sources. Equality of representation is essential to enable voices from all groups to be heard. By auditing bias, our methods help identify blind spots in news coverage.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152955538",
                    "name": "Rebecca Dorn"
                },
                {
                    "authorId": "2181444517",
                    "name": "Yiwen Ma"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "2073018730",
                    "name": "Kristina Lerman"
                }
            ]
        },
        {
            "paperId": "2cbd08401e2ae9c20be465cfc01ff3396f0904b7",
            "title": "Pandemic Culture Wars: Partisan Asymmetries in the Moral Language of COVID-19 Discussions",
            "abstract": "Effective response to the COVID-19 pandemic required coordinated adoption of mitigation measures, like masking and quarantines, to curb virus's spread. However, political divisions that emerged early in the pandemic hindered consensus on the appropriate response. To better understand these divisions, our study examines a vast collection of COVID-19-related tweets. We focus on five contentious issues: coronavirus origins, lockdowns, masking, education, and vaccines. We describe a weakly supervised method to identify issue-relevant tweets and employ state-of-the-art computational methods to analyze moral language and infer political ideology. We explore how ideological divisions and moral language shape conversations about these issues. Our findings reveal ideological differences in issue salience and the dynamics of moral language. We find that conservatives use more negatively-valenced moral language than liberals, but moral language use by conservatives is less persistent and appears to be driven by dynamics of the news cycle. Furthermore, we find that political elites use moral rhetoric to a greater extent than non-elites across most issues. Examining the evolution and moralization on divisive issues can provide valuable insights into the dynamics of COVID-19 discussions and assist policymakers in better understanding the emergence of ideological divisions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2088899596",
                    "name": "Ashwin Rao"
                },
                {
                    "authorId": "2110835750",
                    "name": "Siyi Guo"
                },
                {
                    "authorId": "1663816124",
                    "name": "Sze-Yuh Nina Wang"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "2073018730",
                    "name": "Kristina Lerman"
                }
            ]
        },
        {
            "paperId": "33228c408cff97ef8fd47b3c753a8c2bb862ce5e",
            "title": "Capturing Perspectives of Crowdsourced Annotators in Subjective Learning Tasks",
            "abstract": "Supervised classification heavily depends on datasets annotated by humans. However, in subjective tasks such as toxicity classification, these annotations often exhibit low agreement among raters. Annotations have commonly been aggregated by employing methods like majority voting to determine a single ground truth label. In subjective tasks, aggregating labels will result in biased labeling and, consequently, biased models that can overlook minority opinions. Previous studies have shed light on the pitfalls of label aggregation and have introduced a handful of practical approaches to tackle this issue. Recently proposed multi-annotator models, which predict labels individually per annotator, are vulnerable to under-determination for annotators with few samples. This problem is exacerbated in crowdsourced datasets. In this work, we propose Annotator Aware Representations for Texts (AART) for subjective classification tasks. Our approach involves learning representations of annotators, allowing for exploration of annotation behaviors. We show the improvement of our method on metrics that assess the performance on capturing individual annotators\u2019 perspectives. Additionally, we demonstrate fairness metrics to evaluate our model\u2019s equability of performance for marginalized annotators compared to others.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1620657808",
                    "name": "Negar Mokhberian"
                },
                {
                    "authorId": "10195617",
                    "name": "Myrl G. Marmarelis"
                },
                {
                    "authorId": "17796576",
                    "name": "F. R. Hopp"
                },
                {
                    "authorId": "2266838987",
                    "name": "Valerio Basile"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "2073018730",
                    "name": "Kristina Lerman"
                }
            ]
        },
        {
            "paperId": "7f7956ccfc583a13b31f3d628af5c00a280ab14c",
            "title": "Just Another Day on Twitter: A Complete 24 Hours of Twitter Data",
            "abstract": "At the end of October 2022, Elon Musk concluded his acquisition of Twitter. In the weeks and months before that, several questions were publicly discussed that were not only of interest to the platform's future buyers, but also of high relevance to the Computational Social Science research community. For example, how many active users does the platform have? What percentage of accounts on the site are bots? And, what are the dominating topics and sub-topical spheres on the platform? In a globally coordinated effort of 80 scholars to shed light on these questions, and to offer a dataset that will equip other researchers to do the same, we have collected all 375 million tweets published within a 24-hour time period starting on September 21, 2022. To the best of our knowledge, this is the first complete 24-hour Twitter dataset that is available for the research community. With it, the present work aims to accomplish two goals. First, we seek to answer the aforementioned questions and provide descriptive metrics about Twitter that can serve as references for other researchers. Second, we create a baseline dataset for future research that can be used to study the potential impact of the platform's ownership change.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2161432959",
                    "name": "Juergen Pfeffer"
                },
                {
                    "authorId": "2203245785",
                    "name": "Daniel Matter"
                },
                {
                    "authorId": "1661048796",
                    "name": "Kokil Jaidka"
                },
                {
                    "authorId": "2307347",
                    "name": "Onur Varol"
                },
                {
                    "authorId": "1743965",
                    "name": "A. Mashhadi"
                },
                {
                    "authorId": "23724710",
                    "name": "J. Lasser"
                },
                {
                    "authorId": "12054937",
                    "name": "Dennis Assenmacher"
                },
                {
                    "authorId": "34962733",
                    "name": "Siqi Wu"
                },
                {
                    "authorId": "2143919864",
                    "name": "Diyi Yang"
                },
                {
                    "authorId": "37807016",
                    "name": "C. Brantner"
                },
                {
                    "authorId": "144463004",
                    "name": "Daniel M. Romero"
                },
                {
                    "authorId": "2216583",
                    "name": "Jahna Otterbacher"
                },
                {
                    "authorId": "82463398",
                    "name": "Carsten Schwemmer"
                },
                {
                    "authorId": "3306556",
                    "name": "K. Joseph"
                },
                {
                    "authorId": "2161469197",
                    "name": "David Garcia"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                }
            ]
        },
        {
            "paperId": "7ff6297ca89d8eaef45ca58d867329a27a0e6d32",
            "title": "Ensembled Prediction Intervals for Causal Outcomes Under Hidden Confounding",
            "abstract": "Causal inference of exact individual treatment outcomes in the presence of hidden confounders is rarely possible. Recent work has extended prediction intervals with finite-sample guarantees to partially identifiable causal outcomes, by means of a sensitivity model for hidden confounding. In deep learning, predictors can exploit their inductive biases for better generalization out of sample. We argue that the structure inherent to a deep ensemble should inform a tighter partial identification of the causal outcomes that they predict. We therefore introduce an approach termed Caus-Modens, for characterizing causal outcome intervals by modulated ensembles. We present a simple approach to partial identification using existing causal sensitivity models and show empirically that Caus-Modens gives tighter outcome intervals, as measured by the necessary interval size to achieve sufficient coverage. The last of our three diverse benchmarks is a novel usage of GPT-4 for observational experiments with unknown but probeable ground truth.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "10195617",
                    "name": "Myrl G. Marmarelis"
                },
                {
                    "authorId": "1719898",
                    "name": "G. V. Steeg"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                }
            ]
        },
        {
            "paperId": "8b118ad3f9247a188d95d01500e652519cd3ec21",
            "title": "Contextualizing Argument Quality Assessment with Relevant Knowledge",
            "abstract": "Automatic assessment of the quality of arguments has been recognized as a challenging task with significant implications for misinformation and targeted speech. While real-world arguments are tightly anchored in context, existing computational methods analyze their quality in isolation, which affects their accuracy and generalizability. We propose SPARK: a novel method for scoring argument quality based on contextualization via relevant knowledge. We devise four augmentations that leverage large language models to provide feedback, infer hidden assumptions, supply a similar-quality argument, or give a counter-argument. SPARK uses a dual-encoder Transformer architecture to enable the original argument and its augmentation to be considered jointly. Our experiments in both in-domain and zero-shot setups show that SPARK consistently outperforms existing techniques across multiple metrics",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2069729843",
                    "name": "D. Deshpande"
                },
                {
                    "authorId": "2196148017",
                    "name": "Zhivar Sourati"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                }
            ]
        },
        {
            "paperId": "8e74a034bbaee7f715ff6e0cbdd74ccdfa1e2d3a",
            "title": "Pipeline for modeling causal beliefs from natural language",
            "abstract": "We present a causal language analysis pipeline that leverages a Large Language Model to identify causal claims made in natural language documents, and aggregates claims across a corpus to produce a causal claim network. The pipeline then applies a clustering algorithm that groups causal claims based on their semantic topics. We demonstrate the pipeline by modeling causal belief systems surrounding the Covid-19 vaccine from tweets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144915407",
                    "name": "J. Priniski"
                },
                {
                    "authorId": "2176887820",
                    "name": "Ishaan Verma"
                },
                {
                    "authorId": "2775559",
                    "name": "Fred Morstatter"
                }
            ]
        }
    ]
}