{
    "authorId": "2327080",
    "papers": [
        {
            "paperId": "1bb1606d09db36b2129355b44ca3b5fc0febd105",
            "title": "Diversity, Equity and Inclusion Activities in Database Conferences: A 2023 Report",
            "abstract": "The Diversity, Equity and Inclusion (DEI) initiative started as the Diversity/Inclusion initiative in 2020 [4]. The current report summarizes our activities in 2023.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "143970078",
                    "name": "D. Agrawal"
                },
                {
                    "authorId": "2302858948",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1730344",
                    "name": "S. Bhowmick"
                },
                {
                    "authorId": "1404555727",
                    "name": "Renata Borovica-Gajic"
                },
                {
                    "authorId": "2314293279",
                    "name": "Jes\u00fas Camacho-Rodr\u00edguez"
                },
                {
                    "authorId": "2314330906",
                    "name": "Jinli Cao"
                },
                {
                    "authorId": "2314297028",
                    "name": "Barbara Catania"
                },
                {
                    "authorId": "2249901748",
                    "name": "P. Chrysanthis"
                },
                {
                    "authorId": "2278429940",
                    "name": "Carlo Curino"
                },
                {
                    "authorId": "117266605",
                    "name": "A. El Abbadi"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "2178387374",
                    "name": "Juliana Freire"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "1685532",
                    "name": "V. Kalogeraki"
                },
                {
                    "authorId": "51205357",
                    "name": "Sujaya Maiyya"
                },
                {
                    "authorId": "2266690266",
                    "name": "Alexandra Meliou"
                },
                {
                    "authorId": "37168010",
                    "name": "Madhulika Mohanty"
                },
                {
                    "authorId": "2257398736",
                    "name": "Fatma \u00d6zcan"
                },
                {
                    "authorId": "3139922",
                    "name": "L. Peterfreund"
                },
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                },
                {
                    "authorId": "2314297947",
                    "name": "Sana Sellami"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                },
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "2314670128",
                    "name": "Wang-Chiew Tan"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                },
                {
                    "authorId": "2303255329",
                    "name": "Yuanyuan Tian"
                },
                {
                    "authorId": "1393643717",
                    "name": "Genoveva Vargas-Solar"
                },
                {
                    "authorId": "2314731432",
                    "name": "Meihui Zhang"
                },
                {
                    "authorId": "2302886251",
                    "name": "Wenjie Zhang"
                }
            ]
        },
        {
            "paperId": "61bb72c3f4fe8b1e363eace4b84f9c6a3d5e91b7",
            "title": "Rapidash: Efficient Detection of Constraint Violations",
            "abstract": "Denial Constraint (DC) is a well-established formalism that captures a wide range of integrity constraints commonly encountered, including candidate keys, functional dependencies, and ordering constraints, among others. Given their significance, there has been considerable research interest in achieving fast detection of DC violations, especially to support activities related to data exploration and preparation. Despite the significant advancements in the field, prior work exhibits notable limitations when confronted with large-scale datasets: the current state-of-the-art algorithm demonstrates a quadratic (worst-case) time and space complexity relative to the dataset's number of rows. In this paper, we establish a connection between orthogonal range search and DC violation detection. We then introduce Rapidash, a novel algorithm that demonstrates near-linear time and space complexity, representing a theoretical improvement over prior work. To validate the effectiveness of our algorithm, we conduct comprehensive evaluations on both open-source and real-world production datasets, with our production datasets notably being an order of magnitude larger than the datasets employed in prior studies. Our results reveal that Rapidash achieves up to 84\u00d7 faster performance compared to state-of-the-art approaches while also exhibiting superior scalability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2244737561",
                    "name": "Zifan Liu"
                },
                {
                    "authorId": "34691891",
                    "name": "Shaleen Deep"
                },
                {
                    "authorId": "3071906",
                    "name": "Anna Fariha"
                },
                {
                    "authorId": "2493657",
                    "name": "Fotis Psallidas"
                },
                {
                    "authorId": "2219569097",
                    "name": "Ashish Tiwari"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                }
            ]
        },
        {
            "paperId": "9444dba8ea4ff0fa219ed3286f773e5f43dc1b13",
            "title": "PyFroid: Scaling Data Analysis on a Commodity Workstation",
            "abstract": "Almost every organization today is promoting data-driven decision making leveraging advances in data science. According to various surveys, data scientists spend up to 80% of their time cleaning and transforming data. Although data management systems have been carefully optimized for such tasks over several decades, they are seldom leveraged by data scientists who prefer to use libraries such as Pandas, sacrificing performance and scalability in favor of familiarity and ease of use. As a result, data scientists are not able to fully leverage the hardware capabilities of commodity workstations and either end up working on a small sample of their data locally or migrate to more heavyweight frameworks in a cluster environment. In this paper, we present PyFroid, a framework that leverages lightweight relational databases to improve the performance and scalability of Pandas, allowing data scientists to operate on much larger datasets on a commodity workstation. PyFroid has zero learning curve as it maintains all the Pandas APIs and is fully compatible with the tools that data scientists use (e.g., Python notebooks). We experimentally demonstrate that, compared to Pandas, PyFroid is able to analyze up to 20X more data on the same machine, provide comparable or better performance for small datasets as well as near-memory data sizes, and consume much less resources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3416202",
                    "name": "K. V. Emani"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                }
            ]
        },
        {
            "paperId": "ff2d31ac2fbd08ba088e6851f1ccbe482465aea5",
            "title": "NL2SQL is a solved problem... Not!",
            "abstract": "The development of natural language (NL) interfaces for databases has been notably shaped by the rise of Large Language Models (LLMs), which provide an easy way to automate the translation of NL queries into structured SQL queries. While LLMs bring valuable technical advancements, this paper stresses that achieving Enterprise-Grade NL2SQL is still far from being resolved, necessitating extensive novel research in various domains. We present insights from two competing teams dedicated to delivering reliable enterprise-grade NL2SQL technology, shedding light on challenges faced in real-world applications, including handling complex schemata, dealing with ambiguity in natural language statements, and incorporating it in our benchmarking methodologies and responsible AI considerations. While this paper may raise more questions than it answers, its aim is to act as a catalyst for a fruitful discussion on the topic. Additionally, it provides a practical pathway for the community to develop enterprise-grade NL2SQL solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "2493657",
                    "name": "Fotis Psallidas"
                },
                {
                    "authorId": "2087912469",
                    "name": "Fuheng Zhao"
                },
                {
                    "authorId": "34691891",
                    "name": "Shaleen Deep"
                },
                {
                    "authorId": "2294885879",
                    "name": "Gunther Hagleither"
                },
                {
                    "authorId": "2294920551",
                    "name": "Wangda Tan"
                },
                {
                    "authorId": "2253396700",
                    "name": "Joyce Cahoon"
                },
                {
                    "authorId": "2294886834",
                    "name": "Rana Alotaibi"
                },
                {
                    "authorId": "2253399928",
                    "name": "Jordan Henkel"
                },
                {
                    "authorId": "2294886803",
                    "name": "Abhik Singla"
                },
                {
                    "authorId": "2294885809",
                    "name": "Alex Van Grootel"
                },
                {
                    "authorId": "2294886216",
                    "name": "Brandon Chow"
                },
                {
                    "authorId": "2294859223",
                    "name": "Kai Deng"
                },
                {
                    "authorId": "2294953092",
                    "name": "Katherine Lin"
                },
                {
                    "authorId": "2295484068",
                    "name": "Marcos Campos"
                },
                {
                    "authorId": "3416202",
                    "name": "K. V. Emani"
                },
                {
                    "authorId": "2294886770",
                    "name": "Vivek Pandit"
                },
                {
                    "authorId": "2541093",
                    "name": "Victor Shnayder"
                },
                {
                    "authorId": "2294932955",
                    "name": "Wenjing Wang"
                },
                {
                    "authorId": "2278429940",
                    "name": "Carlo Curino"
                }
            ]
        },
        {
            "paperId": "2619e62557bcc8b4dc8f5e8b3724f273df022579",
            "title": "PACMMOD V1 N2 Editorial",
            "abstract": "We are excited to welcome you to the second issue of Volume 1 of the Proceedings of the ACM on Management of Data, PACMMOD. In addition to the 76 research track articles (out of 279 Cycle C submissions), this issue also includes peer-reviewed industrial track papers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143970078",
                    "name": "D. Agrawal"
                },
                {
                    "authorId": "2066060953",
                    "name": "Sihem Amer-Yahia"
                },
                {
                    "authorId": "1720972",
                    "name": "K. Candan"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "2274743",
                    "name": "Hakan Hac\u0131g\u00fcm\u00fc\u015f"
                }
            ]
        },
        {
            "paperId": "58219d9826f9ddde448c73e7ecc690111f5698f4",
            "title": "ReAcTable: Enhancing ReAct for Table Question Answering",
            "abstract": "Table Question Answering (TQA) presents a substantial challenge at the intersection of natural language processing and data analytics. This task involves answering natural language (NL) questions on top of tabular data, demanding proficiency in logical reasoning, understanding of data semantics, and fundamental analytical capabilities. Due to its significance, a substantial volume of research has been dedicated to exploring a wide range of strategies aimed at tackling this challenge including approaches that leverage Large Language Models (LLMs) through in-context learning or Chain-of-Thought (CoT) prompting as well as approaches that train and fine-tune custom models.\n \n Nonetheless, a conspicuous gap exists in the research landscape, where there is limited exploration of how innovative foundational research, which integrates incremental reasoning with external tools in the context of LLMs, as exemplified by the ReAct paradigm, could potentially bring advantages to the TQA task. In this paper, we aim to fill this gap, by introducing ReAcTable (\n ReAct\n for\n Table\n Question Answering tasks), a framework inspired by the ReAct paradigm that is carefully enhanced to address the challenges uniquely appearing in TQA tasks such as interpreting complex data semantics, dealing with errors generated by inconsistent data and generating intricate data transformations. ReAcTable relies on external tools such as SQL and Python code executors, to progressively enhance the data by generating intermediate data representations, ultimately transforming it into a more accessible format for answering the user's questions with greater ease. Through extensive empirical evaluations using three popular TQA benchmarks, we demonstrate that ReAcTable achieves remarkable performance even when compared to fine-tuned approaches. In particular, it outperforms the best prior result on the WikiTQ benchmark by 2.1%, achieving an accuracy of 68.0% without requiring training a new model or fine-tuning.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145049376",
                    "name": "Yunjia Zhang"
                },
                {
                    "authorId": "2253399928",
                    "name": "Jordan Henkel"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "2253396700",
                    "name": "Joyce Cahoon"
                },
                {
                    "authorId": "34691891",
                    "name": "Shaleen Deep"
                },
                {
                    "authorId": "2253413824",
                    "name": "Jignesh M. Patel"
                }
            ]
        },
        {
            "paperId": "8802e7ed7ff24334d9ef1492f9497b63b27352a5",
            "title": "LST-Bench: Benchmarking Log-Structured Tables in the Cloud",
            "abstract": "Data processing engines increasingly leverage distributed file systems for scalable, cost-effective storage. While the Apache Parquet columnar format has become a popular choice for data storage and retrieval, the immutability of Parquet files renders it impractical to meet the demands of frequent updates in contemporary analytical workloads. Log-Structured Tables (LSTs), such as Delta Lake, Apache Iceberg, and Apache Hudi, offer an alternative for scenarios requiring data mutability, providing a balance between efficient updates and the benefits of columnar storage. They provide features like transactions, time-travel, and schema evolution, enhancing usability and enabling access from multiple engines. Moreover, engines like Apache Spark and Trino can be configured to leverage the optimizations and controls offered by LSTs to meet specific business needs. Conventional benchmarks and tools are inadequate for evaluating the transformative changes in the storage layer resulting from these advancements, as they do not allow us to measure the impact of design and optimization choices in this new setting. In this paper, we propose a novel benchmarking approach and metrics that build upon existing benchmarks, aiming to systematically assess LSTs. We develop a framework, LST-Bench, which facilitates effective exploration and evaluation of the collaborative functioning of LSTs and data processing engines through tailored benchmark packages. A package is a mix of use patterns reflecting a target workload; LST-Bench makes it easy to define a wide range of use patterns and combine them into a package, and we include a baseline package for completeness. Our assessment demonstrates the effectiveness of our framework and benchmark packages in extracting valuable insights across diverse environments. The code for LST-Bench is open source and is available at https://github.com/microsoft/lst-bench/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2157042307",
                    "name": "Jes'us Camacho-Rodr'iguez"
                },
                {
                    "authorId": "9490513",
                    "name": "Ashvin Agrawal"
                },
                {
                    "authorId": "3202556",
                    "name": "Anja Gruenheid"
                },
                {
                    "authorId": "1395157337",
                    "name": "Ashit Gosalia"
                },
                {
                    "authorId": "2215898762",
                    "name": "Cristian Petculescu"
                },
                {
                    "authorId": "1398856399",
                    "name": "J. Aguilar-Saborit"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                }
            ]
        },
        {
            "paperId": "953d24f0f81714dd90f59f6fa7ba5d76349601b8",
            "title": "Will LLMs reshape, supercharge, or kill data science?",
            "abstract": "Large language models (LLMs) have recently taken the world by storm, promising potentially game changing opportunities in multiple fields. Naturally, there is significant promise in applying LLMs to the management of structured data, or more generally, to the processes involved in data science. At the very least, LLMs have the potential to provide substantial advancements in long-standing challenges that our community has been tackling for decades. On the other hand, they may introduce completely new capabilities that we have only dreamed of thus far. This panel will bring together a few leading experts who have been thinking about these opportunities from various perspectives and fielding them in research prototypes and even in commercial applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "2242456636",
                    "name": "Yejin Choi"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "2257377251",
                    "name": "Michael J. Franklin"
                },
                {
                    "authorId": "1791431",
                    "name": "Natasha Noy"
                },
                {
                    "authorId": "2242785594",
                    "name": "Haixun Wang"
                }
            ]
        },
        {
            "paperId": "997ed6b4617dfc51a727835f542d9fe1843cb387",
            "title": "Exploiting Structure in Regular Expression Queries",
            "abstract": "Regular expression, or regex, is widely used to extract critical information from a large corpus of formatted text by finding patterns of interest. In tasks like log processing, the speed of regex matching is crucial. Data scientists and developers regularly use regex libraries that implement optimized regular expression matching using modern automata theory. However, computing state transitions in the underlying regex evaluation engine can be inefficient when a regex query contains a multitude of string literals. This inefficiency is further exasperated when analyzing large data volumes. This paper presents BLARE, Blazingly Fast Regular Expression, a regular expression matching framework that is inspired by the mechanisms that are used in database engines, which use a declarative framework to explore multiple equivalent execution plans, all of which produce the correct final result. Similarly, BLARE decomposes a regex into multiple regex and string components and then creates evaluation strategies in which the components can be evaluated in an order that is not strictly a left-to-right translation of the input regex query. Rather than using a cost-based optimization approach, BLARE uses an adaptive runtime strategy based on a multi-armed bandit approach to find an efficient execution plan. BLARE is also modular and can be built on top of any existing regex library. We implemented BLARE on four commonly used regex libraries, RE2, PCRE2, Boost Regex, and ICU Regex, and evaluated it using two production workloads and one open-source workload. BLARE was 1.6\u00d7 to 3.7\u00d7 faster than RE2 and 3.4\u00d7 to 7.9\u00d7 faster than Boost Regex. PCRE2 did not finish on one of the workloads, but on the remaining two workloads, BLARE improved the performance of PCRE2 by 3.1\u00d7 to over 100\u00d7. For the open-source dataset, BLARE provided a speed up of 61.7\u00d7 for ICU Regex. BLARE code is publicly available at https://github.com/mush-zhang/Blare.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1712174",
                    "name": "Ling Zhang"
                },
                {
                    "authorId": "34691891",
                    "name": "Shaleen Deep"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "3202556",
                    "name": "Anja Gruenheid"
                },
                {
                    "authorId": "2149728753",
                    "name": "Jignesh M. Patel"
                },
                {
                    "authorId": "2139103184",
                    "name": "Yiwen Zhu"
                }
            ]
        },
        {
            "paperId": "a85c5d7272371345e28a9910080224cad799972e",
            "title": "Schema Matching using Pre-Trained Language Models",
            "abstract": "Schema matching over relational data has been studied for more than two decades. However, the state-of-the-art methods do not address key modern-day challenges encountered in real customer scenarios, namely: 1) no access to the source (customer) data due to privacy constraints, 2) target schema with a much larger number of entities and attributes compared to the source schema, and 3) different but semantically equivalent entity and attribute names in the source and target schemata. In this paper, we address these shortcomings. Using real-world customer schemata, we demonstrate that existing linguistic matching approaches have low accuracy. Next, we propose the Learned Schema Mapper (LSM), a novel linguistic schema matching system that leverages the natural language understanding capabilities of pre-trained language models to improve the overall accuracy. Combining this with active learning and a smart attribute selection strategy that selects the most informative attributes for users to label, LSM can significantly reduce the overall human labeling cost. Experimental results demonstrate that users can correctly match their full schema while saving as much as 81% of the labeling cost compared to manual labeling.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145049376",
                    "name": "Yunjia Zhang"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "150140157",
                    "name": "Joyce Cahoon"
                },
                {
                    "authorId": "2494730",
                    "name": "Subru Krishnan"
                },
                {
                    "authorId": "2113785713",
                    "name": "Andreas C. M\u00fcller"
                },
                {
                    "authorId": "150963621",
                    "name": "Dalitso Banda"
                },
                {
                    "authorId": "2493657",
                    "name": "Fotis Psallidas"
                },
                {
                    "authorId": "49111633",
                    "name": "J. Patel"
                }
            ]
        }
    ]
}