{
    "authorId": "2892275",
    "papers": [
        {
            "paperId": "421124db6a7901fc1933bf984c1e5ff5fdc27225",
            "title": "Embedding Based Retrieval in Friend Recommendation",
            "abstract": "Friend recommendation systems in online social and professional networks such as Snapchat helps users find friends and build connections, leading to better user engagement and retention. Traditional friend recommendation systems take advantage of the principle of locality and use graph traversal to retrieve friend candidates, e.g. Friends-of-Friends (FoF). While this approach has been adopted and shown efficacy in companies with large online networks such as Linkedin and Facebook, it suffers several challenges: (i) discrete graph traversal offers limited reach in cold-start settings, (ii) it is expensive and infeasible in realtime settings beyond 1 or 2 hop requests owing to latency constraints, and (iii) it cannot well-capture the complexity of graph topology or connection strengths, forcing one to resort to other mechanisms to rank and find top-K candidates. In this paper, we proposed a new Embedding Based Retrieval (EBR) system for retrieving friend candidates, which complements the traditional FoF retrieval by retrieving candidates beyond 2-hop, and providing a natural way to rank FoF candidates. Through online A/B test, we observe statistically significant improvements in the number of friendships made with EBR as an additional retrieval source in both low- and high-density network markets. Our contributions in this work include deploying a novel retrieval system to a large-scale friend recommendation system at Snapchat, generating embeddings for billions of users using Graph Neural Networks, and building EBR infrastructure in production to support Snapchat scale.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223893005",
                    "name": "Jiahui Shi"
                },
                {
                    "authorId": "2226063688",
                    "name": "Vivek Chaurasiya"
                },
                {
                    "authorId": "152891495",
                    "name": "Yozen Liu"
                },
                {
                    "authorId": "2223762996",
                    "name": "Shubham Vij"
                },
                {
                    "authorId": "83331194",
                    "name": "Y. Wu"
                },
                {
                    "authorId": "2892275",
                    "name": "Satya Kanduri"
                },
                {
                    "authorId": "2153429147",
                    "name": "Neil Shah"
                },
                {
                    "authorId": "2223884564",
                    "name": "Peicheng Yu"
                },
                {
                    "authorId": "2223768936",
                    "name": "Nik Srivastava"
                },
                {
                    "authorId": "2117206743",
                    "name": "Lei Shi"
                },
                {
                    "authorId": "2058606101",
                    "name": "Ganesh Venkataraman"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                }
            ]
        },
        {
            "paperId": "7865efa4187a85f1590b13f4e7dfe83beedb5213",
            "title": "Search by Ideal Candidates: Next Generation of Talent Search at LinkedIn",
            "abstract": "One key challenge in talent search is how to translate complex criteria of a hiring position into a search query. This typically requires deep knowledge on which skills are typically needed for the position, what are their alternatives, which companies are likely to have such candidates, etc. However, listing examples of suitable candidates for a given position is a relatively easy job. Therefore, in order to help searchers overcome this challenge, we design a next generation of talent search paradigm at LinkedIn: Search by Ideal Candidates. This new system only needs the searcher to input one or several examples of suitable candidates for the position. The system will generate a query based on the input candidates and then retrieve and rank results based on the query as well as the input candidates. The query is also shown to the searcher to make the system transparent and to allow the searcher to interact with it. As the searcher modifies the initial query and makes it deviate from the ideal candidates, the search ranking function dynamically adjusts an refreshes the ranking results balancing between the roles of query and ideal candidates. As of writing this paper, the new system is being launched to our customers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403844150",
                    "name": "Viet Ha-Thuc"
                },
                {
                    "authorId": "66607868",
                    "name": "Ye Xu"
                },
                {
                    "authorId": "2892275",
                    "name": "Satya Kanduri"
                },
                {
                    "authorId": "2108404748",
                    "name": "Xianren Wu"
                },
                {
                    "authorId": "39144541",
                    "name": "Vijay Dialani"
                },
                {
                    "authorId": "2117858471",
                    "name": "Yan Yan"
                },
                {
                    "authorId": "1410315386",
                    "name": "Abhishek Gupta"
                },
                {
                    "authorId": "2503512",
                    "name": "Shakti Sinha"
                }
            ]
        },
        {
            "paperId": "61a5e7991d75cb66819d603798afa9b9c6e73b1d",
            "title": "Understanding the effects of changes on the cost\u2010effectiveness of regression testing techniques",
            "abstract": "Regression testing is an expensive testing process used to validate modified software. Regression test selection and test\u2010case prioritization can reduce the costs of regression testing by selecting a subset of test cases for execution, or scheduling test cases to meet testing objectives better. The cost\u2010effectiveness of these techniques can vary widely, however, and one cause of this variance is the type and magnitude of changes made in producing a new software version. Engineers unaware of the causes and effects of this variance can make poor choices in designing change integration processes, selecting inappropriate regression testing techniques, designing excessively expensive regression test suites and making unnecessarily costly changes. Engineers aware of causal factors can perform regression testing more cost\u2010effectively. This article reports the results of an embedded multiple case study investigating the modifications made in the evolution of four software systems and their impact on regression testing techniques. The results of this study expose tradeoffs and constraints that affect the success of techniques and provide guidelines for designing and managing regression testing processes. Copyright \u00a9 2003 John Wiley & Sons, Ltd.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143754832",
                    "name": "Sebastian G. Elbaum"
                },
                {
                    "authorId": "2561675",
                    "name": "Praveen Kallakuri"
                },
                {
                    "authorId": "2120544",
                    "name": "Alexey G. Malishevsky"
                },
                {
                    "authorId": "1722304",
                    "name": "G. Rothermel"
                },
                {
                    "authorId": "2892275",
                    "name": "Satya Kanduri"
                }
            ]
        },
        {
            "paperId": "f4ce72a4beeb6a3ab55231704c38094426a8403d",
            "title": "Anomalies as precursors of field failures",
            "abstract": "Reproducing and learning from failures in deployed software is costly and difficult. Those activities can be facilitated, however, if the circumstances leading to a failure are properly captured. In this paper, we empirically investigate how various anomaly detection schemes can serve to identify the conditions that precede failures in deployed software. Our results expose the tradeoffs between different detection algorithms applied to several types of events under varying levels of in-house testing.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "143754832",
                    "name": "Sebastian G. Elbaum"
                },
                {
                    "authorId": "2892275",
                    "name": "Satya Kanduri"
                },
                {
                    "authorId": "1712567",
                    "name": "A. Andrews"
                }
            ]
        },
        {
            "paperId": "391f48bbe62991a0fc934036666a46d302845189",
            "title": "An empirical study of tracing techniques from a failure analysis perspective",
            "abstract": "Tracing is a dynamic analysis technique to continuously capture events of interest on a running program. The occurrence of a statement, the invocation of a function, and the trigger of a signal are examples of traced events. Software engineers employ traces to accomplish various tasks, ranging from performance monitoring to failure analysis. Despite its capabilities, tracing can negatively impact the performance and general behavior of an application. In order to minimize that impact, traces are normally buffered and transferred to (slower) permanent storage at specific intervals. This scenario presents a delicate balance. Increased buffering can minimize the impact on the target program, but it increases the risk of losing valuable collected data in the event of a failure. Frequent disk transfers can ensure traced data integrity, but it risks a high impact on the target program. We conducted an experiment involving six tracing schemes and various buffer sizes to address these trade-offs. Our results highlight opportunities for tailored tracing schemes that would benefit failure analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2892275",
                    "name": "Satya Kanduri"
                },
                {
                    "authorId": "143754832",
                    "name": "Sebastian G. Elbaum"
                }
            ]
        }
    ]
}