{
    "authorId": "2203901",
    "papers": [
        {
            "paperId": "1bb1606d09db36b2129355b44ca3b5fc0febd105",
            "title": "Diversity, Equity and Inclusion Activities in Database Conferences: A 2023 Report",
            "abstract": "The Diversity, Equity and Inclusion (DEI) initiative started as the Diversity/Inclusion initiative in 2020 [4]. The current report summarizes our activities in 2023.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "143970078",
                    "name": "D. Agrawal"
                },
                {
                    "authorId": "2302858948",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1730344",
                    "name": "S. Bhowmick"
                },
                {
                    "authorId": "1404555727",
                    "name": "Renata Borovica-Gajic"
                },
                {
                    "authorId": "2314293279",
                    "name": "Jes\u00fas Camacho-Rodr\u00edguez"
                },
                {
                    "authorId": "2314330906",
                    "name": "Jinli Cao"
                },
                {
                    "authorId": "2314297028",
                    "name": "Barbara Catania"
                },
                {
                    "authorId": "2249901748",
                    "name": "P. Chrysanthis"
                },
                {
                    "authorId": "2278429940",
                    "name": "Carlo Curino"
                },
                {
                    "authorId": "117266605",
                    "name": "A. El Abbadi"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "2178387374",
                    "name": "Juliana Freire"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "1685532",
                    "name": "V. Kalogeraki"
                },
                {
                    "authorId": "51205357",
                    "name": "Sujaya Maiyya"
                },
                {
                    "authorId": "2266690266",
                    "name": "Alexandra Meliou"
                },
                {
                    "authorId": "37168010",
                    "name": "Madhulika Mohanty"
                },
                {
                    "authorId": "2257398736",
                    "name": "Fatma \u00d6zcan"
                },
                {
                    "authorId": "3139922",
                    "name": "L. Peterfreund"
                },
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                },
                {
                    "authorId": "2314297947",
                    "name": "Sana Sellami"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                },
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "2314670128",
                    "name": "Wang-Chiew Tan"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                },
                {
                    "authorId": "2303255329",
                    "name": "Yuanyuan Tian"
                },
                {
                    "authorId": "1393643717",
                    "name": "Genoveva Vargas-Solar"
                },
                {
                    "authorId": "2314731432",
                    "name": "Meihui Zhang"
                },
                {
                    "authorId": "2302886251",
                    "name": "Wenjie Zhang"
                }
            ]
        },
        {
            "paperId": "2efe12dcaf4d4935f82b39c0aa557d371c7f3212",
            "title": "Limousine: Blending Learned and Classical Indexes to Self-Design Larger-than-Memory Cloud Storage Engines",
            "abstract": "We present Limousine, a self-designing key-value storage engine, that can automatically morph to the near-optimal storage engine architecture shape given a workload, a cloud budget, and target performance. At its core, Limousine identifies the fundamental design principles of storage engines as combinations of learned and classical data structures that collaborate through algorithms for data storage and access. By unifying these principles over diverse hardware and three major cloud providers (AWS, GCP, and Azure), Limousine creates a massive design space of quindecillion (1048) storage engine designs the vast majority of which do not exist in literature or industry. Limousine contains a distribution-aware IO model to accurately evaluate any candidate design. Using these models, Limousine searches within the exhaustive design space to construct a navigable continuum of designs connected along a Pareto frontier of cloud cost and performance. If storage engines contain learned components, Limousine also introduces efficient lazy write algorithms to optimize the holistic read-write performance. Once the near-optimal design is decided for the given context, Limousine automatically materializes the corresponding design in Rust code. Using the YCSB benchmark, we demonstrate that storage engines automatically designed and generated by Limousine scale better by up to 3 orders of magnitude when compared with state-of-the-art industry-leading engines such as RocksDB, WiredTiger, FASTER, and Cosine, over diverse workloads, data sets, and cloud budgets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293839893",
                    "name": "Subarna Chatterjee"
                },
                {
                    "authorId": "2293868355",
                    "name": "Mark F. Pekala"
                },
                {
                    "authorId": "2293868024",
                    "name": "Lev Kruglyak"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                }
            ]
        },
        {
            "paperId": "81b2834f3f884785e94a8c4c1b0bf66f858e0d97",
            "title": "The Image Calculator: 10x Faster Image-AI Inference by Replacing JPEG with Self-designing Storage Format",
            "abstract": "Numerous applications today rely on artificial intelligence over images. Image AI is, however, extremely expensive. In particular, the inference cost of image AI dominates the end-to-end cost. We observe that the image storage format lies at the root of the problem. Images today are predominantly stored in JPEG format. JPEG is a storage format designed for the human eye; it maximally compresses images without distorting the components of an image that are visible to the human eye. However, our observation is that during image AI, images are \"seen'' by algorithms, not humans. In addition, every AI application is different regarding which data components of the images are the most relevant. We present the Image Calculator, a self-designing image storage format that adapts to the given AI task, i.e., the specific neural network, the dataset, and the applications' specific accuracy, inference time, and storage requirements. Contrary to the state-of-the-art, the Image Calculator does not use a fixed storage format like JPEG. Instead, it designs and constructs a new storage format tailored to the context. It does so by constructing a massive design space of candidate storage formats from first principles, within which it searches efficiently using composite performance models (inference time, accuracy, storage). This way, it leverages the given AI task's unique characteristics to compress the data maximally. We evaluate the Image Calculator across a diverse set of data, image analysis tasks, AI models, and hardware. We show that the Image Calculator can generate image storage formats that reduce inference time by up to 14.2x and storage by up to 8.2x with a minimal loss in accuracy or gain, compared to JPEG and its state-of-the-art variants.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                }
            ]
        },
        {
            "paperId": "d077cf3287fbe29cd15b0f79ebebc77a5545c908",
            "title": "Data Structures for Data-Intensive Applications: Tradeoffs and Design Guidelines",
            "abstract": "Key-value data structures constitute the core of any datadriven system. They provide the means to store, search, and modify data residing at various levels of the storage and memory hierarchy, from durable storage (spinning disks, solid state disks, and other non-volatile memories) to random access memory, caches, and registers. Designing efficient data structures for given workloads has long been a focus of research and practice in both academia and industry. This book outlines the underlying design dimensions of data structures and shows how they can be combined to support (or fail to support) various workloads. The book further shows how these design dimensions can lead to an understanding of the behavior of individual state-of-the-art data structures and their hybrids. Finally, this systematization of the design space and the accompanying guidelines will enable you to select the most fitting data structure or even to invent an entirely new data structure for a given workload. Manos Athanassoulis, Stratos Idreos and Dennis Shasha (2023), \u201cData Structures for Data-Intensive Applications: Tradeoffs and Design Guidelines\u201d, Foundations and Trends\u00ae in Databases: Vol. 13, No. 1-2, pp 1\u2013168. DOI: 10.1561/1900000059. \u00a92023 M. Athanassoulis et al. Full text available at: http://dx.doi.org/10.1561/1900000059",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1840402",
                    "name": "Manos Athanassoulis"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "1695878",
                    "name": "D. Shasha"
                }
            ]
        },
        {
            "paperId": "ecd3c2bbd3a961ade418c91a75a5ed57c3887c47",
            "title": "Adaptive Indexing of Objects with Spatial Extent",
            "abstract": "\n Can we quickly explore large multidimensional data in main memory?\n Adaptive indexing\n responds to this need by building an index incrementally, in response to queries; in its default form, it indexes a single attribute or, in the presence of several attributes, one attribute per index level. Unfortunately, this approach falters when indexing spatial data objects, encountered in data exploration tasks involving multidimensional range queries. In this paper, we introduce the Adaptive Incremental R-tree (AIR-tree): the first method for the adaptive indexing of non-point spatial objects; the AIR-tree incrementally and progressively constructs an in-memory spatial index over a static array, in response to incoming queries, using a suite of heuristics for creating and splitting nodes. Our thorough experimental study on synthetic and real data and workloads shows that the AIR-tree consistently outperforms prior adaptive indexing methods focusing on multidimensional points and a pre-built static R-tree in cumulative time over at least the first thousand queries.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1586599742",
                    "name": "Fatemeh Zardbani"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "1731655",
                    "name": "Panagiotis Karras"
                }
            ]
        },
        {
            "paperId": "23350255b3fe8ab00dc73ac1d13fac4ec6874131",
            "title": "The Case for Distributed Shared-Memory Databases with RDMA-Enabled Memory Disaggregation",
            "abstract": "\n Memory disaggregation (MD) allows for scalable and elastic data center design by separating compute (CPU) from memory. With MD, compute and memory are no longer coupled into the same server box. Instead, they are connected to each other via ultra-fast networking such as RDMA. MD can bring many advantages, e.g., higher memory utilization, better independent scaling (of compute and memory), and lower cost of ownership. This paper makes the case that MD can fuel the next wave of innovation on database systems. We observe that MD revives the great debate of \"shared what\" in the database community. We envision that\n distributed shared-memory databases (DSM-DB, for short)\n - that have not received much attention before - can be promising in the future with MD. We present a list of challenges and opportunities that can inspire next steps in system design making the case for DSM-DB.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2282997262",
                    "name": "Ruihong Wang"
                },
                {
                    "authorId": "2118443454",
                    "name": "Jianguo Wang"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "9107867",
                    "name": "M. Tamer Ozsu"
                },
                {
                    "authorId": "1709661",
                    "name": "W. Aref"
                }
            ]
        },
        {
            "paperId": "7c3af255d08c33581ebe4bb267212ca25fe51565",
            "title": "Publication Culture and Review Processes in the Data Management Community: An Open Discussion",
            "abstract": "The Data Management community has explored many options in recent years to improve our publication culture and review processes, ranging from innovative journal-conference hybrids that decouple publication from presentation, incorporating journal-style reviewing for conference-style papers, requesting code reproducibility and code/data availability, multiple submission deadlines in a year, new categories of papers, informal shepherding processes, guidelines for diversity and inclusion, automated COI check, and so on. This panel seeks to examine our many experiments, comparing them with other CS disciplines, and help determine (i) have our experiments worked? (ii) what has their impact been? and (iii) can we do better?",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1730344",
                    "name": "S. Bhowmick"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "ae728adf50ea299e480a6f7c28925baebb0f995d",
            "title": "Entropy-Learned Hashing: Constant Time Hashing with Controllable Uniformity",
            "abstract": "Hashing is a widely used technique for creating uniformly random numbers from arbitrary data. This is required in a large range of core data-driven operations including indexing, partitioning, filters, and sketches. As such, hashing is a core component in numerous systems including relational data systems, key-value stores, compilers, and networks. Due to both the computational and data heavy nature of hashing, it is a core systems bottleneck. For example, a typical database query in the standard TPC-H benchmark may spend 50% of its total cost in hash tables. Similarly, Google spends at least 2% of its total computational cost on C++ hash tables, resulting in a massive yearly cost footprint just from one hashing operation. We propose a new hashing method, called Entropy-Learned Hashing, which reduces the computational cost of hashing by up to an order of magnitude. We look at hashing from a pseudorandomness point of view and the key question we ask is ''how much randomness is needed?'' We show that state-of-the-art hash functions do too much work to perform their core task: extracting randomness from a data source to create random outputs. Entropy-Learned Hashing 1) models and estimates the randomness (entropy) of the input data, and then 2) creates data-specific hash functions that use only the parts of the data that are needed to differentiate the outputs. The resulting hash functions dramatically reduce the amount of computation needed while we prove their output is similarly uniform to that of traditional hash functions. We test Entropy-Learned Hashing across diverse and core hashing operations such as hash tables, Bloom filters, and partitioning and we demonstrate an increase in throughput in the order of 3.7x, 4.0x, and 14x respectively compared to the best in-class hash functions and implementations used at scale by Google and Meta. In this paper we propose a new method, called Entropy-Learned Hashing, which reduces the computational cost of hashing by up to an order of magnitude. The key question we ask is \"how much randomness is needed?'': We look at hashing from a pseudorandom point of view, wherein hashing is viewed as extracting randomness from a data source to create random outputs and we show that state-of-the-art hash functions do too much work. Entropy-Learned Hashing 1) models and estimates the randomness (entropy) of the input data, and then 2) creates data-specific hash functions that use only the parts of the data that are needed to differentiate the outputs. Thus the resulting hash functions can minimize the amount of computation needed while we prove that they act similarly to traditional hash functions in terms of the uniformity of their outputs. We test Entropy-Learned Hashing across diverse and core hashing operations such as hash tables, Bloom filters, and partitioning and we observe an increase in throughput in the order of 3.7X, 4.0X, and 14X respectively compared to the best in-class hash functions and implementations used at scale by Google and Meta.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064396353",
                    "name": "Brian Hentschel"
                },
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                }
            ]
        },
        {
            "paperId": "b616b27c27a184715813b78a40684691c743ef40",
            "title": "Proteus: A Self-Designing Range Filter",
            "abstract": "We introduce Proteus, a novel self-designing approximate range filter, which configures itself based on sampled data in order to optimize its false positive rate (FPR) for a given space requirement. Proteus unifies the probabilistic and deterministic design spaces of state-of-the-art range filters to achieve robust performance across a larger variety of use cases. At the core of Proteus lies our Contextual Prefix FPR (CPFPR) model - a formal framework for the FPR of prefix-based filters across their design spaces. We empirically demonstrate the accuracy of our model and Proteus' ability to optimize over both synthetic workloads and real-world datasets. We further evaluate Proteus in RocksDB and show that it is able to improve end-to-end performance by as much as 5.3x over more brittle state-of-the-art methods such as SuRF and Rosetta. Our experiments also indicate that the cost of modeling is not significant compared to the end-to-end performance gains and that Proteus is robust to workload shifts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41067161",
                    "name": "Eric R. Knorr"
                },
                {
                    "authorId": "2169800379",
                    "name": "Baptiste Lemaire"
                },
                {
                    "authorId": "2053312324",
                    "name": "Andrew Lim"
                },
                {
                    "authorId": "3270262",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "2043439",
                    "name": "Huanchen Zhang"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "1745699",
                    "name": "M. Mitzenmacher"
                }
            ]
        },
        {
            "paperId": "ea2ef12859ff10f5c847dd8e2f821625a957fc13",
            "title": "SNARF: A Learning-Enhanced Range Filter",
            "abstract": "We present Sparse Numerical Array-Based Range Filters (SNARF), a learned range filter that efficiently supports range queries for numerical data. SNARF creates a model of the data distribution to map the keys into a bit array which is stored in a compressed form. The model along with the compressed bit array which constitutes SNARF are used to answer membership queries.\n We evaluate SNARF on multiple synthetic and real-world datasets as a stand-alone filter and by integrating it into RocksDB. For range queries, SNARF provides up to 50x better false positive rate than state-of-the-art range filters, such as SuRF and Rosetta, with the same space usage. We also evaluate SNARF in RocksDB as a filter replacement for filtering requests before they access on-disk data structures. For RocksDB, SNARF can improve the execution time of the system up to 10x compared to SuRF and Rosetta for certain read-only workloads.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7188311",
                    "name": "Kapil Vaidya"
                },
                {
                    "authorId": "1746961",
                    "name": "Tim Kraska"
                },
                {
                    "authorId": "1753423",
                    "name": "Subarna Chatterjee"
                },
                {
                    "authorId": "41067161",
                    "name": "Eric R. Knorr"
                },
                {
                    "authorId": "1745699",
                    "name": "M. Mitzenmacher"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                }
            ]
        }
    ]
}