{
    "authorId": "2248034991",
    "papers": [
        {
            "paperId": "274c4fb336ceea6535318e4c374feac45de01aec",
            "title": "Error Correction by Paying Attention to Both Acoustic and Confidence References for Automatic Speech Recognition",
            "abstract": "Accurately finding the wrong words in the automatic speech recognition (ASR) hypothesis and recovering them well-founded is the goal of speech error correction. In this paper, we propose a non-autoregressive speech error correction method. A Confidence Module measures the uncertainty of each word of the N-best ASR hypotheses as the reference to find the wrong word position. Besides, the acoustic feature from the ASR encoder is also used to provide the correct pronunciation references. N-best candidates from ASR are aligned using the edit path, to confirm each other and recover some missing character errors. Furthermore, the cross-attention mechanism fuses the information between error correction references and the ASR hypothesis. The experimental results show that both the acoustic and confidence references help with error correction. The proposed system reduces the error rate by 21% compared with the ASR model.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2066476983",
                    "name": "Yuchun Shu"
                },
                {
                    "authorId": "2312110065",
                    "name": "Bo Hu"
                },
                {
                    "authorId": "2312008041",
                    "name": "Yifeng He"
                },
                {
                    "authorId": "2112498288",
                    "name": "Hao Shi"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                }
            ]
        },
        {
            "paperId": "40809ba9b1c603897a20e421cacf42eaacdff6f8",
            "title": "An Initial Investigation of Language Adaptation for TTS Systems under Low-resource Scenarios",
            "abstract": "Self-supervised learning (SSL) representations from massively multilingual models offer a promising solution for low-resource language speech tasks. Despite advancements, language adaptation in TTS systems remains an open problem. This paper explores the language adaptation capability of ZMM-TTS, a recent SSL-based multilingual TTS system proposed in our previous work. We conducted experiments on 12 languages using limited data with various fine-tuning configurations. We demonstrate that the similarity in phonetics between the pre-training and target languages, as well as the language category, affects the target language's adaptation performance. Additionally, we find that the fine-tuning dataset size and number of speakers influence adaptability. Surprisingly, we also observed that using paired data for fine-tuning is not always optimal compared to audio-only data. Beyond speech intelligibility, our analysis covers speaker similarity, language identification, and predicted MOS.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2057185712",
                    "name": "Cheng Gong"
                },
                {
                    "authorId": "2239099011",
                    "name": "Erica Cooper"
                },
                {
                    "authorId": "2306431082",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2007407453",
                    "name": "Chunyu Qiang"
                },
                {
                    "authorId": "2306061299",
                    "name": "Mengzhe Geng"
                },
                {
                    "authorId": "152337381",
                    "name": "Dan Wells"
                },
                {
                    "authorId": "2277569329",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                },
                {
                    "authorId": "2180670416",
                    "name": "Marc Tessier"
                },
                {
                    "authorId": "51183422",
                    "name": "Aidan Pine"
                },
                {
                    "authorId": "144181051",
                    "name": "Korin Richmond"
                },
                {
                    "authorId": "2253664522",
                    "name": "Junichi Yamagishi"
                }
            ]
        },
        {
            "paperId": "832df5e1cb4ed0a6868e162f697779a28871f069",
            "title": "VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing",
            "abstract": "Deep learning has brought significant improvements to the field of cross-modal representation learning. For tasks such as text-to-speech (TTS), voice conversion (VC), and automatic speech recognition (ASR), a cross-modal fine-grained (frame-level) sequence representation is desired, emphasizing the semantic content of the text modality while de-emphasizing the paralinguistic information of the speech modality. We propose a method called\"Vector Quantized Contrastive Token-Acoustic Pre-training (VQ-CTAP)\", which uses the cross-modal aligned sequence transcoder to bring text and speech into a joint multimodal space, learning how to connect text and speech at the frame level. The proposed VQ-CTAP is a paradigm for cross-modal sequence representation learning, offering a promising solution for fine-grained generation and recognition tasks in speech processing. The VQ-CTAP can be directly applied to VC and ASR tasks without fine-tuning or additional structures. We propose a sequence-aware semantic connector, which connects multiple frozen pre-trained modules for the TTS task, exhibiting a plug-and-play capability. We design a stepping optimization strategy to ensure effective model convergence by gradually injecting and adjusting the influence of various loss components. Furthermore, we propose a semantic-transfer-wise paralinguistic consistency loss to enhance representational capabilities, allowing the model to better generalize to unseen data and capture the nuances of paralinguistic information. In addition, VQ-CTAP achieves high-compression speech coding at a rate of 25Hz from 24kHz input waveforms, which is a 960-fold reduction in the sampling rate. The audio demo is available at https://qiangchunyu.github.io/VQCTAP/",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2007407453",
                    "name": "Chunyu Qiang"
                },
                {
                    "authorId": "2315921491",
                    "name": "Wang Geng"
                },
                {
                    "authorId": "2292210639",
                    "name": "Yi Zhao"
                },
                {
                    "authorId": "3418514",
                    "name": "Ruibo Fu"
                },
                {
                    "authorId": "2156633072",
                    "name": "Tao Wang"
                },
                {
                    "authorId": "2057185712",
                    "name": "Cheng Gong"
                },
                {
                    "authorId": "2276154427",
                    "name": "Tianrui Wang"
                },
                {
                    "authorId": "2315944244",
                    "name": "Qiuyu Liu"
                },
                {
                    "authorId": "3491973",
                    "name": "Jiangyan Yi"
                },
                {
                    "authorId": "1718662",
                    "name": "Zhengqi Wen"
                },
                {
                    "authorId": "2311824945",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1381260894",
                    "name": "Hao Che"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                },
                {
                    "authorId": "2239200441",
                    "name": "Jianhua Tao"
                }
            ]
        },
        {
            "paperId": "9ad6c4aeb2ce9bc72ed3c839deeb65d2879cafd7",
            "title": "AIMDiT: Modality Augmentation and Interaction via Multimodal Dimension Transformation for Emotion Recognition in Conversations",
            "abstract": "Emotion Recognition in Conversations (ERC) is a popular task in natural language processing, which aims to recognize the emotional state of the speaker in conversations. While current research primarily emphasizes contextual modeling, there exists a dearth of investigation into effective multimodal fusion methods. We propose a novel framework called AIMDiT to solve the problem of multimodal fusion of deep features. Specifically, we design a Modality Augmentation Network which performs rich representation learning through dimension transformation of different modalities and parameter-efficient inception block. On the other hand, the Modality Interaction Network performs interaction fusion of extracted inter-modal features and intra-modal features. Experiments conducted using our AIMDiT framework on the public benchmark dataset MELD reveal 2.34% and 2.87% improvements in terms of the Acc-7 and w-F1 metrics compared to the state-of-the-art (SOTA) models.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2309205750",
                    "name": "Sheng Wu"
                },
                {
                    "authorId": "2108354640",
                    "name": "Jiaxing Liu"
                },
                {
                    "authorId": "2277569329",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                }
            ]
        },
        {
            "paperId": "a4fe48146630df3a37c3b9437cc6f551db4aa909",
            "title": "Progressive Residual Extraction based Pre-training for Speech Representation Learning",
            "abstract": "Self-supervised learning (SSL) has garnered significant attention in speech processing, excelling in linguistic tasks such as speech recognition. However, jointly improving the performance of pre-trained models on various downstream tasks, each requiring different speech information, poses significant challenges. To this purpose, we propose a progressive residual extraction based self-supervised learning method, named ProgRE. Specifically, we introduce two lightweight and specialized task modules into an encoder-style SSL backbone to enhance its ability to extract pitch variation and speaker information from speech. Furthermore, to prevent the interference of reinforced pitch variation and speaker information with irrelevant content information learning, we residually remove the information extracted by these two modules from the main branch. The main branch is then trained using HuBERT's speech masking prediction to ensure the performance of the Transformer's deep-layer features on content tasks. In this way, we can progressively extract pitch variation, speaker, and content representations from the input speech. Finally, we can combine multiple representations with diverse speech information using different layer weights to obtain task-specific representations for various downstream tasks. Experimental results indicate that our proposed method achieves joint performance improvements on various tasks, such as speaker identification, speech recognition, emotion recognition, speech enhancement, and voice conversion, compared to excellent SSL methods such as wav2vec2.0, HuBERT, and WavLM.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2276154427",
                    "name": "Tianrui Wang"
                },
                {
                    "authorId": "2319828763",
                    "name": "Jin Li"
                },
                {
                    "authorId": "2116609277",
                    "name": "Ziyang Ma"
                },
                {
                    "authorId": "2275198079",
                    "name": "Rui Cao"
                },
                {
                    "authorId": "2276453166",
                    "name": "Xie Chen"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "47589289",
                    "name": "Meng Ge"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2319424769",
                    "name": "Yuguang Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                },
                {
                    "authorId": "4577369",
                    "name": "N. Tashi"
                }
            ]
        },
        {
            "paperId": "efd754649f9283fad98aea8c9ce375fd5dd64db0",
            "title": "A Prompt-Based Hierarchical Pipeline for Cross-Domain Slot Filling",
            "abstract": "In task-oriented dialogue systems, slot filling aims to identify the semantic slot types of each token in user utterances. Due to the lack of sufficient supervised data in many scenarios, it is necessary to transfer relevant knowledge by using cross-domain slot filling. Previous studies rely on manually additional meta-information to build the relationships among similar slots across domains, yet not fully utilizing the knowledge learned by language models in the pre-training stage. In this study, we propose a prompt-based hierarchical pipeline (PHP) with three innovations. First, we design a hierarchical pipeline to separately model domain-independent syntactic structures and domain-specific semantic structures, i.e., span detection and slot prediction. Second, we improve the prompt paradigm with discriminative structure to fully utilize pre-trained language models, which reformulates downstream tasks into pre-trained tasks. Finally, we polish our template and verbalizer to effectively utilize task-specific prior knowledge, adding some meta-information and updating their additional trainable parameters. We conducted extensive experiments on three datasets to evaluate our method, and experimental results show that our method significantly outperforms the previous state-of-the-art results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185358961",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2226454143",
                    "name": "Yuhang Li"
                },
                {
                    "authorId": "10785308",
                    "name": "Yuke Si"
                },
                {
                    "authorId": "2277569329",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                }
            ]
        },
        {
            "paperId": "055d3e78d84cf3a2be9236009958dce244003c9f",
            "title": "A Refining Underlying Information Framework for Monaural Speech Enhancement",
            "abstract": "Supervised speech enhancement has gained significantly from recent advancements in neural networks, especially due to their ability to non-linearly fit the diverse representations of target speech, such as waveform or spectrum. However, these direct-fitting solutions continue to face challenges with degraded speech and residual noise in hearing evaluations. By bridging the speech enhancement and the Information Bottleneck principle in this letter, we rethink a universal plug-and-play strategy and propose a Refining Underlying Information framework called RUI to rise to the challenges both in theory and practice. Specifically, we first transform the objective of speech enhancement into an incremental convergence problem of mutual information between comprehensive speech characteristics and individual speech characteristics, e.g., spectral and acoustic characteristics. By doing so, compared with the existing direct-fitting solutions, the underlying information stems from the conditional entropy of acoustic characteristic given spectral characteristics. Therefore, we design a dual-path multiple refinement iterator based on the chain rule of entropy to refine this underlying information for further approximating target speech. Experimental results on DNS-Challenge dataset show that our solution consistently improves 0.3+ PESQ score over baselines, with only additional 1.18 M parameters. The source code is available at https://github.com/caoruitju/RUI_SE.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275198079",
                    "name": "Rui Cao"
                },
                {
                    "authorId": "2276154427",
                    "name": "Tianrui Wang"
                },
                {
                    "authorId": "47589289",
                    "name": "Meng Ge"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                }
            ]
        },
        {
            "paperId": "27d7492b047eef5dd931bfbd15b8f3d74fe13b15",
            "title": "High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models",
            "abstract": "Text-to-speech (TTS) methods have shown promising results in voice cloning, but they require a large number of labeled text-speech pairs. Minimally-supervised speech synthesis decouples TTS by combining two types of discrete speech representations(semantic & acoustic) and using two sequence-to-sequence tasks to enable training with minimal supervision. However, existing methods suffer from information redundancy and dimension explosion in semantic representation, and high-frequency waveform distortion in discrete acoustic representation. Autoregressive frameworks exhibit typical instability and uncontrollability issues. And non-autoregressive frameworks suffer from prosodic averaging caused by duration prediction models. To address these issues, we propose a minimally-supervised high-fidelity speech synthesis method, where all modules are constructed based on the diffusion models. The non-autoregressive framework enhances controllability, and the duration diffusion model enables diversified prosodic expression. Contrastive Token-Acoustic Pretraining (CTAP) is used as an intermediate semantic representation to solve the problems of information redundancy and dimension explosion in existing semantic coding methods. Mel-spectrogram is used as the acoustic representation. Both semantic and acoustic representations are predicted by continuous variable regression tasks to solve the problem of high-frequency fine-grained waveform distortion. Experimental results show that our proposed method outperforms the baseline method. We provide audio samples on our website.1",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2007407453",
                    "name": "Chunyu Qiang"
                },
                {
                    "authorId": "144966714",
                    "name": "Hao Li"
                },
                {
                    "authorId": "2237429121",
                    "name": "Yixin Tian"
                },
                {
                    "authorId": "2292210639",
                    "name": "Yi Zhao"
                },
                {
                    "authorId": "2248285429",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                }
            ]
        },
        {
            "paperId": "301cf88e7a0b5471c0225c638ef238f8207c9b7a",
            "title": "ZMM-TTS: Zero-Shot Multilingual and Multispeaker Speech Synthesis Conditioned on Self-Supervised Discrete Speech Representations",
            "abstract": "Neural text-to-speech (TTS) has achieved human-like synthetic speech for single-speaker, single-language synthesis. Multilingual TTS systems are limited to resource-rich languages due to the lack of large paired text and studio-quality audio data. TTS systems are typically built using a single speaker's voice, but there is growing interest in developing systems that can synthesize voices for new speakers using only a few seconds of their speech. This paper presents ZMM-TTS, a multilingual and multispeaker framework utilizing quantized latent speech representations from a large-scale, pre-trained, self-supervised model. Our paper combines text-based and speech-based self-supervised learning models for multilingual speech synthesis. Our proposed model has zero-shot generalization ability not only for unseen speakers but also for unseen languages. We have conducted comprehensive subjective and objective evaluations through a series of experiments. Our model has proven effective in terms of speech naturalness and similarity for both seen and unseen speakers in six high-resource languages. We also tested the efficiency of our method on two hypothetically low-resource languages. The results are promising, indicating that our proposed approach can synthesize audio that is intelligible and has a high degree of similarity to the target speaker's voice, even without any training data for the new, unseen language.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2057185712",
                    "name": "Cheng Gong"
                },
                {
                    "authorId": "38435748",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2239099011",
                    "name": "Erica Cooper"
                },
                {
                    "authorId": "152337381",
                    "name": "Dan Wells"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                },
                {
                    "authorId": "144181051",
                    "name": "Korin Richmond"
                },
                {
                    "authorId": "2261050358",
                    "name": "J. Yamagishi"
                }
            ]
        },
        {
            "paperId": "51f4b7a4d122206f4d877e63300dc7a26f036615",
            "title": "CFDRN: A Cognition-Inspired Feature Decomposition and Recombination Network for Dysarthric Speech Recognition",
            "abstract": "As an essential technology in human\u2013computer interactions, automatic speech recognition (ASR) ensures a convenient life for healthy people; however, people with speech disorders, who truly need support from such a technology, have experienced difficulties in the use of ASR. Disordered ASR is challenging because of the large variabilities in disordered speech. Humans tend to separately process different spectro-temporal features of speech in the left and right hemispheres of their brain, showing significantly better ability in speech perception than machines, especially in disordered speech perception. Inspired by human speech processing, this article proposes a cognition-inspired feature decomposition and recombination network (CFDRN) for dysarthric ASR. In the CFDRN, slow- and rapid-varying temporal processors are designed to decompose features into stable and changeable features, respectively. A gated fusion module was developed to selectively recombine the decomposed features. Moreover, this study utilised an adaptation approach based on unsupervised pre-training techniques to alleviate data scarcity issues in dysarthric ASR. The CFDRNs were added to the layers of the pre-trained model, and the entire model is adapted from normal speech to disordered speech. The effectiveness of the proposed method was validated on the widely used TORGO and UASpeech dysarthria datasets under three popular unsupervised pre-training techniques, wav2vec 2.0, HuBERT, and data2vec. When compared to the baseline methods, the proposed CFDRN with the three pre-training techniques achieved 13.73%$\\sim$16.23% and 4.50%$\\sim$13.20% word error rate reductions on the TORGO and UASpeech datasets, respectively. Furthermore, this study clarified several major factors affecting dysarthric ASR performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107919875",
                    "name": "Yuqin Lin"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2189501338",
                    "name": "Yanbing Yang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                }
            ]
        }
    ]
}