{
    "authorId": "1994333",
    "papers": [
        {
            "paperId": "9aac23b61414400493c2dcf95d7dc7a5a4e3b068",
            "title": "InspectorRAGet: An Introspection Platform for RAG Evaluation",
            "abstract": "Large Language Models (LLM) have become a popular approach for implementing Retrieval Augmented Generation (RAG) systems, and a significant amount of effort has been spent on building good models and metrics. In spite of increased recognition of the need for rigorous evaluation of RAG systems, few tools exist that go beyond the creation of model output and automatic calculation. We present InspectorRAGet, an introspection platform for RAG evaluation. InspectorRAGet allows the user to analyze aggregate and instance-level performance of RAG systems, using both human and algorithmic metrics as well as annotator quality. InspectorRAGet is suitable for multiple use cases and is available publicly to the community. The demo video is available at https://youtu.be/MJhe8QIXcEc",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3468202",
                    "name": "Kshitij P. Fadnis"
                },
                {
                    "authorId": "2298938126",
                    "name": "Siva Sankalp Patel"
                },
                {
                    "authorId": "1997603",
                    "name": "O. Boni"
                },
                {
                    "authorId": "2114890014",
                    "name": "Yannis Katsis"
                },
                {
                    "authorId": "144063596",
                    "name": "Sara Rosenthal"
                },
                {
                    "authorId": "2298756560",
                    "name": "Benjamin Sznajder"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                }
            ]
        },
        {
            "paperId": "0105c96f23482db3ad84bc2d7fe55394c89118a7",
            "title": "Evaluating Robustness of Dialogue Summarization Models in the Presence of Naturally Occurring Variations",
            "abstract": "Dialogue summarization task involves summarizing long conversations while preserving the most salient information. Real-life dialogues often involve naturally occurring variations (e.g., repetitions, hesitations) and existing dialogue summarization models suffer from performance drop on such conversations. In this study, we systematically investigate the impact of such variations on state-of-the-art dialogue summarization models using publicly available datasets. To simulate real-life variations, we introduce two types of perturbations: utterance-level perturbations that modify individual utterances with errors and language variations, and dialogue-level perturbations that add non-informative exchanges (e.g., repetitions, greetings). We conduct our analysis along three dimensions of robustness: consistency, saliency, and faithfulness, which capture different aspects of the summarization model's performance. We find that both fine-tuned and instruction-tuned models are affected by input variations, with the latter being more susceptible, particularly to dialogue-level perturbations. We also validate our findings via human evaluation. Finally, we investigate if the robustness of fine-tuned models can be improved by training them with a fraction of perturbed data and observe that this approach is insufficient to address robustness challenges with current models and thus warrants a more thorough investigation to identify better solutions. Overall, our work highlights robustness challenges in dialogue summarization and provides insights for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266804876",
                    "name": "Ankita Gupta"
                },
                {
                    "authorId": "66161659",
                    "name": "Chulaka Gunasekara"
                },
                {
                    "authorId": "2052564414",
                    "name": "H. Wan"
                },
                {
                    "authorId": "2504586",
                    "name": "Jatin Ganhotra"
                },
                {
                    "authorId": "2266810569",
                    "name": "Sachindra Joshi"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                }
            ]
        },
        {
            "paperId": "0db8fe527bed5e5f5467b7f1a630236efcc8c1af",
            "title": "How Can Context Help? Exploring Joint Retrieval of Passage and Personalized Context",
            "abstract": "The integration of external personalized context information into document-grounded conversational systems has significant potential business value, but has not been well-studied. Motivated by the concept of personalized context-aware document-grounded conversational systems, we introduce the task of context-aware passage retrieval. We also construct a dataset specifically curated for this purpose. We describe multiple baseline systems to address this task, and propose a novel approach, Personalized Context-Aware Search (PCAS), that effectively harnesses contextual information during passage retrieval. Experimental evaluations conducted on multiple popular dense retrieval systems demonstrate that our proposed approach not only outperforms the baselines in retrieving the most relevant passage but also excels at identifying the pertinent context among all the available contexts. We envision that our contributions will serve as a catalyst for inspiring future research endeavors in this promising direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052564414",
                    "name": "H. Wan"
                },
                {
                    "authorId": "2162633775",
                    "name": "Hongkang Li"
                },
                {
                    "authorId": "1606015788",
                    "name": "Songtao Lu"
                },
                {
                    "authorId": "1857396",
                    "name": "Xiao-Ping Cui"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                }
            ]
        },
        {
            "paperId": "62f7f806a0e914c360f0640d247f4df1974c1f3b",
            "title": "Semi-Structured Object Sequence Encoders",
            "abstract": "In this paper we explore the task of modeling semi-structured object sequences; in particular, we focus our attention on the problem of developing a structure-aware input representation for such sequences. Examples of such data include user activity on websites, machine logs, and many others. This type of data is often represented as a sequence of sets of key-value pairs over time and can present modeling challenges due to an ever-increasing sequence length. We propose a two-part approach, which first considers each key independently and encodes a representation of its values over time; we then self-attend over these value-aware key representations to accomplish a downstream task. This allows us to operate on longer object sequences than existing methods. We introduce a novel shared-attention-head architecture between the two modules and present an innovative training schedule that interleaves the training of both modules with shared weights for some attention heads. Our experiments on multiple prediction tasks using real-world data demonstrate that our approach outperforms a unified network with hierarchical encoding, as well as other methods including a record-centric representation and a flattened representation of the sequence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2680174",
                    "name": "V. Rudramurthy"
                },
                {
                    "authorId": "40578334",
                    "name": "Riyaz Ahmad Bhat"
                },
                {
                    "authorId": "66161659",
                    "name": "Chulaka Gunasekara"
                },
                {
                    "authorId": "2052564414",
                    "name": "H. Wan"
                },
                {
                    "authorId": "2952437",
                    "name": "Tejas I. Dhamecha"
                },
                {
                    "authorId": "2075459",
                    "name": "Danish Contractor"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                }
            ]
        },
        {
            "paperId": "6cb99af50b91fed276b7b6cee4c91cea5d222ae3",
            "title": "PriMeSRL-Eval: A Practical Quality Metric for Semantic Role Labeling Systems Evaluation",
            "abstract": "Semantic role labeling (SRL) identifies the predicate-argument structure in a sentence. This task is usually accomplished in four steps: predicate identification, predicate sense disambiguation, argument identification, and argument classification. Errors introduced at one step propagate to later steps. Unfortunately, the existing SRL evaluation scripts do not consider the full effect of this error propagation aspect. They either evaluate arguments independent of predicate sense (CoNLL09) or do not evaluate predicate sense at all (CoNLL05), yielding an inaccurate SRL model performance on the argument classification task. In this paper, we address key practical issues with existing evaluation scripts and propose a more strict SRL evaluation metric PriMeSRL. We observe that by employing PriMeSRL, the quality evaluation of all SoTA SRL models drops significantly, and their relative rankings also change. We also show that PriMeSRLsuccessfully penalizes actual failures in SoTA SRL models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144377686",
                    "name": "Ishan Jindal"
                },
                {
                    "authorId": "13836343",
                    "name": "Alexandre Rademaker"
                },
                {
                    "authorId": "1399212475",
                    "name": "Khoi-Nguyen Tran"
                },
                {
                    "authorId": "2115718238",
                    "name": "Huaiyu Zhu"
                },
                {
                    "authorId": "35562751",
                    "name": "H. Kanayama"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                },
                {
                    "authorId": "1718694",
                    "name": "Yunyao Li"
                }
            ]
        },
        {
            "paperId": "6f00dceaf0717e83f3b75c209b48936db1faf4ac",
            "title": "Label Sleuth: From Unlabeled Text to a Classifier in a Few Hours",
            "abstract": "Text classification can be useful in many real-world scenarios, saving a lot of time for end users. However, building a custom classifier typically requires coding skills and ML knowledge, which poses a significant barrier for many potential users. To lift this barrier, we introduce Label Sleuth, a free open source system for labeling and creating text classifiers. This system is unique for (a) being a no-code system, making NLP accessible to non-experts, (b) guiding users through the entire labeling process until they obtain a custom classifier, making the process efficient -- from cold start to classifier in a few hours, and (c) being open for configuration and extension by developers. By open sourcing Label Sleuth we hope to build a community of users and developers that will broaden the utilization of NLP models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1734246",
                    "name": "Eyal Shnarch"
                },
                {
                    "authorId": "41127252",
                    "name": "Alon Halfon"
                },
                {
                    "authorId": "48835746",
                    "name": "Ariel Gera"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                },
                {
                    "authorId": "2208580",
                    "name": "Yannis Katsis"
                },
                {
                    "authorId": "41019330",
                    "name": "Leshem Choshen"
                },
                {
                    "authorId": "2106505319",
                    "name": "M. Cooper"
                },
                {
                    "authorId": "2180021780",
                    "name": "Dina Epelboim"
                },
                {
                    "authorId": "2148906676",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "36743361",
                    "name": "Dakuo Wang"
                },
                {
                    "authorId": "2171374727",
                    "name": "Lucy Yip"
                },
                {
                    "authorId": "1402680837",
                    "name": "L. Ein-Dor"
                },
                {
                    "authorId": "2839128",
                    "name": "Lena Dankin"
                },
                {
                    "authorId": "2627091",
                    "name": "Ilya Shnayderman"
                },
                {
                    "authorId": "48361424",
                    "name": "R. Aharonov"
                },
                {
                    "authorId": "1573872877",
                    "name": "Yunyao Li"
                },
                {
                    "authorId": "2089779821",
                    "name": "Naftali Liberman"
                },
                {
                    "authorId": "2180023871",
                    "name": "Philip Levin Slesarev"
                },
                {
                    "authorId": "2104532982",
                    "name": "Gwilym Newton"
                },
                {
                    "authorId": "1405604910",
                    "name": "Shila Ofek-Koifman"
                },
                {
                    "authorId": "1766595",
                    "name": "N. Slonim"
                },
                {
                    "authorId": "1722434",
                    "name": "Yoav Katz"
                }
            ]
        },
        {
            "paperId": "0ce581e6ce34c31d342f56d1614e01d0a6dd3d4b",
            "title": "Development of an Enterprise-Grade Contract Understanding System",
            "abstract": "Contracts are arguably the most important type of business documents. Despite their significance in business, legal contract review largely remains an arduous, expensive and manual process. In this paper, we describe TECUS: a commercial system designed and deployed for contract understanding and used by a wide range of enterprise users for the past few years. We reflect on the challenges and design decisions when building TECUS. We also summarize the data science life cycle of TECUS and share lessons learned.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31820819",
                    "name": "A. Agarwal"
                },
                {
                    "authorId": "1779119",
                    "name": "Laura Chiticariu"
                },
                {
                    "authorId": "2101315244",
                    "name": "Poornima Chozhiyath Raman"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                },
                {
                    "authorId": "2438612",
                    "name": "D. Ghazi"
                },
                {
                    "authorId": "2110762290",
                    "name": "Ankush Gupta"
                },
                {
                    "authorId": "52205085",
                    "name": "Shanmukha C. Guttula"
                },
                {
                    "authorId": "2208580",
                    "name": "Yannis Katsis"
                },
                {
                    "authorId": "3252167",
                    "name": "R. Krishnamurthy"
                },
                {
                    "authorId": "1573872877",
                    "name": "Yunyao Li"
                },
                {
                    "authorId": "2101320978",
                    "name": "Shubham Mudgal"
                },
                {
                    "authorId": "1821191",
                    "name": "Vitobha Munigala"
                },
                {
                    "authorId": "2057607891",
                    "name": "Nicholas Phan"
                },
                {
                    "authorId": "2101315127",
                    "name": "Dhaval Sonawane"
                },
                {
                    "authorId": "40615569",
                    "name": "S. Srinivasan"
                },
                {
                    "authorId": "39935833",
                    "name": "Sudarshan Rangarajan Thitte"
                },
                {
                    "authorId": "2537800",
                    "name": "M. Vasa"
                },
                {
                    "authorId": "1710483",
                    "name": "R. Venkatachalam"
                },
                {
                    "authorId": "2101322275",
                    "name": "Vinitha Yaski"
                },
                {
                    "authorId": "2115718238",
                    "name": "Huaiyu Zhu"
                }
            ]
        },
        {
            "paperId": "2b9d2983c0012796d214273612f8e02906d0ca06",
            "title": "Learning to Robustly Aggregate Labeling Functions for Semi-supervised Data Programming",
            "abstract": "A critical bottleneck in supervised machine learning is the need for large amounts of labeled data which is expensive and time-consuming to obtain. Although a small amount of labeled data cannot be used to train a model, it can be used effectively for the generation of humaninterpretable labeling functions (LFs). These LFs, in turn, have been used to generate a large amount of additional noisy labeled data in a paradigm that is now commonly referred to as data programming. Previous methods of generating LFs do not attempt to use the given labeled data further to train a model, thus missing opportunities for improving performance. Additionally, since the LFs are generated automatically, they are likely to be noisy, and naively aggregating these LFs can lead to suboptimal results. In this work, we propose an LF-based bi-level optimization framework WISDOM to solve these two critical limitations. WISDOM learns a joint model on the (same) labeled dataset used for LF induction along with any unlabeled data in a semi-supervised manner, and more critically, reweighs each LF according to its goodness, influencing its contribution to the semi-supervised loss using a robust bi-level optimization algorithm. We show that WISDOM significantly outperforms prior approaches on several text classification datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46202956",
                    "name": "Ayush Maheshwari"
                },
                {
                    "authorId": "2112203718",
                    "name": "Krishnateja Killamsetty"
                },
                {
                    "authorId": "150114500",
                    "name": "Ganesh Ramakrishnan"
                },
                {
                    "authorId": "145074006",
                    "name": "Rishabh K. Iyer"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                },
                {
                    "authorId": "145378077",
                    "name": "Lucian Popa"
                }
            ]
        },
        {
            "paperId": "3926f776c27c1ca63b9433b162993fa7089697f0",
            "title": "Explainability for Natural Language Processing",
            "abstract": "This lecture-style tutorial, which mixes in an interactive literature browsing component, is intended for the many researchers and practitioners working with text data and on applications of natural language processing (NLP) in data science and knowledge discovery. The focus of the tutorial is on the issues of transparency and interpretability as they relate to building models for text and their applications to knowledge discovery. As black-box models have gained popularity for a broad range of tasks in recent years, both the research and industry communities have begun developing new techniques to render them more transparent and interpretable. Reporting from an interdisciplinary team of social science, human-computer interaction (HCI), and NLP/knowledge management researchers, our tutorial has two components: an introduction to explainable AI (XAI) in the NLP domain and a review of the state-of-the-art research; and findings from a qualitative interview study of individuals working on real-world NLP projects as they are applied to various knowledge extraction and discovery at a large, multinational technology and consulting corporation. The first component will introduce core concepts related to explainability in NLP. Then, we will discuss explainability for NLP tasks and report on a systematic literature review of the state-of-the-art literature in AI, NLP and HCI conferences. The second component reports on our qualitative interview study, which identifies practical challenges and concerns that arise in real-world development projects that require the modeling and understanding of text data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                },
                {
                    "authorId": "3056989",
                    "name": "Shipi Dhanorkar"
                },
                {
                    "authorId": "1573872877",
                    "name": "Yunyao Li"
                },
                {
                    "authorId": "145378077",
                    "name": "Lucian Popa"
                },
                {
                    "authorId": "2053225294",
                    "name": "Kun Qian"
                },
                {
                    "authorId": "1720067",
                    "name": "Anbang Xu"
                }
            ]
        },
        {
            "paperId": "46efbc5108e3176ebb6f4df74d38ba16f6d3ed0c",
            "title": "SemEval-2021 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS)",
            "abstract": "Understanding tables is an important and relevant task that involves understanding table structure as well as being able to compare and contrast information within cells. In this paper, we address this challenge by presenting a new dataset and tasks that addresses this goal in a shared task in SemEval 2020 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS). Our dataset contains 981 manually-generated tables and an auto-generated dataset of 1980 tables providing over 180K statement and over 16M evidence annotations. SEM-TAB-FACTS featured two sub-tasks. In sub-task A, the goal was to determine if a statement is supported, refuted or unknown in relation to a table. In sub-task B, the focus was on identifying the specific cells of a table that provide evidence for the statement. 69 teams signed up to participate in the task with 19 successful submissions to subtask A and 12 successful submissions to subtask B. We present our results and main findings from the competition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31856827",
                    "name": "N. Wang"
                },
                {
                    "authorId": "40665082",
                    "name": "Diwakar Mahajan"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                },
                {
                    "authorId": "144063596",
                    "name": "Sara Rosenthal"
                }
            ]
        }
    ]
}