{
    "authorId": "2109085368",
    "papers": [
        {
            "paperId": "0872173a4358b11f1fdbed18a432aa9cbe8bf67d",
            "title": "Lq Lower Bounds on Distributed Estimation via Fisher Information",
            "abstract": "Van Trees inequality, also known as the Bayesian Cram\u00e9r- Rao lower bound, is a powerful tool for establishing lower bounds for minimax estimation through Fisher information. It easily adapts to different statistical models and often yields tight bounds. Recently, its application has been extended to distributed estimation with privacy and communication constraints where it yields order-wise optimal minimax lower bounds for various parametric tasks under squared $L_{2}$ loss. However, a widely perceived drawback of the van Trees inequality is that it is limited to squared $L_{2}$ loss. The goal of this paper is to dispel that perception by introducing a strengthened version of the van Trees inequality that applies to general $L_{q}$ loss functions by building on the Efroimovich's inequality - a lesser-known entropic inequality dating back to the $1970\\mathrm{s}$. We then apply the generalized van Trees inequality to lower bound $L_{q}$ loss in distributed minimax estimation under communication and local differential privacy constraints. This leads to lower bounds for $L_{q}$ loss that apply to sequentially interactive and blackboard communication protocols. Additionally, we show how the generalized van Trees inequality can be used to obtain local and non-asymptotic minimax results that capture the hardness of estimating each instance at finite sample sizes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109085368",
                    "name": "Wei-Ning Chen"
                },
                {
                    "authorId": "2266472645",
                    "name": "Ayfer \u00d6zg\u00fcr"
                }
            ]
        },
        {
            "paperId": "282b3f1e3fec0ad906bc7369de6b3eab30226e53",
            "title": "Over-the-Air Histogram Estimation",
            "abstract": "We consider the problem of secure histogram es-timation, where $n$ users hold private items xi from a size-d domain and a server aims to estimate the histogram of the user items. Previous results utilizing orthogonal communication schemes have shown that this problem can be solved securely with a total communication cost of O(n2log(d)) bits by hiding each item xi with a mask. In this paper, we offer a different approach to achieving secure aggregation. Instead of masking the data, our scheme protects individuals by aggregating their messages via a multiple-access channel. A naive communication scheme over the multiple-access channel requires $d$ channel uses, which is generally worse than the O(n21og(d)) bits communication cost of the prior art in the most relevant regime $d$ >> $n$. Instead, we propose a new scheme that we call Over-the-Air Group Testing (AirG T) which uses group testing codes to solve the histogram estimation problem in O(n log(d)) channel uses. AirGT reconstructs the histogram exactly with a vanishing probability of error Perror= O(d-T) that drops exponentially in the number of channel uses $T$.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065817189",
                    "name": "Henrik Hellstr\u00f6m"
                },
                {
                    "authorId": "2316834141",
                    "name": "Jiwon Jeong"
                },
                {
                    "authorId": "2109085368",
                    "name": "Wei-Ning Chen"
                },
                {
                    "authorId": "2266472645",
                    "name": "Ayfer \u00d6zg\u00fcr"
                },
                {
                    "authorId": "2248167519",
                    "name": "Viktoria Fodor"
                },
                {
                    "authorId": "2285059879",
                    "name": "Carlo Fischione"
                }
            ]
        },
        {
            "paperId": "296e87dfabab50505685574bb0698b35e1244a48",
            "title": "$L_q$ Lower Bounds on Distributed Estimation via Fisher Information",
            "abstract": "Van Trees inequality, also known as the Bayesian Cram\\'er-Rao lower bound, is a powerful tool for establishing lower bounds for minimax estimation through Fisher information. It easily adapts to different statistical models and often yields tight bounds. Recently, its application has been extended to distributed estimation with privacy and communication constraints where it yields order-wise optimal minimax lower bounds for various parametric tasks under squared $L_2$ loss. However, a widely perceived drawback of the van Trees inequality is that it is limited to squared $L_2$ loss. The goal of this paper is to dispel that perception by introducing a strengthened version of the van Trees inequality that applies to general $L_q$ loss functions by building on the Efroimovich's inequality -- a lesser-known entropic inequality dating back to the 1970s. We then apply the generalized van Trees inequality to lower bound $L_q$ loss in distributed minimax estimation under communication and local differential privacy constraints. This leads to lower bounds for $L_q$ loss that apply to sequentially interactive and blackboard communication protocols. Additionally, we show how the generalized van Trees inequality can be used to obtain \\emph{local} and \\emph{non-asymptotic} minimax results that capture the hardness of estimating each instance at finite sample sizes.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2109085368",
                    "name": "Wei-Ning Chen"
                },
                {
                    "authorId": "9499046",
                    "name": "Ayfer Ozgur"
                }
            ]
        },
        {
            "paperId": "934d295b01d8fc5b039491e9b8745950b690a221",
            "title": "Universal Exact Compression of Differentially Private Mechanisms",
            "abstract": "To reduce the communication cost of differential privacy mechanisms, we introduce a novel construction, called Poisson private representation (PPR), designed to compress and simulate any local randomizer while ensuring local differential privacy. Unlike previous simulation-based local differential privacy mechanisms, PPR exactly preserves the joint distribution of the data and the output of the original local randomizer. Hence, the PPR-compressed privacy mechanism retains all desirable statistical properties of the original privacy mechanism such as unbiasedness and Gaussianity. Moreover, PPR achieves a compression size within a logarithmic gap from the theoretical lower bound. Using the PPR, we give a new order-wise trade-off between communication, accuracy, central and local differential privacy for distributed mean estimation. Experiment results on distributed mean estimation show that PPR consistently gives a better trade-off between communication, accuracy and central differential privacy compared to the coordinate subsampled Gaussian mechanism, while also providing local differential privacy.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2283783294",
                    "name": "Yanxiao Liu"
                },
                {
                    "authorId": "2109085368",
                    "name": "Wei-Ning Chen"
                },
                {
                    "authorId": "2266472645",
                    "name": "Ayfer \u00d6zg\u00fcr"
                },
                {
                    "authorId": "2281716099",
                    "name": "Cheuk Ting Li"
                }
            ]
        },
        {
            "paperId": "1630ec8588eadd6f5af3a2fc7bd2f8902bddbac1",
            "title": "Exact Optimality of Communication-Privacy-Utility Tradeoffs in Distributed Mean Estimation",
            "abstract": "We study the mean estimation problem under communication and local differential privacy constraints. While previous work has proposed \\emph{order}-optimal algorithms for the same problem (i.e., asymptotically optimal as we spend more bits), \\emph{exact} optimality (in the non-asymptotic setting) still has not been achieved. In this work, we take a step towards characterizing the \\emph{exact}-optimal approach in the presence of shared randomness (a random variable shared between the server and the user) and identify several conditions for \\emph{exact} optimality. We prove that one of the conditions is to utilize a rotationally symmetric shared random codebook. Based on this, we propose a randomization mechanism where the codebook is a randomly rotated simplex -- satisfying the properties of the \\emph{exact}-optimal codebook. The proposed mechanism is based on a $k$-closest encoding which we prove to be \\emph{exact}-optimal for the randomly rotated simplex codebook.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1707440322",
                    "name": "Berivan Isik"
                },
                {
                    "authorId": "2109085368",
                    "name": "Wei-Ning Chen"
                },
                {
                    "authorId": "9499046",
                    "name": "Ayfer Ozgur"
                },
                {
                    "authorId": "4820756",
                    "name": "T. Weissman"
                },
                {
                    "authorId": "3268846",
                    "name": "Albert No"
                }
            ]
        },
        {
            "paperId": "227ecc702ff3b63a62755d77ac6ac72902d558cd",
            "title": "Training Generative Models From Privatized Data via Entropic Optimal Transport",
            "abstract": "Local differential privacy is a powerful method for privacy-preserving data collection. In this paper, we develop a framework for training Generative Adversarial Networks (GANs) on differentially privatized data. We show that entropic regularization of optimal transport \u2013 a popular regularization method in the literature that has often been leveraged for its computational benefits \u2013 enables the generator to learn the raw (unprivatized) data distribution even though it only has access to privatized samples. We prove that at the same time this leads to fast statistical convergence at the parametric rate. This shows that entropic regularization of optimal transport uniquely enables the mitigation of both the effects of privatization noise and the curse of dimensionality in statistical convergence. We provide experimental evidence to support the efficacy of our framework in practice.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2297079625",
                    "name": "Daria Reshetova"
                },
                {
                    "authorId": "2109085368",
                    "name": "Wei-Ning Chen"
                },
                {
                    "authorId": "145326369",
                    "name": "Ayfer \u00d6zg\u00fcr"
                }
            ]
        },
        {
            "paperId": "41c8a2a6ca140eeb9da52cb852e49d5ce7ffa162",
            "title": "Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection",
            "abstract": "GNNs can inadvertently expose sensitive user information and interactions through their model predictions. To address these privacy concerns, Differential Privacy (DP) protocols are employed to control the trade-off between provable privacy protection and model utility. Applying standard DP approaches to GNNs directly is not advisable due to two main reasons. First, the prediction of node labels, which relies on neighboring node attributes through graph convolutions, can lead to privacy leakage. Second, in practical applications, the privacy requirements for node attributes and graph topology may differ. In the latter setting, existing DP-GNN models fail to provide multigranular trade-offs between graph topology privacy, node attribute privacy, and GNN utility. To address both limitations, we propose a new framework termed Graph Differential Privacy (GDP), specifically tailored to graph learning. GDP ensures both provably private model parameters as well as private predictions. Additionally, we describe a novel unified notion of graph dataset adjacency to analyze the properties of GDP for different levels of graph topology privacy. Our findings reveal that DP-GNNs, which rely on graph convolutions, not only fail to meet the requirements for multigranular graph topology privacy but also necessitate the injection of DP noise that scales at least linearly with the maximum node degree. In contrast, our proposed Differentially Private Decoupled Graph Convolutions (DPDGCs) represent a more flexible and efficient alternative to graph convolutions that still provides the necessary guarantees of GDP. To validate our approach, we conducted extensive experiments on seven node classification benchmarking and illustrative synthetic datasets. The results demonstrate that DPDGCs significantly outperform existing DP-GNNs in terms of privacy-utility trade-offs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "121307942",
                    "name": "Eli Chien"
                },
                {
                    "authorId": "2109085368",
                    "name": "Wei-Ning Chen"
                },
                {
                    "authorId": "2084643427",
                    "name": "Chao Pan"
                },
                {
                    "authorId": "1561672016",
                    "name": "Pan Li"
                },
                {
                    "authorId": "9499046",
                    "name": "Ayfer Ozgur"
                },
                {
                    "authorId": "1743831",
                    "name": "O. Milenkovic"
                }
            ]
        },
        {
            "paperId": "5e7c0f64a0f228362c69951c2157245ff227258d",
            "title": "Federated Experiment Design under Distributed Differential Privacy",
            "abstract": "Experiment design has a rich history dating back over a century and has found many critical applications across various fields since then. The use and collection of users' data in experiments often involve sensitive personal information, so additional measures to protect individual privacy are required during data collection, storage, and usage. In this work, we focus on the rigorous protection of users' privacy (under the notion of differential privacy (DP)) while minimizing the trust toward service providers. Specifically, we consider the estimation of the average treatment effect (ATE) under DP, while only allowing the analyst to collect population-level statistics via secure aggregation, a distributed protocol enabling a service provider to aggregate information without accessing individual data. Although a vital component in modern A/B testing workflows, private distributed experimentation has not previously been studied. To achieve DP, we design local privatization mechanisms that are compatible with secure aggregation and analyze the utility, in terms of the width of confidence intervals, both asymptotically and non-asymptotically. We show how these mechanisms can be scaled up to handle the very large number of participants commonly found in practice. In addition, when introducing DP noise, it is imperative to cleverly split privacy budgets to estimate both the mean and variance of the outcomes and carefully calibrate the confidence intervals according to the DP noise. Last, we present comprehensive experimental evaluations of our proposed schemes and show the privacy-utility trade-offs in experiment design.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2109085368",
                    "name": "Wei-Ning Chen"
                },
                {
                    "authorId": "2254587151",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2528900",
                    "name": "Akash Bharadwaj"
                },
                {
                    "authorId": "2265647990",
                    "name": "Peter Romov"
                },
                {
                    "authorId": "2266472645",
                    "name": "Ayfer \u00d6zg\u00fcr"
                }
            ]
        },
        {
            "paperId": "67c317418d1a7fa035586ced0f4acdd1ec1cd778",
            "title": "Noisy Adaptive Group Testing for Community-Oriented Models",
            "abstract": "We consider the group testing problem over probabilistic community-oriented infection models, which have attracted significant attention in the wake of the COVID-19 pandemic. To the best of our knowledge, existing theoretical results on the complexity of group testing in such settings are derived under the assumption that tests are noiseless. We present novel upper and lower bounds for the noisy case, focusing on adaptive group testing schemes tailored to the community structure of the population. For the achievability result, we devise an algorithm which incorporates knowledge of the community structure into a noisy binary search procedure from [1]. Our algorithm exhibits favorable performance in the context of the recently-introduced stochastic block infection model [2]. Furthermore, our lower bound applies to any adaptive algorithm, any probabilistic infection model, and any (noisy or noiseless) testing model satisfying certain natural criteria, and thus can be of independent interest.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051810",
                    "name": "Surin Ahn"
                },
                {
                    "authorId": "2109085368",
                    "name": "Wei-Ning Chen"
                },
                {
                    "authorId": "145326369",
                    "name": "Ayfer \u00d6zg\u00fcr"
                }
            ]
        },
        {
            "paperId": "e9066836c4fbbfe241a8d27783e83c567787f9cd",
            "title": "Privacy Amplification via Compression: Achieving the Optimal Privacy-Accuracy-Communication Trade-off in Distributed Mean Estimation",
            "abstract": "Privacy and communication constraints are two major bottlenecks in federated learning (FL) and analytics (FA). We study the optimal accuracy of mean and frequency estimation (canonical models for FL and FA respectively) under joint communication and $(\\varepsilon, \\delta)$-differential privacy (DP) constraints. We show that in order to achieve the optimal error under $(\\varepsilon, \\delta)$-DP, it is sufficient for each client to send $\\Theta\\left( n \\min\\left(\\varepsilon, \\varepsilon^2\\right)\\right)$ bits for FL and $\\Theta\\left(\\log\\left( n\\min\\left(\\varepsilon, \\varepsilon^2\\right) \\right)\\right)$ bits for FA to the server, where $n$ is the number of participating clients. Without compression, each client needs $O(d)$ bits and $\\log d$ bits for the mean and frequency estimation problems respectively (where $d$ corresponds to the number of trainable parameters in FL or the domain size in FA), which means that we can get significant savings in the regime $ n \\min\\left(\\varepsilon, \\varepsilon^2\\right) = o(d)$, which is often the relevant regime in practice. Our algorithms leverage compression for privacy amplification: when each client communicates only partial information about its sample, we show that privacy can be amplified by randomly selecting the part contributed by each client.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2109085368",
                    "name": "Wei-Ning Chen"
                },
                {
                    "authorId": "2112739207",
                    "name": "Danni Song"
                },
                {
                    "authorId": "145326369",
                    "name": "Ayfer \u00d6zg\u00fcr"
                },
                {
                    "authorId": "3115341",
                    "name": "P. Kairouz"
                }
            ]
        }
    ]
}