{
    "authorId": "1749590",
    "papers": [
        {
            "paperId": "704d9cec55737e4e49549d28971df9921f9fedeb",
            "title": "ATL-BP: A Student Engagement Dataset and Model for Affect Transfer Learning for Behavior Prediction",
            "abstract": "We propose a video-based transfer learning approach for predicting problem outcomes of students working with an intelligent tutoring system (ITS) by analyzing their faces and gestures. The ability to predict such outcomes enables tutoring systems to adjust interventions and ultimately yield improved student learning. We collected and released a labeled dataset of 2,749 problem-solving interaction samples of 54 students working with an intelligent online math tutor. Our transfer-learning challenge was then to design a representation in the source domain of images obtained from the Internet for facial expression analysis, and transfer this learned representation for human behavior prediction in the domain of webcam videos of students in a classroom environment. We developed a novel facial affect representation and a user-personalized training scheme that unlocks the potential of this representation. We designed several variants of a recurrent neural network that models the temporal structure of video sequences. Our final model, named ATL-BP for Affect Transfer Learning for Behavior Prediction, achieves a relative increase in the mean F-score of 50% over the state-of-the-art method on this new dataset. We also propose an additional set of annotations to predict students\u2019 engagement while solving a specific problem, and present models that can predict such engagement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31601235",
                    "name": "Nataniel Ruiz"
                },
                {
                    "authorId": "2149179009",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "3408856",
                    "name": "Danielle A. Allessio"
                },
                {
                    "authorId": "47801182",
                    "name": "Mona Jalal"
                },
                {
                    "authorId": "1702222",
                    "name": "Ajjen Joshi"
                },
                {
                    "authorId": "145934474",
                    "name": "T. Murray"
                },
                {
                    "authorId": "1703932",
                    "name": "John J. Magee"
                },
                {
                    "authorId": "2186698768",
                    "name": "Kevin Manuel Delgado"
                },
                {
                    "authorId": "1852308",
                    "name": "Vitaly Ablavsky"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                },
                {
                    "authorId": "145323574",
                    "name": "I. Arroyo"
                },
                {
                    "authorId": "3325410",
                    "name": "B. Woolf"
                },
                {
                    "authorId": "2141772809",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                }
            ]
        },
        {
            "paperId": "82122ba1ecf64fce607b5f73567d9e046209cbdf",
            "title": "Video Frame Interpolation With Many-to-Many Splatting and Spatial Selective Refinement",
            "abstract": "In this work, we first propose a fully differentiable Many-to-Many (M2M) splatting framework to interpolate frames efficiently. Given a frame pair, we estimate multiple bidirectional flows to directly forward warp the pixels to the desired time step before fusing any overlapping pixels. In doing so, each source pixel renders multiple target pixels and each target pixel can be synthesized from a larger area of visual context, establishing a many-to-many splatting scheme with robustness to undesirable artifacts. For each input frame pair, M2M has a minuscule computational overhead when interpolating an arbitrary number of in-between frames, hence achieving fast multi-frame interpolation. However, directly warping and fusing pixels in the intensity domain is sensitive to the quality of motion estimation and may suffer from less effective representation capacity. To improve interpolation accuracy, we further extend an M2M++ framework by introducing a flexible Spatial Selective Refinement (SSR) component, which allows for trading computational efficiency for interpolation quality and vice versa. Instead of refining the entire interpolated frame, SSR only processes difficult regions selected under the guidance of an estimated error map, thereby avoiding redundant computation. Evaluation on multiple benchmark datasets shows that our method is able to improve the efficiency while maintaining competitive video interpolation quality, and it can be adjusted to use more or less compute as needed.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261460139",
                    "name": "Ping Hu"
                },
                {
                    "authorId": "39644974",
                    "name": "Simon Niklaus"
                },
                {
                    "authorId": "2261475436",
                    "name": "Lu Zhang"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                },
                {
                    "authorId": "2142735650",
                    "name": "Kate Saenko"
                }
            ]
        },
        {
            "paperId": "8a570041fb80b01d0684e84c39c04434dc25a506",
            "title": "Practical Disruption of Image Translation Deepfake Networks",
            "abstract": "By harnessing the latest advances in deep learning, image-to-image translation architectures have recently achieved impressive capabilities. Unfortunately, the growing representational power of these architectures has prominent unethical uses. Among these, the threats of (1) face manipulation (\"DeepFakes\") used for misinformation or pornographic use (2) \"DeepNude\" manipulations of body images to remove clothes from individuals, etc. Several works tackle the task of disrupting such image translation networks by inserting imperceptible adversarial attacks into the input image. Nevertheless, these works have limitations that may result in disruptions that are not practical in the real world. Specifically, most works generate disruptions in a white-box scenario, assuming perfect knowledge about the image translation network. The few remaining works that assume a black-box scenario require a large number of queries to successfully disrupt the adversary's image translation network. In this work we propose Leaking Transferable Perturbations (LTP), an algorithm that significantly reduces the number of queries needed to disrupt an image translation network by dynamically re-purposing previous disruptions into new query efficient disruptions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31601235",
                    "name": "Nataniel Ruiz"
                },
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "3011497",
                    "name": "Cihang Xie"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                }
            ]
        },
        {
            "paperId": "94917c34a35ef3fd9719e54071d9d489c7da4823",
            "title": "DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition With Limited Annotations",
            "abstract": "Multi-label image recognition in the low-label regime is a task of great challenge and practical significance. Previous works have focused on learning the alignment between textual and visual spaces to compensate for limited image labels, yet may suffer from reduced accuracy due to the scarcity of high-quality multi-label annotations. In this research, we leverage the powerful alignment between textual and visual features pretrained with millions of auxiliary image-text pairs. We introduce an efficient and effective framework called Evidence-guided Dual Context Optimization (DualCoOp++), which serves as a unified approach for addressing partial-label and zero-shot multi-label recognition. In DualCoOp++ we separately encode evidential, positive, and negative contexts for target classes as parametric components of the linguistic input (i.e., prompts). The evidential context aims to discover all the related visual content for the target class, and serves as guidance to aggregate positive and negative contexts from the spatial domain of the image, enabling better distinguishment between similar categories. Additionally, we introduce a Winner-Take-All module that promotes inter-class interaction during training, while avoiding the need for extra parameters and costs. As DualCoOp++ imposes minimal additional learnable overhead on the pretrained vision-language framework, it enables rapid adaptation to multi-label recognition tasks with limited annotations and even unseen classes. Experiments on standard multi-label recognition benchmarks across two challenging low-label settings demonstrate the superior performance of our approach compared to state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067770617",
                    "name": "Ping Hu"
                },
                {
                    "authorId": "2125994125",
                    "name": "Ximeng Sun"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                }
            ]
        },
        {
            "paperId": "a7719bfbdfc537eb6c9af4848edbff7db51e5aa4",
            "title": "The 7th AI City Challenge",
            "abstract": "The AI City Challenge\u2019s seventh edition emphasizes two domains at the intersection of computer vision and artificial intelligence - retail business and Intelligent Traffic Systems (ITS) - that have considerable untapped potential. The 2023 challenge had five tracks, which drew a record-breaking number of participation requests from 508 teams across 46 countries. Track 1 was a brand new track that focused on multi-target multi-camera (MTMC) people tracking, where teams trained and evaluated using both real and highly realistic synthetic data. Track 2 centered around natural-language-based vehicle track retrieval. Track 3 required teams to classify driver actions in naturalistic driving analysis. Track 4 aimed to develop an automated checkout system for retail stores using a single view camera. Track 5, another new addition, tasked teams with detecting violations of the helmet rule for motorcyclists. Two leader boards were released for submissions based on different methods: a public leader board for the contest where external private data wasn\u2019t allowed and a general leader board for all results submitted. The participating teams\u2019 top performances established strong baselines and even outperformed the state-of-the-art in the proposed challenge tracks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1780080",
                    "name": "M. Naphade"
                },
                {
                    "authorId": "2117010880",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2079573",
                    "name": "D. Anastasiu"
                },
                {
                    "authorId": "2212696732",
                    "name": "Zhenghang Tang"
                },
                {
                    "authorId": "9323852",
                    "name": "Ming-Ching Chang"
                },
                {
                    "authorId": "1390925314",
                    "name": "Yue Yao"
                },
                {
                    "authorId": "2149970164",
                    "name": "Liang Zheng"
                },
                {
                    "authorId": "2110138996",
                    "name": "Mohammed Shaiqur Rahman"
                },
                {
                    "authorId": "9201469",
                    "name": "Meenakshi S. Arya"
                },
                {
                    "authorId": "2154296768",
                    "name": "Anuj Sharma"
                },
                {
                    "authorId": "1993676410",
                    "name": "Qi Feng"
                },
                {
                    "authorId": "1852308",
                    "name": "Vitaly Ablavsky"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                },
                {
                    "authorId": "49845882",
                    "name": "Pranamesh Chakraborty"
                },
                {
                    "authorId": "2220548960",
                    "name": "Sanjita Prajapati"
                },
                {
                    "authorId": "2224462976",
                    "name": "Alice Li"
                },
                {
                    "authorId": "2227892270",
                    "name": "Shangru Li"
                },
                {
                    "authorId": "2227826546",
                    "name": "Krishna Kunadharaju"
                },
                {
                    "authorId": "2227940936",
                    "name": "Shenxin Jiang"
                },
                {
                    "authorId": "69416958",
                    "name": "Ramalingam Chellappa"
                }
            ]
        },
        {
            "paperId": "ccb65b9459a5016c0bd917848913a3c44c86a1d3",
            "title": "Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing",
            "abstract": "Vision transformers (ViTs) have significantly changed the computer vision landscape and have periodically exhibited superior performance in vision tasks compared to convolutional neural networks (CNNs). Although the jury is still out on which model type is superior, each has unique inductive biases that shape their learning and generalization performance. For example, ViTs have interesting properties with respect to early layer non-local feature dependence, as well as self-attention mechanisms which enhance learning flexibility, enabling them to ignore out-of-context image information more effectively. We hypothesize that this power to ignore out-of-context information (which we name $\\textit{patch selectivity}$), while integrating in-context information in a non-local manner in early layers, allows ViTs to more easily handle occlusion. In this study, our aim is to see whether we can have CNNs $\\textit{simulate}$ this ability of patch selectivity by effectively hardwiring this inductive bias using Patch Mixing data augmentation, which consists of inserting patches from another image onto a training image and interpolating labels between the two image classes. Specifically, we use Patch Mixing to train state-of-the-art ViTs and CNNs, assessing its impact on their ability to ignore out-of-context patches and handle natural occlusions. We find that ViTs do not improve nor degrade when trained using Patch Mixing, but CNNs acquire new capabilities to ignore out-of-context information and improve on occlusion benchmarks, leaving us to conclude that this training method is a way of simulating in CNNs the abilities that ViTs already possess. We will release our Patch Mixing implementation and proposed datasets for public use. Project page: https://arielnlee.github.io/PatchMixing/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220131789",
                    "name": "Ariel N. Lee"
                },
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "2220964246",
                    "name": "Janavi Kasera"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                },
                {
                    "authorId": "2142735650",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "31601235",
                    "name": "Nataniel Ruiz"
                }
            ]
        },
        {
            "paperId": "00ce21a2772446a70bbc2ae9feaba49577b92fa3",
            "title": "Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing",
            "abstract": "Modern deep neural networks tend to be evaluated on static test sets. One shortcoming of this is the fact that these deep neural networks cannot be easily evaluated for robustness issues with respect to specific scene variations. For example, it is hard to study the robustness of these networks to variations of object scale, object pose, scene lighting and 3D occlusions. The main reason is that collecting real datasets with fine-grained naturalistic variations of sufficient scale can be extremely time-consuming and expensive. In this work, we present Counterfactual Simulation Testing, a counterfactual framework that allows us to study the robustness of neural networks with respect to some of these naturalistic variations by building realistic synthetic scenes that allow us to ask counterfactual questions to the models, ultimately providing answers to questions such as\"Would your classification still be correct if the object were viewed from the top?\"or\"Would your classification still be correct if the object were partially occluded by another object?\". Our method allows for a fair comparison of the robustness of recently released, state-of-the-art Convolutional Neural Networks and Vision Transformers, with respect to these naturalistic variations. We find evidence that ConvNext is more robust to pose and scale variations than Swin, that ConvNext generalizes better to our simulated domain and that Swin handles partial occlusion better than ConvNext. We also find that robustness for all networks improves with network scale and with data scale and variety. We release the Naturalistic Variation Object Dataset (NVD), a large simulated dataset of 272k images of everyday objects with naturalistic variations such as object pose, scale, viewpoint, lighting and occlusions. Project page: https://counterfactualsimulation.github.io",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31601235",
                    "name": "Nataniel Ruiz"
                },
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "3011497",
                    "name": "Cihang Xie"
                },
                {
                    "authorId": "2142735650",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                }
            ]
        },
        {
            "paperId": "360d1fcbc86bba8f1f27d298dcbe6cdbc0a11789",
            "title": "A Unified Framework for Domain Adaptive Pose Estimation",
            "abstract": "While pose estimation is an important computer vision task, it requires expensive annotation and suffers from domain shift. In this paper, we investigate the problem of domain adaptive 2D pose estimation that transfers knowledge learned on a synthetic source domain to a target domain without supervision. While several domain adaptive pose estimation models have been proposed recently, they are not generic but only focus on either human pose or animal pose estimation, and thus their effectiveness is somewhat limited to specific scenarios. In this work, we propose a unified framework that generalizes well on various domain adaptive pose estimation problems. We propose to align representations using both input-level and output-level cues (pixels and pose labels, respectively), which facilitates the knowledge transfer from the source domain to the unlabeled target domain. Our experiments show that our method achieves state-of-the-art performance under various domain shifts. Our method outperforms existing baselines on human pose estimation by up to 4.5 percent points (pp), hand pose estimation by up to 7.4 pp, and animal pose estimation by up to 4.8 pp for dogs and 3.3 pp for sheep. These results suggest that our method is able to mitigate domain shift on diverse tasks and even unseen domains and objects (e.g., trained on horse and tested on dog). Our code will be publicly available at: https://github.com/VisionLearningGroup/UDA_PoseEstimation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Donghyun Kim"
                },
                {
                    "authorId": "9359517",
                    "name": "Kaihong Wang"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                }
            ]
        },
        {
            "paperId": "81c7627c0e52a1379e4cec42809aed41c25a2722",
            "title": "Many-to-many Splatting for Efficient Video Frame Interpolation",
            "abstract": "Motion-based video frame interpolation commonly relies on optical flow to warp pixels from the inputs to the desired interpolation instant. Yet due to the inherent challenges of motion estimation (e.g. occlusions and discontinuities), most state-of-the-art interpolation approaches require subsequent refinement of the warped result to generate satisfying outputs, which drastically decreases the efficiency for multi-frame interpolation. In this work, we propose a fully differentiable Many-to-Many (M2M) splatting framework to interpolate frames efficiently. Specifically, given a frame pair, we estimate multiple bidirectional flows to directly forward warp the pixels to the desired time step, and then fuse any overlapping pixels. In doing so, each source pixel renders multiple target pixels and each target pixel can be synthesized from a larger area of visual context. This establishes a many-to-many splatting scheme with robustness to artifacts like holes. Moreover, for each input frame pair, M2M only performs motion estimation once and has a minuscule computational overhead when interpolating an arbitrary number of in-between frames, hence achieving fast multi-frame interpolation. We conducted extensive experiments to analyze M2M, and found that it significantly improves the efficiency while maintaining high effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067770642",
                    "name": "Ping Hu"
                },
                {
                    "authorId": "39644974",
                    "name": "Simon Niklaus"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                },
                {
                    "authorId": "2142735650",
                    "name": "Kate Saenko"
                }
            ]
        },
        {
            "paperId": "b168b1e138f992f3828fdb9d04694a0288a5bbf4",
            "title": "Leveraging Geometric Structure for Label-Efficient Semi-Supervised Scene Segmentation",
            "abstract": "Label-efficient scene segmentation aims to achieve effective per-pixel classification with reduced labeling effort. Recent approaches for this task focus on leveraging unlabelled images by formulating consistency regularization or pseudo labels for individual pixels. Yet most of these methods ignore the 3D geometric structures naturally conveyed by image scenes, which is free for enhancing training segmentation models with better discrimination of image details. In this work, we present a novel Geometric Structure Refinement (GSR) framework to explicitly exploit the geometric structures of image scenes to enhance the semi-supervised training of segmentation models. In the training phase, we generate initial dense pseudo labels based on fast and coarse annotations, and then utilize the free unsupervised 3D reconstruction of the image scene to calibrate the dense pseudo labels with more reliable details. With the calibrated pseudo groundtruth, we are able to conveniently train any existing image segmentation models without increasing the costs of annotations or modifying the models\u2019 architectures. Moreover, we explore different strategies for allocating labeling effort in semi-supervised scene segmentation, and find that a combination of finely-labeled samples and coarsely-labeled samples performs better than the traditional dense-fine only annotations. Extensive experiments on datasets including Cityscapes and KITTI are conducted to evaluate our proposed methods. The results demonstrate that GSR can be easily applied to boost the performance of existing models like PSPNet, DeepLabv3+, etc with reduced annotations. With half of the annotation effort, GSR achieves 99% of the accuracy of its fully supervised state-of-the-art counterparts.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2067770617",
                    "name": "Ping Hu"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                },
                {
                    "authorId": "2142735650",
                    "name": "Kate Saenko"
                }
            ]
        }
    ]
}