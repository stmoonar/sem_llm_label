{
    "authorId": "145028030",
    "papers": [
        {
            "paperId": "5156b3be33abbc3267b2d41e35537bf33c149420",
            "title": "Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?",
            "abstract": "Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the generation of coherent sentences resembling human writing on a large scale, resulting in the creation of so-called deepfake texts. However, this progress poses security and privacy concerns, necessitating effective solutions for distinguishing deepfake texts from human-written ones. Although prior works studied humans\u2019 ability to detect deepfake texts, none has examined whether \u201ccollaboration\u201d among humans improves the detection of deepfake texts. In this study, to address this gap of understanding on deepfake texts, we conducted experiments with two groups: (1) nonexpert individuals from the AMT platform and (2) writing experts from the Upwork platform. The results demonstrate that collaboration among humans can potentially improve the detection of deepfake texts for both groups, increasing detection accuracies by 6.36% for non-experts and 12.76% for experts, respectively, compared to individuals\u2019 detection accuracies. We further analyze the explanations that humans used for detecting a piece of text as deepfake text, and find that the strongest indicator of deepfake texts is their lack of coherence and consistency. Our study provides useful insights for future tools and framework designs to facilitate the collaborative human detection of deepfake texts. The experiment datasets and AMT implementations are available at: https://github.com/huashen218/llm-deepfake-human-study.git",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150035131",
                    "name": "Adaku Uchendu"
                },
                {
                    "authorId": "2159644449",
                    "name": "Jooyoung Lee"
                },
                {
                    "authorId": "145028030",
                    "name": "Hua Shen"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                },
                {
                    "authorId": "2158951945",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "a58c5fec83bfaa48aec8637a9b86b89fbdcf33f4",
            "title": "Gentopia: A Collaborative Platform for Tool-Augmented LLMs",
            "abstract": "Augmented Language Models (ALMs) empower large language models with the ability to use tools, transforming them into intelligent agents for real-world interactions. However, most existing frameworks for ALMs, to varying degrees, are deficient in the following critical features: flexible customization, collaborative democratization, and holistic evaluation. We present gentopia, an ALM framework enabling flexible customization of agents through simple configurations, seamlessly integrating various language models, task formats, prompting modules, and plugins into a unified paradigm. Furthermore, we establish gentpool, a public platform enabling the registration and sharing of user-customized agents. Agents registered in gentpool are composable such that they can be assembled together for agent collaboration, advancing the democratization of artificial intelligence. To ensure high-quality agents, gentbench, an integral component of gentpool, is designed to thoroughly evaluate user-customized agents across diverse aspects such as safety, robustness, efficiency, etc. We release gentopia on Github and will continuously move forward.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2228288888",
                    "name": "Binfeng Xu"
                },
                {
                    "authorId": "14910774",
                    "name": "Xukun Liu"
                },
                {
                    "authorId": "145028030",
                    "name": "Hua Shen"
                },
                {
                    "authorId": "48269112",
                    "name": "Zeyu Han"
                },
                {
                    "authorId": "2110539208",
                    "name": "Yuhan Li"
                },
                {
                    "authorId": "2228468973",
                    "name": "Murong Yue"
                },
                {
                    "authorId": "2113952851",
                    "name": "Zhi-Ping Peng"
                },
                {
                    "authorId": "2183078810",
                    "name": "Yuchen Liu"
                },
                {
                    "authorId": "3366595",
                    "name": "Ziyu Yao"
                },
                {
                    "authorId": "2116459424",
                    "name": "Dongkuan Xu"
                }
            ]
        },
        {
            "paperId": "b50c27607d7a858297232310bbec9819ade875a8",
            "title": "SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks",
            "abstract": "Prompt tuning is a technology that tunes a small set of parameters to steer a pre-trained language model (LM) to directly generate the output for downstream tasks. Recently, prompt tuning has demonstrated its storage and computation efficiency in both natural language processing (NLP) and speech processing fields. These advantages have also revealed prompt tuning as a candidate approach to serving pre-trained LM for multiple tasks in a unified manner. For speech processing, SpeechPrompt shows its high parameter efficiency and competitive performance on a few speech classification tasks. However, whether SpeechPrompt is capable of serving a large number of tasks is unanswered. In this work, we propose SpeechPrompt v2, a prompt tuning framework capable of performing a wide variety of speech classification tasks, covering multiple languages and prosody-related tasks. The experiment result shows that SpeechPrompt v2 achieves performance on par with prior works with less than 0.15M trainable parameters in a unified framework.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2782886",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2267221266",
                    "name": "Yu-Kai Wang"
                },
                {
                    "authorId": "145028030",
                    "name": "Hua Shen"
                },
                {
                    "authorId": "2210186034",
                    "name": "Iu-thing Kang"
                },
                {
                    "authorId": "34646412",
                    "name": "W. Tseng"
                },
                {
                    "authorId": "2530311",
                    "name": "Shang-Wen Li"
                },
                {
                    "authorId": "1706104",
                    "name": "Hung-yi Lee"
                }
            ]
        },
        {
            "paperId": "d64c76b359c2cd73c554f23aad61de6e58c9215a",
            "title": "Parachute: Evaluating Interactive Human-LM Co-writing Systems",
            "abstract": "A surge of advances in language models (LMs) has led to significant interest in using LMs to build co-writing systems, in which humans and LMs interactively contribute to a shared writing artifact. However, there is a lack of studies assessing co-writing systems in interactive settings. We propose a human-centered evaluation framework, Parachute, for interactive co-writing systems. Parachute showcases an integrative view of interaction evaluation, where each evaluation aspect consists of categorized practical metrics. Furthermore, we present Parachute with a use case to demonstrate how to evaluate and compare co-writing systems using Parachute.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145028030",
                    "name": "Hua Shen"
                },
                {
                    "authorId": "35232494",
                    "name": "Tongshuang Sherry Wu"
                }
            ]
        },
        {
            "paperId": "e7003e2a313558e62d9aec3db4154d4efdd46016",
            "title": "ConvXAI : Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing",
            "abstract": "Despite a surge collection of XAI methods, users still struggle to obtain required AI explanations. Previous research suggests chatbots as dynamic solutions, but the effective design of conversational XAI agents for practical human needs remains under-explored. This paper focuses on Conversational XAI for AI-assisted scientific writing tasks. Drawing from human linguistic theories and formative studies, we identify four design rationales: \u201cmultifaceted\u201d, \u201ccontrollability\u201d, \u201cmix-initiative\u201d, \u201ccontext-aware drill-down\u201d. We incorporate them into an interactive prototype, ConvXAI 1, which facilitates heterogeneous AI explanations for scientific writing through dialogue. In two studies with 21 users, ConvXAI outperforms a GUI-based baseline on improving human-perceived understanding and writing improvement. The paper further discusses the practical human usage patterns in interacting with ConvXAI for scientific co-writing2.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145028030",
                    "name": "Hua Shen"
                },
                {
                    "authorId": "1414028847",
                    "name": "Huang Chieh-Yang"
                },
                {
                    "authorId": "35232494",
                    "name": "Tongshuang Sherry Wu"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                }
            ]
        },
        {
            "paperId": "ee805f55c98920f74d0182aaf136330a97b4123f",
            "title": "ScatterShot: Interactive In-context Example Curation for Text Transformation",
            "abstract": "The in-context learning capabilities of LLMs like GPT-3 allow annotators to customize an LLM to their specific tasks with a small number of examples. However, users tend to include only the most obvious patterns when crafting examples, resulting in underspecified in-context functions that fall short on unseen cases. Further, it is hard to know when \u201cenough\u201d examples have been included even for known patterns. In this work, we present ScatterShot, an interactive system for building high-quality demonstration sets for in-context learning. ScatterShot iteratively slices unlabeled data into task-specific patterns, samples informative inputs from underexplored or not-yet-saturated slices in an active learning manner, and helps users label more efficiently with the help of an LLM and the current example set. In simulation studies on two text perturbation scenarios, ScatterShot sampling improves the resulting few-shot functions by 4-5 percentage points over random sampling, with less variance as more examples are added. In a user study, ScatterShot greatly helps users in covering different patterns in the input space and labeling in-context examples more efficiently, resulting in better in-context learning and less user effort.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35232494",
                    "name": "Tongshuang Sherry Wu"
                },
                {
                    "authorId": "145028030",
                    "name": "Hua Shen"
                },
                {
                    "authorId": "1780531",
                    "name": "Daniel S. Weld"
                },
                {
                    "authorId": "1803140",
                    "name": "Jeffrey Heer"
                },
                {
                    "authorId": "78846919",
                    "name": "Marco Tulio Ribeiro"
                }
            ]
        },
        {
            "paperId": "f631c058c2c6a6e3fe720929b96959fd93351956",
            "title": "MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup",
            "abstract": "Current disfluency detection models focus on individual utterances each from a single speaker. However, numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns, hampering human readability and the performance of downstream NLP tasks. This study addresses these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken conversational transcripts and collecting a new dataset, MultiTurnCleanup1. We design a data labeling schema to collect the high-quality dataset and provide extensive data analysis. Furthermore, we leverage two modeling approaches for experimental evaluation as benchmarks for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145028030",
                    "name": "Hua Shen"
                },
                {
                    "authorId": "143689491",
                    "name": "V. Zayats"
                },
                {
                    "authorId": "2854301",
                    "name": "Johann C. Rocholl"
                },
                {
                    "authorId": "28265704",
                    "name": "D. D. Walker"
                },
                {
                    "authorId": "1745683",
                    "name": "D. Padfield"
                }
            ]
        },
        {
            "paperId": "28700560f7b6691a9421fa74bc20e40a011bc5d1",
            "title": "Improving Fairness in Speaker Verification via Group-Adapted Fusion Network",
            "abstract": "Modern speaker verification models use deep neural networks to encode utterance audio into discriminative embedding vectors. During the training process, these networks are typically optimized to differentiate arbitrary speakers. This learning process biases the learning of fine voice characteristics towards dominant demographic groups, which can lead to an unfair performance disparity across different groups. This is observed especially with underrepresented demographic groups sharing similar voice characteristics. In this work, we investigate the fairness of speaker verification models on controlled datasets with imbalanced gender distributions, providing direct evidence that model performance suffers for underrepresented groups. To mitigate this disparity we propose the group-adapted fusion network (GFN) architecture, a modular architecture based on group embedding adaptation and score fusion. We show that our method alleviates model unfairness by improving speaker verification both overall and for individual groups. Given imbalanced group representation in training, our proposed method achieves overall equal error rate (EER) reduction of 9.6% to 29.0% relative, reduces minority group EER by 13.7% to 18.6%, and results in 20.0% to 25.4% less EER disparity, compared to baselines. The approach is applicable to other types of training data skew in speaker recognition systems.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "145028030",
                    "name": "Hua Shen"
                },
                {
                    "authorId": "2128674950",
                    "name": "Yuguang Yang"
                },
                {
                    "authorId": "2148800312",
                    "name": "G. Sun"
                },
                {
                    "authorId": "2155885892",
                    "name": "Ryan Langman"
                },
                {
                    "authorId": "2056284949",
                    "name": "Eunjung Han"
                },
                {
                    "authorId": "1755472",
                    "name": "J. Droppo"
                },
                {
                    "authorId": "1762744",
                    "name": "A. Stolcke"
                }
            ]
        },
        {
            "paperId": "70fdbd25bc513577f0023b1436039cec975ff457",
            "title": "Identify, Guess and Reconstruct: Three Principles for Cloud Removal Task",
            "abstract": "Remote sensing images serve a significant role in earth observation to tackle climate change and post-disaster reconstruction concerns. However, optical images are obscured by clouds or haze, preventing precise earth observation; hence, cloud removal has been a hot topic among concerned scholars. The objective of this article is to make cloud removal more efficient and explicable by proposing three principles: identifying clouds, guessing objects beneath the clouds, and reconstructing the cloudy area. In addition, a modified dual contrastive learning Generative Adversarial Network is proposed based on these three principles by adding cloud detection and weight sharing strategy to obtain cloud semantics. In particular, we align two datasets by forming a quaternary sample pair that includes not only optical pictures and SAR images, but also region information for a more precise reconstruction. Our experiment results on the integrated dataset reveal the superiority of proposed method over previous cloud removal methods and the effectiveness of added modules through ablation experiments, with PSNR and SSIM values of 26.2 and 0.728, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2202021858",
                    "name": "Sibo Wu"
                },
                {
                    "authorId": "2110684424",
                    "name": "Mengqiu Xu"
                },
                {
                    "authorId": "153138926",
                    "name": "Ming Wu"
                },
                {
                    "authorId": "144585219",
                    "name": "Chuang Zhang"
                },
                {
                    "authorId": "145028030",
                    "name": "Hua Shen"
                }
            ]
        },
        {
            "paperId": "9f3540baf1f1e5bf3dd4efdd776d6e2ffc3f55e9",
            "title": "Are Shortest Rationales the Best Explanations for Human Understanding?",
            "abstract": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201cresponsible for\u201d corresponding output \u2014 to explain the model prediction, with the assumption that shorter rationales are more intuitive to humans. However, this assumption has yet to be validated. Is the shortest rationale indeed the most human-understandable? To answer this question, we design a self-explaining model, LimitedInk, which allows users to extract rationales at any target length. Compared to existing baselines, LimitedInk achieves compatible end-task performance and human-annotated rationale agreement, making it a suitable representation of the recent class of self-explaining models. We use LimitedInk to conduct a user study on the impact of rationale length, where we ask human judges to predict the sentiment label of documents based only on LimitedInk-generated rationales with different lengths. We show rationales that are too short do not help humans predict labels better than randomly masked text, suggesting the need for more careful design of the best human rationales.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145028030",
                    "name": "Hua Shen"
                },
                {
                    "authorId": "35232494",
                    "name": "Tongshuang Sherry Wu"
                },
                {
                    "authorId": "3138660",
                    "name": "Wenbo Guo"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                }
            ]
        }
    ]
}