{
    "authorId": "2280928438",
    "papers": [
        {
            "paperId": "5044cd6fa7f2ff4d2978cd956554ad38b52607f2",
            "title": "Correlation-aware Coarse-to-fine MLPs for Deformable Medical Image Registration",
            "abstract": "Deformable image registration is a fundamental step for medical image analysis. Recently, transformers have been used for registration and outperformed Convolutional Neural Networks (CNNs). Transformers can capture long-range dependence among image features, which have been shown beneficial for registration. However, due to the high computation/memory loads of self-attention, transformers are typically used at downsampled feature resolutions and cannot capture fine-grained long-range dependence at the full image resolution. This limits deformable registration as it necessitates precise dense correspondence between each image pixel. Multi-layer Perceptrons (MLPs) without self-attention are efficient in computation/memory usage, enabling the feasibility of capturing fine-grained long-range dependence at full resolution. Nevertheless, MLPs have not been extensively explored for image registration and are lacking the consideration of inductive bias crucial for medical registration tasks. In this study, we propose the first correlation-aware MLP-based registration network (CorrMLP) for deformable medical image registration. Our CorrMLP introduces a correlation-aware multi-window MLP block in a novel coarse-to-fine registration architecture, which captures fine-grained multi-range dependence to perform correlation-aware coarse-to-fine registration. Extensive experiments with seven public medical datasets show that our CorrMLP outperforms state-of-the-art deformable registration methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "49117537",
                    "name": "Lei Bi"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "87b2f50d4d09655185834c98d19db67a06e9fe03",
            "title": "3DPX: Progressive 2D-to-3D Oral Image Reconstruction with Hybrid MLP-CNN Networks",
            "abstract": "Panoramic X-ray (PX) is a prevalent modality in dental practice for its wide availability and low cost. However, as a 2D projection image, PX does not contain 3D anatomical information, and therefore has limited use in dental applications that can benefit from 3D information, e.g., tooth angular misa-lignment detection and classification. Reconstructing 3D structures directly from 2D PX has recently been explored to address limitations with existing methods primarily reliant on Convolutional Neural Networks (CNNs) for direct 2D-to-3D mapping. These methods, however, are unable to correctly infer depth-axis spatial information. In addition, they are limited by the in-trinsic locality of convolution operations, as the convolution kernels only capture the information of immediate neighborhood pixels. In this study, we propose a progressive hybrid Multilayer Perceptron (MLP)-CNN pyra-mid network (3DPX) for 2D-to-3D oral PX reconstruction. We introduce a progressive reconstruction strategy, where 3D images are progressively re-constructed in the 3DPX with guidance imposed on the intermediate recon-struction result at each pyramid level. Further, motivated by the recent ad-vancement of MLPs that show promise in capturing fine-grained long-range dependency, our 3DPX integrates MLPs and CNNs to improve the semantic understanding during reconstruction. Extensive experiments on two large datasets involving 464 studies demonstrate that our 3DPX outperforms state-of-the-art 2D-to-3D oral reconstruction methods, including standalone MLP and transformers, in reconstruction quality, and also im-proves the performance of downstream angular misalignment classification tasks.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2314773871",
                    "name": "Xiaoshuang Li"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2281291244",
                    "name": "Zimo Huang"
                },
                {
                    "authorId": "2280306478",
                    "name": "Lei Bi"
                },
                {
                    "authorId": "2281306199",
                    "name": "Eduardo Delamare"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "2241372738",
                    "name": "Bin Sheng"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "9d8bd7f38a90054635a976f7ced071780c432e4f",
            "title": "3DPX: Single Panoramic X-ray Analysis Guided by 3D Oral Structure Reconstruction",
            "abstract": "Panoramic X-ray (PX) is a prevalent modality in dentistry practice owing to its wide availability and low cost. However, as a 2D projection of a 3D structure, PX suffers from anatomical information loss and PX diagnosis is limited compared to that with 3D imaging modalities. 2D-to-3D reconstruction methods have been explored for the ability to synthesize the absent 3D anatomical information from 2D PX for use in PX image analysis. However, there are challenges in leveraging such 3D synthesized reconstructions. First, inferring 3D depth from 2D images remains a challenging task with limited accuracy. The second challenge is the joint analysis of 2D PX with its 3D synthesized counterpart, with the aim to maximize the 2D-3D synergy while minimizing the errors arising from the synthesized image. In this study, we propose a new method termed 3DPX - PX image analysis guided by 2D-to-3D reconstruction, to overcome these challenges. 3DPX consists of (i) a novel progressive reconstruction network to improve 2D-to-3D reconstruction and, (ii) a contrastive-guided bidirectional multimodality alignment module for 3D-guided 2D PX classification and segmentation tasks. The reconstruction network progressively reconstructs 3D images with knowledge imposed on the intermediate reconstructions at multiple pyramid levels and incorporates Multilayer Perceptrons to improve semantic understanding. The downstream networks leverage the reconstructed images as 3D anatomical guidance to the PX analysis through feature alignment, which increases the 2D-3D synergy with bidirectional feature projection and decease the impact of potential errors with contrastive guidance. Extensive experiments on two oral datasets involving 464 studies demonstrate that 3DPX outperforms the state-of-the-art methods in various tasks including 2D-to-3D reconstruction, PX classification and lesion segmentation.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2314773871",
                    "name": "Xiaoshuang Li"
                },
                {
                    "authorId": "2281291244",
                    "name": "Zimo Huang"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2281306199",
                    "name": "Eduardo Delamare"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "2280306478",
                    "name": "Lei Bi"
                },
                {
                    "authorId": "2323370873",
                    "name": "Bin Sheng"
                },
                {
                    "authorId": "2323703676",
                    "name": "Lingyong Jiang"
                },
                {
                    "authorId": "2323376308",
                    "name": "Bo Li"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "b27152219dcd97b3c6b4ffa84b135adff0af983a",
            "title": "Dynamic Traceback Learning for Medical Report Generation",
            "abstract": "Automated medical report generation has the potential to significantly reduce the workload associated with the time-consuming process of medical reporting. Recent generative representation learning methods have shown promise in integrating vision and language modalities for medical report generation. However, when trained end-to-end and applied directly to medical image-to-text generation, they face two significant challenges: i) difficulty in accurately capturing subtle yet crucial pathological details, and ii) reliance on both visual and textual inputs during inference, leading to performance degradation in zero-shot inference when only images are available. To address these challenges, this study proposes a novel multi-modal dynamic traceback learning framework (DTrace). Specifically, we introduce a traceback mechanism to supervise the semantic validity of generated content and a dynamic learning strategy to adapt to various proportions of image and text input, enabling text generation without strong reliance on the input from both modalities during inference. The learning of cross-modal knowledge is enhanced by supervising the model to recover masked semantic information from a complementary counterpart. Extensive experiments conducted on two benchmark datasets, IU-Xray and MIMIC-CXR, demonstrate that the proposed DTrace framework outperforms state-of-the-art methods for medical report generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281398403",
                    "name": "Shuchang Ye"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2209861054",
                    "name": "Mingjian Li"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "da8103f9c09f20db5433b917e0a3adc9e0556cdc",
            "title": "SGSeg: Enabling Text-free Inference in Language-guided Segmentation of Chest X-rays via Self-guidance",
            "abstract": "Segmentation of infected areas in chest X-rays is pivotal for facilitating the accurate delineation of pulmonary structures and pathological anomalies. Recently, multi-modal language-guided image segmentation methods have emerged as a promising solution for chest X-rays where the clinical text reports, depicting the assessment of the images, are used as guidance. Nevertheless, existing language-guided methods require clinical reports alongside the images, and hence, they are not applicable for use in image segmentation in a decision support context, but rather limited to retrospective image analysis after clinical reporting has been completed. In this study, we propose a self-guided segmentation framework (SGSeg) that leverages language guidance for training (multi-modal) while enabling text-free inference (uni-modal), which is the first that enables text-free inference in language-guided segmentation. We exploit the critical location information of both pulmonary and pathological structures depicted in the text reports and introduce a novel localization-enhanced report generation (LERG) module to generate clinical reports for self-guidance. Our LERG integrates an object detector and a location-based attention aggregator, weakly-supervised by a location-aware pseudo-label extraction module. Extensive experiments on a well-benchmarked QaTa-COV19 dataset demonstrate that our SGSeg achieved superior performance than existing uni-modal segmentation methods and closely matched the state-of-the-art performance of multi-modal language-guided segmentation methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281398403",
                    "name": "Shuchang Ye"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2209861054",
                    "name": "Mingjian Li"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "6f5beb87e5da0df8fd1ddf9fdd5c0a35f200b295",
            "title": "Semantic-Driven Global-Local Cooperative Contrastive Learning for Medical Report Generation",
            "abstract": "As the demand for radiology continues to increase, the shortage of specialized professionals becomes a challenging issue. Such shortage highlights the need to utilize advancements in artificial intelligence (AI) to automatically generate draft medical reports from radiology images. Recently, the application of contrastive learning has been leveraged in image-to-text generation because it allows the model to learn meaningful representations in latent space by contrasting similar and dissimilar image-text pairs. However, existing approaches to applying contrastive learning in medical report generation are limited by the following: 1) they are performed as an independent pretraining step, which hinders the cooperation between contrastive learning and the subsequent report generation step; 2) these methods are contingent on pairing images with their corresponding reports, thus establishing similarity based solely on this association. Such contingency inadvertently overlooks the situation where unpaired reports could also be relevant to a given image, thereby failing to accurately capture and understand the semantic relationships within the data; and 3) existing contrastive learning in medical report generation only utilizes the global representation, which cannot capture subtle but crucial local visual information. To address these limitations, we propose a Semantic-Driven Global-Local Cooperative Contrastive Learning Network (SGLCCNet), which integrates contrastive learning into the training process of report generation, enriched with semantic information extracted from reports and enhanced by the inclusion of local feature exploration. Extensive experiments on the IU-Xray dataset demonstrate that our method achieved the state-of-the-art. Further, we demonstrate how each of our proposed steps adds to the overall performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281398403",
                    "name": "Shuchang Ye"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2281343537",
                    "name": "David Dagan Feng"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        }
    ]
}