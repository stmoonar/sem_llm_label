{
    "authorId": "33752120",
    "papers": [
        {
            "paperId": "c0c23ece5892da553c4f4a02198f81c11757c489",
            "title": "Native Multi-Band Audio Coding Within Hyper-Autoencoded Reconstruction Propagation Networks",
            "abstract": "Spectral sub-bands do not portray the same perceptual relevance. In audio coding, it is therefore desirable to have independent control over each of the constituent bands so that bitrate assignment and signal reconstruction can be achieved efficiently. In this work, we present a novel neural audio coding network that natively supports a multi-band coding paradigm. Our model extends the idea of compressed skip connections in the U-Net-based codec, allowing for independent control over both core and high band-specific reconstructions and bit allocation. Our system reconstructs the full-band signal mainly from the condensed core-band code, therefore exploiting and showcasing its bandwidth extension capabilities to its fullest. Meanwhile, the low-bitrate high-band code helps the high-band reconstruction similarly to MPEG audio codecs' spectral bandwidth replication. MUSHRA tests show that the proposed model not only improves the quality of the core band by explicitly assigning more bits to it but retains a good quality in the high-band as well.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1882664215",
                    "name": "Darius Petermann"
                },
                {
                    "authorId": "3348559",
                    "name": "Inseon Jang"
                },
                {
                    "authorId": "33752120",
                    "name": "Minje Kim"
                }
            ]
        },
        {
            "paperId": "347c2c79b120f38604273997f8c70d2b641884ff",
            "title": "Upmixing Via Style Transfer: A Variational Autoencoder for Disentangling Spatial Images And Musical Content",
            "abstract": "In the stereo-to-multichannel upmixing problem for music, one of the main tasks is to set the directionality of the instrument sources in the multichannel rendering results. In this paper, we propose a modified variational autoencoder model that learns a latent space to describe the spatial images in multichannel music. We seek to disentangle the spatial images and music content, so the learned latent variables are invariant to the music. At test time, we use the latent variables to control the panning of sources. We propose two upmixing use cases: transferring the spatial images from one song to another and blind panning based on the generative model. We report objective and subjective evaluation results to empirically show that our model captures spatial images separately from music content and achieves transfer-based interactive panning.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1492176978",
                    "name": "Haici Yang"
                },
                {
                    "authorId": "40447796",
                    "name": "Sanna Wager"
                },
                {
                    "authorId": "2055581760",
                    "name": "S. Russell"
                },
                {
                    "authorId": "2159668491",
                    "name": "Mike Luo"
                },
                {
                    "authorId": "33752120",
                    "name": "Minje Kim"
                },
                {
                    "authorId": "2040567887",
                    "name": "Wontak Kim"
                }
            ]
        },
        {
            "paperId": "80665271c3e337e342bb2b92dd030d99daaeed06",
            "title": "Spain-Net: Spatially-Informed Stereophonic Music Source Separation",
            "abstract": "With the recent advancements of data driven approaches using deep neural networks, music source separation has been formulated as an instrument-specific supervised problem. While existing deep learning models implicitly absorb the spatial information conveyed by the multi-channel input signals, we argue that a more explicit and active use of spatial information could not only improve the separation process but also provide an entry-point for many user-interaction based tools. To this end, we introduce a control method based on the stereophonic location of the sources of interest, expressed as the panning angle. We present various conditioning mechanisms, including the use of raw angle and its derived feature representations, and show that spatial information helps. Our proposed approaches improve the separation performance compared to location agnostic architectures by 1.8 dB SI-SDR in our Slakh-based simulated experiments. Furthermore, the proposed methods allow for the disentanglement of same-class instruments, for example, in mixtures containing two guitar tracks. Finally, we also demonstrate that our approach is robust to incorrect source panning information, which can be incurred by our proposed user interaction.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1882664215",
                    "name": "Darius Petermann"
                },
                {
                    "authorId": "33752120",
                    "name": "Minje Kim"
                }
            ]
        },
        {
            "paperId": "839aca3c850eec2431078abce9cee194daf5db24",
            "title": "Boosted Locality Sensitive Hashing: Discriminative, Efficient, and Scalable Binary Codes for Source Separation",
            "abstract": "We propose a novel adaptive boosting approach to learn discriminative binary hash codes, boosted locality sensitive hashing (BLSH), that can represent audio spectra efficiently. We aim to use the learned hash codes in the single-channel speech denoising task by designing a nearest neighborhood search method that operates in the hashed feature space. To achieve the optimal denoising results given the highly compact binary feature representation, our proposed BLSH algorithm learns simple logistic regressors as the weak learners in an incremental way (i.e., one by one) so that each weak learner is trained to complement the mistake its predecessors have made. Upon testing, their binary classification results transform each spectrum of noisy speech into a bit string, where the bits are ordered based on their significance, adding scalability to the denoising system. Simple bitwise operations calculate Hamming distance to find the $\\boldsymbol{K}$-nearest matching hashed frames in the dictionary of training noisy speech spectra, whose associated ideal binary masks are averaged to estimate the denoising mask for that test mixture. In contrast to the locality sensitive hashing method's random projections, our proposed supervised learning algorithm trains the projections such that the distance between the self-similarity matrix of the hash codes and that of the original spectra is minimized. Likewise, the process conceptually aligns to the Adaboost algorithm, although ours is specialized in learning binary features for source separation rather than classification. Experimental results on speech denoising suggest that the BLSH algorithm learns more discriminative representations than Fourier or mel spectra and the nonlinear kernels derived from them. Our compact binary representation is expected to facilitate model deployment onto resource-constrained environments, where comprehensive models (e.g., deep neural networks) are unaffordable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109652166",
                    "name": "Sunwoo Kim"
                },
                {
                    "authorId": "33752120",
                    "name": "Minje Kim"
                }
            ]
        },
        {
            "paperId": "9b72a0f3be2062ec141e4dc9f4c983d76f52969b",
            "title": "Neural Feature Predictor and Discriminative Residual Coding for Low-Bitrate Speech Coding",
            "abstract": "Low and ultra-low-bitrate neural speech codecs achieved unprecedented coding gain by generating speech signals from compact features. This paper introduces additional coding efficiency in speech coding by reducing the temporal redundancy existing in the frame-level feature sequence via a feature predictor. This predictor produces low-entropy residual representations, and we discriminatively code them based on their contribution to the signal reconstruction. Combining feature prediction and discriminative coding optimizes bitrate efficiency by assigning more bits to hard-to-predict events. We demonstrate the advantage of the proposed methods using the LPCNet as a neural vocoder, resulting in a scalable, lightweight, low-latency, and low-bitrate neural speech coding system. While our approach guarantees strict causality in the frame-level prediction, the subjective tests and feature space analysis show that our model achieves superior coding efficiency compared to the loosely-causal LPCNet and Lyra V2 in the very low bitrates.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1492176978",
                    "name": "Haici Yang"
                },
                {
                    "authorId": "2056948504",
                    "name": "Wootaek Lim"
                },
                {
                    "authorId": "33752120",
                    "name": "Minje Kim"
                }
            ]
        },
        {
            "paperId": "b00c0ac75d645f75a36f3a746b47843cdf555b6f",
            "title": "The Potential of Neural Speech Synthesis-Based Data Augmentation for Personalized Speech Enhancement",
            "abstract": "With the advances in deep learning, speech enhancement systems benefited from large neural network architectures and achieved state-of-the-art quality. However, speaker-agnostic methods are not always desirable, both in terms of quality and their complexity, when they are to be used in a resource-constrained environment. One promising way is personalized speech enhancement (PSE), which is a smaller and easier speech enhancement problem for small models to solve, because it focuses on a particular test-time user. To achieve the personalization goal, while dealing with the typical lack of personal data, we investigate the effect of data augmentation based on neural speech synthesis (NSS). In the proposed method, we show that the quality of the NSS system\u2019s synthetic data matters, and if they are good enough the augmented dataset can be used to improve the PSE system that outperforms the speaker-agnostic baseline. The proposed PSE systems show significant complexity reduction while preserving the enhancement quality.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2134705213",
                    "name": "Anastasia Kuznetsova"
                },
                {
                    "authorId": "35390620",
                    "name": "Aswin Sivaraman"
                },
                {
                    "authorId": "33752120",
                    "name": "Minje Kim"
                }
            ]
        },
        {
            "paperId": "b8f602259b9872b58291e5f0f3aa1d2fa2f60c75",
            "title": "Deep Adaptive Aec: Hybrid of Deep Learning and Adaptive Acoustic Echo Cancellation",
            "abstract": "In this paper we integrate classic adaptive filtering algorithms with modern deep learning to propose a new approach called deep adaptive AEC. The main idea is to represent the linear adaptive algorithm as a differentiable layer within a deep neural network (DNN) framework. This enables the gradients to flow through the adaptive layer during back propagation and the inner layers of the DNN are trained to estimate the playback reference signal and the time-varying learning factors. The proposed approach combines the power of DNNs with adaptive filters. Experimental results show the effectiveness of the proposed method in scenarios where the echo path changes continuously and signal-to-echo ratio (SER) and signal-to-noise ratio (SNR) are low. Furthermore, compared to fully DNN-based baseline methods, integrating adaptive algorithm consistently improves performance and leads to easier training using smaller models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38952862",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2227364",
                    "name": "S. Kandadai"
                },
                {
                    "authorId": "2650449",
                    "name": "Harsha I. K. Rao"
                },
                {
                    "authorId": "33752120",
                    "name": "Minje Kim"
                },
                {
                    "authorId": "2168573981",
                    "name": "Tarun Pruthi"
                },
                {
                    "authorId": "2769671",
                    "name": "T. Kristjansson"
                }
            ]
        },
        {
            "paperId": "0b3e8a23d04b4ef5f646b7b4c9d8ec4fc5596f6c",
            "title": "An Open-Sourced Time-Frequency Domain RF Classification Framework",
            "abstract": "In this paper we present a machine learning-based approach to solving the radio-frequency (RF) signal classification problem in a data-driven way. To this end, we propose an efficient and easy-to-use graphical user interface (GUI) for researchers to collect their own data to build a customized RF classification system. The GUI operates in the time-frequency (TF) domain, which is achieved by applying short-time Fourier transform to the in-phase and quadrature (IQ) time domain signals. Using the proposed GUI, a radio frequency (RF) dataset is collected from the ultra high frequency industrial, scientific, and medical (ISM) bands using commercial-off-the-shelf (COTS) transceivers, and COTS transceiver modules. We train three different variants of convolutional neural network models, such as VGG and ResNet, using the collected dataset and show that they can perform acceptable test-time classification (up to 95% accuracy) on unseen real-world RF signal recordings. Our experimental results also show that a carefully prepared TF domain without a loss of information can achieve better performance than a magnitude-only representation that loses phase information during the TF transformation. We open-source our project to provide the public with access to the labeled datasets, programming code, and the GUI software that can expedite the labeling process.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143491699",
                    "name": "R. Badger"
                },
                {
                    "authorId": "1978673463",
                    "name": "Kristopher H. Jung"
                },
                {
                    "authorId": "33752120",
                    "name": "Minje Kim"
                }
            ]
        },
        {
            "paperId": "18db2019bd6ca61987bde915c08ada95d407b804",
            "title": "Neural Remixer: Learning to Remix Music with Interactive Control",
            "abstract": "The task of manipulating the level and/or effects of individual instruments to recompose a mixture of recording, or remixing, is common across a variety of applications such as music production, audio-visual post-production, podcasts, and more. This process, however, traditionally requires access to individual source recordings, restricting the creative process. To work around this, source separation algorithms can separate a mixture into its respective components. Then, a user can adjust their levels and mix them back together. This two-step approach, however, still suffers from audible artifacts and motivates further work. In this work, we seek to learn to remix music directly. To do this, we propose two neural remixing architectures that extend Conv-TasNet to either remix via a) source estimates directly or b) their latent representations. Both methods leverage a remixing data augmentation scheme as well as a mixture reconstruction loss to achieve an end-to-end separation and remixing process. We evaluate our methods us-ing the Slakh and MUSDB datasets and report both source separation performance and the remixing quality. Our re-sults suggest learning-to-remix signi\ufb01cantly outperforms a strong separation baseline, is particularly useful for small changes, and can provide interactive user-controls.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1492176978",
                    "name": "Haici Yang"
                },
                {
                    "authorId": "2121398859",
                    "name": "Shivani Firodiya"
                },
                {
                    "authorId": "32125125",
                    "name": "Nicholas J. Bryan"
                },
                {
                    "authorId": "33752120",
                    "name": "Minje Kim"
                }
            ]
        },
        {
            "paperId": "2ac5a2508d7e219eb3c18739f54883e54985e0c9",
            "title": "Self-Supervised Learning for Personalized Speech Enhancement",
            "abstract": "Speech enhancement systems can show improved performance by adapting the model towards a single test-time speaker. In this personalization context, the test-time user might only provide a small amount of noise-free speech data, likely insuf\ufb01cient for traditional fully-supervised learning. One way to overcome the lack of personal data is to transfer the model parameters from a speaker-agnostic model to initialize the personalized model, and then to \ufb01netune the model us-ing the small amount of personal speech data. This baseline marginally adapts over the scarce clean speech data. Alternatively, we propose self-supervised methods that are designed speci\ufb01cally to learn personalized and discriminative features from abundant in-the-wild noisy, but still personal speech recordings. Our experiment shows that the proposed self-supervised learning methods initialize personalized speech enhancement models better than the baseline fully-supervised meth-ods, yielding superior speech enhancement performance. The proposed methods also result in a more robust feature set under the real-world conditions: compressed model sizes and fewness of the labeled data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35390620",
                    "name": "Aswin Sivaraman"
                },
                {
                    "authorId": "33752120",
                    "name": "Minje Kim"
                }
            ]
        }
    ]
}