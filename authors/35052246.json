{
    "authorId": "35052246",
    "papers": [
        {
            "paperId": "edf3d52f83dfdbbb286ace27d009841b32bc3396",
            "title": "Team INF-UFRGS at SemEval-2023 Task 7: Supervised Contrastive Learning for Pair-level Sentence Classification and Evidence Retrieval",
            "abstract": "This paper describes the EvidenceSCL system submitted by our team (INF-UFRGS) to SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data (NLI4CT). NLI4CT is divided into two tasks, one for determining the inference relation between a pair of statements in clinical trials and a second for retrieving a set of supporting facts from the premises necessary to justify the label predicted in the first task. Our approach uses pair-level supervised contrastive learning to classify pairs of sentences. We trained EvidenceSCL on two datasets created from NLI4CT and additional data from other NLI datasets. We show that our approach can address both goals of NLI4CT, and although it reached an intermediate position, there is room for improvement in the technique.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223257486",
                    "name": "Abel Corr\u00eaa Dias"
                },
                {
                    "authorId": "2221318608",
                    "name": "Filipe Dias"
                },
                {
                    "authorId": "2221318642",
                    "name": "Higor Moreira"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                }
            ]
        },
        {
            "paperId": "0a6f958734a87afb5c165ecba62801fcd8af8e08",
            "title": "Unsupervised Aspect Term Extraction for Sentiment Analysis through Automatic Labeling",
            "abstract": ": In sentiment analysis, there has been growing interest in performing \ufb01ner granularity analysis focusing on entities and their aspects. This is the goal of Aspect-based Sentiment Analysis which commonly involves the following tasks: Opinion Target Extraction (OTE), Aspect term extraction (ATE), and polarity Classi\ufb01cation (PC). This work focuses on the second task, which is the more challenging and least explored in the unsupervised context. The dif\ufb01culty arises mainly due to the nature of the data (user-generated contents or product reviews) and the inconsistent annotation of the evaluation datasets. Existing approaches for ATE and OTE either depend on annotated data or are limited by the availability of domain-or language-speci\ufb01c resources. To overcome these limitations, we propose UNsupervised Aspect Term Extractor (U N ATE), an end-to-end unsupervised ATE solution. Our solution relies on a combination of topic models, word embeddings, and a BERT-based classi\ufb01er to extract aspects even in the absence of annotated data. Experimental results on datasets from different domains have shown that U N ATE achieves precision and F-measure scores comparable to the semi-supervised and unsupervised state-of-the-art ATE solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3419174",
                    "name": "Danny Suarez Vargas"
                },
                {
                    "authorId": "66919789",
                    "name": "L. Pessutto"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "2c97e097e12527e5d8f338c6ba501d51b5662dc0",
            "title": "EurOpi: Multilingual Aspect-Based Sentiment Analysis enabled by a Knowledge Base",
            "abstract": "Opinionated texts are widely available as the use of e-commerce platforms has become more popular around the world. Sentiment Analysis is the field of study which aims to understand users\u2019 preferences and opinions, extracting useful knowledge from this kind of data. Despite advances in the last years, some topics remain unexplored. This is the case of multilingual aspect-based sentiment analysis (MABSA), which aims to process reviews written in multiple languages. In this work, we propose EurOpi, a MABSA technique that relies on a knowledge base, i.e., a multilingual product catalog, to map the aspects evaluated by users from opinionated texts into product attributes. We show that (i) EurOpi effectively maps aspects to catalog attributes, and (ii) a multilingual product catalog enriched with opinions makes it easy to compare opinions across different languages. Finally, we release a dataset, containing reviews in English, Spanish, and Italian annotated at the aspect level to help increment research on MABSA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3419174",
                    "name": "Danny Suarez Vargas"
                },
                {
                    "authorId": "66919789",
                    "name": "L. Pessutto"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "2047197345",
                    "name": "Tiago de Melo"
                },
                {
                    "authorId": "10688179",
                    "name": "Altigran Soares da Silva"
                }
            ]
        },
        {
            "paperId": "72e19e9a4aa47e7ccd6433c2bd5a85b508e45b80",
            "title": "UFRGSent at SemEval-2022 Task 10: Structured Sentiment Analysis using a Question Answering Model",
            "abstract": "This paper describes the system submitted by our team (UFRGSent) to SemEval-2022 Task 10: Structured Sentiment Analysis. We propose a multilingual approach that relies on a Question Answering model to find tuples consisting of aspect, opinion, and holder. The approach starts from general questions and uses the extracted tuple elements to find the remaining components. Finally, we employ an aspect sentiment classification model to classify the polarity of the entire tuple. Despite our method being in a mid-rank position on SemEval competition, we show that the question-answering approach can achieve good coverage retrieving sentiment tuples, allowing room for improvements in the technique.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66919789",
                    "name": "L. Pessutto"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "c8717980e736a35b68b791572ed264d6f3a0a8a8",
            "title": "BRCars: a Dataset for Fine-Grained Classification of Car Images",
            "abstract": "Fine-grained computer vision tasks refer to the ability of distinguishing objects that belong to the same parent class, differentiating themselves by subtle visual elements. Image classification in car models is considered a fine-grained classification task. In this work, we introduce BRCars, a dataset that seeks to replicate the main challenges inherent to the task of classifying car images in many practical applications. BRCars contains around 300K images collected from a Brazilian car advertising website. The images correspond to 52K car instances and are distributed among 427 different models. The images are both from the exterior and the interior of the cars and present an unbalanced distribution across the different models. In addition, they are characterized by a lack of standardization in terms of perspective. We adopted a semi-automated annotation pipeline with the help of the new CLIP neural network, which enabled distinguishing thousands of images among different perspectives using textual queries. Experiments with standard deep learning classifiers were performed to serve as baseline results for future work on this topic. BRCars dataset is available at https://github.com/danimtk/brcars-dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2147058712",
                    "name": "Daniel M. Kuhn"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "e03b656d5d7552c92d7b8efaf72e4b91ecafeb4f",
            "title": "REGIS: A Test Collection for Geoscientific Documents in Portuguese",
            "abstract": "Experimental validation is key to the development of Information Retrieval (IR) systems. The standard evaluation paradigm requires a test collection with documents, queries, and relevance judgments. Creating test collections requires significant human effort, mainly for providing relevance judgments. As a result, there are still many domains and languages that, to this day, lack a proper evaluation testbed. Portuguese is an example of a major world language that has been overlooked in terms of IR research -- the only test collection available is composed of news articles from 1994 and a hundred queries. With the aim of bridging this gap, in this paper, we developed REGIS (Retrieval Evaluation for Geoscientific Information Systems), a test collection for the geoscientific domain in Portuguese. REGIS contains 20K documents and 34 query topics along with relevance assessments. We describe the procedures for document collection, topic creation, and relevance assessment. In addition, we report on results of standard IR techniques on REGIS so that they can serve as a baseline for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2151578158",
                    "name": "Lucas Lima de Oliveira"
                },
                {
                    "authorId": "52478087",
                    "name": "R. Romeu"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "167554451669bc4e493135ccf7443b8935eafd12",
            "title": "Mono vs Multilingual Transformer-based Models: a Comparison across Several Language Tasks",
            "abstract": "BERT (Bidirectional Encoder Representations from Transformers) and ALBERT (A Lite BERT) are methods for pre-training language models which can later be fine-tuned for a variety of Natural Language Understanding tasks. These methods have been applied to a number of such tasks (mostly in English), achieving results that outperform the state-of-the-art. In this paper, our contribution is twofold. First, we make available our trained BERT and Albert model for Portuguese. Second, we compare our monolingual and the standard multilingual models using experiments in semantic textual similarity, recognizing textual entailment, textual category classification, sentiment analysis, offensive comment detection, and fake news detection, to assess the effectiveness of the generated language representations. The results suggest that both monolingual and multilingual models are able to achieve state-of-the-art and the advantage of training a single language model, if any, is small.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51285753",
                    "name": "Diego de Vargas Feij\u00f3"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "2c16aa5365e97932027fd5d56a8410083456553d",
            "title": "Identifying and Fusing Duplicate Features for Data Mining",
            "abstract": "This work addresses the problem of identifying and fusing duplicate features in machine learning datasets. Our goal is to evaluate the hypothesis that fusing duplicate features can improve the predictive power of the data while reducing training time. We propose a simple method for duplicate detection and fusion based on a small set of features. An evaluation comparing the duplicate detection against a manually generated ground truth obtained F1 of 0.91. Then,the effects of fusion were measured on a mortality prediction test. The results were inferior to the ones obtained with the original dataset. Thus we concluded that the investigated hypothesis does not hold.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2080047902",
                    "name": "Hort\u00eansia Costa Barcelos"
                },
                {
                    "authorId": "2127723",
                    "name": "M. R. Mendoza"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "85d9bcd0277e765d68ca16f22ad05a95ebd55169",
            "title": "Offensive Video Detection: Dataset and Baseline Results",
            "abstract": "Web-users produce and publish high volumes of data of various types, such as text, images, and videos. The platforms try to restrain their users from publishing offensive content to keep a friendly and respectful environment and rely on moderators to filter the posts. However, this method is insufficient due to the high volume of publications. The identification of offensive material can be performed automatically using machine learning, which needs annotated datasets. Among the published datasets in this matter, the Portuguese language is underrepresented, and videos are little explored. We investigated the problem of offensive video detection by assembling and publishing a dataset of videos in Portuguese containing mostly textual features. We ran experiments using popular machine learning classifiers used in this domain and reported our findings, alongside multiple evaluation metrics. We found that using word embedding with Deep Learning classifiers achieved the best results on average. CNN architectures, Naive Bayes, and Random Forest ranked top among different experiments. Transfer Learning models outperformed Classic algorithms when processing video transcriptions, but scored lower using other feature sets. These findings can be used as a baseline for future works on this subject.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059558483",
                    "name": "Cleber Alc\u00e2ntara"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "51285753",
                    "name": "Diego de Vargas Feij\u00f3"
                }
            ]
        },
        {
            "paperId": "9a439cc030634961e76aa2ed8c6dba3f8c42e53f",
            "title": "BabelEnconding at SemEval-2020 Task 3: Contextual Similarity as a Combination of Multilingualism and Language Models",
            "abstract": "This paper describes the system submitted by our team (BabelEnconding) to SemEval-2020 Task 3: Predicting the Graded Effect of Context in Word Similarity. We propose an approach that relies on translation and multilingual language models in order to compute the contextual similarity between pairs of words. Our hypothesis is that evidence from additional languages can leverage the correlation with the human generated scores. BabelEnconding was applied to both subtasks and ranked among the top-3 in six out of eight task/language combinations and was the highest scoring system three times.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66919789",
                    "name": "L. Pessutto"
                },
                {
                    "authorId": "145988207",
                    "name": "Tiago de Melo"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "1690426",
                    "name": "A. D. Silva"
                }
            ]
        },
        {
            "paperId": "e5882177e1327965110866398d94230cd8ff46d9",
            "title": "Embeddings for Named Entity Recognition in Geoscience Portuguese Literature",
            "abstract": "This work focuses on Portuguese Named Entity Recognition (NER) in the Geology domain. The only domain-specific dataset in the Portuguese language annotated for NER is the GeoCorpus. Our approach relies on BiLSTM-CRF neural networks (a widely used type of network for this area of research) that use vector and tensor embedding representations. Three types of embedding models were used (Word Embeddings, Flair Embeddings, and Stacked Embeddings) under two versions (domain-specific and generalized). The domain specific Flair Embeddings model was originally trained with a generalized context in mind, but was then fine-tuned with domain-specific Oil and Gas corpora, as there simply was not enough domain corpora to properly train such a model. Each of these embeddings was evaluated separately, as well as stacked with another embedding. Finally, we achieved state-of-the-art results for this domain with one of our embeddings, and we performed an error analysis on the language model that achieved the best results. Furthermore, we investigated the effects of domain-specific versus generalized embeddings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "91020086",
                    "name": "B. Consoli"
                },
                {
                    "authorId": "152527770",
                    "name": "Joaquim Santos"
                },
                {
                    "authorId": "120412319",
                    "name": "Diogo Gomes"
                },
                {
                    "authorId": "1411699034",
                    "name": "F\u00e1bio Corr\u00eaa Cordeiro"
                },
                {
                    "authorId": "144845513",
                    "name": "R. Vieira"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "8a1af7197ede2b4ccf8eea2d90b2dd411bca5b7b",
            "title": "Summarizing Legal Rulings: Comparative Experiments",
            "abstract": "In the context of text summarization, texts in the legal domain have peculiarities related to their length and to their specialized vocabulary. Recent neural network-based approaches can achieve high-quality scores for text summarization. However, these approaches have been used mostly for generating very short abstracts for news articles. Thus, their applicability to the legal domain remains an open issue. In this work, we experimented with ten extractive and four abstractive models in a real dataset of legal rulings. These models were compared with an extractive baseline based on heuristics to select the most relevant parts of the text. Our results show that abstractive approaches significantly outperform extractive methods in terms of ROUGE scores.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51285753",
                    "name": "Diego de Vargas Feij\u00f3"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "ae82add77d39b61a3342ab1397796d5dc0b1b20f",
            "title": "UserDEV: A Mixed-Initiative System for User Group Analytics",
            "abstract": "The increasing availability of user data constitutes new opportunities in various applications ranging from behavioral analytics to recommendations. A common way of analyzing user data is through \"user group analytics\" whose purpose is to breakdown users into groups to gain a more focused understanding of their collective behavior. The process consists of group discovery, group exploration, and group visualization. To date, user group analytics is done using separate tools which makes it fragmented and burdensome for analysts. In this paper, we describe UserDEV, a full-fledged user group analytics pipeline which combines discovery, exploration, and visualization of user groups, in a fully-connected fashion. UserDEV contributes a star-like architecture as well as a common data exchange model to tighten connections between the analytics components. We provide a realistic use case to show how UserDEV helps analysts perform analytical tasks on user groups. While we report a preliminary user study, we also discuss opportunities for an end-to-end evaluation of a group analytics framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2060863572",
                    "name": "Eric Simon"
                },
                {
                    "authorId": "31749899",
                    "name": "Fabian Colque Zegarra"
                },
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "267ee5550937b9e6e7f8ba1306c806e16f1939c7",
            "title": "A Classifier Ensemble for Offensive Text Detection",
            "abstract": "Offensive posts are a constant nuisance in many Web platforms. As a consequence, there has been growing interest in devising methods to automatically identify such posts. In this paper, we present Hate2Vec -- an approach for detecting offensive comments on the Web. Hate2Vec relies on a classifier ensemble. The base learners include: (i) a lexicon-based classifier which leverages the semantic relatedness of word embeddings; (ii) a logistic regression classifier based on comment embeddings; (iii) and a standard bag-of-words (BOW) classifier based on unigram features. Our experiments with datasets in English and Portuguese have yielded high classification results (F-measure above 0.9) and significantly outperformed a traditional BOW classifier.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51424998",
                    "name": "Rogers Pelle"
                },
                {
                    "authorId": "2059558483",
                    "name": "Cleber Alc\u00e2ntara"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "6bffaa5b18954df89322477f8af4b85040b72c84",
            "title": "A Large Parallel Corpus of Full-Text Scientific Articles",
            "abstract": "The Scielo database is an important source of scientific information in Latin America, containing articles from several research domains. A striking characteristic of Scielo is that many of its full-text contents are presented in more than one language, thus being a potential source of parallel corpora. In this article, we present the development of a parallel corpus from Scielo in three languages: English, Portuguese, and Spanish. Sentences were automatically aligned using the Hunalign algorithm for all language pairs, and for a subset of trilingual articles also. We demonstrate the capabilities of our corpus by training a Statistical Machine Translation system (Moses) for each language pair, which outperformed related works on scientific articles. Sentence alignment was also manually evaluated, presenting an average of 98.8% correctly aligned sentences across all languages. Our parallel corpus is freely available in the TMX format, with complementary information regarding article metadata.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "11802876",
                    "name": "Felipe Soares"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "49629811",
                    "name": "Karin Becker"
                }
            ]
        },
        {
            "paperId": "ee84d884e9f0b31939553086bfe45e59d919dc57",
            "title": "Clustering Multilingual Aspect Phrases for Sentiment Analysis",
            "abstract": "The area of sentiment analysis has experienced significant developments in the last few years. More specifically, there has been growing interest in aspect-based sentiment analysis in which the goal is to extract, group, and rate the overall opinion about the features of the entity being evaluated. Techniques for aspect extraction can produce an undesirably large number of aspects - with many of those relating to the same product feature. This problem is aggravated when the reviews are written in many languages. In this paper, we address the novel task of multilingual aspect clustering which aims at grouping together the aspects extracted from reviews written in several languages. We contribute with a proposal of techniques to tackle this problem and test them on reviews written in five languages. Our experiments show that our unsupervised clustering technique achieves results that outperform a semi-supervised baseline in many cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66919789",
                    "name": "L. Pessutto"
                },
                {
                    "authorId": "3419174",
                    "name": "Danny Suarez Vargas"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "85370d0bd43cd0a202013413348b012d3167768a",
            "title": "Exploration of User Groups in VEXUS",
            "abstract": "We demonstrate VEXUS, an interactive visualization framework for exploring user data to fulfill tasks such as finding a set of experts, forming discussion groups and analyzing collective behaviors. User data is characterized by a combination of demographics like age and occupation, and actions such as rating a movie, writing a paper or following a medical treatment. The ubiquity of user data requires tools that help explorers, be they specialists or novice users, acquire new insights. VEXUS lets explorers interact with user data via visual primitives and builds an exploration profile to recommend the next exploration steps. VEXUS combines state-of-the-art visualization techniques with appropriate indexing of user data to provide fast and relevant exploration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                },
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "31749899",
                    "name": "Fabian Colque Zegarra"
                }
            ]
        },
        {
            "paperId": "e2550efcf150a5371deab4904fa36de22efdb733",
            "title": "Offensive Comments in the Brazilian Web: a dataset and baseline results",
            "abstract": ". Brazilian Web users are among the most active in social networks and very keen on interacting with others. Offensive comments, known as hate speech , have been plaguing online media and originating a number of law-suits against companies which publish Web content. Given the massive number of user generated text published on a daily basis, manually \ufb01ltering offensive comments becomes infeasible. The identi\ufb01cation of offensive comments can be treated as a supervised classi\ufb01cation task. In order to obtain a model to classify comments, an annotated dataset containing positive and negative examples is necessary. The lack of such a dataset in Portuguese, limits the development of detection approaches for this language. In this paper, we describe how we created annotated datasets of offensive comments for Portuguese by collecting news comments on the Brazilian Web. In addition, we provide classi\ufb01cation re-sults achieved by standard classi\ufb01cation algorithms on these datasets which can serve as baseline for future work on this topic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51424998",
                    "name": "Rogers Pelle"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "02d7a50e6557b24e38a28329e45aa9a23323b292",
            "title": "Identifying Sentiment-Based Contradictions",
            "abstract": "Contradiction Analysis is a relatively new multidisciplinary and complex area with the main goal of identifying contradictory pieces of text. It can be addressed from the perspectives of different research areas such as Natural Language Processing, Opinion Mining, Information Retrieval, and Information Extraction. This paper focuses on the problem of detecting sentiment-based contradictions which occur in the sentences of a given review text. Unlike other types of contradictions, the detection of sentiment-based contradictions can be tackled as a post-processing step in the traditional sentiment analysis task. In this context, we adapted and extended an existing contradiction analysis framework by filtering its results to remove the reviews that are erroneously labeled as contradictory. The filtering method is based on two simple term similarity algorithms. An experimental evaluation on real product reviews has shown proportional improvements of up to 30% in classification accuracy and 26% in the precision of contradiction detection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3419174",
                    "name": "Danny Suarez Vargas"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "b27645453f57b6bd0a8eeb527e49d4b48f2339f3",
            "title": "Comparing and combining Content\u2010 and Citation\u2010based approaches for plagiarism detection",
            "abstract": "The vast amount of scientific publications available online makes it easier for students and researchers to reuse text from other authors and makes it harder for checking the originality of a given text. Reusing text without crediting the original authors is considered plagiarism. A number of studies have reported the prevalence of plagiarism in academia. As a consequence, numerous institutions and researchers are dedicated to devising systems to automate the process of checking for plagiarism. This work focuses on the problem of detecting text reuse in scientific papers. The contributions of this paper are twofold: (a) we survey the existing approaches for plagiarism detection based on content, based on content and structure, and based on citations and references; and (b) we compare content and citation\u2010based approaches with the goal of evaluating whether they are complementary and if their combination can improve the quality of the detection. We carry out experiments with real data sets of scientific papers and concluded that a combination of the methods can be beneficial.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2076176108",
                    "name": "Solange de L. Pertile"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "143752702",
                    "name": "Paolo Rosso"
                }
            ]
        },
        {
            "paperId": "5244cb13720f0d9f7390a4489626b67bd02b6757",
            "title": "Automatic Filling of Hidden Web Forms: A Survey",
            "abstract": "A significant part of the information available on the Web is stored in online databases which compose what is known as Hidden Web or Deep Web. In order to access information from the Hidden Web, one must fill an HTML form that is submitted as a query to the underlying database. In recent years, many works have focused on how to automate the process of form filling by creating methods for choosing values to fill the fields in the forms. This is a challenging task since forms may contain fields for which there are no predefined values to choose from. This article presents a survey of methods for Web Form Filling, analyzing the existing solutions with respect to the type of forms that they handle and the filling strategy adopted. We provide a comparative analysis of 15 key works in this area and discuss directions for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3107648",
                    "name": "G. Kantorski"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "1714151",
                    "name": "C. Heuser"
                }
            ]
        },
        {
            "paperId": "744fcf1198b2c219c3bd2be6437f2e8c0a6dbc55",
            "title": "Detecting Contrastive Sentences for Sentiment Analysis",
            "abstract": "Contradiction Analysis is a relatively new multidisciplinary and complex area with the main goal of identifying contradictory pieces of text. It can be addressed from the perspectives of different research areas such as Natural Language Processing, Opinion Mining, Information Retrieval, and Information Extraction. This work focuses on the problem of detecting sentiment-based contradictions which occur in the sentences of a given review text. Unlike other types of contradictions, the detection of sentiment-based contradictions can be tackled as a post-processing step in the traditional sentiment analysis task. In this context, we make two main contributions. The first is an exploratory study of the classification task, in which we identify and use different tools and resources. Our second contribution is adapting and extending an existing contradiction analysis framework by filtering its results to remove the reviews that are erroneously labeled as contradictory. The filtering method is based on two simple term similarity algorithms. An experimental evaluation on real product reviews has shown proportional improvements of up to 30% in classification accuracy and 26% in the precision of contradiction detection.",
            "fieldsOfStudy": [
                "Art",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3419174",
                    "name": "Danny Suarez Vargas"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "9597469ce0d13699dc032447f3f0f54022d3a5f7",
            "title": "UFRGS: Identifying Categories and Targets in Customer Reviews",
            "abstract": "This paper reports on our participation in SemEval-2015 Task 12, which was devoted to Aspect-Based Sentiment Analysis. Participants were required to identify the category (entity and attribute), the opinion target, and the polarity of customer reviews. The system we built relies on classification algorithms to identify aspect categories and on a set of rules to identify the opinion target. We propose a two-phase classification approach for category identification and use a simple method for polarity detection. Our results outperform the baseline in many cases, which means our system could be used as an alternative for aspect classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3429513",
                    "name": "Anderson Uilian Kauer"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "1d0a16be37a9f636cf891bde0de6decacc366b31",
            "title": "Exploring Information Retrieval Features for Author Profiling",
            "abstract": "This paper describes the methods we have employed to solve the author profiling task at PAN-2014. Our goal was to rely mainly on features from Information Retrieval to identify the age group and the gender of the author of a given text. We describe the features, the classification algorithms employed, and how the experiments were run. Also, we provide an analysis of our results compared to other groups.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2643867",
                    "name": "Edson R. D. Weren"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "1808076",
                    "name": "J. Oliveira"
                }
            ]
        },
        {
            "paperId": "4305b6bffae196938dc4616e8479a38fdbecc5cd",
            "title": "Exploring Information Retrieval features for Author Profiling Notebook for PAN at CLEF 2014",
            "abstract": "This paper describes the methods we have employed to solve the au- thor profiling task at PAN-2014. Our goal was to rely mainly on features from Information Retrieval to identify the age group and the gender of the author of a given text. We describe the features, the classification algorithms employed, and how the experiments were run. Also, we provide an analysis of our results compared to other groups.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2643867",
                    "name": "Edson R. D. Weren"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "1808076",
                    "name": "J. Oliveira"
                }
            ]
        },
        {
            "paperId": "8f32f2be7ff30bf46d38dccd56a24412c8d3d3c6",
            "title": "Examining Multiple Features for Author Profiling",
            "abstract": "Authorship analysis aims at classifying texts based on the stylistic choices of their authors. The idea is to discover characteristics of the authors of the texts. This task has a growing importance in forensics, security, and marketing. In this work, we focus on discovering age and gender from blog authors. With this goal in mind, we analyzed a large number of features -- ranging from Information Retrieval to Sentiment Analysis. This paper reports on the usefulness of these features. Experiments on a corpus of over 236K blogs show that a classifier using the features explored here have outperformed the state-of-the art. More importantly, the experiments show that the Information Retrieval features proposed in our work are the most discriminative and yield the best class predictions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2643867",
                    "name": "Edson R. D. Weren"
                },
                {
                    "authorId": "3429513",
                    "name": "Anderson Uilian Kauer"
                },
                {
                    "authorId": "1919608",
                    "name": "Lucas Mizusaki"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "1808076",
                    "name": "J. Oliveira"
                },
                {
                    "authorId": "2525073",
                    "name": "Leandro Krug Wives"
                }
            ]
        },
        {
            "paperId": "ba22ba9bb683e668503e5059391e6675b157d2d0",
            "title": "ARCTIC: metadata extraction from scientific papers in pdf using two-layer CRF",
            "abstract": "Most scientific articles are available in PDF format. The PDF standard allows the generation of metadata that is included within the document. However, many authors do not define this information, making this feature unreliable or incomplete. This fact has been motivating research which aims to extract metadata automatically. Automatic metadata extraction has been identified as one of the most challenging tasks in document engineering. This work proposes Artic, a method for metadata extraction from scientific papers which employs a two-layer probabilistic framework based on Conditional Random Fields. The first layer aims at identifying the main sections with metadata information, and the second layer finds, for each section, the corresponding metadata. Given a PDF file containing a scientific paper, Artic extracts the title, author names, emails, affiliations, and venue information. We report on experiments using 100 real papers from a variety of publishers. Our results outperformed the state-of-the-art system used as the baseline, achieving a precision of over 99%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "98440846",
                    "name": "Alan Souza"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "1714151",
                    "name": "C. Heuser"
                }
            ]
        },
        {
            "paperId": "bfaa8079fc769498ed59cf444a6a57792ca82d77",
            "title": "Comparing the Quality of Focused Crawlers and of the Translation Resources Obtained from them",
            "abstract": "Comparable corpora have been used as an alternative for parallel corpora as resources for computational tasks that involve domain-specific natural language processing. One way to gather documents related to a specific topic of interest is to traverse a portion of the web graph in a targeted way, using focused crawling algorithms. In this paper, we compare several focused crawling algorithms using them to collect comparable corpora on a specific domain. Then, we compare the evaluation of the focused crawling algorithms to the performance of linguistic processes executed after training with the corresponding generated corpora. Also, we propose a novel approach for focused crawling, exploiting the expressive power of multiword expressions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2080324700",
                    "name": "Bruno Laranjeira"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "145585242",
                    "name": "Aline Villavicencio"
                },
                {
                    "authorId": "2325592",
                    "name": "Carlos Ramisch"
                },
                {
                    "authorId": "103586459",
                    "name": "M. J. Finatto"
                }
            ]
        },
        {
            "paperId": "113f7e859685659dc954e8507cb881f3ff34f3b3",
            "title": "Prequery Discovery of Domain-Specific Query Forms: A Survey",
            "abstract": "The discovery of HTML query forms is one of the main challenges in Deep Web crawling. Automatic solutions for this problem perform two main tasks. The first is locating HTML forms on the Web, which is done through the use of traditional/focused crawlers. The second is identifying which of these forms are indeed meant for querying, which also typically involves determining a domain for the underlying data source (and thus for the form as well). This problem has attracted a great deal of interest, resulting in a long list of algorithms and techniques. Some methods submit requests through the forms and then analyze the data retrieved in response, typically requiring a great deal of knowledge about the domain as well as semantic processing. Others do not employ form submission, to avoid such difficulties, although some techniques rely to some extent on semantics and domain knowledge. This survey gives an up-to-date review of methods for the discovery of domain-specific query forms that do not involve form submission. We detail these methods and discuss how form discovery has become increasingly more automated over time. We conclude with a forecast of what we believe are the immediate next steps in this trend.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "88556334",
                    "name": "Mauricio C. Moraes"
                },
                {
                    "authorId": "1714151",
                    "name": "C. Heuser"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "145690522",
                    "name": "Denilson Barbosa"
                }
            ]
        },
        {
            "paperId": "674cbe4e3354bc5af3c6d38cc935d7377bb01646",
            "title": "Using Simple Content Features for the Author Profiling Task Notebook for PAN at CLEF 2013",
            "abstract": "This paper describes the methods we have employed to solve the au- thor profiling task at PAN-2013. Our goal was to use simple features to identify the age group and the gender of the author of a given text. We introduce the fea- tures, detail how the classifiers were trained, and how the experiments were run.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2643867",
                    "name": "Edson R. D. Weren"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "1808076",
                    "name": "J. Oliveira"
                }
            ]
        },
        {
            "paperId": "a47d5a3286079989a6b7e86dc7fb81f4b724e04e",
            "title": "Finding Missing Cross-Language Links in Wikipedia",
            "abstract": "Wikipedia is a public encyclopedia composed of millions of articles written daily by volunteer authors from different regions of the world. The articles contain links called cross-language links which relate corresponding articles across different languages. This feature is extremely useful for applications that work with automatic translation and multilingual information retrieval as it allows the assembly of comparable corpora. Thus, it is important to have a mechanism that automatically creates such links. This has been motivating the development of techniques to identify missing cross-language links. In this article, we present CLLFinder, an approach for finding missing cross-language links. The approach makes use of the links between categories and of the transitivity between existing cross-language links, as well as textual features extracted from the articles. Experiments using one million articles from the English and Portuguese Wikipedias attest the viability of CLLFinder. The results show that our approach has a recall of 96% and a precision of 98%, outperforming the baseline system, even though we employ simpler and fewer features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50589072",
                    "name": "Carlos Eduardo M. Moreira"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "0da97af12b6f20751619a342327968b1bf58b77d",
            "title": "Identifying Parallel Web Pages",
            "abstract": "Research on statistical machine translation and corpus-based approaches for cross-language information retrieval depend on the availability of multilingual data, particularly in the form of parallel corpora (collections of equivalent texts in two or more languages). However, the scarcity of parallel corpora limits the development of these applications. The Web is a vast repository of multilingual information, which has motivated research aimed at mining corpora from it. In this article, we present PPLocator an approach for locating parallel Web pages. PPLocator was designed to be effective while keeping a low processing cost, thus it avoids making exhaustive pairwise comparisons in order to identify the candidate pairs. In addition, it tries to minimize the number of pages that need to be downloaded during the intra-site crawl. An important characteristic of our approach is that it does not rely on resources such as dictionaries, translators, or language identifiers. PPLocator demands little effort from the human expert. Experiments using real Web data from over 284K pages attest for the viability of PPLocator. The results show superiority in relation to a baseline system in terms of both recall and precision, despite the fact that the baseline uses more resources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47737054",
                    "name": "Marcela Macedo Vieira"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "7396de3633d7d64150e37f681b964d27f0f38bfa",
            "title": "Clustering Wikipedia infoboxes to discover their types",
            "abstract": "Wikipedia has emerged as an important source of structured information on the Web. But while the success of Wikipedia can be attributed in part to the simplicity of adding and modifying content, this has also created challenges when it comes to using, querying, and integrating the information. Even though authors are encouraged to select appropriate categories and provide infoboxes that follow pre-defined templates, many do not follow the guidelines or follow them loosely. This leads to undesirable effects, such as template duplication, heterogeneity, and schema drift. As a step towards addressing this problem, we propose a new unsupervised approach for clustering Wikipedia infoboxes. Instead of relying on manually assigned categories and template labels, we use the structured information available in infoboxes to group them and infer their entity types. Experiments using over 48,000 infoboxes indicate that our clustering approach is effective and produces high quality clusters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144101748",
                    "name": "T. Nguyen"
                },
                {
                    "authorId": "2110492602",
                    "name": "Huong Nguyen"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                }
            ]
        },
        {
            "paperId": "58cdbcfe0f844458b1f10db89b9f4d9d40462ec8",
            "title": "Multilingual Schema Matching for Wikipedia Infoboxes",
            "abstract": "Recent research has taken advantage of Wikipedia's multi-lingualism as a resource for cross-language information retrieval and machine translation, as well as proposed techniques for enriching its cross-language structure. The availability of documents in multiple languages also opens up new opportunities for querying structured Wikipedia content, and in particular, to enable answers that straddle different languages. As a step towards supporting such queries, in this paper, we propose a method for identifying mappings between attributes from infoboxes that come from pages in different languages. Our approach finds mappings in a completely automated fashion. Because it does not require training data, it is scalable: not only can it be used to find mappings between many language pairs, but it is also effective for languages that are under-represented and lack sufficient training samples. Another important benefit of our approach is that it does not depend on syntactic similarity between attribute names, and thus, it can be applied to language pairs that have distinct morphologies. We have performed an extensive experimental evaluation using a corpus consisting of pages in Portuguese, Vietnamese, and English. The results show that not only does our approach obtain high precision and recall, but it also outperforms state-of-the-art techniques. We also present a case study which demonstrates that the multilingual mappings we derive lead to substantial improvements in answer quality and coverage for structured queries over Wikipedia content.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144101748",
                    "name": "T. Nguyen"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "2110492602",
                    "name": "Huong Nguyen"
                },
                {
                    "authorId": "48832564",
                    "name": "Hoa Nguyen"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                }
            ]
        },
        {
            "paperId": "688ed7d0c93457efb958388b588ad14734460a6f",
            "title": "Cell assemblies for query expansion in Information Retrieval",
            "abstract": "One of the main tasks in Information Retrieval is to match a user query to the documents that are relevant for it. This matching is challenging because in many cases the keywords the user chooses will be different from the words the authors of the relevant documents have used. Throughout the years, many approaches have been proposed to deal with this problem. One of the most popular consists in expanding the query with related terms with the goal of retrieving more relevant documents. In this paper, we propose a new method in which a Cell Assembly model is applied for query expansion. Cell Assemblies are reverberating circuits of neurons that can persist long beyond the initial stimulus has ceased. They learn through Hebbian Learning rules and have been used to simulate the formation and the usage of human concepts. We adapted the Cell Assembly model to learn relationships between the terms in a document collection. These relationships are then used to augment the original queries. Our experiments use standard Information Retrieval test collections and show that some queries significantly improved their results with our technique.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34617558",
                    "name": "Isabel Volpe"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "3342056",
                    "name": "C. Huyck"
                }
            ]
        },
        {
            "paperId": "c674eada553b01a82c7fd22fee338fd8cf68b0f4",
            "title": "Identification and Treatment of Multiword Expressions Applied to Information Retrieval",
            "abstract": "The extensive use of Multiword Expressions (MWE) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions. A MWE typically expresses concepts and ideas that usually cannot be expressed by a single word. Intuitively, with the appropriate treatment of MWEs, the results of an Information Retrieval (IR) system could be improved. The aim of this paper is to apply techniques for the automatic extraction of MWEs from corpora to index them as a single unit. Experimental results show improvements on the retrieval of relevant documents when identifying MWEs and treating them as a single indexing unit.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2428899",
                    "name": "O. Acosta"
                },
                {
                    "authorId": "145585242",
                    "name": "Aline Villavicencio"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "0c956bccfef2bf4c8474531a48ed20fdd2c6ef50",
            "title": "Statistics for Ranking Program Committees and Editorial Boards",
            "abstract": "Ranking groups of researchers is important in several contexts and can serve many purposes such as the fair distribution of grants based on the scientist's publication output, concession of research projects, classification of journal editorial boards and many other applications in a social context. In this paper, we propose a method for measuring the performance of groups of researchers. The proposed method is called alpha-index and it is based on two parameters: (i) the homogeneity of the h-indexes of the researchers in the group; and (ii) the h-group, which is an extension of the h-index for groups. Our method integrates the concepts of homogeneity and absolute value of the h-index into a single measure which is appropriate for the evaluation of groups. We report on experiments that assess computer science conferences based on the h-indexes of their program committee members. Our results are similar to a manual classification scheme adopted by a research agency.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "50823563",
                    "name": "Roberto da Silva"
                },
                {
                    "authorId": "1808076",
                    "name": "J. Oliveira"
                },
                {
                    "authorId": "2113230553",
                    "name": "J. V. D. Lima"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "75505b9ad77ebd3888ee531008f09744af8cdced",
            "title": "UFRGS@PAN2010: Detecting External Plagiarism - Lab Report for PAN at CLEF 2010",
            "abstract": "This paper presents our approach to detect plagiar ism in the PAN'10 competition. To accomplish this task we applied a method which aims at detect- ing external plagiarism cases. The method is specia lly designed to detect cross- language plagiarism and is composed by five phases: language normalization, retrieval of candidate documents, classifier traini ng, plagiarism analysis, and post-processing. Our group got the seventh place in the competition with an overall score of 0.5175. It is important to notice that the final score was affected by our low recall (0.4036) which arose as a result of not detecting intrinsic pla- giarism cases, which were also present in the compe tition corpus.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32712917",
                    "name": "R. C. Pereira"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "1758473",
                    "name": "R. Galante"
                }
            ]
        },
        {
            "paperId": "756d788afde6aa94dcb75f0fb677fdcb776fbb95",
            "title": "Cell Assemblies for Query Expansion",
            "abstract": "This paper applies the Cell Assemblies (CAs) model to Information Retrieval. CAs are reverberating circuits of neurons that can persist long beyond the initial stimulus has ceased. CAs are learned through Hebbian learning rules and have been used to simulate the formation and the usage of human concepts. We adapted the CAs model to learn relationships between the terms in a document collection. The method will be validated by means of experiments on standard IR test collections.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34617558",
                    "name": "Isabel Volpe"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "403c0c830aa4a7af43e43d93972988fcf73b0745",
            "title": "BBK-UFRGS@CLEF2009: Query Expansion of Geographic Place Names",
            "abstract": "For our first participation on CLEF, our aim was to compare plain information retrieval strategies and query expansion and emphasis of geographic terms. ANNIE was used to recognise geographic entities which were expanded using Google's Hierarchical List of Geographical Place Names. The idea was that the expansion would produce more accurate answers. The results have shown the opposite. Our best performing run was the baseline. Future work will include further experiments and a deeper analysis of our results in order to enable the design of a better performing strategy.",
            "fieldsOfStudy": [
                "Computer Science",
                "Geography"
            ],
            "authors": [
                {
                    "authorId": "2506429",
                    "name": "Richard Flemmings"
                },
                {
                    "authorId": "145234690",
                    "name": "J. Barros"
                },
                {
                    "authorId": "39594688",
                    "name": "A. Geraldo"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "f66994d3e4f2fcb860fcc2669354952731e0f573",
            "title": "UFRGS@CLEF2008: Indexing Multiword Expressions for Information Retrieval",
            "abstract": "For UFRGS\u2019s participation on CLEF\u2019s Robust task, our aim was to assess the benefits of identifying and indexing Multiword Expressions (MWEs) for Information Retrieval. The approach used for MWE identification was totally statistical, based association measures such as Mutual Information and Chi-square. Contradicting our results on the training topics, the results on the test topics did not show any significant improvements. However, for some queries, the identification of MWEs was very important. We have also performed bilingual experiments which achieved 84% of their monolingual counterparts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2428899",
                    "name": "O. Acosta"
                },
                {
                    "authorId": "39594688",
                    "name": "A. Geraldo"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "145585242",
                    "name": "Aline Villavicencio"
                }
            ]
        },
        {
            "paperId": "6abc98ad645d09fb0837b9ace20c8d40cd58944e",
            "title": "SimEval - A Tool for Evaluating the Quality of Similarity Functions",
            "abstract": "Approximate data matching applications typically use similarity functions to quantify the degree of likeness between two data instances. There are several similarity functions available, thus, it is often necessary to evaluate a number of them aiming at choosing the function that is more adequate to a specific application. This paper presents a tool that uses average precision and discernability to evaluate the quality of similarity functions over a data set.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1714151",
                    "name": "C. Heuser"
                },
                {
                    "authorId": "3008220",
                    "name": "Francisco N. A. Krieser"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                }
            ]
        },
        {
            "paperId": "df78daceb3fe703c0d51b9da71b29aa73564f1cd",
            "title": "A strategy for allowing meaningful and comparable scores in approximate matching",
            "abstract": "The goal of approximate data matching is to assess whether two distinct data instances represent the same real world object. This is usually achieved through the use of a similarity function, which returns a score that defines how similar two data instances are. If this score surpasses a given threshold, both data instances are considered as representing the same real world object. The score values returned by a similarity function depend on the algorithm that implements the function and have no meaning to the user (apart from the fact that a higher similarity value means that two data instances are more similar). In this paper, we propose that instead of defining the threshold in terms of the scores returned by a similarity function, the user specifies the precision that is expected from the matching process. Precision is a well known quality measure and has a clear interpretation from the user's point of view. Our approach relies on mapping between similarity scores and precision values based on a training data set. Experimental results show the training may be executed against a representative data set, and reused for other databases from the same domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2299778",
                    "name": "C. Dorneles"
                },
                {
                    "authorId": "1714151",
                    "name": "C. Heuser"
                },
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "1690426",
                    "name": "A. D. Silva"
                },
                {
                    "authorId": "1733769",
                    "name": "E. Moura"
                }
            ]
        },
        {
            "paperId": "25ecbf0f674be46575b71dc4344ecc4f067722fb",
            "title": "Schema versioning: queries to the generalized temporal database system",
            "abstract": "Raw data and database structures are evolving entities that require adequate support for past, present and even future versions. Temporal databases supporting schema versioning were developed with the aim of satisfying this requirement. This paper considers a generalized temporal database system, which provides support for time at both intensional and extensional levels. The support for schema versioning raises two complex subjects: the storage of the several schema versions and their associate data, and the processing of queries involving more than one schema version. The main goal of this paper is to analyse the second aspect in order to propose a strategy to answer multi-schema queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "1784640",
                    "name": "Nina Edelweiss"
                }
            ]
        },
        {
            "paperId": "4d21cf29843c6578a0c85e863454a077d95efa94",
            "title": "Queries to Temporal Databases Supporting Schema Versioning",
            "abstract": "The conceptual schema (intention) and raw data (extension) are evolving entities which require adequate support for past, present and even future versions. Temporal Databases supporting schema evolution were developed with the aim of satisfying this need. The support for schema versioning raises two complex subjects: the storage of the several schema versions and their associate data, and the processing of queries that involve more than one schema version. The main objective of this work is to analyse the second aspect in order to propose a strategy for answering those queries. In an environment supporting schema versioning the complete history of schema evolution is kept. In many occasions it can be necessary to query the database\u2019s structure, so this work proposes an extension to the temporal query language TSQL2 in order to support queries to intentional data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35052246",
                    "name": "V. Moreira"
                },
                {
                    "authorId": "1784640",
                    "name": "Nina Edelweiss"
                }
            ]
        }
    ]
}