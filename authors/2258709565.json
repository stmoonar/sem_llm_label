{
    "authorId": "2258709565",
    "papers": [
        {
            "paperId": "092f833b61414b6fe314b9695367df9f8a1cf324",
            "title": "Lifelong Personalized Low-Rank Adaptation of Large Language Models for Recommendation",
            "abstract": "We primarily focus on the field of large language models (LLMs) for recommendation, which has been actively explored recently and poses a significant challenge in effectively enhancing recommender systems with logical reasoning abilities and open-world knowledge. Current mainstream efforts mainly center around injecting personalized information from recommendation models into LLMs by customizing input templates or aligning representations between semantic and recommendation spaces at the prediction layer. However, they face three significant limitations: (1) LoRA is mostly used as a core component in existing works, but personalization is not well established in LoRA parameters as the LoRA matrix shared by every user may not cater to different users' characteristics, leading to suboptimal performance. (2) Although lifelong personalized behavior sequences are ideal for personalization, their use raises effectiveness and efficiency issues since LLMs require escalating training and inference time to extend text lengths. (3) Existing approaches aren't scalable for large datasets due to training efficiency constraints. Thus, LLMs only see a small fraction of the datasets (e.g., less than 10%) instead of the whole datasets, limiting their exposure to the full training space. To address these problems, we propose RecLoRA. This model incorporates a Personalized LoRA module that maintains independent LoRAs for different users and a Long-Short Modality Retriever that retrieves different history lengths for different modalities, significantly improving performance while adding minimal time cost. Furthermore, we design a Few2Many Learning Strategy, using a conventional recommendation model as a lens to magnify small training spaces to full spaces. Extensive experiments on public datasets demonstrate the efficacy of our RecLoRA compared to existing baseline models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2296232413",
                    "name": "Jiachen Zhu"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "2105646417",
                    "name": "Xinyi Dai"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2304552745",
                    "name": "Rong Shan"
                },
                {
                    "authorId": "2315165729",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2237958078",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "2240768092",
                    "name": "Weinan Zhang"
                }
            ]
        },
        {
            "paperId": "1616ed0b343c196f33bad2aa704119e13f7b315e",
            "title": "MemoCRS: Memory-enhanced Sequential Conversational Recommender Systems with Large Language Models",
            "abstract": "Conversational recommender systems (CRSs) aim to capture user preferences and provide personalized recommendations through multi-round natural language dialogues. However, most existing CRS models mainly focus on dialogue comprehension and preferences mining from the current dialogue session, overlooking user preferences in historical dialogue sessions. The preferences embedded in the user's historical dialogue sessions and the current session exhibit continuity and sequentiality, and we refer to CRSs with this characteristic as sequential CRSs. In this work, we leverage memory-enhanced LLMs to model the preference continuity, primarily focusing on addressing two key issues: (1) redundancy and noise in historical dialogue sessions, and (2) the cold-start users problem. To this end, we propose a Memory-enhanced Conversational Recommender System Framework with Large Language Models (dubbed MemoCRS) consisting of user-specific memory and general memory. User-specific memory is tailored to each user for their personalized interests and implemented by an entity-based memory bank to refine preferences and retrieve relevant memory, thereby reducing the redundancy and noise of historical sessions. The general memory, encapsulating collaborative knowledge and reasoning guidelines, can provide shared knowledge for users, especially cold-start users. With the two kinds of memory, LLMs are empowered to deliver more precise and tailored recommendations for each user. Extensive experiments on both Chinese and English datasets demonstrate the effectiveness of MemoCRS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056826850",
                    "name": "Yunjia Xi"
                },
                {
                    "authorId": "2130051800",
                    "name": "Weiwen Liu"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2240768092",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2237958078",
                    "name": "Yong Yu"
                }
            ]
        },
        {
            "paperId": "450a0d2ba0d689537230e774219a7695ede1533e",
            "title": "Retrieval-Oriented Knowledge for Click-Through Rate Prediction",
            "abstract": "Click-through rate (CTR) prediction is crucial for personalized online services. Sample-level retrieval-based models, such as RIM, have demonstrated remarkable performance. However, they face challenges including inference inefficiency and high resource consumption due to the retrieval process, which hinder their practical application in industrial settings. To address this, we propose a universal plug-and-play \\underline{r}etrieval-\\underline{o}riented \\underline{k}nowledge (\\textbf{\\name}) framework that bypasses the real retrieval process. The framework features a knowledge base that preserves and imitates the retrieved \\&aggregated representations using a decomposition-reconstruction paradigm. Knowledge distillation and contrastive learning optimize the knowledge base, enabling the integration of retrieval-enhanced representations with various CTR models. Experiments on three large-scale datasets demonstrate \\name's exceptional compatibility and performance, with the neural knowledge base serving as an effective surrogate for the retrieval pool. \\name surpasses the teacher model while maintaining superior inference efficiency and demonstrates the feasibility of distilling knowledge from non-parametric methods using a parametric approach. These results highlight \\name's strong potential for real-world applications and its ability to transform retrieval-based methods into practical solutions. Our implementation code is available to support reproducibility in \\url{https://github.com/HSLiu-Initial/ROK.git}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167046956",
                    "name": "Huanshuo Liu"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2238203237",
                    "name": "Menghui Zhu"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "79494403",
                    "name": "Jiarui Qin"
                },
                {
                    "authorId": "2290248265",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "2298987734",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "4d24f1dcf52cea69277ede519b3c27969fa93a47",
            "title": "Large Language Models Make Sample-Efficient Recommender Systems",
            "abstract": "Large language models (LLMs) have achieved remarkable progress in the field of natural language processing (NLP), demonstrating remarkable abilities in producing text that resembles human language for various tasks. This opens up new opportunities for employing them in recommender systems (RSs). In this paper, we specifically examine the sample efficiency of LLM-enhanced recommender systems, which pertains to the model's capacity to attain superior performance with a limited quantity of training data. Conventional recommendation models (CRMs) often need a large amount of training data because of the sparsity of features and interactions. Hence, we propose and verify our core viewpoint: Large Language Models Make Sample-Efficient Recommender Systems. We propose a simple yet effective framework (i.e., Laser) to validate the viewpoint from two aspects: (1) LLMs themselves are sample-efficient recommenders; and (2) LLMs, as feature generators and encoders, make CRMs more sample-efficient. Extensive experiments on two public datasets show that Laser requires only a small fraction of training samples to match or even surpass CRMs that are trained on the entire training set, demonstrating superior sample efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "2105646417",
                    "name": "Xinyi Dai"
                },
                {
                    "authorId": "2304552745",
                    "name": "Rong Shan"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2237958078",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "2240768092",
                    "name": "Weinan Zhang"
                }
            ]
        },
        {
            "paperId": "52dc538cf0f0b979105712ae073ca125602cda25",
            "title": "AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising",
            "abstract": "Click-Through Rate (CTR) prediction is a fundamental technique for online advertising recommendation and the complex online competitive auction process also brings many difficulties to CTR optimization. Recent studies have shown that introducing posterior auction information contributes to the performance of CTR prediction. However, existing work doesn't fully capitalize on the benefits of auction information and overlooks the data bias brought by the auction, leading to biased and suboptimal results. To address these limitations, we propose Auction Information Enhanced Framework (AIE) for CTR prediction in online advertising, which delves into the problem of insufficient utilization of auction signals and first reveals the auction bias. Specifically, AIE introduces two pluggable modules, namely Adaptive Market-price Auxiliary Module (AM2) and Bid Calibration Module (BCM), which work collaboratively to excavate the posterior auction signals better and enhance the performance of CTR prediction. Furthermore, the two proposed modules are lightweight, model-agnostic, and friendly to inference latency. Extensive experiments are conducted on a public dataset and an industrial dataset to demonstrate the effectiveness and compatibility of AIE. Besides, a one-month online A/B test in a large-scale advertising platform shows that AIE improves the base model by 5.76% and 2.44% in terms of eCPM and CTR, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2290248265",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2115802321",
                    "name": "Chenxu Zhu"
                },
                {
                    "authorId": "2238203237",
                    "name": "Menghui Zhu"
                },
                {
                    "authorId": "2105646417",
                    "name": "Xinyi Dai"
                },
                {
                    "authorId": "3339005",
                    "name": "Huifeng Guo"
                },
                {
                    "authorId": "2287873067",
                    "name": "Muyu Zhang"
                },
                {
                    "authorId": "2274021958",
                    "name": "Zhenhua Dong"
                },
                {
                    "authorId": "2262216857",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "5d12028de85e84cb0081a3f7eda9a4c6fd031eb3",
            "title": "Extracting Essential and Disentangled Knowledge for Recommendation Enhancement",
            "abstract": "Recommender models play a vital role in various industrial scenarios, while often faced with the catastrophic forgetting problem caused by the fast shifting data distribution, e.g., the evolving user interests, click signals fluctuation during sales promotions, etc. To alleviate this problem, a common approach is to reuse knowledge from the historical data. However, preserving the vast and fast-accumulating data is hard, which causes dramatic storage overhead. Memorizing old data through a parametric knowledge base is then proposed, which compresses the vast amount of raw data into model parameters. Despite the flexibility, how to improve the memorization and generalization capabilities of the parametric knowledge base is challenging. In this paper, we propose two constraints to extract Essential and Disentangled Knowledge from past data for rational and generalized recommendation enhancement, which improves the capabilities of the parametric knowledge base without increasing the size of it. The essential principle helps to compress the input into representative vectors that capture the task-relevant information and filter out the noisy information. The disentanglement principle reduces the redundancy of stored information and pushes the knowledge base to focus on capturing the disentangled invariant patterns. These two rules together promote rational compression of information for robust and generalized knowledge representations. Extensive experiments on two datasets justify the effectiveness of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2304517128",
                    "name": "Jizheng Chen"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "2238203237",
                    "name": "Menghui Zhu"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2305537473",
                    "name": "Shuai Li"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "76d87ee792ac0c47cae7bd55d68694e9dfed091d",
            "title": "ELCoRec: Enhance Language Understanding with Co-Propagation of Numerical and Categorical Features for Recommendation",
            "abstract": "Large language models have been flourishing in the natural language processing (NLP) domain, and their potential for recommendation has been paid much attention to. Despite the intelligence shown by the recommendation-oriented finetuned models, LLMs struggle to fully understand the user behavior patterns due to their innate weakness in interpreting numerical features and the overhead for long context, where the temporal relations among user behaviors, subtle quantitative signals among different ratings, and various side features of items are not well explored. Existing works only fine-tune a sole LLM on given text data without introducing that important information to it, leaving these problems unsolved. In this paper, we propose ELCoRec to Enhance Language understanding with CoPropagation of numerical and categorical features for Recommendation. Concretely, we propose to inject the preference understanding capability into LLM via a GAT expert model where the user preference is better encoded by parallelly propagating the temporal relations, and rating signals as well as various side information of historical items. The parallel propagation mechanism could stabilize heterogeneous features and offer an informative user preference encoding, which is then injected into the language models via soft prompting at the cost of a single token embedding. To further obtain the user's recent interests, we proposed a novel Recent interaction Augmented Prompt (RAP) template. Experiment results over three datasets against strong baselines validate the effectiveness of ELCoRec. The code is available at https://anonymous.4open.science/r/CIKM_Code_Repo-E6F5/README.md.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2304517128",
                    "name": "Jizheng Chen"
                },
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2240768092",
                    "name": "Weinan Zhang"
                }
            ]
        },
        {
            "paperId": "8a439444d888202a711b8b8a195934cdb138342e",
            "title": "Towards Efficient and Effective Unlearning of Large Language Models for Recommendation",
            "abstract": "The significant advancements in large language models (LLMs) give rise to a promising research direction, i.e., leveraging LLMs as recommenders (LLMRec). The efficacy of LLMRec arises from the open-world knowledge and reasoning capabilities inherent in LLMs. LLMRec acquires the recommendation capabilities through instruction tuning based on user interaction data. However, in order to protect user privacy and optimize utility, it is also crucial for LLMRec to intentionally forget specific user data, which is generally referred to as recommendation unlearning. In the era of LLMs, recommendation unlearning poses new challenges for LLMRec in terms of \\textit{inefficiency} and \\textit{ineffectiveness}. Existing unlearning methods require updating billions of parameters in LLMRec, which is costly and time-consuming. Besides, they always impact the model utility during the unlearning process. To this end, we propose \\textbf{E2URec}, the first \\underline{E}fficient and \\underline{E}ffective \\underline{U}nlearning method for LLM\\underline{Rec}. Our proposed E2URec enhances the unlearning efficiency by updating only a few additional LoRA parameters, and improves the unlearning effectiveness by employing a teacher-student framework, where we maintain multiple teacher networks to guide the unlearning process. Extensive experiments show that E2URec outperforms state-of-the-art baselines on two real-world datasets. Specifically, E2URec can efficiently forget specific data without affecting recommendation performance. The source code is at \\url{https://github.com/justarter/E2URec}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283141638",
                    "name": "Hangyu Wang"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2290248265",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2240768092",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2237958078",
                    "name": "Yong Yu"
                }
            ]
        },
        {
            "paperId": "941c29bd7a655418b2c825ef079a1987cce9946b",
            "title": "A Decoding Acceleration Framework for Industrial Deployable LLM-based Recommender Systems",
            "abstract": "Recently, increasing attention has been paid to LLM-based recommender systems, but their deployment is still under exploration in the industry. Most deployments utilize LLMs as feature enhancers, generating augmentation knowledge in the offline stage. However, in recommendation scenarios, involving numerous users and items, even offline generation with LLMs consumes considerable time and resources. This generation inefficiency stems from the autoregressive nature of LLMs, and a promising direction for acceleration is speculative decoding, a Draft-then-Verify paradigm that increases the number of generated tokens per decoding step. In this paper, we first identify that recommendation knowledge generation is suitable for retrieval-based speculative decoding. Then, we discern two characteristics: (1) extensive items and users in RSs bring retrieval inefficiency, and (2) RSs exhibit high diversity tolerance for text generated by LLMs. Based on the above insights, we propose a Decoding Acceleration Framework for LLM-based Recommendation (dubbed DARE), with Customized Retrieval Pool to improve retrieval efficiency and Relaxed Verification to increase the acceptance rate of draft tokens, respectively. Extensive experiments demonstrate that DARE achieves a 3-5x speedup and is compatible with various frameworks and backbone LLMs. DARE has also been deployed to online advertising scenarios within a large-scale commercial environment, achieving a 3.45x speedup while maintaining the downstream performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056826850",
                    "name": "Yunjia Xi"
                },
                {
                    "authorId": "2283141638",
                    "name": "Hangyu Wang"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "2238203237",
                    "name": "Menghui Zhu"
                },
                {
                    "authorId": "2130051800",
                    "name": "Weiwen Liu"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2240768092",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2237958078",
                    "name": "Yong Yu"
                }
            ]
        },
        {
            "paperId": "9ffab7e041dc10f4ae3f8acb20aed55bd8fc90f7",
            "title": "LLM-enhanced Reranking in Recommender Systems",
            "abstract": "Reranking is a critical component in recommender systems, playing an essential role in refining the output of recommendation algorithms. Traditional reranking models have focused predominantly on accuracy, but modern applications demand consideration of additional criteria such as diversity and fairness. Existing reranking approaches often fail to harmonize these diverse criteria effectively at the model level. Moreover, these models frequently encounter challenges with scalability and personalization due to their complexity and the varying significance of different reranking criteria in diverse scenarios. In response, we introduce a comprehensive reranking framework enhanced by LLM, designed to seamlessly integrate various reranking criteria while maintaining scalability and facilitating personalized recommendations. This framework employs a fully connected graph structure, allowing the LLM to simultaneously consider multiple aspects such as accuracy, diversity, and fairness through a coherent Chain-of-Thought (CoT) process. A customizable input mechanism is also integrated, enabling the tuning of the language model's focus to meet specific reranking needs. We validate our approach using three popular public datasets, where our framework demonstrates superior performance over existing state-of-the-art reranking models in balancing multiple criteria. The code for this implementation is publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2161309826",
                    "name": "Jingtong Gao"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2238104000",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2130051800",
                    "name": "Weiwen Liu"
                },
                {
                    "authorId": "2181637944",
                    "name": "Xiangyang Li"
                },
                {
                    "authorId": "2262403807",
                    "name": "Yichao Wang"
                },
                {
                    "authorId": "2187882131",
                    "name": "Zijian Zhang"
                },
                {
                    "authorId": "2211473272",
                    "name": "Wanyu Wang"
                },
                {
                    "authorId": "2307408755",
                    "name": "Yuyang Ye"
                },
                {
                    "authorId": "2293567811",
                    "name": "Shanru Lin"
                },
                {
                    "authorId": "3339005",
                    "name": "Huifeng Guo"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                }
            ]
        }
    ]
}