{
    "authorId": "145485960",
    "papers": [
        {
            "paperId": "1d6ba7435383ab645a8b6d02c48a95a863eeda2c",
            "title": "Unified Contextual Query Rewriting",
            "abstract": "Query rewriting (QR) is an important technique for user friction (i.e. recovering ASR error or system error) reduction and contextual carryover (i.e. ellipsis and co-reference) in conversational AI systems. Recently, generation-based QR models have achieved promising results on these two tasks separately. Although these two tasks have many similarities such as they both use the previous dialogue along with the current request as model input, there is no unified model to solve them jointly. To this end, we propose a unified contextual query rewriting model that unifies QR for both reducing friction and contextual carryover purpose. Moreover, we involve multiple auxiliary tasks such as trigger prediction and NLU interpretation tasks to boost the performance of the rewrite. We leverage the text-to-text unified framework which uses independent tasks with weighted loss to account for task importance. Then we propose new unified multitask learning strategies including a sequential model which outputs one sentence for multi-tasks, and a hybrid model where some tasks are independent and some tasks are sequentially generated. Our experimental results demonstrate the effectiveness of the proposed unified learning methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118861187",
                    "name": "Yingxue Zhou"
                },
                {
                    "authorId": "145485960",
                    "name": "Jie Hao"
                },
                {
                    "authorId": "35315626",
                    "name": "Mukund Rungta"
                },
                {
                    "authorId": "2152797245",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "4006425",
                    "name": "Eunah Cho"
                },
                {
                    "authorId": "2117800368",
                    "name": "Xing Fan"
                },
                {
                    "authorId": "2220668748",
                    "name": "Yanbin Lu"
                },
                {
                    "authorId": "7159482",
                    "name": "V. Vasudevan"
                },
                {
                    "authorId": "39312387",
                    "name": "Kellen Gillespie"
                },
                {
                    "authorId": "3274284",
                    "name": "Zeynab Raeesy"
                }
            ]
        },
        {
            "paperId": "53871ac59c687d312dfdd89f0dde82f79c5faf76",
            "title": "PAIGE: Personalized Adaptive Interactions Graph Encoder for Query Rewriting in Dialogue Systems",
            "abstract": "Unexpected responses or repeated clarification questions from conversational agents detract from the users\u2019 experience with technology meant to streamline their daily tasks. To reduce these frictions, Query Rewriting ( QR ) techniques replace transcripts of faulty queries with alternatives that lead to responses that satisfy the users\u2019 needs. Despite their successes, existing QR approaches are limited in their ability to fix queries that require considering users\u2019 personal preferences. We improve QR by proposing P ersonalized A daptive I nteractions G raph E ncoder (PAIGE). PAIGE is the first QR architecture that jointly models user\u2019s affinities and query semantics end-to-end. The core idea is to represent previous user-agent interactions and world knowledge in a structured form \u2014 a heterogeneous graph \u2014 and apply message passing to propagate latent representations of users\u2019 affinities to refine utterance embeddings. Using these embeddings, PAIGE can potentially provide different rewrites given the same query for users with different preferences. Our model, trained without any human-annotated data, improves the rewrite retrieval precision of state-of-the-art baselines by 12.5\u201317.5% while having nearly ten times fewer parameters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "65987423",
                    "name": "Daniel Bis"
                },
                {
                    "authorId": "2118973203",
                    "name": "Saurabh Gupta"
                },
                {
                    "authorId": "145485960",
                    "name": "Jie Hao"
                },
                {
                    "authorId": "2117800368",
                    "name": "Xing Fan"
                },
                {
                    "authorId": "1941997",
                    "name": "Chenlei Guo"
                }
            ]
        },
        {
            "paperId": "baa45dc8820f0ffe457d4c31d054e3ea1fce136b",
            "title": "Improving Mandarin Speech Recogntion with Block-augmented Transformer",
            "abstract": "Recently Convolution-augmented Transformer (Conformer) has shown promising results in Automatic Speech Recognition (ASR), outperforming the previous best published Transformer Transducer. In this work, we believe that the output information of each block in the encoder and decoder is not completely inclusive, in other words, their output information may be complementary. We study how to take advantage of the complementary information of each block in a parameter-efficient way, and it is expected that this may lead to more robust performance. Therefore we propose the Block-augmented Transformer for speech recognition, named Blockformer. We have implemented two block ensemble methods: the base Weighted Sum of the Blocks Output (Base-WSBO), and the Squeeze-and-Excitation module to Weighted Sum of the Blocks Output (SE-WSBO). Experiments have proved that the Blockformer significantly outperforms the state-of-the-art Conformer-based models on AISHELL-1, our model achieves a CER of 4.29\\% without using a language model and 4.05\\% with an external language model on the testset.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2179187148",
                    "name": "Xiaoming Ren"
                },
                {
                    "authorId": "2115717933",
                    "name": "Huifeng Zhu"
                },
                {
                    "authorId": "2111320252",
                    "name": "Liuwei Wei"
                },
                {
                    "authorId": "2145209743",
                    "name": "Minghui Wu"
                },
                {
                    "authorId": "145485960",
                    "name": "Jie Hao"
                }
            ]
        },
        {
            "paperId": "c8525b6119a6ec89bd64a97a6ee5d7d377f8841c",
            "title": "CGF: Constrained Generation Framework for Query Rewriting in Conversational AI",
            "abstract": "In conversational AI agents, Query Rewriting 001 (QR) plays a crucial role in reducing users fric- 002 tions and satisfying their daily demands. Users 003 frictions are caused by various reasons, such 004 as errors in the spoken dialogue system, users\u2019 005 accent or their abridged language. In this work, 006 we present a novel Constrained Generation 007 Framework (CGF) for query rewriting at both 008 global and personalized level. The proposed 009 framework is based on the encoder-decoder 010 framework and consists of a context-enhanced 011 encoding and constrained generation decoding 012 phrases. The model takes the query and its 013 previous dialogue context information as the 014 encoder input, then the decoder relies on the 015 pre-defined global or personalized constrained 016 decoding space to generate the rewrites. Ex- 017 tensive offline and online A/B experimental re- 018 sults show that the proposed CGF significantly 019 boosts the query rewriting performance. 020",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145485960",
                    "name": "Jie Hao"
                },
                {
                    "authorId": "2152797245",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2117800368",
                    "name": "Xing Fan"
                },
                {
                    "authorId": "2118973203",
                    "name": "Saurabh Gupta"
                },
                {
                    "authorId": "2805456",
                    "name": "Saleh Soltan"
                },
                {
                    "authorId": "147887099",
                    "name": "Rakesh Chada"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "1941997",
                    "name": "Chenlei Guo"
                },
                {
                    "authorId": "1748051",
                    "name": "G\u00f6khan T\u00fcr"
                }
            ]
        },
        {
            "paperId": "dde0be499b277f52dc3a730e90005c6c19a049cb",
            "title": "Overcoming Catastrophic Forgetting During Domain Adaptation of Seq2seq Language Generation",
            "abstract": "Seq2seq language generation models that are trained offline with multiple domains in a sequential fashion often suffer from catastrophic forgetting. Lifelong learning has been proposed to handle this problem. However, existing work such as experience replay or elastic weighted consolidation requires incremental memory space. In this work, we propose an innovative framework, RMR_DSEthat leverages a recall optimization mechanism to selectively memorize important parameters of previous tasks via regularization, and uses a domain drift estimation algorithm to compensate the drift between different do-mains in the embedding space. These designs enable the model to be trained on the current task while keep-ing the memory of previous tasks, and avoid much additional data storage. Furthermore, RMR_DSE can be combined with existing lifelong learning approaches. Our experiments on two seq2seq language generation tasks, paraphrase and dialog response generation, show thatRMR_DSE outperforms SOTA models by a considerable margin and reduces forgetting greatly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34377382",
                    "name": "Dingcheng Li"
                },
                {
                    "authorId": "2141144864",
                    "name": "Zheng Chen"
                },
                {
                    "authorId": "4006425",
                    "name": "Eunah Cho"
                },
                {
                    "authorId": "145485960",
                    "name": "Jie Hao"
                },
                {
                    "authorId": "8015600",
                    "name": "Xiaohu Liu"
                },
                {
                    "authorId": "2117800368",
                    "name": "Xing Fan"
                },
                {
                    "authorId": "1941997",
                    "name": "Chenlei Guo"
                }
            ]
        },
        {
            "paperId": "e0caecf4f6fc3af361b93e10999a4b5c7c49c2a3",
            "title": "Reversible data hiding scheme based on image partitioning and histogram shifting",
            "abstract": "This paper proposes a new reversible data hiding scheme based on image partitioning and histogram shifting. The existing histogram shifting reversible data hiding scheme has been presented based on finding the values of the peak point and the zero point and then moving the histogram to modify the relevant pixel values to embed information. Although this scheme has several advantages, its embedding capacity is greatly restricted, and its image quality is not very good. Aiming at this vulnerability, a new method is proposed: first dividing the image into blocks, then sorting the embedding order of the sub-images according to specific rules, and finally, embedding information according to the sorted sub-image order. The sorting rule is an ascending order based on the number of invalid pixel values that need to be modified. Experimental results show that the image embedding capacity is significantly improved with guaranteed quality. And when the amount of payload to be embedded is less than the entire payload, this scheme improves image quality by reducing unnecessary pixel value shifts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145485960",
                    "name": "Jie Hao"
                },
                {
                    "authorId": "49111107",
                    "name": "Ping Ping"
                },
                {
                    "authorId": "2115813542",
                    "name": "Xia Peng"
                },
                {
                    "authorId": "49538679",
                    "name": "Zeyu Gao"
                }
            ]
        },
        {
            "paperId": "ecdee4c3e7c6a5ce0c25c4d24bbfa363e1bbb5aa",
            "title": "PENTATRON: PErsonalized coNText-Aware Transformer for Retrieval-based cOnversational uNderstanding",
            "abstract": "Conversational understanding is an integral part of modern intelligent devices. In a large fraction of the global traffic from customers using smart digital assistants, frictions in dialogues may be attributed to incorrect understanding of the entities in a customer's query due to factors including ambiguous mentions, mispronunciation, background noise and faulty on-device signal processing. Such errors are compounded by two common deficiencies from intelligent devices namely, (1) the device not being tailored to individual customers, and (2) the device responses being unaware of the context in the conversation session. Viewing this problem via the lens of retrieval-based search engines, we build and evaluate a scalable entity correction system, PENTATRON. The system leverages a parametric transformer-based language model to learn patterns from in-session customer-device interactions coupled with a non-parametric personalized entity index to compute the correct query, which aids downstream components in reasoning about the best response. In addition to establishing baselines and demonstrating the value of personalized and context-aware systems, we use multitasking to learn the domain of the correct entity. We also investigate the utility of language model prompts. Through extensive experiments, we show a significant upward movement of the key metric (Exact Match) by up to 500.97% (relative to the baseline).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143795841",
                    "name": "Niranjan Uma Naresh"
                },
                {
                    "authorId": "2112347577",
                    "name": "Ziyan Jiang"
                },
                {
                    "authorId": "2128405653",
                    "name": "Ankit"
                },
                {
                    "authorId": "2108230831",
                    "name": "Sungjin Lee"
                },
                {
                    "authorId": "145485960",
                    "name": "Jie Hao"
                },
                {
                    "authorId": "2117800368",
                    "name": "Xing Fan"
                },
                {
                    "authorId": "1941997",
                    "name": "Chenlei Guo"
                }
            ]
        },
        {
            "paperId": "2cb55edf88bd45dbcc65c126611adba3bdb99b6b",
            "title": "Wind Power Short-Term Forecasting Model Based on the Hierarchical Output Power and Poisson Re-Sampling Random Forest Algorithm",
            "abstract": "Under the background of big data, the use of massive online data to improve the real-time characteristics and reliability of wind power prediction and to reduce the impact of wind farms on the power grid makes the power supply and demand balance important problems to solve. This paper provides a new solution for short-term wind power forecasting to address these problems. In this paper, an improved random forest short-term prediction model based on the hierarchical output power is proposed, and it is used to forecast the power output of a real wind farm located in Northwest China. First, a chi-square test is adopted to discretize the power data to divide the large-scale training data and remove abnormal data. The novelty of this study is the establishment of a classification model with the output wind power as the classification target and the use of Poisson re-sampling to replace the bootstrap method of the random forest, that is, to improve the training speed of the random forest algorithm. The results indicate that the proposed technique can estimate the output wind power with an MSE of 0.0232, and the comparison illustrates the effectiveness and superiority of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145485960",
                    "name": "Jie Hao"
                },
                {
                    "authorId": "2118508723",
                    "name": "Changsheng Zhu"
                },
                {
                    "authorId": "1575844494",
                    "name": "Xiuting Guo"
                }
            ]
        },
        {
            "paperId": "56dc1ce4df5ce80a48743be8ebd38025bdba24bf",
            "title": "RAST: Domain-Robust Dialogue Rewriting as Sequence Tagging",
            "abstract": "The task of dialogue rewriting aims to reconstruct the latest dialogue utterance by copying the missing content from the dialogue context. Until now, the existing models for this task suffer from the robustness issue, i.e., performances drop dramatically when testing on a different dataset. We address this robustness issue by proposing a novel sequence-tagging-based model so that the search space is significantly reduced, yet the core of this task is still well covered. As a common issue of most tagging models for text generation, the model\u2019s outputs may lack fluency. To alleviate this issue, we inject the loss signal from BLEU or GPT-2 under a REINFORCE framework. Experiments show huge improvements of our model over the current state-of-the-art systems when transferring to another dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145485960",
                    "name": "Jie Hao"
                },
                {
                    "authorId": "50258954",
                    "name": "Linfeng Song"
                },
                {
                    "authorId": "2142450232",
                    "name": "Liwei Wang"
                },
                {
                    "authorId": "2113451979",
                    "name": "Kun Xu"
                },
                {
                    "authorId": "2909321",
                    "name": "Zhaopeng Tu"
                },
                {
                    "authorId": "2111505433",
                    "name": "Dong Yu"
                }
            ]
        },
        {
            "paperId": "6895ba57095946a5e013bfa5dae214dcd9ad301c",
            "title": "Optimal Quaternary (r, delta)-Locally Repairable Codes Achieving the Singleton-type Bound",
            "abstract": "Locally repairable codes enables fast repair of node failure in a distributed storage system. The code symbols in a codeword are stored in different storage nodes, such that a disk failure can be recovered by accessing a small fraction of the storage nodes. The number of storage nodes that are contacted during the repair of a failed node is a parameter called locality. We consider locally repairable codes that can be locally recovered in the presence of multiple node failures. The punctured code obtained by removing the code symbols in the complement of a repair group is called a local code. We aim at designing a code such that all local codes have a prescribed minimum distance, so that any node failure can be repaired locally, provided that the total number of node failures is less than the tolerance parameter. We consider linear locally repairable codes defined over a finite field of size four. This alphabet has characteristic 2, and hence is amenable to practical implementation. We classify all quaternary locally repairable codes that attain the Singleton-type upper bound for minimum distance. For each combination of achievable code parameters, an explicit code construction is given.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1731849",
                    "name": "K. Shum"
                },
                {
                    "authorId": "145485960",
                    "name": "Jie Hao"
                }
            ]
        }
    ]
}