{
    "authorId": "7621447",
    "papers": [
        {
            "paperId": "105669ec59a58fb2d4dd3021a984af33c227c5ab",
            "title": "Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs",
            "abstract": "Learning on Graphs has attracted immense attention due to its wide real-world applications. The most popular pipeline for learning on graphs with textual node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes shallow text embedding as initial node representations, which has limitations in general knowledge and profound semantic understanding. In recent years, Large Language Models (LLMs) have been proven to possess extensive common knowledge and powerful semantic comprehension abilities that have revolutionized existing workflows to handle text data. In this paper, we aim to explore the potential of LLMs in graph machine learning, especially the node classification task, and investigate two possible pipelines: LLMs-as-Enhancers and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text attributes with their massive knowledge and then generate predictions through GNNs. The latter attempts to directly employ LLMs as standalone predictors. We conduct comprehensive and systematical studies on these two pipelines under various settings. From comprehensive empirical results, we make original observations and find new insights that open new possibilities and suggest promising directions to leverage LLMs for learning on graphs. Our codes and datasets are available at: https://github.com/CurryTang/Graph-LLM .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109393101",
                    "name": "Zhikai Chen"
                },
                {
                    "authorId": "2125202063",
                    "name": "Haitao Mao"
                },
                {
                    "authorId": "2145571830",
                    "name": "Hang Li"
                },
                {
                    "authorId": "144767914",
                    "name": "Wei Jin"
                },
                {
                    "authorId": "30580446",
                    "name": "Haifang Wen"
                },
                {
                    "authorId": "7621447",
                    "name": "Xiaochi Wei"
                },
                {
                    "authorId": "2386396",
                    "name": "Shuaiqiang Wang"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "86fb5ac3cdd1413a6d040b0d892a909bcdfbf901",
            "title": "Boosting Event Extraction with Denoised Structure-to-Text Augmentation",
            "abstract": "Event extraction aims to recognize pre-defined event triggers and arguments from texts, which suffer from the lack of high-quality annotations. In most NLP applications, involving a large scale of synthetic training data is a practical and effective approach to alleviate the problem of data scarcity. However, when applying to the task of event extraction, recent data augmentation methods often neglect the problem of grammatical incorrectness, structure misalignment, and semantic drifting, leading to unsatisfactory performances. In order to solve these problems, we propose a denoised structure-to-text augmentation framework for event extraction DAEE, which generates additional training data through the knowledge-based structure-to-text generation model and selects the effective subset from the generated data iteratively with a deep reinforcement learning agent. Experimental results on several datasets demonstrate that the proposed method generates more diverse text representations for event extraction and achieves comparable results with the state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2217713470",
                    "name": "Bo Wang"
                },
                {
                    "authorId": "4590286",
                    "name": "Heyan Huang"
                },
                {
                    "authorId": "7621447",
                    "name": "Xiaochi Wei"
                },
                {
                    "authorId": "2067725506",
                    "name": "Ge Shi"
                },
                {
                    "authorId": "49544272",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "144579978",
                    "name": "Chong Feng"
                },
                {
                    "authorId": "2114110926",
                    "name": "Tong Zhou"
                },
                {
                    "authorId": "2146516597",
                    "name": "Shuai Wang"
                },
                {
                    "authorId": "2136400100",
                    "name": "Dawei Yin"
                }
            ]
        },
        {
            "paperId": "5e17da61b4c7565a8b2a91780fcda456cf6af705",
            "title": "Multi-Modal Relational Graph for Cross-Modal Video Moment Retrieval",
            "abstract": "Given an untrimmed video and a query sentence, cross-modal video moment retrieval aims to rank a video moment from pre-segmented video moment candidates that best matches the query sentence. Pioneering work typically learns the representations of the textual and visual content separately and then obtains the interactions or alignments between different modalities. However, the task of cross-modal video moment retrieval is not yet thoroughly addressed as it needs to further identify the fine-grained differences of video moment candidates with high repeatability and similarity. Moveover, the relation among objects in both video and sentence is intuitive and efficient for understanding semantics but is rarely considered.Toward this end, we contribute a multi-modal relational graph to capture the interactions among objects from the visual and textual content to identify the differences among similar video moment candidates. Specifically, we first introduce a visual relational graph and a textual relational graph to form relation-aware representations via message propagation. Thereafter, a multi-task pre-training is designed to capture domain-specific knowledge about objects and relations, enhancing the structured visual representation after explicitly defined relation. Finally, the graph matching and boundary regression are employed to perform the cross-modal retrieval. We conduct extensive experiments on two datasets about daily activities and cooking activities, demonstrating significant improvements over state-of-the-art solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1993664989",
                    "name": "Yawen Zeng"
                },
                {
                    "authorId": "8775916",
                    "name": "Da Cao"
                },
                {
                    "authorId": "7621447",
                    "name": "Xiaochi Wei"
                },
                {
                    "authorId": "2152972434",
                    "name": "Meng Liu"
                },
                {
                    "authorId": "47122432",
                    "name": "Zhou Zhao"
                },
                {
                    "authorId": "1489386471",
                    "name": "Zheng Qin"
                }
            ]
        },
        {
            "paperId": "57e32d2bec0cc2e775a28bf8737287d841878e9d",
            "title": "A Discriminative Convolutional Neural Network with Context-aware Attention",
            "abstract": "Feature representation and feature extraction are two crucial procedures in text mining. Convolutional Neural Networks (CNN) have shown overwhelming success for text-mining tasks, since they are capable of efficiently extracting n-gram features from source data. However, vanilla CNN has its own weaknesses on feature representation and feature extraction. A certain amount of filters in CNN are inevitably duplicate and thus hinder to discriminatively represent a given text. In addition, most existing CNN models extract features in a fixed way (i.e., max pooling) that either limit the CNN to local optimum nor without considering the relation between all features, thereby unable to learn a contextual n-gram features adaptively. In this article, we propose a discriminative CNN with context-aware attention to solve the challenges of vanilla CNN. Specifically, our model mainly encourages discrimination across different filters via maximizing their earth mover distances and estimates the salience of feature candidates by considering the relation between context features. We validate carefully our findings against baselines on five benchmark datasets of classification and two datasets of summarization. The results of the experiments verify the competitive performance of our proposed model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2180531922",
                    "name": "Yuxiang Zhou"
                },
                {
                    "authorId": "3000498",
                    "name": "L. Liao"
                },
                {
                    "authorId": "2145971366",
                    "name": "Yang Gao"
                },
                {
                    "authorId": "4590286",
                    "name": "Heyan Huang"
                },
                {
                    "authorId": "7621447",
                    "name": "Xiaochi Wei"
                }
            ]
        },
        {
            "paperId": "7928ff4b66e866e7052915cd34861f1db2288cc4",
            "title": "Adversarial Video Moment Retrieval by Jointly Modeling Ranking and Localization",
            "abstract": "Retrieving video moments from an untrimmed video given a natural language as the query is a challenging task in both academia and industry. Although much effort has been made to address this issue, traditional video moment ranking methods are unable to generate reasonable video moment candidates and video moment localization approaches are not applicable to large-scale retrieval scenario. How to combine ranking and localization into a unified framework to overcome their drawbacks and reinforce each other is rarely considered. Toward this end, we contribute a novel solution to thoroughly investigate the video moment retrieval issue under the adversarial learning paradigm. The key of our solution is to formulate the video moment retrieval task as an adversarial learning problem with two tightly connected components. Specifically, a reinforcement learning is employed as a generator to produce a set of possible video moments. Meanwhile, a pairwise ranking model is utilized as a discriminator to rank the generated video moments and the ground truth. Finally, the generator and the discriminator are mutually reinforced in the adversarial learning framework, which is able to jointly optimize the performance of both video moment ranking and video moment localization. Extensive experiments on two well-known datasets have well verified the effectiveness and rationality of our proposed solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8775916",
                    "name": "Da Cao"
                },
                {
                    "authorId": "1993664989",
                    "name": "Yawen Zeng"
                },
                {
                    "authorId": "7621447",
                    "name": "Xiaochi Wei"
                },
                {
                    "authorId": "143982887",
                    "name": "Liqiang Nie"
                },
                {
                    "authorId": "48043335",
                    "name": "Richang Hong"
                },
                {
                    "authorId": "1489386471",
                    "name": "Zheng Qin"
                }
            ]
        },
        {
            "paperId": "131019f4c3c6f7fad917156e979c2d111639c1be",
            "title": "Neural Variational Correlated Topic Modeling",
            "abstract": "With the rapid development of the Internet, millions of documents, such as news and web pages, are generated everyday. Mining the topics and knowledge on them has attracted a lot of interest on both academic and industrial areas. As one of the prevalent unsupervised data mining tools, topic models are usually explored as probabilistic generative models for large collections of texts. Traditional probabilistic topic models tend to find a closed form solution of model parameters and approach the intractable posteriors via approximation methods, which usually lead to the inaccurate inference of parameters and low efficiency when it comes to a quite large volume of data. Recently, an emerging trend of neural variational inference can overcome the above issues, which offers a scalable and powerful deep generative framework for modeling latent topics via neural networks. Interestingly, a common assumption for the most neural variational topic models is that topics are independent and irrelevant to each other. However, this assumption is unreasonable in many practical scenarios. In this paper, we propose a novel Centralized Transformation Flow to capture the correlations among topics by reshaping topic distributions. Furthermore, we present the Transformation Flow Lower Bound to improve the performance of the proposed model. Extensive experiments on two standard benchmark datasets have well-validated the effectiveness of the proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118467297",
                    "name": "Luyang Liu"
                },
                {
                    "authorId": "4590286",
                    "name": "Heyan Huang"
                },
                {
                    "authorId": "145644809",
                    "name": "Yang Gao"
                },
                {
                    "authorId": "1739818",
                    "name": "Yongfeng Zhang"
                },
                {
                    "authorId": "7621447",
                    "name": "Xiaochi Wei"
                }
            ]
        },
        {
            "paperId": "4616edabd1c77c1056d7400d07dc59f8298c2bea",
            "title": "Quality Matters: Assessing cQA Pair Quality via Transductive Multi-View Learning",
            "abstract": "Community-based question answering (cQA) sites have become important knowledge sharing platforms, as massive cQA pairs are archived, but the uneven quality of cQA pairs leaves information seekers unsatisfied. Various efforts have been dedicated to predicting the quality of cQA contents. Most of them concatenate different features into single vectors and then feed them into regression models. In fact, the quality of cQA pairs is influenced by different views, and the agreement among them is essential for quality assessment. Besides, the lacking of labeled data significantly hinders the quality prediction performance. Toward this end, we present a transductive multi-view learning model. It is designed to find a latent common space by unifying and preserving information from various views, including question, answer, QA relevance, asker, and answerer. Additionally, rich information in the unlabeled test cQA pairs are utilized via transductive learning to enhance the representation ability of the common space. Extensive experiments on real-world datasets have well-validated the proposed model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7621447",
                    "name": "Xiaochi Wei"
                },
                {
                    "authorId": "4590286",
                    "name": "Heyan Huang"
                },
                {
                    "authorId": "143982887",
                    "name": "Liqiang Nie"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "48043335",
                    "name": "Richang Hong"
                },
                {
                    "authorId": "144078686",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "50f4dce0af8749447692a987a86b12b43693d65a",
            "title": "Task-oriented Word Embedding for Text Classification",
            "abstract": "Distributed word representation plays a pivotal role in various natural language processing tasks. In spite of its success, most existing methods only consider contextual information, which is suboptimal when used in various tasks due to a lack of task-specific features. The rational word embeddings should have the ability to capture both the semantic features and task-specific features of words. In this paper, we propose a task-oriented word embedding method and apply it to the text classification task. With the function-aware component, our method regularizes the distribution of words to enable the embedding space to have a clear classification boundary. We evaluate our method using five text classification datasets. The experiment results show that our method significantly outperforms the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50384137",
                    "name": "Qian Liu"
                },
                {
                    "authorId": "4590286",
                    "name": "Heyan Huang"
                },
                {
                    "authorId": "145644809",
                    "name": "Yang Gao"
                },
                {
                    "authorId": "7621447",
                    "name": "Xiaochi Wei"
                },
                {
                    "authorId": "2152949867",
                    "name": "Yuxin Tian"
                },
                {
                    "authorId": "2118467297",
                    "name": "Luyang Liu"
                }
            ]
        },
        {
            "paperId": "8f7ff8e0106ba38986af25304e2f2527ef894487",
            "title": "Distant Supervision for Relation Extraction with Linear Attenuation Simulation and Non-IID Relevance Embedding",
            "abstract": "Distant supervision for relation extraction is an efficient method to reduce labor costs and has been widely used to seek novel relational facts in large corpora, which can be identified as a multi-instance multi-label problem. However, existing distant supervision methods suffer from selecting important words in the sentence and extracting valid sentences in the bag. Towards this end, we propose a novel approach to address these problems in this paper. Firstly, we propose a linear attenuation simulation to reflect the importance of words in the sentence with respect to the distances between entities and words. Secondly, we propose a non-independent and identically distributed (non-IID) relevance embedding to capture the relevance of sentences in the bag. Our method can not only capture complex information of words about hidden relations, but also express the mutual information of instances in the bag. Extensive experiments on a benchmark dataset have well-validated the effectiveness of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117729931",
                    "name": "Changsen Yuan"
                },
                {
                    "authorId": "4590286",
                    "name": "Heyan Huang"
                },
                {
                    "authorId": "144579978",
                    "name": "Chong Feng"
                },
                {
                    "authorId": "49544272",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "7621447",
                    "name": "Xiaochi Wei"
                }
            ]
        },
        {
            "paperId": "c8565e84f97173144a396a63bcdfaa76c583f251",
            "title": "From Question to Text",
            "abstract": "Understanding unstructured texts is an essential skill for human beings as it enables knowledge acquisition. Although understanding unstructured texts is easy for we human beings with good education, it is a great challenge for machines. Recently, with the rapid development of artificial intelligence techniques, researchers put efforts to teach machines to understand texts and justify the educated machines by letting them solve the questions upon the given unstructured texts, inspired by the reading comprehension test as we humans do. However, feature effectiveness with respect to different questions significantly hinders the performance of answer selection, because different questions may focus on various aspects of the given text and answer candidates. To solve this problem, we propose a question-oriented feature attention (QFA) mechanism, which learns to weight different engineering features according to the given question, so that important features with respect to the specific question is emphasized accordingly. Experiments on MCTest dataset have well-validated the effectiveness of the proposed method. Additionally, the proposed QFA is applicable to various IR tasks, such as question answering and answer selection. We have verified the applicability on a crawled community-based question-answering dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4590286",
                    "name": "Heyan Huang"
                },
                {
                    "authorId": "7621447",
                    "name": "Xiaochi Wei"
                },
                {
                    "authorId": "143982887",
                    "name": "Liqiang Nie"
                },
                {
                    "authorId": "134880677",
                    "name": "Xian-Ling Mao"
                },
                {
                    "authorId": "40620796",
                    "name": "Xin-Shun Xu"
                }
            ]
        }
    ]
}