{
    "authorId": "144178604",
    "papers": [
        {
            "paperId": "967b9a9d2bcf64f081f6cdb8f13087de5d97ddc6",
            "title": "Fusion Transformer with Object Mask Guidance for Image Forgery Analysis",
            "abstract": "In this work, we introduce OMG-Fuser, a fusion transformer-based network designed to extract information from various forensic signals to enable robust image forgery detection and localization. Our approach can operate with an arbitrary number of forensic signals and leverages object information for their analysis \u2013 unlike previous methods that rely on fusion schemes with few signals and often disregard image semantics. To this end, we design a forensic signal stream composed of a transformer guided by an object attention mechanism, associating patches that depict the same objects. In that way, we incorporate object-level information from the image. Each forensic signal is processed by a different stream that adapts to its peculiarities. A token fusion transformer efficiently aggregates the outputs of an arbitrary number of network streams and generates a fused representation for each image patch. We assess two fusion variants on top of the proposed approach: (i) score-level fusion that fuses the outputs of multiple image forensics algorithms and (ii) feature-level fusion that fuses low-level forensic traces directly. Both variants exceed state-of-the-art performance on seven datasets for image forgery detection and localization, with a relative average improvement of 12.1% and 20.4% in terms of F1. Our model is robust against traditional and novel forgery attacks and can be expanded with new signals without training from scratch. Our code is publicly available at: https://github.com/mever-team/omgfuser",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292194332",
                    "name": "Dimitrios Karageorgiou"
                },
                {
                    "authorId": "1403953272",
                    "name": "Giorgos Kordopatis-Zilos"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                }
            ]
        },
        {
            "paperId": "00670e1e999611ad02dc6268a323830b4a2dfbd7",
            "title": "Improving Synthetically Generated Image Detection in Cross-Concept Settings",
            "abstract": "New advancements for the detection of synthetic images are critical for fighting disinformation, as the capabilities of generative AI models continuously evolve and can lead to hyper-realistic synthetic imagery at unprecedented scale and speed. In this paper, we focus on the challenge of generalizing across different concept classes, e.g., when training a detector on human faces and testing on synthetic animal images \u2013 highlighting the ineffectiveness of existing approaches that randomly sample generated images to train their models. By contrast, we propose an approach based on the premise that the robustness of the detector can be enhanced by training it on realistic synthetic images that are selected based on their quality scores according to a probabilistic quality estimation model. We demonstrate the effectiveness of the proposed approach by conducting experiments with generated images from two seminal architectures, StyleGAN2 and Latent Diffusion, and using three different concepts for each, so as to measure the cross-concept generalization ability. Our results show that our quality-based sampling method leads to higher detection performance for nearly all concepts, improving the overall effectiveness of the synthetic image detectors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "97760096",
                    "name": "P. Dogoulis"
                },
                {
                    "authorId": "1403953272",
                    "name": "Giorgos Kordopatis-Zilos"
                },
                {
                    "authorId": "119661806",
                    "name": "I. Kompatsiaris"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                }
            ]
        },
        {
            "paperId": "06501511a28fc18d11fe39aa957727d8d9c6127b",
            "title": "The 2023 Video Similarity Dataset and Challenge",
            "abstract": "This work introduces a dataset, benchmark, and challenge for the problem of video copy detection and localization. The problem comprises two distinct but related tasks: determining whether a query video shares content with a reference video (\"detection\"), and additionally temporally localizing the shared content within each video (\"localization\"). The benchmark is designed to evaluate methods on these two tasks, and simulates a realistic needle-in-haystack setting, where the majority of both query and reference videos are\"distractors\"containing no copied content. We propose a metric that reflects both detection and localization accuracy. The associated challenge consists of two corresponding tracks, each with restrictions that reflect real-world settings. We provide implementation code for evaluation and baselines. We also analyze the results and methods of the top submissions to the challenge. The dataset, baseline methods and evaluation code is publicly available and will be discussed at a dedicated CVPR'23 workshop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1900487455",
                    "name": "Ed Pizzi"
                },
                {
                    "authorId": "1403953272",
                    "name": "Giorgos Kordopatis-Zilos"
                },
                {
                    "authorId": "2220218778",
                    "name": "Hiral Patel"
                },
                {
                    "authorId": "2290249",
                    "name": "Gheorghe Postelnicu"
                },
                {
                    "authorId": "2155482435",
                    "name": "Sugosh Nagavara Ravindra"
                },
                {
                    "authorId": "50178628",
                    "name": "A. Gupta"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                },
                {
                    "authorId": "1706195",
                    "name": "Giorgos Tolias"
                },
                {
                    "authorId": "3271933",
                    "name": "Matthijs Douze"
                }
            ]
        },
        {
            "paperId": "1fc58c4cd8616606c653401d2ffeb6d522b5fb0d",
            "title": "Towards an international standard to establish trust in media production, distribution and consumption",
            "abstract": "Advances in media content manipulation and artificially generated content pose new challenges to the assessment of media authenticity. While automated detection methods can provide meaningful insights and decision support in some scenarios, they cannot provide trustworthy and comprehensive information about the origin and provenance of media assets. Therefore, a longer-term approach should rather focus on secure and interoperable annotations related to the creation and provenance of media. In October 2020, the JPEG Committee initiated a standardization exploration named \"JPEG Fake Media\" to address these needs. Subsequently, since many of the requirements, for example related to secure annotation and identification of media assets, are also relevant to achieve interoperability in Non-Fungible Tokens (NFTs) an additional exploration was initiated, specifically focused on standardization needs for NFTs. In April 2022 a first Call for Proposals on JPEG Fake Media was issued. Based on the responses to the call, a new standardization project named JPEG Trust was initiated to specify an interoperable framework for establishing trust in media production, distribution, and consumption. This paper presents the journey of JPEG to leverage formal methods of standardization in this context, starting from the initial JPEG Fake Media exploration, followed by the subsequent consideration of NFT use cases and requirements, through to the commencement of the new JPEG Trust international standard.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2673221",
                    "name": "F. Temmermans"
                },
                {
                    "authorId": "2221122042",
                    "name": "Sabrina B. Caldwell"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                },
                {
                    "authorId": "2067616770",
                    "name": "Fernando Pereira"
                },
                {
                    "authorId": "2221117228",
                    "name": "Philippe Rixhon"
                }
            ]
        },
        {
            "paperId": "25968884ace61f9213ad60893513585ed7b57fbf",
            "title": "Mitigating Viewer Impact from Disturbing Imagery using AI Filters: A User-Study",
            "abstract": "Exposure to disturbing imagery can significantly impact individuals, especially professionals who encounter such content as part of their work. This paper presents a user study, involving 107 participants, predominantly journalists and human rights investigators, that explores the capability of Artificial Intelligence (AI)-based image filters to potentially mitigate the emotional impact of viewing such disturbing content. We tested five different filter styles, both traditional (Blurring and Partial Blurring) and AI-based (Drawing, Colored Drawing, and Painting), and measured their effectiveness in terms of conveying image information while reducing emotional distress. Our findings suggest that the AI-based Drawing style filter demonstrates the best performance, offering a promising solution for reducing negative feelings (-30.38%) while preserving the interpretability of the image (97.19%). Despite the requirement for many professionals to eventually inspect the original images, participants suggested potential strategies for integrating AI filters into their workflow, such as using AI filters as an initial, preparatory step before viewing the original image. Overall, this paper contributes to the development of a more ethically considerate and effective visual environment for professionals routinely engaging with potentially disturbing imagery.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388052763",
                    "name": "Ioannis Sarridis"
                },
                {
                    "authorId": "31325972",
                    "name": "Jochen Spangenberg"
                },
                {
                    "authorId": "144447962",
                    "name": "Olga Papadopoulou"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                }
            ]
        },
        {
            "paperId": "381cb527076a89d652e44f7be93be2d7b2bf2443",
            "title": "FLAC: Fairness-Aware Representation Learning by Suppressing Attribute-Class Associations",
            "abstract": "Bias in computer vision systems can perpetuate or even amplify discrimination against certain populations. Considering that bias is often introduced by biased visual datasets, many recent research efforts focus on training fair models using such data. However, most of them heavily rely on the availability of protected attribute labels in the dataset, which limits their applicability, while label-unaware approaches, i.e., approaches operating without such labels, exhibit considerably lower performance. To overcome these limitations, this work introduces FLAC, a methodology that minimizes mutual information between the features extracted by the model and a protected attribute, without the use of attribute labels. To do that, FLAC proposes a sampling strategy that highlights underrepresented samples in the dataset, and casts the problem of learning fair representations as a probability matching problem that leverages representations extracted by a bias-capturing classifier. It is theoretically shown that FLAC can indeed lead to fair representations, that are independent of the protected attributes. FLAC surpasses the current state-of-the-art on Biased MNIST, CelebA, and UTKFace, by 29.1%, 18.1%, and 21.9%, respectively. Additionally, FLAC exhibits 2.2% increased accuracy on ImageNet-A consisting of the most challenging samples of ImageNet. Finally, in most experiments, FLAC even outperforms the bias label-aware state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388052763",
                    "name": "Ioannis Sarridis"
                },
                {
                    "authorId": "2036760",
                    "name": "C. Koutlis"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                },
                {
                    "authorId": "2165480599",
                    "name": "Christos Diou"
                }
            ]
        },
        {
            "paperId": "3f9ace289fd35b9d94a6e53ed121da3b90b328ef",
            "title": "Towards Fair Face Verification: An In-depth Analysis of Demographic Biases",
            "abstract": "Deep learning-based person identification and verification systems have remarkably improved in terms of accuracy in recent years; however, such systems, including widely popular cloud-based solutions, have been found to exhibit significant biases related to race, age, and gender, a problem that requires in-depth exploration and solutions. This paper presents an in-depth analysis, with a particular emphasis on the intersectionality of these demographic factors. Intersectional bias refers to the performance discrepancies w.r.t. the different combinations of race, age, and gender groups, an area relatively unexplored in current literature. Furthermore, the reliance of most state-of-the-art approaches on accuracy as the principal evaluation metric often masks significant demographic disparities in performance. To counter this crucial limitation, we incorporate five additional metrics in our quantitative analysis, including disparate impact and mistreatment metrics, which are typically ignored by the relevant fairness-aware approaches. Results on the Racial Faces in-the-Wild (RFW) benchmark indicate pervasive biases in face recognition systems, extending beyond race, with different demographic factors yielding significantly disparate outcomes. In particular, Africans demonstrate an 11.25% lower True Positive Rate (TPR) compared to Caucasians, while only a 3.51% accuracy drop is observed. Even more concerning, the intersections of multiple protected groups, such as African females over 60 years old, demonstrate a +39.89% disparate mistreatment rate compared to the highest Caucasians rate. By shedding light on these biases and their implications, this paper aims to stimulate further research towards developing fairer, more equitable face recognition and verification systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388052763",
                    "name": "Ioannis Sarridis"
                },
                {
                    "authorId": "2036760",
                    "name": "C. Koutlis"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                },
                {
                    "authorId": "2165480599",
                    "name": "Christos Diou"
                }
            ]
        },
        {
            "paperId": "66818a9d410216214897ef3e0bdcb732f3aaad96",
            "title": "Self-Supervised Video Similarity Learning",
            "abstract": "We introduce S2VS, a video similarity learning approach with self-supervision. Self-Supervised Learning (SSL) is typically used to train deep models on a proxy task so as to have strong transferability on target tasks after fine-tuning. Here, in contrast to prior work, SSL is used to perform video similarity learning and address multiple retrieval and detection tasks at once with no use of labeled data. This is achieved by learning via instance-discrimination with task-tailored augmentations and the widely used InfoNCE loss together with an additional loss operating jointly on self-similarity and hard-negative similarity. We benchmark our method on tasks where video relevance is defined with varying granularity, ranging from video copies to videos depicting the same incident or event. We learn a single universal model that achieves state-of-the-art performance on all tasks, surpassing previously proposed methods that use labeled data. The code and pretrained models are publicly available at: https://github.com/gkordo/s2vs",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403953272",
                    "name": "Giorgos Kordopatis-Zilos"
                },
                {
                    "authorId": "1706195",
                    "name": "Giorgos Tolias"
                },
                {
                    "authorId": "1694090",
                    "name": "Christos Tzelepis"
                },
                {
                    "authorId": "119661806",
                    "name": "I. Kompatsiaris"
                },
                {
                    "authorId": "50058816",
                    "name": "I. Patras"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                }
            ]
        },
        {
            "paperId": "7c05cd2c7077cf0c51eb28cfef96fb6aee4b8a8d",
            "title": "MemeFier: Dual-stage Modality Fusion for Image Meme Classification",
            "abstract": "Hate speech is a societal problem that has significantly grown through the Internet. New forms of digital content such as image memes have given rise to spread of hate using multimodal means, being far more difficult to analyse and detect compared to the unimodal case. Accurate automatic processing, analysis and understanding of this kind of content will facilitate the endeavor of hindering hate speech proliferation through the digital world. To this end, we propose MemeFier, a deep learning-based architecture for fine-grained classification of Internet image memes, utilizing a dual-stage modality fusion module. The first fusion stage produces feature vectors containing modality alignment information that captures non-trivial connections between the text and image of a meme. The second fusion stage leverages the power of a Transformer encoder to learn inter-modality correlations at the token level and yield an informative representation. Additionally, we consider external knowledge as an additional input, and background image caption supervision as a regularizing component. Extensive experiments on three widely adopted benchmarks, i.e., Facebook Hateful Memes, Memotion7k and MultiOFF, indicate that our approach competes and in some cases surpasses state-of-the-art. Our code is available on GitHub1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2036760",
                    "name": "C. Koutlis"
                },
                {
                    "authorId": "151135851",
                    "name": "Manos Schinas"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                }
            ]
        },
        {
            "paperId": "8f5ee0d55b3744459c347ee6bed0dcb506431b90",
            "title": "MAAM: Media Asset Annotation and Management",
            "abstract": "Artificial intelligence can facilitate the management of large amounts of media content and enable media organisations to extract valuable insights from their data. Although AI for media understanding has made rapid progress over the recent years, its deployment in applications and professional sectors poses challenges, especially to organizations with no AI expertise. This motivated the creation of the Media Asset Annotation and Management platform (MAAM) that employs state-of-the-art deep learning models to annotate and facilitate the management of image and video assets. Annotation models provided by MAAM include automatic captioning, object detection, action recognition and moderation models, such as NSFW and disturbing content classifiers. By annotating media assets with these models, MAAM can support easy navigation, filtering and retrieval of media assets. In addition, our platform leverages the power of deep learning to support advanced visual and multi-modal retrieval capabilities. That allows accurately identifying assets that convey a similar idea, or concept even if they are not visually identical, and support a state-of-the-art reverse search facility for images and videos.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151135851",
                    "name": "Manos Schinas"
                },
                {
                    "authorId": "2093237167",
                    "name": "Panagiotis Galopoulos"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                }
            ]
        }
    ]
}