{
    "authorId": "3188181",
    "papers": [
        {
            "paperId": "013157c0f88471694de0e8cb1804328a83a38273",
            "title": "Adversarial Behavior Exclusion for Safe Reinforcement Learning",
            "abstract": "Learning by exploration makes reinforcement learning (RL) potentially attractive for many real-world applications. However, this learning process makes RL inherently too vulnerable to be used in real-world applications where safety is of utmost importance. Most prior studies consider exploration at odds with safety and thereby restrict it using either joint optimization of task and safety or imposing constraints for safe exploration. This paper migrates from the current convention to using exploration as a key to safety by learning safety as a robust behavior that completely excludes any behavioral pattern responsible for safety violations. Adversarial Behavior Exclusion for Safe RL (AdvEx-RL) learns a behavioral representation of the agent's safety violations by approximating an optimal adversary utilizing exploration and later uses this representation to learn a separate safety policy that excludes those unsafe behaviors. In addition, AdvEx-RL ensures safety in a task-agnostic manner by acting as a safety firewall and therefore can be integrated with any RL task policy. We demonstrate the robustness of AdvEx-RL via comprehensive experiments in standard constrained Markov decision processes (CMDP) environments under 2 white-box action space perturbations as well as with changes in environment dynamics against 7 baselines. Consistently, AdvEx-RL outperforms the baselines by achieving an average safety performance of over 75% in the continuous action space with 10 times more variations in the testing environment dynamics. By using a standalone safety policy independent of conflicting objectives, AdvEx-RL also paves the way for interpretable safety behavior analysis as we show in our user study.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10707785",
                    "name": "Md Asifur Rahman"
                },
                {
                    "authorId": "2111150708",
                    "name": "Tongtong Liu"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                }
            ]
        },
        {
            "paperId": "5aff89e78591446298f82d88ff07be1fb7f0b7ff",
            "title": "Task-Agnostic Safety for Reinforcement Learning",
            "abstract": "Reinforcement learning (RL) has been an attractive potential for designing autonomous systems due to its learning-by-exploration approach. However, this learning process makes RL inherently vulnerable and thus unsuitable for applications where safety is a top priority. To address this issue, researchers have either jointly optimized task and safety or imposed constraints to restrict exploration. This paper takes a different approach, by utilizing exploration as an adaptive means to learn a robust and safe behavior. To this end, we propose Task-Agnostic Safety for Reinforcement Learning (TAS-RL) framework to ensure safety in RL by learning a representation of unsafe behaviors and excluding them from the agent's policy. TAS-RL is task-agnostic and can be integrated with any RL task policy in the same environment, providing a self-protection layer for the system. To evaluate the robustness of TAS-RL, we present a novel study where TAS-RL and 7 safe RL baselines are tested in constrained Markov decision processes (CMDP) environments under white-box action space perturbations and changes in the environment dynamics. The results show that TAS-RL outperforms all baselines by achieving consistent near-zero safety constraint violations in continuous action space with 10 times more variations in the testing environment dynamics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10707785",
                    "name": "Md Asifur Rahman"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                }
            ]
        },
        {
            "paperId": "6a5ab4a74a9f567eda276a6e5a276aa89e61e690",
            "title": "Safe Reinforcement Learning via Observation Shielding",
            "abstract": "Reinforcement Learning (RL) algorithms have shown success in scaling up to large problems. However, deploying those algorithms in real-world applications remains challenging due to their vulnerability to adversarial perturbations. Existing RL robustness methods against adversarial attacks are weak to large perturbations - a scenario that cannot be ruled out for RL adversarial threats, as is the case for deep neural networks in classification tasks. This paper proposes a method called observation-shielding RL (OSRL) to increase the robustness of RL against large perturbations using predictive models and threat detection. Instead of changing the RL algorithms with robustness regularization or retrain them with adversarial perturbations, we depart considerably from previous approaches and develop an add-on safety feature for existing RL algorithms during runtime. OSRL builds on the idea of model predictive shielding, where an observation predictive model is used to override the perturbed observations as needed to ensure safety. Extensive experiments on various MuJoco 1 environments (Ant, Hoope, InvertedPendulum, Reacher) environment demonstrate that our proposed OSRL is safer and more efficient than state-of-the-art robustness methods under large perturbations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2042750482",
                    "name": "Joe McCalmon"
                },
                {
                    "authorId": "2111150708",
                    "name": "Tongtong Liu"
                },
                {
                    "authorId": "2207250593",
                    "name": "Reid Goldsmith"
                },
                {
                    "authorId": "2207266743",
                    "name": "Andrew Cyhaniuk"
                },
                {
                    "authorId": "9628020",
                    "name": "Talal Halabi"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                }
            ]
        },
        {
            "paperId": "80a7a15a61983134481d9cc6a20c8645cc7e860c",
            "title": "Automated Extraction of Security Profile Information from XAI Outcomes",
            "abstract": "Security applications use machine learning (ML) models and artificial intelligence (AI) to autonomously protect systems. However, security decisions are more impactful if they are coupled with their rationale. The explanation behind an ML model's result provides the rationale necessary for a security decision. Explainable AI (XAI) techniques provide insights into the state of a model's attributes and their contribution to the model's results to gain the end user's confidence. It requires human intervention to investigate and interpret the explanation. The interpretation must align system's security profile(s). A security profile is an abstraction of the system's security requirements and related functionalities to comply with them. Relying on human intervention for interpretation is infeasible for an autonomous system (AS) since it must self-adapt its functionalities in response to uncertainty at runtime. Thus, an AS requires an automated approach to extract security profile information from ML model XAI outcomes. The challenge is unifying the XAI outcomes with the security profile to represent the interpretation in a structured form. This paper presents a component to facilitate AS information extraction from ML model XAI outcomes related to predictions and generating an interpretation considering the security profile.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2238213361",
                    "name": "Sharmin Jahan"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                },
                {
                    "authorId": "2273975499",
                    "name": "Rose F. Gamble"
                },
                {
                    "authorId": "2273977431",
                    "name": "Masrufa Bayesh"
                }
            ]
        },
        {
            "paperId": "3892a9bee7c9f0314a513c0933a588e1e069722b",
            "title": "CAPS: Comprehensible Abstract Policy Summaries for Explaining Reinforcement Learning Agents",
            "abstract": "As reinforcement learning (RL) continues to improve and be applied in situations alongside humans, the need to explain the learned behaviors of RL agents to end-users becomes more important. Strategies for explaining the reasoning behind an agent\u2019s policy, called policy-level explanations , can lead to important insights about both the task and the agent\u2019s behaviors. Following this line of research, in this work, we propose a novel approach, named as CAPS , that summarizes an agent\u2019s policy in the form of a directed graph with natural language descriptions. A decision tree based clustering method is utilized to abstract the state space of the task into fewer, condensed states which makes the policy graphs more digestible to end-users. This abstraction allows the users to control the size of the policy graph to achieve their desired balance between comprehensi-bility and accuracy. In addition, we develop a heuristic optimization method to find the most explainable graph policy and present it to the users. Finally, we use the user-defined predicates to enrich the abstract states with semantic meaning. We test our approach on 5 RL tasks, using both deterministic and stochastic policies, and show that our method is: (1) agnostic to the algorithms used to train the policies, and (2) comparable in accuracy and superior in explanation capabilities to existing baselines. Especially, when provided with our explanation graph, end-users are able to accurately interpret policies of trained RL agents 80% of the time, compared to 10% when provided with the next best baseline. We make our code and datasets available to ensure the reproducibility of our research",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2042750482",
                    "name": "Joe McCalmon"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                },
                {
                    "authorId": "2150817685",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "784f9d103d6a80c3e2f81037add06f326cc8557a",
            "title": "A Scary Peek into The Future: Advanced Persistent Threats in Emerging Computing Environments",
            "abstract": "The last decade witnessed a gradual shift from cloud-based computing towards ubiquitous computing, which has put at a greater security risk every element of the computing ecosystem including devices, data, network, and decision making. Indeed, emerging pervasive computing paradigms have introduced an un-charted territory of security vulnerabilities and a wider attack surface, mainly due to network openness, the underlying mechanics that enable intelligent functions, and the deeply integrated physical and cyber spaces. Furthermore, interconnected computing environments now enjoy many unconventional characteristics that mandate a radical change in security engineering tools. This need is further exacerbated by the rapid emergence of new Advanced Persistent Threats (APTs) that target critical infrastructures and aim to stealthily undermine their operations in innovative and intelligent ways. To enable system and network designers to be prepared to face this new wave of dangerous threats, this paper overviews recent APTs in emerging computing systems and proposes a new approach to APTs that is more tailored towards such systems compared to traditional IT infrastructures. The proposed APT lifecycle will inform security decisions and implementation choices in future pervasive networked systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9628020",
                    "name": "Talal Halabi"
                },
                {
                    "authorId": "2179375240",
                    "name": "Aawista Chaudhry"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                },
                {
                    "authorId": "3288497",
                    "name": "Mohammad Zulkernine"
                }
            ]
        },
        {
            "paperId": "ada1f098bd0d45456d9f3e8911a4a2f6bc1984bc",
            "title": "Change Detection of Amazonian Alluvial Gold Mining Using Deep Learning and Sentinel-2 Imagery",
            "abstract": "Monitoring changes within the land surface and open water bodies is critical for natural resource management, conservation, and environmental policy. While the use of satellite imagery for these purposes is common, fine-scale change detection can be a technical challenge. Difficulties arise from variable atmospheric conditions and the problem of assigning pixels to individual objects. We examined the degree to which two machine learning approaches can better characterize change detection in the context of a current conservation challenge, artisanal small-scale gold mining (ASGM). We obtained Sentinel-2 imagery and consulted with domain experts to construct an open-source labeled land-cover change dataset. The focus of this dataset is the Madre de Dios (MDD) region in Peru, a hotspot of ASGM activity. We also generated datasets of active ASGM areas in other countries (Venezuela, Indonesia, and Myanmar) for out-of-sample testing. With these labeled data, we utilized a supervised (E-ReCNN) and semi-supervised (SVM-STV) approach to study binary and multi-class change within mining ponds in the MDD region. Additionally, we tested how the inclusion of multiple channels, histogram matching, and La*b* color metrics improved the performance of the models and reduced the influence of atmospheric effects. Empirical results show that the supervised E-ReCNN method on 6-Channel histogram-matched images generated the most accurate detection of change not only in the focal region (Kappa: 0.92 (\u00b1 0.04), Jaccard: 0.88 (\u00b1 0.07), F1: 0.88 (\u00b1 0.05)) but also in the out-of-sample prediction regions (Kappa: 0.90 (\u00b1 0.03), Jaccard: 0.84 (\u00b1 0.04), and F1: 0.77 (\u00b1 0.04)). While semi-supervised methods did not perform as accurately on 6- or 10-channel imagery, histogram matching and the inclusion of La*b* metrics generated accurate results with low memory and resource costs. These results show that E-ReCNN is capable of accurately detecting specific and object-oriented environmental changes related to ASGM. E-ReCNN is scalable to areas outside the focal area and is a method of change detection that can be extended to other forms of land-use modification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "28297456",
                    "name": "Seda Camalan"
                },
                {
                    "authorId": "2120345366",
                    "name": "Kangning Cui"
                },
                {
                    "authorId": "2873829",
                    "name": "V. P. Pauca"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                },
                {
                    "authorId": "3310049",
                    "name": "M. Silman"
                },
                {
                    "authorId": "2161597584",
                    "name": "Raymond Chan"
                },
                {
                    "authorId": "2161600086",
                    "name": "R. Plemmons"
                },
                {
                    "authorId": "108306170",
                    "name": "E. Dethier"
                },
                {
                    "authorId": "1571113845",
                    "name": "Luis E. Fernandez"
                },
                {
                    "authorId": "145051499",
                    "name": "D. Lutz"
                }
            ]
        },
        {
            "paperId": "e196e0c89cc6a2824750160a0264b7a5c6866574",
            "title": "LSTM-Based Anomalous Behavior Detection in Multi-Agent Reinforcement Learning",
            "abstract": "Multi-Agent Reinforcement Learning (MARL) extends individual reinforcement learning to enable a team of agents to collaboratively determine the global optimal policy that maximizes the sum of their local accumulated rewards. It has been recently deployed in multiple application domains such as edge computing, wireless networks, and Cyber-Physical Systems. Nonetheless, the security of MARL and its potential exposure to cyberattacks have not yet been fully investigated. This paper examines one of the most serious vulnerabilities in MARL algorithms: the compromised agent. This newly-engineered adversarial vulnerability is exploited when a malicious user compromises an agent to directly control its actions, and subsequently pushes its cooperative agents to act off-policy. We present a novel stacked-LSTM ensemble approach to detect such an attack. The results show that our anomalous behavior detection system significantly outperforms five baselines from the literature.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2181836823",
                    "name": "Cameron Lischke"
                },
                {
                    "authorId": "2111150708",
                    "name": "Tongtong Liu"
                },
                {
                    "authorId": "2042750482",
                    "name": "Joe McCalmon"
                },
                {
                    "authorId": "10707785",
                    "name": "Md Asifur Rahman"
                },
                {
                    "authorId": "9628020",
                    "name": "Talal Halabi"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                }
            ]
        },
        {
            "paperId": "fe35e3eaf4ea90412ef02d74c8f48c79fdbaef46",
            "title": "Semi-Supervised Change Detection Of Small Water Bodies Using Rgb And Multispectral Images In Peruvian Rainforests",
            "abstract": "Artisanal and Small-scale Gold Mining (ASGM) is an important source of income for many households, but it can have large social and environmental effects, especially in rainforests of developing countries. The Sentinel-2 satellites collect multispectral images that can be used for the purpose of detecting changes in water extent and quality which indicates the locations of mining sites. This work focuses on the recognition of ASGM activities in Peruvian Amazon rainforests. We tested several semi-supervised classifiers based on Support Vector Machines (SVMs) to detect the changes of water bodies from 2019 to 2021 in the Madre de Dios region, which is one of the global hotspots of ASGM activities. Experiments show that SVM-based models can achieve reasonable performance for both RGB (using Cohen\u2019s \u03ba 0.49) and 6-channel images (using Cohen\u2019s \u03ba 0.71) with very limited annotations. The efficacy of incorporating Lab color space for change detection is analyzed as well.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2120345366",
                    "name": "Kangning Cui"
                },
                {
                    "authorId": "28297456",
                    "name": "Seda Camalan"
                },
                {
                    "authorId": "98532587",
                    "name": "R. Li"
                },
                {
                    "authorId": "32791879",
                    "name": "V. Pauca"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                },
                {
                    "authorId": "2262995",
                    "name": "R. Plemmons"
                },
                {
                    "authorId": "3310049",
                    "name": "M. Silman"
                },
                {
                    "authorId": "108306170",
                    "name": "E. Dethier"
                },
                {
                    "authorId": "145051499",
                    "name": "D. Lutz"
                },
                {
                    "authorId": "2161597584",
                    "name": "Raymond Chan"
                }
            ]
        },
        {
            "paperId": "5d5bfcd4066d0b644271bef7e78ec887753b703e",
            "title": "A Symbolic-AI Approach for UAV Exploration Tasks",
            "abstract": "Performing autonomous exploration and exploitation is essential for un- manned aerial vehicles (UAVs) operating in unknown environments. Often, such missions involve first building a map of the environment via pure exploration and subsequently exploiting it for specific downstream tasks. But, conducting separate exploration and exploitation steps is not always feasible in practice. In this paper, we develop a novel exploration approach enabling exploration and exploitation in a single step for an area-of-interest (AoI) search task. The basic idea is to employ a probabilistic information gain map, called a belief map, as a prior to guide the exploration trajectory, while efficiently reducing false positive information in the process. The approach is composed of three layers. The first is an information potential layer to decide the exploration direction for the UAV. Next, the proximity layer exploits detected AoI by exploring their proximal areas. The last layer, a forced movement layer, is responsible for enabling the UAV to escape local maxima caused by the previous layers. We tested the performance of our approach in two different tasks relative to two exploration methods published in the literature. The results demonstrate that our proposed approach is capable of navigating through randomly generated environments and covering more AoI in fewer time steps compared to the baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108247709",
                    "name": "Yixin Zhang"
                },
                {
                    "authorId": "2042750482",
                    "name": "Joe McCalmon"
                },
                {
                    "authorId": "2042699781",
                    "name": "Ashley Peake"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                },
                {
                    "authorId": "2873829",
                    "name": "V. P. Pauca"
                }
            ]
        }
    ]
}