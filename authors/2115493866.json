{
    "authorId": "2115493866",
    "papers": [
        {
            "paperId": "1e706b7f994feaa20316cd28ac503b39090de75e",
            "title": "A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal Reasoning",
            "abstract": "This paper presents a pilot study aimed at introducing multi-agent debate into multimodal reasoning. The study addresses two key challenges: the trivialization of opinions resulting from excessive summarization and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are confined to a blueprint graph to prevent opinion trivialization through world-level summarization. Moreover, by storing evidence in branches within the graph, BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate that BDoG is able to achieve state-of-the-art results in ScienceQA and MMBench with significant improvements over previous methods. The source code can be accessed at https://github.com/thecharm/BDoG.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150068355",
                    "name": "Changmeng Zheng"
                },
                {
                    "authorId": "2293172458",
                    "name": "Dayong Liang"
                },
                {
                    "authorId": "2293233081",
                    "name": "Wengyu Zhang"
                },
                {
                    "authorId": "2115493866",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2314918948",
                    "name": "Tat-Seng Chua"
                },
                {
                    "authorId": "2293397899",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "482b65d96e8f353d1a06b34db7918954e954fc4b",
            "title": "Mean of Means: A 10-dollar Solution for Human Localization with Calibration-free and Unconstrained Camera Settings",
            "abstract": "Accurate human localization is crucial for various applications, especially in the Metaverse era. Existing high precision solutions rely on expensive, tag-dependent hardware, while vision-based methods offer a cheaper, tag-free alternative. However, current vision solutions based on stereo vision face limitations due to rigid perspective transformation principles and error propagation in multi-stage SVD solvers. These solutions also require multiple high-resolution cameras with strict setup constraints. To address these limitations, we propose a probabilistic approach that considers all points on the human body as observations generated by a distribution centered around the body's geometric center. This enables us to improve sampling significantly, increasing the number of samples for each point of interest from hundreds to billions. By modeling the relation between the means of the distributions of world coordinates and pixel coordinates, leveraging the Central Limit Theorem, we ensure normality and facilitate the learning process. Experimental results demonstrate human localization accuracy of 95% within a 0.3m range and nearly 100% accuracy within a 0.5m range, achieved at a low cost of only 10 USD using two web cameras with a resolution of 640x480 pixels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2274083481",
                    "name": "Tianyi Zhang"
                },
                {
                    "authorId": "2293233081",
                    "name": "Wengyu Zhang"
                },
                {
                    "authorId": "2108162240",
                    "name": "Xu-Lu Zhang"
                },
                {
                    "authorId": "2313746412",
                    "name": "Jiaxin Wu"
                },
                {
                    "authorId": "2115493866",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2115870728",
                    "name": "Jiannong Cao"
                },
                {
                    "authorId": "2293397899",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "57d288508838a433a122e0fccf99fc540c73fff2",
            "title": "Multi-Level Querying using A Knowledge Pyramid",
            "abstract": "This paper addresses the need for improved precision in existing Retrieval-Augmented Generation (RAG) methods that primarily focus on enhancing recall. We propose a multi-layer knowledge pyramid approach within the RAG framework to achieve a better balance between precision and recall. The knowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs), and chunk-based raw text. We employ cross-layer augmentation techniques for comprehensive knowledge coverage and dynamic updates of the Ontology schema and instances. To ensure compactness, we utilize cross-layer filtering methods for knowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall model for retrieval, starting from the top of the pyramid and progressing down until a confident answer is obtained. We introduce two benchmarks for domain-specific knowledge retrieval, one in the academic domain and the other in the financial domain. The effectiveness of the methods has been validated through comprehensive experiments by outperforming 19 SOTA methods. An encouraging observation is that the proposed method has augmented the GPT-4, providing 395\\% F1 gain by improving its performance from 0.1636 to 0.8109.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2314169880",
                    "name": "Rubing Chen"
                },
                {
                    "authorId": "2108162240",
                    "name": "Xu-Lu Zhang"
                },
                {
                    "authorId": "2313746412",
                    "name": "Jiaxin Wu"
                },
                {
                    "authorId": "2291324376",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2115493866",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2293397899",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "593d3c49cd654fa2b0879ba9ea9a71065e2b9559",
            "title": "A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning",
            "abstract": "This paper presents a pilot study aimed at introducing multi-agent debate into multimodal reasoning. The study addresses two key challenges: the trivialization of opinions resulting from excessive summarization and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are confined to a blueprint graph to prevent opinion trivi-alization through world-level summarization. Moreover, by storing evidence in branches within the graph, BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate BDoG, achieving state-of-the-art results in Science QA and MMBench with significant improvements over previous methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150068355",
                    "name": "Changmeng Zheng"
                },
                {
                    "authorId": "2293172458",
                    "name": "Dayong Liang"
                },
                {
                    "authorId": "2293233081",
                    "name": "Wengyu Zhang"
                },
                {
                    "authorId": "2115493866",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2270722858",
                    "name": "Tat-Seng Chua"
                },
                {
                    "authorId": "2293397899",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "bc1341ae6827d480e7f58258fc472a821de53865",
            "title": "Generative Active Learning for Image Synthesis Personalization",
            "abstract": "This paper presents a pilot study that explores the application of active learning, traditionally studied in the context of discriminative models, to generative models. We specifically focus on image synthesis personalization tasks. The primary challenge in conducting active learning on generative models lies in the open-ended nature of querying, which differs from the closed form of querying in discriminative models that typically target a single concept. We introduce the concept of anchor directions to transform the querying process into a semi-open problem. We propose a direction-based uncertainty sampling strategy to enable generative active learning and tackle the exploitation-exploration dilemma. Extensive experiments are conducted to validate the effectiveness of our approach, demonstrating that an open-source model can achieve superior performance compared to closed-source models developed by large companies, such as Google's StyleDrop. The source code is available at https://github.com/zhangxulu1996/GAL4Personalization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108162240",
                    "name": "Xu-Lu Zhang"
                },
                {
                    "authorId": "2293233081",
                    "name": "Wengyu Zhang"
                },
                {
                    "authorId": "2115493866",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2273949621",
                    "name": "Jinlin Wu"
                },
                {
                    "authorId": "2267782881",
                    "name": "Zhaoxiang Zhang"
                },
                {
                    "authorId": "2273930303",
                    "name": "Zhen Lei"
                },
                {
                    "authorId": "2293397899",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "be938281ea9f1e3ebee4a27da29ef20c14d060cf",
            "title": "Towards Bridged Vision and Language: Learning Cross-Modal Knowledge Representation for Relation Extraction",
            "abstract": "In natural language processing, relation extraction (RE) is to detect and classify the semantic relationship of two given entities within a sentence. Previous RE methods consider only the textual contents and suffer performance decline in social media when texts lack contexts. Incorporating text-related visual information can supplement the missing semantics for relation extraction in social media posts. However, textual relations are usually abstract and of high-level semantics, which causes the semantic gap between visual contents and textual expressions. In this paper, we propose RECK - a neural network for relation extraction with cross-modal knowledge representations. Different from previous multimodal methods training a common subspace for all modalities, we bridge the semantic gaps by explicitly selecting knowledge paths from external knowledge through the cross-modal object-entity pairs. We further extend the paths into a knowledge graph, and adopt a graph attention network to capture the multi-grained relevant concepts which can provide higher level and key semantics information from external knowledge. Besides, we employ a cross-modal attention mechanism to align and fuse the multimodal information. Experimental results on a multimodal RE dataset show that our model achieves new state-of-the-art performance with knowledge evidence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30416727",
                    "name": "Junhao Feng"
                },
                {
                    "authorId": "2110630349",
                    "name": "Guohua Wang"
                },
                {
                    "authorId": "150068355",
                    "name": "Changmeng Zheng"
                },
                {
                    "authorId": "1752876325",
                    "name": "Yi Cai"
                },
                {
                    "authorId": "2139307995",
                    "name": "Ze Fu"
                },
                {
                    "authorId": "2187446521",
                    "name": "Yaowei Wang"
                },
                {
                    "authorId": "2115493866",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "1930238",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "e9b5505500085040b87a6a6e364c039d7474f4b4",
            "title": "A Survey on Personalized Content Synthesis with Diffusion Models",
            "abstract": "Recent advancements in generative models have significantly impacted content creation, leading to the emergence of Personalized Content Synthesis (PCS). With a small set of user-provided examples, PCS aims to customize the subject of interest to specific user-defined prompts. Over the past two years, more than 150 methods have been proposed. However, existing surveys mainly focus on text-to-image generation, with few providing up-to-date summaries on PCS. This paper offers a comprehensive survey of PCS, with a particular focus on the diffusion models. Specifically, we introduce the generic frameworks of PCS research, which can be broadly classified into optimization-based and learning-based approaches. We further categorize and analyze these methodologies, discussing their strengths, limitations, and key techniques. Additionally, we delve into specialized tasks within the field, such as personalized object generation, face synthesis, and style personalization, highlighting their unique challenges and innovations. Despite encouraging progress, we also present an analysis of the challenges such as overfitting and the trade-off between subject fidelity and text alignment. Through this detailed overview and analysis, we propose future directions to advance the development of PCS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108162240",
                    "name": "Xu-Lu Zhang"
                },
                {
                    "authorId": "2115493866",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2293233081",
                    "name": "Wengyu Zhang"
                },
                {
                    "authorId": "2273949621",
                    "name": "Jinlin Wu"
                },
                {
                    "authorId": "2267782881",
                    "name": "Zhaoxiang Zhang"
                },
                {
                    "authorId": "2273930303",
                    "name": "Zhen Lei"
                },
                {
                    "authorId": "2293397899",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "073e4f0c3a66b7557abd053301b5104cdc582636",
            "title": "Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective",
            "abstract": "Molecule discovery plays a crucial role in various scientific fields, advancing the design of tailored materials and drugs. However, most of the existing methods heavily rely on domain experts, require excessive computational cost, or suffer from sub-optimal performance. On the other hand, Large Language Models (LLMs), like ChatGPT, have shown remarkable performance in various cross-modal tasks due to their powerful capabilities in natural language understanding, generalization, and in-context learning (ICL), which provides unprecedented opportunities to advance molecule discovery. Despite several previous works trying to apply LLMs in this task, the lack of domain-specific corpus and difficulties in training specialized LLMs still remain challenges. In this work, we propose a novel LLM-based framework (MolReGPT) for molecule-caption translation, where an In-Context Few-Shot Molecule Learning paradigm is introduced to empower molecule discovery with LLMs like ChatGPT to perform their in-context learning capability without domain-specific pre-training and fine-tuning. MolReGPT leverages the principle of molecular similarity to retrieve similar molecules and their text descriptions from a local database to enable LLMs to learn the task knowledge from context examples. We evaluate the effectiveness of MolReGPT on molecule-caption translation, including molecule understanding and text-based molecule generation. Experimental results show that compared to fine-tuned models, MolReGPT outperforms MolT5-base and is comparable to MolT5-large without additional training. To the best of our knowledge, MolReGPT is the first work to leverage LLMs via in-context learning in molecule-caption translation for advancing molecule discovery. Our work expands the scope of LLM applications, as well as providing a new paradigm for molecule discovery and design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109018826",
                    "name": "Jiatong Li"
                },
                {
                    "authorId": "2208630682",
                    "name": "Yunqing Liu"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2115493866",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "2117897052",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "5bc29fba16ad5fd7f575154b84f09970dae92491",
            "title": "Multi-agent Attacks for Black-box Social Recommendations",
            "abstract": "The rise of online social networks has facilitated the evolution of social recommender systems, which incorporate social relations to enhance users' decision-making process. With the great success of Graph Neural Networks (GNNs) in learning node representations, GNN-based social recommendations have been widely studied to model user-item interactions and user-user social relations simultaneously. Despite their great successes, recent studies have shown that these advanced recommender systems are highly vulnerable to adversarial attacks, in which attackers can inject well-designed fake user profiles to disrupt recommendation performances. While most existing studies mainly focus on argeted attacks to promote target items on vanilla recommender systems, untargeted attacks to degrade the overall prediction performance are less explored on social recommendations under a black-box scenario. To perform untargeted attacks on social recommender systems, attackers can construct malicious social relationships for fake users to enhance the attack performance. However, the coordination of social relations and item profiles is challenging for attacking black-box social recommendations. To address this limitation, we first conduct several preliminary studies to demonstrate the effectiveness of cross-community connections and cold-start items in degrading recommendations performance. Specifically, we propose a novel framework MultiAttack based on multi-agent reinforcement learning to coordinate the generation of cold-start item profiles and cross-community social relations for conducting untargeted attacks on black-box social recommendations. Comprehensive experiments on various real-world datasets demonstrate the effectiveness of our proposed attacking framework under the black-box setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2266567589",
                    "name": "Shijie Wang"
                },
                {
                    "authorId": "2115493866",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2221127240",
                    "name": "Xiaowei Mei"
                },
                {
                    "authorId": "2254366521",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "a951b2e73fadcd2e78453569619980e373659fce",
            "title": "Compositional Inversion for Stable Diffusion Models",
            "abstract": "Inversion methods, such as Textual Inversion, generate personalized images by incorporating concepts of interest provided by user images. However, existing methods often suffer from overfitting issues, where the dominant presence of inverted concepts leads to the absence of other desired concepts. It stems from the fact that during inversion, the irrelevant semantics in the user images are also encoded, forcing the inverted concepts to occupy locations far from the core distribution in the embedding space. To address this issue, we propose a method that guides the inversion process towards the core distribution for compositional embeddings. Additionally, we introduce a spatial regularization approach to balance the attention on the concepts being composed. Our method is designed as a post-training approach and can be seamlessly integrated with other inversion methods. Experimental results demonstrate the effectiveness of our proposed approach in mitigating the overfitting problem and generating more diverse and balanced compositions of concepts in the synthesized images. The source code is available at https://github.com/zhangxulu1996/Compositional-Inversion.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108162240",
                    "name": "Xu-Lu Zhang"
                },
                {
                    "authorId": "2115493866",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2273949621",
                    "name": "Jinlin Wu"
                },
                {
                    "authorId": "2274083481",
                    "name": "Tianyi Zhang"
                },
                {
                    "authorId": "2267782881",
                    "name": "Zhaoxiang Zhang"
                },
                {
                    "authorId": "2273930303",
                    "name": "Zhen Lei"
                },
                {
                    "authorId": "2281964322",
                    "name": "Qing Li"
                }
            ]
        }
    ]
}