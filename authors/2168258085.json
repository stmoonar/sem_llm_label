{
    "authorId": "2168258085",
    "papers": [
        {
            "paperId": "0c7ce5898dab92da540457b754254d72b8592fc2",
            "title": "Parameter-efficient Tuning of Large-scale Multimodal Foundation Model",
            "abstract": "Driven by the progress of large-scale pre-training, parameter-efficient transfer learning has gained immense popularity across different subfields of Artificial Intelligence. The core is to adapt the model to downstream tasks with only a small set of parameters. Recently, researchers have leveraged such proven techniques in multimodal tasks and achieve promising results. However, two critical issues remain unresolved: how to further reduce the complexity with lightweight design and how to boost alignment between modalities under extremely low parameters. In this paper, we propose A graceful prompt framework for cross-modal transfer (Aurora) to overcome these challenges. Considering the redundancy in existing architectures, we first utilize the mode approximation to generate 0.1M trainable parameters to implement the multimodal prompt tuning, which explores the low intrinsic dimension with only 0.04% parameters of the pre-trained model. Then, for better modality alignment, we propose the Informative Context Enhancement and Gated Query Transformation module under extremely few parameters scenes. A thorough evaluation on six cross-modal benchmarks shows that it not only outperforms the state-of-the-art but even outperforms the full fine-tuning approach. Our code is available at: https://github.com/WillDreamer/Aurora.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2150441370",
                    "name": "Xinlong Yang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "2168258085",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "29a7644583d7042c4476af126f3fe0a372897abe",
            "title": "LION: Implicit Vision Prompt Tuning",
            "abstract": "Despite recent promising performances across a range of vision tasks, vision Transformers still have an issue of high computational costs.\nRecently, vision prompt learning has provided an economical solution to this problem without fine-tuning the whole large-scale model. \nHowever, the efficiency and effectiveness of existing models are still far from satisfactory due to the parameter cost of extensive prompt blocks and tricky prompt framework designs. \nIn this paper, we propose a light-weight prompt framework named impLicit vIsion prOmpt tuNing (LION), which is motivated by deep implicit models with stable low memory costs for various complex tasks.\nIn particular, we merely insect two equilibrium implicit layers in two ends of the pre-trained backbone with parameters frozen. Moreover, according to the lottery hypothesis, we further prune the parameters to relieve the computation burden in implicit layers. Various experiments have validated that our LION obtains promising performances on a wide range of datasets. Most importantly, LION reduces up to 11.5 % of training parameter numbers while obtaining higher performance than the state-of-the-art VPT, especially under challenging scenes. Furthermore, we find that our proposed LION has an excellent generalization performance, making it an easy way to boost transfer learning in the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2168258085",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "33383055",
                    "name": "Zhouchen Lin"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "2a38daf98d506477f8180806f503409d5036eaf4",
            "title": "TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer",
            "abstract": "We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanisms, tensor normalization, and inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism for smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over $20\\%$. Furthermore, we develop a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages. We also implement an efficient model parallel schema for TransNormerLLM, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, i.e., LLMs with 175B parameters. We validate our model design through a series of ablations and train models with sizes of 385M, 1B, and 7B on our self-collected corpus. Benchmark results demonstrate that our models not only match the performance of state-of-the-art LLMs with Transformer but are also significantly faster. Code is released at: https://github.com/OpenNLPLab/TransnormerLLM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171650015",
                    "name": "Zhen Qin"
                },
                {
                    "authorId": "2179703418",
                    "name": "Dong Li"
                },
                {
                    "authorId": "2225238340",
                    "name": "Weigao Sun"
                },
                {
                    "authorId": "8397429",
                    "name": "Weixuan Sun"
                },
                {
                    "authorId": "2116517206",
                    "name": "Xuyang Shen"
                },
                {
                    "authorId": "2118234357",
                    "name": "Xiaodong Han"
                },
                {
                    "authorId": "2156252901",
                    "name": "Yunshen Wei"
                },
                {
                    "authorId": "2154762987",
                    "name": "Baohong Lv"
                },
                {
                    "authorId": "40247395",
                    "name": "Fei Yuan"
                },
                {
                    "authorId": "2168258085",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "145858545",
                    "name": "Y. Qiao"
                },
                {
                    "authorId": "2015152",
                    "name": "Yiran Zhong"
                }
            ]
        },
        {
            "paperId": "d73ccaa9cedff36dc7a930319506d04c2fce6719",
            "title": "PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video Prediction",
            "abstract": "In this paper, we investigate the challenge of spatio-temporal video prediction, which involves generating future videos based on historical data streams. Existing approaches typically utilize external information such as semantic maps to enhance video prediction, which often neglect the inherent physical knowledge embedded within videos. Furthermore, their high computational demands could impede their applications for high-resolution videos. To address these constraints, we introduce a novel approach called Physics-assisted Spatio-temporal Network (PastNet) for generating high-quality video predictions. The core of our PastNet lies in incorporating a spectral convolution operator in the Fourier domain, which efficiently introduces inductive biases from the underlying physical laws. Additionally, we employ a memory bank with the estimated intrinsic dimensionality to discretize local features during the processing of complex spatio-temporal signals, thereby reducing computational costs and facilitating efficient high-resolution video prediction. Extensive experiments on various widely-used datasets demonstrate the effectiveness and efficiency of the proposed PastNet compared with state-of-the-art methods, particularly in high-resolution scenarios. Our code is available at https://github.com/easylearningscores/PastNet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119798365",
                    "name": "Hao Wu"
                },
                {
                    "authorId": "71787357",
                    "name": "Wei Xion"
                },
                {
                    "authorId": "2217950059",
                    "name": "Fan Xu"
                },
                {
                    "authorId": "2168258085",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "48240541",
                    "name": "C. Chen"
                },
                {
                    "authorId": "2053903039",
                    "name": "Xiansheng Hua"
                },
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                }
            ]
        },
        {
            "paperId": "15b2d9f1398ff1c5dd5e0ef44724019b2fe37e2e",
            "title": "Attention-based Adversarial Partial Domain Adaptation",
            "abstract": "With the rapid development of vision-based deep learning (DL), it is an effective method to generate large-scale synthetic data to supplement real data to train the DL models for domain adaptation. However, previous vanilla domain adaptation methods generally assume the same label space, and such an assumption is no longer valid for a more realistic scenario where it requires adaptation from a larger and more diverse source domain to a smaller target domain with less number of classes. To handle this problem, we propose an attention-based adversarial partial domain adaptation (AADA). Specifically, we leverage adversarial domain adaptation to augment the target domain by using source domain, then we can readily turn this task into a vanilla domain adaptation. Meanwhile, to accurately focus on the transferable features, we apply attention-based method to train the adversarial networks to obtain better transferable semantic features. Experiments on four benchmarks demonstrate that the proposed method outperforms existing methods by a large margin, especially on the tough domain adaptation tasks, e.g. VisDA-2017.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145364607",
                    "name": "Mengzhu Wang"
                },
                {
                    "authorId": "2072529340",
                    "name": "Shan An"
                },
                {
                    "authorId": "2168258085",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2168261097",
                    "name": "Xiong Peng"
                },
                {
                    "authorId": "2118684992",
                    "name": "Wei Yu"
                },
                {
                    "authorId": "50763255",
                    "name": "Junyang Chen"
                },
                {
                    "authorId": "2114938960",
                    "name": "Z. Luo"
                }
            ]
        },
        {
            "paperId": "eaf8c94d8579be7c356ae161250c59f452d9b557",
            "title": "Expediting Large-Scale Vision Transformer for Dense Prediction without Fine-tuning",
            "abstract": "Vision transformers have recently achieved competitive results across various vision tasks but still suffer from heavy computation costs when processing a large number of tokens. Many advanced approaches have been developed to reduce the total number of tokens in large-scale vision transformers, especially for image classification tasks. Typically, they select a small group of essential tokens according to their relevance with the class token, then fine-tune the weights of the vision transformer. Such fine-tuning is less practical for dense prediction due to the much heavier computation and GPU memory cost than image classification. In this paper, we focus on a more challenging problem, i.e., accelerating large-scale vision transformers for dense prediction without any additional re-training or fine-tuning. In response to the fact that high-resolution representations are necessary for dense prediction, we present two non-parametric operators, a token clustering layer to decrease the number of tokens and a token reconstruction layer to increase the number of tokens. The following steps are performed to achieve this: (i) we use the token clustering layer to cluster the neighboring tokens together, resulting in low-resolution representations that maintain the spatial structures; (ii) we apply the following transformer layers only to these low-resolution representations or clustered tokens; and (iii) we use the token reconstruction layer to re-create the high-resolution representations from the refined low-resolution representations. The results obtained by our method are promising on five dense prediction tasks, including object detection, semantic segmentation, panoptic segmentation, instance segmentation, and depth estimation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2186825281",
                    "name": "Weicong Liang"
                },
                {
                    "authorId": "49521390",
                    "name": "Yuhui Yuan"
                },
                {
                    "authorId": "49441320",
                    "name": "Henghui Ding"
                },
                {
                    "authorId": "2168258085",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2108811137",
                    "name": "Weihong Lin"
                },
                {
                    "authorId": "2117587273",
                    "name": "Ding Jia"
                },
                {
                    "authorId": "2148904543",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2164073950",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "2118879491",
                    "name": "Hanhua Hu"
                }
            ]
        }
    ]
}