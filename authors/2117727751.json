{
    "authorId": "2117727751",
    "papers": [
        {
            "paperId": "4884e516d10626bf0ee4e4bedc174534598e29b2",
            "title": "MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields",
            "abstract": "Previous research has demonstrated the advantages of integrating data from multiple sources over traditional unimodal data, leading to the emergence of numerous novel multimodal applications. We propose a multimodal classification benchmark MuG with eight datasets that allows researchers to evaluate and improve their models. These datasets are collected from four various genres of games that cover tabular, textual, and visual modalities. We conduct multi-aspect data analysis to provide insights into the benchmark, including label balance ratios, percentages of missing features, distributions of data within each modality, and the correlations between labels and input modalities. We further present experimental results obtained by several state-of-the-art unimodal classifiers and multimodal classifiers, which demonstrate the challenging and multimodal-dependent properties of the benchmark. MuG is released at https://github.com/lujiaying/MUG-Bench with the data, tutorials, and implemented baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "2204596717",
                    "name": "Yongchen Qian"
                },
                {
                    "authorId": "67152280",
                    "name": "Shifan Zhao"
                },
                {
                    "authorId": "2994506",
                    "name": "Yuanzhe Xi"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                }
            ]
        },
        {
            "paperId": "6847b9658f287f430098199cd81bf26308da13f9",
            "title": "Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey",
            "abstract": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to make large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to better summarize and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059988575",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "50879401",
                    "name": "Xujiang Zhao"
                },
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "2182238045",
                    "name": "Can Zheng"
                },
                {
                    "authorId": "2120473483",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "2123930262",
                    "name": "Tanmoy Chowdhury"
                },
                {
                    "authorId": "2110425042",
                    "name": "Yun-Qing Li"
                },
                {
                    "authorId": "2112821580",
                    "name": "Hejie Cui"
                },
                {
                    "authorId": "2048981220",
                    "name": "Xuchao Zhang"
                },
                {
                    "authorId": "2211987764",
                    "name": "Tian-yu Zhao"
                },
                {
                    "authorId": "2218486790",
                    "name": "Amit Panalkar"
                },
                {
                    "authorId": "145859270",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": null,
                    "name": "Haoyu Wang"
                },
                {
                    "authorId": "3215702",
                    "name": "Yanchi Liu"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2204622281",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2218495127",
                    "name": "Chris White"
                },
                {
                    "authorId": "144966687",
                    "name": "Quanquan Gu"
                },
                {
                    "authorId": "2188744953",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2151579859",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "75c08892179fc478f87d7020b5daff9fca4f3389",
            "title": "Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models",
            "abstract": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a \u201cchatbot\u201d, and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059988349",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "50879401",
                    "name": "Xujiang Zhao"
                },
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "2182238045",
                    "name": "Can Zheng"
                },
                {
                    "authorId": "4142921",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "2123930262",
                    "name": "Tanmoy Chowdhury"
                },
                {
                    "authorId": "2110425042",
                    "name": "Yun-Qing Li"
                },
                {
                    "authorId": "2112821580",
                    "name": "Hejie Cui"
                },
                {
                    "authorId": "2211987764",
                    "name": "Tian-yu Zhao"
                },
                {
                    "authorId": "2218486790",
                    "name": "Amit Panalkar"
                },
                {
                    "authorId": "145859270",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": null,
                    "name": "Haoyu Wang"
                },
                {
                    "authorId": "3215702",
                    "name": "Yanchi Liu"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2204622281",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2218495127",
                    "name": "Chris White"
                },
                {
                    "authorId": "9937103",
                    "name": "Quanquan Gu"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2151579859",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "9e545ea405b5c5ae42c4df7a10701ee9c4221c87",
            "title": "A Review on Knowledge Graphs for Healthcare: Resources, Applications, and Promises",
            "abstract": "Healthcare knowledge graphs (HKGs) are valuable tools for organizing biomedical concepts and their relationships with interpretable structures. The recent advent of large language models (LLMs) has paved the way for building more comprehensive and accurate HKGs. This, in turn, can improve the reliability of generated content and enable better evaluation of LLMs. However, the challenges of HKGs such as regarding data heterogeneity and limited coverage are not fully understood, highlighting the need for detailed reviews. This work provides the first comprehensive review of HKGs. It summarizes the pipeline and key techniques for HKG construction, as well as the common utilization approaches, i.e., model-free and model-based. The existing HKG resources are also organized based on the data types they capture and application domains they cover, along with relevant statistical information (Resource available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase). At the application level, we delve into the successful integration of HKGs across various health domains, ranging from fine-grained basic science research to high-level clinical decision support and public health. Lastly, the paper highlights the opportunities for HKGs in the era of LLMs. This work aims to serve as a valuable resource for understanding the potential and opportunities of HKG in health research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112821580",
                    "name": "Hejie Cui"
                },
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "2130352205",
                    "name": "Shiyu Wang"
                },
                {
                    "authorId": "47462790",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "2214023328",
                    "name": "Wenjing Ma"
                },
                {
                    "authorId": "2219270021",
                    "name": "Shaojun Yu"
                },
                {
                    "authorId": "2218865512",
                    "name": "Yue Yu"
                },
                {
                    "authorId": "2052319438",
                    "name": "Xuan Kan"
                },
                {
                    "authorId": "2219702470",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "152316651",
                    "name": "Joyce Ho"
                },
                {
                    "authorId": "2148957822",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                }
            ]
        },
        {
            "paperId": "a88018cbbaa547d14c19ec7c5abdea70c1e61211",
            "title": "HiPrompt: Few-Shot Biomedical Knowledge Fusion via Hierarchy-Oriented Prompting",
            "abstract": "Medical decision-making processes can be enhanced by comprehensive biomedical knowledge bases, which require fusing knowledge graphs constructed from different sources via a uniform index system. The index system often organizes biomedical terms in a hierarchy to provide the aligned entities with fine-grained granularity. To address the challenge of scarce supervision in the biomedical knowledge fusion (BKF) task, researchers have proposed various unsupervised methods. However, these methods heavily rely on ad-hoc lexical and structural matching algorithms, which fail to capture the rich semantics conveyed by biomedical entities and terms. Recently, neural embedding models have proved effective in semantic-rich tasks, but they rely on sufficient labeled data to be adequately trained. To bridge the gap between the scarce-labeled BKF and neural embedding models, we propose HiPrompt, a supervision-efficient knowledge fusion framework that elicits the few-shot reasoning ability of large language models through hierarchy-oriented prompts. Empirical results on the collected KG-Hi-BKF benchmark datasets demonstrate the effectiveness of HiPrompt.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "3363642",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "2065364720",
                    "name": "Bo Xiong"
                },
                {
                    "authorId": "2214023328",
                    "name": "Wenjing Ma"
                },
                {
                    "authorId": "2067038375",
                    "name": "Steffen Staab"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                }
            ]
        },
        {
            "paperId": "b89af748ae77930da76e654f9e851b35d6b9897b",
            "title": "Visual Diagnostics of Parallel Performance in Training Large-Scale DNN Models",
            "abstract": "Diagnosing the cluster-based performance of large-scale deep neural network (DNN) models during training is essential for improving training efficiency and reducing resource consumption. However, it remains challenging due to the incomprehensibility of the parallelization strategy and the sheer volume of complex data generated in the training processes. Prior works visually analyze performance profiles and timeline traces to identify anomalies from the perspective of individual devices in the cluster, which is not amenable for studying the root cause of anomalies. In this article, we present a visual analytics approach that empowers analysts to visually explore the parallel training process of a DNN model and interactively diagnose the root cause of a performance issue. A set of design requirements is gathered through discussions with domain experts. We propose an enhanced execution flow of model operators for illustrating parallelization strategies within the computational graph layout. We design and implement an enhanced Marey's graph representation, which introduces the concept of time-span and a banded visual metaphor to convey training dynamics and help experts identify inefficient training processes. We also propose a visual aggregation technique to improve visualization efficiency. We evaluate our approach using case studies, a user study and expert interviews on two large-scale models run in a cluster, namely, the PanGu-<inline-formula><tex-math notation=\"LaTeX\">$\\alpha$</tex-math><alternatives><mml:math><mml:mi>\u03b1</mml:mi></mml:math><inline-graphic xlink:href=\"wei-ieq1-3243228.gif\"/></alternatives></inline-formula> 13B model (40 layers), and the Resnet model (50 layers).",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112599341",
                    "name": "Yating Wei"
                },
                {
                    "authorId": "2109046224",
                    "name": "Zhiyong Wang"
                },
                {
                    "authorId": "2205420209",
                    "name": "Zhongwei Wang"
                },
                {
                    "authorId": "2205064711",
                    "name": "Yong Dai"
                },
                {
                    "authorId": "2197503020",
                    "name": "Gongchang Ou"
                },
                {
                    "authorId": "2112514104",
                    "name": "Han Gao"
                },
                {
                    "authorId": "27642839",
                    "name": "Haitao Yang"
                },
                {
                    "authorId": "2205191770",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "3151540",
                    "name": "Caleb Chen Cao"
                },
                {
                    "authorId": "2186416127",
                    "name": "Luoxuan Weng"
                },
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "2070270044",
                    "name": "Rongchen Zhu"
                },
                {
                    "authorId": "92896059",
                    "name": "Wei Chen"
                }
            ]
        },
        {
            "paperId": "f20ba3ff2d08af88af691f41006b0c07b1c30f20",
            "title": "Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models",
            "abstract": "Large Vision-Language Models (LVLMs) offer remarkable benefits for a variety of vision-language tasks. However, a challenge hindering their application in real-world scenarios, particularly regarding safety, robustness, and reliability, is their constrained semantic grounding ability, which pertains to connecting language to the physical-world entities or concepts referenced in images. Therefore, a crucial need arises for a comprehensive study to assess the semantic grounding ability of widely used LVLMs. Despite the significance, sufficient investigation in this direction is currently lacking. Our work bridges this gap by designing a pipeline for generating large-scale evaluation datasets covering fine-grained semantic information, such as color, number, material, etc., along with a thorough assessment of seven popular LVLMs' semantic grounding ability. Results highlight prevalent misgrounding across various aspects and degrees. To address this issue, we propose a data-centric enhancement method that aims to improve LVLMs' semantic grounding ability through multimodal instruction tuning on fine-grained conversations. Experiments on enhanced LVLMs demonstrate notable improvements in addressing misgrounding issues.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "2238635148",
                    "name": "Jinmeng Rao"
                },
                {
                    "authorId": "2238911526",
                    "name": "Kezhen Chen"
                },
                {
                    "authorId": null,
                    "name": "Xiaoyuan Guo"
                },
                {
                    "authorId": "2145040404",
                    "name": "Yawen Zhang"
                },
                {
                    "authorId": "2238900563",
                    "name": "Baochen Sun"
                },
                {
                    "authorId": "2238138844",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2239161755",
                    "name": "Jie Yang"
                }
            ]
        },
        {
            "paperId": "7b6715f3a7729385456ebde8c91451e85caf19b4",
            "title": "Closed-book Question Generation via Contrastive Learning",
            "abstract": "Question Generation (QG) is a fundamental NLP task for many downstream applications. Recent studies on open-book QG, where supportive answer-context pairs are provided to models, have achieved promising progress. However, generating natural questions under a more practical closed-book setting that lacks these supporting documents still remains a challenge. In this work, we propose a new QG model for this closed-book setting that is designed to better understand the semantics of long-form abstractive answers and store more information in its parameters through contrastive learning and an answer reconstruction module. Through experiments, we validate the proposed QG model on both public datasets and a new WikiCQA dataset. Empirical results show that the proposed QG model outperforms baselines in both automatic evaluation and human evaluation. In addition, we show how to leverage the proposed model to improve existing question-answering systems. These results further indicate the effectiveness of our QG model for enhancing closed-book question-answering tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1716200686",
                    "name": "Xiangjue Dong"
                },
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "46584367",
                    "name": "Jianling Wang"
                },
                {
                    "authorId": "1697232",
                    "name": "James Caverlee"
                }
            ]
        },
        {
            "paperId": "fdb4f99471ad5911239893577aea89b5957b3d56",
            "title": "Open-World Taxonomy and Knowledge Graph Co-Learning",
            "abstract": "Taxonomies and knowledge graphs (KGs), which represent real-world entities\u2019 abstract concepts and properties/behaviors/facts, constitute the essential information in knowledge bases (KBs). How-ever, most existing KBs are constructed under the closed-world assumption, which often corresponds to a \ufffd xed schema and requires ad-hoc canonicalization to include new knowledge. To empower KBs towards easy accommodation of emerging entities and relations, we propose to create open-world T \ufffd\ufffd\ufffd KG based on existing automatically constructed taxonomies and open KGs, where taxonomies serve to provide a loosely-de \ufffd ned schema and mitigate the reliance on ad-hoc canonicalization. To further improve the com-pleteness of T \ufffd\ufffd\ufffd KG , we collect several new benchmark datasets towards the development of H \ufffd\ufffd\ufffd GCN , an innovative hierarchy-aware graph-friendly model for T \ufffd\ufffd\ufffd KG completion. H \ufffd\ufffd\ufffd GCN learns to leverage the mutual enhancement between taxonomies and KGs, following the human reasoning process to generalize and conceptualize over taxonomic and non-taxonomic relations. Through extensive experiments, we demonstrate H \ufffd\ufffd\ufffd GCN to outperform various state-of-the-art KB completion methods on both taxonomy concept prediction and KG relation prediction tasks based on both standard metrics and human evaluations. The benchmark datasets and the implementation of H \ufffd\ufffd\ufffd GCN are available at https://anonymous.4open.science/r/Hake-GCN/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                }
            ]
        },
        {
            "paperId": "499bf43dd94faf008ce48ba46882610b1e6b14cf",
            "title": "Evaluation of Unsupervised Entity and Event Salience Estimation",
            "abstract": "Salience Estimation aims to predict term importance in documents.Due to few existing human-annotated datasets and the subjective notion of salience, previous studies typically generate pseudo-ground truth for evaluation. However, our investigation reveals that the evaluation protocol proposed by prior work is difficult to replicate, thus leading to few follow-up studies existing. Moreover, the evaluation process is problematic: the entity linking tool used for entity matching is very noisy, while the ignorance of event argument for event evaluation leads to boosted performance. In this work, we propose a light yet practical entity and event salience estimation evaluation protocol, which incorporates the more reliable syntactic dependency parser. Furthermore, we conduct a comprehensive analysis among popular entity and event definition standards, and present our own definition for the Salience Estimation task to reduce noise during the pseudo-ground truth generation process. Furthermore, we construct dependency-based heterogeneous graphs to capture the interactions of entities and events. The empirical results show that both baseline methods and the novel GNN method utilizing the heterogeneous graph consistently outperform the previous SOTA model in all proposed metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "4724587",
                    "name": "Jinho D. Choi"
                }
            ]
        }
    ]
}