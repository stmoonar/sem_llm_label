{
    "authorId": "2148822529",
    "papers": [
        {
            "paperId": "161956dc055a680cfe1a0bbdc22fabd9822cadc8",
            "title": "Simple and Scalable Nearest Neighbor Machine Translation",
            "abstract": "$k$NN-MT is a straightforward yet powerful approach for fast domain adaptation, which directly plugs pre-trained neural machine translation (NMT) models with domain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval to achieve domain adaptation without retraining. Despite being conceptually attractive, $k$NN-MT is burdened with massive storage requirements and high computational complexity since it conducts nearest neighbor searches over the entire reference corpus. In this paper, we propose a simple and scalable nearest neighbor machine translation framework to drastically promote the decoding and storage efficiency of $k$NN-based models while maintaining the translation performance. To this end, we dynamically construct an extremely small datastore for each input via sentence-level retrieval to avoid searching the entire datastore in vanilla $k$NN-MT, based on which we further introduce a distance-aware adapter to adaptively incorporate the $k$NN retrieval results into the pre-trained NMT models. Experiments on machine translation in two general settings, static domain adaptation and online learning, demonstrate that our proposed approach not only achieves almost 90% speed as the NMT model without performance degradation, but also significantly reduces the storage requirements of $k$NN-MT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26579978",
                    "name": "Yu-Hsiu Dai"
                },
                {
                    "authorId": "4947404",
                    "name": "Zhirui Zhang"
                },
                {
                    "authorId": "47362275",
                    "name": "Qiuzhi Liu"
                },
                {
                    "authorId": "2057040490",
                    "name": "Qu Cui"
                },
                {
                    "authorId": "48624955",
                    "name": "Wei-Hong Li"
                },
                {
                    "authorId": "121526469",
                    "name": "Yichao Du"
                },
                {
                    "authorId": "2148822529",
                    "name": "Tong Xu"
                }
            ]
        },
        {
            "paperId": "1a9e47b94e1c1fcbefedb75f034dae5fc66adc56",
            "title": "Reversible Graph Neural Network-based Reaction Distribution Learning for Multiple Appropriate Facial Reactions Generation",
            "abstract": "Generating facial reactions in a human-human dyadic interaction is complex and highly dependent on the context since more than one facial reactions can be appropriate for the speaker's behaviour. This has challenged existing machine learning (ML) methods, whose training strategies enforce models to reproduce a specific (not multiple) facial reaction from each input speaker behaviour. This paper proposes the first multiple appropriate facial reaction generation framework that re-formulates the one-to-many mapping facial reaction generation problem as a one-to-one mapping problem. This means that we approach this problem by considering the generation of a distribution of the listener's appropriate facial reactions instead of multiple different appropriate facial reactions, i.e., 'many' appropriate facial reaction labels are summarised as 'one' distribution label during training. Our model consists of a perceptual processor, a cognitive processor, and a motor processor. The motor processor is implemented with a novel Reversible Multi-dimensional Edge Graph Neural Network (REGNN). This allows us to obtain a distribution of appropriate real facial reactions during the training process, enabling the cognitive processor to be trained to predict the appropriate facial reaction distribution. At the inference stage, the REGNN decodes an appropriate facial reaction by using this distribution as input. Experimental results demonstrate that our approach outperforms existing models in generating more appropriate, realistic, and synchronized facial reactions. The improved performance is largely attributed to the proposed appropriate facial reaction distribution learning strategy and the use of a REGNN. The code is available at https://github.com/TongXu-05/REGNN-Multiple-Appropriate-Facial-Reaction-Generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2148822529",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "73772115",
                    "name": "Micol Spitale"
                },
                {
                    "authorId": "2109238731",
                    "name": "Haozhan Tang"
                },
                {
                    "authorId": "145412331",
                    "name": "Lu Liu"
                },
                {
                    "authorId": "150122398",
                    "name": "Hatice Gunes"
                },
                {
                    "authorId": "1455999484",
                    "name": "Siyang Song"
                }
            ]
        },
        {
            "paperId": "32ffb5e528b54f03a8a359e4573e0f3926a1202b",
            "title": "CgT-GAN: CLIP-guided Text GAN for Image Captioning",
            "abstract": "The large-scale visual-language pre-trained model, Contrastive Language-Image Pre-training (CLIP), has significantly improved image captioning for scenarios without human-annotated image-caption pairs. Recent advanced CLIP-based image captioning without human annotations follows a text-only training paradigm, i.e., reconstructing text from shared embedding space. Nevertheless, these approaches are limited by the training/inference gap or huge storage requirements for text embeddings. Given that it is trivial to obtain images in the real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates images into the training process to enable the model to \"see\" real visual modality. Particularly, we use adversarial training to teach CgT-GAN to mimic the phrases of an external text corpus and CLIP-based reward to provide semantic guidance. The caption generator is jointly rewarded based on the caption naturalness to human language calculated from the GAN's discriminator and the semantic guidance reward computed by the CLIP-based reward module. In addition to the cosine similarity as the semantic guidance reward (i.e., CLIP-cos), we further introduce a novel semantic guidance reward called CLIP-agg, which aligns the generated caption with a weighted text embedding by attentively aggregating the entire corpus. Experimental results on three subtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms state-of-the-art methods significantly across all metrics. Code is available at https://github.com/Lihr747/CgtGAN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7177756",
                    "name": "Jiarui Yu"
                },
                {
                    "authorId": "2145539796",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "48387349",
                    "name": "Y. Hao"
                },
                {
                    "authorId": "2106718459",
                    "name": "B. Zhu"
                },
                {
                    "authorId": "2148822529",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "bd5c2b36743578eeaa69eb4b1100bfdde03d7d20",
            "title": "Multi-Grained Multimodal Interaction Network for Entity Linking",
            "abstract": "Multimodal entity linking (MEL) task, which aims at resolving ambiguous mentions to a multimodal knowledge graph, has attracted wide attention in recent years. Though large efforts have been made to explore the complementary effect among multiple modalities, however, they may fail to fully absorb the comprehensive expression of abbreviated textual context and implicit visual indication. Even worse, the inevitable noisy data may cause inconsistency of different modalities during the learning process, which severely degenerates the performance. To address the above issues, in this paper, we propose a novel Multi-GraIned Multimodal InteraCtion Network (MIMIC) framework for solving the MEL task. Specifically, the unified inputs of mentions and entities are first encoded by textual/visual encoders separately, to extract global descriptive features and local detailed features. Then, to derive the similarity matching score for each mention-entity pair, we device three interaction units to comprehensively explore the intra-modal interaction and inter-modal fusion among features of entities and mentions. In particular, three modules, namely the Text-based Global-Local interaction Unit (TGLU), Vision-based DuaL interaction Unit (VDLU) and Cross-Modal Fusion-based interaction Unit (CMFU) are designed to capture and integrate the fine-grained representation lying in abbreviated text and implicit visual cues. Afterwards, we introduce a unit-consistency objective function via contrastive learning to avoid inconsistency and model degradation. Experimental results on three public benchmark datasets demonstrate that our solution outperforms various state-of-the-art baselines, and ablation studies verify the effectiveness of designed modules.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212857769",
                    "name": "Pengfei Luo"
                },
                {
                    "authorId": "2148822529",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "2142349315",
                    "name": "Shiwei Wu"
                },
                {
                    "authorId": "2118043140",
                    "name": "Chen Zhu"
                },
                {
                    "authorId": "2230211",
                    "name": "Linli Xu"
                },
                {
                    "authorId": "2173129111",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "388d310b973591922acb742caf77a1c69fd070e1",
            "title": "Joint Attention-Driven Domain Fusion and Noise-Tolerant Learning for Multi-Source Domain Adaptation",
            "abstract": "As a study on the efficient usage of data, Multi-source Unsupervised Domain Adaptation transfers knowledge from multiple source domains with labeled data to an unlabeled target domain. However, the distribution discrepancy between different domains and the noisy pseudo-labels in the target domain both lead to performance bottlenecks of the Multi-source Unsupervised Domain Adaptation methods. In light of this, we propose an approach that integrates Attention-driven Domain fusion and Noise-Tolerant learning (ADNT) to address the two issues mentioned above. Firstly, we establish a contrary attention structure to perform message passing between features and to induce domain movement. Through this approach, the discriminability of the features can also be significantly improved while the domain discrepancy is reduced. Secondly, based on the characteristics of the unsupervised domain adaptation training, we design an Adaptive Reverse Cross Entropy loss, which can directly impose constraints on the generation of pseudo-labels. Finally, combining these two approaches, experimental results on several benchmarks further validate the effectiveness of our proposed ADNT and demonstrate superior performance over the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2148822529",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "2150529209",
                    "name": "Wu Ning"
                },
                {
                    "authorId": "2150530971",
                    "name": "Chunyan Lyu"
                },
                {
                    "authorId": "2131138502",
                    "name": "Kejun Wang"
                }
            ]
        },
        {
            "paperId": "912bffae96699170ca85830069e1ad58561ad103",
            "title": "Attention in Attention: Modeling Context Correlation for Efficient Video Classification",
            "abstract": "Attention mechanisms have significantly boosted the performance of video classification neural networks thanks to the utilization of perspective contexts. However, the current research on video attention generally focuses on adopting a specific aspect of contexts (e.g., channel, spatial/temporal, or global context) to refine the features and neglects their underlying correlation when computing attentions. This leads to incomplete context utilization and hence bears the weakness of limited performance improvement. To tackle the problem, this paper proposes an efficient attention-in-attention (AIA) method for element-wise feature refinement, which investigates the feasibility of inserting the channel context into the spatio-temporal attention learning module, referred to as CinST, and also its reverse variant, referred to as STinC. Specifically, we instantiate the video feature contexts as dynamics aggregated along a specific axis with global average and max pooling operations. The workflow of an AIA module is that the first attention block uses one kind of context information to guide the gating weights calculation of the second attention that targets at the other context. Moreover, all the computational operations in attention units act on the pooled dimension, which results in quite few computational cost increase (< 0.02%). To verify our method, we densely integrate it into two classical video network backbones and conduct extensive experiments on several standard video classification benchmarks. The source code of our AIA is available at https://github.com/haoyanbin918/Attention-in-Attention.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48387349",
                    "name": "Y. Hao"
                },
                {
                    "authorId": "2117011312",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2129268695",
                    "name": "P. Cao"
                },
                {
                    "authorId": "1994707",
                    "name": "Xinjian Gao"
                },
                {
                    "authorId": "2148822529",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "92959364",
                    "name": "Jinmeng Wu"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "9153e5212a72e1a895b235cccfda940d951efa6d",
            "title": "Domain-Invariant Proposals based on a Balanced Domain Classifier for Object Detection",
            "abstract": "Object recognition from images means to automatically find object(s) of interest and to return their category and location information. Benefiting from research on deep learning, like convolutional neural networks~(CNNs) and generative adversarial networks, the performance in this field has been improved significantly, especially when training and test data are drawn from similar distributions. However, mismatching distributions, i.e., domain shifts, lead to a significant performance drop. In this paper, we build domain-invariant detectors by learning domain classifiers via adversarial training. Based on the previous works that align image and instance level features, we mitigate the domain shift further by introducing a domain adaptation component at the region level within Faster \\mbox{R-CNN}. We embed a domain classification network in the region proposal network~(RPN) using adversarial learning. The RPN can now generate accurate region proposals in different domains by effectively aligning the features between them. To mitigate the unstable convergence during the adversarial learning, we introduce a balanced domain classifier as well as a network learning rate adjustment strategy. We conduct comprehensive experiments using four standard datasets. The results demonstrate the effectiveness and robustness of our object detection approach in domain shift scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2169546",
                    "name": "Zhize Wu"
                },
                {
                    "authorId": "2108504856",
                    "name": "Xiaofeng Wang"
                },
                {
                    "authorId": "2148822529",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "1878935777",
                    "name": "Xuebin Yang"
                },
                {
                    "authorId": "2111883674",
                    "name": "Lekang Zou"
                },
                {
                    "authorId": "2834768",
                    "name": "Lixiang Xu"
                },
                {
                    "authorId": "8950208",
                    "name": "T. Weise"
                }
            ]
        },
        {
            "paperId": "e03fbb4c3ef5203ad12dc659707423fd95b78f16",
            "title": "Non-Parametric Domain Adaptation for End-to-End Speech Translation",
            "abstract": "The end-to-end speech translation (E2E-ST) has received increasing attention due to the potential of its less error propagation, lower latency and fewer parameters. However, the effectiveness of neural-based approaches to this task is severely limited by the available training corpus, especially for domain adaptation where in-domain triplet data is scarce or nonexistent. In this paper, we propose a novel non-parametric method that leverages in-domain text translation corpus to achieve domain adaptation for E2E-ST systems. To this end, we first incorporate an additional encoder into the pre-trained E2E-ST model to realize text translation modeling, based on which the decoder\u2019s output representations for text and speech translation tasks are unified by reducing the correspondent representation mismatch in available triplet training data. During domain adaptation, a k-nearest-neighbor (kNN) classifier is introduced to produce the final translation distribution using the external datastore built by the domain-specific text translation corpus, while the universal output representation is adopted to perform a similarity search. Experiments on the Europarl-ST benchmark demonstrate that when in-domain text translation data is involved only, our proposed approach significantly improves baseline by 12.82 BLEU on average in all translation directions, even outperforming the strong in-domain fine-tuning strategy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "121526469",
                    "name": "Yichao Du"
                },
                {
                    "authorId": "2108438929",
                    "name": "Weizhi Wang"
                },
                {
                    "authorId": "4947404",
                    "name": "Zhirui Zhang"
                },
                {
                    "authorId": "2152687324",
                    "name": "Boxing Chen"
                },
                {
                    "authorId": "2148822529",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "145626731",
                    "name": "Jun Xie"
                },
                {
                    "authorId": "2088931329",
                    "name": "Enhong Chen"
                }
            ]
        },
        {
            "paperId": "139a83b5eab873dd464e2cf897887a95c8d07e13",
            "title": "Regularizing End-to-End Speech Translation with Triangular Decomposition Agreement",
            "abstract": "End-to-end speech-to-text translation (E2E-ST) is becoming increasingly popular due to the potential of its less error propagation, lower latency, and fewer parameters. Given the triplet training corpus\u3008speech, transcription, translation\u3009, the conventional high-quality E2E-ST system leverages the\u3008speech, transcription\u3009pair to pre-train the model and then utilizes the\u3008speech, translation\u3009pair to optimize it further. However, this process only involves two-tuple data at each stage, and this loose coupling fails to fully exploit the association between triplet data. In this paper, we attempt to model the joint probability of transcription and translation based on the speech input to directly leverage such triplet data. Based on that, we propose a novel regularization method for model training to improve the agreement of dual-path decomposition within triplet data, which should be equal in theory. To achieve this goal, we introduce two Kullback-Leibler divergence regularization terms into the model training objective to reduce the mismatch between output probabilities of dual-path. Then the well-trained model can be naturally transformed as the E2E-ST models by a pre-defined early stop tag. Experiments on the MuST-C benchmark demonstrate that our proposed approach significantly outperforms state-of-the-art E2E-ST baselines on all 8 language pairs while achieving better performance in the automatic speech recognition task.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "121526469",
                    "name": "Yichao Du"
                },
                {
                    "authorId": "4947404",
                    "name": "Zhirui Zhang"
                },
                {
                    "authorId": "2108438929",
                    "name": "Weizhi Wang"
                },
                {
                    "authorId": "2152687324",
                    "name": "Boxing Chen"
                },
                {
                    "authorId": "2109934951",
                    "name": "Jun Xie"
                },
                {
                    "authorId": "2148822529",
                    "name": "Tong Xu"
                }
            ]
        },
        {
            "paperId": "6fe5f32e9b66e8a0b9f7a73ee5ab6116efa9b127",
            "title": "VIRT: Improving Representation-based Models for Text Matching through Virtual Interaction",
            "abstract": "With the booming of pre-trained transformers, remarkable progress has been made on textual pair modeling to support relevant natural language applications. Two lines of approaches are developed for text matching: interaction-based models performing full interactions over the textual pair, and representation-based models encoding the pair independently with siamese encoders. The former achieves compelling performance due to its deep interaction modeling ability, yet with a sacri\ufb01ce in inference latency. The latter is ef\ufb01cient and widely adopted for practical use, however, suffers from severe performance degradation due to the lack of interactions. Though some prior work attempts to integrate interactive knowledge into representation-based models, considering the computational cost, they only perform late interaction or knowledge transferring at the top layers. Interactive information in the lower layers is still missing, which limits the performance of representation-based models. To remedy this, we propose a novel Virtual InteRacTion mechanism, termed as VIRT, to enable full and deep interaction modeling in representation-based models without actual inference computations. Concretely, VIRT asks representation-based encoders to conduct virtual interactions that mimic the behaviors as interaction-based models do. In addition, the knowledge distilled from interaction-based encoders is taken as supervised signals to promise the effectiveness of virtual interactions. Since virtual interactions only happen at the training stage, VIRT would not increase the inference cost. Furthermore, we design a VIRT-adapted late interaction strategy to fully utilize the learned virtual interactive knowledge. We implements VIRT and VIRT-adapted late interaction strategy jointly to a representation-based model, denoted as VIRT-encoder. Extensive experiments on multiple text matching datasets demonstrate that the proposed VIRT-encoder outperforms an array of state-of-the-art representation-based models. Last but not least, VIRT can be integrated into existing representation-based methods seamlessly to achieve further improvements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150383636",
                    "name": "Dan Li"
                },
                {
                    "authorId": "2152915671",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "3449368",
                    "name": "Hongyin Tang"
                },
                {
                    "authorId": "2109593338",
                    "name": "Jingang Wang"
                },
                {
                    "authorId": "2148822529",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "50224935",
                    "name": "Wei Yu Wu"
                },
                {
                    "authorId": "2173129111",
                    "name": "Enhong Chen"
                }
            ]
        }
    ]
}