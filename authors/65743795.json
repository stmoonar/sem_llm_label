{
    "authorId": "65743795",
    "papers": [
        {
            "paperId": "3090d5ef973e34e054ed520a118b2df8b16a5702",
            "title": "Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help Multiple Graph Applications",
            "abstract": "Model pre-training on large text corpora has been demonstrated effective for various downstream applications in the NLP domain. In the graph mining domain, a similar analogy can be drawn for pre-training graph models on large graphs in the hope of benefiting downstream graph applications, which has also been explored by several recent studies. However, no existing study has ever investigated the pre-training of text plus graph models on large heterogeneous graphs with abundant textual information (a.k.a. large graph corpora) and then fine-tuning the model on different related downstream applications with different graph schemas. To address this problem, we propose a framework of graph-aware language model pre-training (GaLM) on a large graph corpus, which incorporates large language models and graph neural networks, and a variety of fine-tuning methods on downstream applications. We conduct extensive experiments on Amazon's real internal datasets and large public datasets. Comprehensive empirical results and in-depth analysis demonstrate the effectiveness of our proposed methods along with lessons learned.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117714080",
                    "name": "Han Xie"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "65743795",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "92156482",
                    "name": "Houyu Zhang"
                },
                {
                    "authorId": "40043851",
                    "name": "V. Ioannidis"
                },
                {
                    "authorId": "2118943843",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "40492634",
                    "name": "Q. Ping"
                },
                {
                    "authorId": "2151487092",
                    "name": "Sheng Wang"
                },
                {
                    "authorId": "2695365",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2110290078",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "3191220",
                    "name": "Trishul M. Chilimbi"
                }
            ]
        },
        {
            "paperId": "40b0074cb865241b71f58f35e311da41b69152a2",
            "title": "Towards Text-to-SQL over Aggregate Tables",
            "abstract": "ABSTRACT Text-to-SQL aims at translating textual questions into the corresponding SQL queries. Aggregate tables are widely created for high-frequent queries. Although text-to-SQL has emerged as an important task, recent studies paid little attention to the task over aggregate tables. The increased aggregate tables bring two challenges: (1) mapping of natural language questions and relational databases will suffer from more ambiguity, (2) modern models usually adopt self-attention mechanism to encode database schema and question. The mechanism is of quadratic time complexity, which will make inferring more time-consuming as input sequence length grows. In this paper, we introduce a novel approach named WAGG for text-to-SQL over aggregate tables. To effectively select among ambiguous items, we propose a relation selection mechanism for relation computing. To deal with high computation costs, we introduce a dynamical pruning strategy to discard unrelated items that are common for aggregate tables. We also construct a new large-scale dataset SpiderwAGG extended from Spider dataset for validation, where extensive experiments show the effectiveness and efficiency of our proposed method with 4% increase of accuracy and 15% decrease of inference time w.r.t a strong baseline RAT-SQL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2205248199",
                    "name": "Shuqin Li"
                },
                {
                    "authorId": "2205551417",
                    "name": "Kaibin Zhou"
                },
                {
                    "authorId": "2168465079",
                    "name": "Zeyang Zhuang"
                },
                {
                    "authorId": "21606013",
                    "name": "Haofen Wang"
                },
                {
                    "authorId": "65743795",
                    "name": "Jun Ma"
                }
            ]
        },
        {
            "paperId": "51484cf02592a3551f944b7c6bf94fe902c0aa66",
            "title": "Train Your Own GNN Teacher: Graph-Aware Distillation on Textual Graphs",
            "abstract": "How can we learn effective node representations on textual graphs? Graph Neural Networks (GNNs) that use Language Models (LMs) to encode textual information of graphs achieve state-of-the-art performance in many node classification tasks. Yet, combining GNNs with LMs has not been widely explored for practical deployments due to its scalability issues. In this work, we tackle this challenge by developing a Graph-Aware Distillation framework (GRAD) to encode graph structures into an LM for graph-free, fast inference. Different from conventional knowledge distillation, GRAD jointly optimizes a GNN teacher and a graph-free student over the graph's nodes via a shared LM. This encourages the graph-free student to exploit graph information encoded by the GNN teacher while at the same time, enables the GNN teacher to better leverage textual information from unlabeled nodes. As a result, the teacher and the student models learn from each other to improve their overall performance. Experiments in eight node classification benchmarks in both transductive and inductive settings showcase GRAD's superiority over existing distillation approaches for textual graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1944251405",
                    "name": "Costas Mavromatis"
                },
                {
                    "authorId": "40043851",
                    "name": "V. Ioannidis"
                },
                {
                    "authorId": "2151226309",
                    "name": "Shen Wang"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "65743795",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "1390716752",
                    "name": "Han Zhao"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "aa1011bd50b7e3fefa68bb2593b36eeccbebd679",
            "title": "Efficient and effective training of language and graph neural network models",
            "abstract": "Can we combine heterogenous graph structure with text to learn high-quality semantic and behavioural representations? Graph neural networks (GNN)s encode numerical node attributes and graph structure to achieve impressive performance in a variety of supervised learning tasks. Current GNN approaches are challenged by textual features, which typically need to be encoded to a numerical vector before provided to the GNN that may incur some information loss. In this paper, we put forth an efficient and effective framework termed language model GNN (LM-GNN) to jointly train large-scale language models and graph neural networks. The effectiveness in our framework is achieved by applying stage-wise fine-tuning of the BERT model first with heterogenous graph information and then with a GNN model. Several system and design optimizations are proposed to enable scalable and efficient training. LM-GNN accommodates node and edge classification as well as link prediction tasks. We evaluate the LM-GNN framework in different datasets performance and showcase the effectiveness of the proposed approach. LM-GNN provides competitive results in an Amazon query-purchase-product application.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40043851",
                    "name": "V. Ioannidis"
                },
                {
                    "authorId": "2118943843",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "92156482",
                    "name": "Houyu Zhang"
                },
                {
                    "authorId": "65743795",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "2110290078",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "3191220",
                    "name": "Trishul M. Chilimbi"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "0656f32c16a4da834734e262047e26be1ab6911e",
            "title": "EX3: Explainable Attribute-aware Item-set Recommendations",
            "abstract": "Existing recommender systems in the e-commerce domain primarily focus on generating a set of relevant items as recommendations; however, few existing systems utilize underlying item attributes as a key organizing principle in presenting recommendations to users. Mining important attributes of items from customer perspectives and presenting them along with item sets as recommendations can provide users more explainability and help them make better purchase decision. In this work, we generalize the attribute-aware item-set recommendation problem, and develop a new approach to generate sets of items (recommendations) with corresponding important attributes (explanations) that can best justify why the items are recommended to users. In particular, we propose a system that learns important attributes from historical user behavior to derive item set recommendations, so that an organized view of recommendations and their attribute-driven explanations can help users more easily understand how the recommendations relate to their preferences. Our approach is geared towards real world scenarios: we expect a solution to be scalable to billions of items, and be able to learn item and attribute relevance automatically from user behavior without human annotations. To this end, we propose a multi-step learning-based framework called Extract-Expect-Explain (EX3), which is able to adaptively select recommended items and important attributes for users. We experiment on a large-scale real-world benchmark and the results show that our model outperforms state-of-the-art baselines by an 11.35% increase on NDCG with adaptive explainability for item set recommendation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2885287",
                    "name": "Yikun Xian"
                },
                {
                    "authorId": "2088210968",
                    "name": "Tong Zhao"
                },
                {
                    "authorId": "2133302300",
                    "name": "Jin Li"
                },
                {
                    "authorId": "2151246824",
                    "name": "Jim Chan"
                },
                {
                    "authorId": "2063962350",
                    "name": "Andrey Kan"
                },
                {
                    "authorId": "65743795",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "2143918523",
                    "name": "Xin Dong"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                },
                {
                    "authorId": "1591136873",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "5841743b05a5cbab84c81194620eeb0321493cd2",
            "title": "End-to-End Conversational Search for Online Shopping with Utterance Transfer",
            "abstract": "Successful conversational search systems can present natural, adaptive and interactive shopping experience for online shopping customers. However, building such systems from scratch faces real word challenges from both imperfect product schema/knowledge and lack of training dialog data. In this work we first propose ConvSearch, an end-to-end conversational search system that deeply combines the dialog system with search. It leverages the text profile to retrieve products, which is more robust against imperfect product schema/knowledge compared with using product attributes alone. We then address the lack of data challenges by proposing an utterance transfer approach that generates dialogue utterances by using existing dialog from other domains, and leveraging the search behavior data from e-commerce retailer. With utterance transfer, we introduce a new conversational search dataset for online shopping. Experiments show that our utterance transfer method can significantly improve the availability of training dialogue data without crowd-sourcing, and the conversational search system significantly outperformed the best tested baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149551771",
                    "name": "Liqiang Xiao"
                },
                {
                    "authorId": "65743795",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "2143918523",
                    "name": "Xin Dong"
                },
                {
                    "authorId": "1401946168",
                    "name": "Pascual Mart\u00ednez-G\u00f3mez"
                },
                {
                    "authorId": "2814303",
                    "name": "Nasser Zalmout"
                },
                {
                    "authorId": "2154941685",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2153707145",
                    "name": "Tong Zhao"
                },
                {
                    "authorId": "144111414",
                    "name": "Hao He"
                },
                {
                    "authorId": "35692109",
                    "name": "Yaohui Jin"
                }
            ]
        },
        {
            "paperId": "2e44f39e9887e1cdd91d48ab18a0bae53ff7f81a",
            "title": "AutoKnow: Self-Driving Knowledge Collection for Products of Thousands of Types",
            "abstract": "Can one build a knowledge graph (KG) for all products in the world? Knowledge graphs have firmly established themselves as valuable sources of information for search and question answering, and it is natural to wonder if a KG can contain information about products offered at online retail sites. There have been several successful examples of generic KGs, but organizing information about products poses many additional challenges, including sparsity and noise of structured data for products, complexity of the domain with millions of product types and thousands of attributes, heterogeneity across large number of categories, as well as large and constantly growing number of products. We describe AutoKnow, our automatic (self-driving) system that addresses these challenges. The system includes a suite of novel techniques for taxonomy construction, product property identification, knowledge extraction, anomaly detection, and synonym discovery. AutoKnow is (a) automatic, requiring little human intervention, (b) multi-scalable, scalable in multiple dimensions (many domains, many products, and many attributes), and (c) integrative, exploiting rich customer behavior logs. AutoKnow has been operational in collecting product knowledge for over 11K product types.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143917898",
                    "name": "Xin Dong"
                },
                {
                    "authorId": "2111080293",
                    "name": "Xiang He"
                },
                {
                    "authorId": "2063962350",
                    "name": "Andrey Kan"
                },
                {
                    "authorId": "2116235540",
                    "name": "Xian Li"
                },
                {
                    "authorId": "2116799460",
                    "name": "Yan Liang"
                },
                {
                    "authorId": "65743795",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "15574937",
                    "name": "Y. Xu"
                },
                {
                    "authorId": "2418496",
                    "name": "Chenwei Zhang"
                },
                {
                    "authorId": "2152225582",
                    "name": "Tong Zhao"
                },
                {
                    "authorId": "1753857667",
                    "name": "Gabriel Blanco Saldana"
                },
                {
                    "authorId": "2054184362",
                    "name": "Saurabh Deshpande"
                },
                {
                    "authorId": "153307803",
                    "name": "A. Manduca"
                },
                {
                    "authorId": "1753812304",
                    "name": "Jay Ren"
                },
                {
                    "authorId": "2109039708",
                    "name": "Surender Pal Singh"
                },
                {
                    "authorId": "2057533745",
                    "name": "Fan Xiao"
                },
                {
                    "authorId": "144827671",
                    "name": "Haw-Shiuan Chang"
                },
                {
                    "authorId": "8458211",
                    "name": "Giannis Karamanolakis"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "2143381988",
                    "name": "Yaqing Wang"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "143753639",
                    "name": "A. McCallum"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "4d15d83ae7e61fded91a9455b4a268273718fd7d",
            "title": "TXtract: Taxonomy-Aware Knowledge Extraction for Thousands of Product Categories",
            "abstract": "Extracting structured knowledge from product profiles is crucial for various applications in e-Commerce. State-of-the-art approaches for knowledge extraction were each designed for a single category of product, and thus do not apply to real-life e-Commerce scenarios, which often contain thousands of diverse categories. This paper proposes TXtract, a taxonomy-aware knowledge extraction model that applies to thousands of product categories organized in a hierarchical taxonomy. Through category conditional self-attention and multi-task learning, our approach is both scalable, as it trains a single model for thousands of categories, and effective, as it extracts category-specific attribute values. Experiments on products from a taxonomy with 4,000 categories show that TXtract outperforms state-of-the-art approaches by up to 10% in F1 and 15% in coverage across all categories.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "8458211",
                    "name": "Giannis Karamanolakis"
                },
                {
                    "authorId": "65743795",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "2143917898",
                    "name": "Xin Dong"
                }
            ]
        },
        {
            "paperId": "4f066299d62bdc87be1f6173a906a1f07a15a496",
            "title": "LinkNBed: Multi-Graph Representation Learning with Entity Linkage",
            "abstract": "Knowledge graphs have emerged as an important model for studying complex multi-relational data. This has given rise to the construction of numerous large scale but incomplete knowledge graphs encoding information extracted from various resources. An effective and scalable approach to jointly learn over multiple graphs and eventually construct a unified graph is a crucial next step for the success of knowledge-based inference for many downstream applications. To this end, we propose LinkNBed, a deep relational learning framework that learns entity and relationship representations across multiple graphs. We identify entity linkage across graphs as a vital component to achieve our goal. We design a novel objective that leverage entity linkage and build an efficient multi-task training procedure. Experiments on link prediction and entity linkage demonstrate substantial improvements over the state-of-the-art relational learning approaches.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3439813",
                    "name": "Rakshit S. Trivedi"
                },
                {
                    "authorId": "2582063",
                    "name": "Bunyamin Sisman"
                },
                {
                    "authorId": "2143917898",
                    "name": "Xin Dong"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "65743795",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "145203884",
                    "name": "H. Zha"
                }
            ]
        }
    ]
}