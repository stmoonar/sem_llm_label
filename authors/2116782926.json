{
    "authorId": "2116782926",
    "papers": [
        {
            "paperId": "00c1ff63468305ea3fa430c2b3aef156d580c4ff",
            "title": "PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3",
            "abstract": "Knowledge-based visual question answering (VQA) involves questions that require world knowledge beyond the image to yield the correct answer. Large language models (LMs) like GPT-3 are particularly helpful for this task because of their strong knowledge retrieval and reasoning capabilities. To enable LM to understand images, prior work uses a captioning model to convert images into text. However, when summarizing an image in a single caption sentence, which visual entities to describe are often underspecified. Generic image captions often miss visual details essential for the LM to answer visual questions correctly. To address this challenge, we propose PromptCap (Prompt-guided image Captioning), a captioning model designed to serve as a better connector between images and black-box LMs. Different from generic captions, PromptCap takes a natural-language prompt to control the visual entities to describe in the generated caption. The prompt contains a question that the caption should aid in answering. To avoid extra annotation, PromptCap is trained by examples synthesized with GPT-3 and existing datasets. We demonstrate Prompt-Cap\u2019s effectiveness on an existing pipeline in which GPT-3 is prompted with image captions to carry out VQA. Prompt-Cap outperforms generic captions by a large margin and achieves state-of-the-art accuracy on knowledge-based VQA tasks (60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that PromptCap generalizes well to unseen domains.1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112209725",
                    "name": "Yushi Hu"
                },
                {
                    "authorId": "2118198092",
                    "name": "Hang Hua"
                },
                {
                    "authorId": "2149231840",
                    "name": "Zhengyuan Yang"
                },
                {
                    "authorId": "3040379",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "144365875",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2116782926",
                    "name": "Jiebo Luo"
                }
            ]
        },
        {
            "paperId": "042f599fe8e62b567a84f80183100b9f1b01c6ba",
            "title": "Occluded Visible-Infrared Person Re-Identification",
            "abstract": "Visible-infrared person re-identification (VI-ReID) aims to match person images between the visible and near-infrared modalities. Previous VI-ReID methods are based on holistic pedestrian images and achieve excellent performance. However, in real-world scenarios, images captured by visible and near-infrared cameras usually contain occlusions. The performance of these methods degrades significantly due to the loss of information of discriminative features from the occlusion of the images. We define visible-infrared person re-identification in this occlusion scene as Occluded VI-ReID, where only partial content information of pedestrian images can be used to match images of different modalities from different cameras. In this paper, we propose a matching framework for occlusion scenes, which contains a local feature enhance module (LFEM) and a modality information fusion module (MIFM). LFEM adopts Transformer to learn features of each modality, and adjusts the importance of patches to enhance the representation ability of local features of the non-occluded areas. MIFM utilizes a co-attention mechanism to infer the correlation between each image for reducing the difference between modalities. We construct two occluded VI-ReID datasets, namely Occluded-SYSU-MM01 and Occluded-RegDB datasets. Our approach outperforms existing state-of-the-art methods on two occlusion datasets, while remains top performance on two holistic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500382103",
                    "name": "Yujian Feng"
                },
                {
                    "authorId": "2114168440",
                    "name": "Yimu Ji"
                },
                {
                    "authorId": "2110920795",
                    "name": "Fei Wu"
                },
                {
                    "authorId": "3306402",
                    "name": "Guangwei Gao"
                },
                {
                    "authorId": null,
                    "name": "Yang Gao"
                },
                {
                    "authorId": "1716708",
                    "name": "Tianliang Liu"
                },
                {
                    "authorId": "9296568",
                    "name": "Shangdong Liu"
                },
                {
                    "authorId": "15132338",
                    "name": "Xiaoyuan Jing"
                },
                {
                    "authorId": "2116782926",
                    "name": "Jiebo Luo"
                }
            ]
        },
        {
            "paperId": "0af7a93282e322f8eb618b01f0c5e4c9b8a855bd",
            "title": "ICDAR 2023 Competition on Structured Text Extraction from Visually-Rich Document Images",
            "abstract": "Structured text extraction is one of the most valuable and challenging application directions in the field of Document AI. However, the scenarios of past benchmarks are limited, and the corresponding evaluation protocols usually focus on the submodules of the structured text extraction scheme. In order to eliminate these problems, we organized the ICDAR 2023 competition on Structured text extraction from Visually-Rich Document images (SVRD). We set up two tracks for SVRD including Track 1: HUST-CELL and Track 2: Baidu-FEST, where HUST-CELL aims to evaluate the end-to-end performance of Complex Entity Linking and Labeling, and Baidu-FEST focuses on evaluating the performance and generalization of Zero-shot / Few-shot Structured Text extraction from an end-to-end perspective. Compared to the current document benchmarks, our two tracks of competition benchmark enriches the scenarios greatly and contains more than 50 types of visually-rich document images (mainly from the actual enterprise applications). The competition opened on 30th December, 2022 and closed on 24th March, 2023. There are 35 participants and 91 valid submissions received for Track 1, and 15 participants and 26 valid submissions received for Track 2. In this report we will presents the motivation, competition datasets, task definition, evaluation protocol, and submission summaries. According to the performance of the submissions, we believe there is still a large gap on the expected information extraction performance for complex and zero-shot scenarios. It is hoped that this competition will attract many researchers in the field of CV and NLP, and bring some new thoughts to the field of Document AI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2048569409",
                    "name": "Wenwen Yu"
                },
                {
                    "authorId": "1979323",
                    "name": "Chengquan Zhang"
                },
                {
                    "authorId": "50732729",
                    "name": "H. Cao"
                },
                {
                    "authorId": "2052831093",
                    "name": "Wei Hua"
                },
                {
                    "authorId": "2165246108",
                    "name": "Bohan Li"
                },
                {
                    "authorId": "2145302520",
                    "name": "Huang-wei Chen"
                },
                {
                    "authorId": "145111960",
                    "name": "Ming Liu"
                },
                {
                    "authorId": "2145944845",
                    "name": "Mingrui Chen"
                },
                {
                    "authorId": "2217114051",
                    "name": "Jianfeng Kuang"
                },
                {
                    "authorId": "2149615914",
                    "name": "Mengjun Cheng"
                },
                {
                    "authorId": "2867809",
                    "name": "Yuning Du"
                },
                {
                    "authorId": "2113511541",
                    "name": "Shikun Feng"
                },
                {
                    "authorId": "2109752359",
                    "name": "Xiaoguang Hu"
                },
                {
                    "authorId": "10344582",
                    "name": "Pengyuan Lyu"
                },
                {
                    "authorId": "2140371004",
                    "name": "Kun Yao"
                },
                {
                    "authorId": "1631405123",
                    "name": "Yu Yu"
                },
                {
                    "authorId": "2108353180",
                    "name": "Yuliang Liu"
                },
                {
                    "authorId": "2256319",
                    "name": "Wanxiang Che"
                },
                {
                    "authorId": "12081764",
                    "name": "Errui Ding"
                },
                {
                    "authorId": "2213535021",
                    "name": "Chengxi Liu"
                },
                {
                    "authorId": "2116782926",
                    "name": "Jiebo Luo"
                },
                {
                    "authorId": "2111618103",
                    "name": "Shuicheng Yan"
                },
                {
                    "authorId": "39767557",
                    "name": "M. Zhang"
                },
                {
                    "authorId": "1694974",
                    "name": "Dimosthenis Karatzas"
                },
                {
                    "authorId": "11774864",
                    "name": "Xingchao Sun"
                },
                {
                    "authorId": "2109534192",
                    "name": "Jingdong Wang"
                },
                {
                    "authorId": "2117845424",
                    "name": "Xiang Bai"
                }
            ]
        },
        {
            "paperId": "1569b20f1e96a14373f165d913c80015de61a6d6",
            "title": "SegPrompt: Using Segmentation Map as a Better Prompt to Finetune Deep Models for Kidney Stone Classification",
            "abstract": "Recently, deep learning has produced encouraging results for kidney stone classification using endoscope images. However, the shortage of annotated training data poses a severe problem in improving the performance and generalization ability of the trained model. It is thus crucial to fully exploit the limited data at hand. In this paper, we propose SegPrompt to alleviate the data shortage problems by exploiting segmentation maps from two aspects. First, SegPrompt integrates segmentation maps to facilitate classification training so that the classification model is aware of the regions of interest. The proposed method allows the image and segmentation tokens to interact with each other to fully utilize the segmentation map information. Second, we use the segmentation maps as prompts to tune the pretrained deep model, resulting in much fewer trainable parameters than vanilla finetuning. We perform extensive experiments on the collected kidney stone dataset. The results show that SegPrompt can achieve an advantageous balance between the model fitting ability and the generalization ability, eventually leading to an effective model with limited training data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144700079",
                    "name": "Wei Zhu"
                },
                {
                    "authorId": "2190841740",
                    "name": "Runtao Zhou"
                },
                {
                    "authorId": "2116945047",
                    "name": "Yao Yuan"
                },
                {
                    "authorId": "2211605358",
                    "name": "Campbell Timothy"
                },
                {
                    "authorId": "2113688194",
                    "name": "Rajat Jain"
                },
                {
                    "authorId": "2116782926",
                    "name": "Jiebo Luo"
                }
            ]
        },
        {
            "paperId": "2cfbeb0c2a0c268773e3b952f022aada9cfd0538",
            "title": "SurgicalSAM: Efficient Class Promptable Surgical Instrument Segmentation",
            "abstract": "The Segment Anything Model (SAM) is a powerful foundation model that has revolutionised image segmentation. To apply SAM to surgical instrument segmentation, a common approach is to locate precise points or boxes of instruments and then use them as prompts for SAM in a zero-shot manner. However, we observe two problems with this naive pipeline: (1) the domain gap between natural objects and surgical instruments leads to inferior generalisation of SAM; and (2) SAM relies on precise point or box locations for accurate segmentation, requiring either extensive manual guidance or a well-performing specialist detector for prompt preparation, which leads to a complex multi-stage pipeline. To address these problems, we introduce SurgicalSAM, a novel end-to-end efficient-tuning approach for SAM to effectively integrate surgical-specific information with SAM\u2019s pre-trained knowledge for improved generalisation. Specifically, we propose a lightweight prototype-based class prompt encoder for tuning, which directly generates prompt embeddings from class prototypes and eliminates the use of explicit prompts for improved robustness and a simpler pipeline. In addition, to address the low inter-class variance among surgical instrument categories, we propose contrastive prototype learning, further enhancing the discrimination of the class prototypes for more accurate class prompting. The results of extensive experiments on both EndoVis2018 and EndoVis2017 datasets demonstrate that SurgicalSAM achieves state-of-the-art performance while only requiring a small number of tunable parameters. The source code is available at https://github.com/wenxi-yue/SurgicalSAM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2214120021",
                    "name": "Wenxi Yue"
                },
                {
                    "authorId": "2257097930",
                    "name": "Jing Zhang"
                },
                {
                    "authorId": "2055735829",
                    "name": "Kun Hu"
                },
                {
                    "authorId": "2190919982",
                    "name": "Yong Xia"
                },
                {
                    "authorId": "2116782926",
                    "name": "Jiebo Luo"
                },
                {
                    "authorId": "2184760304",
                    "name": "Zhiyong Wang"
                }
            ]
        },
        {
            "paperId": "402c2947d832da2fed981d5bc245995e090a0526",
            "title": "Domain-Scalable Unpaired Image Translation via Latent Space Anchoring",
            "abstract": "Unpaired image-to-image translation (UNIT) aims to map images between two visual domains without paired training data. However, given a UNIT model trained on certain domains, it is difficult for current methods to incorporate new domains because they often need to train the full model on both existing and new domains. To address this problem, we propose a new domain-scalable UNIT method, termed as latent space anchoring, which can be efficiently extended to new visual domains and does not need to fine-tune encoders and decoders of existing domains. Our method anchors images of different domains to the same latent space of frozen GANs by learning lightweight encoder and regressor models to reconstruct single-domain images. In the inference phase, the learned encoders and decoders of different domains can be arbitrarily combined to translate images between any two domains without fine-tuning. Experiments on various datasets show that the proposed method achieves superior performance on both standard and domain-scalable UNIT tasks in comparison with the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48669017",
                    "name": "Siyu Huang"
                },
                {
                    "authorId": "1733982458",
                    "name": "Jie An"
                },
                {
                    "authorId": "1766333",
                    "name": "D. Wei"
                },
                {
                    "authorId": "151389936",
                    "name": "Zudi Lin"
                },
                {
                    "authorId": "2116782926",
                    "name": "Jiebo Luo"
                },
                {
                    "authorId": "143758236",
                    "name": "H. Pfister"
                }
            ]
        },
        {
            "paperId": "6c33123180ba3570e4d3770c5d050afd4553d7d0",
            "title": "Grounding 3D Object Affordance from 2D Interactions in Images",
            "abstract": "Grounding 3D object affordance seeks to locate objects\u2019 \"action possibilities\" regions in the 3D space, which serves as a link between perception and operation for embodied agents. Existing studies primarily focus on connecting visual affordances with geometry structures, e.g., relying on annotations to declare interactive regions of interest on the object and establishing a mapping between the regions and affordances. However, the essence of learning object affordance is to understand how to use it, and the manner that detaches interactions is limited in generalization. Normally, humans possess the ability to perceive object affordances in the physical world through demonstration images or videos. Motivated by this, we introduce a novel task setting: grounding 3D object affordance from 2D interactions in images, which faces the challenge of anticipating affordance through interactions of different sources. To address this problem, we devise a novel Interaction-driven 3D Affordance Grounding Network (IAG), which aligns the region feature of objects from different sources and models the interactive contexts for 3D object affordance grounding. Besides, we collect a Point-Image Affordance Dataset (PIAD) to support the proposed task. Comprehensive experiments on PIAD demonstrate the reliability of the proposed task and the superiority of our method. The project is available at https://github.com/yyvhang/IAGNet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2162386933",
                    "name": "Yuhang Yang"
                },
                {
                    "authorId": "2000495780",
                    "name": "Wei Zhai"
                },
                {
                    "authorId": "47029826",
                    "name": "Hongcheng Luo"
                },
                {
                    "authorId": "145871531",
                    "name": "Yang Cao"
                },
                {
                    "authorId": "2116782926",
                    "name": "Jiebo Luo"
                },
                {
                    "authorId": "143962510",
                    "name": "Zhengjun Zha"
                }
            ]
        },
        {
            "paperId": "730f15f042e13c713325a98b4cf512867c169080",
            "title": "VideoXum: Cross-Modal Visual and Textural Summarization of Videos",
            "abstract": "Video summarization aims to distill the most important information from a source video into either an abridged video clip or a textual narrative. Existing methods often treat the generation of video and text summaries as independent tasks, thus neglecting the semantic correlation between visual and textual summarization. In other words, these methods only study a single modality as output without considering coherent video and text as outputs. In this work, we first introduce a novel task: cross-modal video summarization. This task seeks to transfer a long video into a condensed video clip and a semantically aligned textual summary, collectively referred to as a cross-modal summary. We then establish VideoXum (X refers to different modalities), a new large-scale human-annotated video benchmark for cross-modal video summarization. VideoXum is reannotated based on ActivityNet Captions with diverse open-domain videos. In the current version, VideoXum provides 14 K long videos, with a total of 140 K pairs of aligned video and text summaries. Compared to existing datasets, VideoXum offers superior scalability while preserving a comparable level of annotation quality. To validate the dataset's quality, we provide a comprehensive analysis of VideoXum, comparing it with existing datasets. Further, we perform an extensive empirical evaluation of several state-of-the-art methods on this dataset. Our findings highlight the impressive generalization capability of the vision-language encoder-decoder framework yields on VideoXum. Particularly, we propose VTSUM-BLIP, an end-to-end framework, serving as a strong baseline for this novel benchmark. Moreover, we adapt CLIPScore for VideoXum to measure the semantic consistency of cross-modal summaries effectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144383782",
                    "name": "Jingyang Lin"
                },
                {
                    "authorId": "2118198092",
                    "name": "Hang Hua"
                },
                {
                    "authorId": "2108633405",
                    "name": "Ming Chen"
                },
                {
                    "authorId": "2110468365",
                    "name": "Yikang Li"
                },
                {
                    "authorId": "2064727535",
                    "name": "Jenhao Hsiao"
                },
                {
                    "authorId": "2097800792",
                    "name": "C. Ho"
                },
                {
                    "authorId": "2116782926",
                    "name": "Jiebo Luo"
                }
            ]
        },
        {
            "paperId": "81d971e33231557e190c38b054eebe0f4d1302e6",
            "title": "Event-Guided Person Re-Identification via Sparse-Dense Complementary Learning",
            "abstract": "Video-based person reidentification (Re-ID) is a prominent computer vision topic due to its wide range of video surveillance applications. Most existing methods utilize spatial and temporal correlations in frame sequences to obtain discriminative person features. However, inevitable degradation, e.g., motion blur contained in frames, leading to the loss of identity-discriminating cues. Recently, a new bio-inspired sensor called event camera, which can asynchronously record intensity changes, brings new vitality to the Re-ID task. With the microsecond resolution and low latency, it can accurately capture the movements of pedestrians even in the degraded environments. In this work, we propose a Sparse-Dense Complementary Learning (SDCL) Framework, which effectively extracts identity features by fully exploiting the complementary information of dense frames and sparse events. Specifically, for frames, we build a CNN-based module to aggregate the dense features of pedestrian appearance step by step, while for event streams, we design a bio-inspired spiking neural network (SNN) backbone, which encodes event signals into sparse feature maps in a spiking form, to extract the dynamic motion cues of pedestrians. Finally, a cross feature alignment module is constructed to fuse motion information from events and appearance cues from frames to enhance identity representation learning. Experiments on several benchmarks show that by employing events and SNN into Re-ID, our method significantly outperforms competitive methods. The code is available at https://github.com/Chengzhi-Cao/SDCL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187312488",
                    "name": "Chengzhi Cao"
                },
                {
                    "authorId": "3061449",
                    "name": "Xueyang Fu"
                },
                {
                    "authorId": "2155560642",
                    "name": "HongJiang Liu"
                },
                {
                    "authorId": "2145952181",
                    "name": "Yukun Huang"
                },
                {
                    "authorId": "2220665843",
                    "name": "Kunyu Wang"
                },
                {
                    "authorId": "2116782926",
                    "name": "Jiebo Luo"
                },
                {
                    "authorId": "143962510",
                    "name": "Zhengjun Zha"
                }
            ]
        },
        {
            "paperId": "8ad199f11f386319ebd2706c372562677c98fae3",
            "title": "Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation",
            "abstract": "We propose Latent-Shift -- an efficient text-to-video generation method based on a pretrained text-to-image generation model that consists of an autoencoder and a U-Net diffusion model. Learning a video diffusion model in the latent space is much more efficient than in the pixel space. The latter is often limited to first generating a low-resolution video followed by a sequence of frame interpolation and super-resolution models, which makes the entire pipeline very complex and computationally expensive. To extend a U-Net from image generation to video generation, prior work proposes to add additional modules like 1D temporal convolution and/or temporal attention layers. In contrast, we propose a parameter-free temporal shift module that can leverage the spatial U-Net as is for video generation. We achieve this by shifting two portions of the feature map channels forward and backward along the temporal dimension. The shifted features of the current frame thus receive the features from the previous and the subsequent frames, enabling motion learning without additional parameters. We show that Latent-Shift achieves comparable or better results while being significantly more efficient. Moreover, Latent-Shift can generate images despite being finetuned for T2V generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1733982458",
                    "name": "Jie An"
                },
                {
                    "authorId": "3178508",
                    "name": "Songyang Zhang"
                },
                {
                    "authorId": "2110162580",
                    "name": "Harry Yang"
                },
                {
                    "authorId": "2118343423",
                    "name": "Sonal Gupta"
                },
                {
                    "authorId": "2238908937",
                    "name": "Jia-Bin Huang"
                },
                {
                    "authorId": "2116782926",
                    "name": "Jiebo Luo"
                },
                {
                    "authorId": "1557290137",
                    "name": "Xiaoyue Yin"
                }
            ]
        }
    ]
}