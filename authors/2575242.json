{
    "authorId": "2575242",
    "papers": [
        {
            "paperId": "1bb1606d09db36b2129355b44ca3b5fc0febd105",
            "title": "Diversity, Equity and Inclusion Activities in Database Conferences: A 2023 Report",
            "abstract": "The Diversity, Equity and Inclusion (DEI) initiative started as the Diversity/Inclusion initiative in 2020 [4]. The current report summarizes our activities in 2023.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "143970078",
                    "name": "D. Agrawal"
                },
                {
                    "authorId": "2302858948",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1730344",
                    "name": "S. Bhowmick"
                },
                {
                    "authorId": "1404555727",
                    "name": "Renata Borovica-Gajic"
                },
                {
                    "authorId": "2314293279",
                    "name": "Jes\u00fas Camacho-Rodr\u00edguez"
                },
                {
                    "authorId": "2314330906",
                    "name": "Jinli Cao"
                },
                {
                    "authorId": "2314297028",
                    "name": "Barbara Catania"
                },
                {
                    "authorId": "2249901748",
                    "name": "P. Chrysanthis"
                },
                {
                    "authorId": "2278429940",
                    "name": "Carlo Curino"
                },
                {
                    "authorId": "117266605",
                    "name": "A. El Abbadi"
                },
                {
                    "authorId": "2327080",
                    "name": "Avrilia Floratou"
                },
                {
                    "authorId": "2178387374",
                    "name": "Juliana Freire"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "1685532",
                    "name": "V. Kalogeraki"
                },
                {
                    "authorId": "51205357",
                    "name": "Sujaya Maiyya"
                },
                {
                    "authorId": "2266690266",
                    "name": "Alexandra Meliou"
                },
                {
                    "authorId": "37168010",
                    "name": "Madhulika Mohanty"
                },
                {
                    "authorId": "2257398736",
                    "name": "Fatma \u00d6zcan"
                },
                {
                    "authorId": "3139922",
                    "name": "L. Peterfreund"
                },
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                },
                {
                    "authorId": "2314297947",
                    "name": "Sana Sellami"
                },
                {
                    "authorId": "14398962",
                    "name": "Roee Shraga"
                },
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "2314670128",
                    "name": "Wang-Chiew Tan"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                },
                {
                    "authorId": "2303255329",
                    "name": "Yuanyuan Tian"
                },
                {
                    "authorId": "1393643717",
                    "name": "Genoveva Vargas-Solar"
                },
                {
                    "authorId": "2314731432",
                    "name": "Meihui Zhang"
                },
                {
                    "authorId": "2302886251",
                    "name": "Wenjie Zhang"
                }
            ]
        },
        {
            "paperId": "dbea7141f8f3f1c1a298308721e56b749613c0c5",
            "title": "Are Large Language Models the New Interface for Data Pipelines?",
            "abstract": "A Language Model is a term that encompasses various types of models designed to understand and generate human communication. Large Language Models (LLMs) have gained significant attention due to their ability to process text with human-like fluency and coherence, making them valuable for a wide range of data-related tasks fashioned as pipelines. The capabilities of LLMs in natural language understanding and generation, combined with their scalability, versatility, and state-of-the-art performance, enable innovative applications across various AI-related fields, including eXplainable Artificial Intelligence (XAI), Automated Machine Learning (AutoML), and Knowledge Graphs (KG). Furthermore, we believe these models can extract valuable insights and make data-driven decisions at scale, a practice commonly referred to as Big Data Analytics (BDA). In this position paper, we provide some discussions in the direction of unlocking synergies among these technologies, which can lead to more powerful and intelligent AI solutions, driving improvements in data pipelines across a wide range of applications and domains integrating humans, computers, and knowledge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1742529781",
                    "name": "Sylvio Barbon J\u00fanior"
                },
                {
                    "authorId": "2265761542",
                    "name": "Paolo Ceravolo"
                },
                {
                    "authorId": "2286159830",
                    "name": "Sven Groppe"
                },
                {
                    "authorId": "2304486767",
                    "name": "Mustafa Jarrar"
                },
                {
                    "authorId": "1443738204",
                    "name": "S. Maghool"
                },
                {
                    "authorId": "2260650405",
                    "name": "Florence S\u00e8des"
                },
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                },
                {
                    "authorId": "118152506",
                    "name": "M. van Keulen"
                }
            ]
        },
        {
            "paperId": "ef1f443f0143dc36c11f7b08ca5b55c6bc866997",
            "title": "Research Trends for the Interplay between Large Language Models and Knowledge Graphs",
            "abstract": "This survey investigates the synergistic relationship between Large Language Models (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's capabilities in understanding, reasoning, and language processing. It aims to address gaps in current research by exploring areas such as KG Question Answering, ontology generation, KG validation, and the enhancement of KG accuracy and consistency through LLMs. The paper further examines the roles of LLMs in generating descriptive texts and natural language queries for KGs. Through a structured analysis that includes categorizing LLM-KG interactions, examining methodologies, and investigating collaborative uses and potential biases, this study seeks to provide new insights into the combined potential of LLMs and KGs. It highlights the importance of their interaction for improving AI applications and outlines future research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2007922846",
                    "name": "H. Khorashadizadeh"
                },
                {
                    "authorId": "40496856",
                    "name": "Fatima Zahra Amara"
                },
                {
                    "authorId": "2305825988",
                    "name": "M. Ezzabady"
                },
                {
                    "authorId": "2305819959",
                    "name": "Fr\u00e9d\u00e9ric Ieng"
                },
                {
                    "authorId": "2265763817",
                    "name": "Sanju Tiwari"
                },
                {
                    "authorId": "2689774",
                    "name": "Nandana Mihindukulasooriya"
                },
                {
                    "authorId": "2378835",
                    "name": "Jinghua Groppe"
                },
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                },
                {
                    "authorId": "2305811437",
                    "name": "Farah Benamara"
                },
                {
                    "authorId": "2286159830",
                    "name": "Sven Groppe"
                }
            ]
        },
        {
            "paperId": "9e70100274a5d68adb39945e059102257dbbf81e",
            "title": "Customized Eager-Lazy Data Cleansing for Satisfactory Big Data Veracity",
            "abstract": "Big data systems are becoming mainstream for big data management either for batch processing or real-time processing. In order to extract insights from data, quality issues are very important to address, particularly. A veracity assessment model is consequently needed. In this paper, we propose a model which ties quality of datasets and quality of query resultsets. We particularly examine quality issues raised by a given dataset, order attributes along their fitness for use and correlate veracity metrics to business queries. We validate our work using the open dataset NYC taxi\u2019 trips.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                },
                {
                    "authorId": "2540341",
                    "name": "Rim Moussa"
                }
            ]
        },
        {
            "paperId": "08e0b2c4a8321e2f8753d5d9e03ad9516e3b8681",
            "title": "A Big Data Platform for Enhancing Life Imaging Activities",
            "abstract": "The field of life imaging spans a large spectrum of scientific study from mathematics and computer science to medical, passing by physics, biology, etc. The challenge of IDV project is to enrich a multi-parametrized, quantitative, qualitative, integrative, and correlative life imaging in health. It deals with linking the current research developments and applications of life imaging in medicine and biology to develop computational models and methods for imaging and quantitative image analysis and validate the added diagnostic and therapeutic value of new imaging methods and biomarkers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3146976",
                    "name": "Leila Abidi"
                },
                {
                    "authorId": "1787431",
                    "name": "Hanene Azzag"
                },
                {
                    "authorId": "1795027",
                    "name": "S. Benbernou"
                },
                {
                    "authorId": "2885205",
                    "name": "M. Bentounsi"
                },
                {
                    "authorId": "2392661",
                    "name": "C. C\u00e9rin"
                },
                {
                    "authorId": "2121994",
                    "name": "T. Duong"
                },
                {
                    "authorId": "6570762",
                    "name": "P. Garteiser"
                },
                {
                    "authorId": "1802000",
                    "name": "M. Lebbah"
                },
                {
                    "authorId": "1803906",
                    "name": "Mourad Ouziri"
                },
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                },
                {
                    "authorId": "74315516",
                    "name": "M. Smadja"
                }
            ]
        },
        {
            "paperId": "6f317a976b24727be1526eff76b7032c7d95f98b",
            "title": "Towards Big Data in Medical Imaging",
            "abstract": "We present our vision to implement a big medical imaging platform to improve medical diagnosis. We aim to link multi-scale and multimodal images through open data and ontologies to discover new correlations and scientific knowledges. The platform is based on CIRRUS, a Sorbonne-Paris-Cite private cloud for research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1795027",
                    "name": "S. Benbernou"
                },
                {
                    "authorId": "2885205",
                    "name": "M. Bentounsi"
                },
                {
                    "authorId": "3948115",
                    "name": "P. Bourdoncle"
                },
                {
                    "authorId": "1802000",
                    "name": "M. Lebbah"
                },
                {
                    "authorId": "1803906",
                    "name": "Mourad Ouziri"
                },
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                }
            ]
        },
        {
            "paperId": "7b3089a824fd6d63d0b403361c1b5417989e7f39",
            "title": "Quality-Based Online Data Reconciliation",
            "abstract": "One of the main challenges in data matching and data cleaning, in highly integrated systems, is duplicates detection. While the literature abounds of approaches detecting duplicates corresponding to the same real-world entity, most of these approaches tend to eliminate duplicates (wrong information) from the sources, hence leading to what is called data repair. In this article, we propose a framework that automatically detects duplicates at query time and effectively identifies the consistent version of the data, while keeping inconsistent data in the sources. Our framework uses matching dependencies (MDs) to detect duplicates through the concept of data reconciliation rules (DRR) and conditional function dependencies (CFDs) to assess the quality of different attribute values. We also build a duplicate reconciliation index (DRI), based on clusters of duplicates detected by a set of DRRs to speed up the online data reconciliation process. Our experiments of a real-world data collection show the efficiency and effectiveness of our framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2672144",
                    "name": "Asma Abboura"
                },
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                },
                {
                    "authorId": "1403893293",
                    "name": "L. Baba-hamed"
                },
                {
                    "authorId": "1803906",
                    "name": "Mourad Ouziri"
                },
                {
                    "authorId": "1795027",
                    "name": "S. Benbernou"
                }
            ]
        },
        {
            "paperId": "45254b2dda5bcb0ed069a58c1173cced4d776660",
            "title": "CrowdMD: Crowdsourcing-based approach for deduplication",
            "abstract": "Matching dependencies (MDs) were recently introduced as quality rules for data cleaning and entity resolution. They are rules that specify what values should be considered duplicates, and have to be matched. Defining such quality rules on a database instance, is a very expensive and a time consuming process, and requires huge efforts to analyse the whole database. In this demo paper, we present CrowdMD, a hybrid machine-crowd system for generating MDs. It first asks the crowd to determine whether a given pair, from training sample pairs, match or not. Then, it uses data mining techniques to generate attributes constituting an MD. Using a Restaurant database, we will show how the crowders can help to generate MDs by labelling the training sample through the CrowdMD user interface and how MDs can be mined from this training set.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2672144",
                    "name": "Asma Abboura"
                },
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                },
                {
                    "authorId": "1803906",
                    "name": "Mourad Ouziri"
                },
                {
                    "authorId": "1795027",
                    "name": "S. Benbernou"
                }
            ]
        },
        {
            "paperId": "ed96c4d38d405770352b471fba991fb4b26e6d41",
            "title": "Indexing Uncertain Categorical Data over Distributed Environment",
            "abstract": "Today, a large amount of uncertain data is produced by several applications where the management systems of traditional databases incuding indexing methods are not suitable to handle such type of data. In this paper, we propose an inverted based index method for effciently searching uncertain categorical data over distributed environments. We adress two kinds of query over the distributed uncertain databases, one a distributed probabilistic thresholds query, where all results sastisfying the query with probablities that meet a probablistic threshold requirement are returned, and another a distributed top k-queries, where all results optimizing the transfer of the tuples and the time treatment are returned.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3058544",
                    "name": "A. Benaissa"
                },
                {
                    "authorId": "1795027",
                    "name": "S. Benbernou"
                },
                {
                    "authorId": "1803906",
                    "name": "Mourad Ouziri"
                },
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                }
            ]
        },
        {
            "paperId": "b2e66eca4bf327feff5fa2d416094c84def8336a",
            "title": "Design Issues of Shingled Write Disk for Database",
            "abstract": "To maintain the continuing growth of bit density in magnetic recording media, the disk industry will have to change technologies. Shingled write disks are expected to be the next generation of high capacity magnetic disks and alreadyin prototype. Shingled write technology is not disruptive at the level of disk design and manufacturing, but as shingled writes prevent updates in place, the technology is disruptive at the level of usage. It is possible to design a disk device driver or disk firmware that allows a shingled write disk to be used as a drop in replacement for traditional disks. Database implementations however have traditionally bypassed the file system and accessed the disk directly in order to achieve better performance. We discuss here adaptation of B+-trees and linear hash tables to shingled write disk to support indexed database tables and secondary indices. Our proposal is based on dividing the disk in low-capacity Random Access Zones (RAZ) and high capacity Log Access Zones (LAZ). The LAZ use the shingled disk effectively while RAZ places guard bands around each track in the zone in order to regain the capacity of in-place updates at the costs of loosing capacity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2575242",
                    "name": "S. Sahri"
                },
                {
                    "authorId": "31407761",
                    "name": "T. Schwarz"
                }
            ]
        }
    ]
}