{
    "authorId": "24895235",
    "papers": [
        {
            "paperId": "03d131aaf8a933c0c1fdc42530210449958b5036",
            "title": "Semantic Consistency for Assuring Reliability of Large Language Models",
            "abstract": "Large Language Models (LLMs) exhibit remarkable fluency and competence across various natural language tasks. However, recent research has highlighted their sensitivity to variations in input prompts. To deploy LLMs in a safe and reliable manner, it is crucial for their outputs to be consistent when prompted with expressions that carry the same meaning or intent. While some existing work has explored how state-of-the-art LLMs address this issue, their evaluations have been confined to assessing lexical equality of single- or multi-word answers, overlooking the consistency of generative text sequences. For a more comprehensive understanding of the consistency of LLMs in open-ended text generation scenarios, we introduce a general measure of semantic consistency, and formulate multiple versions of this metric to evaluate the performance of various LLMs. Our proposal demonstrates significantly higher consistency and stronger correlation with human evaluations of output consistency than traditional metrics based on lexical consistency. Finally, we propose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance semantic consistency. When evaluated for closed-book question answering based on answer variations from the TruthfulQA benchmark, A2C increases accuracy metrics for pretrained and finetuned LLMs by up to 47%, and semantic consistency metrics for instruction-tuned models by up to 7-fold.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056998774",
                    "name": "Harsh Raj"
                },
                {
                    "authorId": "2110652561",
                    "name": "Vipul Gupta"
                },
                {
                    "authorId": "24895235",
                    "name": "Domenic Rosati"
                },
                {
                    "authorId": "152632230",
                    "name": "S. Majumdar"
                }
            ]
        },
        {
            "paperId": "357613ea0e90bd41fb942fd65f39498e71e2dbc3",
            "title": "Mixture of Soft Prompts for Controllable Data Generation",
            "abstract": "Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations. We flip the problem on its head by leveraging the LLM as a tool for data augmentation rather than direct prediction. Our proposed Mixture of Soft Prompts (MSP) serves as a parameter-efficient procedure for generating data in a controlled manner. Denoising mechanisms are further applied to improve the quality of synthesized data. Automatic metrics show our method is capable of producing diverse and natural text, while preserving label semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks when compared against strong baselines. Our method offers an alternate data-centric approach for applying LLMs to complex prediction tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111900571",
                    "name": "Derek Chen"
                },
                {
                    "authorId": "2109420234",
                    "name": "Celine Lee"
                },
                {
                    "authorId": "46215479",
                    "name": "Yunan Lu"
                },
                {
                    "authorId": "24895235",
                    "name": "Domenic Rosati"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                }
            ]
        },
        {
            "paperId": "3e33db112ac76321739bd2096eae800789ae9b2c",
            "title": "Discovering substantive disagreement with review articles?",
            "abstract": "Disagreements help drive science. How does one identify and track them in scholarly literature? We ask the research question will searching review articles (RA) will be more time efficient for this purpose than searching non-review ones (NRA). This is especially so to the extent NRAs exceed RAs in a given field. We also discuss a metric for whether RAs report more substantive disagreements than NRAs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "24895235",
                    "name": "Domenic Rosati"
                },
                {
                    "authorId": "52386669",
                    "name": "Brian Simboli"
                }
            ]
        },
        {
            "paperId": "9e26a2328626a978edab42fea3c606bbaee61837",
            "title": "Background Knowledge Grounding for Readable, Relevant, and Factual Biomedical Lay Summaries",
            "abstract": "Communication of scientific findings to the public is important for keeping non-experts informed of developments such as life-saving medical treatments. However, generating readable lay summaries from scientific documents is challenging, and currently, these summaries suffer from critical factual errors. One popular intervention for improving factuality is using additional external knowledge to provide factual grounding. However, it is unclear how these grounding sources should be retrieved, selected, or integrated, and how supplementary grounding documents might affect the readability or relevance of the generated summaries. We develop a simple method for selecting grounding sources and integrating them with source documents. We then use the BioLaySum summarization dataset to evaluate the effects of different grounding sources on summary quality. We found that grounding source documents improves the relevance and readability of lay summaries but does not improve factuality of lay summaries. This continues to be true in zero-shot summarization settings where we hypothesized that grounding might be even more important for factual lay summaries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "24895235",
                    "name": "Domenic Rosati"
                }
            ]
        },
        {
            "paperId": "9e6e3d796f97c3b6dcc0ef56114ef1416d92b7b7",
            "title": "GRASUM at BioLaySumm Task 1: Background Knowledge Grounding for Readable, Relevant, and Factual Biomedical Lay Summaries",
            "abstract": "Communication of scientific findings to the public is important for keeping non-experts informed of developments such as life-saving medical treatments. However, generating readable lay summaries from scientific documents is challenging, and currently, these summaries suffer from critical factual errors. One popular intervention for improving factuality is using additional external knowledge to provide factual grounding. However, it is unclear how these grounding sources should be retrieved, selected, or integrated, and how supplementary grounding documents might affect the readability or relevance of the generated summaries. We develop a simple method for selecting grounding sources and integrating them with source documents. We then use the BioLaySum summarization dataset to evaluate the effects of different grounding sources on summary quality. We found that grounding source documents improves the relevance and readability of lay summaries but does not improve factuality of lay summaries. This continues to be true in zero-shot summarization settings where we hypothesized that grounding might be even more important for factual lay summaries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "24895235",
                    "name": "Domenic Rosati"
                }
            ]
        },
        {
            "paperId": "dee83ee947ea95f54b40a3c101552b4eb447bbf6",
            "title": "Self-Consistency of Large Language Models under Ambiguity",
            "abstract": "Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency\u2013e.g. question-answering, explanations, etc. Our work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct. We conduct a series of behavioral experiments on the OpenAI model suite using an ambiguous integer sequence completion task. We find that average consistency ranges from 67% to 82%, far higher than would be predicted if a model\u2019s consistency was random, and increases as model capability improves. Furthermore, we show that models tend to maintain self-consistency across a series of robustness checks, including prompting speaker changes and sequence length changes. These results suggest that self-consistency arises as an emergent capability without specifically training for it. Despite this, we find that models are uncalibrated when judging their own consistency, with models displaying both over- and under-confidence. We also propose a nonparametric test for determining from token output distribution whether a model assigns non-trivial probability to alternative answers. Using this test, we find that despite increases in self-consistency, models usually place significant weight on alternative, inconsistent answers. This distribution of probability mass provides evidence that even highly self-consistent models internally compute multiple possible responses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261082130",
                    "name": "Henning Bartsch"
                },
                {
                    "authorId": "2261081951",
                    "name": "Ole Jorgensen"
                },
                {
                    "authorId": "24895235",
                    "name": "Domenic Rosati"
                },
                {
                    "authorId": "2261081835",
                    "name": "Jason Hoelscher-Obermaier"
                },
                {
                    "authorId": "2261082452",
                    "name": "Jacob Pfau"
                }
            ]
        },
        {
            "paperId": "48ce4398a2884a7b8848f09d6916015051e56a16",
            "title": "Using contradictions improves question answering systems",
            "abstract": "This work examines the use of contradiction in natural language inference (NLI) for question answering (QA). Typically, NLI systems help answer questions by determining if a potential answer is entailed (supported) by some background context. But is it useful to also determine if an answer contradicts the context? We test this in two settings, multiple choice and extractive QA, and find that systems that incorporate contradiction can do slightly better than entailment-only systems on certain datasets. However, the best performances come from using contradiction, entailment, and QA model confidence scores together. This has implications for the deployment of QA systems in domains such as medicine and science where safety is an issue.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218916797",
                    "name": "'Etienne Fortier-Dubois"
                },
                {
                    "authorId": "24895235",
                    "name": "Domenic Rosati"
                }
            ]
        },
        {
            "paperId": "5b7356936460c8f544a5e953f4409fc349c6a174",
            "title": "SynSciPass: detecting appropriate uses of scientific text generation",
            "abstract": "Approaches to machine generated text detection tend to focus on binary classification of human versus machine written text. In the scientific domain where publishers might use these models to examine manuscripts under submission, misclassification has the potential to cause harm to authors. Additionally, authors may appropriately use text generation models such as with the use of assistive technologies like translation tools. In this setting, a binary classification scheme might be used to flag appropriate uses of assistive text generation technology as simply machine generated which is a cause of concern. In our work, we simulate this scenario by presenting a state-of-the-art detector trained on the DAGPap22 with machine translated passages from Scielo and find that the model performs at random. Given this finding, we develop a framework for dataset development that provides a nuanced approach to detecting machine generated text by having labels for the type of technology used such as for translation or paraphrase resulting in the construction of SynSciPass. By training the same model that performed well on DAGPap22 on SynSciPass, we show that not only is the model more robust to domain shifts but also is able to uncover the type of technology used for machine generated text. Despite this, we conclude that current datasets are neither comprehensive nor realistic enough to understand how these models would perform in the wild where manuscript submissions can come from many unknown or novel distributions, how they would perform on scientific full-texts rather than small passages, and what might happen when there is a mix of appropriate and inappropriate uses of natural language generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "24895235",
                    "name": "Domenic Rosati"
                }
            ]
        },
        {
            "paperId": "622b1978aa46a19b58fa40faa7c8e60709287515",
            "title": "Using contradictions to improve QA systems",
            "abstract": "Ensuring the safety of question answering (QA) systems is critical for deploying them in biomedical and scienti\ufb01c domains. One approach to improving these systems uses nat-ural language inference (NLI) to determine whether answers are supported, or entailed, by some background context. However, these systems are vulnerable to supporting an answer with a source that is wrong or misleading. Our work proposes a critical approach by selecting answers based on whether they have been contradicted by some background context. We evaluate this system on multiple choice and extractive QA and \ufb01nd that while the contradiction-based systems are competitive with and often better than entailment-only systems, models that incorporate contra-diction, entailment, and QA model con\ufb01dence scores together are the best. Based on this result, we explore unique opportunities for leveraging contradiction-based approaches such for improving interpretability and selecting better answers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "24895235",
                    "name": "Domenic Rosati"
                }
            ]
        },
        {
            "paperId": "7c3e240732b0232577f781240de4e60777c5c054",
            "title": "Moving beyond word lists: towards abstractive topic labels for human-like topics of scientific documents",
            "abstract": "Topic models represent groups of documents as a list of words (the topic labels). This work asks whether an alternative approach to topic labeling can be developed that is closer to a natural language description of a topic than a word list. To this end, we present an approach to generating human-like topic labels using abstractive multi-document summarization (MDS). We investigate our approach with an exploratory case study. We model topics in citation sentences in order to understand what further research needs to be done to fully operationalize MDS for topic labeling. Our case study shows that in addition to more human-like topics there are additional advantages to evaluation by using clustering and summarization measures instead of topic model measures. However, we find that there are several developments needed before we can design a well-powered study to evaluate MDS for topic modeling fully. Namely, improving cluster cohesion, improving the factuality and faithfulness of MDS, and increasing the number of documents that might be supported by MDS. We present a number of ideas on how these can be tackled and conclude with some thoughts on how topic modeling can also be used to improve MDS in general.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "24895235",
                    "name": "Domenic Rosati"
                }
            ]
        }
    ]
}