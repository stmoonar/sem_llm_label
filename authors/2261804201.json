{
    "authorId": "2261804201",
    "papers": [
        {
            "paperId": "3dea23e10eeff848f7352b17bbc1fdce38112acc",
            "title": "Knowledge Graph-Enhanced Large Language Models via Path Selection",
            "abstract": "Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems. Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261678191",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "2117075272",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2261804201",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "123918726",
                    "name": "Yushun Dong"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                }
            ]
        },
        {
            "paperId": "44c5c804442b635a745390d7d17b1ecb5e3ca89a",
            "title": "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era",
            "abstract": "Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency. This extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the\"black box\"to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems, and (2) how LLMs can contribute to the advancement of XAI. We introduce 10 strategies, introducing the key techniques for each and discussing their associated challenges. We also provide case studies to demonstrate how to obtain and leverage explanations. The code used in this paper can be found at: https://github.com/JacksonWuxs/UsableXAI_LLM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145346360",
                    "name": "Xuansheng Wu"
                },
                {
                    "authorId": "2237987232",
                    "name": "Haiyan Zhao"
                },
                {
                    "authorId": "2261804201",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "2249001715",
                    "name": "Yucheng Shi"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2291322576",
                    "name": "Tianming Liu"
                },
                {
                    "authorId": "2262445470",
                    "name": "Xiaoming Zhai"
                },
                {
                    "authorId": "2291141500",
                    "name": "Wenlin Yao"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2256183798",
                    "name": "Ninghao Liu"
                }
            ]
        },
        {
            "paperId": "827b1fcc89920d5e8efa75f513355508596d2112",
            "title": "Understanding and Modeling Job Marketplace with Pretrained Language Models",
            "abstract": "Job marketplace is a heterogeneous graph composed of interactions among members (job-seekers), companies, and jobs. Understanding and modeling job marketplace can benefit both job seekers and employers, ultimately contributing to the greater good of the society. However, existing graph neural network (GNN)-based methods have shallow understandings of the associated textual features and heterogeneous relations. To address the above challenges, we propose PLM4Job, a job marketplace foundation model that tightly couples pretrained language models (PLM) with job market graph, aiming to fully utilize the pretrained knowledge and reasoning ability to model member/job textual features as well as various member-job relations simultaneously. In the pretraining phase, we propose a heterogeneous ego-graph-based prompting strategy to model and aggregate member/job textual features based on the topological structure around the target member/job node, where entity type embeddings and graph positional embeddings are introduced accordingly to model different entities and their heterogeneous relations. Meanwhile, a proximity-aware attention alignment strategy is designed to dynamically adjust the attention of the PLM on ego-graph node tokens in the prompt, such that the attention can be better aligned with job marketplace semantics. Extensive experiments at LinkedIn demonstrate the effectiveness of PLM4Job.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261804201",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "2264480350",
                    "name": "Liang Wu"
                },
                {
                    "authorId": "2134483590",
                    "name": "Binchi Zhang"
                },
                {
                    "authorId": "2117075272",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2170992709",
                    "name": "Qilnli Guo"
                },
                {
                    "authorId": "2264620213",
                    "name": "Liangjie Hong"
                },
                {
                    "authorId": "2284861972",
                    "name": "Luke Simon"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                }
            ]
        },
        {
            "paperId": "94a3653fa6f468daf8e4a85f90525e76921b583b",
            "title": "Causal Inference with Latent Variables: Recent Advances and Future Prospectives",
            "abstract": "Causality lays the foundation for the trajectory of our world. Causal inference (CI), which aims to infer intrinsic causal relations among variables of interest, has emerged as a crucial research topic. Nevertheless, the lack of observation of important variables (e.g., confounders, mediators, exogenous variables, etc.) severely compromises the reliability of CI methods. The issue may arise from the inherent difficulty in measuring the variables. Additionally, in observational studies where variables are passively recorded, certain covariates might be inadvertently omitted by the experimenter. Depending on the type of unobserved variables and the specific CI task, various consequences can be incurred if these latent variables are carelessly handled, such as biased estimation of causal effects, incomplete understanding of causal mechanisms, lack of individual-level causal consideration, etc. In this survey, we provide a comprehensive review of recent developments in CI with latent variables. We start by discussing traditional CI techniques when variables of interest are assumed to be fully observed. Afterward, under the taxonomy of circumvention and inference-based methods, we provide an in-depth discussion of various CI strategies to handle latent variables, covering the tasks of causal effect estimation, mediation analysis, counterfactual reasoning, and causal discovery. Furthermore, we generalize the discussion to graph data where interference among units may exist. Finally, we offer fresh aspects for further advancement of CI with latent variables, especially new opportunities in the era of large language models (LLMs).",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2261804201",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "2302556971",
                    "name": "Yinhan He"
                },
                {
                    "authorId": "2157405959",
                    "name": "Jing Ma"
                },
                {
                    "authorId": "2215174877",
                    "name": "Mengxuan Hu"
                },
                {
                    "authorId": "2293760697",
                    "name": "Sheng Li"
                },
                {
                    "authorId": "2303431466",
                    "name": "Jundong Li"
                }
            ]
        },
        {
            "paperId": "bf6f865337faf618bc2be057d6c8e97972e9b520",
            "title": "Consistency and Discrepancy-Based Contrastive Tripartite Graph Learning for Recommendations",
            "abstract": "Tripartite graph-based recommender systems markedly diverge from traditional models by recommending unique combinations such as user groups and item bundles. Despite their effectiveness, these systems exacerbate the longstanding cold-start problem in traditional recommender systems, because any number of user groups or item bundles can be formed among users or items. To address this issue, we introduce a Consistency and Discrepancy-based graph contrastive learning method for tripartite graph-based Recommendation. This approach leverages two novel meta-path-based metrics consistency and discrepancy to capture nuanced, implicit associations between the recommended objects and the recommendees. These metrics, indicative of high-order similarities, can be efficiently calculated with infinite graph convolutional networks layers under a multi-objective optimization framework, using the limit theory of GCN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188777629",
                    "name": "Linxin Guo"
                },
                {
                    "authorId": "2261804201",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "2299334688",
                    "name": "Min Gao"
                },
                {
                    "authorId": "2158332892",
                    "name": "Yinghui Tao"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                },
                {
                    "authorId": "2310713515",
                    "name": "Chen Chen"
                }
            ]
        },
        {
            "paperId": "42016f91e5b1da63174d45acb96bc89b64aa124d",
            "title": "Knowledge Editing for Large Language Models: A Survey",
            "abstract": "\n Large Language Models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently,\n Knowledge-based Model Editing\n (KME), also known as\n Knowledge Editing\n or\n Model Editing\n , has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim to provide a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117075272",
                    "name": "Song Wang"
                },
                {
                    "authorId": "2261804201",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "2261678191",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "2262086932",
                    "name": "Zaiyi Zheng"
                },
                {
                    "authorId": "2127380428",
                    "name": "Chen Chen"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                }
            ]
        },
        {
            "paperId": "6531e6b6b8e43901a804fe3f03dd941c4e781718",
            "title": "Collaborative Large Language Model for Recommender Systems",
            "abstract": "Recently, there has been growing interest in developing the next-generation recommender systems (RSs) based on pretrained large language models (LLMs). However, the semantic gap between natural language and recommendation tasks is still not well addressed, leading to multiple issues such as spuriously correlated user/item descriptors, ineffective language modeling on user/item data, inefficient recommendations via auto-regression, etc. In this paper, we propose CLLM4Rec, the first generative RS that tightly integrates the LLM paradigm and ID paradigm of RSs, aiming to address the above challenges simultaneously. We first extend the vocabulary of pretrained LLMs with user/item ID tokens to faithfully model user/item collaborative and content semantics. Accordingly, a novel soft+hard prompting strategy is proposed to effectively learn user/item collaborative/content token embeddings via language modeling on RS-specific corpora, where each document is split into a prompt consisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and a main text consisting of homogeneous item tokens or vocab tokens to facilitate stable and effective language modeling. In addition, a novel mutual regularization strategy is introduced to encourage CLLM4Rec to capture recommendation-related information from noisy user/item content. Finally, we propose a novel recommendation-oriented finetuning strategy for CLLM4Rec, where an item prediction head with multinomial likelihood is added to the pretrained CLLM4Rec backbone to predict hold-out items based on soft+hard prompts established from masked user-item interaction history, where recommendations of multiple items can be generated efficiently without hallucination.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261804201",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "2264480350",
                    "name": "Liang Wu"
                },
                {
                    "authorId": "2170992709",
                    "name": "Qilnli Guo"
                },
                {
                    "authorId": "2264620213",
                    "name": "Liangjie Hong"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                }
            ]
        }
    ]
}