{
    "authorId": "145116511",
    "papers": [
        {
            "paperId": "b7ce596828e6e64aa01c57fc516f05ad5d625c13",
            "title": "Improving Quantification with Minimal In-Domain Annotations: Beyond Classify and Count",
            "abstract": "Quantification is the task of estimating the class distribution in a given collection. With the growing availability of classification models, the use of classifiers for quantification has become increasingly popular, carrying the promise of eliminating the need for manual annotation. However, the naive classify and count approach presents clear limitations, especially evident in the face of domain discrepancies. In this work, we introduce two novel quantification methods, called CPCC and BCC, which can adapt to new target datasets with a small number of annotated in-domain samples (N = 100). To explore their real-world applicability, we apply our methods to a range of quantification tasks in the realm of hateful and offensive language, where they perform markedly better than classify and count and other existing methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1737826518",
                    "name": "P. von D\u00e4niken"
                },
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "2304215580",
                    "name": "Alvaro Rodrigo"
                },
                {
                    "authorId": "2648584",
                    "name": "Mark Cieliebak"
                }
            ]
        },
        {
            "paperId": "c3dce1ec33526adcae120a9555c1dd9a34358c8a",
            "title": "Favi-Score: A Measure for Favoritism in Automated Preference Ratings for Generative AI Evaluation",
            "abstract": "Generative AI systems have become ubiquitous for all kinds of modalities, which makes the issue of the evaluation of such models more pressing. One popular approach is preference ratings, where the generated outputs of different systems are shown to evaluators who choose their preferences. In recent years the field shifted towards the development of automated (trained) metrics to assess generated outputs, which can be used to create preference ratings automatically. In this work, we investigate the evaluation of the metrics themselves, which currently rely on measuring the correlation to human judgments or computing sign accuracy scores. These measures only assess how well the metric agrees with the human ratings. However, our research shows that this does not tell the whole story. Most metrics exhibit a disagreement with human system assessments which is often skewed in favor of particular text generation systems, exposing a degree of favoritism in automated metrics. This paper introduces a formal definition of favoritism in preference metrics, and derives the Favi-Score, which measures this phenomenon. In particular we show that favoritism is strongly related to errors in final system rankings. Thus, we propose that preference-based metrics ought to be evaluated on both sign accuracy scores and favoritism.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1986596516",
                    "name": "Pius von Daniken"
                },
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "2470429",
                    "name": "Don Tuggener"
                },
                {
                    "authorId": "2648584",
                    "name": "Mark Cieliebak"
                }
            ]
        },
        {
            "paperId": "ef19aae24031c88e43d9358401a280f23c9b53df",
            "title": "Error-preserving Automatic Speech Recognition of Young English Learners' Language",
            "abstract": "One of the central skills that language learners need to practice is speaking the language. Currently, students in school do not get enough speaking opportunities and lack conversational practice. Recent advances in speech technology and natural language processing allow for the creation of novel tools to practice their speaking skills. In this work, we tackle the first component of such a pipeline, namely, the automated speech recognition module (ASR), which faces a number of challenges: first, state-of-the-art ASR models are often trained on adult read-aloud data by native speakers and do not transfer well to young language learners' speech. Second, most ASR systems contain a powerful language model, which smooths out errors made by the speakers. To give corrective feedback, which is a crucial part of language learning, the ASR systems in our setting need to preserve the errors made by the language learners. In this work, we build an ASR system that satisfies these requirements: it works on spontaneous speech by young language learners and preserves their errors. For this, we collected a corpus containing around 85 hours of English audio spoken by learners in Switzerland from grades 4 to 6 on different language learning tasks, which we used to train an ASR model. Our experiments show that our model benefits from direct fine-tuning on children's voices and has a much higher error preservation rate than other models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2180203598",
                    "name": "Janick Michot"
                },
                {
                    "authorId": "2165661802",
                    "name": "Manuela Hurlimann"
                },
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "2304746145",
                    "name": "Luzia Sauer"
                },
                {
                    "authorId": "1637256642",
                    "name": "Katsiaryna Mlynchyk"
                },
                {
                    "authorId": "2648584",
                    "name": "Mark Cieliebak"
                }
            ]
        },
        {
            "paperId": "2381a77dcee05c363cc04a1988f4e46ad51db2d4",
            "title": "Dialect Transfer for Swiss German Speech Translation",
            "abstract": "This paper investigates the challenges in building Swiss German speech translation systems, specifically focusing on the impact of dialect diversity and differences between Swiss German and Standard German. Swiss German is a spoken language with no formal writing system, it comprises many diverse dialects and is a low-resource language with only around 5 million speakers. The study is guided by two key research questions: how does the inclusion and exclusion of dialects during the training of speech translation models for Swiss German impact the performance on specific dialects, and how do the differences between Swiss German and Standard German impact the performance of the systems? We show that dialect diversity and linguistic differences pose significant challenges to Swiss German speech translation, which is in line with linguistic hypotheses derived from empirical investigations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218119693",
                    "name": "Claudio Paonessa"
                },
                {
                    "authorId": "1565016196",
                    "name": "Yanick Schraner"
                },
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "2165661802",
                    "name": "Manuela Hurlimann"
                },
                {
                    "authorId": "83887111",
                    "name": "Manfred Vogel"
                },
                {
                    "authorId": "2648584",
                    "name": "Mark Cieliebak"
                }
            ]
        },
        {
            "paperId": "38c17de46df1eff62e1cb643ad97e299a6e3545c",
            "title": "Correction of Errors in Preference Ratings from Automated Metrics for Text Generation",
            "abstract": "A major challenge in the field of Text Generation is evaluation: Human evaluations are cost-intensive, and automated metrics often display considerable disagreement with human judgments. In this paper, we propose a statistical model of Text Generation evaluation that accounts for the error-proneness of automated metrics when used to generate preference rankings between system outputs. We show that existing automated metrics are generally over-confident in assigning significant differences between systems in this setting. However, our model enables an efficient combination of human and automated ratings to remedy the error-proneness of the automated metrics. We show that using this combination, we only require about 50% of the human annotations typically used in evaluations to arrive at robust and statistically significant results while yielding the same evaluation outcome as the pure human evaluation in 95% of cases. We showcase the benefits of approach for three text generation tasks: dialogue systems, machine translation, and text summarization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "1986596516",
                    "name": "Pius von Daniken"
                },
                {
                    "authorId": "2470429",
                    "name": "Don Tuggener"
                },
                {
                    "authorId": "2648584",
                    "name": "Mark Cieliebak"
                }
            ]
        },
        {
            "paperId": "7331e529754d2e5049ca440c7d674e04e261d79f",
            "title": "STT4SG-350: A Speech Corpus for All Swiss German Dialect Regions",
            "abstract": "We present STT4SG-350, a corpus of Swiss German speech, annotated with Standard German text at the sentence level. The data is collected using a web app in which the speakers are shown Standard German sentences, which they translate to Swiss German and record. We make the corpus publicly available. It contains 343 hours of speech from all dialect regions and is the largest public speech corpus for Swiss German to date. Application areas include automatic speech recognition (ASR), text-to-speech, dialect identification, and speaker recognition. Dialect information, age group, and gender of the 316 speakers are provided. Genders are equally represented and the corpus includes speakers of all ages. Roughly the same amount of speech is provided per dialect region, which makes the corpus ideally suited for experiments with speech technology for different dialects. We provide training, validation, and test splits of the data. The test set consists of the same spoken sentences for each dialect region and allows a fair evaluation of the quality of speech technologies in different dialects. We train an ASR model on the training set and achieve an average BLEU score of 74.7 on the test set. The model beats the best published BLEU scores on 2 other Swiss German ASR test sets, demonstrating the quality of the corpus.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "24993958",
                    "name": "Michel Pl\u00fcss"
                },
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "1565016196",
                    "name": "Yanick Schraner"
                },
                {
                    "authorId": "2218119693",
                    "name": "Claudio Paonessa"
                },
                {
                    "authorId": "2060135722",
                    "name": "Julia Hartmann"
                },
                {
                    "authorId": "2073228880",
                    "name": "Larissa Schmidt"
                },
                {
                    "authorId": "2081372700",
                    "name": "C. Scheller"
                },
                {
                    "authorId": "2165661802",
                    "name": "Manuela Hurlimann"
                },
                {
                    "authorId": "2219309201",
                    "name": "Tanja Samardvzi'c"
                },
                {
                    "authorId": "83887111",
                    "name": "Manfred Vogel"
                },
                {
                    "authorId": "2648584",
                    "name": "Mark Cieliebak"
                }
            ]
        },
        {
            "paperId": "80044e9018831e56bcc6b9630645ddc938cefca6",
            "title": "Text-to-Speech Pipeline for Swiss German - A comparison",
            "abstract": "In this work, we studied the synthesis of Swiss German speech using different Text-to-Speech (TTS) models. We evaluated the TTS models on three corpora, and we found, that VITS models performed best, hence, using them for further testing. We also introduce a new method to evaluate TTS models by letting the discriminator of a trained vocoder GAN model predict whether a given waveform is human or synthesized. In summary, our best model delivers speech synthesis for different Swiss German dialects with previously unachieved quality.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2219299518",
                    "name": "Tobias Bollinger"
                },
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "83887111",
                    "name": "Manfred Vogel"
                }
            ]
        },
        {
            "paperId": "a9c058948da555a04c7a50f6827ad812fdf9f0a6",
            "title": "From Concept to Implementation: The Data-Centric Development Process for AI in Industry",
            "abstract": "We examine the paradigm of data-centric artificial intelligence (DCAI) as a solution to the obstacles that small and medium-sized enterprises (SMEs) face in adopting AI. While the prevalent model-centric approach emphasizes collecting large amounts of data, SMEs often suffer from small datasets, data drift, and sparse ML knowledge, which hinders them from implementing AI. DCAI, on the other hand, emphasizes to systematically engineer the data used to build an AI system. Our contribution is to provide a concrete, transferable implementation of a DCAI development process geared towards industrial application, specffically in machining and manufacturing, and demonstrate how it enhances data quality by fostering collaboration between domain experts and ML engineers. This added value can place AI at the disposal of more SMEs. We provide the necessary background for practitioners to follow the rationale behind DCAI and successfully deploy the provided process template.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2226522002",
                    "name": "Paul-Philipp Luley"
                },
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "2226523431",
                    "name": "Peng Yan"
                },
                {
                    "authorId": "2223114064",
                    "name": "Gerrit A. Schatte"
                },
                {
                    "authorId": "2793787",
                    "name": "Thilo Stadelmann"
                }
            ]
        },
        {
            "paperId": "e7a4e7b349b70b1681978cd57a59a6e1684b219f",
            "title": "ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural Language to SQL Systems",
            "abstract": "Natural Language to SQL systems (NL-to-SQL) have recently shown improved accuracy (exceeding 80%) for natural language to SQL query translation due to the emergence of transformer-based language models, and the popularity of the Spider benchmark. However, Spider mainly contains simple databases with few tables, columns, and entries, which do not reflect a realistic setting. Moreover, complex real-world databases with domain-specific content have little to no training data available in the form of NL/SQL-pairs leading to poor performance of existing NL-to-SQL systems.\n \n In this paper, we introduce\n ScienceBenchmark\n , a new complex NL-to-SQL benchmark for three real-world, highly domain-specific databases. For this new benchmark, SQL experts and domain experts created high-quality NL/SQL-pairs for each domain. To garner more data, we extended the small amount of human-generated data with synthetic data generated using GPT-3. We show that our benchmark is highly challenging, as the top performing systems on Spider achieve a very low performance on our benchmark. Thus, the challenge is many-fold: creating NL-to-SQL systems for highly complex domains with a small amount of hand-made training data augmented with synthetic data. To our knowledge,\n ScienceBenchmark\n is the first NL-to-SQL benchmark designed with complex real-world scientific databases, containing challenging training and test data carefully validated by domain experts.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46867002",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "2055401797",
                    "name": "George Katsogiannis-Meimarakis"
                },
                {
                    "authorId": "2072251887",
                    "name": "Catherine Kosten"
                },
                {
                    "authorId": "1680709",
                    "name": "Georgia Koutrika"
                },
                {
                    "authorId": "2113917675",
                    "name": "Kurt Stockinger"
                }
            ]
        },
        {
            "paperId": "00b1c039c3233ae5803803e4acb276f9242508a8",
            "title": "Improving NL-to-Query Systems through Re-ranking of Semantic Hypothesis",
            "abstract": "Natural Language-to-Query systems translate a natural language question into a formal query language such as SQL. Typically the translation results in a set of candidate query statements due to the ambiguity of natural language. Hence, an important aspect of NL-to-Query systems is to rank the query statements so that the most relevant query is ranked on top. We pro-pose a novel approach to significantly improve the query ranking and thus the accuracy of such systems . First, we use existing methods to translate the natural language question ( NL in ) into k query statements and rank them. Then we translate each of the k query statements back into a natural language question ( NL gen ) and use the semantic similarity between the original question NL in and each of the k generated questions NL gen to re-rank the output . Our experiments on two standard datasets, OTTA and Spider, show that this technique improves even strong state-of-the-art NL-to-Query systems by up to 9 percentage points. A detailed error analysis shows that our method correctly down-ranks queries with missing relations and wrong query types. While this work is focused on NL-to-Query, our method could be applied to any other semantic parsing problems as long as a text generation method is available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1737826518",
                    "name": "P. von D\u00e4niken"
                },
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "1733049",
                    "name": "Eneko Agirre"
                },
                {
                    "authorId": "151240818",
                    "name": "Ursin Brunner"
                },
                {
                    "authorId": "2648584",
                    "name": "Mark Cieliebak"
                },
                {
                    "authorId": "2113917675",
                    "name": "Kurt Stockinger"
                }
            ]
        }
    ]
}