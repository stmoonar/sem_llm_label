{
    "authorId": "2270077823",
    "papers": [
        {
            "paperId": "dd8a811c1e5cc033849ea2dcdc648af48bb82bef",
            "title": "Online and Offline Evaluation in Search Clarification",
            "abstract": "The effectiveness of clarification question models in engaging users within search systems is currently constrained, casting doubt on their overall usefulness. To improve the performance of these models, it is crucial to employ assessment approaches that encompass both real-time feedback from users (online evaluation) and the characteristics of clarification questions evaluated through human assessment (offline evaluation). However, the relationship between online and offline evaluations has been debated in information retrieval. This study aims to investigate how this discordance holds in search clarification. We use user engagement as ground truth and employ several offline labels to investigate to what extent the offline ranked lists of clarification resemble the ideal ranked lists based on online user engagement. Contrary to the current understanding that offline evaluations fall short of supporting online evaluations, we indicate that when identifying the most engaging clarification questions from the user\u2019s perspective, online and offline evaluations correspond with each other. We show that the query length does not influence the relationship between online and offline evaluations, and reducing uncertainty in online evaluation strengthens this relationship. We illustrate that an engaging clarification needs to excel from multiple perspectives, and SERP quality and characteristics of the clarification are equally important. We also investigate if human labels can enhance the performance of Large Language Models (LLMs) and Learning-to-Rank (LTR) models in identifying the most engaging clarification questions from the user\u2019s perspective by incorporating offline evaluations as input features. Our results indicate that Learning-to-Rank models do not perform better than individual offline labels. However, GPT, an LLM, emerges as the standout performer, surpassing all Learning-to-Rank models and offline labels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2104141869",
                    "name": "Leila Tavakoli"
                },
                {
                    "authorId": "2528063",
                    "name": "Johanne R. Trippas"
                },
                {
                    "authorId": "2291137161",
                    "name": "Hamed Zamani"
                },
                {
                    "authorId": "2256152195",
                    "name": "Falk Scholer"
                },
                {
                    "authorId": "2270077823",
                    "name": "Mark Sanderson"
                }
            ]
        },
        {
            "paperId": "d442aa9dfa507f8b8079eac3cf46623cfcf87046",
            "title": "A Practical Guide for the Effective Evaluation of Twitter User Geolocation",
            "abstract": "Geolocating Twitter users\u2014the task of identifying their home locations\u2014serves a wide range of community and business applications such as managing natural crises, journalism, and public health. Many approaches have been proposed for automatically geolocating users based on their tweets; at the same time, various evaluation metrics have been proposed to measure the effectiveness of these approaches, making it challenging to understand which of these metrics is the most suitable for this task. In this article, we propose a guide for a standardized evaluation of Twitter user geolocation by analyzing 15 models and two baselines in a controlled experimental setting. Models are evaluated using 10 metrics over four geographic granularities. We use rank correlations to assess the effectiveness of these metrics. Our results demonstrate that the choice of effectiveness metric can have a substantial impact on the conclusions drawn from a geolocation system experiment, potentially leading experimenters to contradictory results about relative effectiveness. We show that for general evaluations, a range of performance metrics should be reported, to ensure that a complete picture of system effectiveness is conveyed. Given the global geographic coverage of this task, we specifically recommend evaluation at micro vs. macro levels to measure the impact of the bias in distribution over locations. Although a lot of complex geolocation algorithms have been applied in recent years, a majority class baseline is still competitive at coarse geographic granularity. We propose a suite of statistical analysis tests, based on the employed metric, to ensure that the results are not coincidental.1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261417902",
                    "name": "Ahmed Mourad"
                },
                {
                    "authorId": "1732541",
                    "name": "Falk Scholer"
                },
                {
                    "authorId": "2253603880",
                    "name": "Walid Magdy"
                },
                {
                    "authorId": "2270077823",
                    "name": "Mark Sanderson"
                }
            ]
        },
        {
            "paperId": "2c53a40aef9980ba590bcef32175d2e46a225f39",
            "title": "The troubles with using a logical model of IR on a large collection of documents experimenting retrieval by logical imaging on TREC",
            "abstract": "The evaluation of an implication by Imaging is a logical technique developed in the framework of Conditional Logics. In 1993 a logical model of IR called Retrieval by logial Imaging was proposed by some of the authors of this paper and tested using some classical IR test collections. In this paper we report on the challenges posed by trying to apply such a model to a large test colection of the size of TREC-B. The problems we found and the way we put together ideas and efforts to solve them are indicative of the troubles one might find in trying to implement and experiment with a complex logical model of IR. We believe our efforts could set an example for other researchers working on logical models of IR to try to implement their models in such a way that they can cope with the size of real life colections, though preserving the formal beauty of their logical models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2247237409",
                    "name": "Fabio Crestani"
                },
                {
                    "authorId": "145191197",
                    "name": "I. Ruthven"
                },
                {
                    "authorId": "2270077823",
                    "name": "Mark Sanderson"
                },
                {
                    "authorId": "1709413",
                    "name": "C. J. Rijsbergen"
                }
            ]
        }
    ]
}