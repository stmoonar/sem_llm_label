{
    "authorId": "2155468861",
    "papers": [
        {
            "paperId": "187bc1a6d97ddd8e5afe1e82221657766af0bc76",
            "title": "Interpretable Knowledge Tracing via Response Influence-based Counterfactual Reasoning",
            "abstract": "Knowledge tracing (KT) plays a crucial role in computer-aided education and intelligent tutoring systems, aiming to assess students' knowledge proficiency by predicting their future performance on new questions based on their past response records. While existing deep learning knowledge tracing (DLKT) methods have significantly improved prediction accuracy and achieved state-of-the-art results, they often suffer from a lack of interpretability. To address this limitation, current approaches have explored incorporating psychological influences to achieve more explainable predictions, but they tend to overlook the potential influences of historical responses. In fact, understanding how models make predictions based on response influences can enhance the transparency and trustworthiness of the knowledge tracing process, presenting an opportunity for a new paradigm of interpretable KT. However, measuring unobservable response influences is challenging. In this paper, we resort to counterfactual reasoning that intervenes in each response to answer what if a student had answered a question incorrectly that he/she actually answered correctly, and vice versa. Based on this, we propose RCKT, a novel response influence-based counterfactual knowledge tracing framework. RCKT generates response influences by comparing prediction outcomes from factual sequences and constructed counterfactual sequences after interventions. Additionally, we introduce maximization and inference techniques to leverage accumulated influences from different past responses, further improving the model's performance and credibility. Extensive experimental results demonstrate that our RCKT method outperforms state-of-the-art knowledge tracing methods on four datasets against six baselines, and provides credible interpretations of response influences. The source code is available at https://github.com/JJCui96IRCKT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2199041878",
                    "name": "Jiajun Cui"
                },
                {
                    "authorId": "2275988637",
                    "name": "Minghe Yu"
                },
                {
                    "authorId": "2275785259",
                    "name": "Bo Jiang"
                },
                {
                    "authorId": "2273936831",
                    "name": "Aimin Zhou"
                },
                {
                    "authorId": "2254333075",
                    "name": "Jianyong Wang"
                },
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                }
            ]
        },
        {
            "paperId": "4d25c5259232c29c836d31b60f6057e176287e83",
            "title": "Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution",
            "abstract": "Embedding plays a key role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision-making models. In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate module, SEvo is able to directly inject graph structural information into embedding with minimal computational overhead during training. The convergence properties of SEvo along with its potential variants are theoretically analyzed to justify the validity of the designs. Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. Particularly SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively utilize graph structural information beyond explicit GNN modules.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254273566",
                    "name": "Cong Xu"
                },
                {
                    "authorId": "2152811374",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2254333075",
                    "name": "Jianyong Wang"
                },
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                }
            ]
        },
        {
            "paperId": "7f7750ce028f6d413bbefdd9db11647c9747cdb2",
            "title": "Learning to Binarize Continuous Features for Neuro-Rule Networks",
            "abstract": "Neuro-Rule Networks (NRNs) emerge as a promising neuro-symbolic method, enjoyed by the ability to equate fully-connected neural networks with logic rules. To support learning logic rules consisting of boolean variables, converting input features into binary representations is required. Different from discrete features that could be directly transformed by one-hot encodings, continuous features need to be binarized based on some numerical intervals. Existing studies usually select the bound values of intervals based on empirical strategies (e.g., equal-width interval). However, it is not optimal since the bounds are fixed and cannot be optimized to accommodate the ultimate training target. In this paper, we propose AutoInt, an approach that automatically binarizes continuous features and enables the intervals to be optimized with NRNs in an end-to-end fashion. Specifically, AutoInt automatically selects an interval for a given continuous feature in a soft manner to enable a differentiable learning procedure of interval-related parameters. Moreover, it introduces an additional soft K-means clustering loss to make the interval centres approach the original feature value distribution, thus reducing the risk of overfitting intervals. We conduct comprehensive experiments on public datasets and demonstrate the effectiveness of AutoInt in boosting the performance of NRNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "1679704",
                    "name": "Y. Liu"
                },
                {
                    "authorId": "2227501773",
                    "name": "Zhuo Wang"
                },
                {
                    "authorId": "2115642141",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "98cf72b7d263e86a41cb7075cd134ff96a885856",
            "title": "Learning Interpretable Rules for Scalable Data Representation and Classification",
            "abstract": "Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. A novel design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Exhaustive experiments on ten small and four large data sets show that RRL outperforms the competitive interpretable approaches and can be easily adjusted to obtain a trade-off between classification accuracy and model complexity for different scenarios.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2261359416",
                    "name": "Zhuo Wang"
                },
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "2152354910",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "2254333075",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "a34a7fb0fa1717f8a10fc9082c2c4da39226d99d",
            "title": "Knowledge-Aware Collaborative Filtering With Pre-Trained Language Model for Personalized Review-Based Rating Prediction",
            "abstract": "Personalized review-based rating prediction aims at leveraging existing reviews to model user interests and item characteristics for rating prediction. Most of the existing studies mainly encounter two issues. First, the rich knowledge contained in the fine-grained aspects of each review and the knowledge graph is rarely considered to complement the pure text for better modeling user-item interactions. Second, the power of pre-trained language models is not carefully studied for personalized review-based rating prediction. To address these issues, we propose an approach named Knowledge-aware Collaborative Filtering with Pre-trained Language Model (KCF-PLM). For the first issue, to utilize rich knowledge, KCF-PLM develops a transformer network to model the interactions of the extracted aspects w.r.t. a user-item pair. For the second issue, to better represent users and items, KCF-PLM takes all the historical reviews of a user or an item as input to pre-trained language models. Moreover, KCF-PLM integrates the transformer network and the pre-trained language models through representation propagation on the knowledge graph and user-item guided attention of the aspect representations. Thus KCF-PLM combines review text, aspect, knowledge graph, and pre-trained language models together for review-based rating prediction. We conduct comprehensive experiments on several public datasets, demonstrating the effectiveness of KCF-PLM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12696819",
                    "name": "Quanxiu Wang"
                },
                {
                    "authorId": "2149215126",
                    "name": "Xinlei Cao"
                },
                {
                    "authorId": "2115642141",
                    "name": "Jianyong Wang"
                },
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                }
            ]
        },
        {
            "paperId": "c7dbd6c80ae941daf1de81439b0d1f992da130ab",
            "title": "Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today",
            "abstract": "Recent investigations show that large language models (LLMs), specifically GPT-4, not only have remarkable capabilities in common Natural Language Processing (NLP) tasks but also exhibit human-level performance on various professional and academic benchmarks. However, whether GPT-4 can be directly used in practical applications and replace traditional artificial intelligence (AI) tools in specialized domains requires further experimental validation. In this paper, we explore the potential of LLMs such as GPT-4 to outperform traditional AI tools in dementia diagnosis. Comprehensive comparisons between GPT-4 and traditional AI tools are conducted to examine their diagnostic accuracy in a clinical setting. Experimental results on two real clinical datasets show that, although LLMs like GPT-4 demonstrate potential for future advancements in dementia diagnosis, they currently do not surpass the performance of traditional AI tools. The interpretability and faithfulness of GPT-4 are also evaluated by comparison with real doctors. We discuss the limitations of GPT-4 in its current state and propose future research directions to enhance GPT-4 in dementia diagnosis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218836424",
                    "name": "Zhuo Wang"
                },
                {
                    "authorId": "2209392929",
                    "name": "R. Li"
                },
                {
                    "authorId": "50660465",
                    "name": "Bowen Dong"
                },
                {
                    "authorId": "31737970",
                    "name": "Jie Wang"
                },
                {
                    "authorId": "2116521868",
                    "name": "Xiuxing Li"
                },
                {
                    "authorId": "2152354910",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "39245374",
                    "name": "C. Mao"
                },
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "5851496",
                    "name": "L. Dong"
                },
                {
                    "authorId": "2115556978",
                    "name": "Jing Gao"
                },
                {
                    "authorId": "2115642141",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "df233ac795d9a5f6908616bc4c884bb82fcab835",
            "title": "Fine-Grained Interaction Modeling with Multi-Relational Transformer for Knowledge Tracing",
            "abstract": "Knowledge tracing, the goal of which is predicting students\u2019 future performance given their past question response sequences to trace their knowledge states, is pivotal for computer-aided education and intelligent tutoring systems. Although many technical efforts have been devoted to modeling students based on their question-response sequences, fine-grained interaction modeling between question-response pairs within each sequence is underexplored. This causes question-response representations less contextualized and further limits student modeling. To address this issue, we first conduct a data analysis and reveal the existence of complex cross effects between different question-response pairs within a sequence. Consequently, we propose MRT-KT, a multi-relational transformer for knowledge tracing, to enable fine-grained interaction modeling between question-response pairs. It introduces a novel relation encoding scheme based on knowledge concepts and student performance. Comprehensive experimental results show that MRT-KT outperforms state-of-the-art knowledge tracing methods on four widely-used datasets, validating the effectiveness of considering fine-grained interaction for knowledge tracing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2199041878",
                    "name": "Jiajun Cui"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "145927131",
                    "name": "Aimin Zhou"
                },
                {
                    "authorId": "2115642141",
                    "name": "Jianyong Wang"
                },
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                }
            ]
        },
        {
            "paperId": "dde3f99f8ca4ed11076d84c4ae2bce1a77de19ac",
            "title": "Effective Few-Shot Named Entity Linking by Meta-Learning",
            "abstract": "Entity linking aims to link ambiguous mentions to their corresponding entities in a knowledge base, which is significant and fundamental for various downstream applications, e.g., knowledge base completion, question answering, and information extraction. While great efforts have been devoted to this task, most of these studies follow the assumption that large-scale labeled data is available. However, when the labeled data is insufficient for specific domains due to labor-intensive annotation work, the performance of existing algorithms will suffer an intolerable decline. In this paper, we endeavor to solve the problem of few-shot entity linking, which only requires a minimal amount of in-domain labeled data and is more practical in real situations. Specifically, we firstly propose a novel weak supervision strategy to generate non-trivial synthetic entity-mention pairs based on mention rewriting. Since the quality of the synthetic data has a critical impact on effective model training, we further design a meta-learning mechanism to assign different weights to each synthetic entity-mention pair automatically. Through this way, we can profoundly exploit rich and precious semantic information to derive a well-trained entity linking model under the few-shot setting. The experiments on real-world datasets show that the proposed method can extensively improve the state-of-the-art few-shot entity linking model and achieve impressive performance when only a small amount of labeled data is available. Moreover, we also demonstrate the outstanding ability of the model's transferability. Our code and models will be open-sourced.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116521868",
                    "name": "Xiuxing Li"
                },
                {
                    "authorId": "2155354022",
                    "name": "Zhenyu Li"
                },
                {
                    "authorId": "2621696",
                    "name": "Zhengyan Zhang"
                },
                {
                    "authorId": "2152354910",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "34825854",
                    "name": "Haitao Yuan"
                },
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": null,
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2115642141",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "18981e60887244d898f00ef60738eaeae1453f76",
            "title": "Graph-Based Tri-Attention Network for Answer Ranking in CQA",
            "abstract": "In community-based question answering (CQA) platforms, automatic answer ranking for a given question is critical for finding potentially popular answers in early times. The mainstream approaches learn to generate answer ranking scores based on the matching degree between question and answer representations as well as the influence of respondents. However, they encounter two main limitations: (1) Correlations between answers in the same question are often overlooked. (2) Question and respondent representations are built independently of specific answers before affecting answer representations. To address the limitations, we devise a novel graph-based tri-attention network, namely GTAN, which has two innovations. First, GTAN proposes to construct a graph for each question and learn answer correlations from each graph through graph neural networks (GNNs). Second, based on the representations learned from GNNs, an alternating tri-attention method is developed to alternatively build target-aware respondent representations, answer-specific question representations, and context-aware answer representations by attention computation. GTAN finally integrates the above representations to generate answer ranking scores. Experiments on three real-world CQA datasets demonstrate GTAN significantly outperforms state-of-the-art answer ranking methods, validating the rationality of the network architecture.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "2113540586",
                    "name": "Chao Dong"
                },
                {
                    "authorId": "2108908267",
                    "name": "Wen Wang"
                },
                {
                    "authorId": "145203884",
                    "name": "H. Zha"
                },
                {
                    "authorId": "2447408",
                    "name": "Jianyong Wang"
                }
            ]
        },
        {
            "paperId": "2b49743fb1c1f0fe9503035f618726c6fd870e03",
            "title": "Learning from Substitutable and Complementary Relations for Graph-based Sequential Product Recommendation",
            "abstract": "Sequential product recommendation, aiming at predicting the products that a target user will interact with soon, has become a hotspot topic. Most of the sequential recommendation models focus on learning from users\u2019 interacted product sequences in a purely data-driven manner. However, they largely overlook the knowledgeable substitutable and complementary relations between products. To address this issue, we propose a novel Substitutable and Complementary Graph-based Sequential Product Recommendation model, namely, SCG-SPRe. The innovations of SCG-SPRe lie in its two main modules: (1) The module of interactive graph neural networks jointly encodes the high-order product correlations in the substitutable graph and the complementary graph into two types of relation-specific product representations. (2) The module of kernel-enhanced transformer networks adaptively fuses multiple temporal kernels to characterize the unique temporal patterns between a candidate product to be recommended and any interacted product in a target behavior sequence. Thanks to the seamless integration of the two modules, SCG-SPRe obtains candidate-dependent user representations for different candidate products to compute the corresponding ranking scores. We conduct extensive experiments on three public datasets, demonstrating SCG-SPRe is superior to competitive sequential recommendation baselines and validating the benefits of explicitly modeling the product-product relations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "2284733603",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "2292243192",
                    "name": "Hongyuan Zha"
                },
                {
                    "authorId": "2254333075",
                    "name": "Jianyong Wang"
                }
            ]
        }
    ]
}