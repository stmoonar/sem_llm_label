{
    "authorId": "1959025244",
    "papers": [
        {
            "paperId": "0e9c1a12eb1eeabdbad261b3cd775dbbd7b4163f",
            "title": "Hierarchical Quantum Control Gates for Functional MRI Understanding",
            "abstract": "Quantum computing has emerged as a powerful tool for solving complex problems intractable for classical computers, particularly in popular fields such as cryptography, optimization, and neurocomputing. In this paper, we present a new quantum-based approach named the Hierarchical Quantum Control Gates (HQCG) method for efficient understanding of Functional Magnetic Resonance Imaging (fMRI) data. This approach includes two novel modules: the Local Quantum Control Gate (LQCG) and the Global Quantum Control Gate (GQCG), which are designed to extract local and global features of fMRI signals, respectively. Our method operates end-to-end on a quantum machine, leveraging quantum mechanics to learn patterns within extremely high-dimensional fMRI signals, such as 30,000 samples which is a challenge for classical computers. Empirical results demonstrate that our approach significantly outperforms classical methods. Additionally, we found that the proposed quantum model is more stable and less prone to overfitting than the classical methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "1959025244",
                    "name": "Xuan-Bac Nguyen"
                },
                {
                    "authorId": "2269131655",
                    "name": "Hoang-Quan Nguyen"
                },
                {
                    "authorId": "2068577467",
                    "name": "Hugh Churchill"
                },
                {
                    "authorId": "2257363043",
                    "name": "Samee U. Khan"
                },
                {
                    "authorId": "2243185830",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "386c4a47f6e034ba92e5411cc32d951b02662004",
            "title": "BRACTIVE: A Brain Activation Approach to Human Visual Brain Learning",
            "abstract": "The human brain is a highly efficient processing unit, and understanding how it works can inspire new algorithms and architectures in machine learning. In this work, we introduce a novel framework named Brain Activation Network (BRACTIVE), a transformer-based approach to studying the human visual brain. The main objective of BRACTIVE is to align the visual features of subjects with corresponding brain representations via fMRI signals. It allows us to identify the brain's Regions of Interest (ROI) of the subjects. Unlike previous brain research methods, which can only identify ROIs for one subject at a time and are limited by the number of subjects, BRACTIVE automatically extends this identification to multiple subjects and ROIs. Our experiments demonstrate that BRACTIVE effectively identifies person-specific regions of interest, such as face and body-selective areas, aligning with neuroscience findings and indicating potential applicability to various object categories. More importantly, we found that leveraging human visual brain activity to guide deep neural networks enhances performance across various benchmarks. It encourages the potential of BRACTIVE in both neuroscience and machine intelligence studies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1959025244",
                    "name": "Xuan-Bac Nguyen"
                },
                {
                    "authorId": "2237130506",
                    "name": "Hojin Jang"
                },
                {
                    "authorId": "2268627059",
                    "name": "Xin Li"
                },
                {
                    "authorId": "2257363043",
                    "name": "Samee U. Khan"
                },
                {
                    "authorId": "2303651350",
                    "name": "Pawan Sinha"
                },
                {
                    "authorId": "2243185830",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "526c4c36e23f416a369dda58d8024810dde49333",
            "title": "Quantum Visual Feature Encoding Revisited",
            "abstract": "Although quantum machine learning has been introduced for a while, its applications in computer vision are still limited. This paper, therefore, revisits the quantum visual encoding strategies, the initial step in quantum machine learning. Investigating the root cause, we uncover that the existing quantum encoding design fails to ensure information preservation of the visual features after the encoding process, thus complicating the learning process of the quantum machine learning models. In particular, the problem, termed the \u201cQuantum Information Gap\u201d (QIG), leads to an information gap between classical and corresponding quantum features. We provide theoretical proof and practical examples with visualization for that found and underscore the significance of QIG, as it directly impacts the performance of quantum machine learning algorithms. To tackle this challenge, we introduce a simple but efficient new loss function named Quantum Information Preserving (QIP) to minimize this gap, resulting in enhanced performance of quantum machine learning algorithms. Extensive experiments validate the effectiveness of our approach, showcasing superior performance compared to current methodologies and consistently achieving state-of-the-art results in quantum modeling.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "1959025244",
                    "name": "Xuan-Bac Nguyen"
                },
                {
                    "authorId": "2269131655",
                    "name": "Hoang-Quan Nguyen"
                },
                {
                    "authorId": "2068577467",
                    "name": "Hugh Churchill"
                },
                {
                    "authorId": "2257363043",
                    "name": "Samee U. Khan"
                },
                {
                    "authorId": "2243185830",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "81e70496774b9bb2a9751bbe5003cd3292472429",
            "title": "Video-Based Autism Detection with Deep Learning",
            "abstract": "Individuals with Autism Spectrum Disorder (ASD) often experience challenges in health, communication, and sen-sory processing; therefore, early diagnosis is necessary for proper treatment and care. In this work, we consider the problem of detecting or classifying ASD children to aid medical professionals in early diagnosis. We develop a deep learning model that analyzes video clips of children reacting to sensory stimuli, with the intent of capturing key differences in reactions and behavior between ASD and non-ASD participants. Unlike many recent studies in ASD classification with MRI data, which require expensive specialized equipment, our method utilizes a powerful but relatively affordable GPU, a standard computer setup, and a video camera for inference. Results show that our model effectively generalizes and understands key differences in the distinct movements of the children. It is noteworthy that our model exhibits successful classification performance despite the limited amount of data for a deep learning problem and limited temporal information available for learning, even with the motion artifacts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2123065865",
                    "name": "Manuel Serna-Aguilera"
                },
                {
                    "authorId": "1959025244",
                    "name": "Xuan-Bac Nguyen"
                },
                {
                    "authorId": "1435211279",
                    "name": "Asmita Singh"
                },
                {
                    "authorId": "2219523941",
                    "name": "Lydia Rockers"
                },
                {
                    "authorId": "2196134324",
                    "name": "Se-Woong Park"
                },
                {
                    "authorId": "2287843307",
                    "name": "Leslie Neely"
                },
                {
                    "authorId": "2159536733",
                    "name": "Han-Seok Seo"
                },
                {
                    "authorId": "2269140591",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "8b916682a704fb4e340c6dd066272e42b733e3d6",
            "title": "A Novel Dataset for Video-Based Autism Classification Leveraging Extra-Stimulatory Behavior",
            "abstract": "Autism Spectrum Disorder (ASD) can affect individuals at varying degrees of intensity, from challenges in overall health, communication, and sensory processing, and this often begins at a young age. Thus, it is critical for medical professionals to be able to accurately diagnose ASD in young children, but doing so is difficult. Deep learning can be responsibly leveraged to improve productivity in addressing this task. The availability of data, however, remains a considerable obstacle. Hence, in this work, we introduce the Video ASD dataset--a dataset that contains video frame convolutional and attention map feature data--to foster further progress in the task of ASD classification. The original videos showcase children reacting to chemo-sensory stimuli, among auditory, touch, and vision This dataset contains the features of the frames spanning 2,467 videos, for a total of approximately 1.4 million frames. Additionally, head pose angles are included to account for head movement noise, as well as full-sentence text labels for the taste and smell videos that describe how the facial expression changes before, immediately after, and long after interaction with the stimuli. In addition to providing features, we also test foundation models on this data to showcase how movement noise affects performance and the need for more data and more complex labels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2123065865",
                    "name": "Manuel Serna-Aguilera"
                },
                {
                    "authorId": "1959025244",
                    "name": "Xuan-Bac Nguyen"
                },
                {
                    "authorId": "2159536733",
                    "name": "Han-Seok Seo"
                },
                {
                    "authorId": "2243185830",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "9c2e0c01c4cb04fa9577843a9179f0731f295d0e",
            "title": "Diffusion-Inspired Quantum Noise Mitigation in Parameterized Quantum Circuits",
            "abstract": "Parameterized Quantum Circuits (PQCs) have been acknowledged as a leading strategy to utilize near-term quantum advantages in multiple problems, including machine learning and combinatorial optimization. When applied to specific tasks, the parameters in the quantum circuits are trained to minimize the target function. Although there have been comprehensive studies to improve the performance of the PQCs on practical tasks, the errors caused by the quantum noise downgrade the performance when running on real quantum computers. In particular, when the quantum state is transformed through multiple quantum circuit layers, the effect of the quantum noise happens cumulatively and becomes closer to the maximally mixed state or complete noise. This paper studies the relationship between the quantum noise and the diffusion model. Then, we propose a novel diffusion-inspired learning approach to mitigate the quantum noise in the PQCs and reduce the error for specific tasks. Through our experiments, we illustrate the efficiency of the learning strategy and achieve state-of-the-art performance on classification tasks in the quantum noise scenarios.",
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2269131655",
                    "name": "Hoang-Quan Nguyen"
                },
                {
                    "authorId": "1959025244",
                    "name": "Xuan-Bac Nguyen"
                },
                {
                    "authorId": "2303901307",
                    "name": "Samuel Yen-Chi Chen"
                },
                {
                    "authorId": "2068577467",
                    "name": "Hugh Churchill"
                },
                {
                    "authorId": "2170580132",
                    "name": "Nicholas Borys"
                },
                {
                    "authorId": "2257363043",
                    "name": "Samee U. Khan"
                },
                {
                    "authorId": "2243185830",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "f9efe4918b9a20c2e8084d7a5ec624b87ec1f211",
            "title": "QClusformer: A Quantum Transformer-based Framework for Unsupervised Visual Clustering",
            "abstract": "Unsupervised vision clustering, a cornerstone in computer vision, has been studied for decades, yielding significant outcomes across numerous vision tasks. However, these algorithms involve substantial computational demands when confronted with vast amounts of unlabeled data. Conversely, quantum computing holds promise in expediting unsupervised algorithms when handling large-scale databases. In this study, we introduce QClusformer, a pioneering Transformer-based framework leveraging quantum machines to tackle unsupervised vision clustering challenges. Specifically, we design the Transformer architecture, including the self-attention module and transformer blocks, from a quantum perspective to enable execution on quantum hardware. In addition, we present QClusformer, a variant based on the Transformer architecture, tailored for unsupervised vision clustering tasks. By integrating these elements into an end-to-end framework, QClusformer consistently outperforms previous methods running on classical computers. Empirical evaluations across diverse benchmarks, including MS-Celeb-1M and DeepFashion, underscore the superior performance of QClusformer compared to state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1959025244",
                    "name": "Xuan-Bac Nguyen"
                },
                {
                    "authorId": "2269131655",
                    "name": "Hoang-Quan Nguyen"
                },
                {
                    "authorId": "2303901307",
                    "name": "Samuel Yen-Chi Chen"
                },
                {
                    "authorId": "2257363043",
                    "name": "Samee U. Khan"
                },
                {
                    "authorId": "2068577467",
                    "name": "Hugh Churchill"
                },
                {
                    "authorId": "2243185830",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "13707ea1d3ed16900f1f72d22504ae3136d9e760",
            "title": "Quantum Vision Clustering",
            "abstract": "Unsupervised visual clustering has garnered significant attention in recent times, aiming to characterize distributions of unlabeled visual images through clustering based on a parameterized appearance approach. Alternatively, clustering algorithms can be viewed as assignment problems, often characterized as NP-hard, yet precisely solvable for small instances on contemporary hardware. Adiabatic quantum computing (AQC) emerges as a promising solution, poised to deliver substantial speedups for a range of NP-hard optimization problems. However, existing clustering formulations face challenges in quantum computing adoption due to scalability issues. In this study, we present the first clustering formulation tailored for resolution using Adiabatic quantum computing. An Ising model is introduced to represent the quantum mechanical system implemented on AQC. The proposed approach demonstrates high competitiveness compared to state-of-the-art optimization-based methods, even when utilizing off-the-shelf integer programming solvers. Lastly, this work showcases the solvability of the proposed clustering problem on current-generation real quantum computers for small examples and analyzes the properties of the obtained solutions",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "1959025244",
                    "name": "Xuan-Bac Nguyen"
                },
                {
                    "authorId": "2242933706",
                    "name": "Benjamin Thompson"
                },
                {
                    "authorId": "2068577467",
                    "name": "Hugh Churchill"
                },
                {
                    "authorId": "2243185830",
                    "name": "Khoa Luu"
                },
                {
                    "authorId": "2257363043",
                    "name": "Samee U. Khan"
                }
            ]
        },
        {
            "paperId": "1aeac2aa50f364b88a6f145161a13da9dda7f3b8",
            "title": "Brainformer: Modeling MRI Brain Functions to Machine Vision",
            "abstract": "\u201cPerception is reality\u201d. Human perception plays a vital role in forming beliefs and understanding reality. Ex-ploring how the human brain works in the visual system facilitates bridging the gap between human visual perception and computer vision models. However, neuroscientists study the brain via Neuroimaging, i.e., Functional Magnetic Resonance Imaging (fMRI), to discover the brain\u2019s functions. These approaches face interpretation challenges where fMRI data can be complex and require expertise. Therefore, neuroscientists make inferences about cognitive processes based on patterns of brain activities, which can lead to potential misinterpretation or limited functional understanding. In this work, we first present a simple yet effective Brainformer approach, a novel Transformer-based framework, to analyze the patterns of fMRI in the human perception system from the machine learning perspective. Secondly, we introduce a novel mechanism incorporating fMRI, which represents the human brain activities, as the supervision for the machine vision model. This work also introduces a novel perspective on transferring knowledge from human perception to neural networks. Through our experiments, we demonstrated that by leveraging fMRI information, the machine vision model can achieve potential results compared to the current State-of-the-art methods in various image recognition tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1959025244",
                    "name": "Xuan-Bac Nguyen"
                },
                {
                    "authorId": "2268627059",
                    "name": "Xin Li"
                },
                {
                    "authorId": "2257363043",
                    "name": "Samee U. Khan"
                },
                {
                    "authorId": "2243185830",
                    "name": "Khoa Luu"
                }
            ]
        },
        {
            "paperId": "5f2ffecd734106f061dec51f6600dcec5b9a404f",
            "title": "Fairness in Visual Clustering: A Novel Transformer Clustering Approach",
            "abstract": "Promoting fairness for deep clustering models in unsupervised clustering settings to reduce demographic bias is a challenging goal. This is because of the limitation of large-scale balanced data with well-annotated labels for sensitive or protected attributes. In this paper, we first evaluate demographic bias in deep clustering models from the perspective of cluster purity, which is measured by the ratio of positive samples within a cluster to their correlation degree. This measurement is adopted as an indication of demographic bias. Then, a novel loss function is introduced to encourage a purity consistency for all clusters to maintain the fairness aspect of the learned clustering model. Moreover, we present a novel attention mechanism, Cross-attention, to measure correlations between multiple clusters, strengthening faraway positive samples and improving the purity of clusters during the learning process. Experimental results on a large-scale dataset with numerous attribute settings have demonstrated the effectiveness of the proposed approach on both clustering accuracy and fairness enhancement on several sensitive attributes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1959025244",
                    "name": "Xuan-Bac Nguyen"
                },
                {
                    "authorId": "1876581",
                    "name": "C. Duong"
                },
                {
                    "authorId": "1794486",
                    "name": "M. Savvides"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                },
                {
                    "authorId": "1769788",
                    "name": "Khoa Luu"
                }
            ]
        }
    ]
}