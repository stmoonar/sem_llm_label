{
    "authorId": "2254047333",
    "papers": [
        {
            "paperId": "3cc84a5a1e02661fc3681d8be4134205b0b09954",
            "title": "DyG-Mamba: Continuous State Space Modeling on Dynamic Graphs",
            "abstract": "Dynamic graph learning aims to uncover evolutionary laws in real-world systems, enabling accurate social recommendation (link prediction) or early detection of cancer cells (classification). Inspired by the success of state space models, e.g., Mamba, for efficiently capturing long-term dependencies in language modeling, we propose DyG-Mamba, a new continuous state space model (SSM) for dynamic graph learning. Specifically, we first found that using inputs as control signals for SSM is not suitable for continuous-time dynamic network data with irregular sampling intervals, resulting in models being insensitive to time information and lacking generalization properties. Drawing inspiration from the Ebbinghaus forgetting curve, which suggests that memory of past events is strongly correlated with time intervals rather than specific details of the events themselves, we directly utilize irregular time spans as control signals for SSM to achieve significant robustness and generalization. Through exhaustive experiments on 12 datasets for dynamic link prediction and dynamic node classification tasks, we found that DyG-Mamba achieves state-of-the-art performance on most of the datasets, while also demonstrating significantly improved computation and memory efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2242195007",
                    "name": "Dongyuan Li"
                },
                {
                    "authorId": "148149386",
                    "name": "Shiyin Tan"
                },
                {
                    "authorId": "2153391362",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2298723734",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2283854880",
                    "name": "Manabu Okumura"
                },
                {
                    "authorId": "2299193401",
                    "name": "Renhe Jiang"
                }
            ]
        },
        {
            "paperId": "5090846e033f3da2b47e1b73daa39ffa5e923f59",
            "title": "All Nodes are created Not Equal: Node-Specific Layer Aggregation and Filtration for GNN",
            "abstract": "The ever-designed Graph Neural Networks, though opening a promising path for the modeling of the graph-structure data, unfortunately introduce two daunting obstacles to their deployment on devices. (I) Most of existing GNNs are shallow, due mostly to the over-smoothing and gradient-vanish problem as they go deeper as convolutional architectures. (II) The vast majority of GNNs adhere to the homophily assumption, where the central node and its adjacent nodes share the same label. This assumption often poses challenges for many GNNs working with heterophilic graphs. Addressing the aforementioned issue has become a looming challenge in enhancing the robustness and scalability of GNN applications. In this paper, we take a comprehensive and systematic approach to overcoming the two aforementioned challenges for the first time. We propose a Node-Specific Layer Aggregation and Filtration architecture, termed NoSAF, a framework capable of filtering and processing information from each individual nodes. NoSAF introduces the concept of\"All Nodes are Created Not Equal\"into every layer of deep networks, aiming to provide a reliable information filter for each layer's nodes to sieve out information beneficial for the subsequent layer. By incorporating a dynamically updated codebank, NoSAF dynamically optimizes the optimal information outputted downwards at each layer. This effectively overcomes heterophilic issues and aids in deepening the network. To compensate for the information loss caused by the continuous filtering in NoSAF, we also propose NoSAF-D (Deep), which incorporates a compensation mechanism that replenishes information in every layer of the model, allowing NoSAF to perform meaningful computations even in very deep layers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2274309064",
                    "name": "Shilong Wang"
                },
                {
                    "authorId": "2285959396",
                    "name": "Hao Wu"
                },
                {
                    "authorId": "2287230409",
                    "name": "Yifan Duan"
                },
                {
                    "authorId": "2232926268",
                    "name": "Guibin Zhang"
                },
                {
                    "authorId": "2301242230",
                    "name": "Guohao Li"
                },
                {
                    "authorId": "2268400993",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2285434255",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "2283666223",
                    "name": "Yang Wang"
                }
            ]
        },
        {
            "paperId": "7a54af2a48c2a10d9db52e4ecf5484c0f59241df",
            "title": "Two Heads Are Better Than One: Boosting Graph Sparse Training via Semantic and Topological Awareness",
            "abstract": "Graph Neural Networks (GNNs) excel in various graph learning tasks but face computational challenges when applied to large-scale graphs. A promising solution is to remove non-essential edges to reduce the computational overheads in GNN. Previous literature generally falls into two categories: topology-guided and semantic-guided. The former maintains certain graph topological properties yet often underperforms on GNNs due to low integration with neural network training. The latter performs well at lower sparsity on GNNs but faces performance collapse at higher sparsity levels. With this in mind, we take the first step to propose a new research line and concept termed Graph Sparse Training (GST), which dynamically manipulates sparsity at the data level. Specifically, GST initially constructs a topology&semantic anchor at a low training cost, followed by performing dynamic sparse training to align the sparse graph with the anchor. We introduce the Equilibria Sparsification Principle to guide this process, effectively balancing the preservation of both topological and semantic information. Ultimately, GST produces a sparse graph with maximum topological integrity and no performance degradation. Extensive experiments on 6 datasets and 5 backbones showcase that GST (I) identifies subgraphs at higher graph sparsity levels (1.67%~15.85% $\\uparrow$) than state-of-the-art sparsification methods, (II) preserves more key spectral properties, (III) achieves 1.27-3.42$\\times$ speedup in GNN inference and (IV) successfully helps graph adversarial defense and graph lottery tickets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2232926268",
                    "name": "Guibin Zhang"
                },
                {
                    "authorId": "2282473982",
                    "name": "Yanwei Yue"
                },
                {
                    "authorId": "2283296590",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "2159830802",
                    "name": "Junfeng Fang"
                },
                {
                    "authorId": "2003767516",
                    "name": "Yongduo Sui"
                },
                {
                    "authorId": "2283053585",
                    "name": "Kai Wang"
                },
                {
                    "authorId": "2268400993",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2282580031",
                    "name": "Dawei Cheng"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2282507790",
                    "name": "Tianlong Chen"
                }
            ]
        },
        {
            "paperId": "ade46150fbb93b4e473f2fafbe39dfbb3346ee94",
            "title": "A Survey on Diffusion Models for Time Series and Spatio-Temporal Data",
            "abstract": "The study of time series is crucial for understanding trends and anomalies over time, enabling predictive insights across various sectors. Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic perspective on complex system interactions. Recently, diffusion models have seen widespread application in time series and spatio-temporal data mining. Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but they also extend to other downstream tasks. In this survey, we comprehensively and thoroughly review the use of diffusion models in time series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain. In detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series and spatio-temporal data separately. Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models, serving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation. Conditioned models, on the other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks. Our survey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and transportation, providing a foundational understanding of how these models analyze and generate data. Through this structured overview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and spatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring innovative solutions within the diffusion model framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2303519564",
                    "name": "Yiyuan Yang"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2262445381",
                    "name": "Haomin Wen"
                },
                {
                    "authorId": "2152737103",
                    "name": "Chaoli Zhang"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2253908414",
                    "name": "Lintao Ma"
                },
                {
                    "authorId": "2259354682",
                    "name": "Yi Wang"
                },
                {
                    "authorId": "2275745358",
                    "name": "Cheng-Ming Liu"
                },
                {
                    "authorId": "2298971233",
                    "name": "Bin Yang"
                },
                {
                    "authorId": "2298925809",
                    "name": "Zenglin Xu"
                },
                {
                    "authorId": "2298896985",
                    "name": "Jiang Bian"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "c30abb2ad76fcbfd4e6dc8881850b591d3434a3e",
            "title": "Foundation Models for Time Series Analysis: A Tutorial and Survey",
            "abstract": "Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advances in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored for time series analysis. This survey aims to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either application or pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a methodology-centric classification, delineating various pivotal elements of time-series FMs, including model architectures, pre-training techniques, adaptation methods, and data modalities. Overall, this survey serves to consolidate the latest advancements in FMs pertinent to time series analysis, accentuating their theoretical underpinnings, recent strides in development, and avenues for future exploration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2262445381",
                    "name": "Haomin Wen"
                },
                {
                    "authorId": "2300149940",
                    "name": "Yuqi Nie"
                },
                {
                    "authorId": "2214140574",
                    "name": "Yushan Jiang"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2292142150",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "d622d8b2d5adc4b638a73b105686840e264dfb8f",
            "title": "Position: What Can Large Language Models Tell Us about Time Series Analysis",
            "abstract": "Time series analysis is essential for comprehending the complexities inherent in various realworld systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including time series modality switching and question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2304517120",
                    "name": "Yifan Zhang"
                },
                {
                    "authorId": "2301489384",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2304920924",
                    "name": "Kexin Zhang"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2298971233",
                    "name": "Bin Yang"
                },
                {
                    "authorId": "2304515949",
                    "name": "Jindong Wang"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2303259263",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "f3ca1515d610ec8811d3b4f9a209eae099523048",
            "title": "Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective",
            "abstract": "In long-term time series forecasting (LTSF) tasks, an increasing number of models have acknowledged that discrete time series originate from continuous dynamic systems and have attempted to model their dynamical structures. Recognizing the chaotic nature of real-world data, our model, \\textbf{\\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes non-parametric Phase Space Reconstruction embedding and the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets with only one-twelfth of the parameters compared to PatchTST.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2284730172",
                    "name": "Jiaxi Hu"
                },
                {
                    "authorId": "2284735864",
                    "name": "Yuehong Hu"
                },
                {
                    "authorId": "2262453292",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                }
            ]
        },
        {
            "paperId": "16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277",
            "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
            "abstract": "Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2255363760",
                    "name": "Shiyu Wang"
                },
                {
                    "authorId": "2253908414",
                    "name": "Lintao Ma"
                },
                {
                    "authorId": "2237992280",
                    "name": "Zhixuan Chu"
                },
                {
                    "authorId": "2253786576",
                    "name": "James Y. Zhang"
                },
                {
                    "authorId": "2119204984",
                    "name": "X. Shi"
                },
                {
                    "authorId": "2254173316",
                    "name": "Pin-Yu Chen"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2256011160",
                    "name": "Yuan-Fang Li"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "5b038c1a93967072cc76689fd805e756f804cc42",
            "title": "Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook",
            "abstract": "Temporal data, notably time series and spatio-temporal data, are prevalent in real-world applications. They capture dynamic system measurements and are produced in vast quantities by both physical and virtual sensors. Analyzing these data types is vital to harnessing the rich information they encompass and thus benefits a wide range of downstream tasks. Recent advances in large language and other foundational models have spurred increased use of these models in time series and spatio-temporal data mining. Such methodologies not only enable enhanced pattern recognition and reasoning across diverse domains but also lay the groundwork for artificial general intelligence capable of comprehending and processing common temporal data. In this survey, we offer a comprehensive and up-to-date review of large models tailored (or adapted) for time series and spatio-temporal data, spanning four key facets: data types, model categories, model scopes, and application areas/tasks. Our objective is to equip practitioners with the knowledge to develop applications and further research in this underexplored domain. We primarily categorize the existing literature into two major clusters: large models for time series analysis (LM4TS) and spatio-temporal data mining (LM4STD). On this basis, we further classify research based on model scopes (i.e., general vs. domain-specific) and application areas/tasks. We also provide a comprehensive collection of pertinent resources, including datasets, model assets, and useful tools, categorized by mainstream applications. This survey coalesces the latest strides in large model-centric research on time series and spatio-temporal data, underscoring the solid foundations, current advances, practical applications, abundant resources, and future research opportunities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2152737103",
                    "name": "Chaoli Zhang"
                },
                {
                    "authorId": "2149919635",
                    "name": "Siqiao Xue"
                },
                {
                    "authorId": "2258781263",
                    "name": "Xue Wang"
                },
                {
                    "authorId": "2253786576",
                    "name": "James Y. Zhang"
                },
                {
                    "authorId": "2259354682",
                    "name": "Yi Wang"
                },
                {
                    "authorId": "2259822635",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2284734617",
                    "name": "Xiaoli Li"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2258722690",
                    "name": "Vincent S. Tseng"
                },
                {
                    "authorId": "2149514422",
                    "name": "Yu Zheng"
                },
                {
                    "authorId": "2260428467",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2258712531",
                    "name": "Hui Xiong"
                }
            ]
        },
        {
            "paperId": "b47e96762351b2dbf7e863ece4640df6194bcc0c",
            "title": "Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning",
            "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2238130759",
                    "name": "Linhao Luo"
                },
                {
                    "authorId": "2256011160",
                    "name": "Yuan-Fang Li"
                },
                {
                    "authorId": "2561045",
                    "name": "Gholamreza Haffari"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                }
            ]
        }
    ]
}