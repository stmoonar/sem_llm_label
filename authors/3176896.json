{
    "authorId": "3176896",
    "papers": [
        {
            "paperId": "9bd93ace4b6f69b1586a22596fad671cf56a1e06",
            "title": "Discrimination and Class Imbalance Aware Online Naive Bayes",
            "abstract": "Fairness-aware mining of massive data streams is a growing and challenging concern in the contemporary domain of machine learning. Many stream learning algorithms are used to replace humans at critical decision-making points e.g., hiring staff, assessing credit risk, etc. This calls for handling massive incoming information with minimum response delay while ensuring fair and high quality decisions. Recent discrimination-aware learning methods are optimized based on overall accuracy. However, the overall accuracy is biased in favor of the majority class; therefore, state-of-the-art methods mainly diminish discrimination by partially or completely ignoring the minority class. In this context, we propose a novel adaptation of Na\\\"ive Bayes to mitigate discrimination embedded in the streams while maintaining high predictive performance for both the majority and minority classes. Our proposed algorithm is simple, fast, and attains multi-objective optimization goals. To handle class imbalance and concept drifts, a dynamic instance weighting module is proposed, which gives more importance to recent instances and less importance to obsolete instances based on their membership in minority or majority class. We conducted experiments on a range of streaming and static datasets and deduced that our proposed methodology outperforms existing state-of-the-art fairness-aware methods in terms of both discrimination score and balanced accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51192415",
                    "name": "Maryam Badar"
                },
                {
                    "authorId": "2140238020",
                    "name": "M. Fisichella"
                },
                {
                    "authorId": "3176896",
                    "name": "Vasileios Iosifidis"
                },
                {
                    "authorId": "1744808",
                    "name": "W. Nejdl"
                }
            ]
        },
        {
            "paperId": "33cce76322cc5615aa93c167241a3e7cfa363686",
            "title": "Online Fairness-Aware Learning with Imbalanced Data Streams",
            "abstract": "Data-driven learning algorithms are employed in many online applications, in which data become available over time, like network monitoring, stock price prediction, job applications, etc. The underlying data distribution might evolve over time calling for model adaptation as new instances arrive and old instances become obsolete. In such dynamic environments, the so-called data streams, fairness-aware learning cannot be considered as a one-off requirement, but rather it should comprise a continual requirement over the stream. Recent fairness-aware stream classifiers ignore the problem of class imbalance, which manifests in many real-life applications, and mitigate discrimination mainly because they\"reject\"minority instances at large due to their inability to effectively learn all classes. In this work, we propose \\ours, an online fairness-aware approach that maintains a valid and fair classifier over the stream. \\ours~is an online boosting approach that changes the training distribution in an online fashion by monitoring stream's class imbalance and tweaks its decision boundary to mitigate discriminatory outcomes over the stream. Experiments on 8 real-world and 1 synthetic datasets from different domains with varying class imbalance demonstrate the superiority of our method over state-of-the-art fairness-aware stream approaches with a range (relative) increase [11.2\\%-14.2\\%] in balanced accuracy, [22.6\\%-31.8\\%] in gmean, [42.5\\%-49.6\\%] in recall, [14.3\\%-25.7\\%] in kappa and [89.4\\%-96.6\\%] in statistical parity (fairness).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3176896",
                    "name": "Vasileios Iosifidis"
                },
                {
                    "authorId": "2027662138",
                    "name": "Wenbin Zhang"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                }
            ]
        },
        {
            "paperId": "e39a083d28ee99165f960e8c686c86e46e726ab4",
            "title": "Multi-Fair Pareto Boosting",
            "abstract": ". Fairness-aware machine learning for multiple protected attributes (referred to as multi-fairness hereafter) is receiving increasing attention as traditional single-protected attribute approaches cannot ensure fairness w.r.t. other protected attributes. Existing methods, however, still ignore the fact that datasets in this domain are often imbalanced, leading to unfair decisions towards the minority class. Thus, solutions are needed that achieve multi-fairness , accurate predictive performance in overall, and balanced performance across the di\ufb00erent classes. To this end, we introduce a new fairness notion, Multi-Max Mistreat-ment ( MMM ), which measures unfairness while considering both (multi-attribute) protected group and class membership of instances. To learn an MMM -fair classi\ufb01er, we propose a multi-objective problem formulation. We solve the problem using a boosting approach that in-training, incorporates multi-fairness treatment in the distribution update and post-training, \ufb01nds multiple Pareto-optimal solutions; then uses pseudo-weight based decision making to select optimal solution(s) among accurate, balanced, and multi-attribute fair solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47762613",
                    "name": "Arjun Roy"
                },
                {
                    "authorId": "3176896",
                    "name": "Vasileios Iosifidis"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                }
            ]
        },
        {
            "paperId": "ebb0df8229e7dcf47f79c96e77b37e6fa3dae485",
            "title": "A survey on datasets for fairness\u2010aware machine learning",
            "abstract": "As decision\u2010making increasingly relies on machine learning (ML) and (big) data, the issue of fairness in data\u2010driven artificial intelligence systems is receiving increasing attention from both research and industry. A large variety of fairness\u2010aware ML solutions have been proposed which involve fairness\u2010related interventions in the data, learning algorithms, and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real\u2010world datasets used for fairness\u2010aware ML. We focus on tabular data as the most common data representation for fairness\u2010aware ML. We start our analysis by identifying relationships between the different attributes, particularly with respect to protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate interesting relationships using exploratory analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1488731837",
                    "name": "Tai Le Quy"
                },
                {
                    "authorId": "47762613",
                    "name": "Arjun Roy"
                },
                {
                    "authorId": "3176896",
                    "name": "Vasileios Iosifidis"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                }
            ]
        },
        {
            "paperId": "17f423a5e542a4bd4de0243548e127038dea6ab5",
            "title": "Bias in data\u2010driven artificial intelligence systems\u2014An introductory survey",
            "abstract": "Artificial Intelligence (AI)\u2010based systems are widely employed nowadays to make decisions that have far\u2010reaching impact on individuals and society. Their decisions might affect everyone, everywhere, and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training, and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multidisciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well\u2010grounded in a legal frame. In this survey, we focus on data\u2010driven AI, as a large part of AI is powered nowadays by (big) data and powerful machine learning algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features such as race, sex, and so forth.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                },
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                },
                {
                    "authorId": "2516584",
                    "name": "U. Gadiraju"
                },
                {
                    "authorId": "3176896",
                    "name": "Vasileios Iosifidis"
                },
                {
                    "authorId": "1744808",
                    "name": "W. Nejdl"
                },
                {
                    "authorId": "143858195",
                    "name": "Maria-Esther Vidal"
                },
                {
                    "authorId": "2325428",
                    "name": "S. Ruggieri"
                },
                {
                    "authorId": "1707206",
                    "name": "F. Turini"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                },
                {
                    "authorId": "28075447",
                    "name": "Emmanouil Krasanakis"
                },
                {
                    "authorId": "119661806",
                    "name": "I. Kompatsiaris"
                },
                {
                    "authorId": "1404596968",
                    "name": "K. Kinder-Kurlanda"
                },
                {
                    "authorId": "144065562",
                    "name": "Claudia Wagner"
                },
                {
                    "authorId": "51118506",
                    "name": "F. Karimi"
                },
                {
                    "authorId": "152179862",
                    "name": "Miriam Fern\u00e1ndez"
                },
                {
                    "authorId": "145842687",
                    "name": "Harith Alani"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                },
                {
                    "authorId": "3144221",
                    "name": "Tina Kruegel"
                },
                {
                    "authorId": "49729602",
                    "name": "C. Heinze"
                },
                {
                    "authorId": "2102011",
                    "name": "Klaus Broelemann"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                },
                {
                    "authorId": "1726746",
                    "name": "T. Tiropanis"
                },
                {
                    "authorId": "1752093",
                    "name": "Steffen Staab"
                }
            ]
        },
        {
            "paperId": "579c240103f417a8b6fe6d5b59c171bf9229baa0",
            "title": "A Data-driven Human Responsibility Management System",
            "abstract": "An ideal safe workplace is described as a place where staffs fulfill responsibilities in a well-organized order, potential hazardous events are being monitored in real-time, as well as the number of accidents and relevant damages are minimized. However, occupational-related death and injury are still increasing and have been highly attended in the last decades due to the lack of comprehensive safety management. A smart safety management system is therefore urgently needed, in which the staffs are instructed to fulfill responsibilities as well as automating risk evaluations and alerting staffs and departments when needed. In this paper, a smart system for safety management in the workplace based on responsibility big data analysis and the internet of things (IoT) are proposed. The real world implementation and assessment demonstrate that the proposed systems have superior accountability performance and improve the responsibility fulfillment through real-time supervision and self-reminder.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1485719213",
                    "name": "Xuejiao Tang"
                },
                {
                    "authorId": "2116036788",
                    "name": "J. Qiu"
                },
                {
                    "authorId": "120346869",
                    "name": "R. Chen"
                },
                {
                    "authorId": "2027662138",
                    "name": "Wenbin Zhang"
                },
                {
                    "authorId": "3176896",
                    "name": "Vasileios Iosifidis"
                },
                {
                    "authorId": "2109342120",
                    "name": "Zhen Liu"
                },
                {
                    "authorId": "2105541847",
                    "name": "Wei Meng"
                },
                {
                    "authorId": "2129402690",
                    "name": "Mingli Zhang"
                },
                {
                    "authorId": "2116922654",
                    "name": "Ji Zhang"
                }
            ]
        },
        {
            "paperId": "8de3c0d82673657e39aeefef589c4a06b45d4ae1",
            "title": "Bias in Data-driven AI Systems - An Introductory Survey",
            "abstract": "AI-based systems are widely employed nowadays to make decisions that have far-reaching impacts on individuals and society. Their decisions might affect everyone, everywhere and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multi-disciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well-grounded in a legal frame. In this survey, we focus on data-driven AI, as a large part of AI is powered nowadays by (big) data and powerful Machine Learning (ML) algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features like race, sex, etc.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                },
                {
                    "authorId": "2393008",
                    "name": "P. Fafalios"
                },
                {
                    "authorId": "2516584",
                    "name": "U. Gadiraju"
                },
                {
                    "authorId": "3176896",
                    "name": "Vasileios Iosifidis"
                },
                {
                    "authorId": "1744808",
                    "name": "W. Nejdl"
                },
                {
                    "authorId": "143858195",
                    "name": "Maria-Esther Vidal"
                },
                {
                    "authorId": "2325428",
                    "name": "S. Ruggieri"
                },
                {
                    "authorId": "1707206",
                    "name": "F. Turini"
                },
                {
                    "authorId": "144178604",
                    "name": "S. Papadopoulos"
                },
                {
                    "authorId": "28075447",
                    "name": "Emmanouil Krasanakis"
                },
                {
                    "authorId": "119661806",
                    "name": "I. Kompatsiaris"
                },
                {
                    "authorId": "1404596968",
                    "name": "K. Kinder-Kurlanda"
                },
                {
                    "authorId": "144065562",
                    "name": "Claudia Wagner"
                },
                {
                    "authorId": "51118506",
                    "name": "F. Karimi"
                },
                {
                    "authorId": "152179862",
                    "name": "Miriam Fern\u00e1ndez"
                },
                {
                    "authorId": "145842687",
                    "name": "Harith Alani"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                },
                {
                    "authorId": "3144221",
                    "name": "Tina Kruegel"
                },
                {
                    "authorId": "49729602",
                    "name": "C. Heinze"
                },
                {
                    "authorId": "2102011",
                    "name": "Klaus Broelemann"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                },
                {
                    "authorId": "1726746",
                    "name": "T. Tiropanis"
                },
                {
                    "authorId": "1752093",
                    "name": "Steffen Staab"
                }
            ]
        },
        {
            "paperId": "efdf20149c29612150805538208865eb9c4e44f1",
            "title": "Using Machine Learning to Automate Mammogram Images Analysis",
            "abstract": "Breast cancer is the second leading cause of cancer-related death after lung cancer in women. Early detection of breast cancer in X-ray mammography is believed to have effectively reduced the mortality rate since 1989. However, a relatively high false positive rate and a low specificity in mammography technology still exist. In this work, a computer-aided automatic mammogram analysis system is proposed to process the mammogram images and automatically discriminate them as either normal or cancerous, consisting of three consecutive image processing, feature selection, and image classification stages. In designing the system, the discrete wavelet transforms (Daubechies 2, Daubechies 4, and Biorthogonal 6.8) and the Fourier cosine transform were first used to parse the mammogram images and extract statistical features. Then, an entropy-based feature selection method was implemented to reduce the number of features. Finally, different pattern recognition methods (including the Back-propagation Network, the Linear Discriminant Analysis, and the Naive Bayes Classifier) and a voting classification scheme were employed. The performance of each classification strategy was evaluated for sensitivity, specificity, and accuracy and for general performance using the Receiver Operating Curve. Our method is validated on the dataset from the Eastern Health in Newfoundland and Labrador of Canada. The experimental results demonstrated that the proposed automatic mammogram analysis system could effectively improve the classification performances.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1485719213",
                    "name": "Xuejiao Tang"
                },
                {
                    "authorId": "2107948412",
                    "name": "Liuhua Zhang"
                },
                {
                    "authorId": "2027662138",
                    "name": "Wenbin Zhang"
                },
                {
                    "authorId": "2152663439",
                    "name": "Xin Huang"
                },
                {
                    "authorId": "3176896",
                    "name": "Vasileios Iosifidis"
                },
                {
                    "authorId": "2109342349",
                    "name": "Zhen Liu"
                },
                {
                    "authorId": "2129402690",
                    "name": "Mingli Zhang"
                },
                {
                    "authorId": "143837599",
                    "name": "E. Messina"
                },
                {
                    "authorId": "2116922654",
                    "name": "Ji Zhang"
                }
            ]
        },
        {
            "paperId": "18fe4800f3c85f315d79063d6b0fe38c7610ad45",
            "title": "AdaFair: Cumulative Fairness Adaptive Boosting",
            "abstract": "The widespread use of ML-based decision making in domains with high societal impact such as recidivism, job hiring and loan credit has raised a lot of concerns regarding potential discrimination. In particular, in certain cases it has been observed that ML algorithms can provide different decisions based on sensitive attributes such as gender or race and therefore can lead to discrimination. Although, several fairness-aware ML approaches have been proposed, their focus has been largely on preserving the overall classification accuracy while improving fairness in predictions for both protected and non-protected groups (defined based on the sensitive attribute(s)). The overall accuracy however is not a good indicator of performance in case of class imbalance, as it is biased towards the majority class. As we will see in our experiments, many of the fairness-related datasets suffer from class imbalance and therefore, tackling fairness requires also tackling the imbalance problem. To this end, we propose AdaFair, a fairness-aware classifier based on AdaBoost that further updates the weights of the instances in each boosting round taking into account a cumulative notion of fairness based upon all current ensemble members, while explicitly tackling class-imbalance by optimizing the number of ensemble members for balanced classification error. Our experiments show that our approach can achieve parity in true positive and true negative rates for both protected and non-protected groups, while it significantly outperforms existing fairness-aware methods up to 25% in terms of balanced error.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3176896",
                    "name": "Vasileios Iosifidis"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                }
            ]
        },
        {
            "paperId": "6237739988bdfd9c7e1d0235399ceefb463425de",
            "title": "FAE: A Fairness-Aware Ensemble Framework",
            "abstract": "Automated decision making based on big data and machine learning (ML) algorithms can result in discriminatory decisions against certain protected groups defined upon personal data like gender, race, sexual orientation etc. Such algorithms designed to discover patterns in big data might not only pick up any encoded societal biases in the training data, but even worse, they might reinforce such biases resulting in more severe discrimination. The majority of thus far proposed fairness-aware machine learning approaches focus solely on the pre-, in- or post-processing steps of the machine learning process, that is, input data, learning algorithms or derived models, respectively. However, the fairness problem cannot be isolated to a single step of the ML process. Rather, discrimination is often a result of complex interactions between big data and algorithms, and therefore, a more holistic approach is required.The proposed FAE (Fairness-Aware Ensemble) framework combines fairness-related interventions at both pre-and post-processing steps of the data analysis process. In the pre-processing step, we tackle the problems of under-representation of the protected group (group imbalance) and of class-imbalance by generating balanced training samples. In the post-processing step, we tackle the problem of class overlapping by shifting the decision boundary in the direction of fairness.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3176896",
                    "name": "Vasileios Iosifidis"
                },
                {
                    "authorId": "1923602",
                    "name": "B. Fetahu"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                }
            ]
        }
    ]
}