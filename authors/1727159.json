{
    "authorId": "1727159",
    "papers": [
        {
            "paperId": "02c009f41b66d2f977fb663f3cb69329f0f03d3f",
            "title": "Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images",
            "abstract": "In this paper, we introduce Recipe1M+, a new large-scale, structured corpus of over one million cooking recipes and 13 million food images. As the largest publicly available collection of recipe data, Recipe1M+ affords the ability to train high-capacity models on aligned, multimodal data. Using these data, we train a neural network to learn a joint embedding of recipes and images that yields impressive results on an image-recipe retrieval task. Moreover, we demonstrate that regularization via the addition of a high-level classification objective both improves retrieval performance to rival that of humans and enables semantic vector arithmetic. We postulate that these embeddings will provide a basis for further exploration of the Recipe1M+ dataset and food and cooking in general. Code, data and models are publicly available.<xref rid=\"fn1\" ref-type=\"fn\"><sup>1</sup></xref><fn id=\"fn1\"><label>1.</label><p><uri>http://im2recipe.csail.mit.edu</uri>.</p> </fn>",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2061199565",
                    "name": "Javier Mar\u00edn"
                },
                {
                    "authorId": "2130796",
                    "name": "A. Biswas"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "49230103",
                    "name": "Nick Hynes"
                },
                {
                    "authorId": "31571033",
                    "name": "Amaia Salvador"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                }
            ]
        },
        {
            "paperId": "12c12405c20bca738d070370f9ce6785e12e9c17",
            "title": "Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response",
            "abstract": "During a disaster event, images shared on social media helps crisis managers gain situational awareness and assess incurred damages, among other response tasks. Recent advances in computer vision and deep neural networks have enabled the development of models for real-time image classification for a number of tasks, including detecting crisis incidents, filtering irrelevant images, classifying images into specific humanitarian categories, and assessing the severity of damage. Despite several efforts, past works mainly suffer from limited resources (i.e., labeled images) available to train more robust deep learning models. In this study, we propose new datasets for disaster type detection, and informativeness classification, and damage severity assessment. Moreover, we relabel existing publicly available datasets for new tasks. We identify exact- and near-duplicates to form non-overlapping data splits, and finally consolidate them to create larger datasets. In our extensive experiments, we benchmark several state-of-the-art deep learning models and achieve promising results. We release our datasets and models publicly, aiming to provide proper baselines as well as to spur further research in the crisis informatics community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "1992544035",
                    "name": "Tanvirul Alam"
                },
                {
                    "authorId": "48166270",
                    "name": "U. Qazi"
                }
            ]
        },
        {
            "paperId": "1c9fd716c005a297ac972f610e8ff8798898e01e",
            "title": "The Relative Value of Facebook Advertising Data for Poverty Mapping",
            "abstract": "Having reliable and up-to-date poverty data is a prerequisite for monitoring the United Nations Sustainable Development Goals (SDGs) and for planning effective poverty reduction interventions. Unfortunately, traditional data sources are often outdated or lacking appropriate disaggregation. As a remedy, satellite imagery has recently become prominent in obtaining geographically-fine-grained and up-to-date poverty estimates. Satellite data can pick up signals of economic activity by detecting light at night, it can pick up development status by detecting infrastructure such as roads, and it can pick up signals for individual household wealth by detecting different building footprints and roof types. It can, however, not look inside the households and pick up signals from individuals. On the other hand, alternative data sources such as audience estimates from Facebook's advertising platform provide insights into the devices and internet connection types used by individuals in different locations. Previous work has shown the value of such anonymous, publicly-accessible advertising data from Facebook for studying migration, gender gaps, crime rates, and health, among others. In this work, we evaluate the added value of using Facebook data over satellite data for mapping socioeconomic development in two low and middle income countries \u2013 the Philippines and India. We show that Facebook features perform roughly similar to satellite data in the Philippines with value added for urban locations. In India, however, where Facebook penetration is lower, satellite data perform better.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72339731",
                    "name": "M. Fatehkia"
                },
                {
                    "authorId": "4435175",
                    "name": "Benjamin Coles"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                }
            ]
        },
        {
            "paperId": "4a888afd8fb93e9d294acdc7022ee20a5db8d617",
            "title": "Analysis of Social Media Data using Multimodal Deep Learning for Disaster Response",
            "abstract": "Multimedia content in social media platforms provides significant information during disaster events. The types of information shared include reports of injured or deceased people, infrastructure damage, and missing or found people, among others. Although many studies have shown the usefulness of both text and image content for disaster response purposes, the research has been mostly focused on analyzing only the text modality in the past. In this paper, we propose to use both text and image modalities of social media data to learn a joint representation using state-of-the-art deep learning techniques. Specifically, we utilize convolutional neural networks to define a multimodal deep learning architecture with a modality-agnostic shared representation. Extensive experiments on real-world disaster datasets show that the proposed multimodal architecture yields better performance than models trained using a single modality (e.g., either text or image).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                }
            ]
        },
        {
            "paperId": "a9e96acdccf33eac6215ceefb48994d0b5b7bf37",
            "title": "Detection of Disaster-Affected Cultural Heritage Sites from Social Media Images Using Deep Learning Techniques",
            "abstract": "This article describes a method for early detection of disaster-related damage to cultural heritage. It is based on data from social media, a timely and large-scale data source that is nevertheless quite noisy. First, we collect images posted on social media that may refer to a cultural heritage site. Then, we automatically categorize these images according to two dimensions: whether they are indeed a photo in which a cultural heritage resource is the main subject, and whether they represent damage. Both categorizations are challenging image classification tasks, given the ambiguity of these visual categories; we tackle both tasks using a convolutional neural network. We test our methodology on a large collection of thousands of images from the web and social media, which exhibit the diversity and noise that is typical of these sources, and contain buildings and other architectural elements, heritage and not-heritage, damaged by disasters as well as intact. Our results show that while the automatic classification is not perfect, it can greatly reduce the manual effort required to find photos of damaged cultural heritage by accurately detecting relevant candidates to be examined by a cultural heritage professional.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1505837278",
                    "name": "Pakhee Kumar"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "32584303",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "2065333595",
                    "name": "C. Castillo"
                }
            ]
        },
        {
            "paperId": "17faab87b74d42e02b407c149c398c33ef4cfc0a",
            "title": "How to Make a Pizza: Learning a Compositional Layer-Based GAN Model",
            "abstract": "A food recipe is an ordered set of instructions for preparing a particular dish. From a visual perspective, every instruction step can be seen as a way to change the visual appearance of the dish by adding extra objects (e.g., adding an ingredient) or changing the appearance of the existing ones (e.g., cooking the dish). In this paper, we aim to teach a machine how to make a pizza by building a generative model that mirrors this step-by-step procedure. To do so, we learn composable module operations which are able to either add or remove a particular ingredient. Each operator is designed as a Generative Adversarial Network (GAN). Given only weak image-level supervision, the operators are trained to generate a visual layer that needs to be added to or removed from the existing image. The proposed model is able to decompose an image into an ordered sequence of layers by applying sequentially in the right order the corresponding removing modules. Experimental results on synthetic and real pizza images demonstrate that our proposed model is able to: (1) segment pizza toppings in a weakly- supervised fashion, (2) remove them by revealing what is occluded underneath them (i.e., inpainting), and (3) infer the ordering of the toppings without any depth ordering supervision. Code, data, and models are available online.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1749373",
                    "name": "Dim P. Papadopoulos"
                },
                {
                    "authorId": "3430216",
                    "name": "Y. Tamaazousti"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                }
            ]
        },
        {
            "paperId": "789ea84f44e03b3d50bb85987fb237b7765df317",
            "title": "CrisisDPS: Crisis Data Processing Services",
            "abstract": "Over the last few years, extensive research has been conducted to develop technologies to support humanitarian aid tasks. However, many technologies are still limited as they require both manual and automatic approaches, and more importantly, are not ready to be integrated into the disaster response work\ufb02ows. To tackle this limitation, we develop automatic data processing services that are freely and publicly available, and made to be simple, e\ufb03cient, and accessible to non-experts. Our services take textual messages (e.g., tweets, Facebook posts, SMS) as input to determine (i) which disaster type the message belongs to, (ii) whether it is informative or not, and (iii) what type of humanitarian information it conveys. We built our services upon machine learning classi\ufb01ers that are obtained from large-scale comparative experiments utilizing both classical and deep learning algorithms. Our services outperform state-of-the-art publicly available tools in terms of classi\ufb01cation accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                }
            ]
        },
        {
            "paperId": "f2944e3c0fe4908afeb421fff3bc8965c44b6aae",
            "title": "Descriptive and visual summaries of disaster events using artificial intelligence techniques: case studies of Hurricanes Harvey, Irma, and Maria",
            "abstract": "ABSTRACT People increasingly use microblogging platforms such as Twitter during natural disasters and emergencies. Research studies have revealed the usefulness of the data available on Twitter for several disaster response tasks. However, making sense of social media data is a challenging task due to several reasons such as limitations of available tools to analyse high-volume and high-velocity data streams, dealing with information overload, among others. To eliminate such limitations, in this work, we first show that textual and imagery content on social media provide complementary information useful to improve situational awareness. We then explore ways in which various Artificial Intelligence techniques from Natural Language Processing and Computer Vision fields can exploit such complementary information generated during disaster events. Finally, we propose a methodological approach that combines several computational techniques effectively in a unified framework to help humanitarian organisations in their relief efforts. We conduct extensive experiments using textual and imagery content from millions of tweets posted during the three major disaster events in the 2017 Atlantic Hurricane season. Our study reveals that the distributions of various types of useful information can inform crisis managers and responders and facilitate the development of future automated systems for disaster management.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                }
            ]
        },
        {
            "paperId": "048d133e2ec513ce385c8e736df715d8ff496e17",
            "title": "Recipe1M: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images",
            "abstract": "In this paper, we introduce Recipe1M+, a new large-scale, structured corpus of over one million cooking recipes and 13 million food images. As the largest publicly available collection of recipe data, Recipe1M+ affords the ability to train high-capacity modelson aligned, multimodal data. Using these data, we train a neural network to learn a joint embedding of recipes and images that yields impressive results on an image-recipe retrieval task. Moreover, we demonstrate that regularization via the addition of a high-level classification objective both improves retrieval performance to rival that of humans and enables semantic vector arithmetic. We postulate that these embeddings will provide a basis for further exploration of the Recipe1M+ dataset and food and cooking in general. Code, data and models are publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31571033",
                    "name": "Amaia Salvador"
                },
                {
                    "authorId": "49230103",
                    "name": "Nick Hynes"
                },
                {
                    "authorId": "3152281",
                    "name": "Y. Aytar"
                },
                {
                    "authorId": "2061199565",
                    "name": "Javier Mar\u00edn"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                }
            ]
        },
        {
            "paperId": "2a29f7cc10fc18ce6e031e0cf6fc307a6b85d8e9",
            "title": "CrisisMMD: Multimodal Twitter Datasets from Natural Disasters",
            "abstract": "\n \n During natural and man-made disasters, people use social media platforms such as Twitter to post textual and multimedia content to report updates about injured or dead people, infrastructure damage, missing or found people, among other information types. Studies have revealed that this online information, if processed timely and effectively, is extremely useful for humanitarian organizations to gain situational awareness and plan relief operations. In addition to the analysis of textual content, recent studies have shown that imagery content on social media can boost disaster response significantly. Despite extensive research that mainly focuses on textual content to extract useful information, limited work has focused on the use of imagery content or the combination of both content types. One of the reasons is the lack of labeled imagery data in this domain. Therefore, in this paper, we aim to tackle this limitation by releasing a large multi-modal dataset from natural disasters collected from Twitter. We provide three types of annotations, which are useful to address a number of crisis response and management tasks for different humanitarian organizations.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "32584303",
                    "name": "Muhammad Imran"
                }
            ]
        }
    ]
}