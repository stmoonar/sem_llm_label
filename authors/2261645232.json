{
    "authorId": "2261645232",
    "papers": [
        {
            "paperId": "01911d5badfd2397fba9f0b71c7873071c392b33",
            "title": "HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent",
            "abstract": "Recent advancements in Large Language Models (LLMs) have been reshaping Natural Language Processing (NLP) task in several domains. Their use in the field of Human Resources (HR) has still room for expansions and could be beneficial for several time consuming tasks. Examples such as time-off submissions, medical claims filing, and access requests are noteworthy, but they are by no means the sole instances. However the aforementioned developments must grapple with the pivotal challenge of constructing a high-quality training dataset. On one hand, most conversation datasets are solving problems for customers not employees. On the other hand, gathering conversations with HR could raise privacy concerns. To solve it, we introduce HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR domains. Our work has the following contributions:(1) It is the first labeled open-sourced conversation dataset in the HR domain for NLP research. (2) It provides a detailed recipe for the data generation procedure along with data analysis and human evaluations. The data generation pipeline is transferrable and can be easily adapted for labeled conversation data generation in other domains. (3) The proposed data-collection pipeline is mostly based on LLMs with minimal human involvement for annotation, which is time and cost-efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2282498869",
                    "name": "Zicheng Huang"
                },
                {
                    "authorId": "2261645232",
                    "name": "Wenxiang Hu"
                },
                {
                    "authorId": "2288381545",
                    "name": "Xi Fang"
                },
                {
                    "authorId": "2282467353",
                    "name": "Rajesh Kumar Cherukuri"
                },
                {
                    "authorId": "40511871",
                    "name": "Naumaan Nayyar"
                },
                {
                    "authorId": "2282467375",
                    "name": "Lorenzo Malandri"
                },
                {
                    "authorId": "1757518",
                    "name": "Srinivasan H. Sengamedu"
                }
            ]
        },
        {
            "paperId": "e5968ff7af575e014a4cb76a75f1f0f4378d16d7",
            "title": "DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM",
            "abstract": "In the burgeoning field of natural language processing (NLP), Neural Topic Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic based text generation. NTMs have never been combined with diffusion model for text generation. Our study addresses these gaps by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion model, our framework also provides the capability to do topic based text generation. This dual functionality allows users to efficiently produce highly clustered topics and topic based text generation simultaneously. DeTiME's potential extends to generating clustered embeddings as well. Notably, our proposed framework(both encoder-decoder based LLM and diffusion model) proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2261645232",
                    "name": "Wenxiang Hu"
                },
                {
                    "authorId": "2261413304",
                    "name": "Fanyou Wu"
                },
                {
                    "authorId": "1757518",
                    "name": "Srinivasan H. Sengamedu"
                }
            ]
        }
    ]
}