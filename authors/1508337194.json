{
    "authorId": "1508337194",
    "papers": [
        {
            "paperId": "1211ba752bfc4f35ab93f47cefed37f4accf5360",
            "title": "CEP3: Community Event Prediction with Neural Point Process on Graph",
            "abstract": "Many real world applications can be formulated as event forecasting on Continuous Time Dynamic Graphs (CTDGs) where the occurrence of a timed event between two entities is represented as an edge along with its occurrence timestamp in the graphs.However, most previous works approach the problem in compromised settings, either formulating it as a link prediction task on the graph given the event time or a time prediction problem given which event will happen next. In this paper, we propose a novel model combining Graph Neural Networks and Marked Temporal Point Process (MTPP) that jointly forecasts multiple link events and their timestamps on communities over a CTDG. Moreover, to scale our model to large graphs, we factorize the jointly event prediction problem into three easier conditional probability modeling problems.To evaluate the effectiveness of our model and the rationale behind such a decomposition, we establish a set of benchmarks and evaluation metrics for this event forecasting task. Our experiments demonstrate the superior performance of our model in terms of both model accuracy and training efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47119335",
                    "name": "Xuhong Wang"
                },
                {
                    "authorId": "2144340010",
                    "name": "Sirui Chen"
                },
                {
                    "authorId": "2118917694",
                    "name": "Yixuan He"
                },
                {
                    "authorId": "1508337194",
                    "name": "Minjie Wang"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "153690188",
                    "name": "Yupu Yang"
                },
                {
                    "authorId": "3063894",
                    "name": "Junchi Yan"
                }
            ]
        },
        {
            "paperId": "40f2b55a2fc836bb2b05d1f6f86e0300ec213e12",
            "title": "Graphiler: Optimizing Graph Neural Networks with Message Passing Data Flow Graph",
            "abstract": "Graph neural networks (GNNs) are a new class of powerful machine learning models, but easy programming and efficient computing is often at odds. Current GNN frameworks are based on a message passing paradigm, and allow the concise expression of GNN models using built-in primitives and user defined functions (UDFs). While built-in primitives offer high performance, they are limited in expressiveness; UDFs are flexible, but often have low performance and use excessive memory. In this paper, we propose Graphiler, a compiler stack for GNNs which achieves high performance while offering the flexibility of the UDF programming interface. At the core of Graphiler is a novel abstraction called Message Passing Data Flow Graph (MP-DFG), which enables optimizations that substantially reduce computational redundancy and memory footprint, and optimizes both homogeneous and heterogeneous GNNs under a unified framework. Experiments show Graphiler can accelerate UDF GNNs by up to two orders of magnitude, and achieve performance close to or superior to expert implementations, and do so with substantial memory savings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2165648459",
                    "name": "Zhiqiang Xie"
                },
                {
                    "authorId": "1508337194",
                    "name": "Minjie Wang"
                },
                {
                    "authorId": "3060913",
                    "name": "Zihao Ye"
                },
                {
                    "authorId": "38448016",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2165589301",
                    "name": "Rui Fan"
                }
            ]
        },
        {
            "paperId": "10767523c131afa618373b30d6fb9bfef0e72024",
            "title": "Gaussian Graphical Model Selection for Huge Data via Minipatch Learning",
            "abstract": "Gaussian graphical models are essential unsupervised learning techniques to estimate conditional dependence relationships between sets of nodes. While graphical model selection is a well-studied problem with many popular techniques, there are typically three key practical challenges: i) many existing methods become computationally intractable in huge-data settings with tens of thousands of nodes; ii) the need for separate data-driven tuning hyperparameter selection procedures considerably adds to the computational burden; iii) the statistical accuracy of selected edges often deteriorates as the dimension and/or the complexity of the underlying graph structures increase. We tackle these problems by proposing the Minipatch Graph (MPGraph) estimator. Our approach builds upon insights from the latent variable graphical model problem and utilizes ensembles of thresholded graph estimators \ufb01t to tiny, random subsets of both the observations and the nodes, termed minipatches. As estimates are \ufb01t on small problems, our approach is computationally fast with integrated stability-based hyperparameter tuning. Additionally, we prove that under certain conditions our MPGraph algorithm achieves \ufb01nite-sample graph selection consistency. We compare our approach to state-of-the-art computational approaches to Gaussian graphical model selection including the BigQUIC algorithm, and empirically demonstrate that our approach is not only more accurate but also extensively faster for huge graph selection problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150326071",
                    "name": "Tianyi Yao"
                },
                {
                    "authorId": "1508337194",
                    "name": "Minjie Wang"
                },
                {
                    "authorId": "2302572",
                    "name": "Genevera I. Allen"
                }
            ]
        },
        {
            "paperId": "978835a27075a58cdf38a9ec014f8774ebd705e9",
            "title": "Thresholded Graphical Lasso Adjusts for Latent Variables: Application to Functional Neural Connectivity",
            "abstract": "In neuroscience, researchers seek to uncover the connectivity of neurons from largescale neural recordings or imaging; often people employ graphical model selection and estimation techniques for this purpose. But, existing technologies can only record from a small subset of neurons leading to a challenging problem of graph selection in the presence of extensive latent variables. Chandrasekaran et al. (2012) proposed a convex program to address this problem that poses challenges from both a computational and statistical perspective. To solve this problem, we propose an incredibly simple solution: apply a hard thresholding operator to existing graph selection methods. Conceptually simple and computationally attractive, we demonstrate that thresholding the graphical Lasso, neighborhood selection, or CLIME estimators have superior theoretical properties in terms of graph selection consistency as well as stronger empirical results than existing approaches for the latent variable graphical model problem. We also demonstrate the applicability of our approach through a neuroscience case study on calcium-imaging data to estimate functional neural connections.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1508337194",
                    "name": "Minjie Wang"
                },
                {
                    "authorId": "2302572",
                    "name": "Genevera I. Allen"
                }
            ]
        },
        {
            "paperId": "ee49c89a4619f28ff67fffbf5093fb0c88ca2b58",
            "title": "Scalable Graph Neural Networks with Deep Graph Library",
            "abstract": "Learning from graph and relational data plays a major role in many applications including social network analysis, marketing, e-commerce, information retrieval, knowledge modeling, medical and biological sciences, engineering, and others. Recently, Graph Neural Networks (GNNs) have emerged as a promising new learning framework capable of bringing the power of deep representation learning to graph and relational data. This ever-growing body of research has shown that GNNs achieve state-of-the-art performance for problems such as link prediction, fraud detection, target-ligand binding activity prediction, knowledge-graph completion, and product recommendations. In practice, many of the real-world graphs are very large. It is urgent to have scalable solutions to train GNN on large graphs efficiently. The objective of this tutorial is twofold. First, it will provide an overview of the theory behind GNNs, discuss the types of problems that GNNs are well suited for, and introduce some of the most widely used GNN model architectures and problems/applications that are designed to solve. Second, it will introduce the Deep Graph Library (DGL), a scalable GNN framework that simplifies the development of efficient GNN-based training and inference programs at a large scale. To make things concrete, the tutorial will cover state-of-the-art training methods to scale GNN to large graphs and provide hands-on sessions to show how to use DGL to perform scalable training in different settings (multi-GPU training and distributed training). This hands-on part will start with basic graph applications (e.g., node classification and link prediction) to set up the context and move on to train GNNs on large graphs. It will provide tutorials to demonstrate how to apply the techniques in DGL to train GNNs for real-world applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "1508337194",
                    "name": "Minjie Wang"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "2118943843",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "1852415",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "037df1500b9b8d4a57455b7ad205f86cc94a0b13",
            "title": "DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs",
            "abstract": "Graph neural networks (GNN) have shown great success in learning from graph-structured data. They are widely used in various applications, such as recommendation, fraud detection, and search. In these domains, the graphs are typically large, containing hundreds of millions of nodes and several billions of edges. To tackle this challenge, we develop DistDGL, a system for training GNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the Deep Graph Library (DGL), a popular GNN development framework. DistDGL distributes the graph and its associated data (initial features and embeddings) across the machines and uses this distribution to derive a computational decomposition by following an owner-compute rule. DistDGL follows a synchronous training approach and allows ego-networks forming the mini-batches to include non-local nodes. To minimize the overheads associated with distributed computations, DistDGL uses a high-quality and light-weight min-cut graph partitioning algorithm along with multiple balancing constraints. This allows it to reduce communication overheads and statically balance the computations. It further reduces the communication by replicating halo nodes and by using sparse embedding updates. The combination of these design choices allows DistDGL to train high-quality models while achieving high parallel efficiency and memory scalability. We demonstrate our optimizations on both inductive and transductive GNN models. Our results show that DistDGL achieves linear speedup without compromising model accuracy and requires only 13 seconds to complete a training epoch for a graph with 100 million nodes and 3 billion edges on a cluster with 16 machines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "2112662910",
                    "name": "Chao Ma"
                },
                {
                    "authorId": "1508337194",
                    "name": "Minjie Wang"
                },
                {
                    "authorId": "9695889",
                    "name": "Jinjing Zhou"
                },
                {
                    "authorId": "1994202136",
                    "name": "Qidong Su"
                },
                {
                    "authorId": "2118943843",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "1852415",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "0f2b13daab7feca76b406bc6855d50069ca60caa",
            "title": "Supervised convex clustering",
            "abstract": "Clustering has long been a popular unsupervised learning approach to identify groups of similar objects and discover patterns from unlabeled data in many applications. Yet, coming up with meaningful interpretations of the estimated clusters has often been challenging precisely due to their unsupervised nature. Meanwhile, in many real\u2010world scenarios, there are some noisy supervising auxiliary variables, for instance, subjective diagnostic opinions, that are related to the observed heterogeneity of the unlabeled data. By leveraging information from both supervising auxiliary variables and unlabeled data, we seek to uncover more scientifically interpretable group structures that may be hidden by completely unsupervised analyses. In this work, we propose and develop a new statistical pattern discovery method named supervised convex clustering (SCC) that borrows strength from both information sources and guides towards finding more interpretable patterns via a joint convex fusion penalty. We develop several extensions of SCC to integrate different types of supervising auxiliary variables, to adjust for additional covariates, and to find biclusters. We demonstrate the practical advantages of SCC through simulations and a case study on Alzheimer's disease genomics. Specifically, we discover new candidate genes as well as new subtypes of Alzheimer's disease that can potentially lead to better understanding of the underlying genetic mechanisms responsible for the observed heterogeneity of cognitive decline in older adults.",
            "fieldsOfStudy": [
                "Mathematics",
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1508337194",
                    "name": "Minjie Wang"
                },
                {
                    "authorId": "150326071",
                    "name": "Tianyi Yao"
                },
                {
                    "authorId": "2302572",
                    "name": "Genevera I. Allen"
                }
            ]
        },
        {
            "paperId": "248031105cc00737877ded134852d0edeb43b73c",
            "title": "FeatGraph: A Flexible and Efficient Backend for Graph Neural Network Systems",
            "abstract": "Graph neural networks (GNNs) are gaining popularity as a promising approach to machine learning on graphs. Unlike traditional graph workloads where each vertex/edge is associated with a scalar, GNNs attach a feature tensor to each vertex/edge. This additional feature dimension, along with consequently more complex vertex- and edge-wise computations, has enormous implications on locality and parallelism, which existing graph processing systems fail to exploit. This paper proposes FeatGraph to accelerate GNN workloads by co-optimizing graph traversal and feature dimension computation. FeatGraph provides a flexible programming interface to express diverse GNN models by composing coarse-grained sparse templates with fine-grained user-defined functions (UDFs) on each vertex/edge. FeatGraph incorporates optimizations for graph traversal into the sparse templates and allows users to specify optimizations for UDFs with a feature dimension schedule (FDS). FeatGraph speeds up end-to-end GNN training and inference by up to 32$\\times$ on CPU and 7$\\times$ on GPU.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49994783",
                    "name": "Yuwei Hu"
                },
                {
                    "authorId": "3060913",
                    "name": "Zihao Ye"
                },
                {
                    "authorId": "1508337194",
                    "name": "Minjie Wang"
                },
                {
                    "authorId": "2051623872",
                    "name": "Jiali Yu"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "2112144126",
                    "name": "Mu Li"
                },
                {
                    "authorId": "1852415",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "50316001",
                    "name": "Zhiru Zhang"
                },
                {
                    "authorId": null,
                    "name": "Yida Wang"
                }
            ]
        },
        {
            "paperId": "264cb3838c6cab1f48a7382e884c714f4f8b967c",
            "title": "Learning Graph Neural Networks with Deep Graph Library",
            "abstract": "Learning from graph and relational data plays a major role in many applications including social network analysis, marketing, e-commerce, information retrieval, knowledge modeling, medical and biological sciences, engineering, and others. In the last few years, Graph Neural Networks (GNNs) have emerged as a promising new supervised learning framework capable of bringing the power of deep representation learning to graph and relational data. This ever-growing body of research has shown that GNNs achieve state-of-the-art performance for problems such as link prediction, fraud detection, target-ligand binding activity prediction, knowledge-graph completion, and product recommendations. The objective of this tutorial is twofold. First, it will provide an overview of the theory behind GNNs, discuss the types of problems that GNNs are well suited for, and introduce some of the most widely used GNN model architectures and problems/applications that are designed to solve. Second, it will introduce the Deep Graph Library (DGL), a new software framework that simplifies the development of efficient GNN-based training and inference programs. To make things concrete, the tutorial will provide hands-on sessions using DGL. This hands-on part will cover both basic graph applications (e.g., node classification and link prediction), as well as more advanced topics including training GNNs on large graphs and in a distributed setting. In addition, it will provide hands-on tutorials on using GNNs and DGL for real-world applications such as recommendation and fraud detection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "1508337194",
                    "name": "Minjie Wang"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "38448016",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "833e39087f70e91ad02e9d1de197912efc7154b5",
            "title": "Flexible and Efficient Systems for Training Emerging Deep Neural Networks",
            "abstract": "The success of deep neural networks (DNNs) is due to its strong capability to learn from data. To continue such success, DNNs must handle the ever-increasing size and complexity of data. Two concrete challenges stand out. One, to leverage more data, one needs to train a large DNN model, the size of which becomes limited by the memory capacity of a single GPU. The other, to leverage graph structured data, one needs to use DNN models that perform sparse numerical computation. Unfortunately, current deep learning systems do not provide adequate support for very large or sparse models. This thesis develops two systems, Tofu and DGL, to enable efficient training of these emerging DNNs while minimizing user programming efforts. Tofu supports very large DNNs by partitioning the computation across multiple GPUs to reduce per-GPU memory footprint. To automatically partition each operator, we propose a description language for annotating the semantics of an operator. To optimally partition the whole training, Tofu proposes an algorithm that minimizes the total communication cost. We evaluate and assess the capability of Tofu to train very models demonstrating the substantial gains by applying the design. DGL is a new framework for training DNNs for graph structured data. DGL provides an intuitive and expressive message-passing interface that can cover a wide range of graph DNN models. We introduce batching and kernel fusion techniques that enable training GNNs on large graphs and achieve significant improvements in performance relative to existing systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1508337194",
                    "name": "Minjie Wang"
                }
            ]
        }
    ]
}