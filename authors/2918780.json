{
    "authorId": "2918780",
    "papers": [
        {
            "paperId": "031de9248af3f729f36385f5a929ad1238c867ac",
            "title": "An Analysis on Quantizing Diffusion Transformers",
            "abstract": "Diffusion Models (DMs) utilize an iterative denoising process to transform random noise into synthetic data. Initally proposed with a UNet structure, DMs excel at producing images that are virtually indistinguishable with or without conditioned text prompts. Later transformer-only structure is composed with DMs to achieve better performance. Though Latent Diffusion Models (LDMs) reduce the computational requirement by denoising in a latent space, it is extremely expensive to inference images for any operating devices due to the shear volume of parameters and feature sizes. Post Training Quantization (PTQ) offers an immediate remedy for a smaller storage size and more memory-efficient computation during inferencing. Prior works address PTQ of DMs on UNet structures have addressed the challenges in calibrating parameters for both activations and weights via moderate optimization. In this work, we pioneer an efficient PTQ on transformer-only structure without any optimization. By analysing challenges in quantizing activations and weights for diffusion transformers, we propose a single-step sampling calibration on activations and adapt group-wise quantization on weights for low-bit quantization. We demonstrate the efficiency and effectiveness of proposed methods with preliminary experiments on conditional image generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273530402",
                    "name": "Yuewei Yang"
                },
                {
                    "authorId": "2247901055",
                    "name": "Jialiang Wang"
                },
                {
                    "authorId": "4527324",
                    "name": "Xiaoliang Dai"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "2273559769",
                    "name": "Hongbo Zhang"
                }
            ]
        },
        {
            "paperId": "6325e77616f7b171a6ad6b2bf10e272638d8bf20",
            "title": "Imagine yourself: Tuning-Free Personalized Image Generation",
            "abstract": "Diffusion models have demonstrated remarkable efficacy across various image-to-image tasks. In this research, we introduce Imagine yourself, a state-of-the-art model designed for personalized image generation. Unlike conventional tuning-based personalization techniques, Imagine yourself operates as a tuning-free model, enabling all users to leverage a shared framework without individualized adjustments. Moreover, previous work met challenges balancing identity preservation, following complex prompts and preserving good visual quality, resulting in models having strong copy-paste effect of the reference images. Thus, they can hardly generate images following prompts that require significant changes to the reference image, \\eg, changing facial expression, head and body poses, and the diversity of the generated images is low. To address these limitations, our proposed method introduces 1) a new synthetic paired data generation mechanism to encourage image diversity, 2) a fully parallel attention architecture with three text encoders and a fully trainable vision encoder to improve the text faithfulness, and 3) a novel coarse-to-fine multi-stage finetuning methodology that gradually pushes the boundary of visual quality. Our study demonstrates that Imagine yourself surpasses the state-of-the-art personalization model, exhibiting superior capabilities in identity preservation, visual quality, and text alignment. This model establishes a robust foundation for various personalization applications. Human evaluation results validate the model's SOTA superiority across all aspects (identity preservation, text faithfulness, and visual appeal) compared to the previous personalization models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2313688915",
                    "name": "Zecheng He"
                },
                {
                    "authorId": "2313961507",
                    "name": "Bo Sun"
                },
                {
                    "authorId": "2313677278",
                    "name": "Felix Juefei-Xu"
                },
                {
                    "authorId": "2313782100",
                    "name": "Haoyu Ma"
                },
                {
                    "authorId": "1453469113",
                    "name": "Ankit Ramchandani"
                },
                {
                    "authorId": "2313676940",
                    "name": "Vincent Cheung"
                },
                {
                    "authorId": "2313675009",
                    "name": "Siddharth Shah"
                },
                {
                    "authorId": "2199318406",
                    "name": "Anmol Kalia"
                },
                {
                    "authorId": "2322094813",
                    "name": "Harihar Subramanyam"
                },
                {
                    "authorId": "2322094811",
                    "name": "Alireza Zareian"
                },
                {
                    "authorId": "2287762612",
                    "name": "Li Chen"
                },
                {
                    "authorId": "2287848816",
                    "name": "Ankit Jain"
                },
                {
                    "authorId": "2313696252",
                    "name": "Ning Zhang"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "1722889",
                    "name": "Roshan Sumbaly"
                },
                {
                    "authorId": "2283763097",
                    "name": "Peter Vajda"
                },
                {
                    "authorId": "2147354973",
                    "name": "Animesh Sinha"
                }
            ]
        },
        {
            "paperId": "0de381c3236258faaaed72a69bbe408dfcabc979",
            "title": "Auto-CARD: Efficient and Robust Codec Avatar Driving for Real-time Mobile Telepresence",
            "abstract": "Real-time and robust photorealistic avatars for telepresence in AR/VR have been highly desired for enabling im-mersive photorealistic telepresence. However, there still exists one key bottleneck: the considerable computational expense needed to accurately infer facial expressions captured from headset-mounted cameras with a quality level that can match the realism of the avatar's human appearance. To this end, we propose a framework called Auto-CARD, which for the first time enables realtime and robust driving of Codec Avatars when exclusively using merely on-device computing resources. This is achieved by minimizing two sources of redundancy. First, we develop a dedicated neural architecture search technique called AVE-NAS for avatar encoding in AR/VR, which explicitly boosts both the searched architectures' robustness in the presence of extreme facial ex-pressions and hardware friendliness on fast evolving AR/VR headsets. Second, we leverage the temporal redundancy in consecutively captured images during continuous rendering and develop a mechanism dubbed LATEX to skip the computation of redundant frames. Specifically, we first identify an opportunity from the linearity of the latent space derived by the avatar decoder and then propose to perform adaptive latent extrapolation for redundant frames. For evaluation, we demonstrate the efficacy of our Auto-CARD framework in realtime Codec Avatar driving settings, where we achieve a $5.05\\times$ speedup on Meta Quest 2 while maintaining a compa-rable or even better animation quality than state-of-the-art avatar encoder designs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "108145103",
                    "name": "Y. Fu"
                },
                {
                    "authorId": "2144462446",
                    "name": "Yuecheng Li"
                },
                {
                    "authorId": "1962403710",
                    "name": "Chenghui Li"
                },
                {
                    "authorId": "2398245",
                    "name": "Jason M. Saragih"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "4527324",
                    "name": "Xiaoliang Dai"
                },
                {
                    "authorId": "3138925",
                    "name": "Yingyan Lin"
                }
            ]
        },
        {
            "paperId": "43bafa19f94a42caea89b32a86489aa850317617",
            "title": "FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis",
            "abstract": "Diffusion models have transformed the image-to-image (I2I) synthesis and are now permeating into videos. How-ever, the advancement of video-to-video (V2V) synthesis has been hampered by the challenge of maintaining temporal consistency across video frames. This paper proposes a consistent V2V synthesis framework by jointly leveraging spatial conditions and temporal optical flow clues within the source video. Contrary to prior methods that strictly adhere to optical flow, our approach harnesses its benefits while handling the imperfection in flow estimation. We encode the optical flow via warping from the first frame and serve it as a supplementary reference in the diffusion model. This enables our model for video synthesis by editing the first frame with any prevalent I2I models and then propagating edits to successive frames. Our V2V model, Flow Vid, demon-strates remarkable properties: (1) Flexibility: Flow Vid works seamlessly with existing I2I models, facilitating various modifications, including stylization, object swaps, and local edits. (2) Efficiency: Generation of a 4-second video with 30 FPS and 512\u00d7512 resolution takes only 1.5 minutes, which is 3.1\u00d7, 7.2\u00d7, and 10.5\u00d7 faster than CoDeF, Rerender, and TokenFlow, respectively. (3) High-quality: In user studies, our FlowVid is preferred 45.7% of the time, outperforming CoDeF (3.5%), Rerender (10.2%), and TokenFlow (40.4%).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275604366",
                    "name": "Feng Liang"
                },
                {
                    "authorId": "2269776842",
                    "name": "Bichen Wu"
                },
                {
                    "authorId": "2247901055",
                    "name": "Jialiang Wang"
                },
                {
                    "authorId": "2269696579",
                    "name": "Licheng Yu"
                },
                {
                    "authorId": "2256701243",
                    "name": "Kunpeng Li"
                },
                {
                    "authorId": "2247515549",
                    "name": "Yinan Zhao"
                },
                {
                    "authorId": "2267241285",
                    "name": "Ishan Misra"
                },
                {
                    "authorId": "2238908906",
                    "name": "Jia-Bin Huang"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                },
                {
                    "authorId": "92419662",
                    "name": "D. Marculescu"
                }
            ]
        },
        {
            "paperId": "4ddfdf3747c70ae9267cb78a7ff7772ef3148803",
            "title": "Efficient Quantization Strategies for Latent Diffusion Models",
            "abstract": "Latent Diffusion Models (LDMs) capture the dynamic evolution of latent variables over time, blending patterns and multimodality in a generative system. Despite the proficiency of LDM in various applications, such as text-to-image generation, facilitated by robust text encoders and a variational autoencoder, the critical need to deploy large generative models on edge devices compels a search for more compact yet effective alternatives. Post Training Quantization (PTQ), a method to compress the operational size of deep learning models, encounters challenges when applied to LDM due to temporal and structural complexities. This study proposes a quantization strategy that efficiently quantize LDMs, leveraging Signal-to-Quantization-Noise Ratio (SQNR) as a pivotal metric for evaluation. By treating the quantization discrepancy as relative noise and identifying sensitive part(s) of a model, we propose an efficient quantization approach encompassing both global and local strategies. The global quantization process mitigates relative quantization noise by initiating higher-precision quantization on sensitive blocks, while local treatments address specific challenges in quantization-sensitive and time-sensitive modules. The outcomes of our experiments reveal that the implementation of both global and local treatments yields a highly efficient and effective Post Training Quantization (PTQ) of LDMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273530402",
                    "name": "Yuewei Yang"
                },
                {
                    "authorId": "4527324",
                    "name": "Xiaoliang Dai"
                },
                {
                    "authorId": "2247901055",
                    "name": "Jialiang Wang"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "2273559769",
                    "name": "Hongbo Zhang"
                }
            ]
        },
        {
            "paperId": "4f4991f93ed86b777c8b0f192dac034a3144b165",
            "title": "Pruning Compact ConvNets for Efficient Inference",
            "abstract": "Neural network pruning is frequently used to compress over-parameterized networks by large amounts, while incurring only marginal drops in generalization performance. However, the impact of pruning on networks that have been highly optimized for efficient inference has not received the same level of attention. In this paper, we analyze the effect of pruning for computer vision, and study state-of-the-art ConvNets, such as the FBNetV3 family of models. We show that model pruning approaches can be used to further optimize networks trained through NAS (Neural Architecture Search). The resulting family of pruned models can consistently obtain better performance than existing FBNetV3 models at the same level of computation, and thus provide state-of-the-art results when trading off between computational complexity and generalization performance on the ImageNet benchmark. In addition to better generalization performance, we also demonstrate that when limited computation resources are available, pruning FBNetV3 models incur only a fraction of GPU-hours involved in running a full-scale NAS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143032877",
                    "name": "Sayan Ghosh"
                },
                {
                    "authorId": "2107060033",
                    "name": "Karthik Prasad"
                },
                {
                    "authorId": "4527324",
                    "name": "Xiaoliang Dai"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "3130257",
                    "name": "Bichen Wu"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                }
            ]
        },
        {
            "paperId": "d62daf809266e02a3e3be4bec160579ff3839cc9",
            "title": "An Investigation on Hardware-Aware Vision Transformer Scaling",
            "abstract": "Vision Transformer (ViT) has demonstrated promising performance in various computer vision tasks, and recently attracted a lot of research attention. Many recent works have focused on proposing new architectures to improve ViT and deploying it into real-world applications. However, little effort has been made to analyze and understand ViT\u2019s architecture design space and its implication for hardware costs on different devices. In this work, by simply scaling ViT\u2019s depth, width, input size, and other basic configurations, we show that a scaled vanilla ViT model without bells and whistles can achieve comparable or superior accuracy-efficiency trade-off than most of the latest ViT variants. Specifically, compared with DeiT-Tiny, our scaled model achieves a \u2191 1.9% higher ImageNet top-1 accuracy under the same FLOPs and a \u2191 3.7% better ImageNet top-1 accuracy under the same latency on an NVIDIA Edge GPU TX2. Motivated by this, we further investigate the extracted scaling strategies from the following two aspects: (1) can these scaling strategies be transferred across different real hardware devices? and (2) can these scaling strategies be transferred to different ViT variants and tasks?. For (1), our exploration, based on various devices with different resource budgets, indicates that the transferability effectiveness depends on the underlying device together with its corresponding deployment tool. For (2), we validate the effective transferability of the aforementioned scaling strategies obtained from a vanilla ViT model on top of an image classification task to the PiT model, a strong ViT variant targeting efficiency as well as object detection and video classification tasks. In particular, when transferred to PiT, our scaling strategies lead to a boosted ImageNet top-1 accuracy of from 74.6% to 76.7% (\u2191 2.1%) under the same 0.7G FLOPs. When transferred to the COCO object detection task, the average precision is boosted by \u2191 0.7% under a similar throughput on a V100 GPU.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "28987646",
                    "name": "Chaojian Li"
                },
                {
                    "authorId": "2110017910",
                    "name": "Kyungmin Kim"
                },
                {
                    "authorId": "3130257",
                    "name": "Bichen Wu"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "2232778335",
                    "name": "Hang Zhang"
                },
                {
                    "authorId": "4527324",
                    "name": "Xiaoliang Dai"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                },
                {
                    "authorId": "2198041939",
                    "name": "Yingyan Lin"
                }
            ]
        },
        {
            "paperId": "dad14d19f8b0bf70a820acd84eeb99bab654397c",
            "title": "DIME-FM : DIstilling Multimodal and Efficient Foundation Models",
            "abstract": "Large Vision-Language Foundation Models (VLFM), such as CLIP, ALIGN and Florence, are trained on large-scale datasets of image-caption pairs and achieve superior transferability and robustness on downstream tasks, but they are difficult to use in many practical applications due to their large size, high latency and fixed architectures. Unfortunately, recent work shows training a small custom VLFM for resource-limited applications is currently very difficult using public and smaller-scale data. In this paper, we introduce a new distillation mechanism (DIME-FM) that allows us to transfer the knowledge contained in large VLFMs to smaller, customized foundation models using a relatively small amount of inexpensive, unpaired images and sentences. We transfer the knowledge from the pre-trained CLIP-ViT-L/14 model to a ViT-B/32 model, with only 40M public images and 28.4M unpaired public sentences. The resulting model \"Distill-ViT-B/32\" rivals the CLIP-ViT-B/32 model pre-trained on its private WiT dataset (400M image-text pairs): Distill-ViT-B/32 achieves similar results in terms of zero-shot and linear-probing performance on both Ima-geNet and the ELEVATER (20 image classification tasks) benchmarks. It also displays comparable robustness when evaluated on five datasets with natural distribution shifts from ImageNet. Please refer to our project page for code and more details.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2125994125",
                    "name": "Ximeng Sun"
                },
                {
                    "authorId": "9325940",
                    "name": "Pengchuan Zhang"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "2187300386",
                    "name": "Hardik Shah"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "3302135",
                    "name": "Xide Xia"
                }
            ]
        },
        {
            "paperId": "e04da3c945aae8e2211222d373e7bf771d6412a7",
            "title": "Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack",
            "abstract": "Training text-to-image models with web scale image-text pairs enables the generation of a wide range of visual concepts from text. However, these pre-trained models often face challenges when it comes to generating highly aesthetic images. This creates the need for aesthetic alignment post pre-training. In this paper, we propose quality-tuning to effectively guide a pre-trained model to exclusively generate highly visually appealing images, while maintaining generality across visual concepts. Our key insight is that supervised fine-tuning with a set of surprisingly small but extremely visually appealing images can significantly improve the generation quality. We pre-train a latent diffusion model on $1.1$ billion image-text pairs and fine-tune it with only a few thousand carefully selected high-quality images. The resulting model, Emu, achieves a win rate of $82.9\\%$ compared with its pre-trained only counterpart. Compared to the state-of-the-art SDXLv1.0, Emu is preferred $68.4\\%$ and $71.3\\%$ of the time on visual appeal on the standard PartiPrompts and our Open User Input benchmark based on the real-world usage of text-to-image models. In addition, we show that quality-tuning is a generic approach that is also effective for other architectures, including pixel diffusion and masked generative transformer models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4527324",
                    "name": "Xiaoliang Dai"
                },
                {
                    "authorId": "2249723114",
                    "name": "Ji Hou"
                },
                {
                    "authorId": "2250489747",
                    "name": "Chih-Yao Ma"
                },
                {
                    "authorId": "2225238191",
                    "name": "Sam S. Tsai"
                },
                {
                    "authorId": "2247901055",
                    "name": "Jialiang Wang"
                },
                {
                    "authorId": "2248766592",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "83754395",
                    "name": "Simon Vandenhende"
                },
                {
                    "authorId": "2248423317",
                    "name": "Xiaofang Wang"
                },
                {
                    "authorId": "2479521",
                    "name": "Abhimanyu Dubey"
                },
                {
                    "authorId": "2110144262",
                    "name": "Matthew Yu"
                },
                {
                    "authorId": "89942851",
                    "name": "Abhishek Kadian"
                },
                {
                    "authorId": "2708577",
                    "name": "Filip Radenovic"
                },
                {
                    "authorId": "144542135",
                    "name": "D. Mahajan"
                },
                {
                    "authorId": "2256701243",
                    "name": "Kunpeng Li"
                },
                {
                    "authorId": "2248075665",
                    "name": "Yue Zhao"
                },
                {
                    "authorId": "2162195471",
                    "name": "Vladan Petrovic"
                },
                {
                    "authorId": "2247874378",
                    "name": "Mitesh Kumar Singh"
                },
                {
                    "authorId": "121255235",
                    "name": "Simran Motwani"
                },
                {
                    "authorId": "148416622",
                    "name": "Yiqian Wen"
                },
                {
                    "authorId": "1705408",
                    "name": "Yi-Zhe Song"
                },
                {
                    "authorId": "1722889",
                    "name": "Roshan Sumbaly"
                },
                {
                    "authorId": "34066479",
                    "name": "Vignesh Ramanathan"
                },
                {
                    "authorId": "2558787",
                    "name": "Zijian He"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                },
                {
                    "authorId": "2248278031",
                    "name": "Devi Parikh"
                }
            ]
        },
        {
            "paperId": "e7ceec6f384db0ddf8c2b387c5172dee28326567",
            "title": "ControlRoom3D: Room Generation Using Semantic Proxy Rooms",
            "abstract": "Manually creating 3D environments for AR/VR applications is a complex process requiring expert knowledge in 3D modeling software. Pioneering works facilitate this process by generating room meshes conditioned on textual style descriptions. Yet, many of these automatically generated 3D meshes do not adhere to typical room layouts, compromising their plausibility, e.g., by placing several beds in one bedroom. To address these challenges, we present ControlRoom3D, a novel method to generate high-quality room meshes. Central to our approach is a user-defined 3D semantic proxy room that outlines a rough room layout based on semantic bounding boxes and a textual description of the overall room style. Our key insight is that when rendered to 2D, this 3D representation provides valuable geometric and semantic information to control powerful 2D models to generate 3D consistent textures and geometry that aligns well with the proxy room. Backed up by an extensive study including quantitative metrics and qualitative user evaluations, our method generates diverse and globally plausible 3D room meshes, thus empowering users to design 3D rooms effortlessly without specialized knowledge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "79955689",
                    "name": "Jonas Schult"
                },
                {
                    "authorId": "2225238191",
                    "name": "Sam S. Tsai"
                },
                {
                    "authorId": "2142793267",
                    "name": "Lukas H\u00f6llein"
                },
                {
                    "authorId": "2271890529",
                    "name": "Bichen Wu"
                },
                {
                    "authorId": "2247901055",
                    "name": "Jialiang Wang"
                },
                {
                    "authorId": "2250489747",
                    "name": "Chih-Yao Ma"
                },
                {
                    "authorId": "2256701243",
                    "name": "Kunpeng Li"
                },
                {
                    "authorId": "2248423317",
                    "name": "Xiaofang Wang"
                },
                {
                    "authorId": "2028358105",
                    "name": "Felix Wimbauer"
                },
                {
                    "authorId": "2558787",
                    "name": "Zijian He"
                },
                {
                    "authorId": "2918780",
                    "name": "Peizhao Zhang"
                },
                {
                    "authorId": "2064068973",
                    "name": "Bastian Leibe"
                },
                {
                    "authorId": "48682997",
                    "name": "P\u00e9ter Vajda"
                },
                {
                    "authorId": "2249723114",
                    "name": "Ji Hou"
                }
            ]
        }
    ]
}