{
    "authorId": "2187907974",
    "papers": [
        {
            "paperId": "71b99a53892409720dc8867afffe64bf3632af6b",
            "title": "Learning Concise and Descriptive Attributes for Visual Recognition",
            "abstract": "Recent advances in foundation models present new opportunities for interpretable visual recognition \u2013 one can first query Large Language Models (LLMs) to obtain a set of attributes that describe each class, then apply vision-language models to classify images via these attributes. Pioneering work shows that querying thousands of attributes can achieve performance competitive with image features. However, our further investigation on 8 datasets reveals that LLM-generated attributes in a large quantity perform almost the same as random words. This surprising finding suggests that significant noise may be present in these attributes. We hypothesize that there exist subsets of attributes that can maintain the classification performance with much smaller sizes, and propose a novel learning-to-search method to discover those concise sets of attributes. As a result, on the CUB dataset, our method achieves performance close to that of massive LLM-generated attributes (e.g., 10k attributes for CUB), yet using only 32 attributes in total to distinguish 200 bird species. Furthermore, our new paradigm demonstrates several additional benefits: higher interpretability and interactivity for humans, and the ability to summarize knowledge for a recognition task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064233490",
                    "name": "Andy Yan"
                },
                {
                    "authorId": "2153604285",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "1828787912",
                    "name": "Yiwu Zhong"
                },
                {
                    "authorId": "2113540861",
                    "name": "Chengyu Dong"
                },
                {
                    "authorId": "2116458151",
                    "name": "Zexue He"
                },
                {
                    "authorId": "47006228",
                    "name": "Yujie Lu"
                },
                {
                    "authorId": "2187907974",
                    "name": "William Wang"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                }
            ]
        },
        {
            "paperId": "7a45d88122badbcca29c5adcf151f08c5faf8c86",
            "title": "Lifelong Online Learning from Accumulated Knowledge",
            "abstract": "In this article, we formulate lifelong learning as an online transfer learning procedure over consecutive tasks, where learning a given task depends on the accumulated knowledge. We propose a novel theoretical principled framework, lifelong online learning, where the learning process for each task is in an incremental manner. Specifically, our framework is composed of two-level predictions: the prediction information that is solely from the current task; and the prediction from the knowledge base by previous tasks. Moreover, this article tackled several fundamental challenges: arbitrary or even non-stationary task generation process, an unknown number of instances in each task, and constructing an efficient accumulated knowledge base. Notably, we provide a provable bound of the proposed algorithm, which offers insights on the how the accumulated knowledge improves the predictions. Finally, empirical evaluations on both synthetic and real datasets validate the effectiveness of the proposed algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35701651",
                    "name": "Changjian Shui"
                },
                {
                    "authorId": "2187907974",
                    "name": "William Wang"
                },
                {
                    "authorId": "1999632",
                    "name": "Ihsen Hedhli"
                },
                {
                    "authorId": "6020015",
                    "name": "C. Wong"
                },
                {
                    "authorId": "143814405",
                    "name": "Feng Wan"
                },
                {
                    "authorId": "40228420",
                    "name": "Boyu Wang"
                },
                {
                    "authorId": "1712120",
                    "name": "Christian Gagn\u00e9"
                }
            ]
        },
        {
            "paperId": "f143659c51896efe0aa59c339e591a7bf4bd5e53",
            "title": "Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks",
            "abstract": "Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and generalization to novel tasks. Recent advances with architectures have allowed for improved scaling along one or two of these axes, but are still computationally prohibitive to use. In this paper, we propose to address all three axes by leveraging \\textbf{L}anguage to \\textbf{C}ontrol \\textbf{D}iffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions, as a step towards generalist agents. Comparing LCD with other state-of-the-art models on the CALVIN language robotics benchmark finds that LCD outperforms other SOTA methods in multi-task success rates, whilst improving inference speed over other comparable diffusion models by 3.3x~15x. We show that LCD can successfully leverage the unique strength of diffusion models to produce coherent long range plans while addressing their weakness in generating low-level details and control.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "6549913",
                    "name": "Edwin Zhang"
                },
                {
                    "authorId": "47006228",
                    "name": "Yujie Lu"
                },
                {
                    "authorId": "2187907974",
                    "name": "William Wang"
                },
                {
                    "authorId": "2111672235",
                    "name": "Amy Zhang"
                }
            ]
        },
        {
            "paperId": "f367ce68505e01d0452fe4be113e27f7a98c9d6d",
            "title": "LAD: Language Augmented Diffusion for Reinforcement Learning",
            "abstract": "Learning skills from language provides a powerful avenue for generalization in re-inforcement learning, although it remains a challenging task as it requires agents to capture the complex interdependencies between language, actions, and states. In this paper, we propose leveraging L anguage A ugmented D iffusion models as a planner conditioned on language (LAD). We demonstrate the comparable performance of LAD with the state-of-the-art on the CALVIN language robotics benchmark with a much simpler architecture that contains no inductive biases specialized to robotics, achieving an average success rate (SR) of 72% compared to the best performance of 76%. We also conduct an analysis on the properties of language conditioned diffusion in reinforcement learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285087796",
                    "name": "Edwin Zhang"
                },
                {
                    "authorId": "2257339858",
                    "name": "Yujie Lu"
                },
                {
                    "authorId": "2187907974",
                    "name": "William Wang"
                },
                {
                    "authorId": "2287241698",
                    "name": "Amy Zhang"
                }
            ]
        }
    ]
}