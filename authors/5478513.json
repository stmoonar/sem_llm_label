{
    "authorId": "5478513",
    "papers": [
        {
            "paperId": "372bc41602bbd21f192305775f0a58de9880e454",
            "title": "HIVE: Harnessing Human Feedback for Instructional Visual Editing",
            "abstract": "Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1.1M training dataset, a 3.6K reward dataset for rewards learning, and a 1 K evaluation dataset to boost the performance of instructional image editing. We conduct extensive empirical experiments quantitatively and qualitatively, showing that HIVE is favored over previous state-of-the-art instructional image editing approaches by a large margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108088736",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "2150441485",
                    "name": "Xinyi Yang"
                },
                {
                    "authorId": "22758695",
                    "name": "Yihao Feng"
                },
                {
                    "authorId": "12282768",
                    "name": "Can Qin"
                },
                {
                    "authorId": "2211960332",
                    "name": "Chia-Chih Chen"
                },
                {
                    "authorId": "2211974297",
                    "name": "Ning Yu"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "46507194",
                    "name": "Haiquan Wang"
                },
                {
                    "authorId": "1702137",
                    "name": "S. Savarese"
                },
                {
                    "authorId": "2490652",
                    "name": "Stefano Ermon"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "2115800155",
                    "name": "Ran Xu"
                }
            ]
        },
        {
            "paperId": "4c7fbfdd777b67d70ae203ef9c8a6a64a7faf26a",
            "title": "REX: Rapid Exploration and eXploitation for AI Agents",
            "abstract": "In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniques. Notably, REX-based methods exhibit remarkable reductions in execution time, enhancing their practical applicability across a diverse set of scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223748790",
                    "name": "Rithesh Murthy"
                },
                {
                    "authorId": "71926704",
                    "name": "Shelby Heinecke"
                },
                {
                    "authorId": "9200530",
                    "name": "Juan Carlos Niebles"
                },
                {
                    "authorId": "2223887365",
                    "name": "Zhiwei Liu"
                },
                {
                    "authorId": "2147380988",
                    "name": "Le Xue"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "22758695",
                    "name": "Yihao Feng"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "1752893100",
                    "name": "Akash Gokul"
                },
                {
                    "authorId": "2309967",
                    "name": "Devansh Arpit"
                },
                {
                    "authorId": "2115800155",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "2122258484",
                    "name": "P. M\u00f9i"
                },
                {
                    "authorId": "46507194",
                    "name": "Haiquan Wang"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "1702137",
                    "name": "S. Savarese"
                }
            ]
        },
        {
            "paperId": "81b10e64133e775dab53153cc82277d276efe1f7",
            "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
            "abstract": "Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "71926704",
                    "name": "Shelby Heinecke"
                },
                {
                    "authorId": "9200530",
                    "name": "Juan Carlos Niebles"
                },
                {
                    "authorId": "2223887365",
                    "name": "Zhiwei Liu"
                },
                {
                    "authorId": "22758695",
                    "name": "Yihao Feng"
                },
                {
                    "authorId": "2147380988",
                    "name": "Le Xue"
                },
                {
                    "authorId": "2223748790",
                    "name": "Rithesh Murthy"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "2108313930",
                    "name": "Jianguo Zhang"
                },
                {
                    "authorId": "2309967",
                    "name": "Devansh Arpit"
                },
                {
                    "authorId": "2115800155",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "2122258484",
                    "name": "P. M\u00f9i"
                },
                {
                    "authorId": "46507194",
                    "name": "Haiquan Wang"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "1702137",
                    "name": "S. Savarese"
                }
            ]
        },
        {
            "paperId": "a88e005e4a0e268d68cfd0bffc9975687b8ff918",
            "title": "GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation",
            "abstract": "Text-to-image (T2I) models based on diffusion processes have achieved remarkable success in controllable image generation using user-provided captions. However, the tight coupling between the current text encoder and image decoder in T2I models makes it challenging to replace or upgrade. Such changes often require massive fine-tuning or even training from scratch with the prohibitive expense. To address this problem, we propose GlueGen, which applies a newly proposed GlueNet model to align features from single-modal or multi-modal encoders with the latent space of an existing T2I model. The approach introduces a new training objective that leverages parallel corpora to align the representation spaces of different encoders. Empirical results show that GlueNet can be trained efficiently and enables various capabilities beyond previous state-of-the-art models: 1) multilingual language models such as XLM-Roberta can be aligned with existing T2I models, allowing for the generation of high-quality images from captions beyond English; 2) GlueNet can align multi-modal encoders such as AudioCLIP with the Stable Diffusion model, enabling sound-to-image generation; 3) it can also upgrade the current text encoder of the latent diffusion model for challenging case generation. By the alignment of various feature representations, the GlueNet allows for flexible and efficient integration of new functionality into existing T2I models and sheds light on X-to-image (X2I) generation.1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12282768",
                    "name": "Can Qin"
                },
                {
                    "authorId": "2052212417",
                    "name": "Ning Yu"
                },
                {
                    "authorId": "50461046",
                    "name": "Chen Xing"
                },
                {
                    "authorId": "2108088736",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "2490652",
                    "name": "Stefano Ermon"
                },
                {
                    "authorId": "2156255943",
                    "name": "Yun Fu"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "2115800155",
                    "name": "Ran Xu"
                }
            ]
        },
        {
            "paperId": "ce212cb873a54e5716da53a66b10298ac013008a",
            "title": "BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents",
            "abstract": "The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, \\textit{i.e.} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at \\url{https://github.com/salesforce/BOLAA}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223887365",
                    "name": "Zhiwei Liu"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "2108313930",
                    "name": "Jianguo Zhang"
                },
                {
                    "authorId": "2147380988",
                    "name": "Le Xue"
                },
                {
                    "authorId": "71926704",
                    "name": "Shelby Heinecke"
                },
                {
                    "authorId": "2223748790",
                    "name": "Rithesh Murthy"
                },
                {
                    "authorId": "22758695",
                    "name": "Yihao Feng"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "9200530",
                    "name": "Juan Carlos Niebles"
                },
                {
                    "authorId": "2309967",
                    "name": "Devansh Arpit"
                },
                {
                    "authorId": "2115800155",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "2122258484",
                    "name": "P. M\u00f9i"
                },
                {
                    "authorId": "46507194",
                    "name": "Haiquan Wang"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "1702137",
                    "name": "S. Savarese"
                }
            ]
        },
        {
            "paperId": "df233ac795d9a5f6908616bc4c884bb82fcab835",
            "title": "Fine-Grained Interaction Modeling with Multi-Relational Transformer for Knowledge Tracing",
            "abstract": "Knowledge tracing, the goal of which is predicting students\u2019 future performance given their past question response sequences to trace their knowledge states, is pivotal for computer-aided education and intelligent tutoring systems. Although many technical efforts have been devoted to modeling students based on their question-response sequences, fine-grained interaction modeling between question-response pairs within each sequence is underexplored. This causes question-response representations less contextualized and further limits student modeling. To address this issue, we first conduct a data analysis and reveal the existence of complex cross effects between different question-response pairs within a sequence. Consequently, we propose MRT-KT, a multi-relational transformer for knowledge tracing, to enable fine-grained interaction modeling between question-response pairs. It introduces a novel relation encoding scheme based on knowledge concepts and student performance. Comprehensive experimental results show that MRT-KT outperforms state-of-the-art knowledge tracing methods on four widely-used datasets, validating the effectiveness of considering fine-grained interaction for knowledge tracing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2199041878",
                    "name": "Jiajun Cui"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "145927131",
                    "name": "Aimin Zhou"
                },
                {
                    "authorId": "2115642141",
                    "name": "Jianyong Wang"
                },
                {
                    "authorId": "2155468861",
                    "name": "Wei Zhang"
                }
            ]
        },
        {
            "paperId": "f4a4effd100f140e01f1ff1b258aae50a540df0d",
            "title": "TransMatch: A Transformer-Based Multilevel Dual-Stream Feature Matching Network for Unsupervised Deformable Image Registration",
            "abstract": "Feature matching, which refers to establishing the correspondence of regions between two images (usually voxel features), is a crucial prerequisite of feature-based registration. For deformable image registration tasks, traditional feature-based registration methods typically use an iterative matching strategy for interest region matching, where feature selection and matching are explicit, but specific feature selection schemes are often useful in solving application-specific problems and require several minutes for each registration. In the past few years, the feasibility of learning-based methods, such as VoxelMorph and TransMorph, has been proven, and their performance has been shown to be competitive compared to traditional methods. However, these methods are usually single-stream, where the two images to be registered are concatenated into a 2-channel whole, and then the deformation field is output directly. The transformation of image features into interimage matching relationships is implicit. In this paper, we propose a novel end-to-end dual-stream unsupervised framework, named TransMatch, where each image is fed into a separate stream branch, and each branch performs feature extraction independently. Then, we implement explicit multilevel feature matching between image pairs via the query-key matching idea of the self-attention mechanism in the Transformer model. Comprehensive experiments are conducted on three 3D brain MR datasets, LPBA40, IXI, and OASIS, and the results show that the proposed method achieves state-of-the-art performance in several evaluation metrics compared to the commonly utilized registration methods, including SyN, NiftyReg, VoxelMorph, CycleMorph, ViT-V-Net, and TransMorph, demonstrating the effectiveness of our model in deformable medical image registration.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "1746086",
                    "name": "Yuanjie Zheng"
                },
                {
                    "authorId": "144716639",
                    "name": "J. Gee"
                }
            ]
        },
        {
            "paperId": "13c8a429501dbe8a79eb2c8b1fcd25718f10269d",
            "title": "VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution",
            "abstract": "Videos typically record the streaming and continuous visual data as discrete consecutive frames. Since the storage cost is expensive for videos of high fidelity, most of them are stored in a relatively low resolution and frame rate. Recent works of Space-Time Video Super-Resolution (STVSR) are developed to incorporate temporal interpolation and spatial super-resolution in a unified framework. However, most of them only support a fixed up-sampling scale, which limits their flexibility and applications. In this work, instead of following the discrete representations, we propose Video Implicit Neural Representation (VideoINR), and we show its applications for STVSR. The learned implicit neural representation can be decoded to videos of arbitrary spatial resolution and frame rate. We show that VideoINR achieves competitive performances with state-of-the-art STVSR methods on common up-sampling scales and significantly outperforms prior works on continuous and out-of-training-distribution scales. Our project page is at here and code is available at https://github.com/Picsart-AI-Research/VideoINR-Continuous-Space-Time-Super-Resolution.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "47559228",
                    "name": "Yinbo Chen"
                },
                {
                    "authorId": "2144128630",
                    "name": "Jingwen Liu"
                },
                {
                    "authorId": "101246501",
                    "name": "Xingqian Xu"
                },
                {
                    "authorId": "1665048469",
                    "name": "Vidit Goel"
                },
                {
                    "authorId": "2969311",
                    "name": "Zhangyang Wang"
                },
                {
                    "authorId": "48667025",
                    "name": "Humphrey Shi"
                },
                {
                    "authorId": "1709719",
                    "name": "Xiaolong Wang"
                }
            ]
        },
        {
            "paperId": "70299b50bbb9554cdf095862b4dd3c4e0b97e126",
            "title": "LayoutDETR: Detection Transformer Is a Good Multimodal Layout Designer",
            "abstract": "Graphic layout designs play an essential role in visual communication. Yet handcrafting layout designs is skill-demanding, time-consuming, and non-scalable to batch production. Generative models emerge to make design automation scalable but it remains non-trivial to produce designs that comply with designers' multimodal desires, i.e., constrained by background images and driven by foreground content. We propose LayoutDETR that inherits the high quality and realism from generative modeling, while reformulating content-aware requirements as a detection problem: we learn to detect in a background image the reasonable locations, scales, and spatial relations for multimodal foreground elements in a layout. Our solution sets a new state-of-the-art performance for layout generation on public benchmarks and on our newly-curated ad banner dataset. We integrate our solution into a graphical system that facilitates user studies, and show that users prefer our designs over baselines by significant margins. Code, models, dataset, and demos are available at https://github.com/salesforce/LayoutDETR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143878318",
                    "name": "Ning Yu"
                },
                {
                    "authorId": "1786891",
                    "name": "Chia-Chih Chen"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "2197083127",
                    "name": "Rui Meng"
                },
                {
                    "authorId": "2120363524",
                    "name": "Ganglu Wu"
                },
                {
                    "authorId": "2183766446",
                    "name": "P. Josel"
                },
                {
                    "authorId": "9200530",
                    "name": "Juan Carlos Niebles"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "2115800155",
                    "name": "Ran Xu"
                }
            ]
        },
        {
            "paperId": "a7c2fed1a7bf4c338c938e0bf657a15332fa1327",
            "title": "SIMulation: Demystifying (Insecure) Cellular Network based One-Tap Authentication Services",
            "abstract": "A recently emerged cellular network based One-Tap Authentication (OTAuth) scheme allows app users to quickly sign up or log in to their accounts conveniently: Mobile Network Operator (MNO) provided tokens instead of user passwords are used as identity credentials. After conducting a first in-depth security analysis, however, we have revealed several fundamental design flaws among popular OTAuth services, which allow an adversary to easily (1) perform unauthorized login and register new accounts as the victim, (2) illegally obtain identities of victims, and (3) interfere OTAuth services of legitimate apps. To further evaluate the impact of our identified issues, we propose a pipeline that integrates both static and dynamic analysis. We examined 1,025/894 Android/iOS apps, each app holding more than 100 million installations. We confirmed 396/398 Android/iOS apps are affected. Our research systematically reveals the threats against OTAuth services. Finally, we provide suggestions on how to mitigate these threats accordingly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109495378",
                    "name": "Ziyi Zhou"
                },
                {
                    "authorId": "2189943417",
                    "name": "Xing Han"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "3217464",
                    "name": "Yuhong Nan"
                },
                {
                    "authorId": "1750979",
                    "name": "Juanru Li"
                },
                {
                    "authorId": "144316608",
                    "name": "Dawu Gu"
                }
            ]
        }
    ]
}