{
    "authorId": "1721029",
    "papers": [
        {
            "paperId": "01f516f0b0798973454cbdfd997594fe4e93eec6",
            "title": "From Minimum Change to Maximum Density: On Determining Near-Optimal S-Repair",
            "abstract": "Dirty data are commonly observed in real applications, making cleaning them a key step in data preparation. The widely adopted idea of cleaning dirty data is based on detecting conflicts w.r.t. integrity constraints. Typical S-repair methods remove a minimal set of tuples (to avoid excessive removal and information loss) such that integrity constraints are no longer violated in remaining tuples. Unfortunately, multiple candidates of minimal removal sets may exist and are difficult to determine which one is indeed proper. We intuitively notice that a clean tuple often has more close neighbors (i.e., higher density) than dirty tuples. Hence, in this paper, we study the problem of finding the optimal S-repair under integrity constraints with the highest density, among various minimal removal sets. Our major contributions include (1) the np-hardness analysis on solving the problem, (2) a heuristic algorithm for efficiently tackling the problem and returning the optimal solution in certain cases, (3) an approximation performance bounded method with the same optimal solution guarantee. Experiments on real datasets collected from industry with real-world errors demonstrate the superiority of our work in cleaning dirty tuples.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143830649",
                    "name": "Yu Sun"
                },
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "4a25db8b5ea36935c28e90fbdbfd4fb184fdebea",
            "title": "Super Resolution Graph With Conditional Normalizing Flows for Temporal Link Prediction",
            "abstract": "Temporal link prediction on dynamic graphs has attracted considerable attention. Most methods focus on the graph at each timestamp and extract features for prediction. As graphs are directly compressed into feature matrices, the important latent information at each timestamp has not been well revealed. Eventually, the acquisition of dynamic evolution-related patterns is rendered inadequately. In this paper, inspired by the process of Super-Resolution (SR), a novel deep generative model SRG (Super Resolution Graph) is proposed. We innovatively introduce the concepts of the Low-Resolution (LR) graph, which is a single adjacent matrix at a timestamp, and the High-Resolution (HR) graph, which includes the link status of surrounding snapshots. Specifically, two major aspects are considered regarding the construction of the HR graph. For edges, we endeavor to obtain an extensive information transmission description that affects the current link status. For nodes, similar to the SR process, the neighbor relationship among nodes is maintained. In this form, we could predict the link status from a new perspective: Under the supervision of the graph moving average strategy, the conditional normalizing flow effectively realizes the transformation between LR and HR graphs. Extensive experiments on six real-world datasets from different applications demonstrate the effectiveness of our proposal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223554832",
                    "name": "Yanting Yin"
                },
                {
                    "authorId": "2107922909",
                    "name": "Yajing Wu"
                },
                {
                    "authorId": "1860612",
                    "name": "Xuebing Yang"
                },
                {
                    "authorId": "2108167911",
                    "name": "Wensheng Zhang"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "52dd29bd1d968e5db8661aa0a9e1d3f5d6c8c827",
            "title": "MCIL: Multimodal Counterfactual Instance Learning for Low-resource Entity-based Multimodal Information Extraction",
            "abstract": "Multimodal information extraction (MIE) is a challenging task which aims to extract the structural information in free text coupled with the image for constructing the multimodal knowledge graph. The entity-based MIE tasks are based on the entity information to complete the specific tasks. However, the existing methods only investigated the entity-based MIE tasks under supervised learning with adequate labeled data. In the real-world scenario, collecting enough data and annotating the entity-based samples are time-consuming, and impractical. Therefore, we propose to investigate the entity-based MIE tasks under the low-resource settings. The conventional models are prone to overfitting on limited labeled data, which can result in poor performance. This is because the models tend to learn the bias existing in the limited samples, which can lead them to model the spurious correlations between multimodal features and task labels. To provide a more comprehensive understanding of the bias inherent in multimodal features of MIE samples, we decompose the features into image, entity, and context factors. Furthermore, we investigate the causal relationships between these factors and model performance, leveraging the structural causal model to delve into the correlations between the input features and output labels. Based on this, we propose the multimodal counterfactual instance learning framework to generate the counterfactual instances by the interventions on the limited observational samples. In the framework, we analyze the causal effect of the counterfactual instances and exploit it as a supervisory signal to maximize the effect for reducing the bias and improving the generalization of the model. Empirically, we evaluate the proposed method on the two public MIE benchmark datasets and the experimental results verify the effectiveness of it.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "2301654681",
                    "name": "Hongru Wang"
                },
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2168240441",
                    "name": "Xuhui Sui"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "8c2cf43026879adf5d0faf0ab041fe61a63c73d5",
            "title": "Look before You Leap: Dual Logical Verification for Knowledge-based Visual Question Generation",
            "abstract": "Knowledge-based Visual Question Generation aims to generate visual questions with outside knowledge other than the image. Existing approaches are answer-aware, which incorporate answers into the question-generation process. However, these methods just focus on leveraging the semantics of inputs to propose questions, ignoring the logical coherence among generated questions (Q), images (V), answers (A), and corresponding acquired outside knowledge (K). It results in generating many non-expected questions with low quality, lacking insight and diversity, and some of them are even without any corresponding answer. To address this issue, we inject logical verification into the processes of knowledge acquisition and question generation, which is defined as LV\u02c62-Net. Through checking the logical structure among V, A, K, ground-truth and generated Q twice in the whole KB-VQG procedure, LV\u02c62-Net can propose diverse and insightful knowledge-based visual questions. And experimental results on two commonly used datasets demonstrate the superiority of LV\u02c62-Net. Our code will be released to the public soon.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2162443483",
                    "name": "Xumeng Liu"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2240858056",
                    "name": "Xubo Liu"
                },
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "50882212",
                    "name": "Shenglong Yu"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "8fdafdd43a224b0d9cfd8fb5bf9cae60a582067e",
            "title": "Bring Invariant to Variant: A Contrastive Prompt-based Framework for Temporal Knowledge Graph Forecasting",
            "abstract": "Temporal knowledge graph forecasting aims to reason over known facts to complete the missing links in the future. Existing methods are highly dependent on the structures of temporal knowledge graphs and commonly utilize recurrent or graph neural networks for forecasting. However, entities that are infrequently observed or have not been seen recently face challenges in learning effective knowledge representations due to insufficient structural contexts. To address the above disadvantages, in this paper, we propose a Contrastive Prompt-based framework with Entity background information for TKG forecasting, which we named CoPET. Specifically, to bring the time-invariant entity background information to time-variant structural information, we employ a dual encoder architecture consisting of a candidate encoder and a query encoder. A contrastive learning framework is used to encourage the query representation to be closer to the candidate representation. We further propose three kinds of trainable time-variant prompts aimed at capturing temporal structural information. Experiments on two datasets demonstrate that our method is effective and stays competitive in inference with limited structural information. Our code is available at https://github.com/qianxinying/CoPET.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2301634520",
                    "name": "Xinying Qian"
                },
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "0941d750525d6dcea5c4a497ea8d80bd4e7bfba3",
            "title": "Incorporating Object-Level Visual Context for Multimodal Fine-Grained Entity Typing",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2273905684",
                    "name": "Wenbo Fan"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2168240441",
                    "name": "Xuhui Sui"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "221e5e14f14537bb65efec79f60aad5010834a66",
            "title": "AoM: Detecting Aspect-oriented Information for Multimodal Aspect-Based Sentiment Analysis",
            "abstract": "Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects from text-image pairs and recognize their sentiments. Existing methods make great efforts to align the whole image to corresponding aspects. However, different regions of the image may relate to different aspects in the same sentence, and coarsely establishing image-aspect alignment will introduce noise to aspect-based sentiment analysis (i.e., visual noise). Besides, the sentiment of a specific aspect can also be interfered by descriptions of other aspects (i.e., textual noise). Considering the aforementioned noises, this paper proposes an Aspect-oriented Method (AoM) to detect aspect-relevant semantic and sentiment information. Specifically, an aspect-aware attention module is designed to simultaneously select textual tokens and image blocks that are semantically related to the aspects. To accurately aggregate sentiment information, we explicitly introduce sentiment embedding into AoM, and use a graph convolutional network to model the vision-text and text-text interaction. Extensive experiments demonstrate the superiority of AoM to existing methods. The source code is publicly released at https://github.com/SilyRab/AoM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219327022",
                    "name": "Ru Zhou"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "2162443483",
                    "name": "Xumeng Liu"
                },
                {
                    "authorId": "50882212",
                    "name": "Shenglong Yu"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "69e5e4f61a68d5b39f889aba5514673631673cb2",
            "title": "HGWaveNet: A Hyperbolic Graph Neural Network for Temporal Link Prediction",
            "abstract": "Temporal link prediction, aiming to predict future edges between paired nodes in a dynamic graph, is of vital importance in diverse applications. However, existing methods are mainly built upon uniform Euclidean space, which has been found to be conflict with the power-law distributions of real-world graphs and unable to represent the hierarchical connections between nodes effectively. With respect to the special data characteristic, hyperbolic geometry offers an ideal alternative due to its exponential expansion property. In this paper, we propose HGWaveNet, a novel hyperbolic graph neural network that fully exploits the fitness between hyperbolic spaces and data distributions for temporal link prediction. Specifically, we design two key modules to learn the spatial topological structures and temporal evolutionary information separately. On the one hand, a hyperbolic diffusion graph convolution (HDGC) module effectively aggregates information from a wider range of neighbors. On the other hand, the internal order of causal correlation between historical states is captured by hyperbolic dilated causal convolution (HDCC) modules. The whole model is built upon the hyperbolic spaces to preserve the hierarchical structural information in the entire data flow. To prove the superiority of HGWaveNet, extensive experiments are conducted on six real-world graph datasets and the results show a relative improvement by up to 6.67% on AUC for temporal link prediction over SOTA methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "107674597",
                    "name": "Qijie Bai"
                },
                {
                    "authorId": "2056561563",
                    "name": "Chang Nie"
                },
                {
                    "authorId": "46702498",
                    "name": "Haiwei Zhang"
                },
                {
                    "authorId": "2110996023",
                    "name": "Dongming Zhao"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "8ceddb1ca62f6ddf3e4999c85c19410016af9585",
            "title": "TreeMAN: Tree-enhanced Multimodal Attention Network for ICD Coding",
            "abstract": "ICD coding is designed to assign the disease codes to electronic health records (EHRs) upon discharge, which is crucial for billing and clinical statistics. In an attempt to improve the effectiveness and efficiency of manual coding, many methods have been proposed to automatically predict ICD codes from clinical notes. However, most previous works ignore the decisive information contained in structured medical data in EHRs, which is hard to be captured from the noisy clinical notes. In this paper, we propose a Tree-enhanced Multimodal Attention Network (TreeMAN) to fuse tabular features and textual features into multimodal representations by enhancing the text representations with tree-based features via the attention mechanism. Tree-based features are constructed according to decision trees learned from structured multimodal medical data, which capture the decisive information about ICD coding. We can apply the same multi-label classifier from previous text models to the multimodal representations to predict ICD codes. Experiments on two MIMIC datasets show that our method outperforms prior state-of-the-art ICD coding approaches. The code is available at https://github.com/liu-zichen/TreeMAN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117942578",
                    "name": "Zichen Liu"
                },
                {
                    "authorId": "2108736365",
                    "name": "Xuyuan Liu"
                },
                {
                    "authorId": "2311315",
                    "name": "Yanlong Wen"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "48570355",
                    "name": "Fen Xia"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "9823e7d0bc60968e0bc793593811092c8a0146ef",
            "title": "Selecting Key Views for Zero-Shot Entity Linking",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2168240441",
                    "name": "Xuhui Sui"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                },
                {
                    "authorId": "2273574757",
                    "name": "Wensheng Zhang"
                }
            ]
        }
    ]
}