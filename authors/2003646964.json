{
    "authorId": "2003646964",
    "papers": [
        {
            "paperId": "3f27a7dad2f93c2b2ea8ad4719f8099bc386e8dc",
            "title": "Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks",
            "abstract": "Searching troves of videos with textual descriptions is a core multimodal retrieval task. Owing to the lack of a purpose-built dataset for text-to-video retrieval, video captioning datasets have been re-purposed to evaluate models by (1) treating captions as positive matches to their respective videos and (2) assuming all other videos to be negatives. However, this methodology leads to a fundamental flaw during evaluation: since captions are marked as relevant only to their original video, many alternate videos also match the caption, which introduces false-negative caption-video pairs. We show that when these false negatives are corrected, a recent state-of-the-art model gains 25% recall points\u2014a difference that threatens the validity of the benchmark itself. To diagnose and mitigate this issue, we annotate and release 683K additional caption-video pairs. Using these, we recompute effectiveness scores for three models on two standard benchmarks (MSR-VTT and MSVD). We find that (1) the recomputed metrics are up to 25% recall points higher for the best models, (2) these benchmarks are nearing saturation for Recall@10, (3) caption length (generality) is related to the number of positives, and (4) annotation costs can be mitigated through sampling. We recommend retiring these benchmarks in their current form, and we make recommendations for future text-to-video retrieval benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145009056",
                    "name": "Pedro Rodriguez"
                },
                {
                    "authorId": "2065276988",
                    "name": "Mahmoud Azab"
                },
                {
                    "authorId": "2003646964",
                    "name": "Becka Silvert"
                },
                {
                    "authorId": "2187443212",
                    "name": "Renato Sanchez"
                },
                {
                    "authorId": "2187431339",
                    "name": "Linzy Labson"
                },
                {
                    "authorId": "2187300386",
                    "name": "Hardik Shah"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                }
            ]
        },
        {
            "paperId": "3c64247b4c75c64b4a67d306200bab5b5d3b5bfa",
            "title": "Adding Chit-Chat to Enhance Task-Oriented Dialogues",
            "abstract": "Existing dialogue corpora and models are typically designed under two disjoint motives: while task-oriented systems focus on achieving functional goals (e.g., booking hotels), open-domain chatbots aim at making socially engaging conversations. In this work, we propose to integrate both types of systems by Adding Chit-Chat to ENhance Task-ORiented dialogues (ACCENTOR), with the goal of making virtual assistant conversations more engaging and interactive. Specifically, we propose a Human <-> AI collaborative data collection approach for generating diverse chit-chat responses to augment task-oriented dialogues with minimal annotation effort. We then present our new chit-chat-based annotations to 23.8K dialogues from two popular task-oriented datasets (Schema-Guided Dialogue and MultiWOZ 2.1) and demonstrate their advantage over the originals via human evaluation. Lastly, we propose three new models for adding chit-chat to task-oriented dialogues, explicitly trained to predict user goals and to generate contextually relevant chit-chat responses. Automatic and human evaluations show that, compared with the state-of-the-art task-oriented baseline, our models can code-switch between task and chit-chat to be more engaging, interesting, knowledgeable, and humanlike, while maintaining competitive task performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49871029",
                    "name": "Kai Sun"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                },
                {
                    "authorId": "3849208",
                    "name": "Stephen Roller"
                },
                {
                    "authorId": "2003646964",
                    "name": "Becka Silvert"
                },
                {
                    "authorId": "47655430",
                    "name": "Bing Liu"
                },
                {
                    "authorId": "40623617",
                    "name": "Zhiguang Wang"
                },
                {
                    "authorId": "2109023202",
                    "name": "Honglei Liu"
                },
                {
                    "authorId": "34685327",
                    "name": "Eunjoon Cho"
                },
                {
                    "authorId": "1748501",
                    "name": "Claire Cardie"
                }
            ]
        },
        {
            "paperId": "c23893e334d7237793ba83ee96620e8c9c0e1168",
            "title": "Adding Chit-Chats to Enhance Task-Oriented Dialogues",
            "abstract": "The existing dialogue corpora and models are typically designed under two disjoint motives: while task-oriented systems focus on achieving functional goals (e.g., booking hotels), open-domain chatbots aim at making socially engaging conversations. In this work, we propose to integrate both types of systems by Adding Chit-Chats to ENhance Task-ORiented dialogues (ACCENTOR), with the goal of making virtual assistant conversations more engaging and interactive. Specifically, we propose a flexible approach for generating diverse chit-chat responses to augment task-oriented dialogues with minimal annotation effort. We then present our new chit-chat annotations to 23.8K dialogues from the popular task-oriented datasets (Schema-Guided Dialogue and MultiWOZ 2.1) and demonstrate their advantage over the originals via human evaluation. Lastly, we propose three new models for ACCENTOR explicitly trained to predict user goals and to generate contextually relevant chit-chat responses. Automatic and human evaluations show that, compared with the state-of-the-art task-oriented baseline, our models can code-switch between task and chit-chat to be more engaging, interesting, knowledgeable, and humanlike, while maintaining competitive task performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257202854",
                    "name": "Kai Sun"
                },
                {
                    "authorId": "2256132675",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                },
                {
                    "authorId": "3849208",
                    "name": "Stephen Roller"
                },
                {
                    "authorId": "2003646964",
                    "name": "Becka Silvert"
                },
                {
                    "authorId": "2257401374",
                    "name": "Bing Liu"
                },
                {
                    "authorId": "2108218300",
                    "name": "Zhiguang Wang"
                },
                {
                    "authorId": "2257374859",
                    "name": "Honglei Liu"
                },
                {
                    "authorId": "34685327",
                    "name": "Eunjoon Cho"
                },
                {
                    "authorId": "2064285348",
                    "name": "Claire Cardie"
                }
            ]
        }
    ]
}