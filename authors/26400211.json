{
    "authorId": "26400211",
    "papers": [
        {
            "paperId": "97ec7cf78c6f0b8a9cd9f4afdccc369ec3898f19",
            "title": "On Advances in Text Generation from Images Beyond Captioning: A Case Study in Self-Rationalization",
            "abstract": "Combining the visual modality with pretrained language models has been surprisingly effective for simple descriptive tasks such as image captioning. More general text generation however remains elusive. We take a step back and ask: How do these models work for more complex generative tasks, i.e. conditioning on both text and images? Are multimodal models simply visually adapted language models, or do they combine they reason jointly over modalities? We investigate these questions in the context of self-rationalization (jointly generating task labels/answers and free-text explanations) of three tasks: (i) visual question answering in VQA-X, (ii) visual commonsense reasoning in VCR, and (iii) visual-textual entailment in e-SNLI-VE. We show that recent unimodal advances, CLIP image representations and scaling of language models, do not consistently improve self-rationalization in multimodal tasks. We find that no single model type works universally best across tasks, datasets, and finetuning data sizes. Our findings motivate the need for novel general backbones approach that move text generation from images and text beyond image captioning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26400211",
                    "name": "Shruti Palaskar"
                },
                {
                    "authorId": "2166136235",
                    "name": "Akshita Bhagia"
                },
                {
                    "authorId": "3312309",
                    "name": "Yonatan Bisk"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "1690706",
                    "name": "A. Black"
                },
                {
                    "authorId": "3451494",
                    "name": "Ana Marasovi\u0107"
                }
            ]
        },
        {
            "paperId": "20ba6a7d6db49641bc40e70ca374644bec6cae05",
            "title": "End-to-End Speech Summarization Using Restricted Self-Attention",
            "abstract": "Speech summarization is typically performed by using a cascade of speech recognition and text summarization models. End-to-end modeling of speech summarization models is challenging due to memory and compute constraints arising from long input audio sequences. Recent work in document summarization has inspired methods to reduce the complexity of self-attentions, which enables transformer models to handle long sequences. In this work, we introduce a single model optimized end-to-end for speech summarization. We apply the restricted self-attention technique from text-based models to speech models to address the memory and compute constraints. We demonstrate that the proposed model learns to directly summarize speech for the How-2 corpus of instructional videos. The proposed end-to-end model outperforms the previously proposed cascaded model by 3 points absolute on ROUGE. Further, we consider the spoken language understanding task of predicting concepts from speech inputs and show that the proposed end-to-end model outperforms the cascade model by 4 points absolute F-1.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "145521253",
                    "name": "Roshan Sharma"
                },
                {
                    "authorId": "26400211",
                    "name": "Shruti Palaskar"
                },
                {
                    "authorId": "1690706",
                    "name": "A. Black"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                }
            ]
        },
        {
            "paperId": "297b515fa8e5a129d757dd48065e51656dae55ba",
            "title": "Multimodal Speech Summarization Through Semantic Concept Learning",
            "abstract": "We propose a cascaded multimodal abstractive speech summarization model that generates semantic concepts as an intermediate step towards summarization. We describe a method to leverage existing multimodal dataset annotations to curate groundtruth labels for such intermediate concept modeling. In addition to cascaded training, the concept labels also provide an interpretable intermediate output level that helps improve performance on the downstream summarization task. On the open-domain How2 data, we conduct utterance-level and video-level experiments for two granularities of concepts: Speci\ufb01c and Abstract. We compare various multimodal fusion models for concept generation based on the respective input modalities. We observe consistent improvements in concept modeling by using multimodal adaptation models over unimodal models. Using the cascaded multimodal speech summarization model, we see a signi\ufb01cant improvement of 7.5 METEOR points and 5.1 ROUGE-L points compared to previous methods of speech summarization. Finally, we show the bene\ufb01ts of scalability of the proposed approaches on 2000h of video data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26400211",
                    "name": "Shruti Palaskar"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                },
                {
                    "authorId": "1690706",
                    "name": "A. Black"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                }
            ]
        },
        {
            "paperId": "1353db0b6534fdf49a1c554da413705ce580deb9",
            "title": "Towards Understanding ASR Error Correction for Medical Conversations",
            "abstract": "Domain Adaptation for Automatic Speech Recognition (ASR) error correction via machine translation is a useful technique for improving out-of-domain outputs of pre-trained ASR systems to obtain optimal results for specific in-domain tasks. We use this technique on our dataset of Doctor-Patient conversations using two off-the-shelf ASR systems: Google ASR (commercial) and the ASPIRE model (open-source). We train a Sequence-to-Sequence Machine Translation model and evaluate it on seven specific UMLS Semantic types, including Pharmacological Substance, Sign or Symptom, and Diagnostic Procedure to name a few. Lastly, we breakdown, analyze and discuss the 7% overall improvement in word error rate in view of each Semantic type.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1566609731",
                    "name": "Anirudh Mani"
                },
                {
                    "authorId": "26400211",
                    "name": "Shruti Palaskar"
                },
                {
                    "authorId": "2249518",
                    "name": "Sandeep Konam"
                }
            ]
        },
        {
            "paperId": "14aadd24040edaa9ce9978b53b00aeede015f859",
            "title": "Grounded Sequence to Sequence Transduction",
            "abstract": "Speech recognition and machine translation have made major progress over the past decades, providing practical systems to map one language sequence to another. Although multiple modalities such as sound and video are becoming increasingly available, the state-of-the-art systems are inherently unimodal, in the sense that they take a single modality \u2014 either speech or text \u2014 as input. Evidence from human learning suggests that additional modalities can provide disambiguating signals crucial for many language tasks. In this article, we describe the How2 dataset\u00a0, a large, open-domain collection of videos with transcriptions and their translations. We then show how this single dataset can be used to develop systems for a variety of language tasks and present a number of models meant as starting points. Across tasks, we find that building multimodal architectures that perform better than their unimodal counterpart remains a challenge. This leaves plenty of room for the exploration of more advanced solutions that fully exploit the multimodal nature of the How2 dataset\u00a0, and the general direction of multimodal learning with other datasets as well.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702974",
                    "name": "Lucia Specia"
                },
                {
                    "authorId": "143886499",
                    "name": "Raman Arora"
                },
                {
                    "authorId": "2934336",
                    "name": "Lo\u00efc Barrault"
                },
                {
                    "authorId": "10791325",
                    "name": "Ozan Caglayan"
                },
                {
                    "authorId": "1381525702",
                    "name": "A. Duarte"
                },
                {
                    "authorId": "50369944",
                    "name": "Desmond Elliott"
                },
                {
                    "authorId": "2921001",
                    "name": "Spandana Gella"
                },
                {
                    "authorId": "2269457539",
                    "name": "Nils Holzenberger"
                },
                {
                    "authorId": "1908331",
                    "name": "Chiraag Lala"
                },
                {
                    "authorId": "2108140944",
                    "name": "S. Lee"
                },
                {
                    "authorId": "3448602",
                    "name": "Jind\u0159ich Libovick\u00fd"
                },
                {
                    "authorId": "3238408",
                    "name": "P. Madhyastha"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "2065159752",
                    "name": "Karl Mulligan"
                },
                {
                    "authorId": "1752987954",
                    "name": "Alissa Ostapenka"
                },
                {
                    "authorId": "26400211",
                    "name": "Shruti Palaskar"
                },
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "2635321",
                    "name": "Josiah Wang"
                }
            ]
        },
        {
            "paperId": "932168d1c27390506049adf679b42d3aac53f61c",
            "title": "How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language",
            "abstract": "One of the factors that have hindered progress in the areas of sign language recognition, translation, and production is the absence of large annotated datasets. Towards this end, we introduce How2Sign, a multimodal and multiview continuous American Sign Language (ASL) dataset, consisting of a parallel corpus of more than 80 hours of sign language videos and a set of corresponding modalities including speech, English transcripts, and depth. A three-hour subset was further recorded in the Panoptic studio enabling detailed 3D pose estimation. To evaluate the potential of How2Sign for real-world impact, we conduct a study with ASL signers and show that synthesized videos using our dataset can indeed be understood. The study further gives insights on challenges that computer vision should address in order to make progress in this field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1381525702",
                    "name": "A. Duarte"
                },
                {
                    "authorId": "26400211",
                    "name": "Shruti Palaskar"
                },
                {
                    "authorId": "2028234",
                    "name": "Deepti Ghadiyaram"
                },
                {
                    "authorId": "1884668614",
                    "name": "Kenneth DeHaan"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "147166602",
                    "name": "Jordi Torres"
                },
                {
                    "authorId": "1398090762",
                    "name": "Xavier Gir\u00f3-i-Nieto"
                }
            ]
        },
        {
            "paperId": "9e5cf1163da6d4eed550d09e33724a278f202ba4",
            "title": "ASR Error Correction and Domain Adaptation Using Machine Translation",
            "abstract": "Off-the-shelf pre-trained Automatic Speech Recognition (ASR) systems are an increasingly viable service for companies of any size building speech-based products. While these ASR systems are trained on large amounts of data, domain mismatch is still an issue for many such parties that want to use this service as-is leading to not so optimal results for their task. We propose a simple technique to perform domain adaptation for ASR error correction via machine translation. The machine translation model is a strong candidate to learn a mapping from out-of-domain ASR errors to in-domain terms in the corresponding reference files. We use two off-the-shelf ASR systems in this work: Google ASR (commercial) and the ASPIRE model (open-source). We observe 7% absolute improvement in word error rate and 4 point absolute improvement in BLEU score in Google ASR output via our proposed method. We also evaluate ASR error correction via a downstream task of Speaker Diarization that captures speaker style, syntax, structure and semantic improvements we obtain via ASR correction.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1566609731",
                    "name": "Anirudh Mani"
                },
                {
                    "authorId": "26400211",
                    "name": "Shruti Palaskar"
                },
                {
                    "authorId": "1566528965",
                    "name": "Nimshi Venkat Meripo"
                },
                {
                    "authorId": "2249518",
                    "name": "Sandeep Konam"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                }
            ]
        },
        {
            "paperId": "ec6499842d3e51b7dda94f5d0620d6df5c1a1b6d",
            "title": "Speech Technology for Unwritten Languages",
            "abstract": "Speech technology plays an important role in our everyday life. Among others, speech is used for human-computer interaction, for instance for information retrieval and on-line shopping. In the case of an unwritten language, however, speech technology is unfortunately difficult to create, because it cannot be created by the standard combination of pre-trained speech-to-text and text-to-speech subsystems. The research presented in this article takes the first steps towards speech technology for unwritten languages. Specifically, the aim of this work was 1) to learn speech-to-meaning representations without using text as an intermediate representation, and 2) to test the sufficiency of the learned representations to regenerate speech or translated text, or to retrieve images that depict the meaning of an utterance in an unwritten language. The results suggest that building systems that go directly from speech-to-meaning and from meaning-to-speech, bypassing the need for text, is possible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1700735",
                    "name": "O. Scharenborg"
                },
                {
                    "authorId": "2167829",
                    "name": "Lucas Ondel"
                },
                {
                    "authorId": "26400211",
                    "name": "Shruti Palaskar"
                },
                {
                    "authorId": "144332819",
                    "name": "Philip Arthur"
                },
                {
                    "authorId": "35426982",
                    "name": "Francesco Ciannella"
                },
                {
                    "authorId": "2054401198",
                    "name": "Mingxing Du"
                },
                {
                    "authorId": "40651102",
                    "name": "Elin Larsen"
                },
                {
                    "authorId": "35833790",
                    "name": "Danny Merkx"
                },
                {
                    "authorId": "40425637",
                    "name": "Rachid Riad"
                },
                {
                    "authorId": "2109120538",
                    "name": "Liming Wang"
                },
                {
                    "authorId": "2202008",
                    "name": "Emmanuel Dupoux"
                },
                {
                    "authorId": "143823463",
                    "name": "L. Besacier"
                },
                {
                    "authorId": "1690706",
                    "name": "A. Black"
                },
                {
                    "authorId": "1399115926",
                    "name": "M. Hasegawa-Johnson"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                },
                {
                    "authorId": "102181710",
                    "name": "S. Stueker"
                },
                {
                    "authorId": "2058749172",
                    "name": "Pierre Godard"
                },
                {
                    "authorId": "2116238031",
                    "name": "Markus Mueller"
                }
            ]
        },
        {
            "paperId": "182243acb998b1f84c64643dd0a9be54fe927c5d",
            "title": "Learned in Speech Recognition: Contextual Acoustic Word Embeddings",
            "abstract": "End-to-end acoustic-to-word speech recognition models have recently gained popularity because they are easy to train, scale well to large amounts of training data, and do not require a lexicon. In addition, word models may also be easier to integrate with downstream tasks such as spoken language understanding, because inference (search) is much simplified compared to phoneme, character or any other sort of sub-word units. In this paper, we describe methods to construct contextual acoustic word embeddings directly from a supervised sequence-to-sequence acoustic-to-word speech recognition model using the learned attention distribution. On a suite of 16 standard sentence evaluation tasks, our embeddings show competitive performance against a word2vec model trained on the speech transcriptions. In addition, we evaluate these embeddings on a spoken language understanding task, and observe that our embeddings match the performance of text-based embeddings in a pipeline of first performing speech recognition and then constructing word embeddings from transcriptions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "26400211",
                    "name": "Shruti Palaskar"
                },
                {
                    "authorId": "24025563",
                    "name": "Vikas Raunak"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                }
            ]
        },
        {
            "paperId": "37cc2c54cc60ee1301baca2d95bf003c76dd07d5",
            "title": "Multimodal Abstractive Summarization for How2 Videos",
            "abstract": "In this paper, we study abstractive summarization for open-domain videos. Unlike the traditional text news summarization, the goal is less to \u201ccompress\u201d text information but rather to provide a fluent textual summary of information that has been collected and fused from different source modalities, in our case video and audio transcripts (or text). We show how a multi-source sequence-to-sequence model with hierarchical attention can integrate information from different modalities into a coherent output, compare various models trained with different modalities and present pilot experiments on the How2 corpus of instructional videos. We also propose a new evaluation metric (Content F1) for abstractive summarization task that measures semantic adequacy rather than fluency of the summaries, which is covered by metrics like ROUGE and BLEU.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26400211",
                    "name": "Shruti Palaskar"
                },
                {
                    "authorId": "3448602",
                    "name": "Jind\u0159ich Libovick\u00fd"
                },
                {
                    "authorId": "2921001",
                    "name": "Spandana Gella"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                }
            ]
        }
    ]
}