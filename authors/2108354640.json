{
    "authorId": "2108354640",
    "papers": [
        {
            "paperId": "9ad6c4aeb2ce9bc72ed3c839deeb65d2879cafd7",
            "title": "AIMDiT: Modality Augmentation and Interaction via Multimodal Dimension Transformation for Emotion Recognition in Conversations",
            "abstract": "Emotion Recognition in Conversations (ERC) is a popular task in natural language processing, which aims to recognize the emotional state of the speaker in conversations. While current research primarily emphasizes contextual modeling, there exists a dearth of investigation into effective multimodal fusion methods. We propose a novel framework called AIMDiT to solve the problem of multimodal fusion of deep features. Specifically, we design a Modality Augmentation Network which performs rich representation learning through dimension transformation of different modalities and parameter-efficient inception block. On the other hand, the Modality Interaction Network performs interaction fusion of extracted inter-modal features and intra-modal features. Experiments conducted using our AIMDiT framework on the public benchmark dataset MELD reveal 2.34% and 2.87% improvements in terms of the Acc-7 and w-F1 metrics compared to the state-of-the-art (SOTA) models.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2309205750",
                    "name": "Sheng Wu"
                },
                {
                    "authorId": "2108354640",
                    "name": "Jiaxing Liu"
                },
                {
                    "authorId": "2277569329",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                }
            ]
        },
        {
            "paperId": "e2fc375fb7c8143d2f581081889c232f42b13909",
            "title": "Adversarial Domain Generalized Transformer for Cross-Corpus Speech Emotion Recognition",
            "abstract": "Speech emotion recognition (SER) promotes the development of intelligent devices, which enable natural and friendly human-computer interactions. However, the recognition performance of existing approaches is significantly reduced on unseen datasets, and the lack of sufficient training data limits the generalizability of deep learning models. In this article, we analyze the impact of the domain generalization method on cross-corpus SER and propose an adversarial domain generalized transformer (ADoGT), which is aimed at learning a shared feature distribution for the source and target domains. Specifically, we investigate the effect of domain adversarial learning by eliminating nonaffective information. We also combine the center loss with the softmax function as joint supervision to learn discriminative features. Moreover, we introduce unsupervised transfer learning to extract additional features, and incorporate a gated fusion model to learn the complementary information of the features learned by the supervised feature extractor and pretrained model. The proposed transformer based domain generalization method is evaluated using four emotional datasets. We also provide an ablation study of different domain adversarial model structures and feature fusion models. The results of comparative experiments demonstrate the effectiveness of the proposed ADoGT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145975833",
                    "name": "Yuan Gao"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2108354640",
                    "name": "Jiaxing Liu"
                },
                {
                    "authorId": "144699460",
                    "name": "J. Dang"
                },
                {
                    "authorId": "93382502",
                    "name": "S. Okada"
                }
            ]
        },
        {
            "paperId": "c8e5e729b1a25a988d67498bd37c71ef854e8a5d",
            "title": "Improving Fine-tuning Pre-trained Models on Small Source Code Datasets via Variational Information Bottleneck",
            "abstract": "Small datasets are common in software engineering tasks such as linguistic smell detection and code runtime complexity prediction, as crafting these datasets often involves expert knowledge. Prior work usually applies machine learning algorithms (e.g., logistic regression and SVM) with hand-crafted features to tackle them, which could outperform neural models such as CNN. Recently, researchers have employed fine-tuning large pre-trained code models on various code-related tasks thanks to their transferability. However, it might be still instable and overfitting when fine-tuning on small datasets. In this paper, we firstly conduct an empirical study to fine-tune CodeBERT(a) on four code-related small datasets and observe the instability phenomenon. This could be induced by over-capacity and irrelevant features inherent in these large pre-trained code models with respective to those small datasets. To address this issue, we leverage variational information bottleneck to filter out irrelevant features when fine-tuning the models. The experiments demonstrate the out-performance of our method compared to standard fine-tuning and regularization method such as dropout and weight decay. We also experimentally study the stability of our method through varying dataset sizes. Our code and data are available at https://github.com/little-pikachu-hash/VIBCodeBERT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108354640",
                    "name": "Jiaxing Liu"
                },
                {
                    "authorId": "2976257",
                    "name": "Chaofeng Sha"
                },
                {
                    "authorId": "2115816603",
                    "name": "Xin Peng"
                }
            ]
        },
        {
            "paperId": "47271c72a7f11c7f45719c5a6de71fadfaf1405f",
            "title": "Emotion Recognition With Multimodal Transformer Fusion Framework Based on Acoustic and Lexical Information",
            "abstract": "People usually express emotions through paralinguistic and linguistic information in speech. How to effectively integrate linguistic and paralinguistic information for emotion recognition is a challenge. Previous studies have adopted the bidirectional long short-term memory (BLSTM) network to extract acoustic and lexical representations followed by a concatenate layer, and this has become a common method. However, the interaction and influence between different modalities are difficult to promote using simple feature fusion for each sentence. In this article, we propose an implicitly aligned multimodal transformer fusion (IA-MMTF) framework based on acoustic features and text information. This model enables the two modalities to guide and complement each other when learning emotional representations. Thereafter, the weighed fusion is used to control the contributions of different modalities. Thus, we can obtain more complementary emotional representations. Experiments on the interactive emotional dyadic motion capture (IEMOCAP) database and multimodal emotionlines dataset (MELD) show that the proposed method outperforms the baseline BLSTM-based method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49868446",
                    "name": "Lili Guo"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "144699460",
                    "name": "J. Dang"
                },
                {
                    "authorId": "2117787124",
                    "name": "Yahui Fu"
                },
                {
                    "authorId": "2108354640",
                    "name": "Jiaxing Liu"
                },
                {
                    "authorId": "2093193710",
                    "name": "Shifei Ding"
                }
            ]
        },
        {
            "paperId": "7c5a23eefacb00c024a0c50678c23837f8ff0de9",
            "title": "Multi-Stage Graph Representation Learning for Dialogue-Level Speech Emotion Recognition",
            "abstract": "With the development of speech emotion recognition (SER), most of current research is utterance-level and cannot fit the need of actual scenarios. In this paper, we propose a novel strategy that focuses on capturing dialogue-level contextual information. On the basis of utterance-level representation learned by convolutional neural network (CNN) which is followed by the bidirectional long short-term memory network (BLSTM), the proposed dialogue-level method consists of two modules. The first module is Dialogue Multi-stage Graph Representation Learning Algorithm (DialogMSG). The multi-stage graph that modeling from different dialogue scope is introduced to capture more effective information. The other one is a double-constrained module. This module includes not only an utterance-level classifier but also a dialogue-level graph classifier which is named as Atmosphere. The results of extensive experiments show that the proposed method outperforms the current state of the art on the IEMOCAP benchmark dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "78017806",
                    "name": "Yaodong Song"
                },
                {
                    "authorId": "2108354640",
                    "name": "Jiaxing Liu"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2112770579",
                    "name": "Ruiguo Yu"
                },
                {
                    "authorId": "144699460",
                    "name": "J. Dang"
                }
            ]
        },
        {
            "paperId": "8a8ee1d8c39485eae914d35e54552a93d2e16cd9",
            "title": "Domain-Invariant Feature Learning for Cross Corpus Speech Emotion Recognition",
            "abstract": "To deal with speech emotion recognition (SER) in real-life applications, researchers have to focus on cross corpus SER, where the feature distribution of source and target datasets are different. In this paper, we propose an efficient domain adversarial training method to cope with the non-affective information during feature extraction. Through the proposed domain-adversarial learning, we can reduce the domain divergence between train and test data. Furthermore, we incorporate center loss with the emotion classifier to reduce the intra-class variation of features learned from the same emotion. We conduct experiments on four emotional benchmark datasets to verify the performance of the proposed method. The experimental results demonstrate that our proposed model outperform the baseline system in both cross-corpus and multi-corpus evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145975833",
                    "name": "Yuan Gao"
                },
                {
                    "authorId": "93382502",
                    "name": "S. Okada"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2108354640",
                    "name": "Jiaxing Liu"
                },
                {
                    "authorId": "144699460",
                    "name": "J. Dang"
                }
            ]
        },
        {
            "paperId": "d6b00ac9f39e7d80424c23fd7bf189f8ebe67e20",
            "title": "Talking Head Generation Driven by Speech-Related Facial Action Units and Audio- Based on Multimodal Representation Fusion",
            "abstract": "Talking head generation is to synthesize a lip-synchronized talking head video by inputting an arbitrary face image and corresponding audio clips. Existing methods ignore not only the interaction and relationship of cross-modal information, but also the local driving information of the mouth muscles. In this study, we propose a novel generative framework that contains a dilated non-causal temporal convolutional self-attention network as a multimodal fusion module to promote the relationship learning of cross-modal features. In addition, our proposed method uses both audio- and speech-related facial action units (AUs) as driving information. Speech-related AU information can guide mouth movements more accurately. Because speech is highly correlated with speech-related AUs, we propose an audio-to-AU module to predict speech-related AU information. We utilize pre-trained AU classifier to ensure that the generated images contain correct AU information. We verify the effectiveness of the proposed model on the GRID and TCD-TIMIT datasets. An ablation study is also conducted to verify the contribution of each component. The results of quantitative and qualitative experiments demonstrate that our method outperforms existing methods in terms of both image quality and lip-sync accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2133441782",
                    "name": "Sen Chen"
                },
                {
                    "authorId": "2109097810",
                    "name": "Zhilei Liu"
                },
                {
                    "authorId": "2108354640",
                    "name": "Jiaxing Liu"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                }
            ]
        },
        {
            "paperId": "da817e68a91b7a1d9013dd9425920953dae12281",
            "title": "Context- and Knowledge-Aware Graph Convolutional Network for Multimodal Emotion Recognition",
            "abstract": "This work proposes an approach for emotion recognition in conversation that leverages context modeling, knowledge enrichment, and multimodal (text and audio) learning based on a graph convolutional network (GCN). We first construct two distinctive graphs for modeling the contextual interaction and knowledge dynamic. We then introduce an affective lexicon into knowledge graph building to enrich the emotional polarity of each concept, that is the related knowledge of each token in an utterance. Then, we achieve a balance between the context and the affect-enriched knowledge by incorporating them into the new adjacency matrix construction of the GCN architecture, and teach them jointly with multiple modalities to effectively structure the semantics-sensitive and knowledge-sensitive contextual dependence of each conversation. Our model outperforms the state-of-the-art benchmarks by over 22.6% and 11% relative error reduction in terms of weighted-F1 on the IEMOCAP and MELD databases, respectively, demonstrating the superiority of our method in emotion recognition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117787124",
                    "name": "Yahui Fu"
                },
                {
                    "authorId": "93382502",
                    "name": "S. Okada"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "49868446",
                    "name": "Lili Guo"
                },
                {
                    "authorId": "78017806",
                    "name": "Yaodong Song"
                },
                {
                    "authorId": "2108354640",
                    "name": "Jiaxing Liu"
                },
                {
                    "authorId": "144699460",
                    "name": "J. Dang"
                }
            ]
        },
        {
            "paperId": "1efc99af715323b60f06e033cd124e2f8806f3d0",
            "title": "CONSK-GCN: Conversational Semantic- and Knowledge-Oriented Graph Convolutional Network for Multimodal Emotion Recognition",
            "abstract": "Emotion recognition in conversations (ERC) has received significant attention in recent years due to its widespread applications in diverse areas, such as social media, health care, and artificial intelligence interactions. However, different from nonconversational text, it is particularly challenging to model the effective context-aware dependence for the task of ERC. To address this problem, we propose a new Conversational Semantic- and Knowledge-oriented Graph Convolutional Network (ConSK-GCN) approach that leverages both semantic dependence and commonsense knowledge. First, we construct the contextual inter-interaction and intradependence of the interlocutors via a conversational graph-based convolutional network based on multimodal representations. Second, we incorporate commonsense knowledge to guide ConSK-GCN to model the semantic-sensitive and knowledge-sensitive contextual dependence. The results of extensive experiments show that the proposed method outperforms the current state of the art on the IEMOCAP dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117787124",
                    "name": "Yahui Fu"
                },
                {
                    "authorId": "93382502",
                    "name": "S. Okada"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "49868446",
                    "name": "Lili Guo"
                },
                {
                    "authorId": "78017806",
                    "name": "Yaodong Song"
                },
                {
                    "authorId": "2108354640",
                    "name": "Jiaxing Liu"
                },
                {
                    "authorId": "144699460",
                    "name": "J. Dang"
                }
            ]
        },
        {
            "paperId": "43f1e9001dbadc8cde28a5a135feb7fe142e95c9",
            "title": "Talking Head Generation with Audio and Speech Related Facial Action Units",
            "abstract": "The task of talking head generation is to synthesize a lip synchronized talking head video by inputting an arbitrary face image and audio clips. Most existing methods ignore the local driving information of the mouth muscles. In this paper, we propose a novel recurrent generative network that uses both audio and speech-related facial action units (AUs) as the driving information. AU information related to the mouth can guide the movement of the mouth more accurately. Since speech is highly correlated with speech-related AUs, we propose an Audio-to-AU module in our system to predict the speech-related AU information from speech. In addition, we use AU classifier to ensure that the generated images contain correct AU information. Frame discriminator is also constructed for adversarial training to improve the realism of the generated face. We verify the effectiveness of our model on the GRID dataset and TCD-TIMIT dataset. We also conduct an ablation study to verify the contribution of each component in our model. Quantitative and qualitative experiments demonstrate that our method outperforms existing methods in both image quality and lip-sync accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2133441782",
                    "name": "Sen Chen"
                },
                {
                    "authorId": "2109097810",
                    "name": "Zhilei Liu"
                },
                {
                    "authorId": "2108354640",
                    "name": "Jiaxing Liu"
                },
                {
                    "authorId": "2152532295",
                    "name": "Zhengxiang Yan"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                }
            ]
        }
    ]
}