{
    "authorId": "51208524",
    "papers": [
        {
            "paperId": "3414be052766667fe375221804e48f4a9c815ba5",
            "title": "Multimodal Large Language Models to Support Real-World Fact-Checking",
            "abstract": "Multimodal large language models (MLLMs) carry the potential to support humans in processing vast amounts of information. While MLLMs are already being used as a fact-checking tool, their abilities and limitations in this regard are understudied. Here is aim to bridge this gap. In particular, we propose a framework for systematically assessing the capacity of current multimodal models to facilitate real-world fact-checking. Our methodology is evidence-free, leveraging only these models' intrinsic knowledge and reasoning capabilities. By designing prompts that extract models' predictions, explanations, and confidence levels, we delve into research questions concerning model accuracy, robustness, and reasons for failure. We empirically find that (1) GPT-4V exhibits superior performance in identifying malicious and misleading multimodal claims, with the ability to explain the unreasonable aspects and underlying motives, and (2) existing open-source models exhibit strong biases and are highly sensitive to the prompt. Our study offers insights into combating false multimodal information and building secure, trustworthy multimodal models. To the best of our knowledge, we are the first to evaluate MLLMs for real-world fact-checking.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                }
            ]
        },
        {
            "paperId": "3aabd69e13f64f10fd210e4e9e6b2e75c0e734d1",
            "title": "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark",
            "abstract": "Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models to understand and reason on knowledge present in both visual and textual data. However, most of the current VQA models use datasets that are primarily focused on English and a few major world languages, with images that are typically Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, although these datasets often extend their linguistic range via translation or some other approaches, they usually keep images the same, resulting in narrow cultural representation. To address these limitations, we construct CVQA, a new Culturally-diverse multilingual Visual Question Answering benchmark, designed to cover a rich set of languages and cultures, where we engage native speakers and cultural experts in the data collection process. As a result, CVQA includes culturally-driven images and questions from across 28 countries on four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We then benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and show that the dataset is challenging for the current state-of-the-art models. This benchmark can serve as a probing evaluation suite for assessing the cultural capability and bias of multimodal models and hopefully encourage more research efforts toward increasing cultural awareness and linguistic diversity in this field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2305602940",
                    "name": "David Romero"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "49918371",
                    "name": "Haryo Akbarianto Wibowo"
                },
                {
                    "authorId": "2298756162",
                    "name": "Teresa Lynn"
                },
                {
                    "authorId": "3248560",
                    "name": "Injy Hamed"
                },
                {
                    "authorId": "2305619033",
                    "name": "Aditya Nanda Kishore"
                },
                {
                    "authorId": "2305622255",
                    "name": "Aishik Mandal"
                },
                {
                    "authorId": "2305619182",
                    "name": "Alina Dragonetti"
                },
                {
                    "authorId": "1396213362",
                    "name": "Artem Abzaliev"
                },
                {
                    "authorId": "2148631756",
                    "name": "A. Tonja"
                },
                {
                    "authorId": "2305622331",
                    "name": "Bontu Fufa Balcha"
                },
                {
                    "authorId": "2161240241",
                    "name": "Chenxi Whitehouse"
                },
                {
                    "authorId": "51124788",
                    "name": "Christian Salamea"
                },
                {
                    "authorId": "1994718316",
                    "name": "Dan John Velasco"
                },
                {
                    "authorId": "2273673245",
                    "name": "D. Adelani"
                },
                {
                    "authorId": "70145452",
                    "name": "D. Meur"
                },
                {
                    "authorId": "2183780558",
                    "name": "Emilio Villa-Cueva"
                },
                {
                    "authorId": "2789148",
                    "name": "Fajri Koto"
                },
                {
                    "authorId": "2266756359",
                    "name": "Fauzan Farooqui"
                },
                {
                    "authorId": "1738707459",
                    "name": "Frederico Belcavello"
                },
                {
                    "authorId": "2151970366",
                    "name": "Ganzorig Batnasan"
                },
                {
                    "authorId": "2305623074",
                    "name": "Gisela Vallejo"
                },
                {
                    "authorId": "2305619231",
                    "name": "Grainne Caulfield"
                },
                {
                    "authorId": "2213060824",
                    "name": "Guido Ivetta"
                },
                {
                    "authorId": "2980506",
                    "name": "Haiyue Song"
                },
                {
                    "authorId": "2305619369",
                    "name": "Henok Biadglign Ademtew"
                },
                {
                    "authorId": "2139773809",
                    "name": "Hern\u00e1n Maina"
                },
                {
                    "authorId": "116344405",
                    "name": "Holy Lovenia"
                },
                {
                    "authorId": "2304752238",
                    "name": "Israel Abebe Azime"
                },
                {
                    "authorId": "2282499634",
                    "name": "Jan Christian Blaise Cruz"
                },
                {
                    "authorId": "1992915388",
                    "name": "Jay Gala"
                },
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "1724941617",
                    "name": "Jes\u00fas-Germ\u00e1n Ortiz-Barajas"
                },
                {
                    "authorId": "90765684",
                    "name": "Jinheon Baek"
                },
                {
                    "authorId": "2305082736",
                    "name": "Jocelyn Dunstan"
                },
                {
                    "authorId": "2276687",
                    "name": "L. A. Alemany"
                },
                {
                    "authorId": "2290013575",
                    "name": "Kumaranage Ravindu Yasas Nagasinghe"
                },
                {
                    "authorId": "2066254822",
                    "name": "Luciana Benotti"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1738707461",
                    "name": "Marcelo Viridiano"
                },
                {
                    "authorId": "2168569460",
                    "name": "Marcos Estecha-Garitagoitia"
                },
                {
                    "authorId": "2305622553",
                    "name": "Maria Camila Buitrago Cabrera"
                },
                {
                    "authorId": "2220406508",
                    "name": "Mario Rodr'iguez-Cantelar"
                },
                {
                    "authorId": "71090258",
                    "name": "M\u00e9lanie Jouitteau"
                },
                {
                    "authorId": "121947924",
                    "name": "M. Mihaylov"
                },
                {
                    "authorId": "2305619376",
                    "name": "Mohamed Fazli Mohamed Imam"
                },
                {
                    "authorId": "2191731497",
                    "name": "Muhammad Farid Adilazuarda"
                },
                {
                    "authorId": "66556569",
                    "name": "Munkhjargal Gochoo"
                },
                {
                    "authorId": "2159634278",
                    "name": "Munkh-Erdene Otgonbold"
                },
                {
                    "authorId": "1742219452",
                    "name": "Naome A. Etori"
                },
                {
                    "authorId": "2305623065",
                    "name": "Olivier Niyomugisha"
                },
                {
                    "authorId": "2307313242",
                    "name": "Paula M'onica Silva"
                },
                {
                    "authorId": "2040713514",
                    "name": "Pranjal A. Chitale"
                },
                {
                    "authorId": "3209719",
                    "name": "Raj Dabre"
                },
                {
                    "authorId": "2148764367",
                    "name": "Rendi Chevi"
                },
                {
                    "authorId": "49775305",
                    "name": "Ruochen Zhang"
                },
                {
                    "authorId": "2197070752",
                    "name": "Ryandito Diandaru"
                },
                {
                    "authorId": "66986482",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "2305622481",
                    "name": "Santiago G'ongora"
                },
                {
                    "authorId": "8599185",
                    "name": "Soyeong Jeong"
                },
                {
                    "authorId": "152881983",
                    "name": "Sukannya Purkayastha"
                },
                {
                    "authorId": "83446147",
                    "name": "Tatsuki Kuribayashi"
                },
                {
                    "authorId": "2219413815",
                    "name": "Thanmay Jayakumar"
                },
                {
                    "authorId": "2244512282",
                    "name": "T. Torrent"
                },
                {
                    "authorId": "2305621229",
                    "name": "Toqeer Ehsan"
                },
                {
                    "authorId": "2283931820",
                    "name": "Vladimir Araujo"
                },
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                },
                {
                    "authorId": "2305621242",
                    "name": "Zara Burzo"
                },
                {
                    "authorId": "2305621264",
                    "name": "Zheng Wei Lim"
                },
                {
                    "authorId": "2282475073",
                    "name": "Zheng-Xin Yong"
                },
                {
                    "authorId": "2293317558",
                    "name": "Oana Ignat"
                },
                {
                    "authorId": "2218338376",
                    "name": "Joan Nwatu"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                },
                {
                    "authorId": "1794626",
                    "name": "T. Solorio"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                }
            ]
        },
        {
            "paperId": "a5e609a3268c8fd61e2d578c2eb58e5d31a8db00",
            "title": "MuLan: A Study of Fact Mutability in Language Models",
            "abstract": "Facts are subject to contingencies and can be true or false in different circumstances. One such contingency is time, wherein some facts mutate over a given period, e.g., the president of a country or the winner of a championship. Trustworthy language models ideally identify mutable facts as such and process them accordingly. We create MuLan, a benchmark for evaluating the ability of English language models to anticipate time-contingency, covering both 1:1 and 1:N relations. We hypothesize that mutable facts are encoded differently than immutable ones, hence being easier to update. In a detailed evaluation of six popular large language models, we consistently find differences in the LLMs\u2019 confidence, representations, and update behavior, depending on the mutability of a fact. Our findings should inform future work on the injection of and induction of time-contingent knowledge to/from LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50110151",
                    "name": "Constanza Fierro"
                },
                {
                    "authorId": "2281953544",
                    "name": "Nicolas Garneau"
                },
                {
                    "authorId": "83574123",
                    "name": "Emanuele Bugliarello"
                },
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                },
                {
                    "authorId": "2281953769",
                    "name": "Anders S\u00f8gaard"
                }
            ]
        },
        {
            "paperId": "bc1b2cfd0e01bf25cfc62bfc570fab1d92f36b2a",
            "title": "Answerability in Retrieval-Augmented Open-Domain Question Answering",
            "abstract": "The performance of Open-Domain Question Answering (ODQA) retrieval systems can exhibit sub-optimal behavior, providing text excerpts with varying degrees of irrelevance. Unfortunately, many existing ODQA datasets lack examples specifically targeting the identification of irrelevant text excerpts. Previous attempts to address this gap have relied on a simplistic approach of pairing questions with random text excerpts. This paper aims to investigate the effectiveness of models trained using this randomized strategy, uncovering an important limitation in their ability to generalize to irrelevant text excerpts with high semantic overlap. As a result, we observed a substantial decrease in predictive accuracy, from 98% to 1%. To address this limitation, we discovered an efficient approach for training models to recognize such excerpts. By leveraging unanswerable pairs from the SQuAD 2.0 dataset, our models achieve a nearly perfect (~100%) accuracy when confronted with these challenging text excerpts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2290027799",
                    "name": "Rustam Abdumalikov"
                },
                {
                    "authorId": "2261279220",
                    "name": "Pasquale Minervini"
                },
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                }
            ]
        },
        {
            "paperId": "0f33ae44323da0384993596c38688b0f8b2af094",
            "title": "Grammatical Error Correction through Round-Trip Machine Translation",
            "abstract": "Machine translation (MT) operates on the premise of an interlingua which abstracts away from surface form while preserving meaning. A decade ago the idea of using round-trip MT to guide grammatical error correction was proposed as a way to abstract away from potential errors in surface forms (Madnani _et_ _al_., 2012). At the time, it did not pan out due to the low quality of MT systems of the day. Today much stronger MT systems are available so we re-evaluate this idea across five languages and models of various sizes. We find that for extra large models input augmentation through round-trip MT has little to no effect. For more \u2018workable\u2019 model sizes, however, it yields consistent improvements, sometimes bringing the performance of a _base_ or _large_ model up to that of a _large_ or _xl_ model, respectively. The round-trip translation comes at a computational cost though, so one would have to determine whether to opt for a larger model or for input augmentation on a case-by-case basis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                },
                {
                    "authorId": "1700187",
                    "name": "Anders S\u00f8gaard"
                }
            ]
        },
        {
            "paperId": "96c80d9ef85cb5d57b7a6461000098b3c848e82b",
            "title": "Do Vision and Language Models Share Concepts? A Vector Space Alignment Study",
            "abstract": "Abstract Large-scale pretrained language models (LMs) are said to \u201clack the ability to connect utterances to the world\u201d (Bender and Koller, 2020), because they do not have \u201cmental models of the world\u201d (Mitchell and Krakauer, 2023). If so, one would expect LM representations to be unrelated to representations induced by vision models. We present an empirical evaluation across four families of LMs (BERT, GPT-2, OPT, and LLaMA-2) and three vision model architectures (ResNet, SegFormer, and MAE). Our experiments show that LMs partially converge towards representations isomorphic to those of vision models, subject to dispersion, polysemy, and frequency. This has important implications for both multi-modal processing and the LM understanding debate (Mitchell and Krakauer, 2023).1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261336713",
                    "name": "Jiaang Li"
                },
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                },
                {
                    "authorId": "1700187",
                    "name": "Anders S\u00f8gaard"
                }
            ]
        },
        {
            "paperId": "96e37dc703d58da316041fcfcabddb49dfe855b0",
            "title": "Cultural Adaptation of Recipes",
            "abstract": "Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese- and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset composed of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automatic and human evaluation metrics. While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations. We anticipate that these insights will significantly contribute to future research on culturally aware language models and their practical application in culturally diverse contexts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261896052",
                    "name": "Yong Cao"
                },
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                },
                {
                    "authorId": "1717462692",
                    "name": "Ruixiang Cui"
                },
                {
                    "authorId": "2106433317",
                    "name": "Antonia Karamolegkou"
                },
                {
                    "authorId": "2116635928",
                    "name": "Li Zhou"
                },
                {
                    "authorId": "2261739724",
                    "name": "Megan Dare"
                },
                {
                    "authorId": "2261739082",
                    "name": "Lucia Donatelli"
                },
                {
                    "authorId": "2064295987",
                    "name": "Daniel Hershcovich"
                }
            ]
        },
        {
            "paperId": "be831248bfda5dfb604ea1676f767e5af6be500f",
            "title": "Structural Similarities Between Language Models and Neural Response Measurements",
            "abstract": "Large language models (LLMs) have complicated internal dynamics, but induce representations of words and phrases whose geometry we can study. Human language processing is also opaque, but neural response measurements can provide (noisy) recordings of activation during listening or reading, from which we can extract similar representations of words and phrases. Here we study the extent to which the geometries induced by these representations, share similarities in the context of brain decoding. We find that the larger neural language models get, the more their representations are structurally similar to neural response measurements from brain imaging. Code is available at \\url{https://github.com/coastalcph/brainlm}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261336713",
                    "name": "Jiaang Li"
                },
                {
                    "authorId": "2106433317",
                    "name": "Antonia Karamolegkou"
                },
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                },
                {
                    "authorId": "30671790",
                    "name": "Mostafa Abdou"
                },
                {
                    "authorId": "2218827611",
                    "name": "Sune Lehmann"
                },
                {
                    "authorId": "1700187",
                    "name": "Anders S\u00f8gaard"
                }
            ]
        },
        {
            "paperId": "ebb855f6120b3ac66d299443ad56185dc437ad0a",
            "title": "An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text",
            "abstract": "Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks. In this study, we compare four methods for multi-label classification, two based on an encoder only, and two based on an encoder-decoder. We carry out experiments on four datasets -two in the legal domain and two in the biomedical domain, each with two levels of label granularity- and always depart from the same pre-trained model, T5. Our results show that encoder-decoder methods outperform encoder-only methods, with a growing advantage on more complex datasets and labeling schemes of finer granularity. Using encoder-decoder models in a non-autoregressive fashion, in particular, yields the best performance overall, so we further study this approach through ablations to better understand its strengths.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                },
                {
                    "authorId": "2292452673",
                    "name": "Ilias Chalkidis"
                }
            ]
        },
        {
            "paperId": "fb45f67706e7d11eacdc41d28e7651539d773fe1",
            "title": "Retrieval-augmented Multi-label Text Classification",
            "abstract": "Multi-label text classification (MLC) is a challenging task in settings of large label sets, where label support follows a Zipfian distribution. In this paper, we address this problem through retrieval augmentation, aiming to improve the sample efficiency of classification models. Our approach closely follows the standard MLC architecture of a Transformer-based encoder paired with a set of classification heads. In our case, however, the input document representation is augmented through cross-attention to similar documents retrieved from the training set and represented in a task-specific manner. We evaluate this approach on four datasets from the legal and biomedical domains, all of which feature highly skewed label distributions. Our experiments show that retrieval augmentation substantially improves model performance on the long tail of infrequent labels especially so for lower-resource training scenarios and more challenging long-document data scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292452673",
                    "name": "Ilias Chalkidis"
                },
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                }
            ]
        }
    ]
}