{
    "authorId": "2150817685",
    "papers": [
        {
            "paperId": "23627f20f53611a39a5df35642f99d3007863496",
            "title": "LANCER: A Lifetime-Aware News Recommender System",
            "abstract": "From the observation that users reading news tend to not click outdated news, we propose the notion of 'lifetime' of news, with two hypotheses: (i) news has a shorter lifetime, compared to other types of items such as movies or e-commerce products; (ii) news only competes with other news whose lifetimes have not ended, and which has an overlapping lifetime (i.e., limited competitions). By further developing the characteristics of the lifetime of news, then we present a novel approach for news recommendation, namely, Lifetime-Aware News reCommEndeR System (LANCER) that carefully exploits the lifetime of news during training and recommendation. Using real-world news datasets (e.g., Adressa and MIND), we successfully demonstrate that state-of-the-art news recommendation models can get significantly benefited by integrating the notion of lifetime and LANCER, by up to about 40% increases in recommendation accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123063050",
                    "name": "Hong-Kyun Bae"
                },
                {
                    "authorId": "3335618",
                    "name": "Jeewon Ahn"
                },
                {
                    "authorId": "2150817685",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2155626647",
                    "name": "Sang-Wook Kim"
                }
            ]
        },
        {
            "paperId": "ba7d5f9990550975304263c3dc97bc72948734b2",
            "title": "Imputing Knowledge Tracing Data with Subject-Based Training via LSTM Variational Autoencoders Frameworks",
            "abstract": "The issue of missing data poses a great challenge on boosting performance and application of deep learning models in the {\\em Knowledge Tracing} (KT) problem. However, there has been the lack of understanding on the issue in the literature. %are not sufficient studies tackling this problem. In this work, to address this challenge, we adopt a subject-based training method to split and impute data by student IDs instead of row number splitting which we call non-subject based training. The benefit of subject-based training can retain the complete sequence for each student and hence achieve efficient training. Further, we leverage two existing deep generative frameworks, namely variational Autoencoders (VAE) and Longitudinal Variational Autoencoders (LVAE) frameworks and build LSTM kernels into them to form LSTM-VAE and LSTM LVAE (noted as VAE and LVAE for simplicity) models to generate quality data. In LVAE, a Gaussian Process (GP) model is trained to disentangle the correlation between the subject (i.e., student) descriptor information (e.g., age, gender) and the latent space. The paper finally compare the model performance between training the original data and training the data imputed with generated data from non-subject based model VAE-NS and subject-based training models (i.e., VAE and LVAE). We demonstrate that the generated data from LSTM-VAE and LSTM-LVAE can boost the original model performance by about 50%. Moreover, the original model just needs 10% more student data to surpass the original performance if the prediction model is small and 50\\% more data if the prediction model is large with our proposed frameworks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115733400",
                    "name": "J. Shen"
                },
                {
                    "authorId": "2150817685",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "1fdb0cb8bc6d47a16b44541754f0159ffa7a3bb5",
            "title": "Forecasting the Number of Tenants At-Risk of Formal Eviction: A Machine Learning Approach to Inform Public Policy",
            "abstract": "Eviction of tenants has reached a crisis level in the U.S. and its consequences pose significant challenges to society. To tackle this eviction crisis, policymakers have been allocating financial resources but a more efficient resource allocation would need an accurate forecast of the number of tenants at-risk of evictions ahead of time. To help enhance the existing eviction prevention/diversion programs, in this work, we propose a multi-view deep neural network model, named as MARTIAN, that forecasts the number of tenants at-risk of getting formally evicted (at the census tract level) n months into the future. Then, we evaluate MARTIAN\u2019s predictive performance under various conditions using real-world eviction cases filed across Dallas County, TX. The results of empirical evaluation show that MARTIAN outperforms an extensive set of baseline models in terms of predictive performance. Additionally, MARTIAN\u2019s superior predictive performance is generalizable to unseen census tracts, for which no labeled data is available in the training set. This research has been done in collaboration with Child Poverty Action Lab (CPAL), which is a pioneering non-governmental organization (NGO) working for tackling poverty-related issues across Dallas County, TX. The usability of MARTIAN is under review by subject matter experts. We release our codebase at https://github.com/maryam-tabar/MARTIAN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "147612193",
                    "name": "Maryam Tabar"
                },
                {
                    "authorId": "72904870",
                    "name": "Wooyong Jung"
                },
                {
                    "authorId": "34903979",
                    "name": "A. Yadav"
                },
                {
                    "authorId": "2179338800",
                    "name": "Owen Wilson Chavez"
                },
                {
                    "authorId": "2165652279",
                    "name": "Ashley Flores"
                },
                {
                    "authorId": "2150817685",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "3892a9bee7c9f0314a513c0933a588e1e069722b",
            "title": "CAPS: Comprehensible Abstract Policy Summaries for Explaining Reinforcement Learning Agents",
            "abstract": "As reinforcement learning (RL) continues to improve and be applied in situations alongside humans, the need to explain the learned behaviors of RL agents to end-users becomes more important. Strategies for explaining the reasoning behind an agent\u2019s policy, called policy-level explanations , can lead to important insights about both the task and the agent\u2019s behaviors. Following this line of research, in this work, we propose a novel approach, named as CAPS , that summarizes an agent\u2019s policy in the form of a directed graph with natural language descriptions. A decision tree based clustering method is utilized to abstract the state space of the task into fewer, condensed states which makes the policy graphs more digestible to end-users. This abstraction allows the users to control the size of the policy graph to achieve their desired balance between comprehensi-bility and accuracy. In addition, we develop a heuristic optimization method to find the most explainable graph policy and present it to the users. Finally, we use the user-defined predicates to enrich the abstract states with semantic meaning. We test our approach on 5 RL tasks, using both deterministic and stochastic policies, and show that our method is: (1) agnostic to the algorithms used to train the policies, and (2) comparable in accuracy and superior in explanation capabilities to existing baselines. Especially, when provided with our explanation graph, end-users are able to accurately interpret policies of trained RL agents 80% of the time, compared to 10% when provided with the next best baseline. We make our code and datasets available to ensure the reproducibility of our research",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2042750482",
                    "name": "Joe McCalmon"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "3188181",
                    "name": "Sarra M. Alqahtani"
                },
                {
                    "authorId": "2150817685",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "7d411f0c6eec36c0cc3d480501979f95ff8ae6e6",
            "title": "JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and Reasoning",
            "abstract": "In online job marketplaces, it is important to establish a well-defined job title taxonomy for various downstream tasks (e.g., job recommendation, users\u2019 career analysis, and turnover prediction). Job Title Normalization (JTN) is such a cleaning step to classify user-created non-standard job titles into normalized ones. However, solving the JTN problem is non-trivial with challenges: (1) semantic similarity of different job titles, (2) non-normalized user-created job titles, and (3) large-scale and long-tailed job titles in real-world applications. To this end, we propose a novel solution, named JAMES, that constructs three unique embeddings (i.e., graph, contextuat, and syntactic) of a target job title to effectively capture its various traits. We further propose a multi-aspect co-attention mechanism to attentively combine these embeddings, and employ neural logical reasoning representations to collaboratively estimate similarities between messy job titles and normalized job titles in a reasoning space. To evaluate JAMES, we conduct comprehensive experiments against ten competing models on a large-scale real-world dataset with over 350,000 job titles. Our experimental results show that JAMES significantly outperforms the best baseline by 10.06% in Precision@10 and by 17.52% in NDCG@10, respectively. To further facilitate the acquisition of normalized job titles for job-domain applications, our JAMES API is available at: https://tinyurl.con JAMES-job-title-mapping.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "2115733400",
                    "name": "J. Shen"
                },
                {
                    "authorId": "2090288772",
                    "name": "Thanh-Binh Tran"
                },
                {
                    "authorId": "2155650328",
                    "name": "Hamoon Ekhtiari"
                },
                {
                    "authorId": "2150817685",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "af2434d488e7f4c560cbfd76ef11575497058f23",
            "title": "Not All Layers Are Equal: A Layer-Wise Adaptive Approach Toward Large-Scale DNN Training",
            "abstract": "A large-batch training with data parallelism is a widely adopted approach to efficiently train a large deep neural network (DNN) model. Large-batch training, however, often suffers from the problem of the model quality degradation because of its fewer iterations. To alleviate this problem, in general, learning rate (lr) scaling methods have been applied, which increases the learning rate to make an update larger at each iteration. Unfortunately, however, we observe that large-batch training with state-of-the-art lr scaling methods still often degrade the model quality when a batch size crosses a specific limit, rendering such lr methods less useful. To this phenomenon, we hypothesize that existing lr scaling methods overlook the subtle but important differences across \u201clayers\u201d in training, which results in the degradation of the overall model quality. From this hypothesis, we propose a novel approach (LENA) toward the learning rate scaling for large-scale DNN training, employing: (1) a layer-wise adaptive lr scaling to adjust lr for each layer individually, and (2) a layer-wise state-aware warm-up to track the state of the training for each layer and finish its warm-up automatically. The comprehensive evaluation with variations of batch sizes demonstrates that LENA achieves the target accuracy (i.e., the accuracy of single-worker training): (1) within the fewest iterations across different batch sizes (up to 45.2% fewer iterations and 44.7% shorter time than the existing state-of-the-art method), and (2) for training very large-batch sizes, surpassing the limits of all baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3373303",
                    "name": "Yunyong Ko"
                },
                {
                    "authorId": "2150817685",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "2155626647",
                    "name": "Sang-Wook Kim"
                }
            ]
        },
        {
            "paperId": "c759bfc48c4448748adde4a0467d96d56c83be51",
            "title": "ALLIE: Active Learning on Large-scale Imbalanced Graphs",
            "abstract": "Human labeling is time-consuming and costly. This problem is further exacerbated in extremely imbalanced class label scenarios, such as detecting fraudsters in online websites. Active learning selects the most relevant example for human labelers to improve the model performance at a lower cost. However, existing methods for active learning for graph data often assumes that both data and label distributions are balanced. These assumptions fail in extreme rare-class classification scenarios, such as classifying abusive reviews in an e-commerce website. We propose a novel framework ALLIE to address this challenge of active learning in large-scale imbalanced graph data. In our approach, we efficiently sample from both majority and minority classes using a reinforcement learning agent with imbalance-aware reward function. We employ focal loss in the node classification model in order to focus more on rare class and improve the accuracy of the downstream model. Finally, we use a graph coarsening strategy to reduce the search space of the reinforcement learning agent. We conduct extensive experiments on benchmark graph datasets and real-world e-commerce datasets. ALLIE out-performs state-of-the-art graph-based active learning methods significantly, with up to 10% improvement of F1 score for the positive class. We also validate ALLIE on a proprietary e-commerce graph data by tasking it to detect abuse. Our coarsening strategy reduces the computational time by up to 38% in both proprietary and public datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3122003",
                    "name": "Limeng Cui"
                },
                {
                    "authorId": "9333529",
                    "name": "Xian-Juan Tang"
                },
                {
                    "authorId": "47617256",
                    "name": "S. Katariya"
                },
                {
                    "authorId": "145850291",
                    "name": "Nikhil S. Rao"
                },
                {
                    "authorId": "2155993722",
                    "name": "Pallav Agrawal"
                },
                {
                    "authorId": "2691095",
                    "name": "Karthik Subbian"
                },
                {
                    "authorId": "2150817685",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "ca830fd7d0ef10bad2a2545782ce0f58311f4817",
            "title": "JAMES: Job Title Mapping with Multi-Aspect Embeddings and Reasoning",
            "abstract": "One of the most essential tasks needed for various downstream tasks in career analytics (e.g., career trajectory analysis, job mobility prediction, and job recommendation) is Job Title Mapping ( JTM ), where the goal is to map user-created (noisy and non-standard) job titles to predefined and standard job titles. However, solving JTM is domain-specific and non-trivial due to its inherent challenges: (1) user-created job titles are messy, (2) different job titles often overlap their job requirements, (3) job transition trajectories are inconsistent, and (4) the number of job titles in real world applications is large-scale. Toward this JTM problem, in this work, we propose a novel solution, named as JAMES , that constructs three unique embeddings of a target job title: topological, semantic, and syntactic embeddings, together with multi-aspect co-attention. In addition, we employ logical reasoning representations to collaboratively estimate similarities between messy job titles and standard job titles in the reasoning space. We conduct comprehensive experiments against ten competing models on the large-scale real-world dataset with more than 350,000 job titles. Our results show that JAMES significantly outperforms the best baseline by 10.06% in Precision@10 and by 17.52% in NDCG@10, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66848311",
                    "name": "Michiharu Yamashita"
                },
                {
                    "authorId": "2115733400",
                    "name": "J. Shen"
                },
                {
                    "authorId": "2155650328",
                    "name": "Hamoon Ekhtiari"
                },
                {
                    "authorId": "2090288772",
                    "name": "Thanh-Binh Tran"
                },
                {
                    "authorId": "2150817685",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "f6f32504582c220a71742a4de6bbc6ad637b7d4d",
            "title": "KETCH: Knowledge Graph Enhanced Thread Recommendation in Healthcare Forums",
            "abstract": "Health thread recommendation methods aim to suggest the most relevant existing threads for a user. Most of the existing methods tend to rely on modeling the post contents to retrieve relevant answers. However, some posts written by users with different clinical conditions can be lexically similar, as unrelated diseases (e.g., Angina and Osteoporosis) may have the same symptoms (e.g., back pain), yet irrelevant threads to a user. Therefore, it is critical to not only consider the connections between users and threads, but also the descriptions of users' symptoms and clinical conditions. In this paper, towards this problem of thread recommendation in online healthcare forums, we propose a knowledge graph enhanced Threads Recommendation (KETCH) model, which leverages graph neural networks to model the interactions among users and threads, and learn their representations. In our model, the users, threads and posts are three types of nodes in a graph, linked through their associations. KETCH uses the message passing strategy by aggregating information along with the network. In addition, we introduce a knowledge-enhanced attention mechanism to capture the latent conditions and symptoms. We also apply the method to the task of predicting the side effects of drugs, to show that KETCH has the potential to complement the medical knowledge graph. Comparing with the best results of seven competing methods, in terms of MRR, KETCH outperforms all methods by at least 0.125 on the MedHelp dataset, 0.048 on the Patient dataset and 0.092 on HealthBoards dataset, respectively. We release the source code of KETCH at: https://github.com/cuilimeng/KETCH.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3122003",
                    "name": "Limeng Cui"
                },
                {
                    "authorId": "2150817685",
                    "name": "Dongwon Lee"
                }
            ]
        }
    ]
}