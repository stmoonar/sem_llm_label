{
    "authorId": "3063894",
    "papers": [
        {
            "paperId": "e7f58c73de2d921ad5ba7f6b99f679ab1b262400",
            "title": "LinSATNet: The Positive Linear Satisfiability Neural Networks",
            "abstract": "Encoding constraints into neural networks is attractive. This paper studies how to introduce the popular positive linear satisfiability to neural networks. We propose the first differentiable satisfiability layer based on an extension of the classic Sinkhorn algorithm for jointly encoding multiple sets of marginal distributions. We further theoretically characterize the convergence property of the Sinkhorn algorithm for multiple marginals. In contrast to the sequential decision e.g.\\ reinforcement learning-based solvers, we showcase our technique in solving constrained (specifically satisfiability) problems by one-shot neural networks, including i) a neural routing solver learned without supervision of optimal solutions; ii) a partial graph matching network handling graphs with unmatchable outliers on both sides; iii) a predictive network for financial portfolios with continuous constraints. To our knowledge, there exists no one-shot neural solver for these scenarios when they are formulated as satisfiability problems. Source code is available at https://github.com/Thinklab-SJTU/LinSATNet",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "89674661",
                    "name": "Runzhong Wang"
                },
                {
                    "authorId": "2130520888",
                    "name": "Yunhao Zhang"
                },
                {
                    "authorId": "2151169946",
                    "name": "Ziao Guo"
                },
                {
                    "authorId": "2117182150",
                    "name": "Tianyi Chen"
                },
                {
                    "authorId": "2159107948",
                    "name": "Xiaokang Yang"
                },
                {
                    "authorId": "3063894",
                    "name": "Junchi Yan"
                }
            ]
        },
        {
            "paperId": "001c8e3e32ff0c83bcff48f860085c72203f6d64",
            "title": "H2RBox-v2: Incorporating Symmetry for Boosting Horizontal Box Supervised Oriented Object Detection",
            "abstract": "With the rapidly increasing demand for oriented object detection, e.g. in autonomous driving and remote sensing, the recently proposed paradigm involving weakly-supervised detector H2RBox for learning rotated box (RBox) from the more readily-available horizontal box (HBox) has shown promise. This paper presents H2RBox-v2, to further bridge the gap between HBox-supervised and RBox-supervised oriented object detection. Specifically, we propose to leverage the reflection symmetry via flip and rotate consistencies, using a weakly-supervised network branch similar to H2RBox, together with a novel self-supervised branch that learns orientations from the symmetry inherent in visual objects. The detector is further stabilized and enhanced by practical techniques to cope with peripheral issues e.g. angular periodicity. To our best knowledge, H2RBox-v2 is the first symmetry-aware self-supervised paradigm for oriented object detection. In particular, our method shows less susceptibility to low-quality annotation and insufficient training data compared to H2RBox. Specifically, H2RBox-v2 achieves very close performance to a rotation annotation trained counterpart -- Rotated FCOS: 1) DOTA-v1.0/1.5/2.0: 72.31%/64.76%/50.33% vs. 72.44%/64.53%/51.77%; 2) HRSC: 89.66% vs. 88.99%; 3) FAIR1M: 42.27% vs. 41.25%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47111778",
                    "name": "Yi Yu"
                },
                {
                    "authorId": "143989318",
                    "name": "Xue Yang"
                },
                {
                    "authorId": "2108421389",
                    "name": "Qingyun Li"
                },
                {
                    "authorId": "2116451362",
                    "name": "Yue Zhou"
                },
                {
                    "authorId": "3053621",
                    "name": "Gefan Zhang"
                },
                {
                    "authorId": "2056630466",
                    "name": "Feipeng Da"
                },
                {
                    "authorId": "3063894",
                    "name": "Junchi Yan"
                }
            ]
        },
        {
            "paperId": "006aa38414f90a5d4551c8c49fce8daf7a7d730f",
            "title": "HIORE: Leveraging High-order Interactions for Unified Entity Relation Extraction",
            "abstract": "Entity relation extraction consists of two sub-tasks: entity recognition and relation extraction. Existing methods either tackle these two tasks separately or unify them with word-by-word interactions. In this paper, we propose HIORE, a new method for unified entity relation extraction. The key insight is to leverage the high-order interactions, i.e., the complex association among word pairs, which contains richer information than the first-order word-by-word interactions. For this purpose, we first devise a W-shape DNN (WNet) to capture coarse-level high-order connections. Then, we build a heuristic high-order graph and further calibrate the representations with a graph neural network (GNN). Experiments on three benchmarks (ACE04, ACE05, SciERC) show that HIORE achieves the state-of-the-art performance on relation extraction and an improvement of 1.1~1.8 F1 points over the prior best unified model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2136922817",
                    "name": "Yijun Wang"
                },
                {
                    "authorId": "2118133838",
                    "name": "Changzhi Sun"
                },
                {
                    "authorId": "3174675",
                    "name": "Yuanbin Wu"
                },
                {
                    "authorId": "143900010",
                    "name": "L. Li"
                },
                {
                    "authorId": "3063894",
                    "name": "Junchi Yan"
                },
                {
                    "authorId": null,
                    "name": "Hao Zhou"
                }
            ]
        },
        {
            "paperId": "01de6d0c00e7e77050a90945246b2b4acde497a2",
            "title": "NodeFormer: A Scalable Graph Structure Learning Transformer for Node Classification",
            "abstract": "Graph neural networks have been extensively studied for learning with inter-connected data. Despite this, recent evidence has revealed GNNs' deficiencies related to over-squashing, heterophily, handling long-range dependencies, edge incompleteness and particularly, the absence of graphs altogether. While a plausible solution is to learn new adaptive topology for message passing, issues concerning quadratic complexity hinder simultaneous guarantees for scalability and precision in large networks. In this paper, we introduce a novel all-pair message passing scheme for efficiently propagating node signals between arbitrary nodes, as an important building block for a pioneering Transformer-style network for node classification on large graphs, dubbed as \\textsc{NodeFormer}. Specifically, the efficient computation is enabled by a kernerlized Gumbel-Softmax operator that reduces the algorithmic complexity to linearity w.r.t. node numbers for learning latent graph structures from large, potentially fully-connected graphs in a differentiable manner. We also provide accompanying theory as justification for our design. Extensive experiments demonstrate the promising efficacy of the method in various tasks including node classification on graphs (with up to 2M nodes) and graph-enhanced applications (e.g., image classification) where input graphs are missing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51171144",
                    "name": "Qitian Wu"
                },
                {
                    "authorId": "2053249603",
                    "name": "Wentao Zhao"
                },
                {
                    "authorId": "15401196",
                    "name": "Zenan Li"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                },
                {
                    "authorId": "3063894",
                    "name": "Junchi Yan"
                }
            ]
        },
        {
            "paperId": "0dcf24bb23ce5bc17aab8138903af5f049a4db91",
            "title": "Energy-based Out-of-Distribution Detection for Graph Neural Networks",
            "abstract": "Learning on graphs, where instance nodes are inter-connected, has become one of the central problems for deep learning, as relational structures are pervasive and induce data inter-dependence which hinders trivial adaptation of existing approaches that assume inputs to be i.i.d.~sampled. However, current models mostly focus on improving testing performance of in-distribution data and largely ignore the potential risk w.r.t. out-of-distribution (OOD) testing samples that may cause negative outcome if the prediction is overconfident on them. In this paper, we investigate the under-explored problem, OOD detection on graph-structured data, and identify a provably effective OOD discriminator based on an energy function directly extracted from graph neural networks trained with standard classification loss. This paves a way for a simple, powerful and efficient OOD detection model for GNN-based learning on graphs, which we call GNNSafe. It also has nice theoretical properties that guarantee an overall distinguishable margin between the detection scores for in-distribution and OOD samples, which, more critically, can be further strengthened by a learning-free energy belief propagation scheme. For comprehensive evaluation, we introduce new benchmark settings that evaluate the model for detecting OOD data from both synthetic and real distribution shifts (cross-domain graph shifts and temporal graph shifts). The results show that GNNSafe achieves up to $17.0\\%$ AUROC improvement over state-of-the-arts and it could serve as simple yet strong baselines in such an under-developed area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51171144",
                    "name": "Qitian Wu"
                },
                {
                    "authorId": "2109060480",
                    "name": "Yiting Chen"
                },
                {
                    "authorId": null,
                    "name": "Chenxiao Yang"
                },
                {
                    "authorId": "3063894",
                    "name": "Junchi Yan"
                }
            ]
        },
        {
            "paperId": "0e01083eede13f8f19dfa0f0e772e8b6b49f5d0e",
            "title": "ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation",
            "abstract": "Domain shifts such as sensor type changes and geographical situation variations are prevalent in Autonomous Driving (AD), which poses a challenge since AD model relying on the previous domain knowledge can be hardly directly deployed to a new domain without additional costs. In this paper, we provide a new perspective and approach of alleviating the domain shifts, by proposing a Reconstruction-Simulation-Perception (ReSimAD) scheme. Specifically, the implicit reconstruction process is based on the knowledge from the previous old domain, aiming to convert the domain-related knowledge into domain-invariant representations, e.g., 3D scene-level meshes. Besides, the point clouds simulation process of multiple new domains is conditioned on the above reconstructed 3D meshes, where the target-domain-like simulation samples can be obtained, thus reducing the cost of collecting and annotating new-domain data for the subsequent perception process. For experiments, we consider different cross-domain situations such as Waymo-to-KITTI, Waymo-to-nuScenes, Waymo-to-ONCE, etc, to verify the zero-shot target-domain perception using ReSimAD. Results demonstrate that our method is beneficial to boost the domain generalization ability, even promising for 3D pre-training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2141897317",
                    "name": "Bo Zhang"
                },
                {
                    "authorId": "2239169372",
                    "name": "Xinyu Cai"
                },
                {
                    "authorId": "2174741399",
                    "name": "Jiakang Yuan"
                },
                {
                    "authorId": "2239165347",
                    "name": "Donglin Yang"
                },
                {
                    "authorId": "2148903203",
                    "name": "Jianfei Guo"
                },
                {
                    "authorId": "2239108883",
                    "name": "Renqiu Xia"
                },
                {
                    "authorId": "119700639",
                    "name": "Botian Shi"
                },
                {
                    "authorId": "2197075911",
                    "name": "Min Dou"
                },
                {
                    "authorId": "144799987",
                    "name": "Tao Chen"
                },
                {
                    "authorId": "2257073771",
                    "name": "Si Liu"
                },
                {
                    "authorId": "3063894",
                    "name": "Junchi Yan"
                },
                {
                    "authorId": "145858545",
                    "name": "Y. Qiao"
                }
            ]
        },
        {
            "paperId": "1349e9d2734183f92120c5032e4ed188d60d47eb",
            "title": "Adaptive Semi-Supervised Mixup with Implicit Label Learning and Sample Ratio Balancing",
            "abstract": "Despite the impressive performance of deep neural networks, they are prone to over-fitting at labeled points rooting from the scarcity of annotated data. Applying mixup regularization in training provides an effective mechanism to improve generalization performance. On the other hand, semi-supervised learning(SSL) leverages an abundant amount of unlabeled data along with a small amount of labeled data in the training process. In this paper, we have introduced mixup regularization to SSL, along with an exploration-utilization training scheme to enhance the performance. Besides, due to the large volume imbalance between labeled/unlabeled data and the unwanted noise resulting from unlabeled samples, we also implement a balancing ratio between the labeled/unlabeled loss terms. Specifically, we devise a novel Sharp Entropy loss for model optimization with large-scale unlabeled samples and employ an uncertainty estimation technique to weigh unlabeled loss function. Extensive experiments show the state-of-the-art performance of SEMixup and uncertainty balancing ratio superior to baselines on image classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2239965859",
                    "name": "Yulin Su"
                },
                {
                    "authorId": "2153001072",
                    "name": "Liangliang Shi"
                },
                {
                    "authorId": "2240442250",
                    "name": "Ziming Feng"
                },
                {
                    "authorId": "2239898245",
                    "name": "Pengzhi Chu"
                },
                {
                    "authorId": "3063894",
                    "name": "Junchi Yan"
                }
            ]
        },
        {
            "paperId": "16568b42e0167f04a9858af034b4c2f78a4d375e",
            "title": "Learning Generative RNN-ODE for Collaborative Time-Series and Event Sequence Forecasting",
            "abstract": "Time-series and event sequences are widely collected data types in real-world applications. Modeling and forecasting of such temporal data play an important role in an informed decision-making process. A major limitation of previous methods is that they either focus on time-series or events, rather than the combination of the two worlds. In fact, the two types of data often provide complementary information, emphasizing the necessity of jointly modeling the both. In this paper, we propose the RNN-ODE collaborative model for joint modeling and forecasting of heterogeneous time-series and event sequence data, which combines several useful techniques from both Bayesian and deep learning for its interpretability. Specifically, we devise a tailored encoder to combine the advances in deep temporal point processes models and variational recurrent neural networks. To predict the probability of event occurrence over an arbitrary continuous-time horizon, we base our model on the mathematical foundation of Neural Ordinary Differential Equations (NODE). Extensive experimental results on simulations and real data sets show that compared with existing methods, our integrated approach can achieve more competitive forecasting performance of both time-series and event sequences.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144365744",
                    "name": "Longyuan Li"
                },
                {
                    "authorId": "3063894",
                    "name": "Junchi Yan"
                },
                {
                    "authorId": "2130520888",
                    "name": "Yunhao Zhang"
                },
                {
                    "authorId": "2154499886",
                    "name": "Jihai Zhang"
                },
                {
                    "authorId": "143867128",
                    "name": "Jie Bao"
                },
                {
                    "authorId": "35692109",
                    "name": "Yaohui Jin"
                },
                {
                    "authorId": "2159107948",
                    "name": "Xiaokang Yang"
                }
            ]
        },
        {
            "paperId": "19d5572a13560c2a0f310a3e014abaa389941681",
            "title": "MoleRec: Combinatorial Drug Recommendation with Substructure-Aware Molecular Representation Learning",
            "abstract": "Combinatorial drug recommendation involves recommending a personalized combination of medication (drugs) to a patient over his/her longitudinal history, which essentially aims at solving a combinatorial optimization problem that pursues high accuracy under the safety constraint. Among existing learning-based approaches, the association between drug substructures (i.e., a sub-graph of the molecule that contributes to certain chemical effect) and the target disease is largely overlooked, though the function of drugs in fact exhibits strong relevance with particular substructures. To address this issue, we propose a molecular substructure-aware encoding method entitled MoleRec that entails a hierarchical architecture aimed at modeling inter-substructure interactions and individual substructures\u2019 impact on patient\u2019s health condition, in order to identify those substructures that really contribute to healing patients. Specifically, MoleRec learns to attentively pooling over substructure representations which will be element-wisely re-scaled by the model\u2019s inferred relevancy with a patient\u2019s health condition to obtain a prior-knowledge-informed drug representation. We further design a weight annealing strategy for drug-drug-interaction (DDI) objective to adaptively control the balance between accuracy and safety criteria throughout training. Experiments on the MIMIC-III dataset demonstrate that our approach achieves new state-of-the-art performance w.r.t. four accuracy and safety metrics. Our source code is publicly available at https://github.com/yangnianzu0515/MoleRec.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119236740",
                    "name": "Nianzu Yang"
                },
                {
                    "authorId": "2215541117",
                    "name": "Kaipeng Zeng"
                },
                {
                    "authorId": "51171144",
                    "name": "Qitian Wu"
                },
                {
                    "authorId": "3063894",
                    "name": "Junchi Yan"
                }
            ]
        },
        {
            "paperId": "1a4b9ceb3dbecd3ec08b93f68ba44bc3178b1df5",
            "title": "Simplifying and Empowering Transformers for Large-Graph Representations",
            "abstract": "Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Transformers (SGFormer), which is empowered by a simple attention model that can efficiently propagate information among arbitrary nodes in one layer. SGFormer requires none of positional encodings, feature/graph pre-processing or augmented loss. Empirically, SGFormer successfully scales to the web-scale graph ogbn-papers100M and yields up to 141x inference acceleration over SOTA Transformers on medium-sized graphs. Beyond current results, we believe the proposed methodology alone enlightens a new technical path of independent interest for building Transformers on large graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51171144",
                    "name": "Qitian Wu"
                },
                {
                    "authorId": "49260917",
                    "name": "Wen-Long Zhao"
                },
                {
                    "authorId": null,
                    "name": "Chenxiao Yang"
                },
                {
                    "authorId": "35466544",
                    "name": "Hengrui Zhang"
                },
                {
                    "authorId": "2163318329",
                    "name": "Fan Nie"
                },
                {
                    "authorId": "1557293815",
                    "name": "Haitian Jiang"
                },
                {
                    "authorId": "2419616",
                    "name": "Yatao Bian"
                },
                {
                    "authorId": "3063894",
                    "name": "Junchi Yan"
                }
            ]
        }
    ]
}