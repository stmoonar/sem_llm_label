{
    "authorId": "2176479216",
    "papers": [
        {
            "paperId": "9d0bec28fc70ff4b0a6656e162a5cbff5fb3ccc8",
            "title": "VL-FAS: Domain Generalization via Vision-Language Model For Face Anti-Spoofing",
            "abstract": "Recent approaches have demonstrated the effectiveness of Vision Transformer (ViT) with attention mechanisms for domain generalization of Face Anti-Spoofing (FAS). However, current attention algorithms highlight all the salient objects (e.g., background objects, hair, glasses), which results in the feature learned by the model containing face-irrelevant noisy information. Inspired by existing Vision-language works, we propose the VL-FAS to extract more generalized and cleaner discriminative features. Specifically, we leverage fine-grained natural language descriptions of the face region to act as a task-oriented teacher, directing the model\u2019s attention towards the face region through top-down attention regulation. Furthermore, to enhance the domain generalization ability of the model, we propose a Sample-Level Vision-Text optimization module (SLVT). SLVT uses sample-level image-text pairs for contrastive learning, allowing the visual coder to comprehend the intrinsic semantics of each image sample, thereby reducing the dependence on domain information. Extensive experiments show that our approach significantly outperforms the state-of-the-art and improves the performance of the ViT by about twice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113485921",
                    "name": "Hao Fang"
                },
                {
                    "authorId": "144701473",
                    "name": "Ajian Liu"
                },
                {
                    "authorId": "2276463594",
                    "name": "Ning Jiang"
                },
                {
                    "authorId": "2230364398",
                    "name": "Quan Lu"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "2276435779",
                    "name": "Jun Wan"
                }
            ]
        },
        {
            "paperId": "2afd22d8353af0f75c4410cb8743c5f8cdeb625c",
            "title": "Haha-POD: An Attempt for Laughter-Based Non-Verbal Speaker Verification",
            "abstract": "It is widely acknowledged that discriminative representation for speaker verification can be extracted from verbal speech. However, how much speaker information that non-verbal vocalization carries is still a puzzle. This paper explores speaker verification based on the most ubiquitous form of non-verbal voice, laughter. First, we use a semi-automatic pipeline to collect a new Haha-Pod dataset from open-source podcast media. The dataset contains over 240 speakers\u2019 laughter clips with corresponding high-quality verbal speech. Second, we propose a Two-Stage Teacher-Student (2S-TS) framework to minimize the within-speaker embedding distance between verbal and non-verbal (laughter) signals. Considering Haha-Pod as a test set, two trial sets (S2L-Eval) are designed to verify the speaker\u2019s identity through laugh sounds. Experimental results demonstrate that our method can significantly improve the performance of the S2L-Eval test set with only a minor degradation on the VoxCeleb1 test set. The resources for the Haha-Pod dataset can be found at https://github.com/nevermoreLin/HahaPod.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2125430215",
                    "name": "Yuke Lin"
                },
                {
                    "authorId": "7710427",
                    "name": "Xiaoyi Qin"
                },
                {
                    "authorId": "2142142903",
                    "name": "Ning Jiang"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "35834541",
                    "name": "Ming Li"
                }
            ]
        },
        {
            "paperId": "726e9e415c28bf8bf3173fa847d89f1979c5b419",
            "title": "Not All Classes are Equal: Adaptively Focus-Aware Confidence for Semi-Supervised Object Detection",
            "abstract": "Semi-supervised object detection (SSOD) is a significant application of Semi-supervised learning to further improve object detectors but suffers more seriously from confirmation bias and error accumulation caused by the classes imbalance. Existing SSOD approaches have attempted to address this issue but fails to consider dynamically changed detection difficulties of different classes for detectors. In this paper, we propose adaptively focus-aware confidence, which treats object classes differently. Predictions generated from the teacher and student models are stored in a memory dictionary, and the differences between them are utilized to adaptively perceive learning statuses. Based on this, confidence thresholds are flexibly assigned and adjusted for different classes. Extensive experiments are conducted on MS-COCO benchmark dataset with multiple protocols and our SSOD framework out-performs the state-of-the-art competitors by a large margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115718191",
                    "name": "Hui Zhu"
                },
                {
                    "authorId": "152211894",
                    "name": "Yongchun L\u00fc"
                },
                {
                    "authorId": "2216429559",
                    "name": "Hongyu Zhao"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "9272754",
                    "name": "Xiaofang Zhao"
                }
            ]
        },
        {
            "paperId": "7b980fdefd32ec36d9e07c1110c12b55ed09695d",
            "title": "Boosting Multi-Speaker Expressive Speech Synthesis with Semi-Supervised Contrastive Learning",
            "abstract": "This paper aims to build a multi-speaker expressive TTS system, synthesizing a target speaker\u2019s speech with multiple styles and emotions. To this end, we propose a novel contrastive learning-based TTS approach to transfer style and emotion across speakers. Specifically, contrastive learning from different levels, i.e. utterance and category level, is leveraged to extract the disentangled style, emotion, and speaker representations from speech for style and emotion transfer. Furthermore, a semi-supervised training strategy is introduced to improve the data utilization efficiency by involving multi-domain data, including style-labeled data, emotion-labeled data, and abundant unlabeled data. To achieve expressive speech with diverse styles and emotions for a target speaker, the learned disentangled representations are integrated into an improved VITS model. Experiments on multi-domain data demonstrate the effectiveness of the proposed method.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2181790202",
                    "name": "Xinfa Zhu"
                },
                {
                    "authorId": "2257107660",
                    "name": "Yuke Li"
                },
                {
                    "authorId": "51160531",
                    "name": "Yinjiao Lei"
                },
                {
                    "authorId": "2261283025",
                    "name": "Ning Jiang"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "2261367041",
                    "name": "Lei Xie"
                }
            ]
        },
        {
            "paperId": "7eb3b416f4537e431d27bdb5fde2d22b73e5e0cc",
            "title": "SELM: Speech Enhancement using Discrete Tokens and Language Models",
            "abstract": "Language models (LMs) have recently shown superior performances in various speech generation tasks, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information is advantageous for speech enhancement tasks. In light of this, we propose SELM, a novel speech enhancement paradigm that integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a de-tokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics and superior subjective perception results. Our demos are available 1.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2218656562",
                    "name": "Ziqian Wang"
                },
                {
                    "authorId": "2181790202",
                    "name": "Xinfa Zhu"
                },
                {
                    "authorId": "2257183006",
                    "name": "Zihan Zhang"
                },
                {
                    "authorId": "2283937884",
                    "name": "Yuanjun Lv"
                },
                {
                    "authorId": "2261283025",
                    "name": "Ning Jiang"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "2258122238",
                    "name": "Lei Xie"
                }
            ]
        },
        {
            "paperId": "834cd0402081d1f7eb56d2d7c2384b5ce0cc037a",
            "title": "VoxBlink: X-Large Speaker Verification Dataset on Camera",
            "abstract": "In this paper, we contribute a novel and extensive dataset for speaker verification, which contains noisy 38k identities/1.45M utterances (VoxBlink) and relatively cleaned 18k identities/1.02M (VoxBlink-Clean) utterances for training. Firstly, we accumulate a 60K+ users\u2019 list with their avatars and download their short videos on YouTube. We then establish an automatic and scalable pipeline to extract relevant speech/video segments to construct the VoxBlink. We also provide a clean version, VoxBlink-clean, after deep purification. To our knowledge, the VoxBlink dataset is one of the largest speaker recognition datasets available. Secondly, we conduct a series of experiments based on different backbones trained on a mix of the VoxCeleb2 and the VoxBlink-Clean. Our findings highlight a notable performance improvement, ranging from 13% to 30%, across different backbone architectures upon integrating our dataset for training. The dataset will be made publicly available shortly. In this paper, we",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2125430215",
                    "name": "Yuke Lin"
                },
                {
                    "authorId": "7710427",
                    "name": "Xiaoyi Qin"
                },
                {
                    "authorId": "2072996867",
                    "name": "Ming Cheng"
                },
                {
                    "authorId": "2238626113",
                    "name": "Ning Jiang"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "2257315294",
                    "name": "Ming Li"
                }
            ]
        },
        {
            "paperId": "8ceddb1ca62f6ddf3e4999c85c19410016af9585",
            "title": "TreeMAN: Tree-enhanced Multimodal Attention Network for ICD Coding",
            "abstract": "ICD coding is designed to assign the disease codes to electronic health records (EHRs) upon discharge, which is crucial for billing and clinical statistics. In an attempt to improve the effectiveness and efficiency of manual coding, many methods have been proposed to automatically predict ICD codes from clinical notes. However, most previous works ignore the decisive information contained in structured medical data in EHRs, which is hard to be captured from the noisy clinical notes. In this paper, we propose a Tree-enhanced Multimodal Attention Network (TreeMAN) to fuse tabular features and textual features into multimodal representations by enhancing the text representations with tree-based features via the attention mechanism. Tree-based features are constructed according to decision trees learned from structured multimodal medical data, which capture the decisive information about ICD coding. We can apply the same multi-label classifier from previous text models to the multimodal representations to predict ICD codes. Experiments on two MIMIC datasets show that our method outperforms prior state-of-the-art ICD coding approaches. The code is available at https://github.com/liu-zichen/TreeMAN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117942578",
                    "name": "Zichen Liu"
                },
                {
                    "authorId": "2108736365",
                    "name": "Xuyuan Liu"
                },
                {
                    "authorId": "2311315",
                    "name": "Yanlong Wen"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "48570355",
                    "name": "Fen Xia"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "ad86f04206b64a11d8fd4d570be553e3879c3275",
            "title": "The NPU-MSXF Speech-to-Speech Translation System for IWSLT 2023 Speech-to-Speech Translation Task",
            "abstract": "This paper describes the NPU-MSXF system for the IWSLT 2023 speech-to-speech translation (S2ST) task which aims to translate from English speech of multi-source to Chinese speech. The system is built in a cascaded manner consisting of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS). We make tremendous efforts to handle the challenging multi-source input. Specifically, to improve the robustness to multi-source speech input, we adopt various data augmentation strategies and a ROVER-based score fusion on multiple ASR model outputs. To better handle the noisy ASR transcripts, we introduce a three-stage fine-tuning strategy to improve translation accuracy. Finally, we build a TTS model with high naturalness and sound quality, which leverages a two-stage framework, using network bottleneck features as a robust intermediate representation for speaker timbre and linguistic content disentanglement. Based on the two-stage framework, pre-trained speaker embedding is leveraged as a condition to transfer the speaker timbre in the source English speech to the translated Chinese speech. Experimental results show that our system has high translation accuracy, speech naturalness, sound quality, and speaker similarity. Moreover, it shows good robustness to multi-source data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2093282088",
                    "name": "Kun Song"
                },
                {
                    "authorId": "51160531",
                    "name": "Yinjiao Lei"
                },
                {
                    "authorId": "2308974",
                    "name": "Pei-Ning Chen"
                },
                {
                    "authorId": "2035595519",
                    "name": "Yiqing Cao"
                },
                {
                    "authorId": "2052427099",
                    "name": "Kun Wei"
                },
                {
                    "authorId": "2108177443",
                    "name": "Yongmao Zhang"
                },
                {
                    "authorId": "46330520",
                    "name": "Linfu Xie"
                },
                {
                    "authorId": "2142142903",
                    "name": "Ning Jiang"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                }
            ]
        },
        {
            "paperId": "b4c5ea603d4ca829e4b80e9ba6d86a461845dc9b",
            "title": "MADI: Inter-Domain Matching and Intra-Domain Discrimination for Cross-Domain Speech Recognition",
            "abstract": "End-to-end automatic speech recognition (ASR) usually suffers from performance degradation when applied to a new domain due to domain shift. Unsupervised domain adaptation (UDA) aims to improve the performance on the unlabeled target domain by transferring knowledge from the source to the target domain. To improve transferability, existing UDA approaches mainly focus on matching the distributions of the source and target domains globally and/or locally, while ignoring the model discriminability. In this paper, we propose a novel UDA approach for ASR via inter-domain MAtching and intra-domain DIscrimination (MADI), which improves the model transferability by fine-grained inter-domain matching and discriminability by intra-domain contrastive discrimination simultaneously. Evaluations on the Libri-Adapt dataset demonstrate the effectiveness of our approach. MADI reduces the relative word error rate (WER) on cross-device and cross-environment ASR by 17.7% and 22.8%, respectively.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2275780366",
                    "name": "Jiaming Zhou"
                },
                {
                    "authorId": "2516425",
                    "name": "Shiwan Zhao"
                },
                {
                    "authorId": "2142142903",
                    "name": "Ning Jiang"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "2214620743",
                    "name": "Yong Qin"
                }
            ]
        },
        {
            "paperId": "bf47969675b9b3dec937864d26e7a731d5cc9785",
            "title": "Multi-Objective Progressive Clustering for Semi-Supervised Domain Adaptation in Speaker Verification",
            "abstract": "Utilizing the pseudo-labeling algorithm with large-scale unlabeled data becomes crucial for semi-supervised domain adaptation in speaker verification tasks. In this paper, we propose a novel pseudo-labeling method named Multi-objective Progressive Clustering (MoPC), specifically designed for semi-supervised domain adaptation. Firstly, we utilize limited labeled data from the target domain to derive domain-specific descriptors based on multiple distinct objectives, namely within-graph denoising, intra-class denoising and inter-class denoising. Then, the Infomap algorithm is adopted for embedding clustering, and the descriptors are leveraged to further refine the target domain\u2019s pseudo-labels. Moreover, to further improve the quality of pseudo labels, we introduce the subcenter-purification and progressive-merging strategy for label denoising. Our proposed MoPC method achieves 4.95% EER and ranked the 1st place on the evaluation set of VoxSRC 2023 track 3. We also conduct additional experiments on the FFSVC dataset and yield promising results.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2232657654",
                    "name": "Ze Li"
                },
                {
                    "authorId": "2125430215",
                    "name": "Yuke Lin"
                },
                {
                    "authorId": "2238626113",
                    "name": "Ning Jiang"
                },
                {
                    "authorId": "7710427",
                    "name": "Xiaoyi Qin"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "2119019798",
                    "name": "Haiying Wu"
                },
                {
                    "authorId": "2257315294",
                    "name": "Ming Li"
                }
            ]
        }
    ]
}