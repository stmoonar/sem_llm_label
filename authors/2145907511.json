{
    "authorId": "2145907511",
    "papers": [
        {
            "paperId": "5f08ec6254022a2bfaeebb9d9c72404a1351c078",
            "title": "A Dynamics and Task Decoupled Reinforcement Learning Architecture for High-Efficiency Dynamic Target Intercept",
            "abstract": "Due to the flexibility and ease of control, unmanned aerial vehicles (UAVs) have been increasingly used in various scenarios and applications in recent years. Training UAVs with reinforcement learning (RL) for a specific task is often expensive in terms of time and computation. However, it is known that the main effort of the learning process is made to fit the low-level physical dynamics systems instead of the high-level task itself. In this paper, we study to apply UAVs in the dynamic target intercept (DTI) task, where the dynamics systems equipped by different UAV models are correspondingly distinct. To this end, we propose a dynamics and task decoupled RL architecture to address the inefficient learning procedure, where the RL module focuses on modeling the DTI task without involving physical dynamics, and the design of states, actions, and rewards are completely task-oriented while the dynamics control module can adaptively convert actions from the RL module to dynamics signals to control different UAVs without retraining the RL module. We show the efficiency and efficacy of our results in comparison and ablation experiments against state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152510373",
                    "name": "Dora D. Liu"
                },
                {
                    "authorId": "2118850040",
                    "name": "Liang Hu"
                },
                {
                    "authorId": "2145907511",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2215415965",
                    "name": "Tangwei Ye"
                },
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "50737877",
                    "name": "Z. Lai"
                }
            ]
        },
        {
            "paperId": "2dd1116e2b0412ce9bc93486f6bd8a568a46340f",
            "title": "Self-driving scale car trained by Deep reinforcement Learning",
            "abstract": "The self-driving based on deep reinforcement learning, as the most important application of artificial intelligence, has become a popular topic. Most of the current self-driving methods focus on how to directly learn end-to-end self-driving control strategy from the raw sensory data. Essentially, this control strategy can be considered as a mapping between images and driving behavior, which usually faces a problem of low generalization ability. To improve the generalization ability for the driving behavior, the reinforcement learning method requires extrinsic reward from the real environment, which may damage the car. In order to obtain a good generalization ability in safety, a virtual simulation environment that can be constructed different driving scene is designed by Unity. A theoretical model is established and analyzed in the virtual simulation environment, and it is trained by double Deep Q-network. Then, the trained model is migrated to a scale car in real world. This process is also called a sim2real method. The sim2real training method efficiently handle the these two problems. The simulations and experiments are carried out to evaluate the performance and effectiveness of the proposed algorithm. Finally, it is demonstrated that the scale car in real world obtain the capability for autonomous driving.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2145907511",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2150696844",
                    "name": "Tao Du"
                }
            ]
        },
        {
            "paperId": "bcd7706788ac060ac7d3bbfddd56f1fcac242e41",
            "title": "A Sim2real method based on DDQN for training a self-driving scale car",
            "abstract": "The self-driving based on deep reinforcement learning, as the most important application of artificial intelligence, has become a popular topic. Most of the current self-driving methods focus on how to directly learn end-to-end self-driving control strategy from the raw sensory data. Essentially, this control strategy can be considered as a mapping between images and driving behavior, which usually faces a problem of low generalization ability. To improve the generalization ability for the driving behavior, the reinforcement learning method requires extrinsic reward from the real environment, which may damage the car. In order to obtain a good generalization ability in safety, a virtual simulation environment that can be constructed different driving scene is designed by Unity. A theoretical model is established and analyzed in the virtual simulation environment, and it is trained by double Deep Q-network. Then, the trained model is migrated to a scale car in real world. This process is also called a sim2real method. The sim2real training method efficiently handles these two problems. The simulations and experiments are carried out to evaluate the performance and effectiveness of the proposed algorithm. Finally, it is demonstrated that the scale car in real world obtains the capability for autonomous driving.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145907511",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2056897507",
                    "name": "T. Du"
                },
                {
                    "authorId": "1483994101",
                    "name": "Changzheng Tian"
                }
            ]
        },
        {
            "paperId": "a51ae2e3a61cfb25c4ec2faf0a61cac8936b91b2",
            "title": "Learning Behavior Trees for Autonomous Agents with Hybrid Constraints Evolution",
            "abstract": "Featured Application: The proposed approach can learn transparent behavior models represented as Behavior Trees, which could be used to alleviate the heaven endeavor of manual agent programming in game and simulation. Abstract: In modern training, entertainment and education applications, behavior trees (BTs) have already become a fantastic alternative to \ufb01nite state machines (FSMs) in modeling and controlling autonomous agents. However, it is expensive and inef\ufb01cient to create BTs for various task scenarios manually. Thus, the genetic programming (GP) approach has been devised to evolve BTs automatically but only received limited success. The standard GP approaches to evolve BTs fail to scale up and to provide good solutions, while GP approaches with domain-speci\ufb01c constraints can accelerate learning but need signi\ufb01cant knowledge engineering effort. In this paper, we propose a modi\ufb01ed approach, named evolving BTs with hybrid constraints (EBT-HC), to improve the evolution of BTs for autonomous agents. We \ufb01rst propose a novel idea of dynamic constraint based on frequent sub-trees mining, which can accelerate evolution by protecting preponderant behavior sub-trees from undesired crossover. Then we introduce the existing \u2018static\u2019 structural constraint into our dynamic constraint to form the evolving BTs with hybrid constraints. The static structure can constrain expected BT form to reduce the size of the search space, thus the hybrid constraints would lead more ef\ufb01cient learning and \ufb01nd better solutions without the loss of the domain-independence. Preliminary experiments, carried out on the Pac-Man game environment, show that the hybrid EBT-HC outperforms other approaches in facilitating the BT design by achieving better behavior performance within fewer generations. Moreover, the generated behavior models by EBT-HC are human readable and easy to be \ufb01ne-tuned by domain experts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145907511",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2110071401",
                    "name": "Jian Yao"
                },
                {
                    "authorId": "39660976",
                    "name": "Quanjun Yin"
                },
                {
                    "authorId": "2668652",
                    "name": "Yabing Zha"
                }
            ]
        }
    ]
}