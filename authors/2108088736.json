{
    "authorId": "2108088736",
    "papers": [
        {
            "paperId": "3436ff7a1dd4c6547ba78968d3eec2545a6dccb9",
            "title": "Fairness-guided Few-shot Prompting for Large Language Models",
            "abstract": "Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context learning. We perform comprehensive experiments with state-of-the-art mainstream models such as GPT-3 on various downstream tasks. Our results indicate that our method can enhance the model's in-context learning performance in an effective and interpretable manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110817620",
                    "name": "Huan Ma"
                },
                {
                    "authorId": "144704038",
                    "name": "Changqing Zhang"
                },
                {
                    "authorId": "2419616",
                    "name": "Yatao Bian"
                },
                {
                    "authorId": "2978364",
                    "name": "Lemao Liu"
                },
                {
                    "authorId": "4947404",
                    "name": "Zhirui Zhang"
                },
                {
                    "authorId": "144259957",
                    "name": "P. Zhao"
                },
                {
                    "authorId": "2108088736",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "1929093",
                    "name": "H. Fu"
                },
                {
                    "authorId": "2113360928",
                    "name": "Qinghua Hu"
                },
                {
                    "authorId": "2152564746",
                    "name": "Bing Wu"
                }
            ]
        },
        {
            "paperId": "372bc41602bbd21f192305775f0a58de9880e454",
            "title": "HIVE: Harnessing Human Feedback for Instructional Visual Editing",
            "abstract": "Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1.1M training dataset, a 3.6K reward dataset for rewards learning, and a 1 K evaluation dataset to boost the performance of instructional image editing. We conduct extensive empirical experiments quantitatively and qualitatively, showing that HIVE is favored over previous state-of-the-art instructional image editing approaches by a large margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108088736",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "2150441485",
                    "name": "Xinyi Yang"
                },
                {
                    "authorId": "22758695",
                    "name": "Yihao Feng"
                },
                {
                    "authorId": "12282768",
                    "name": "Can Qin"
                },
                {
                    "authorId": "2211960332",
                    "name": "Chia-Chih Chen"
                },
                {
                    "authorId": "2211974297",
                    "name": "Ning Yu"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "46507194",
                    "name": "Haiquan Wang"
                },
                {
                    "authorId": "1702137",
                    "name": "S. Savarese"
                },
                {
                    "authorId": "2490652",
                    "name": "Stefano Ermon"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "2115800155",
                    "name": "Ran Xu"
                }
            ]
        },
        {
            "paperId": "a24ab4c0d758fa62ce18ce8afa2d904563c365c8",
            "title": "Beyond Single Items: Exploring User Preferences in Item Sets with the Conversational Playlist Curation Dataset",
            "abstract": "Users in consumption domains, like music, are often able to more efficiently provide preferences over a set of items (e.g. a playlist or radio) than over single items (e.g. songs). Unfortunately, this is an underexplored area of research, with most existing recommendation systems limited to understanding preferences over single items. Curating an item set exponentiates the search space that recommender systems must consider (all subsets of items!): this motivates conversational approaches-where users explicitly state or refine their preferences and systems elicit preferences in natural language-as an efficient way to understand user needs. We call this task conversational item set curation and present a novel data collection methodology that efficiently collects realistic preferences about item sets in a conversational setting by observing both item-level and set-level feedback. We apply this methodology to music recommendation to build the Conversational Playlist Curation Dataset (CPCD), where we show that it leads raters to express preferences that would not be otherwise expressed. Finally, we propose a wide range of conversational retrieval models as baselines for this task and evaluate them on the dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2719924",
                    "name": "Arun Tejasvi Chaganty"
                },
                {
                    "authorId": "37866790",
                    "name": "Megan Leszczynski"
                },
                {
                    "authorId": "2108088736",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "1958631",
                    "name": "R. Ganti"
                },
                {
                    "authorId": "1680484",
                    "name": "K. Balog"
                },
                {
                    "authorId": "1803571",
                    "name": "Filip Radlinski"
                }
            ]
        },
        {
            "paperId": "a438edd03df28f1c51144ebe9d47c21a33a5dedc",
            "title": "Attention Paper: How Generative AI Reshapes Digital Shadow Industry?",
            "abstract": "The rapid development of digital economy has led to the emergence of various black and shadow internet industries, which pose potential risks that can be identified and managed through digital risk management (DRM) that uses different techniques such as machine learning and deep learning. The evolution of DRM architecture has been driven by changes in data forms. However, the development of AI-generated content (AIGC) technology, such as ChatGPT and Stable Diffusion, has given black and shadow industries powerful tools to personalize data and generate realistic images and conversations for fraudulent activities. This poses a challenge for DRM systems to control risks from the source of data generation and to respond quickly to the fast-changing risk environment. This paper aims to provide a technical analysis of the challenges and opportunities of AIGC from upstream, midstream, and downstream paths of black/shadow industries and suggest future directions for improving existing risk control systems. The paper will explore the new shadow techniques triggered by generative AI technology and provide insights for building the next-generation DRM system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143466459",
                    "name": "Qichao Wang"
                },
                {
                    "authorId": "2110817620",
                    "name": "Huan Ma"
                },
                {
                    "authorId": "2212773394",
                    "name": "Wen-Ke Wei"
                },
                {
                    "authorId": "2179851049",
                    "name": "Hang Li"
                },
                {
                    "authorId": "1853048147",
                    "name": "Liang Chen"
                },
                {
                    "authorId": "144259957",
                    "name": "P. Zhao"
                },
                {
                    "authorId": "2219035983",
                    "name": "Binwen Zhao"
                },
                {
                    "authorId": "2218788071",
                    "name": "Bo Hu"
                },
                {
                    "authorId": "2108088736",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "144291579",
                    "name": "Zibin Zheng"
                },
                {
                    "authorId": "2152564746",
                    "name": "Bing Wu"
                }
            ]
        },
        {
            "paperId": "a88e005e4a0e268d68cfd0bffc9975687b8ff918",
            "title": "GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation",
            "abstract": "Text-to-image (T2I) models based on diffusion processes have achieved remarkable success in controllable image generation using user-provided captions. However, the tight coupling between the current text encoder and image decoder in T2I models makes it challenging to replace or upgrade. Such changes often require massive fine-tuning or even training from scratch with the prohibitive expense. To address this problem, we propose GlueGen, which applies a newly proposed GlueNet model to align features from single-modal or multi-modal encoders with the latent space of an existing T2I model. The approach introduces a new training objective that leverages parallel corpora to align the representation spaces of different encoders. Empirical results show that GlueNet can be trained efficiently and enables various capabilities beyond previous state-of-the-art models: 1) multilingual language models such as XLM-Roberta can be aligned with existing T2I models, allowing for the generation of high-quality images from captions beyond English; 2) GlueNet can align multi-modal encoders such as AudioCLIP with the Stable Diffusion model, enabling sound-to-image generation; 3) it can also upgrade the current text encoder of the latent diffusion model for challenging case generation. By the alignment of various feature representations, the GlueNet allows for flexible and efficient integration of new functionality into existing T2I models and sheds light on X-to-image (X2I) generation.1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12282768",
                    "name": "Can Qin"
                },
                {
                    "authorId": "2052212417",
                    "name": "Ning Yu"
                },
                {
                    "authorId": "50461046",
                    "name": "Chen Xing"
                },
                {
                    "authorId": "2108088736",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "2490652",
                    "name": "Stefano Ermon"
                },
                {
                    "authorId": "2156255943",
                    "name": "Yun Fu"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "2115800155",
                    "name": "Ran Xu"
                }
            ]
        },
        {
            "paperId": "c0f1fb3314b42aeff192bcee8a723cb35cabd78e",
            "title": "Generating Synthetic Data for Conversational Music Recommendation Using Random Walks and Language Models",
            "abstract": "Conversational recommendation systems (CRSs) enable users to use natural language feedback to control their recommendations, overcoming many of the challenges of traditional recommendation systems. However, the practical adoption of CRSs remains limited due to a lack of rich and diverse conversational training data that pairs user utterances with recommendations. To address this problem, we introduce a new method to generate synthetic training data by transforming curated item collections, such as playlists or movie watch lists, into item-seeking conversations. First, we use a biased random walk to generate a sequence of slates, or sets of item recommendations; then, we use a language model to generate corresponding user utterances. We demonstrate our approach by generating a conversational music recommendation dataset with over one million conversations, which were found to be consistent with relevant recommendations by a crowdsourced evaluation. Using the synthetic data to train a CRS, we significantly outperform standard retrieval baselines in offline and online evaluations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37866790",
                    "name": "Megan Leszczynski"
                },
                {
                    "authorId": "1958631",
                    "name": "R. Ganti"
                },
                {
                    "authorId": "2108088736",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "1680484",
                    "name": "K. Balog"
                },
                {
                    "authorId": "2065812052",
                    "name": "Filip Radlinski"
                },
                {
                    "authorId": "2067616758",
                    "name": "Fernando Pereira"
                },
                {
                    "authorId": "2719924",
                    "name": "Arun Tejasvi Chaganty"
                }
            ]
        },
        {
            "paperId": "f32dd0fcb421aaf96593dc9c4be740a1399c9e64",
            "title": "Talk the Walk: Synthetic Data Generation for Conversational Music Recommendation",
            "abstract": "Recommender systems are ubiquitous yet often difficult for users to control, and adjust if recommendation quality is poor. This has motivated conversational recommender systems (CRSs), with control provided through natural language feedback. However, as with most application domains, building robust CRSs requires training data that reflects system usage$\\unicode{x2014}$here conversations with user utterances paired with items that cover a wide range of preferences. This has proved challenging to collect scalably using conventional methods. We address the question of whether it can be generated synthetically, building on recent advances in natural language. We evaluate in the setting of item set recommendation, noting the increasing attention to this task motivated by use cases like music, news, and recipe recommendation. We present TalkTheWalk, which synthesizes realistic high-quality conversational data by leveraging domain expertise encoded in widely available curated item collections, generating a sequence of hypothetical yet plausible item sets, then using a language model to produce corresponding user utterances. We generate over one million diverse playlist curation conversations in the music domain, and show these contain consistent utterances with relevant item sets nearly matching the quality of an existing but small human-collected dataset for this task. We demonstrate the utility of the generated synthetic dataset on a conversational item retrieval task and show that it improves over both unsupervised baselines and systems trained on a real dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37866790",
                    "name": "Megan Leszczynski"
                },
                {
                    "authorId": "1958631",
                    "name": "R. Ganti"
                },
                {
                    "authorId": "2108088736",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "1680484",
                    "name": "K. Balog"
                },
                {
                    "authorId": "2065812052",
                    "name": "Filip Radlinski"
                },
                {
                    "authorId": "2067616758",
                    "name": "Fernando Pereira"
                },
                {
                    "authorId": "2719924",
                    "name": "Arun Tejasvi Chaganty"
                }
            ]
        },
        {
            "paperId": "4aea3aa7c1f9b926a72e9242dab83db85f199e14",
            "title": "Vertical Federated Linear Contextual Bandits",
            "abstract": "In this paper, we investigate a novel problem of building contextual bandits in the vertical federated setting, i.e., contextual information is vertically distributed over different departments. This problem remains largely unexplored in the research community. To this end, we carefully design a customized encryption scheme named orthogonal matrix-based mask mechanism(O3M) for encrypting local contextual information while avoiding expensive conventional cryptographic techniques. We further apply the mechanism to two commonly-used bandit algorithms, LinUCB and LinTS, and instantiate two practical protocols for online recommendation under the vertical federated setting. The proposed protocols can perfectly recover the service quality of centralized bandit algorithms while achieving a satisfactory runtime efficiency, which is theoretically proved and analyzed in this paper. By conducting extensive experiments on both synthetic and real-world datasets, we show the superiority of the proposed method in terms of privacy protection and recommendation performance.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1409948823",
                    "name": "Zeyu Cao"
                },
                {
                    "authorId": "39763382",
                    "name": "Zhipeng Liang"
                },
                {
                    "authorId": "2108088736",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "2179851049",
                    "name": "Hang Li"
                },
                {
                    "authorId": "2068121557",
                    "name": "Ouyang Wen"
                },
                {
                    "authorId": "2188347331",
                    "name": "Yu Rong"
                },
                {
                    "authorId": "144259957",
                    "name": "P. Zhao"
                },
                {
                    "authorId": "2152564746",
                    "name": "Bing Wu"
                }
            ]
        },
        {
            "paperId": "2ec976a16d4280bcbffc70a47eb90a093e890920",
            "title": "Accurate Exercise Recommendation Based on Multidimension -al Feature Analysis",
            "abstract": "With the rapid development of the Internet, online learning has developed rapidly, and learners are increasingly demanding the personalization and practicality of exercise. Facing the massive exercises in the online learning platform and the online examination system, how to choose the exercises that can be targeted and can make up for the knowledge loopholes has become a hot topic in the field of personalized recommendation of current teaching resources. In view of the fact that learners have a variety of learning features, and there are a large number of online exercises, various types and varying degrees of difficulty, this paper proposes a precise recommendation method based on Multidimensional features analysis for exercises. It quantifies the potential relationship between the learner and the exercise from three aspects: the heat of the exercise itself, the relevance of the knowledge among the exercises, and the similarity of the learner\u2019s style, using the linear combination and the Learning to Rank method to build the recommendation model to match learner and exercises exactly. Experiments show that the mean average precision of the ReMFA method reaches 36.8% when recommending five candidate exercises, which can provide learners with personalized exercise recommendation services, thereby improving the learner\u2019s learning efficiency.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2108088736",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "2152629817",
                    "name": "Jiaqi Cai"
                },
                {
                    "authorId": "50102012",
                    "name": "Bin Zhuge"
                },
                {
                    "authorId": "2655054",
                    "name": "Ligang Dong"
                },
                {
                    "authorId": "2143392780",
                    "name": "Xian Jiang"
                }
            ]
        }
    ]
}