{
    "authorId": "1680188",
    "papers": [
        {
            "paperId": "15162e1285a7681d1ea109c2a4a2cfd69e1cfb0f",
            "title": "Ranking with Long-Term Constraints",
            "abstract": "The feedback that users provide through their choices (e.g., clicks, purchases) is one of the most common types of data readily available for training search and recommendation algorithms. However, myopically training systems based on choice data may only improve short-term engagement, but not the long-term sustainability of the platform and the long-term benefits to its users, content providers, and other stakeholders. In this paper, we thus develop a new framework in which decision makers (e.g., platform operators, regulators, users) can express long-term goals for the behavior of the platform (e.g., fairness, revenue distribution, legal requirements). These goals take the form of exposure or impact targets that go well beyond individual sessions, and we provide new control-based algorithms to achieve these goals. In particular, the controllers are designed to achieve the stated long-term goals with minimum impact on short-term engagement. Beyond the principled theoretical derivation of the controllers, we evaluate the algorithms on both synthetic and real-world data. While all controllers perform well, we find that they provide interesting trade-offs in efficiency, robustness, and the ability to plan ahead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "11963742",
                    "name": "Kiant\u00e9 Brantley"
                },
                {
                    "authorId": "40960149",
                    "name": "Zhichong Fang"
                },
                {
                    "authorId": "2166476321",
                    "name": "Sarah Dean"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                }
            ]
        },
        {
            "paperId": "95d498cf07cd54153ec128b49ae7abc7f9012aee",
            "title": "Evaluating a Learned Admission-Prediction Model as a Replacement for Standardized Tests in College Admissions",
            "abstract": "A growing number of college applications has presented an annual challenge for college admissions in the United States. Admission offices have historically relied on standardized test scores to organize large applicant pools into viable subsets for review. However, this approach may be subject to bias in test scores and selection bias in test-taking with recent trends toward test-optional admission. We explore a machine learning-based approach to replace the role of standardized tests in subset generation while taking into account a wide range of factors extracted from student applications to support a more holistic review. We evaluate the approach on data from an undergraduate admission office at a selective US institution (13,248 applications). We find that a prediction model trained on past admission data outperforms an SAT-based heuristic and matches the demographic composition of the last admitted class. We discuss the risks and opportunities for how such a learned model could be leveraged to support human decision-making in college admissions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49923195",
                    "name": "Hansol Lee"
                },
                {
                    "authorId": "2256875887",
                    "name": "Ren\u00e9 F. Kizilcec"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                }
            ]
        },
        {
            "paperId": "a509935ef0dbdb2f47d734afd8df5563f1219e91",
            "title": "Augmenting Holistic Review in University Admission using Natural Language Processing for Essays and Recommendation Letters",
            "abstract": "University admission at many highly selective institutions uses a holistic review process, where all aspects of the application, including protected attributes (e.g., race, gender), grades, essays, and recommendation letters are considered, to compose an excellent and diverse class. In this study, we empirically evaluate how influential protected attributes are for predicting admission decisions using a machine learning (ML) model, and in how far textual information (e.g., personal essay, teacher recommendation) may substitute for the loss of protected attributes in the model. Using data from 14,915 applicants to an undergraduate admission office at a selective U.S. institution in the 2022-2023 cycle, we find that the exclusion of protected attributes from the ML model leads to substantially reduced admission-prediction performance. The inclusion of textual information via both a TF-IDF representation and a Latent Dirichlet allocation (LDA) model partially restores model performance, but does not appear to provide a full substitute for admitting a similarly diverse class. In particular, while the text helps with gender diversity, the proportion of URM applicants is severely impacted by the exclusion of protected attributes, and the inclusion of new attributes generated from the textual information does not recover this performance loss.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120704206",
                    "name": "Jinsook Lee"
                },
                {
                    "authorId": "2005973324",
                    "name": "Bradon Thymes"
                },
                {
                    "authorId": "153823289",
                    "name": "Joyce Zhou"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                },
                {
                    "authorId": "2246853606",
                    "name": "Ren\u00e9 F. Kizilcec"
                }
            ]
        },
        {
            "paperId": "b92999c9935c9ce82f22a5482a6e56d01262b2b0",
            "title": "Variance-Minimizing Augmentation Logging for Counterfactual Evaluation in Contextual Bandits",
            "abstract": "Methods for offline A/B testing and counterfactual learning are seeing rapid adoption in search and recommender systems, since they allow efficient reuse of existing log data. However, there are fundamental limits to using existing log data alone, since the counterfactual estimators that are commonly used in these methods can have large bias and large variance when the logging policy is very different from the target policy being evaluated. To overcome this limitation, we explore the question of how to design data-gathering policies that most effectively augment an existing dataset of bandit feedback with additional observations for both learning and evaluation. To this effect, this paper introduces Minimum Variance Augmentation Logging (MVAL), a method for constructing logging policies that minimize the variance of the downstream evaluation or learning problem. We explore multiple approaches to computing MVAL policies efficiently, and find that they can be substantially more effective in decreasing the variance of an estimator than na\u00efve approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1485602352",
                    "name": "Aaron David Tucker"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                }
            ]
        },
        {
            "paperId": "d3e25ba6d41b622bbaed89d4904953df4a269ef8",
            "title": "How to Explain and Justify Almost Any Decision: Potential Pitfalls for Accountability in AI Decision-Making",
            "abstract": "Discussion of the \u201cright to an explanation\u201d has been increasingly relevant because of its potential utility for auditing automated decision systems, as well as for making objections to such decisions. However, most existing work on explanations focuses on collaborative environments, where designers are motivated to implement good-faith explanations that reveal potential weaknesses of a decision system. This motivation may not hold in an auditing environment. Thus, we ask: how much could explanations be used maliciously to defend a decision system? In this paper, we demonstrate how a black-box explanation system developed to defend a black-box decision system could manipulate decision recipients or auditors into accepting an intentionally discriminatory decision model. In a case-by-case scenario where decision recipients are unable to share their cases and explanations, we find that most individual decision recipients could receive a verifiable justification, even if the decision system is intentionally discriminatory. In a system-wide scenario where every decision is shared, we find that while justifications frequently contradict each other, there is no intuitive threshold to determine if these contradictions are because of malicious justifications or because of simplicity requirements of these justifications conflicting with model behavior. We end with discussion of how system-wide metrics may be more useful than explanation systems for evaluating overall decision fairness, while explanations could be useful outside of fairness auditing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153823289",
                    "name": "Joyce Zhou"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                }
            ]
        },
        {
            "paperId": "f57d3124689f973b97680c624d42eb0566d0770e",
            "title": "Off-Policy Evaluation for Large Action Spaces via Conjunct Effect Modeling",
            "abstract": "We study off-policy evaluation (OPE) of contextual bandit policies for large discrete action spaces where conventional importance-weighting approaches suffer from excessive variance. To circumvent this variance issue, we propose a new estimator, called OffCEM, that is based on the conjunct effect model (CEM), a novel decomposition of the causal effect into a cluster effect and a residual effect. OffCEM applies importance weighting only to action clusters and addresses the residual causal effect through model-based reward estimation. We show that the proposed estimator is unbiased under a new condition, called local correctness, which only requires that the residual-effect model preserves the relative expected reward differences of the actions within each cluster. To best leverage the CEM and local correctness, we also propose a new two-step procedure for performing model-based estimation that minimizes bias in the first step and variance in the second step. We find that the resulting OffCEM estimator substantially improves bias and variance compared to a range of conventional estimators. Experiments demonstrate that OffCEM provides substantial improvements in OPE especially in the presence of many actions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "47046740",
                    "name": "Yuta Saito"
                },
                {
                    "authorId": "101351368",
                    "name": "Qingyang Ren"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                }
            ]
        },
        {
            "paperId": "09b8c29eeb01b8e99ee15f4f277567c230ca2e31",
            "title": "Fairness in the First Stage of Two-Stage Recommender Systems",
            "abstract": "Manylarge-scalerecommendersystemsconsistof twostages, where the first stage focuses on efficiently generating a small subset of promising candidates from a huge pool of items for the second-stage model to curate final recommendations from. In this paper, we investigate how to ensure group fairness to the items in this two-stage paradigm. In particular, we find that existing first-stage recommenders might select an irrecoverably unfair set of candidates such that there is no hope for the second-stage recommender to deliver fair recommendations. To this end, we propose two threshold-policy selection rules that, given any relevance model of queries and items and a point-wise lower confidence bound on the expected number of relevant items for each policy, find near-optimal sets of candidates that contain enough relevant items in expectation from each group of items. To instantiate the rules, we demonstrate how to derive such confidence bounds from potentially partial and biased user feedback data, which are abundant in many large-scale recommender systems. In addition, we provide both finite-sample and asymptotic analysis of how close the two threshold selection rules are to the optimal thresholds. Beyond this theoretical analysis, we show empirically that these two rules can consistently select enough relevant items from each group while minimizing the size of the candidate sets for a wide range of settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40689613",
                    "name": "Lequn Wang"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                }
            ]
        },
        {
            "paperId": "301c1695302cb840da3dc06c364b7e495a20361e",
            "title": "Variance-Optimal Augmentation Logging for Counterfactual Evaluation in Contextual Bandits",
            "abstract": "Methods for offline A/B testing and counterfactual learning are seeing rapid adoption in search and recommender systems, since they allow efficient reuse of existing log data. However, there are fundamental limits to using existing log data alone, since the counterfactual estimators that are commonly used in these methods can have large bias and large variance when the logging policy is very different from the target policy being evaluated. To overcome this limitation, we explore the question of how to design data-gathering policies that most effectively augment an existing dataset of bandit feedback with additional observations for both learning and evaluation. To this effect, this paper introduces Minimum Variance Augmentation Logging (MVAL), a method for constructing logging policies that minimize the variance of the downstream evaluation or learning problem. We explore multiple approaches to computing MVAL policies efficiently, and find that they can be substantially more effective in decreasing the variance of an estimator than n\u00e4\u0131ve approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1485602352",
                    "name": "Aaron David Tucker"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                }
            ]
        },
        {
            "paperId": "46fd28f9ed54d2ad95150125bf679fb7d78e4434",
            "title": "Improving Screening Processes via Calibrated Subset Selection",
            "abstract": "Many selection processes such as finding patients qualifying for a medical trial or retrieval pipelines in search engines consist of multiple stages, where an initial screening stage focuses the resources on shortlisting the most promising candidates. In this paper, we investigate what guarantees a screening classifier can provide, independently of whether it is constructed manually or trained. We find that current solutions do not enjoy distribution-free theoretical guarantees -- we show that, in general, even for a perfectly calibrated classifier, there always exist specific pools of candidates for which its shortlist is suboptimal. Then, we develop a distribution-free screening algorithm -- called Calibrated Subset Selection (CSS) -- that, given any classifier and some amount of calibration data, finds near-optimal shortlists of candidates that contain a desired number of qualified candidates in expectation. Moreover, we show that a variant of CSS that calibrates a given classifier multiple times across specific groups can create shortlists with provable diversity guarantees. Experiments on US Census survey data validate our theoretical results and show that the shortlists provided by our algorithm are superior to those provided by several competitive baselines.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "40689613",
                    "name": "Lequn Wang"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                },
                {
                    "authorId": "2193200302",
                    "name": "Manuel Gomez Rodriguez"
                }
            ]
        },
        {
            "paperId": "5ce7d1281862354109a375b33c35f4b1b5df9ffa",
            "title": "Counterfactual Learning To Rank for Utility-Maximizing Query Autocompletion",
            "abstract": "Conventional methods for query autocompletion aim to predict which completed query a user will select from a list. A shortcoming of this approach is that users often do not know which query will provide the best retrieval performance on the current information retrieval system, meaning that any query autocompletion methods trained to mimic user behavior can lead to suboptimal query suggestions. To overcome this limitation, we propose a new approach that explicitly optimizes the query suggestions for downstream retrieval performance. We formulate this as a problem of ranking a set of rankings, where each query suggestion is represented by the downstream item ranking it produces. We then present a learning method that ranks query suggestions by the quality of their item rankings. The algorithm is based on a counterfactual learning approach that is able to leverage feedback on the items (e.g., clicks, purchases) to evaluate query suggestions through an unbiased estimator, thus avoiding the assumption that users write or select optimal queries. We establish theoretical support for the proposed approach and provide learning-theoretic guarantees. We also present empirical results on publicly available datasets, and demonstrate real-world applicability using data from an online shopping store.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "21590875",
                    "name": "A. Block"
                },
                {
                    "authorId": "1891978",
                    "name": "Rahul Kidambi"
                },
                {
                    "authorId": "2113373342",
                    "name": "Daniel N. Hill"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                },
                {
                    "authorId": "1783667",
                    "name": "I. Dhillon"
                }
            ]
        }
    ]
}