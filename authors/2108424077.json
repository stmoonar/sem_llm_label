{
    "authorId": "2108424077",
    "papers": [
        {
            "paperId": "5f08a63a0bb6eb1e0f5f8c9489c33cd63702796a",
            "title": "Controllable Data Augmentation for Few-Shot Text Mining with Chain-of-Thought Attribute Manipulation",
            "abstract": "Prompting large language models (LLMs) for data augmentation has recently become a common practice in few-shot NLP tasks. In this paper, we propose Chain-of-Thought Attribute Manipulation (CoTAM), a novel approach that generates new data from existing examples by only tweaking in the user-provided, task-specific attribute, e.g., sentiment polarity or topic in movie reviews. Instead of conventional latent representation controlling, we leverage the chain-of-thought prompting to directly edit the text in three steps, (1) attribute decomposition, (2) manipulation proposal, and (3) sentence reconstruction. Extensive results on various tasks, such as text (pair) classification, aspect-based sentiment analysis, and conditional text generation, verify the superiority of CoTAM over other LLM-based augmentation methods with the same number of training examples for both fine-tuning and in-context learning. Remarkably, the 2D visualization of the augmented dataset using principal component analysis revealed a human-recognizable decision boundary that is likely hinted by the attribute manipulation, demonstrating the potential of our proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265617343",
                    "name": "Letian Peng"
                },
                {
                    "authorId": "2108424077",
                    "name": "Yuwei Zhang"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "a7beaf4ad0c59ad6c91a03af6eceaafd2d44cef9",
            "title": "ClusterLLM: Large Language Models as a Guide for Text Clustering",
            "abstract": "We introduce ClusterLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT. Compared with traditional unsupervised methods that builds upon\"small\"embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the emergent capability of LLM even if its embeddings are inaccessible; and (2) it understands the user's preference on clustering through textual instruction and/or a few annotated data. First, we prompt ChatGPT for insights on clustering perspective by constructing hard triplet questions, where A, B and C are similar data points that belong to different clusters according to small embedder. We empirically show that this strategy is both effective for fine-tuning small embedder and cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on clustering granularity by carefully designed pairwise questions, and tune the granularity from cluster hierarchies that is the most consistent with the ChatGPT answers. Extensive experiments on 14 datasets show that ClusterLLM consistently improves clustering quality, at an average cost of ~$0.6 per dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108424077",
                    "name": "Yuwei Zhang"
                },
                {
                    "authorId": "2240689",
                    "name": "Zihan Wang"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "b2fcce1e640e761522030e55f3f9ab53e3b14963",
            "title": "Toward Unsupervised Realistic Visual Question Answering",
            "abstract": "The problem of realistic VQA (RVQA), where a model has to reject unanswerable questions (UQs) and answer answerable ones (AQs), is studied. We first point out 2 drawbacks in current RVQA research, where (1) datasets contain too many unchallenging UQs and (2) a large number of annotated UQs are required for training. To resolve the first drawback, we propose a new testing dataset, RGQA, which combines AQs from an existing VQA dataset with around 29K human-annotated UQs. These UQs consist of both fine-grained and coarse-grained image-question pairs generated with 2 approaches: CLIP-based and Perturbation-based. To address the second drawback, we introduce an unsupervised training approach. This combines pseudo UQs obtained by randomly pairing images and questions, with an RoI Mixup procedure to generate more fine-grained pseudo UQs, and model ensembling to regularize model confidence. Experiments show that using pseudo UQs significantly outperforms RVQA baselines. RoI Mixup and model ensembling further increase the gain. Finally, human evaluation reveals a performance gap between humans and models, showing that more RVQA research is needed. Code and dataset is released on https://github.com/chihhuiho/RGQA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108424077",
                    "name": "Yuwei Zhang"
                },
                {
                    "authorId": "51162868",
                    "name": "Chih-Hui Ho"
                },
                {
                    "authorId": "1699559",
                    "name": "N. Vasconcelos"
                }
            ]
        },
        {
            "paperId": "477ca6071e7ecee225580e61ab6ff6166a82d06d",
            "title": "Fine-tuning Pre-trained Language Models for Few-shot Intent Detection: Supervised Pre-training and Isotropization",
            "abstract": "It is challenging to train a good intent classifier for a task-oriented dialogue system with only a few annotations. Recent studies have shown that fine-tuning pre-trained language models with a small set of labeled utterances from public benchmarks in a supervised manner is extremely helpful. However, we find that supervised pre-training yields an anisotropic feature space, which may suppress the expressive power of the semantic representations. Inspired by recent research in isotropization, we propose to improve supervised pre-training by regularizing the feature space towards isotropy. We propose two regularizers based on contrastive learning and correlation matrix respectively, and demonstrate their effectiveness through extensive experiments. Our main finding is that it is promising to regularize supervised pre-training with isotropization to further improve the performance of few-shot intent detection. The source code can be found at https://github.com/fanolabs/isoIntentBert-main.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135719214",
                    "name": "Haode Zhang"
                },
                {
                    "authorId": "2152874181",
                    "name": "Haowen Liang"
                },
                {
                    "authorId": "2108424077",
                    "name": "Yuwei Zhang"
                },
                {
                    "authorId": "2061239456",
                    "name": "Li-Ming Zhan"
                },
                {
                    "authorId": "19195265",
                    "name": "Xiao-Ming Wu"
                },
                {
                    "authorId": "1491232153",
                    "name": "Xiaolei Lu"
                },
                {
                    "authorId": "1902169",
                    "name": "Albert Y. S. Lam"
                }
            ]
        },
        {
            "paperId": "4df0f9503cea90e9535d0d4e3ae55fb422a3fd5b",
            "title": "New Intent Discovery with Pre-training and Contrastive Learning",
            "abstract": "New intent discovery aims to uncover novel intent categories from user utterances to expand the set of supported intent classes. It is a critical task for the development and service expansion of a practical dialogue system. Despite its importance, this problem remains under-explored in the literature. Existing approaches typically rely on a large amount of labeled utterances and employ pseudo-labeling methods for representation learning and clustering, which are label-intensive, inefficient, and inaccurate. In this paper, we provide new solutions to two important research questions for new intent discovery: (1) how to learn semantic utterance representations and (2) how to better cluster utterances. Particularly, we first propose a multi-task pre-training strategy to leverage rich unlabeled data along with external labeled data for representation learning. Then, we design a new contrastive loss to exploit self-supervisory signals in unlabeled data for clustering. Extensive experiments on three intent recognition benchmarks demonstrate the high effectiveness of our proposed method, which outperforms state-of-the-art methods by a large margin in both unsupervised and semi-supervised scenarios. The source code will be available at https://github.com/zhang-yu-wei/MTP-CLNN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108424077",
                    "name": "Yuwei Zhang"
                },
                {
                    "authorId": "2135719214",
                    "name": "Haode Zhang"
                },
                {
                    "authorId": "2061239456",
                    "name": "Li-Ming Zhan"
                },
                {
                    "authorId": "19195265",
                    "name": "Xiao-Ming Wu"
                },
                {
                    "authorId": "2165226317",
                    "name": "A. Lam"
                }
            ]
        },
        {
            "paperId": "4428c9bcff28d54b8816cd670bcbe5c516ea21bc",
            "title": "KuraNet: Systems of Coupled Oscillators that Learn to Synchronize",
            "abstract": "Networks of coupled oscillators are some of the most studied objects in the theory of dynamical systems. Two important areas of current interest are the study of synchrony in highly disordered systems and the modeling of systems with adaptive network structures. Here, we present a single approach to both of these problems in the form of\"KuraNet\", a deep-learning-based system of coupled oscillators that can learn to synchronize across a distribution of disordered network conditions. The key feature of the model is the replacement of the traditionally static couplings with a coupling function which can learn optimal interactions within heterogeneous oscillator populations. We apply our approach to the eponymous Kuramoto model and demonstrate how KuraNet can learn data-dependent coupling structures that promote either global or cluster synchrony. For example, we show how KuraNet can be used to empirically explore the conditions of global synchrony in analytically impenetrable models with disordered natural frequencies, external field strengths, and interaction delays. In a sequence of cluster synchrony experiments, we further show how KuraNet can function as a data classifier by synchronizing into coherent assemblies. In all cases, we show how KuraNet can generalize to both new data and new network scales, making it easy to work with small systems and form hypotheses about the thermodynamic limit. Our proposed learning-based approach is broadly applicable to arbitrary dynamical systems with wide-ranging relevance to modeling in physics and systems biology.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "39105519",
                    "name": "Matthew Ricci"
                },
                {
                    "authorId": "144248119",
                    "name": "Minju Jung"
                },
                {
                    "authorId": "2108424077",
                    "name": "Yuwei Zhang"
                },
                {
                    "authorId": "1491552732",
                    "name": "Mathieu Chalvidal"
                },
                {
                    "authorId": "2088764671",
                    "name": "Aneri Soni"
                },
                {
                    "authorId": "1981539",
                    "name": "Thomas Serre"
                }
            ]
        },
        {
            "paperId": "c80436a9d7ec5d832e4875c89829294fc25841fd",
            "title": "Effectiveness of Pre-training for Few-shot Intent Classification",
            "abstract": "This paper investigates the effectiveness of pre-training for few-shot intent classification. While existing paradigms commonly further pre-train language models such as BERT on a vast amount of unlabeled corpus, we find it highly effective and efficient to simply fine-tune BERT with a small set of labeled utterances from public datasets. Specifically, fine-tuning BERT with roughly 1,000 labeled data yields a pre-trained model -- IntentBERT, which can easily surpass the performance of existing pre-trained models for few-shot intent classification on novel domains with very different semantics. The high effectiveness of IntentBERT confirms the feasibility and practicality of few-shot intent detection, and its high generalization ability across different domains suggests that intent classification tasks may share a similar underlying structure, which can be efficiently learned from a small set of labeled data. The source code can be found at https://github.com/hdzhang-code/IntentBERT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135719214",
                    "name": "Haode Zhang"
                },
                {
                    "authorId": "2108424077",
                    "name": "Yuwei Zhang"
                },
                {
                    "authorId": "2061239456",
                    "name": "Li-Ming Zhan"
                },
                {
                    "authorId": "2124957356",
                    "name": "Jiaxin Chen"
                },
                {
                    "authorId": "144218801",
                    "name": "Guangyuan Shi"
                },
                {
                    "authorId": "19195265",
                    "name": "Xiao-Ming Wu"
                },
                {
                    "authorId": "1902169",
                    "name": "Albert Y. S. Lam"
                }
            ]
        }
    ]
}