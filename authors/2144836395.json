{
    "authorId": "2144836395",
    "papers": [
        {
            "paperId": "731d5b03567abb0dd3f6e1a286ace2eebc40b2d2",
            "title": "Compact Graph Structure Learning via Mutual Information Compression",
            "abstract": "Graph Structure Learning (GSL) recently has attracted considerable attentions in its capacity of optimizing graph structure as well as learning suitable parameters of Graph Neural Networks (GNNs) simultaneously. Current GSL methods mainly learn an optimal graph structure (final view) from single or multiple information sources (basic views), however the theoretical guidance on what is the optimal graph structure is still unexplored. In essence, an optimal graph structure should only contain the information about tasks while compress redundant noise as much as possible, which is defined as \u201dminimal sufficient structure\u201d, so as to maintain the accurancy and robustness. How to obtain such structure in a principled way? In this paper, we theoretically prove that if we optimize basic views and final view based on mutual information, and keep their performance on labels simultaneously, the final view will be a minimal sufficient structure. With this guidance, we propose a Compact GSL architecture by MI compression, named CoGSL. Specifically, two basic views are extracted from original graph as two inputs of the model, which are refinedly reestimated by a view estimator. Then, we propose an adaptive technique to fuse estimated views into the final view. Furthermore, we maintain the performance of estimated views and the final view and reduce the mutual information of every two views. To comprehensively evaluate the performance of CoGSL, we conduct extensive experiments on several datasets under clean and attacked conditions, which demonstrate the effectiveness and robustness of CoGSL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087010708",
                    "name": "Nian Liu"
                },
                {
                    "authorId": "2118449003",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "3008832",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "2144836395",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "46909769",
                    "name": "Xiaojie Guo"
                },
                {
                    "authorId": "2151458697",
                    "name": "Chuan Shi"
                }
            ]
        },
        {
            "paperId": "0ddd6dcf82032e0c5e10427502941b467167a518",
            "title": "Deep Learning on Graphs for Natural Language Processing",
            "abstract": "Due to its great power in modeling non-Euclidean data like graphs or manifolds, deep learning on graph techniques (i.e., Graph Neural Networks (GNNs)) have opened a new door to solving challenging graph-related NLP problems. There has seen a surge of interests in applying deep learning on graph techniques to NLP, and has achieved considerable success in many NLP tasks, ranging from classification tasks like sentence classification, semantic role labeling and relation extraction, to generation tasks like machine translation, question generation and summarization. Despite these successes, deep learning on graphs for NLP still face many challenges, including automatically transforming original text sequence data into highly graph-structured data, and effectively modeling complex data that involves mapping between graph-based inputs and other highly structured output data such as sequences, trees, and graph data with multi-types in both nodes and edges. This tutorial will cover relevant and interesting topics on applying deep learning on graph techniques to NLP, including automatic graph construction for NLP, graph representation learning for NLP, advanced GNN based models (e.g., graph2seq, graph2tree, and graph2graph) for NLP, and the applications of GNNs in various NLP tasks (e.g., machine translation, natural language generation, information extraction and semantic parsing). In addition, hands-on demonstration sessions will be included to help the audience gain practical experience on applying GNNs to solve challenging NLP problems using our recently developed open source library \u2013 Graph4NLP, the first library for researchers and practitioners for easy use of GNNs for various NLP tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3008832",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "2144836395",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "144016781",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "1573872877",
                    "name": "Yunyao Li"
                }
            ]
        },
        {
            "paperId": "ea443ffb40ddde78172f059f027f1d5fe148013c",
            "title": "Graph Neural Networks for Natural Language Processing: A Survey",
            "abstract": "Deep learning has become the dominant approach in coping with various tasks in Natural LanguageProcessing (NLP). Although text inputs are typically represented as a sequence of tokens, there isa rich variety of NLP problems that can be best expressed with a graph structure. As a result, thereis a surge of interests in developing new deep learning techniques on graphs for a large numberof NLP tasks. In this survey, we present a comprehensive overview onGraph Neural Networks(GNNs) for Natural Language Processing. We propose a new taxonomy of GNNs for NLP, whichsystematically organizes existing research of GNNs for NLP along three axes: graph construction,graph representation learning, and graph based encoder-decoder models. We further introducea large number of NLP applications that are exploiting the power of GNNs and summarize thecorresponding benchmark datasets, evaluation metrics, and open-source codes. Finally, we discussvarious outstanding challenges for making the full use of GNNs for NLP as well as future researchdirections. To the best of our knowledge, this is the first comprehensive overview of Graph NeuralNetworks for Natural Language Processing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257138073",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "2144836395",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "2257049632",
                    "name": "Kai Shen"
                },
                {
                    "authorId": "2257200456",
                    "name": "Xiaojie Guo"
                },
                {
                    "authorId": "116267999",
                    "name": "Hanning Gao"
                },
                {
                    "authorId": "2109104122",
                    "name": "Shucheng Li"
                },
                {
                    "authorId": "2257049357",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "2257044304",
                    "name": "Bo Long"
                }
            ]
        },
        {
            "paperId": "fb72dd373cc88a4e4300742a986799ece457f836",
            "title": "Personalized Food Recommendation as Constrained Question Answering over a Large-scale Food Knowledge Graph",
            "abstract": "Food recommendation has become an important means to help guide users to adopt healthy dietary habits. Previous works on food recommendation either i) fail to consider users' explicit requirements, ii) ignore crucial health factors (e.g., allergies and nutrition needs), or iii) do not utilize the rich food knowledge for recommending healthy recipes. To address these limitations, we propose a novel problem formulation for food recommendation, modeling this task as constrained question answering over a large-scale food knowledge base/graph (KBQA). Besides the requirements from the user query, personalized requirements from the user's dietary preferences and health guidelines are handled in a unified way as additional constraints to the QA system. To validate this idea, we create a QA style dataset for personalized food recommendation based on a large-scale food knowledge graph and health guidelines. Furthermore, we propose a KBQA-based personalized food recommendation framework which is equipped with novel techniques for handling negations and numerical comparisons in the queries. Experimental results on the benchmark show that our approach significantly outperforms non-personalized counterparts (average 59.7% absolute improvement across various evaluation metrics), and is able to recommend more relevant and healthier recipes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144836395",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "3393606",
                    "name": "Ananya Subburathinam"
                },
                {
                    "authorId": "2110286922",
                    "name": "Ching-Hua Chen"
                },
                {
                    "authorId": "1693515",
                    "name": "Mohammed J. Zaki"
                }
            ]
        },
        {
            "paperId": "d2422ea5933ad742117d0f03986da54d6409af1c",
            "title": "Combining User Preferences and Health Needs in Personalized Food Recommendation",
            "abstract": "We frame the problem of generating personalized food recommendations as a constrained question answering task over a food knowledge graph (KG). In particular, we consider the case where the question combines a user\u2019s basic query (e.g., \u201cWhat is a good breakfast that contains bread?\u201d) with the unique health requirements (e.g., allergies, nutritional guidelines that they need to adhere to) of the user. While the basic query is assumed to be dynamic in the sense that it may be context dependent, a user\u2019s health requirements may be relatively stable. We consider the latter to be a user\u2019s \u201cpersonalized\u201d requirements, and treat them as assumed extensions to any basic query. While state-of-the-art question answering over knowledge bases (KBQA) methods exist, we show that for the type of question we are interested in answering (i.e., natural language questions that append several template-based constraints to a freely-formed basic query) the QA system benefits significantly from the query expansion, KG augmentation and constraint modeling methods that we apply in our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144836395",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "2110286922",
                    "name": "Ching-Hua Chen"
                },
                {
                    "authorId": "2057472458",
                    "name": "Mohammed J. Zaki"
                }
            ]
        },
        {
            "paperId": "d365f9c805d59788f9ae5ad36fee69f9abd8d3c7",
            "title": "Toward Subgraph-Guided Knowledge Graph Question Generation With Graph Neural Networks",
            "abstract": "Knowledge graph (KG) question generation (QG) aims to generate natural language questions from KGs and target answers. Previous works mostly focus on a simple setting that is to generate questions from a single KG triple. In this work, we focus on a more realistic setting where we aim to generate questions from a KG subgraph and target answers. In addition, most previous works built on either RNN- or Transformer-based models to encode a linearized KG subgraph, which totally discards the explicit structure information of a KG subgraph. To address this issue, we propose to apply a bidirectional Graph2Seq model to encode the KG subgraph. Furthermore, we enhance our RNN decoder with a node-level copying mechanism to allow direct copying of node attributes from the KG subgraph to the output question. Both automatic and human evaluation results demonstrate that our model achieves new state-of-the-art scores, outperforming existing methods by a significant margin on two QG benchmarks. Experimental results also show that our QG model can consistently benefit the question-answering (QA) task as a means of data augmentation.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2144836395",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "3008832",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "1693515",
                    "name": "Mohammed J. Zaki"
                }
            ]
        },
        {
            "paperId": "ff6a4a9a41b78c8b1fcab185db780266bbb06caf",
            "title": "Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings",
            "abstract": "In this paper, we propose an end-to-end graph learning framework, namely Iterative Deep Graph Learning (IDGL), for jointly and iteratively learning graph structure and graph embedding. The key rationale of IDGL is to learn a better graph structure based on better node embeddings, and vice versa (i.e., better node embeddings based on a better graph structure). Our iterative method dynamically stops when the learned graph approaches close enough to the graph optimized for the prediction task. In addition, we cast the graph learning problem as a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph. Finally, combining the anchor-based approximation technique, we further propose a scalable version of IDGL, namely IDGL-ANCH, which significantly reduces the time and space complexity of IDGL without compromising the performance. Our extensive experiments on nine benchmarks show that our proposed IDGL models can consistently outperform or match state-of-the-art baselines. Furthermore, IDGL can be more robust to adversarial graphs and cope with both transductive and inductive learning.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2144836395",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "3008832",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "1693515",
                    "name": "Mohammed J. Zaki"
                }
            ]
        },
        {
            "paperId": "d513b7fdb47290a63786aea5822b8171c8926560",
            "title": "Deep Iterative and Adaptive Learning for Graph Neural Networks",
            "abstract": "In this paper, we propose an end-to-end graph learning framework, namely Deep Iterative and Adaptive Learning for Graph Neural Networks (DIAL-GNN), for jointly learning the graph structure and graph embeddings simultaneously. We first cast the graph structure learning problem as a similarity metric learning problem and leverage an adapted graph regularization for controlling smoothness, connectivity and sparsity of the generated graph. We further propose a novel iterative method for searching for a hidden graph structure that augments the initial graph structure. Our iterative method dynamically stops when the learned graph structure approaches close enough to the optimal graph. Our extensive experiments demonstrate that the proposed DIAL-GNN model can consistently outperform or match state-of-the-art baselines in terms of both downstream task performance and computational time. The proposed approach can cope with both transductive learning and inductive learning.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2144836395",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "3008832",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "1693515",
                    "name": "Mohammed J. Zaki"
                }
            ]
        },
        {
            "paperId": "e47e6c814d2742527fdd352db13a5fd95b7ce24b",
            "title": "Reinforcement Learning Based Graph-to-Sequence Model for Natural Question Generation",
            "abstract": "Natural question generation (QG) aims to generate questions from a passage and an answer. Previous works on QG either (i) ignore the rich structure information hidden in text, (ii) solely rely on cross-entropy loss that leads to issues like exposure bias and inconsistency between train/test measurement, or (iii) fail to fully exploit the answer information. To address these limitations, in this paper, we propose a reinforcement learning (RL) based graph-to-sequence (Graph2Seq) model for QG. Our model consists of a Graph2Seq generator with a novel Bidirectional Gated Graph Neural Network based encoder to embed the passage, and a hybrid evaluator with a mixed objective combining both cross-entropy and RL losses to ensure the generation of syntactically and semantically valid text. We also introduce an effective Deep Alignment Network for incorporating the answer information into the passage at both the word and contextual levels. Our model is end-to-end trainable and achieves new state-of-the-art scores, outperforming existing methods by a significant margin on the standard SQuAD benchmark.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144836395",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "3008832",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "1693515",
                    "name": "Mohammed J. Zaki"
                }
            ]
        },
        {
            "paperId": "3f8a63906ca762f84df099d68f7a228564ef05bb",
            "title": "Comparative text analytics via topic modeling in banking",
            "abstract": "In this paper, we compare and evaluate multiple topic modeling approaches and their effectiveness in analyzing a large set of SEC filings by US public banks. More specifically, we apply four major topic modeling methods to a corpus of 8-K and 10-K filings, from the years 2005\u20132016, of 578 bank holding companies. These methods include Principal Component Analysis, Non-negative Matrix Factorization, Latent Dirichlet Allocation and KATE, a novel k-competitive autoencoder for text documents. Separately for 8-K and 10-K, the usefulness and effectiveness of these methods is evaluated by comparing their performances on two classification tasks: (i) predicting which section each document corresponds to, where we consider each section within an 8-K or 10-K filing as an individual document, and (ii) detecting text from a bank's year of failure, a task for which we use bank failure data from the 2008 financial crisis. In addition, we qualitatively compare the topics discovered by the different methods. We conclude that topic modeling can be an effective tool in financial decision making and risk management.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144836395",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "2098913227",
                    "name": "Rhaad M. Rabbani"
                },
                {
                    "authorId": "2109972396",
                    "name": "Aparna Gupta"
                },
                {
                    "authorId": "1693515",
                    "name": "Mohammed J. Zaki"
                }
            ]
        }
    ]
}