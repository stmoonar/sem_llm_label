{
    "authorId": "20400898",
    "papers": [
        {
            "paperId": "3a20d70116ffc4f8edab97b13638771a6f22f9ba",
            "title": "Data poisoning attacks on off-policy policy evaluation methods",
            "abstract": "Off-policy Evaluation (OPE) methods are a crucial tool for evaluating policies in high-stakes domains such as healthcare, where exploration is often infeasible, unethical, or expensive. However, the extent to which such methods can be trusted under adversarial threats to data quality is largely unexplored. In this work, we make the first attempt at investigating the sensitivity of OPE methods to marginal adversarial perturbations to the data. We design a generic data poisoning attack framework leveraging influence functions from robust statistics to carefully construct perturbations that maximize error in the policy value estimates. We carry out extensive experimentation with multiple healthcare and control datasets. Our results demonstrate that many existing OPE methods are highly prone to generating value estimates with large errors when subject to data poisoning attacks, even for small adversarial perturbations. These findings question the reliability of policy values derived using OPE methods and motivate the need for developing OPE methods that are statistically robust to train-time data poisoning attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35597739",
                    "name": "Elita Lobo"
                },
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                },
                {
                    "authorId": "145630605",
                    "name": "Marek Petrik"
                },
                {
                    "authorId": "2060638599",
                    "name": "C. Rudin"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "0a12b6670bef76099e8ea30798a6a00b50cb164e",
            "title": "Measures of Disparity and their Efficient Estimation",
            "abstract": "Quantifying disparities, that is differences in outcomes among population groups, is an important task in public health, economics, and increasingly in machine learning. In this work, we study the question of how to collect data to measure disparities. The field of survey statistics provides extensive guidance on sample sizes necessary to accurately estimate quantities such as averages. However, there is limited guidance for estimating disparities. We consider a broad class of disparity metrics including those used in machine learning for measuring fairness of model outputs. For each metric, we derive the number of samples to be collected per group that increases the precision of disparity estimates given a fixed data collection budget. We also provide sample size calculations for hypothesis tests that check for significant disparities. Our methods can be used to determine sample sizes for fairness evaluations. We validate the methods on two nationwide surveys, used for understanding population-level attributes like employment and health, and a prediction model. Absent a priori information on the groups, we find that equally sampling the groups typically performs well.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                },
                {
                    "authorId": "3144230",
                    "name": "R. Chunara"
                }
            ]
        },
        {
            "paperId": "ed0bf9fe699a6f426f18ea92fe96e368c3d95888",
            "title": "When do Minimax-fair Learning and Empirical Risk Minimization Coincide?",
            "abstract": "Minimax-fair machine learning minimizes the error for the worst-off group. However, empirical evidence suggests that when sophisticated models are trained with standard empirical risk minimization (ERM), they often have the same performance on the worst-off group as a minimax-trained model. Our work makes this counter-intuitive observation concrete. We prove that if the hypothesis class is sufficiently expressive and the group information is recoverable from the features, ERM and minimax-fairness learning formulations indeed have the same performance on the worst-off group. We provide additional empirical evidence of how this observation holds on a wide range of datasets and hypothesis classes. Since ERM is fundamentally easier than minimax optimization, our findings have implications on the practice of fair machine learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                },
                {
                    "authorId": "2871632",
                    "name": "Matth\u00e4us Kleindessner"
                },
                {
                    "authorId": "1678641",
                    "name": "V. Cevher"
                },
                {
                    "authorId": "3144230",
                    "name": "R. Chunara"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                }
            ]
        },
        {
            "paperId": "0e9e19ec4512d1398ff4860f3d352a7492a228ae",
            "title": "\"Why did the Model Fail?\": Attributing Model Performance Changes to Distribution Shifts",
            "abstract": "Machine learning models frequently experience performance drops under distribution shifts. The underlying cause of such shifts may be multiple simultaneous factors such as changes in data quality, differences in specific covariate distributions, or changes in the relationship between label and features. When a model does fail during deployment, attributing performance change to these factors is critical for the model developer to identify the root cause and take mitigating actions. In this work, we introduce the problem of attributing performance differences between environments to distribution shifts in the underlying data generating mechanisms. We formulate the problem as a cooperative game where the players are distributions. We define the value of a set of distributions to be the change in model performance when only this set of distributions has changed between environments, and derive an importance weighting method for computing the value of an arbitrary set of distributions. The contribution of each distribution to the total performance change is then quantified as its Shapley value. We demonstrate the correctness and utility of our method on synthetic, semi-synthetic, and real-world case studies, showing its effectiveness in attributing performance changes to a wide range of distribution shifts.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2135690782",
                    "name": "Haoran Zhang"
                },
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                },
                {
                    "authorId": "2804918",
                    "name": "M. Ghassemi"
                },
                {
                    "authorId": "34287745",
                    "name": "Shalmali Joshi"
                }
            ]
        },
        {
            "paperId": "22bc490e8703230f959c226f1e66e735b043782d",
            "title": "Fair, Robust, and Data-Efficient Machine Learning in Healthcare",
            "abstract": "While machine learning systems have shown improvements, often, in carefully curated settings, challenges still exist to their wider deployment, especially for making consequential decisions. The research described here explores three challenges, particularly, emphasizing the interesting issues that arise at their intersection. How do we design machine learning systems to account for the systemic biases of the world, to act reliably under unseen settings, and to handle limited availability of data? Human-facing applications of machine learning such as personalized health commonly encounter these challenges, thus, these are important to address. The research has three components addressing different parts of the above central question. Here, we describe the work done on two components of the above central question and highlight the future work planned as part of the third one. We draw from methods in causal inference, algorithmic fairness, and interactive learning, and apply them to applications in health.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                }
            ]
        },
        {
            "paperId": "4b939f9b4526458ec444732c08e4422a721ef3e1",
            "title": "Towards Robust Off-Policy Evaluation via Human Inputs",
            "abstract": "Off-policy Evaluation (OPE) methods are crucial tools for evaluating policies in high-stakes domains such as healthcare, where direct deployment is often infeasible, unethical, or expensive. When deployment environments are expected to undergo changes (that is, dataset shifts), it is important for OPE methods to perform robust evaluation of the policies amidst such changes. Existing approaches consider robustness against a large class of shifts that can arbitrarily change any observable property of the environment. This often results in highly pessimistic estimates of the utilities, thereby invalidating policies that might have been useful in deployment. In this work, we address the aforementioned problem by investigating how domain knowledge can help provide more realistic estimates of the utilities of policies. We leverage human inputs on which aspects of the environments may plausibly change, and adapt the OPE methods to only consider shifts on these aspects. Specifically, we propose a novel framework, Robust OPE (ROPE), which considers shifts on a subset of covariates in the data based on user inputs, and estimates worst-case utility under these shifts. We then develop computationally efficient algorithms for OPE that are robust to the aforementioned shifts for contextual bandits and Markov decision processes. We also theoretically analyze the sample complexity of these algorithms. Extensive experimentation with synthetic and real world datasets from the healthcare domain demonstrates that our approach not only captures realistic dataset shifts accurately, but also results in less pessimistic policy evaluations.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                },
                {
                    "authorId": "34287745",
                    "name": "Shalmali Joshi"
                },
                {
                    "authorId": "1388372395",
                    "name": "F. Doshi-Velez"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "9212c2c87177a9a634957e1448c38f50c7959657",
            "title": "Segmenting across places: The need for fair transfer learning with satellite imagery",
            "abstract": "The increasing availability of high-resolution satellite imagery has enabled the use of machine learning to support land-cover measurement and inform policy-making. However, labelling satellite images is expensive and is available for only some locations. This prompts the use of transfer learning to adapt models from data-rich locations to others. Given the potential for high-impact applications of satellite imagery across geographies, a systematic assessment of transfer learning implications is warranted. In this work, we consider the task of land-cover segmentation and study the fairness implications of transferring models across locations. We leverage a large satellite image segmentation benchmark with 5987 images from 18 districts (9 urban and 9 rural). Via fairness metrics we quantify disparities in model performance along two axes \u2013 across urban-rural locations and across land-cover classes. Findings show that state-of-the-art models have better overall accuracy in rural areas compared to urban areas, through unsupervised domain adaptation methods transfer learning better to urban versus rural areas and enlarge fairness gaps. In analysis of reasons for these findings, we show that raw satellite images are overall more dissimilar between source and target districts for rural than for urban locations. This work highlights the need to conduct fairness analysis for satellite imagery segmentation models and motivates the development of methods for fair transfer learning in order not to introduce disparities between places, particularly urban and rural locations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150238772",
                    "name": "Miaohui Zhang"
                },
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                },
                {
                    "authorId": "2241884886",
                    "name": "Lazarus Chok"
                },
                {
                    "authorId": "3144230",
                    "name": "R. Chunara"
                }
            ]
        },
        {
            "paperId": "2f58d4a4709933fb1030ab29b14430862418bd49",
            "title": "Learning Under Adversarial and Interventional Shifts",
            "abstract": "Machine learning models are often trained on data from one distribution and deployed on others. So it becomes important to design models that are robust to distribution shifts. Most of the existing work focuses on optimizing for either adversarial shifts or interventional shifts. Adversarial methods lack expressivity in representing plausible shifts as they consider shifts to joint distributions in the data. Interventional methods allow more expressivity but provide robustness to unbounded shifts, resulting in overly conservative models. In this work, we combine the complementary strengths of the two approaches and propose a new formulation, RISe, for designing robust models against a set of distribution shifts that are at the intersection of adversarial and interventional shifts. We employ the distributionally robust optimization framework to optimize the resulting objective in both supervised and reinforcement learning settings. Extensive experimentation with synthetic and real world datasets from healthcare demonstrate the efficacy of the proposed approach.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                },
                {
                    "authorId": "34287745",
                    "name": "Shalmali Joshi"
                },
                {
                    "authorId": "1388372395",
                    "name": "F. Doshi-Velez"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "14b5bcb76c9140b474881b9ea9db23f8cd58bec4",
            "title": "An RNN-Survival Model to Decide Email Send Times",
            "abstract": "Email communications are ubiquitous. Firms control send times of emails and thereby the instants at which emails reach recipients (it is assumed email is received instantaneously from the send time). However, they do not control the duration it takes for recipients to open emails, labeled as time-to-open. Importantly, among emails that are opened, most occur within a short window from their send times. We posit that emails are likely to be opened sooner when send times are convenient for recipients, while for other send times, emails can get ignored. Thus, to compute appropriate send times it is important to predict times-to-open accurately. We propose a recurrent neural network (RNN) in a survival model framework to predict times-to-open, for each recipient. Using that we compute appropriate send times. We experiment on a data set of emails sent to a million customers over five months. The sequence of emails received by a person from a sender is a result of interactions with past emails from the sender, and hence contain useful signal that inform our model. This sequential dependence affords our proposed RNN-Survival (RNN-S) approach to outperform survival analysis approaches in predicting times-to-open. We show that best times to send emails can be computed accurately from predicted times-to-open. This approach allows a firm to tune send times of emails, which is in its control, to favorably influence open rates and engagement.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                },
                {
                    "authorId": "49856495",
                    "name": "Moumita Sinha"
                },
                {
                    "authorId": "144652422",
                    "name": "Atanu R. Sinha"
                },
                {
                    "authorId": "144529280",
                    "name": "S. Garg"
                },
                {
                    "authorId": "2066001946",
                    "name": "Neha Banerjee"
                }
            ]
        },
        {
            "paperId": "b86adffb784f22e8901c7e1de938d7ded8e8dc90",
            "title": "Fair Predictors under Distribution Shift",
            "abstract": "Recent work on fair machine learning adds to a growing set of algorithmic safeguards required for deployment in high societal impact areas. A fundamental concern with model deployment is to guarantee stable performance under changes in data distribution. Extensive work in domain adaptation addresses this concern, albeit with the notion of stability limited to that of predictive performance. We provide conditions under which a stable model both in terms of prediction and fairness performance can be trained. Building on the problem setup of causal domain adaptation, we select a subset of features for training predictors with fairness constraints such that risk with respect to an unseen target data distribution is minimized. Advantages of the approach are demonstrated on synthetic datasets and on the task of diagnosing acute kidney injury in a real-world dataset under an instance of measurement policy shift and selection bias.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                },
                {
                    "authorId": "2109455509",
                    "name": "Rina Singh"
                },
                {
                    "authorId": "51429443",
                    "name": "Vishwali Mhasawade"
                },
                {
                    "authorId": "3144230",
                    "name": "R. Chunara"
                }
            ]
        }
    ]
}