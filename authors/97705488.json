{
    "authorId": "97705488",
    "papers": [
        {
            "paperId": "087e1ceb9e62b2e3732247caa84beb223804178e",
            "title": "Rating Distribution Calibration for Selection Bias Mitigation in Recommendations",
            "abstract": "Real-world recommendation datasets have been shown to be subject to selection bias, which can challenge recommendation models to learn real preferences of users, so as to make accurate recommendations. Existing approaches to mitigate selection bias, such as data imputation and inverse propensity score, are sensitive to the quality of the additional imputation or propensity estimation models. To break these limitations, in this work, we propose a novel self-supervised learning (SSL) framework, i.e., Rating Distribution Calibration (RDC), to tackle selection bias without introducing additional models. In addition to the original training objective, we introduce a rating distribution calibration loss. It aims to correct the predicted rating distribution of biased users by taking advantage of that of their similar unbiased users. We empirically evaluate RDC on two real-world datasets and one synthetic dataset. The experimental results show that RDC outperforms the original model as well as the state-of-the-art debiasing approaches by a significant margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66442354",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "97705488",
                    "name": "Da Tang"
                },
                {
                    "authorId": "2109869278",
                    "name": "Ji Yang"
                },
                {
                    "authorId": "2116710405",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "73416451",
                    "name": "Youlong Cheng"
                }
            ]
        },
        {
            "paperId": "3ca2ba62e130c104b2c01ea496af6f690474a1db",
            "title": "CowClip: Reducing CTR Prediction Model Training Time from 12 hours to 10 minutes on 1 GPU",
            "abstract": "The click-through rate (CTR) prediction task is to predict whether a user will click on the recommended item. As mind-boggling amounts of data are produced online daily, accelerating CTR prediction model training is critical to ensuring an up-to-date model and reducing the training cost. One approach to increase the training speed is to apply large batch training. However, as shown in computer vision and natural language processing tasks, training with a large batch easily suffers from the loss of accuracy. Our experiments show that previous scaling rules fail in the training of CTR prediction neural networks. To tackle this problem, we first theoretically show that different frequencies of ids make it challenging to scale hyperparameters when scaling the batch size. To stabilize the training process in a large batch size setting, we develop the adaptive Column-wise Clipping (CowClip). It enables an easy and effective scaling rule for the embeddings, which keeps the learning rate unchanged and scales the L2 loss. We conduct extensive experiments with four CTR prediction networks on two real-world datasets and successfully scaled 128 times the original batch size without accuracy loss. In particular, for CTR prediction model DeepFM training on the Criteo dataset, our optimization framework enlarges the batch size from 1K to 128K with over 0.1% AUC improvement and reduces training time from 12 hours to 10 minutes on a single V100 GPU. Our code locates at github.com/bytedance/LargeBatchCTR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109654065",
                    "name": "Zangwei Zheng"
                },
                {
                    "authorId": "2091437540",
                    "name": "Peng Xu"
                },
                {
                    "authorId": "2067661999",
                    "name": "Xuan Zou"
                },
                {
                    "authorId": "97705488",
                    "name": "Da Tang"
                },
                {
                    "authorId": "2157133491",
                    "name": "Zhen Li"
                },
                {
                    "authorId": "2963482",
                    "name": "Chenguang Xi"
                },
                {
                    "authorId": "2153093673",
                    "name": "Peng Wu"
                },
                {
                    "authorId": "2162360130",
                    "name": "Leqi Zou"
                },
                {
                    "authorId": "2162738919",
                    "name": "Yijie Zhu"
                },
                {
                    "authorId": "2108003749",
                    "name": "Ming-yue Chen"
                },
                {
                    "authorId": "2117436426",
                    "name": "Xiangzhuo Ding"
                },
                {
                    "authorId": "2144332771",
                    "name": "Fuzhao Xue"
                },
                {
                    "authorId": "2162400714",
                    "name": "Ziheng Qing"
                },
                {
                    "authorId": "73416451",
                    "name": "Youlong Cheng"
                },
                {
                    "authorId": "2054451943",
                    "name": "Yang You"
                }
            ]
        },
        {
            "paperId": "6dc8b0018bbc183c8d15747e5f6cc3fb14678a9f",
            "title": "Toward Annotator Group Bias in Crowdsourcing",
            "abstract": "Crowdsourcing has emerged as a popular approach for collecting annotated data to train supervised machine learning models. However, annotator bias can lead to defective annotations. Though there are a few works investigating individual annotator bias, the group effects in annotators are largely overlooked. In this work, we reveal that annotators within the same demographic group tend to show consistent group bias in annotation tasks and thus we conduct an initial study on annotator group bias. We first empirically verify the existence of annotator group bias in various real-world crowdsourcing datasets. Then, we develop a novel probabilistic graphical framework GroupAnno to capture annotator group bias with an extended Expectation Maximization (EM) algorithm. We conduct experiments on both synthetic and real-world datasets. Experimental results demonstrate the effectiveness of our model in modeling annotator group bias in label aggregation and model learning over competitive baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143856455",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "80601470",
                    "name": "J. Thekinen"
                },
                {
                    "authorId": "2315117721",
                    "name": "Sinem Mollaoglu"
                },
                {
                    "authorId": "97705488",
                    "name": "Da Tang"
                },
                {
                    "authorId": "2109869278",
                    "name": "Ji Yang"
                },
                {
                    "authorId": "73416451",
                    "name": "Youlong Cheng"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "af6b00d172726429c0d5b41eba7bd524b5fc8c68",
            "title": "Active Multitask Learning with Committees",
            "abstract": "The cost of annotating training data has traditionally been a bottleneck for supervised learning approaches. The problem is further exacerbated when supervised learning is applied to a number of correlated tasks simultaneously since the amount of labels required scales with the number of tasks. To mitigate this concern, we propose an active multitask learning algorithm that achieves knowledge transfer between tasks. The approach forms a so-called committee for each task that jointly makes decisions and directly shares data across similar tasks. Our approach reduces the number of queries needed during training while maintaining high accuracy on test data. Empirical results on benchmark datasets show significant improvements on both accuracy and number of query requests.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212230267",
                    "name": "Jingxian Xu"
                },
                {
                    "authorId": "97705488",
                    "name": "Da Tang"
                },
                {
                    "authorId": "1768120",
                    "name": "Tony Jebara"
                }
            ]
        },
        {
            "paperId": "1292e9e9fcba612bcdf32da5bc0e9ea400a61ea0",
            "title": "Learning Correlated Latent Representations with Adaptive Priors",
            "abstract": "Variational Auto-Encoders (VAEs) have been widely applied for learning compact, low-dimensional latent representations of high-dimensional data. When the correlation structure among data points is available, previous work proposed Correlated Variational Auto-Encoders (CVAEs), which employ a structured mixture model as prior and a structured variational posterior for each mixture component to enforce that the learned latent representations follow the same correlation structure. However, as we demonstrate in this work, such a choice cannot guarantee that CVAEs capture all the correlations. Furthermore, it prevents us from obtaining a tractable joint and marginal variational distribution. To address these issues, we propose Adaptive Correlated Variational Auto-Encoders (ACVAEs), which apply an adaptive prior distribution that can be adjusted during training and can learn a tractable joint variational distribution. Its tractable form also enables further refinement with belief propagation. Experimental results on link prediction and hierarchical clustering show that ACVAEs significantly outperform CVAEs among other benchmarks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "97705488",
                    "name": "Da Tang"
                },
                {
                    "authorId": "1702877",
                    "name": "Dawen Liang"
                },
                {
                    "authorId": "2273098",
                    "name": "Nicholas Ruozzi"
                },
                {
                    "authorId": "1768120",
                    "name": "Tony Jebara"
                }
            ]
        },
        {
            "paperId": "2daf769f1bf283ebbd7286146e3021bb57e096bd",
            "title": "The Variational Predictive Natural Gradient",
            "abstract": "Variational inference transforms posterior inference into parametric optimization thereby enabling the use of latent variable models where otherwise impractical. However, variational inference can be finicky when different variational parameters control variables that are strongly correlated under the model. Traditional natural gradients based on the variational approximation fail to correct for correlations when the approximation is not the true posterior. To address this, we construct a new natural gradient called the Variational Predictive Natural Gradient (VPNG). Unlike traditional natural gradients for variational inference, this natural gradient accounts for the relationship between model parameters and variational parameters. We demonstrate the insight with a simple example as well as the empirical value on a classification task, a deep generative model of images, and probabilistic matrix factorization for recommendation.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "97705488",
                    "name": "Da Tang"
                },
                {
                    "authorId": "2615814",
                    "name": "R. Ranganath"
                }
            ]
        },
        {
            "paperId": "813995ec740921c88360fccee4a49befbb1e51c3",
            "title": "Correlated Variational Auto-Encoders",
            "abstract": "Variational Auto-Encoders (VAEs) are capable of learning latent representations for high dimensional data. However, due to the i.i.d. assumption, VAEs only optimize the singleton variational distributions and fail to account for the correlations between data points, which might be crucial for learning latent representations from dataset where a priori we know correlations exist. We propose Correlated Variational Auto-Encoders (CVAEs) that can take the correlation structure into consideration when learning latent representations with VAEs. CVAEs apply a prior based on the correlation structure. To address the intractability introduced by the correlated prior, we develop an approximation by average of a set of tractable lower bounds over all maximal acyclic subgraphs of the undirected correlation graph. Experimental results on matching and link prediction on public benchmark rating datasets and spectral clustering on a synthetic dataset show the effectiveness of the proposed method over baseline algorithms.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "97705488",
                    "name": "Da Tang"
                },
                {
                    "authorId": "1702877",
                    "name": "Dawen Liang"
                },
                {
                    "authorId": "1768120",
                    "name": "Tony Jebara"
                },
                {
                    "authorId": "2273098",
                    "name": "Nicholas Ruozzi"
                }
            ]
        },
        {
            "paperId": "6aae1bc6c8e38c9a1d24a2b48bfe066f3591e1bc",
            "title": "Subgoal Discovery for Hierarchical Dialogue Policy Learning",
            "abstract": "Developing agents to engage in complex goal-oriented dialogues is challenging partly because the main learning signals are very sparse in long conversations. In this paper, we propose a divide-and-conquer approach that discovers and exploits the hidden structure of the task to enable efficient policy learning. First, given successful example dialogues, we propose the Subgoal Discovery Network (SDN) to divide a complex goal-oriented task into a set of simpler subgoals in an unsupervised fashion. We then use these subgoals to learn a multi-level policy by hierarchical reinforcement learning. We demonstrate our method by building a dialogue agent for the composite task of travel planning. Experiments with simulated and real users show that our approach performs competitively against a state-of-the-art method that requires human-defined subgoals. Moreover, we show that the learned subgoals are often human comprehensible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "97705488",
                    "name": "Da Tang"
                },
                {
                    "authorId": "47058148",
                    "name": "Xiujun Li"
                },
                {
                    "authorId": "1800422",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "2146308808",
                    "name": "Chong Wang"
                },
                {
                    "authorId": "47681372",
                    "name": "Lihong Li"
                },
                {
                    "authorId": "1768120",
                    "name": "Tony Jebara"
                }
            ]
        },
        {
            "paperId": "a94d2da1208ffc994b744628a75d93536fe2dcad",
            "title": "Item Recommendation with Variational Autoencoders and Heterogeneous Priors",
            "abstract": "In recent years, Variational Autoencoders (VAEs) have been shown to be highly effective in both standard collaborative filtering applications and extensions such as incorporation of implicit feedback. We extend VAEs to collaborative filtering with side information, for instance when ratings are combined with explicit text feedback from the user. Instead of using a user-agnostic standard Gaussian prior, we incorporate user-dependent priors in the latent VAE space to encode users' preferences as functions of the review text. Taking into account both the rating and the text information to represent users in this multimodal latent space is promising to improve recommendation quality. Our proposed model is shown to outperform the existing VAE models for collaborative filtering (up to 29.41% relative improvement in ranking metric) along with other baselines that incorporate both user ratings and text for item recommendation.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "8458211",
                    "name": "Giannis Karamanolakis"
                },
                {
                    "authorId": "51136372",
                    "name": "Kevin Raji Cherian"
                },
                {
                    "authorId": "49812614",
                    "name": "A. Narayan"
                },
                {
                    "authorId": "2118573410",
                    "name": "Jie Yuan"
                },
                {
                    "authorId": "97705488",
                    "name": "Da Tang"
                },
                {
                    "authorId": "1768120",
                    "name": "Tony Jebara"
                }
            ]
        },
        {
            "paperId": "933acbb7804147eb0027fc01223c0994369aae7d",
            "title": "Initialization and Coordinate Optimization for Multi-way Matching",
            "abstract": "We consider the problem of consistently matching multiple sets of elements to each other, which is a common task in fields such as computer vision. To solve the underlying NP-hard objective, existing methods often relax or approximate it, but end up with unsatisfying empirical performance due to a misaligned objective. We propose a coordinate update algorithm that directly optimizes the target objective. By using pairwise alignment information to build an undirected graph and initializing the permutation matrices along the edges of its Maximum Spanning Tree, our algorithm successfully avoids bad local optima. Theoretically, with high probability our algorithm guarantees an optimal solution under reasonable noise assumptions. Empirically, our algorithm consistently and significantly outperforms existing methods on several benchmark tasks on real datasets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "97705488",
                    "name": "Da Tang"
                },
                {
                    "authorId": "1768120",
                    "name": "Tony Jebara"
                }
            ]
        }
    ]
}