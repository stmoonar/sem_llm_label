{
    "authorId": "1993657377",
    "papers": [
        {
            "paperId": "35c07f14c5edd6dc87d0ad9ab9849bb6ab6055aa",
            "title": "Unsupervised Graph Neural Architecture Search with Disentangled Self-Supervision",
            "abstract": "The existing graph neural architecture search (GNAS) methods heavily rely on supervised labels during the search process, failing to handle ubiquitous scenarios where supervisions are not available. In this paper, we study the problem of unsupervised graph neural architecture search, which remains unexplored in the literature. The key problem is to discover the latent graph factors that drive the formation of graph data as well as the underlying relations between the factors and the optimal neural architectures. Handling this problem is challenging given that the latent graph factors together with architectures are highly entangled due to the nature of the graph and the complexity of the neural architecture search process. To address the challenge, we propose a novel Disentangled Self-supervised Graph Neural Architecture Search (DSGAS) model, which is able to discover the optimal architectures capturing various latent graph factors in a self-supervised fashion based on unlabeled graph data. Specifically, we first design a disentangled graph super-network capable of incorporating multiple architectures with factor-wise disentanglement, which are optimized simultaneously. Then, we estimate the performance of architectures under different factors by our proposed self-supervised training with joint architecture-graph disentanglement. Finally, we propose a contrastive search with architecture augmentations to discover architectures with factor-specific expertise. Extensive experiments on 11 real-world datasets demonstrate that the proposed model is able to achieve state-of-the-art performance against several baseline methods in an unsupervised manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118690469",
                    "name": "Zeyang Zhang"
                },
                {
                    "authorId": "2153687490",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2116460208",
                    "name": "Ziwei Zhang"
                },
                {
                    "authorId": "1993657377",
                    "name": "Guangyao Shen"
                },
                {
                    "authorId": "2279330465",
                    "name": "Shiqi Shen"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "af74aa750f9ffa63a32ff689dded81f6e8b1f32d",
            "title": "DeepLogic: Joint Learning of Neural Perception and Logical Reasoning",
            "abstract": "Neural-symbolic learning, aiming to combine the perceiving power of neural perception and the reasoning power of symbolic logic together, has drawn increasing research attention. However, existing works simply cascade the two components together and optimize them isolatedly, failing to utilize the mutual enhancing information between them. To address this problem, we propose DeepLogic, a framework with joint learning of neural perception and logical reasoning, such that these two components are jointly optimized through mutual supervision signals. In particular, the proposed DeepLogic framework contains a deep-logic module that is capable of representing complex first-order-logic formulas in a tree structure with basic logic operators. We then theoretically quantify the mutual supervision signals and propose the deep&logic optimization algorithm for joint optimization. We further prove the convergence of DeepLogic and conduct extensive experiments on model performance, convergence, and generalization, as well as its extension to the continuous domain. The experimental results show that through jointly learning both perceptual ability and logic formulas in a weakly supervised manner, our proposed DeepLogic framework can significantly outperform DNN-based baselines by a great margin and beat other strong baselines without out-of-box tools.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50997773",
                    "name": "Xuguang Duan"
                },
                {
                    "authorId": "153316152",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "144259957",
                    "name": "P. Zhao"
                },
                {
                    "authorId": "1993657377",
                    "name": "Guangyao Shen"
                },
                {
                    "authorId": "2156154955",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "320d212dd25dfdec58ac6d9f60198eb893d3744e",
            "title": "Relation Attention for Temporal Action Localization",
            "abstract": "Temporal action localization aims to accurately localize and recognize all possible action instances from an untrimmed video automatically. Most existing methods perform this task by first generating a set of proposals and then recognizing each independently. However, due to the complex structures and large content variations in action instances, recognizing them individually can be difficult. Fortunately, some proposals often share information regarding one specific action. Such information, which is ignored in existing methods, can be used to boost recognition performance. In this paper, we propose a novel mechanism, called relation attention, to exploit informative relations among proposals based on their appearance or optical flow features. Specifically, we propose a relation attention module to enhance representation power by capturing useful information from other proposals. This module does not change the dimensions of the original input and output and does not rely on any specific proposal generation methods or feature extraction backbone networks. Experimental results show that the proposed relation attention mechanism improves performance significantly on both Thumos14 and ActivityNet1.3 datasets compared to existing architectures. For example, relying on Structured Segment Networks (SSN), the proposed relation attention module helps to increase the mAP from 41.4 to 43.7 on the Thumos14 dataset and outperforms the state-of-the-art results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2158502526",
                    "name": "Peihao Chen"
                },
                {
                    "authorId": "144158271",
                    "name": "Chuang Gan"
                },
                {
                    "authorId": "1993657377",
                    "name": "Guangyao Shen"
                },
                {
                    "authorId": "123175679",
                    "name": "Wenbing Huang"
                },
                {
                    "authorId": "147992804",
                    "name": "Runhao Zeng"
                },
                {
                    "authorId": "2823637",
                    "name": "Mingkui Tan"
                }
            ]
        },
        {
            "paperId": "b141bea44da3e05a7e00241b37f0e28124157039",
            "title": "MEmoR: A Dataset for Multimodal Emotion Reasoning in Videos",
            "abstract": "Humans can perceive subtle emotions from various cues and contexts, even without hearing or seeing others. However, existing video datasets mainly focus on recognizing the emotions of the speakers from complete modalities. In this work, we present the task of multimodal emotion reasoning in videos. Beyond directly recognizing emotions from multimodal signals of target persons, this task requires a machine capable of reasoning about human emotions from the contexts and surrounding world. To facilitate the study towards this task, we introduce a new dataset, MEmoR, that provides fine-grained emotion annotations for both speakers and non-speakers. The videos in MEmoR are collected from TV shows closely in real-life scenarios. In these videos, while speakers may be non-visually described, non-speakers always deliver no audio-textual signals and are often visually inconspicuous. This modality-missing characteristic makes MEmoR a more practical yet challenging testbed for multimodal emotion reasoning. In support of various reasoning behaviors, the proposed MEmoR dataset provides both short-term contexts and external knowledge. We further propose an attention-based reasoning approach to model the intra-personal emotion contexts, inter-personal emotion propagation, and the personalities of different individuals. Experimental results demonstrate that our proposed approach outperforms related baselines significantly. We isolate and analyze the validity of different reasoning modules across various emotions of speakers and non-speakers. Finally, we draw forth several future research directions for multimodal emotion reasoning with MEmoR, aiming to empower high Emotional Quotient (EQ) in modern artificial intelligence systems. The code and dataset released on https://github.com/sunlightsgy/MEmoR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1993657377",
                    "name": "Guangyao Shen"
                },
                {
                    "authorId": "2153687490",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "50997773",
                    "name": "Xuguang Duan"
                },
                {
                    "authorId": "1786871",
                    "name": "Hongzhi Li"
                },
                {
                    "authorId": "145583986",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "f642ddb69f712e2b517cb6c6d9a062506991d1ed",
            "title": "Commonsense Learning: An Indispensable Path towards Human-centric Multimedia",
            "abstract": "Learning commonsense knowledge and conducting commonsense reasoning are basic human ability to make presumptions about the type and essence of ordinary situation in daily life, which serve as very important goals in human-centric Artificial Intelligence (AI). With the increasing number of media types and quantities provided by various Internet services, commonsense learning and reasoning with no doubt are playing key roles in making progresses for human-centric multimedia analysis. Therefore, this paper first introduces the basic concept of commonsense knowledge and commonsense reasoning, then summarizes commonsense resources and benchmarks, gives an overview on recent commonsense learning and reasoning methods, and discusses several popular applications of commonsense knowledge in real-world scenarios. This work distinguishes itself from existing literature that merely pays attention to natural language processing in focusing more on multimedia which include both natural language processing and computer vision. Furthermore, we also present our insights and thinking on future research directions for commonsense.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107274867",
                    "name": "B. Huang"
                },
                {
                    "authorId": "1993657480",
                    "name": "Siao Tang"
                },
                {
                    "authorId": "1993657377",
                    "name": "Guangyao Shen"
                },
                {
                    "authorId": "1669997484",
                    "name": "Guohao Li"
                },
                {
                    "authorId": "2153687490",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "145583986",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "42733f865df60e735014097f7136cb13bfda0351",
            "title": "Facial Image-to-Video Translation by a Hidden Affine Transformation",
            "abstract": "There has been a prominent emergence of work on video prediction, aiming to extrapolate the future video frames from the past. Existing temporal-based methods are limited to certain numbers of frames. In this paper, we study video prediction from a single still image in the facial expression domain, a.k.a, facial image-to-video translation. Our main approach, dubbed AffineGAN, associates each facial image with an expression intensity and leverages an affine transformation in the latent space. AffineGAN allows users to control the number of frames to predict as well as the expression intensity for each of them. Unlike previous intensity-based methods, We derive an inverse formulation to the affine transformation, enabling automatic inference of the facial expression intensities from videos --- manual annotation is not only tedious but also ambiguous as people express in various ways and have different opinions about the intensity of a facial image. Both quantitative and qualitative results verify the superiority of AffineGAN over the state of the arts. Notably, in a Turing test with web faces, more than 50% of the facial expression videos generated by AffineGAN are considered real by the Amazon Mechanical Turk workers. This work could improve users' communication experience by enabling them to conveniently and creatively produce expression GIFs, which are popular art forms in online messaging and social networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1993657377",
                    "name": "Guangyao Shen"
                },
                {
                    "authorId": "123175679",
                    "name": "Wenbing Huang"
                },
                {
                    "authorId": "144158271",
                    "name": "Chuang Gan"
                },
                {
                    "authorId": "2823637",
                    "name": "Mingkui Tan"
                },
                {
                    "authorId": "1768190",
                    "name": "Junzhou Huang"
                },
                {
                    "authorId": "145583986",
                    "name": "Wenwu Zhu"
                },
                {
                    "authorId": "40206014",
                    "name": "Boqing Gong"
                }
            ]
        },
        {
            "paperId": "7d634e6b82db99e1d4eb41eb22452f1273636448",
            "title": "Cross-Domain Depression Detection via Harvesting Social Media",
            "abstract": "Depression detection is a significant issue for human well-being. In previous studies, online detection has proven effective in Twitter, enabling proactive care for depressed users. Owing to cultural differences, replicating the method to other social media platforms, such as Chinese Weibo, however, might lead to poor performance because of insufficient available labeled (self-reported depression) data for model training. In this paper, we study an interesting but challenging problem of enhancing detection in a certain target domain (e.g. Weibo) with ample Twitter data as the source domain. We first systematically analyze the depression-related feature patterns across domains and summarize two major detection challenges, namely isomerism and divergency. We further propose a cross-domain Deep Neural Network model with Feature Adaptive Transformation & Combination strategy (DNN-FATC) that transfers the relevant information across heterogeneous domains. Experiments demonstrate improved performance compared to existing heterogeneous transfer methods or training directly in the target domain (over 3.4% improvement in F1), indicating the potential of our model to enable depression detection via social media for more countries with different cultural settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057973213",
                    "name": "Tiancheng Shen"
                },
                {
                    "authorId": "144202060",
                    "name": "Jia Jia"
                },
                {
                    "authorId": "1993657377",
                    "name": "Guangyao Shen"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                },
                {
                    "authorId": "3371599",
                    "name": "Huanbo Luan"
                },
                {
                    "authorId": "46199760",
                    "name": "Jie Tang"
                },
                {
                    "authorId": "1726746",
                    "name": "T. Tiropanis"
                },
                {
                    "authorId": "144078686",
                    "name": "Tat-Seng Chua"
                },
                {
                    "authorId": "143786158",
                    "name": "W. Hall"
                }
            ]
        },
        {
            "paperId": "2e579ee93cbc89657d54905a6bc723b00eaa2b22",
            "title": "Detecting Stress Based on Social Interactions in Social Networks",
            "abstract": "Psychological stress is threatening people\u2019s health. It is non-trivial to detect stress timely for proactive care. With the popularity of social media, people are used to sharing their daily activities and interacting with friends on social media platforms, making it feasible to leverage online social network data for stress detection. In this paper, we find that users stress state is closely related to that of his/her friends in social media, and we employ a large-scale dataset from real-world social platforms to systematically study the correlation of users\u2019 stress states and social interactions. We first define a set of stress-related textual, visual, and social attributes from various aspects, and then propose a novel hybrid model - a factor graph model combined with Convolutional Neural Network to leverage tweet content and social interaction information for stress detection. Experimental results show that the proposed model can improve the detection performance by 6-9 percent in F1-score. By further analyzing the social interaction data, we also discover several intriguing phenomena, i.e., the number of social structures of sparse connections (i.e., with no delta connections) of stressed users is around 14 percent higher than that of non-stressed users, indicating that the social structure of stressed users\u2019 friends tend to be less connected and less complicated than that of non-stressed users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2917061",
                    "name": "Huijie Lin"
                },
                {
                    "authorId": "144202060",
                    "name": "Jia Jia"
                },
                {
                    "authorId": "40125294",
                    "name": "J. Qiu"
                },
                {
                    "authorId": "1739818",
                    "name": "Yongfeng Zhang"
                },
                {
                    "authorId": "1993657377",
                    "name": "Guangyao Shen"
                },
                {
                    "authorId": "33650938",
                    "name": "Lexing Xie"
                },
                {
                    "authorId": "46199760",
                    "name": "Jie Tang"
                },
                {
                    "authorId": "1491090122",
                    "name": "Ling Feng"
                },
                {
                    "authorId": "144078686",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "da748238473d9cfbd3a6bda16566136242d7e6ba",
            "title": "Depression Detection via Harvesting Social Media: A Multimodal Dictionary Learning Solution",
            "abstract": "Depression is a major contributor to the overall global burden of diseases. Traditionally, doctors diagnose depressed people face to face via referring to clinical depression criteria. However, more than 70% of the patients would not consult doctors at early stages of depression, which leads to further deterioration of their conditions. Meanwhile, people are increasingly relying on social media to disclose emotions and sharing their daily lives, thus social media have successfully been leveraged for helping detect physical and mental diseases. Inspired by these, our work aims to make timely depression detection via harvesting social media data. We construct well-labeled depression and non-depression dataset on Twitter, and extract six depression-related feature groups covering not only the clinical depression criteria, but also online behaviors on social media. With these feature groups, we propose a multimodal depressive dictionary learning model to detect the depressed users on Twitter. A series of experiments are conducted to validate this model, which outperforms (+3% to +10%) several baselines. Finally, we analyze a large-scale dataset on Twitter to reveal the underlying online behaviors between depressed and non-depressed users.",
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1993657377",
                    "name": "Guangyao Shen"
                },
                {
                    "authorId": "144202060",
                    "name": "Jia Jia"
                },
                {
                    "authorId": "143982887",
                    "name": "Liqiang Nie"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "22703278",
                    "name": "Cunjun Zhang"
                },
                {
                    "authorId": "2112912604",
                    "name": "Tianrui Hu"
                },
                {
                    "authorId": "144078686",
                    "name": "Tat-Seng Chua"
                },
                {
                    "authorId": "145583986",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "e42d055d59f6b5b0bf677975d21544aad26a5417",
            "title": "Learning to Appreciate the Aesthetic Effects of Clothing",
            "abstract": "\n \n How do people describe clothing? The words like \u201cformal\u201dor \"casual\" are usually used. However, recent works often focus on recognizing or extracting visual features (e.g., sleeve length, color distribution and clothing pattern) from clothing images accurately. How can we bridge the gap between the visual features and the aesthetic words? In this paper, we formulate this task to a novel three-level framework: visual features(VF) - image-scale space (ISS) - aesthetic words space(AWS). Leveraging the art-field image-scale space served as an intermediate layer, we first propose a Stacked Denoising Autoencoder Guided by CorrelativeLabels (SDAE-GCL) to map the visual features to the image-scale space; and then according to the semantic distances computed byWordNet::Similarity, we map the most often used aesthetic words in online clothing shops to the image-scale space too. Employing upper body menswear images downloaded from several global online clothing shops as experimental data, the results indicate that the proposed three-level framework can help to capture the subtle relationship between visual features and aesthetic words better compared to several baselines. To demonstrate that our three-level framework and its implementation methods are universally applicable, we finally present some interesting analyses on the fashion trend of menswear in the last 10 years.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144202060",
                    "name": "Jia Jia"
                },
                {
                    "authorId": "2110284474",
                    "name": "Jie Huang"
                },
                {
                    "authorId": "1993657377",
                    "name": "Guangyao Shen"
                },
                {
                    "authorId": "2055131148",
                    "name": "Tao He"
                },
                {
                    "authorId": "49293587",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "3371599",
                    "name": "Huanbo Luan"
                },
                {
                    "authorId": "2116548502",
                    "name": "Chao Yan"
                }
            ]
        }
    ]
}