{
    "authorId": "2112569591",
    "papers": [
        {
            "paperId": "6e3e807b79f30e8ac749a62a37aaafb91b2c0caf",
            "title": "Reset & Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning",
            "abstract": "We argue that the negative transfer problem occurring when the new task to learn arrives is an important problem that needs not be overlooked when developing effective Continual Reinforcement Learning (CRL) algorithms. Through comprehensive experimental validation, we demonstrate that such issue frequently exists in CRL and cannot be effectively addressed by several recent work on mitigating plasticity loss of RL agents. To that end, we develop Reset&Distill (R&D), a simple yet highly effective method, to overcome the negative transfer problem in CRL. R&D combines a strategy of resetting the agent's online actor and critic networks to learn a new task and an offline learning step for distilling the knowledge from the online actor and previous expert's action probabilities. We carried out extensive experiments on long sequence of Meta World tasks and show that our method consistently outperforms recent baselines, achieving significantly higher success rates across a range of tasks. Our findings highlight the importance of considering negative transfer in CRL and emphasize the need for robust strategies like R&D to mitigate its detrimental effects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112569591",
                    "name": "Hongjoon Ahn"
                },
                {
                    "authorId": "2290489417",
                    "name": "Jinu Hyeon"
                },
                {
                    "authorId": "2268981585",
                    "name": "Youngmin Oh"
                },
                {
                    "authorId": "2268974728",
                    "name": "Bosun Hwang"
                },
                {
                    "authorId": "2315133096",
                    "name": "Taesup Moon"
                }
            ]
        },
        {
            "paperId": "8525434adbf25984e55c78063c71bcb958d364e4",
            "title": "Listwise Reward Estimation for Offline Preference-based Reinforcement Learning",
            "abstract": "In Reinforcement Learning (RL), designing precise reward functions remains to be a challenge, particularly when aligning with human intent. Preference-based RL (PbRL) was introduced to address this problem by learning reward models from human feedback. However, existing PbRL methods have limitations as they often overlook the second-order preference that indicates the relative strength of preference. In this paper, we propose Listwise Reward Estimation (LiRE), a novel approach for offline PbRL that leverages second-order preference information by constructing a Ranked List of Trajectories (RLT), which can be efficiently built by using the same ternary feedback type as traditional methods. To validate the effectiveness of LiRE, we propose a new offline PbRL dataset that objectively reflects the effect of the estimated rewards. Our extensive experiments on the dataset demonstrate the superiority of LiRE, i.e., outperforming state-of-the-art baselines even with modest feedback budgets and enjoying robustness with respect to the number of feedbacks and feedback noise. Our code is available at https://github.com/chwoong/LiRE",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2295954954",
                    "name": "Heewoong Choi"
                },
                {
                    "authorId": "47165273",
                    "name": "Sangwon Jung"
                },
                {
                    "authorId": "2112569591",
                    "name": "Hongjoon Ahn"
                },
                {
                    "authorId": "2315133096",
                    "name": "Taesup Moon"
                }
            ]
        },
        {
            "paperId": "409c0a96af63ff0bfc13f4bf64ed5c3e78100f48",
            "title": "Descent Steps of a Relation-Aware Energy Produce Heterogeneous Graph Neural Networks",
            "abstract": "Heterogeneous graph neural networks (GNNs) achieve strong performance on node classification tasks in a semi-supervised learning setting. However, as in the simpler homogeneous GNN case, message-passing-based heterogeneous GNNs may struggle to balance between resisting the oversmoothing that may occur in deep models, and capturing long-range dependencies of graph structured data. Moreover, the complexity of this trade-off is compounded in the heterogeneous graph case due to the disparate heterophily relationships between nodes of different types. To address these issues, we propose a novel heterogeneous GNN architecture in which layers are derived from optimization steps that descend a novel relation-aware energy function. The corresponding minimizer is fully differentiable with respect to the energy function parameters, such that bilevel optimization can be applied to effectively learn a functional form whose minimum provides optimal node representations for subsequent classification tasks. In particular, this methodology allows us to model diverse heterophily relationships between different node types while avoiding oversmoothing effects. Experimental results on 8 heterogeneous graph benchmarks demonstrates that our proposed method can achieve competitive node classification accuracy",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112569591",
                    "name": "Hongjoon Ahn"
                },
                {
                    "authorId": "2108658833",
                    "name": "You\u2010Jun Yang"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                },
                {
                    "authorId": "4842965",
                    "name": "Taesup Moon"
                }
            ]
        },
        {
            "paperId": "ec7ea5ef4a7932c347533b796be28ae3c16a1209",
            "title": "GAN-Based Framework for Unified Estimation of Process-Induced Random Variation in FinFET",
            "abstract": "For higher density of transistors in Integrated Circuit (IC), various scaling technologies have been introduced. In the light of the physical limit in advancing single-gate transistor architecture, the structural transition from planar device architecture toward 3D device architecture (of which the representative one is Fin-shaped Field-Effect Transistor, or FinFET) manifests itself. However, during fabrication, the unexpected process-induced random variations of the transistor\u2019s electrical characteristics have become more extreme with aggressively scaling down the physical dimension of transistor as well as with evolving from 2D to 3D device structure. Consequently, accurate and rapid estimation of the random variations conditioned on the variation sources (e.g., LER, RDF, and WFV) is required. Recently, machine learning-based approaches were utilized to estimate the LER-induced variations, but they were highly dependent on modeling and evaluation assumptions (e.g., Gaussian or independence). To that end, firstly, we introduce a GAN-based framework for the estimation of process-induced random variations. Since GAN is free from distributional assumptions, this enables precise prediction and, more importantly, enables unified estimation, i.e., adaptable to various variation sources. Secondly, to achieve better generalization on unseen conditions, we additionally suggest a two-step learning strategy utilizing the latest Conditional GAN models. Thirdly, we introduce sample-based evaluation procedure which measures the difference between two sample sets from a probabilistic perspective. Finally, the evaluation results on LER and RDF/WFV datasets show that our GAN-based framework is computationally efficient and is able to generate synthetic samples similar to the TCAD simulated samples that contain random variations, both qualitatively and quantitatively. From such results, our GAN-based framework is expected to be successfully applied to real data, and consequently be able to reliably estimate the random variations of fabricated transistors with multiple orders of magnitude speed-up compared to the conventional TCAD simulation-based estimation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "122744007",
                    "name": "Taeeon Park"
                },
                {
                    "authorId": "2029945525",
                    "name": "Jihwan Kwak"
                },
                {
                    "authorId": "2112569591",
                    "name": "Hongjoon Ahn"
                },
                {
                    "authorId": "2108524350",
                    "name": "Jinwoong Lee"
                },
                {
                    "authorId": "25136090",
                    "name": "Jaehyuk Lim"
                },
                {
                    "authorId": "2112266529",
                    "name": "Sangho Yu"
                },
                {
                    "authorId": "31901014",
                    "name": "C. Shin"
                },
                {
                    "authorId": "4842965",
                    "name": "Taesup Moon"
                }
            ]
        },
        {
            "paperId": "5035d0e8703bbb436a7f2e557b7335112b43ad9a",
            "title": "A Simple Class Decision Balancing for Incremental Learning",
            "abstract": "Class incremental learning (CIL) problem, in which a learning agent continuously learns new classes from incrementally arriving training data batches, has gained much attention recently in AI and computer vision community due to both fundamental and practical perspectives of the problem. For mitigating the main difficulty of deep neural network(DNN)-based CIL, the catastrophic forgetting, recent work showed that a simple fine-tuning (FT) based schemes can outperform the earlier attempts of using knowledge distillation, particularly when a small-sized exemplar-memory for storing samples from the previously learned classes is allowed. The core limitation of the vanilla FT, however, is the severe classification score bias between the new and previously learned classes, and several state-of-the-art methods proposed to rectify the bias via additional post-processing of the scores. In this paper, we propose two simple modifications for the vanilla FT, separated softmax (SS) layer and ratio-preserving (RP) mini-batches for SGD updates. Our scheme, dubbed as SS-IL, is shown to give much more balanced class decisions, have much less biased scores, and outperform strong state-of-the-art baselines on several large-scale benchmark datasets, without any sophisticated post-processing of the scores. We also give several novel analyses our and baseline methods, confirming the effectiveness of our approach in CIL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112569591",
                    "name": "Hongjoon Ahn"
                },
                {
                    "authorId": "4842965",
                    "name": "Taesup Moon"
                }
            ]
        },
        {
            "paperId": "515a6784e6d335677d16632d1b4d76471deee8aa",
            "title": "Continual Learning with Node-Importance based Adaptive Group Sparse Regularization",
            "abstract": "We propose a novel regularization-based continual learning method, dubbed as Adaptive Group Sparsity based Continual Learning (AGS-CL), using two group sparsity-based penalties. Our method selectively employs the two penalties when learning each node based its the importance, which is adaptively updated after learning each new task. By utilizing the proximal gradient descent method for learning, the exact sparsity and freezing of the model is guaranteed, and thus, the learner can explicitly control the model capacity as the learning continues. Furthermore, as a critical detail, we re-initialize the weights associated with unimportant nodes after learning each task in order to prevent the negative transfer that causes the catastrophic forgetting and facilitate efficient learning of new tasks. Throughout the extensive experimental results, we show that our AGS-CL uses much less additional memory space for storing the regularization parameters, and it significantly outperforms several state-of-the-art baselines on representative continual learning benchmarks for both supervised and reinforcement learning tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "47165273",
                    "name": "Sangwon Jung"
                },
                {
                    "authorId": "2112569591",
                    "name": "Hongjoon Ahn"
                },
                {
                    "authorId": "34352481",
                    "name": "Sungmin Cha"
                },
                {
                    "authorId": "4842965",
                    "name": "Taesup Moon"
                }
            ]
        },
        {
            "paperId": "8786200b06fe7349b08ff2e2f373392a9d01f529",
            "title": "Adaptive Group Sparse Regularization for Continual Learning",
            "abstract": "We propose a novel regularization-based continual learning method, dubbed as Adaptive Group Sparsity based Continual Learning (AGS-CL), using two group sparsity-based penalties. Our method selectively employs the two penalties when learning each node based its the importance, which is adaptively updated after learning each new task. By utilizing the proximal gradient descent method for learning, the exact sparsity and freezing of the model is guaranteed, and thus, the learner can explicitly control the model capacity as the learning continues. Furthermore, as a critical detail, we re-initialize the weights associated with unimportant nodes after learning each task in order to prevent the negative transfer that causes the catastrophic forgetting and facilitate ef\ufb01cient learning of new tasks. Throughout the extensive experimental results, we show that our AGS-CL uses much less additional memory space for storing the regularization parameters, and it signi\ufb01cantly outperforms several state-of-the-art baselines on representative continual learning benchmarks for both supervised and reinforcement learning tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47165273",
                    "name": "Sangwon Jung"
                },
                {
                    "authorId": "2112569591",
                    "name": "Hongjoon Ahn"
                },
                {
                    "authorId": "34352481",
                    "name": "Sungmin Cha"
                },
                {
                    "authorId": "4842965",
                    "name": "Taesup Moon"
                }
            ]
        },
        {
            "paperId": "c2c2609e56aa150489ddeb8430dcc7feeaccdd85",
            "title": "SS-IL: Separated Softmax for Incremental Learning",
            "abstract": "We consider class incremental learning (CIL) problem, in which a learning agent continuously learns new classes from incrementally arriving training data batches and aims to predict well on all the classes learned so far. The main challenge of the problem is the catastrophic forgetting, and for the exemplar-memory based CIL methods, it is generally known that the forgetting is commonly caused by the classification score bias that is injected due to the data imbalance between the new classes and the old classes (in the exemplar-memory). While several methods have been proposed to correct such score bias by some additional post-processing, e.g., score re-scaling or balanced fine-tuning, no systematic analysis on the root cause of such bias has been done. To that end, we analyze that computing the softmax probabilities by combining the output scores for all old and new classes could be the main cause of the bias. Then, we propose a new method, dubbed as Separated Softmax for Incremental Learning (SS-IL), that consists of separated softmax (SS) output layer combined with task-wise knowledge distillation (TKD) to resolve such bias. Throughout our extensive experimental results on several large-scale CIL benchmark datasets, we show our SS-IL achieves strong state-of-the-art accuracy through attaining much more balanced prediction scores across old and new classes, without any additional post-processing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112569591",
                    "name": "Hongjoon Ahn"
                },
                {
                    "authorId": "2029945525",
                    "name": "Jihwan Kwak"
                },
                {
                    "authorId": "2157846136",
                    "name": "S. Lim"
                },
                {
                    "authorId": "2029945839",
                    "name": "Hyeonsu Bang"
                },
                {
                    "authorId": null,
                    "name": "Hyojun Kim"
                },
                {
                    "authorId": "4842965",
                    "name": "Taesup Moon"
                }
            ]
        },
        {
            "paperId": "7f101fe42dc3133a70fc68eb767140eac57349fb",
            "title": "Iterative Channel Estimation for Discrete Denoising under Channel Uncertainty",
            "abstract": "We propose a novel iterative channel estimation (ICE) algorithm that essentially removes the critical known noisy channel assumption for universal discrete denoising problem. Our algorithm is based on Neural DUDE (N-DUDE), a recently proposed neural network-based discrete denoiser, and it estimates the channel transition matrix as well as the neural network parameters in an alternating manner until convergence. While we do not make any probabilistic assumption on the underlying clean data, our ICE resembles Expectation-Maximization (EM) with variational approximation, and it takes advantage of the property of N-DUDE being locally robust around the true channel. With extensive experiments on several radically different types of data, we show that the ICE equipped N-DUDE (dubbed as ICE-N-DUDE) can perform \\emph{universally} well regardless of the uncertainties in both the channel and the clean source. Moreover, we show ICE-N-DUDE becomes extremely robust to its hyperparameters and significantly outperforms the strong baseline that can deal with the channel uncertainties for denoising, the widely used Baum-Welch (BW) algorithm for hidden Markov models (HMM).",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2112569591",
                    "name": "Hongjoon Ahn"
                },
                {
                    "authorId": "4842965",
                    "name": "Taesup Moon"
                }
            ]
        },
        {
            "paperId": "85639b961f9af904a9468a2063a053a6c0b55b42",
            "title": "Uncertainty-based Continual Learning with Adaptive Regularization",
            "abstract": "We introduce a new neural network-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), which builds on traditional Bayesian online learning framework with variational inference. We focus on two significant drawbacks of the recently proposed regularization-based methods: a) considerable additional memory cost for determining the per-weight regularization strengths and b) the absence of gracefully forgetting scheme, which can prevent performance degradation in learning new tasks. In this paper, we show UCL can solve these two problems by introducing a fresh interpretation on the Kullback-Leibler (KL) divergence term of the variational lower bound for Gaussian mean-field approximation. Based on the interpretation, we propose the notion of node-wise uncertainty, which drastically reduces the number of additional parameters for implementing per-weight regularization. Moreover, we devise two additional regularization terms that enforce stability by freezing important parameters for past tasks and allow plasticity by controlling the actively learning parameters for a new task. Through extensive experiments, we show UCL convincingly outperforms most of recent state-of-the-art baselines not only on popular supervised learning benchmarks, but also on challenging lifelong reinforcement learning tasks. The source code of our algorithm is available at this https URL.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2112569591",
                    "name": "Hongjoon Ahn"
                },
                {
                    "authorId": "2109499598",
                    "name": "Donggyu Lee"
                },
                {
                    "authorId": "34352481",
                    "name": "Sungmin Cha"
                },
                {
                    "authorId": "4842965",
                    "name": "Taesup Moon"
                }
            ]
        }
    ]
}