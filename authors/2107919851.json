{
    "authorId": "2107919851",
    "papers": [
        {
            "paperId": "85ed6806f86be81647e9707d563ec3092c21060a",
            "title": "Leveraging Personalized Sentiment Lexicons for Sentiment Analysis",
            "abstract": "We propose a novel personalized approach for the sentiment analysis task. The approach is based on the intuition that the same sentiment words can carry different sentiment weights for different users. For each user, we learn a language model over a sentiment lexicon to capture her writing style. We further correlate this user-specific language model with the user's historical ratings of reviews. Additionally, we discuss how two standard CNN and CNN+LSTM models can be improved by adding these user-based features. Our evaluation on the Yelp dataset shows that the proposed new personalized sentiment analysis features are effective.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40499153",
                    "name": "Dominic Seyler"
                },
                {
                    "authorId": "3363642",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "46851930",
                    "name": "Jinfeng Xiao"
                },
                {
                    "authorId": "2107919851",
                    "name": "Yiren Wang"
                },
                {
                    "authorId": "143869012",
                    "name": "Chengxiang Zhai"
                }
            ]
        },
        {
            "paperId": "8d2cb5d898a2299d96ca776f75908956ad6de03e",
            "title": "Multi-task Learning for Multilingual Neural Machine Translation",
            "abstract": "While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area. In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data. We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets. We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models. We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data. Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107919851",
                    "name": "Yiren Wang"
                },
                {
                    "authorId": "143869012",
                    "name": "Chengxiang Zhai"
                },
                {
                    "authorId": "3032929",
                    "name": "H. Awadalla"
                }
            ]
        },
        {
            "paperId": "e19945c1bb29356c6936ccc874160b49a220acaf",
            "title": "Transductive Ensemble Learning for Neural Machine Translation",
            "abstract": "Ensemble learning, which aggregates multiple diverse models for inference, is a common practice to improve the accuracy of machine learning tasks. However, it has been observed that the conventional ensemble methods only bring marginal improvement for neural machine translation (NMT) when individual models are strong or there are a large number of individual models. In this paper, we study how to effectively aggregate multiple NMT models under the transductive setting where the source sentences of the test set are known. We propose a simple yet effective approach named transductive ensemble learning (TEL), in which we use all individual models to translate the source test set into the target language space and then finetune a strong model on the translated synthetic corpus. We conduct extensive experiments on different settings (with/without monolingual data) and different language pairs (English\u2194{German, Finnish}). The results show that our approach boosts strong individual models with significant improvement and benefits a lot from more individual models. Specifically, we achieve the state-of-the-art performances on the WMT2016-2018 English\u2194German translations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107919851",
                    "name": "Yiren Wang"
                },
                {
                    "authorId": "47767791",
                    "name": "Lijun Wu"
                },
                {
                    "authorId": "2794096",
                    "name": "Yingce Xia"
                },
                {
                    "authorId": "143826491",
                    "name": "Tao Qin"
                },
                {
                    "authorId": "143869012",
                    "name": "Chengxiang Zhai"
                },
                {
                    "authorId": "2110264337",
                    "name": "Tie-Yan Liu"
                }
            ]
        },
        {
            "paperId": "03e7eafeb3b3939ade1c900db5f16d1dc367f86a",
            "title": "Neural Machine Translation with Soft Prototype",
            "abstract": "Neural machine translation models usually use the encoder-decoder framework and generate translation from left to right (or right to left) without fully utilizing the target-side global information. A few recent approaches seek to exploit the global information through two-pass decoding, yet have limitations in translation quality and model efficiency. In this work, we propose a new framework that introduces a soft prototype into the encoder-decoder architecture, which allows the decoder to have indirect access to both past and future information, such that each target word can be generated based on the better global understanding. We further provide an efficient and effective method to generate the prototype. Empirical studies on various neural machine translation tasks show that our approach brings significant improvement in generation quality over the baseline model, with little extra cost in storage and inference time, demonstrating the effectiveness of our proposed framework. Specially, we achieve state-of-the-art results on WMT2014, 2015 and 2017 English to German translation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107919851",
                    "name": "Yiren Wang"
                },
                {
                    "authorId": "2794096",
                    "name": "Yingce Xia"
                },
                {
                    "authorId": "143853336",
                    "name": "Fei Tian"
                },
                {
                    "authorId": "144516501",
                    "name": "Fei Gao"
                },
                {
                    "authorId": "143826491",
                    "name": "Tao Qin"
                },
                {
                    "authorId": "143869012",
                    "name": "Chengxiang Zhai"
                },
                {
                    "authorId": "2110264337",
                    "name": "Tie-Yan Liu"
                }
            ]
        },
        {
            "paperId": "2aef70dc36ce8c2aba1cc5e823e20a59db3f7326",
            "title": "Microsoft Research Asia\u2019s Systems for WMT19",
            "abstract": "We Microsoft Research Asia made submissions to 11 language directions in the WMT19 news translation tasks. We won the first place for 8 of the 11 directions and the second place for the other three. Our basic systems are built on Transformer, back translation and knowledge distillation. We integrate several of our rececent techniques to enhance the baseline systems: multi-agent dual learning (MADL), masked sequence-to-sequence pre-training (MASS), neural architecture optimization (NAO), and soft contextual data augmentation (SCA).",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2794096",
                    "name": "Yingce Xia"
                },
                {
                    "authorId": "48391466",
                    "name": "Xu Tan"
                },
                {
                    "authorId": "143853336",
                    "name": "Fei Tian"
                },
                {
                    "authorId": "144516501",
                    "name": "Fei Gao"
                },
                {
                    "authorId": "1391126980",
                    "name": "Di He"
                },
                {
                    "authorId": "2139094085",
                    "name": "Weicong Chen"
                },
                {
                    "authorId": "1471698647",
                    "name": "Yang Fan"
                },
                {
                    "authorId": "32816503",
                    "name": "Linyuan Gong"
                },
                {
                    "authorId": "148398655",
                    "name": "Yichong Leng"
                },
                {
                    "authorId": "3440939",
                    "name": "Renqian Luo"
                },
                {
                    "authorId": "2107919851",
                    "name": "Yiren Wang"
                },
                {
                    "authorId": "47767550",
                    "name": "Lijun Wu"
                },
                {
                    "authorId": "151068900",
                    "name": "Jinhua Zhu"
                },
                {
                    "authorId": "143826491",
                    "name": "Tao Qin"
                },
                {
                    "authorId": "2110264337",
                    "name": "Tie-Yan Liu"
                }
            ]
        },
        {
            "paperId": "8dd31a4cfff7d6b5847259f557c5757bf4fc16ba",
            "title": "Exploiting Monolingual Data at Scale for Neural Machine Translation",
            "abstract": "While target-side monolingual data has been proven to be very useful to improve neural machine translation (briefly, NMT) through back translation, source-side monolingual data is not well investigated. In this work, we study how to use both the source-side and target-side monolingual data for NMT, and propose an effective strategy leveraging both of them. First, we generate synthetic bitext by translating monolingual data from the two domains into the other domain using the models pretrained on genuine bitext. Next, a model is trained on a noised version of the concatenated synthetic bitext where each source sequence is randomly corrupted. Finally, the model is fine-tuned on the genuine bitext and a clean version of a subset of the synthetic bitext without adding any noise. Our approach achieves state-of-the-art results on WMT16, WMT17, WMT18 English\\leftrightarrowGerman translations and WMT19 German\\toFrench translations, which demonstrate the effectiveness of our method. We also conduct a comprehensive study on how each part in the pipeline works.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47767791",
                    "name": "Lijun Wu"
                },
                {
                    "authorId": "2107919851",
                    "name": "Yiren Wang"
                },
                {
                    "authorId": "2794096",
                    "name": "Yingce Xia"
                },
                {
                    "authorId": "143826491",
                    "name": "Tao Qin"
                },
                {
                    "authorId": "66117656",
                    "name": "J. Lai"
                },
                {
                    "authorId": "2110264337",
                    "name": "Tie-Yan Liu"
                }
            ]
        },
        {
            "paperId": "cd4df0a6076fc94225684acbf4941c80014eb30a",
            "title": "Improving N-gram Language Models with Pre-trained Deep Transformer",
            "abstract": "Although n-gram language models (LMs) have been outperformed by the state-of-the-art neural LMs, they are still widely used in speech recognition due to its high efficiency in inference. In this paper, we demonstrate that n-gram LM can be improved by neural LMs through a text generation based data augmentation method. In contrast to previous approaches, we employ a large-scale general domain pre-training followed by in-domain fine-tuning strategy to construct deep Transformer based neural LMs. Large amount of in-domain text data is generated with the well trained deep Transformer to construct new n-gram LMs, which are then interpolated with baseline n-gram systems. Empirical studies on different speech recognition tasks show that the proposed approach can effectively improve recognition accuracy. In particular, our proposed approach brings significant relative word error rate reduction up to 6.0% for domains with limited in-domain data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107919851",
                    "name": "Yiren Wang"
                },
                {
                    "authorId": "1829351",
                    "name": "Hongzhao Huang"
                },
                {
                    "authorId": "2116746698",
                    "name": "Zhe Liu"
                },
                {
                    "authorId": "2057013888",
                    "name": "Yutong Pang"
                },
                {
                    "authorId": "2108220460",
                    "name": "Yongqiang Wang"
                },
                {
                    "authorId": "143869012",
                    "name": "Chengxiang Zhai"
                },
                {
                    "authorId": "1682136",
                    "name": "Fuchun Peng"
                }
            ]
        },
        {
            "paperId": "e146050bfe5e00063e55d467125a86daa45d7e1e",
            "title": "Depth Growing for Neural Machine Translation",
            "abstract": "While very deep neural networks have shown effectiveness for computer vision and text classification applications, how to increase the network depth of the neural machine translation (NMT) models for better translation quality remains a challenging problem. Directly stacking more blocks to the NMT model results in no improvement and even drop in performance. In this work, we propose an effective two-stage approach with three specially designed components to construct deeper NMT models, which result in significant improvements over the strong Transformer baselines on WMT14 English\\toGerman and English\\toFrench translation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47767791",
                    "name": "Lijun Wu"
                },
                {
                    "authorId": "2107919851",
                    "name": "Yiren Wang"
                },
                {
                    "authorId": "2794096",
                    "name": "Yingce Xia"
                },
                {
                    "authorId": "143853336",
                    "name": "Fei Tian"
                },
                {
                    "authorId": "144516501",
                    "name": "Fei Gao"
                },
                {
                    "authorId": "143826491",
                    "name": "Tao Qin"
                },
                {
                    "authorId": "66117656",
                    "name": "J. Lai"
                },
                {
                    "authorId": "2110264337",
                    "name": "Tie-Yan Liu"
                }
            ]
        }
    ]
}