{
    "authorId": "144918359",
    "papers": [
        {
            "paperId": "6aced7d7303fd82f8c4218aea2f3387efdf1d3c1",
            "title": "Making AI Machines Work for Humans in FoW",
            "abstract": "The Future of Work (FoW) is witnessing an evolution where AI systems (broadly machines or businesses) are used to the benefit of humans. Work here refers to all forms of paid and unpaid labor in both physical and virtual workplaces and that is enabled by AI systems. This covers crowdsourcing platforms such as Amazon Mechanical Turk, online labor marketplaces such as TaskRabbit and Qapa, but also regular jobs in physical workplaces. Bringing humans back to the frontier of FoW will increase their trust in AI systems and shift their perception to use them as a source of self-improvement, ensure better work performance, and positively shape social and economic outcomes of a society and a nation. To enable that, physical and virtual workplaces will need to capture human traits, behavior, evolving needs, and provide jobs to all. Attitudes, values, opinions regarding the processes and policies will need to be assessed and considered in the design of FoW ecosystems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2034201368",
                    "name": "Senjuti Basu Roy"
                },
                {
                    "authorId": "2146072233",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "34573158",
                    "name": "Atsuyuki Morishima"
                },
                {
                    "authorId": "2034202207",
                    "name": "James Abello Monedero"
                },
                {
                    "authorId": "1807924",
                    "name": "P. Bourhis"
                },
                {
                    "authorId": "2038513",
                    "name": "F. Charoy"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                },
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "1403157540",
                    "name": "D. Gross-Amblard"
                },
                {
                    "authorId": "51493818",
                    "name": "Emilie Hoareau"
                },
                {
                    "authorId": "2808778",
                    "name": "M. Inoguchi"
                },
                {
                    "authorId": "2034202198",
                    "name": "Jared Kenworthy"
                },
                {
                    "authorId": "7251192",
                    "name": "I. Kitahara"
                },
                {
                    "authorId": "2124213925",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1573872877",
                    "name": "Yunyao Li"
                },
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "1802817",
                    "name": "Paolo Papotti"
                },
                {
                    "authorId": "2069605832",
                    "name": "Raghav Rao"
                },
                {
                    "authorId": "31938009",
                    "name": "Sudeepa Roy"
                },
                {
                    "authorId": "1734682",
                    "name": "P. Senellart"
                },
                {
                    "authorId": "2792621",
                    "name": "Keishi Tajima"
                },
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                },
                {
                    "authorId": "1825255212",
                    "name": "M. Tommasi"
                },
                {
                    "authorId": "49697753",
                    "name": "Kazutoshi Umemoto"
                },
                {
                    "authorId": "144918359",
                    "name": "A. Wiggins"
                },
                {
                    "authorId": "2034178749",
                    "name": "Koichiro Yoshida"
                }
            ]
        },
        {
            "paperId": "192e0d68938c504152a35ea7bc176ec062578c34",
            "title": "The Impact of Screen Size on Crowdsourced Image Classification",
            "abstract": "Screen size has potential to affect the results of crowdsourced image classification and impact data quality, even for simple tasks like determining whether an animal is present in an image or not. We evaluated the rates at which volunteers judged images to be blank for six Snapshot Safari citizen science projects on the Zooniverse platform, and found that in all but one project, people using mobile devices were more likely to say an image was blank than those using a desktop computer. Qualitative evaluation further demonstrated that screen brightness can also play a role in reliability for volunteers using mobile devices. These findings can be taken into consideration in the design and testing of apps and platforms for crowdsourced image classification to support reliability and data quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40700178",
                    "name": "V. Ahuja"
                },
                {
                    "authorId": "144918359",
                    "name": "A. Wiggins"
                },
                {
                    "authorId": "1404348493",
                    "name": "Shivani Mudhelli"
                }
            ]
        },
        {
            "paperId": "969d90e350471b23d48b569091a090f55b6db321",
            "title": "Crowds and Camera Traps: Genres in Online Citizen Science Projects",
            "abstract": "Despite the importance of instruction for effective task completion in crowdsourcing, particularly for scientific work, little attention has been given to the design of instructional materials in crowdsourcing and citizen science. Consequences of inattention to tutorial design are further magnified by the diversity of citizen science volunteers. We use digital genre theory to identify the norms of tutorial design for the most abundant citizen science project type on the Zooniverse platform, camera trap image classification, where a highly-standardized task structure makes it a strong candidate as a specific genre of citizen science. Comparative content analysis of 14 projects\u2019 features, tutorial design, and supporting materials identified a great deal of uniformity in some respects (indicating an emergent genre) but surprising variation in others. As further evidence of an emergent genre, the amount of mentoring the science team received and specific task features of the project appeared to impact tutorial design and supporting resources. Our findings suggest that genre theory provides a useful lens for understanding crowd science projects with otherwise disparate characteristics and identifying instances where the digital medium can be deployed more effectively for task instruction.",
            "fieldsOfStudy": [
                "Computer Science",
                "Sociology"
            ],
            "authors": [
                {
                    "authorId": "51894358",
                    "name": "Holly K. Rosser"
                },
                {
                    "authorId": "144918359",
                    "name": "A. Wiggins"
                }
            ]
        },
        {
            "paperId": "05d3b80bbb454416563ec3e10c727a9ed7034139",
            "title": "Exposing the Science in Citizen Science: Fitness to Purpose and Intentional Design.",
            "abstract": "Citizen science is a growing phenomenon. With millions of people involved and billions of in-kind dollars contributed annually, this broad extent, fine grain approach to data collection should be garnering enthusiastic support in the mainstream science and higher education communities. However, many academic researchers demonstrate distinct biases against the use of citizen science as a source of rigorous information. To engage the public in scientific research, and the research community in the practice of citizen science, a mutual understanding is needed of accepted quality standards in science, and the corresponding specifics of project design and implementation when working with a broad public base. We define a science-based typology focused on the degree to which projects deliver the type(s) and quality of data/work needed to produce valid scientific outcomes directly useful in science and natural resource management. Where project intent includes direct contribution to science and the public is actively involved either virtually or hands-on, we examine the measures of quality assurance (methods to increase data quality during the design and implementation phases of a project) and quality control (post hoc methods to increase the quality of scientific outcomes). We suggest that high quality science can be produced with massive, largely one-off, participation if data collection is simple and quality control includes algorithm voting, statistical pruning, and/or computational modeling. Small to mid-scale projects engaging participants in repeated, often complex, sampling can advance quality through expert-led training and well-designed materials, and through independent verification. Both approaches-simplification at scale and complexity with care-generate more robust science outcomes.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21679677",
                    "name": "J. Parrish"
                },
                {
                    "authorId": "41156233",
                    "name": "H. Burgess"
                },
                {
                    "authorId": "2271483",
                    "name": "J. Weltzin"
                },
                {
                    "authorId": "30049690",
                    "name": "L. Fortson"
                },
                {
                    "authorId": "144918359",
                    "name": "A. Wiggins"
                },
                {
                    "authorId": "47747416",
                    "name": "Brooke D. Simmons"
                }
            ]
        },
        {
            "paperId": "801fdcfe4dd8613d9d23b23ae83db3c3d073434c",
            "title": "Tutorial Designs and Task Types in Zooniverse",
            "abstract": "Prior CSCW research has paid little attention to training for crowdsourcing project participants, which can require more than simple instructions. We examined the design of tutorials on the Zooniverse citizen science platform and identified aspects of tutorial design that aligned with task types, including more use of images and rich media for certain tasks. These findings support developing new tools for online tutorial creation, such as standard templates based on task characteristics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51894358",
                    "name": "Holly K. Rosser"
                },
                {
                    "authorId": "144918359",
                    "name": "A. Wiggins"
                }
            ]
        },
        {
            "paperId": "a9089a639ede135ac5c1447e405f2243d5d9ee3f",
            "title": "Exploring visual representations to support data re\u2010use for interdisciplinary science",
            "abstract": "Data discovery and re\u2010use remain challenging, particularly for interdisciplinary research, but visual representations of data may have potential to better support these tasks than text\u2010only metadata records. Our multi\u2010stage exploratory study used think\u2010aloud and intercept interviews to evaluate how scientists across multiple disciplines interpret the content and evaluate the credibility of climate indicators designed for general audiences as a way to understand the potential of visual representations for supporting interdisciplinary research. Climate indicators provided a convenient alternate representation for describing a data set, featuring a visualization of the data along with vernacular textual description. Our contributions include (i) findings on the preferences of scientists from multiple disciplines for establishing the credibility and assessing the content of data for re\u2010use, (ii) the unexpected observation that a minimal set of core metadata was considered adequate by researchers examining data from outside of their specialties, and (iii) discussion of the implications for the design of data discovery interfaces and future research to support data discovery and re\u2010use by increasingly diverse scientific data consumers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144918359",
                    "name": "A. Wiggins"
                },
                {
                    "authorId": "35128558",
                    "name": "A. Young"
                },
                {
                    "authorId": "40441845",
                    "name": "M. Kenney"
                }
            ]
        },
        {
            "paperId": "0308dae85f3bccbb6046c9e288bf0dae9c8d112c",
            "title": "The Science of Citizen Science: Theories, Methodologies and Platforms",
            "abstract": "Citizen science is a form of collaboration that engages non-professionals as contributors to scientific research, typically through the processes of gathering, transforming or analyzing data. To date, research has documented examples of hugely successful citizen science projects, such as Zooniverse and eBird, but citizen science also includes hundreds of smaller citizen science and functionally similar digital humanities projects, operating from small-scale web platforms and in-person collaborative teams. Yet, it is unclear what the \"science\" of citizen science entails: What are the core research questions and methodologies for answering them? What theories and concepts have been associated with citizen science research to date? What are the technology needs for supporting successful research collaboration among diverse stakeholders and across distinctive types of citizen science projects? In this workshop, our goal is to (i) bring together researchers studying citizen science to form a coherent map summarizing the theories, methodologies and platforms that currently defines citizen science research, with a special focus on CHI and CSCW relevant topics (ii) brainstorm a list of fundamental open questions and ways to tackle them, and (iii) form a multidisciplinary community to build synergies for further collaboration.",
            "fieldsOfStudy": [
                "Sociology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2987829",
                    "name": "E. Law"
                },
                {
                    "authorId": "11151983",
                    "name": "Alex C. Williams"
                },
                {
                    "authorId": "144918359",
                    "name": "A. Wiggins"
                },
                {
                    "authorId": "48762323",
                    "name": "Jonathan Brier"
                },
                {
                    "authorId": "143903983",
                    "name": "J. Preece"
                },
                {
                    "authorId": "36267262",
                    "name": "Jennifer Shirk"
                },
                {
                    "authorId": "49801047",
                    "name": "Greg Newman"
                }
            ]
        },
        {
            "paperId": "f69c6e85ebcfd6a640ad84f3f876e09b665d0adf",
            "title": "Crowdsourcing as a Tool for Research: Implications of Uncertainty",
            "abstract": "Numerous crowdsourcing platforms are now available to support research as well as commercial goals. However, crowdsourcing is not yet widely adopted by researchers for generating, processing or analyzing research data. This study develops a deeper understanding of the circumstances under which crowdsourcing is a useful, feasible or desirable tool for research, as well as the factors that may influence researchers' decisions around adopting crowdsourcing technology. We conducted semi-structured interviews with 18 researchers in diverse disciplines, spanning the humanities and sciences, to illuminate how research norms and practitioners' dispositions were related to uncertainties around research processes, data, knowledge, delegation and quality. The paper concludes with a discussion of the design implications for future crowdsourcing systems to support research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2987829",
                    "name": "E. Law"
                },
                {
                    "authorId": "1770992",
                    "name": "Krzysztof Z Gajos"
                },
                {
                    "authorId": "144918359",
                    "name": "A. Wiggins"
                },
                {
                    "authorId": "30818318",
                    "name": "Mary L. Gray"
                },
                {
                    "authorId": "11151983",
                    "name": "Alex C. Williams"
                }
            ]
        },
        {
            "paperId": "0def3216dae658807bc29df1330500bbe0ac351b",
            "title": "Community-based Data Validation Practices in Citizen Science",
            "abstract": "Technology-supported citizen science has created huge volumes of data with increasing potential to facilitate scientific progress, however, verifying data quality is still a substantial hurdle due to the limitations of existing data quality mechanisms. In this study, we adopted a mixed methods approach to investigate community-based data validation practices and the characteristics of records of wildlife species observations that affected the outcomes of collaborative data quality management in an online community where people record what they see in the nature. The findings describe the processes that both relied upon and added to information provenance through information stewardship behaviors, which led to improved reliability and informativity. The likelihood of community-based validation interactions were predicted by several factors, including the types of organisms observed and whether the data were submitted from a mobile device. We conclude with implications for technology design, citizen science practices, and research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144918359",
                    "name": "A. Wiggins"
                },
                {
                    "authorId": "121033922",
                    "name": "Yurong He"
                }
            ]
        },
        {
            "paperId": "3098aa696809c2f64fd49d96e40e8bbff0503477",
            "title": "A Journey of Citizen Science Data in an Online Environment",
            "abstract": "Previous CSCW studies on citizen science have focused on supporting data collection, rather than data sharing among a wider audience. Here we describe the process by which citizen science data are collaboratively shared by individual and organizational social actors. Three sets of collabora-tive efforts are essential for enabling data to travel through the online environments: creating shareable data, assuring data quality, and setting up data sharing partnerships.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "121033922",
                    "name": "Yurong He"
                },
                {
                    "authorId": "143903983",
                    "name": "J. Preece"
                },
                {
                    "authorId": "144052407",
                    "name": "J. Hammock"
                },
                {
                    "authorId": "2110622038",
                    "name": "Michele C. Weber"
                },
                {
                    "authorId": "1401919739",
                    "name": "Seabird McKeon"
                },
                {
                    "authorId": "144918359",
                    "name": "A. Wiggins"
                }
            ]
        }
    ]
}