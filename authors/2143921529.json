{
    "authorId": "2143921529",
    "papers": [
        {
            "paperId": "037d5596abc836d06cc8a3fcaba51bd782fafc76",
            "title": "Understanding Heterophily for Graph Neural Networks",
            "abstract": "Graphs with heterophily have been regarded as challenging scenarios for Graph Neural Networks (GNNs), where nodes are connected with dissimilar neighbors through various patterns. In this paper, we present theoretical understandings of the impacts of different heterophily patterns for GNNs by incorporating the graph convolution (GC) operations into fully connected networks via the proposed Heterophilous Stochastic Block Models (HSBM), a general random graph model that can accommodate diverse heterophily patterns. Firstly, we show that by applying a GC operation, the separability gains are determined by two factors, i.e., the Euclidean distance of the neighborhood distributions and $\\sqrt{\\mathbb{E}\\left[\\operatorname{deg}\\right]}$, where $\\mathbb{E}\\left[\\operatorname{deg}\\right]$ is the averaged node degree. It reveals that the impact of heterophily on classification needs to be evaluated alongside the averaged node degree. Secondly, we show that the topological noise has a detrimental impact on separability, which is equivalent to degrading $\\mathbb{E}\\left[\\operatorname{deg}\\right]$. Finally, when applying multiple GC operations, we show that the separability gains are determined by the normalized distance of the $l$-powered neighborhood distributions. It indicates that the nodes still possess separability as $l$ goes to infinity in a wide range of regimes. Extensive experiments on both synthetic and real-world data verify the effectiveness of our theory.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2155107356",
                    "name": "Junfu Wang"
                },
                {
                    "authorId": "2255826390",
                    "name": "Yuanfang Guo"
                },
                {
                    "authorId": "2143921529",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "2162630201",
                    "name": "Yun-an Wang"
                }
            ]
        },
        {
            "paperId": "00971cfdf54c7ad204719262ded5c53f195c7bfa",
            "title": "Heterophily-Aware Graph Attention Network",
            "abstract": "Graph Neural Networks (GNNs) have shown remarkable success in graph representation learning. Unfortunately, current weight assignment schemes in standard GNNs, such as the calculation based on node degrees or pair-wise representations, can hardly be effective in processing the networks with heterophily, in which the connected nodes usually possess different labels or features. Existing heterophilic GNNs tend to ignore the modeling of heterophily of each edge, which is also a vital part in tackling the heterophily problem. In this paper, we firstly propose a heterophily-aware attention scheme and reveal the benefits of modeling the edge heterophily, i.e., if a GNN assigns different weights to edges according to different heterophilic types, it can learn effective local attention patterns, which enable nodes to acquire appropriate information from distinct neighbors. Then, we propose a novel Heterophily-Aware Graph Attention Network (HA-GAT) by fully exploring and utilizing the local distribution as the underlying heterophily, to handle the networks with different homophily ratios. To demonstrate the effectiveness of the proposed HA-GAT, we analyze the proposed heterophily-aware attention scheme and local distribution exploration, by seeking for an interpretation from their mechanism. Extensive results demonstrate that our HA-GAT achieves state-of-the-art performances on eight datasets with different homophily ratios in both the supervised and semi-supervised node classification tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155107356",
                    "name": "Junfu Wang"
                },
                {
                    "authorId": "2613860",
                    "name": "Yuanfang Guo"
                },
                {
                    "authorId": "2143921529",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "2162630201",
                    "name": "Yun-an Wang"
                }
            ]
        },
        {
            "paperId": "3eb3cb33d2c807b88c78c12d48041ffeba2ef096",
            "title": "Graph Neural Networks without Propagation",
            "abstract": "Due to the simplicity, intuition and explanation, most Graph Neural Networks (GNNs) are proposed by following the pipeline of message passing. Although they achieve superior performances in many tasks, propagation-based GNNs possess three essential drawbacks. Firstly, the propagation tends to produce smooth effect, which meets the inductive bias of homophily, and causes two serious issues: over-smoothing issue and performance drop on networks with heterophily. Secondly, the propagations to each node are irrelevant, which prevents GNNs from modeling high-order relation, and cause the GNNs fragile to the attributes noises. Thirdly, propagation-based GNNs may be fragile to topology noise, since they heavily relay on propagation over the topology. Therefore, the propagation, as the key component of most GNNs, may be the essence of some serious issues in GNNs. To get to the root of these issue, this paper attempts to replace the propagation with a novel local operation. Quantitative experimental analysis reveals: 1) the existence of low-rank characteristic in the node attributes from ego-networks and 2) the performance improvement by reducing its rank. Motivated by this finding, this paper propose the Low-Rank GNNs, whose key component is the low-rank attribute matrix approximation in ego-network. The graph topology is employed to construct the ego-networks instead of message propagation, which is sensitive to topology noises. The proposed Low-Rank GNNs posses some attractive characteristics, including robust to topology and attribute noises, parameter-free and parallelizable. Experimental evaluations demonstrate the superior performance, robustness to noises and universality of the proposed Low-Rank GNNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143921529",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "2215584803",
                    "name": "Qiuliang Zhang"
                },
                {
                    "authorId": "2215543502",
                    "name": "Runjie Shi"
                },
                {
                    "authorId": "2160390715",
                    "name": "Wenmiao Zhou"
                },
                {
                    "authorId": "2661834",
                    "name": "Bingxin Niu"
                },
                {
                    "authorId": "2109150586",
                    "name": "Chuan Wang"
                },
                {
                    "authorId": "2149214322",
                    "name": "Xiaochun Cao"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2188012894",
                    "name": "Zhen Wang"
                },
                {
                    "authorId": "2613860",
                    "name": "Yuanfang Guo"
                }
            ]
        },
        {
            "paperId": "6a8ebe4d640a1b3107c602bb9aa35ad0e63b8439",
            "title": "LSGNN: Towards General Graph Neural Network in Node Classification by Local Similarity",
            "abstract": "Heterophily has been considered as an issue that hurts the performance of Graph Neural Networks (GNNs). To address this issue, some existing work uses a graph-level weighted fusion of the information of multi-hop neighbors to include more nodes with homophily. However, the heterophily might differ among nodes, which requires to consider the local topology. Motivated by it, we propose to use the local similarity (LocalSim) to learn node-level weighted fusion, which can also serve as a plug-and-play module. For better fusion, we propose a novel and efficient Initial Residual Difference Connection (IRDC) to extract more informative multi-hop information. Moreover, we provide theoretical analysis on the effectiveness of LocalSim representing node homophily on synthetic graphs. Extensive evaluations over real benchmark datasets show that our proposed method, namely Local Similarity Graph Neural Network (LSGNN), can offer comparable or superior state-of-the-art performance on both homophilic and heterophilic graphs. Meanwhile, the plug-and-play model can significantly boost the performance of existing GNNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2183577021",
                    "name": "Yuhan Chen"
                },
                {
                    "authorId": "2265724299",
                    "name": "Yihong Luo"
                },
                {
                    "authorId": "2152930282",
                    "name": "Jing Tang"
                },
                {
                    "authorId": "2143921529",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "30499006",
                    "name": "Si-Huang Qiu"
                },
                {
                    "authorId": "2109150586",
                    "name": "Chuan Wang"
                },
                {
                    "authorId": "2149214322",
                    "name": "Xiaochun Cao"
                }
            ]
        },
        {
            "paperId": "e3e269034ebfc94da52d2133239114046c8c12ae",
            "title": "Deepfake Video Detection via Facial Action Dependencies Estimation",
            "abstract": "Deepfake video detection has drawn significant attention from researchers due to the security issues induced by deepfake videos. Unfortunately, most of the existing deepfake detection approaches have not competently modeled the natural structures and movements of human faces. In this paper, we formulate the deepfake video detection problem into a graph classification task, and propose a novel paradigm named Facial Action Dependencies Estimation (FADE) for deepfake video detection. We propose a Multi-Dependency Graph Module (MDGM) to capture abundant dependencies among facial action units, and extracts subtle clues in these dependencies. MDGM can be easily integrated into the existing frame-level detection schemes to provide significant performance gains. Extensive experiments demonstrate the superiority of our method against the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2222942094",
                    "name": "Lingfeng Tan"
                },
                {
                    "authorId": "2162630201",
                    "name": "Yun-an Wang"
                },
                {
                    "authorId": "2155107356",
                    "name": "Junfu Wang"
                },
                {
                    "authorId": "2143921529",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "2149156",
                    "name": "Xunxun Chen"
                },
                {
                    "authorId": "2613860",
                    "name": "Yuanfang Guo"
                }
            ]
        },
        {
            "paperId": "1163c29ae6a26b1cd8d85280740410e99001a959",
            "title": "Self-Supervised Graph Neural Networks via Diverse and Interactive Message Passing",
            "abstract": "By interpreting Graph Neural Networks (GNNs) as the message passing from the spatial perspective, their success is attributed to Laplacian smoothing. However, it also leads to serious over-smoothing issue by stacking many layers. Recently, many efforts have been paid to overcome this issue in semi-supervised learning. Unfortunately, it is more serious in unsupervised node representation learning task due to the lack of supervision information. Thus, most of the unsupervised or self-supervised GNNs often employ \\textit{one-layer GCN} as the encoder. Essentially, the over-smoothing issue is caused by the over-simplification of the existing message passing, which possesses two intrinsic limits: blind message and uniform passing. In this paper, a novel Diverse and Interactive Message Passing (DIMP) is proposed for self-supervised learning by overcoming these limits. Firstly, to prevent the message from blindness and make it interactive between two connected nodes, the message is determined by both the two connected nodes instead of the attributes of one node. Secondly, to prevent the passing from uniformness and make it diverse over different attribute channels, different propagation weights are assigned to different elements in the message. To this end, a natural implementation of the message in DIMP is the element-wise product of the representations of two connected nodes. From the perspective of numerical optimization, the proposed DIMP is equivalent to performing an overlapping community detection via expectation-maximization (EM). Both the objective function of the community detection and the convergence of EM algorithm guarantee that DMIP can prevent from over-smoothing issue. Extensive evaluations on node-level and graph-level tasks demonstrate the superiority of DIMP on improving performance and overcoming over-smoothing issue.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143921529",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "2145775206",
                    "name": "Cheng Chen"
                },
                {
                    "authorId": "1409858224",
                    "name": "Weixun Li"
                },
                {
                    "authorId": "2661834",
                    "name": "Bingxin Niu"
                },
                {
                    "authorId": "6413344",
                    "name": "Junhua Gu"
                },
                {
                    "authorId": "2109150586",
                    "name": "Chuan Wang"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2613860",
                    "name": "Yuanfang Guo"
                },
                {
                    "authorId": "1719250",
                    "name": "Xiaochun Cao"
                }
            ]
        },
        {
            "paperId": "370fb0f5dfb590fad83705e1afc2ba3570335542",
            "title": "Difference Residual Graph Neural Networks",
            "abstract": "Graph Neural Networks have been widely employed for multimodal fusion and embedding. To overcome over-smoothing issue, residual connections, which are designed for alleviating vanishing gradient problem in NNs, are adopted in Graph Neural Networks (GNNs) to incorporate local node information. However, these simple residual connections are ineffective on networks with heterophily, since the roles of both convolutional operations and residual connections in GNNs are significantly different from those in classic NNs. By considering the specific smoothing characteristic of graph convolutional operation, deep layers in GNNs are expected to focus on the data which can't be properly handled in shallow layers. To this end, a novel and universal Difference Residual Connections (DRC), which feed the difference of the output and input of previous layer as the input of the next layer, is proposed. Essentially, Difference Residual Connections is equivalent to inserting layers with opposite effect (e.g., sharpening) into the network to prevent the excessive effect (e.g., over-smoothing issue) induced by too many layers with the similar role (e.g., smoothing) in GNNs. From the perspective of optimization, DRC is the gradient descent method to minimize an objective function with both smoothing and sharpening terms. The analytic solution to this objective function is determined by both graph topology and node attributes, which theoretically proves that DRC can prevent over-smoothing issue. Extensive experiments demonstrate the superiority of DRC on real networks with both homophily and heterophily, and show that DRC can automatically determine the model depth and be adaptive to both shallow and deep models with two complementary components.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143921529",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "118103314",
                    "name": "Weihang Peng"
                },
                {
                    "authorId": "2160390715",
                    "name": "Wenmiao Zhou"
                },
                {
                    "authorId": "2661834",
                    "name": "Bingxin Niu"
                },
                {
                    "authorId": "6413344",
                    "name": "Junhua Gu"
                },
                {
                    "authorId": "2109150586",
                    "name": "Chuan Wang"
                },
                {
                    "authorId": "2613860",
                    "name": "Yuanfang Guo"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2149214322",
                    "name": "Xiaochun Cao"
                }
            ]
        },
        {
            "paperId": "381fdf550a7e9b36ba0cfe7e987ab75b87f261b4",
            "title": "Relation Embedding based Graph Neural Networks for Handling Heterogeneous Graph",
            "abstract": "Heterogeneous graph learning has drawn significant attentions in recent years, due to the success of graph neural networks (GNNs) and the broad applications of heterogeneous information networks. Various heterogeneous graph neural networks have been proposed to generalize GNNs for processing the heterogeneous graphs. Unfortunately, these approaches model the heterogeneity via various complicated modules. This paper aims to propose a simple yet efficient framework to make the homogeneous GNNs have adequate ability to handle heterogeneous graphs. Specifically, we propose Relation Embedding based Graph Neural Networks (RE-GNNs), which employ only one parameter per relation to embed the importance of edge type relations and self-loop connections. To optimize these relation embeddings and the other parameters simultaneously, a gradient scaling factor is proposed to constrain the embeddings to converge to suitable values. Besides, we theoretically demonstrate that our RE-GNNs have more expressive power than the meta-path based heterogeneous GNNs. Extensive experiments on the node classification tasks validate the effectiveness of our proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155107356",
                    "name": "Junfu Wang"
                },
                {
                    "authorId": "2613860",
                    "name": "Yuanfang Guo"
                },
                {
                    "authorId": "2143921529",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "2162630201",
                    "name": "Yun-an Wang"
                }
            ]
        },
        {
            "paperId": "3feeb5cfab3d9f9d48132b9a80cbfe33beb06085",
            "title": "Binary Graph Convolutional Network With Capacity Exploration",
            "abstract": "The current success of Graph Neural Networks (GNNs) usually relies on loading the entire attributed graph for processing, which may not be satisfied with limited memory resources, especially when the attributed graph is large. This paper pioneers to propose a Binary Graph Convolutional Network (Bi-GCN), which binarizes both the network parameters and input node attributes and exploits binary operations instead of floating-point matrix multiplications for network compression and acceleration. Meanwhile, we also propose a new gradient approximation based back-propagation method to properly train our Bi-GCN. According to the theoretical analysis, our Bi-GCN can reduce the memory consumption by an average of <inline-formula><tex-math notation=\"LaTeX\">$\\sim$</tex-math><alternatives><mml:math><mml:mo>\u223c</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3342224.gif\"/></alternatives></inline-formula>31x for both the network parameters and input data, and accelerate the inference speed by an average of <inline-formula><tex-math notation=\"LaTeX\">$\\sim$</tex-math><alternatives><mml:math><mml:mo>\u223c</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq2-3342224.gif\"/></alternatives></inline-formula>51x, on three citation networks, i.e., Cora, PubMed, and CiteSeer. Besides, we introduce a general approach to generalize our binarization method to other variants of GNNs, and achieve similar efficiencies. Although the proposed Bi-GCN and Bi-GNNs are simple yet efficient, these compressed networks may also possess a potential <italic>capacity problem</italic>, i.e., they may not have enough storage capacity to learn adequate representations for specific tasks. To tackle this <italic>capacity problem</italic>, an Entropy Cover Hypothesis is proposed to predict the lower bound of the width of Bi-GNN hidden layers. Extensive experiments have demonstrated that our Bi-GCN and Bi-GNNs can give comparable performances to the corresponding full-precision baselines on seven node classification datasets and verified the effectiveness of our Entropy Cover Hypothesis for solving the <italic>capacity problem</italic>.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2155107356",
                    "name": "Junfu Wang"
                },
                {
                    "authorId": "2613860",
                    "name": "Yuanfang Guo"
                },
                {
                    "authorId": "2143921529",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "2162630201",
                    "name": "Yun-an Wang"
                }
            ]
        },
        {
            "paperId": "93dd8137cc33cca8ca3982da6873ff2aa1ee1467",
            "title": "OPEN: Orthogonal Propagation with Ego-Network Modeling",
            "abstract": "To alleviate the unfavorable effect of noisy topology in Graph Neural networks (GNNs), some efforts perform the local topology re\ufb01nement through the pairwise propagation weight learning and the multi-channel extension. Unfortunately, most of them suffer a common and fatal drawback: irrelevant propagation to one node and in multi-channels. These two kinds of irrelevances make propagation weights in multi-channels free to be determined by the labeled data, and thus the GNNs are exposed to over\ufb01tting. To tackle this issue, a novel Orthogonal Propagation with Ego-Network modeling (OPEN) is proposed by modeling relevances between propagations. Speci\ufb01cally, the relevance between propagations to one node is modeled by whole ego-network modeling, while the relevance between propagations in multi-channels is modeled via diversity requirement. By interpreting the propagations to one node from the perspective of dimension reduction, propagation weights are inferred from principal components of the ego-network, which are orthogonal to each other. Theoretical analysis and experimental evaluations reveal four attractive characteristics of OPEN as modeling high-order relationships beyond pairwise one, preventing over\ufb01tting, robustness, and high ef\ufb01ciency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143921529",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "2216273694",
                    "name": "Lina Kang"
                },
                {
                    "authorId": "2112667898",
                    "name": "Qiuliang Zhang"
                },
                {
                    "authorId": "2004582449",
                    "name": "Mengzhe Li"
                },
                {
                    "authorId": "2661834",
                    "name": "Bingxin Niu"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2188012894",
                    "name": "Zhen Wang"
                },
                {
                    "authorId": "2109150586",
                    "name": "Chuan Wang"
                },
                {
                    "authorId": "2149214322",
                    "name": "Xiaochun Cao"
                },
                {
                    "authorId": "2613860",
                    "name": "Yuanfang Guo"
                }
            ]
        }
    ]
}