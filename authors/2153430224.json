{
    "authorId": "2153430224",
    "papers": [
        {
            "paperId": "13cc7cb462d3146cbc35ccb128f66609c788a96a",
            "title": "Multi-Task Recommendations with Reinforcement Learning",
            "abstract": "In recent years, Multi-task Learning (MTL) has yielded immense success in Recommender System (RS) applications [40]. However, current MTL-based recommendation models tend to disregard the session-wise patterns of user-item interactions because they are predominantly constructed based on item-wise datasets. Moreover, balancing multiple objectives has always been a challenge in this field, which is typically avoided via linear estimations in existing works. To address these issues, in this paper, we propose a Reinforcement Learning (RL) enhanced MTL framework, namely RMTL, to combine the losses of different recommendation tasks using dynamic weights. To be specific, the RMTL structure can address the two aforementioned issues by (i) constructing an MTL environment from session-wise interactions and (ii) training multi-task actor-critic network structure, which is compatible with most existing MTL-based recommendation models, and (iii) optimizing and fine-tuning the MTL loss function using the weights generated by critic networks. Experiments on two real-world public datasets demonstrate the effectiveness of RMTL with a higher AUC against state-of-the-art MTL-based recommendation models. Additionally, we evaluate and validate RMTL\u2019s compatibility and transferability across various MTL models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2204700561",
                    "name": "Ziru Liu"
                },
                {
                    "authorId": "2204719513",
                    "name": "Jiejie Tian"
                },
                {
                    "authorId": "144994208",
                    "name": "Qingpeng Cai"
                },
                {
                    "authorId": "2116711669",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2161309826",
                    "name": "Jingtong Gao"
                },
                {
                    "authorId": "50152132",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "2113596993",
                    "name": "Da Chen"
                },
                {
                    "authorId": "2204642544",
                    "name": "Tonghao He"
                },
                {
                    "authorId": "2153430224",
                    "name": "Dong Zheng"
                },
                {
                    "authorId": "2061280682",
                    "name": "Peng Jiang"
                },
                {
                    "authorId": "20029557",
                    "name": "Kun Gai"
                }
            ]
        },
        {
            "paperId": "2807de52310ca3750ccc8dd2636d5ddc6d14a187",
            "title": "KuaiSim: A Comprehensive Simulator for Recommender Systems",
            "abstract": "Reinforcement Learning (RL)-based recommender systems (RSs) have garnered considerable attention due to their ability to learn optimal recommendation policies and maximize long-term user rewards. However, deploying RL models directly in online environments and generating authentic data through A/B tests can pose challenges and require substantial resources. Simulators offer an alternative approach by providing training and evaluation environments for RS models, reducing reliance on real-world data. Existing simulators have shown promising results but also have limitations such as simplified user feedback, lacking consistency with real-world data, the challenge of simulator evaluation, and difficulties in migration and expansion across RSs. To address these challenges, we propose KuaiSim, a comprehensive user environment that provides user feedback with multi-behavior and cross-session responses. The resulting simulator can support three levels of recommendation problems: the request level list-wise recommendation task, the whole-session level sequential recommendation task, and the cross-session level retention optimization task. For each task, KuaiSim also provides evaluation protocols and baseline recommendation algorithms that further serve as benchmarks for future research. We also restructure existing competitive simulators on the KuaiRand Dataset and compare them against KuaiSim to future assess their performance and behavioral differences. Furthermore, to showcase KuaiSim's flexibility in accommodating different datasets, we demonstrate its versatility and robustness when deploying it on the ML-1m dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187856386",
                    "name": "Kesen Zhao"
                },
                {
                    "authorId": "2244772979",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "2244625023",
                    "name": "Qingpeng Cai"
                },
                {
                    "authorId": "2244774353",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2204700561",
                    "name": "Ziru Liu"
                },
                {
                    "authorId": "2153430224",
                    "name": "Dong Zheng"
                },
                {
                    "authorId": "2244625137",
                    "name": "Peng Jiang"
                },
                {
                    "authorId": "2244624390",
                    "name": "Kun Gai"
                }
            ]
        },
        {
            "paperId": "5197996cb586157dc7f5a7a8bd12c2f0b1377228",
            "title": "Reinforcing User Retention in a Billion Scale Short Video Recommender System",
            "abstract": "Recently, short video platforms have achieved rapid user growth by recommending interesting content to users. The objective of the recommendation is to optimize user retention, thereby driving the growth of DAU (Daily Active Users). Retention is a long-term feedback after multiple interactions of users and the system, and it is hard to decompose retention reward to each item or a list of items. Thus traditional point-wise and list-wise models are not able to optimize retention. In this paper, we choose reinforcement learning methods to optimize the retention as they are designed to maximize the long-term performance. We formulate the problem as an infinite-horizon request-based Markov Decision Process, and our objective is to minimize the accumulated time interval of multiple sessions, which is equal to improving the app open frequency and user retention. However, current reinforcement learning algorithms can not be directly applied in this setting due to uncertainty, bias, and long delay time incurred by the properties of user retention. We propose a novel method, dubbed RLUR, to address the aforementioned challenges. Both offline and live experiments show that RLUR can significantly improve user retention. RLUR has been fully launched in Kuaishou app for a long time, and achieves consistent performance improvement on user retention and DAU.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144994208",
                    "name": "Qingpeng Cai"
                },
                {
                    "authorId": "50152132",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "2170467924",
                    "name": "Xueliang Wang"
                },
                {
                    "authorId": "2204463122",
                    "name": "Tianyou Zuo"
                },
                {
                    "authorId": "2204463602",
                    "name": "Wentao Xie"
                },
                {
                    "authorId": "2118582559",
                    "name": "Bin Yang"
                },
                {
                    "authorId": "2153430224",
                    "name": "Dong Zheng"
                },
                {
                    "authorId": "2061280682",
                    "name": "Peng Jiang"
                },
                {
                    "authorId": "20029557",
                    "name": "Kun Gai"
                }
            ]
        },
        {
            "paperId": "5b9d865f2e8b1e421f42b0d8ca9c85538744bc2d",
            "title": "Exploration and Regularization of the Latent Action Space in Recommendation",
            "abstract": "In recommender systems, reinforcement learning solutions have effectively boosted recommendation performance because of their ability to capture long-term user-system interaction. However, the action space of the recommendation policy is a list of items, which could be extremely large with a dynamic candidate item pool. To overcome this challenge, we propose a hyper-actor and critic learning framework where the policy decomposes the item list generation process into a hyper-action inference step and an effect-action selection step. The first step maps the given state space into a vectorized hyper-action space, and the second step selects the item list based on the hyper-action. In order to regulate the discrepancy between the two action spaces, we design an alignment module along with a kernel mapping function for items to ensure inference accuracy and include a supervision module to stabilize the learning process. We build simulated environments on public datasets and empirically show that our framework is superior in recommendation compared to standard RL baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50152132",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "144994208",
                    "name": "Qingpeng Cai"
                },
                {
                    "authorId": "2020121737",
                    "name": "Bowen Sun"
                },
                {
                    "authorId": "2185248657",
                    "name": "Yuhao Wang"
                },
                {
                    "authorId": "2218282111",
                    "name": "Jiadi Jiang"
                },
                {
                    "authorId": "2153430224",
                    "name": "Dong Zheng"
                },
                {
                    "authorId": "20029557",
                    "name": "Kun Gai"
                },
                {
                    "authorId": "2061280682",
                    "name": "Peng Jiang"
                },
                {
                    "authorId": "2116711312",
                    "name": "Xiang Zhao"
                },
                {
                    "authorId": "1739818",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "6ca1057501f8c2ac06d068cf9f1a8a984f87f60f",
            "title": "Two-Stage Constrained Actor-Critic for Short Video Recommendation",
            "abstract": "The wide popularity of short videos on social media poses new opportunities and challenges to optimize recommender systems on the video-sharing platforms. Users sequentially interact with the system and provide complex and multi-faceted responses, including WatchTime and various types of interactions with multiple videos. On the one hand, the platforms aim at optimizing the users\u2019 cumulative WatchTime (main goal) in the long term, which can be effectively optimized by Reinforcement Learning. On the other hand, the platforms also need to satisfy the constraint of accommodating the responses of multiple user interactions (auxiliary goals) such as Like, Follow, Share, etc. In this paper, we formulate the problem of short video recommendation as a Constrained Markov Decision Process (CMDP). We find that traditional constrained reinforcement learning algorithms fail to work well in this setting. We propose a novel two-stage constrained actor-critic method: At stage one, we learn individual policies to optimize each auxiliary signal. In stage two, we learn a policy to (i) optimize the main signal and (ii) stay close to policies learned in the first stage, which effectively guarantees the performance of this main policy on the auxiliaries. Through extensive offline evaluations, we demonstrate the effectiveness of our method over alternatives in both optimizing the main goal as well as balancing the others. We further show the advantage of our method in live experiments of short video recommendations, where it significantly outperforms other baselines in terms of both WatchTime and interactions. Our approach has been fully launched in the production system to optimize user experiences on the platform.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144994208",
                    "name": "Qingpeng Cai"
                },
                {
                    "authorId": "2093481204",
                    "name": "Zhenghai Xue"
                },
                {
                    "authorId": "2117835555",
                    "name": "Chi Zhang"
                },
                {
                    "authorId": "2052308113",
                    "name": "Wanqi Xue"
                },
                {
                    "authorId": "50152132",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "2088873890",
                    "name": "Ruohan Zhan"
                },
                {
                    "authorId": "2170467924",
                    "name": "Xueliang Wang"
                },
                {
                    "authorId": "2204463122",
                    "name": "Tianyou Zuo"
                },
                {
                    "authorId": "2204463602",
                    "name": "Wentao Xie"
                },
                {
                    "authorId": "2153430224",
                    "name": "Dong Zheng"
                },
                {
                    "authorId": "2061280682",
                    "name": "Peng Jiang"
                },
                {
                    "authorId": "20029557",
                    "name": "Kun Gai"
                }
            ]
        },
        {
            "paperId": "8b87214afe121dc39c342fabc73014c4fefb31e3",
            "title": "State Regularized Policy Optimization on Data with Dynamics Shift",
            "abstract": "In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \\textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\\textbf{S}tate \\textbf{R}egularized \\textbf{P}olicy \\textbf{O}ptimization) algorithm. To conduct theoretical analyses, the intuition of similar environment structures is characterized by the notion of homomorphous MDPs. We then demonstrate a lower-bound performance guarantee on policies regularized by the stationary state distribution. In practice, SRPO can be an add-on module to context-based algorithms in both online and offline RL settings. Experimental results show that SRPO can make several context-based algorithms far more data efficient and significantly improve their overall performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2093481204",
                    "name": "Zhenghai Xue"
                },
                {
                    "authorId": "144994208",
                    "name": "Qingpeng Cai"
                },
                {
                    "authorId": "50152132",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "2153430224",
                    "name": "Dong Zheng"
                },
                {
                    "authorId": "2061280682",
                    "name": "Peng Jiang"
                },
                {
                    "authorId": "20029557",
                    "name": "Kun Gai"
                },
                {
                    "authorId": "143706345",
                    "name": "Bo An"
                }
            ]
        },
        {
            "paperId": "b38eea02e5d003e18a451f0f36c9b39b8b32e15e",
            "title": "A Multi-Agent Framework for Recommendation with Heterogeneous Sources",
            "abstract": "With the ever prospering of the web technologies, there is a common need to make recommendations from heterogeneous sources, such as recommending products and advertisements together on the e-commerce websites. People usually solve such recommendation problem by a two-stage paradigm, where the first stage is generating candidates from each source, and the second one is aggregating and ranking the generated heterogeneous candidates to produce the final results. While existing models have achieved many successes, they mostly optimize the above two stages separately, where the user preferences can only be used to supervise the second stage, while for the first one, there is no signal to tell whether the generated candidates are accurate enough to cover the user preference. To solve the above problem, in this paper, we design a multi-agent framework to jointly optimize the above two stages. In specific, suppose there are N sources in our problem, then we deploy N+1 agents, where the first N agents correspond one-to-one with the sources, aiming to select the sources-specific candidates, and the last agent is designed to aggregate the candidates from different sources for the final recommendation. All the agents play a cooperative game, aiming to maximize the rewards revealing user preferences. We implement our idea based on the Deep Q-network, where we design a decomposable reward to enhance the training efficiency. We adapt our model to a real-world recommendation problem abstracted from a famous short video platform-Kuaishou.com. We conduct extensive experiments to demonstrate the effectiveness of our model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Yabin Zhang"
                },
                {
                    "authorId": "2143609496",
                    "name": "Weiqi Shao"
                },
                {
                    "authorId": "2144230136",
                    "name": "Xu Chen"
                },
                {
                    "authorId": "1390662136",
                    "name": "Yali Du"
                },
                {
                    "authorId": "2112435159",
                    "name": "Xiaoxiao Xu"
                },
                {
                    "authorId": "2153430224",
                    "name": "Dong Zheng"
                },
                {
                    "authorId": "3438562",
                    "name": "Changhua Pei"
                },
                {
                    "authorId": "2226839640",
                    "name": "Shuai Zhang"
                },
                {
                    "authorId": "2061280682",
                    "name": "Peng Jiang"
                },
                {
                    "authorId": "20029557",
                    "name": "Kun Gai"
                }
            ]
        },
        {
            "paperId": "34d1b6b3e73848caebe759de74d54753c9c2f3f5",
            "title": "PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement",
            "abstract": "Current advances in recommender systems have been remarkably successful in optimizing immediate engagement. However, long-term user engagement, a more desirable performance metric, remains difficult to improve. Meanwhile, recent reinforcement learning (RL) algorithms have shown their effectiveness in a variety of long-term goal optimization tasks. For this reason, RL is widely considered as a promising framework for optimizing long-term user engagement in recommendation. Though promising, the application of RL heavily relies on well-designed rewards, but designing rewards related to long-term user engagement is quite difficult. To mitigate the problem, we propose a novel paradigm, recommender systems with human preferences (or Preference-based Recommender systems), which allows RL recommender systems to learn from preferences about users' historical behaviors rather than explicitly defined rewards. Such preferences are easily accessible through techniques such as crowdsourcing, as they do not require any expert knowledge. With PrefRec, we can fully exploit the advantages of RL in optimizing long-term goals, while avoiding complex reward engineering. PrefRec uses the preferences to automatically train a reward function in an end-to-end manner. The reward function is then used to generate learning signals to train the recommendation policy. Furthermore, we design an effective optimization method for PrefRec, which uses an additional value function, expectile regression and reward model pre-training to improve the performance. We conduct experiments on a variety of long-term user engagement optimization tasks. The results show that PrefRec significantly outperforms previous state-of-the-art methods in all the tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052308113",
                    "name": "Wanqi Xue"
                },
                {
                    "authorId": "144994208",
                    "name": "Qingpeng Cai"
                },
                {
                    "authorId": "2093481204",
                    "name": "Zhenghai Xue"
                },
                {
                    "authorId": "1992684938",
                    "name": "Shuo Sun"
                },
                {
                    "authorId": "50152132",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "2153430224",
                    "name": "Dong Zheng"
                },
                {
                    "authorId": "2061280682",
                    "name": "Peng Jiang"
                },
                {
                    "authorId": "20029557",
                    "name": "Kun Gai"
                },
                {
                    "authorId": "143706345",
                    "name": "Bo An"
                }
            ]
        },
        {
            "paperId": "4240f9fe3db7267e8266ae9646d0c7ca447fbaee",
            "title": "Constrained Reinforcement Learning for Short Video Recommendation",
            "abstract": "The wide popularity of short videos on social media poses new opportunities and challenges to optimize recommender systems on the video-sharing platforms. Users provide complex and multi-faceted responses towards recommendations, including watch time and various types of interactions with videos. As a result, established recommendation algorithms that concern a single objective are not adequate to meet this new demand of optimizing comprehensive user experiences. In this paper, we formulate the problem of short video recommendation as a constrained Markov Decision Process (MDP), where platforms want to optimize the main goal of user watch time in long term, with the constraint of accommodating the auxiliary responses of user interactions such as sharing/downloading videos. To solve the constrained MDP, we propose a two-stage reinforcement learning approach based on actor-critic framework. At stage one, we learn individual policies to optimize each auxiliary response. At stage two, we learn a policy to (i) optimize the main response and (ii) stay close to policies learned at the first stage, which effectively guarantees the performance of this main policy on the auxiliaries. Through extensive simulations, we demonstrate effectiveness of our approach over alternatives in both optimizing the main goal as well as balancing the others. We further show the advantage of our approach in live experiments of short video recommendations, where it significantly outperforms other baselines in terms of watch time and interactions from video views. Our approach has been fully launched in the production system to optimize user experiences on the platform.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144994208",
                    "name": "Qingpeng Cai"
                },
                {
                    "authorId": "2088873890",
                    "name": "Ruohan Zhan"
                },
                {
                    "authorId": null,
                    "name": "Chi Zhang"
                },
                {
                    "authorId": "2115692728",
                    "name": "Jie Zheng"
                },
                {
                    "authorId": "2166477178",
                    "name": "Guangwei Ding"
                },
                {
                    "authorId": "2925921",
                    "name": "Pinghua Gong"
                },
                {
                    "authorId": "2153430224",
                    "name": "Dong Zheng"
                },
                {
                    "authorId": "2061280682",
                    "name": "Peng Jiang"
                }
            ]
        },
        {
            "paperId": "aa211fe8b25eaf2d38ac4c69599f12ce61c584ec",
            "title": "ResAct: Reinforcing Long-term Engagement in Sequential Recommendation with Residual Actor",
            "abstract": "Long-term engagement is preferred over immediate engagement in sequential recommendation as it directly affects product operational metrics such as daily active users (DAUs) and dwell time. Meanwhile, reinforcement learning (RL) is widely regarded as a promising framework for optimizing long-term engagement in sequential recommendation. However, due to expensive online interactions, it is very difficult for RL algorithms to perform state-action value estimation, exploration and feature extraction when optimizing long-term engagement. In this paper, we propose ResAct which seeks a policy that is close to, but better than, the online-serving policy. In this way, we can collect sufficient data near the learned policy so that state-action values can be properly estimated, and there is no need to perform online exploration. ResAct optimizes the policy by first reconstructing the online behaviors and then improving it via a Residual Actor. To extract long-term information, ResAct utilizes two information-theoretical regularizers to confirm the expressiveness and conciseness of features. We conduct experiments on a benchmark dataset and a large-scale industrial dataset which consists of tens of millions of recommendation requests. Experimental results show that our method significantly outperforms the state-of-the-art baselines in various long-term engagement optimization tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052308113",
                    "name": "Wanqi Xue"
                },
                {
                    "authorId": "144994208",
                    "name": "Qingpeng Cai"
                },
                {
                    "authorId": "2088873890",
                    "name": "Ruohan Zhan"
                },
                {
                    "authorId": "2153430224",
                    "name": "Dong Zheng"
                },
                {
                    "authorId": "2061280682",
                    "name": "Peng Jiang"
                },
                {
                    "authorId": "2057964623",
                    "name": "Bo An"
                }
            ]
        }
    ]
}