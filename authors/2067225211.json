{
    "authorId": "2067225211",
    "papers": [
        {
            "paperId": "b9e27dcb825d40634ecb0729fa60bc96b88a84c0",
            "title": "Visual Debugging of Behavioural Models",
            "abstract": "In this paper, we present the CLEAR visualizer tool, which supports the debugging task of behavioural models being analyzed using model checking techniques. The tool provides visualization techniques for simplifying the comprehension of counterexamples by highlighting some specific states in the model where a choice is possible between executing a correct behaviour or falling into an erroneous part of the model. Our tool was applied successfully to many case studies and allowed us to visually identify several kinds of typical bugs. Video URL: https://youtu.be/nJLOnRaPe1A",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3374562",
                    "name": "G. Barbon"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1866308",
                    "name": "Gwen Sala\u00fcn"
                },
                {
                    "authorId": "133934735",
                    "name": "Emmanuel Yah"
                }
            ]
        },
        {
            "paperId": "c0793eec454d031e6cd3f3b7169048b802f42e8e",
            "title": "Debugging of Behavioural Models using Counterexample Analysis",
            "abstract": "Model checking is an established technique for automatically verifying that a model satisfies a given temporal property. When the model violates the property, the model checker returns a counterexample, which is a sequence of actions leading to a state where the property is not satisfied. Understanding this counterexample for debugging the specification is a complicated task for several reasons: (i) the counterexample can contain a large number of actions, (ii) the debugging task is mostly achieved manually, and (iii) the counterexample does not explicitly highlight the source of the bug that is hidden in the model. This article presents a new approach that improves the usability of model checking by simplifying the comprehension of counterexamples. To do so, we first extract in the model all the counterexamples. Second, we define an analysis algorithm that identifies actions that make the model skip from incorrect to correct behaviours, making these actions relevant from a debugging perspective. Third, we develop a set of abstraction techniques to extract these actions from counterexamples. Our approach is fully automated by a tool we implemented and was applied on real-world case studies from various application areas for evaluation purposes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3374562",
                    "name": "G. Barbon"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1866308",
                    "name": "Gwen Sala\u00fcn"
                }
            ]
        },
        {
            "paperId": "843b86e94ee75f40f458b73de6efb4b125e8e4ee",
            "title": "Personalized and Diverse Task Composition in Crowdsourcing",
            "abstract": "We study task composition in crowdsourcing and the effect of personalization and diversity on performance. A central process in crowdsourcing is task assignment, the mechanism through which workers find tasks. On popular platforms such as Amazon Mechanical Turk, task assignment is facilitated by the ability to sort tasks by dimensions such as creation date or reward amount. Task composition improves task assignment by producing for each worker, a personalized summary of tasks, referred to as a Composite Task (CT). We propose different ways of producing CTs and formulate an optimization problem that finds for a worker, the most relevant and diverse CTs. We show empirically that workers\u2019 experience is greatly improved due to personalization that enforces an adequation of CTs with workers\u2019 skills and preferences. We also study and formalize various ways of diversifying tasks in each CT. Task diversity is grounded in organization studies that have shown its impact on worker motivation\u00a0 [33] . Our experiments show that diverse CTs contribute to improving outcome quality. More specifically, we show that while task throughput and worker retention are best with ranked lists, crowdwork quality reaches its best with CTs diversified by requesters, thereby confirming that workers look to expose their \u201cgood\u201d work to many requesters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27052776",
                    "name": "Maha Alsayasneh"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "3416184",
                    "name": "Julien Pilourdault"
                },
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "48449806",
                    "name": "Motomichi Toyama"
                },
                {
                    "authorId": "2822140",
                    "name": "J. Renders"
                }
            ]
        },
        {
            "paperId": "164ec4b071adb8b3aa153d00b00af5487b494dda",
            "title": "Data Analysis at Scale: Systems, Algorithms and Information",
            "abstract": "HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L\u2019archive ouverte pluridisciplinaire HAL, est destin\u00e9e au d\u00e9p\u00f4t et \u00e0 la diffusion de documents scientifiques de niveau recherche, publi\u00e9s ou non, \u00e9manant des \u00e9tablissements d\u2019enseignement et de recherche fran\u00e7ais ou \u00e9trangers, des laboratoires publics ou priv\u00e9s. Data Analysis at Scale: Systems, Algorithms and Information Vincent Leroy",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                }
            ]
        },
        {
            "paperId": "582b1e74517077a7b4c5796409314eabce9e23cb",
            "title": "Efficient and Versatile FPGA Acceleration of Support Counting for Stream Mining of Sequences and Frequent Itemsets",
            "abstract": "Stream processing has become extremely popular for analyzing huge volumes of data for a variety of applications, including IoT, social networks, retail, and software logs analysis. Streams of data are produced continuously and are mined to extract patterns characterizing the data. A class of data mining algorithm, called generate-and-test, produces a set of candidate patterns that are then evaluated over data. The main challenges of these algorithms are to achieve high throughput, low latency, and reduced power consumption. In this article, we present a novel power-efficient, fast, and versatile hardware architecture whose objective is to monitor a set of target patterns to maintain their frequency over a stream of data. This accelerator can be used to accelerate data-mining algorithms, including itemsets and sequences mining. The massive fine-grain reconfiguration capability of field-programmable gate array (FPGA) technologies is ideal to implement the high number of pattern-detection units needed for these intensive data-mining applications. We have thus designed and implemented an IP that features high-density FPGA occupation and high working frequency. We provide detailed description of the IP internal micro-architecture and its actual implementation and optimization for the targeted FPGA resources. We validate our architecture by developing a co-designed implementation of the Apriori Frequent Itemset Mining (FIM) algorithm, and perform numerous experiments against existing hardware and software solutions. We demonstrate that FIM hardware acceleration is particularly efficient for large and low-density datasets (i.e., long-tailed datasets). Our IP reaches a data throughput of 250 million items/s and monitors up to 11.6k patterns simultaneously, on a prototyping board that overall consumes 24W in the worst case. Furthermore, our hardware accelerator remains generic and can be integrated to other generate and test algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1395722868",
                    "name": "Adrien Prost-Boucle"
                },
                {
                    "authorId": "150103462",
                    "name": "F. P\u00e9trot"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "2065179888",
                    "name": "Hande Alemdar"
                }
            ]
        },
        {
            "paperId": "a519141a7febd9503c044045d06d41b1187ea47c",
            "title": "Scalable high-performance architecture for convolutional ternary neural networks on FPGA",
            "abstract": "Thanks to their excellent performances on typical artificial intelligence problems, deep neural networks have drawn a lot of interest lately. However, this comes at the cost of large computational needs and high power consumption. Benefiting from high precision at acceptable hardware cost on these difficult problems is a challenge. To address it, we advocate the use of ternary neural networks (TNN) that, when properly trained, can reach results close to the state of the art using floatingpoint arithmetic. We present a highly versatile FPGA friendly architecture for TNN in which we can vary both the number of bits of the input data and the level of parallelism at synthesis time, allowing to trade throughput for hardware resources and power consumption. To demonstrate the efficiency of our proposal, we implement high-complexity convolutional neural networks on the Xilinx Virtex-7 VC709 FPGA board. While reaching a better accuracy than comparable designs, we can target either high throughput or low power. We measure a throughput up to 27 000 fps at \u22487W or up to 8.36 TMAC/s at \u224813 W.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1395722868",
                    "name": "Adrien Prost-Boucle"
                },
                {
                    "authorId": "2872902",
                    "name": "A. Bourge"
                },
                {
                    "authorId": "150103462",
                    "name": "F. P\u00e9trot"
                },
                {
                    "authorId": "2065179888",
                    "name": "Hande Alemdar"
                },
                {
                    "authorId": "2058115816",
                    "name": "Nicholas Caldwell"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                }
            ]
        },
        {
            "paperId": "d3681507771ddb7918b82fdccc428367c3ec28bf",
            "title": "Crowdsourcing Strategies for Text Creation Tasks",
            "abstract": "We examine deployment strategies for text translation and text summarization tasks. We formalize a deployment strategy along three dimensions: work structure, workforce organization , and work style. Work structure can be either simultaneous or sequential, workforce organization independent or collaborative, and work style either crowd-only or hybrid. We use Amazon Mechanical Turk to evaluate the cost, latency, and quality of various deployment strategies. We asses our strategies for different scenarios: short/long text, presence/absence of an outline, and popular/unpopular topics. Our findings serve as a basis to automate the deployment of text creation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "27052776",
                    "name": "Maha Alsayasneh"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                }
            ]
        },
        {
            "paperId": "1d26471711c953dc1a2369cd3dfe674c65f6381b",
            "title": "Task Composition in Crowdsourcing",
            "abstract": "Crowdsourcing has gained popularity in a variety of domains as an increasing number of jobs are \"taskified\" and completed independently by a set of workers. A central process in crowdsourcing is the mechanism through which workers find tasks. On popular platforms such as Amazon Mechanical Turk, tasks can be sorted by dimensions such as creation date or reward amount. Research efforts on task assignment have focused on adopting a requester-centric approach whereby tasks are proposed to workers in order to maximize overall task throughput, result quality and cost. In this paper, we advocate the need to complement that with a worker-centric approach to task assignment, and examine the problem of producing, for each worker, a personalized summary of tasks that preserves overall task throughput. We formalize task composition for workers as an optimization problem that finds a representative set of k valid and relevant Composite Tasks (CTs). Validity enforces that a composite task complies with the task arrival rate and satisfies the worker's expected wage. Relevance imposes that tasks match the worker's qualifications. We show empirically that workers' experience is greatly improved due to task homogeneity in each CT and to the adequation of CTs with workers' skills. As a result task throughput is improved.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "3416184",
                    "name": "Julien Pilourdault"
                },
                {
                    "authorId": "3039205",
                    "name": "R. M. Borromeo"
                },
                {
                    "authorId": "48449806",
                    "name": "Motomichi Toyama"
                }
            ]
        },
        {
            "paperId": "58624a3e0651b891c6a976c2e0f49488c3dc27d2",
            "title": "Testing Interestingness Measures in Practice: A Large-Scale Analysis of Buying Patterns",
            "abstract": "Understanding customer buying patterns is of great interest to the retail industry. Association rule mining is a common technique for extracting correlations such as people in the South of France buy ros\u00e9 wine or customers who buy pat\u00e9 also buy salted butter and sour bread. Unfortunately, sifting through a high number of buying patterns is not useful in practice, because of the predominance of popular products in the top rules. As a result, a number of \"interestingness\" measures (over 30) have been proposed to rank rules. However, there is no agreement on which measures are more appropriate for retail data. Moreover, since pattern mining algorithms output thousands of association rules for each product, the ability for an analyst to rely on ranking measures to identify the most interesting ones is crucial. In this paper, we develop CAPA (Comparative Analysis of PAtterns), a framework that provides analysts with the ability to compare different rule rankings. We report on how we used C A PA to compare 34 interestingness measures applied to patterns extracted from customer receipts of more than 1,800 stores for a period of one year.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3359814",
                    "name": "M. Kirchgessner"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "48043042",
                    "name": "Shashwati Mishra"
                }
            ]
        },
        {
            "paperId": "8cc5827df6cee340b2dca9fb1a7c04b3bfd7fc2a",
            "title": "Locality-Aware Routing in Stateful Streaming Applications",
            "abstract": "Distributed stream processing engines continuously execute series of operators on data streams. Horizontal scaling is achieved by deploying multiple instances of each operator in order to process data tuples in parallel. As the application is distributed on an increasingly high number of servers, the likelihood that the stream is sent to a different server for each operator increases. This is particularly important in the case of stateful applications that rely on keys to deterministically route messages to a specific instance of an operator. Since network is a bottleneck for many stream applications, this behavior significantly degrades their performance. Our objective is to improve stream locality for stateful stream processing applications. We propose to analyse traces of the application to uncover correlations between the keys used in successive routing operations. By assigning correlated keys to instances hosted on the same server, we significantly reduce network consumption and increase performance while preserving load balance. Furthermore, this approach is executed online, so that the assignment can automatically adapt to changes in the characteristics of the data. Data migration is handled seamlessly with each routing configuration update. We implemented and evaluated our protocol using Apache Storm, with a real workload consisting of geo-tagged Flickr pictures as well as Twitter publications. Our results show a significant improvement in throughput.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2850502",
                    "name": "Matthieu Caneill"
                },
                {
                    "authorId": "1400960363",
                    "name": "Ahmed El-Rheddane"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1718674",
                    "name": "N. D. Palma"
                }
            ]
        },
        {
            "paperId": "f1c08fa7fe24750bda532cc2761987d76410931b",
            "title": "Distributed Evaluation of Top-k Temporal Joins",
            "abstract": "We study a particular kind of join, coined Ranked Temporal Join (RTJ), featuring predicates that compare time intervals and a scoring function associated with each predicate to quantify how well it is satisfied. RTJ queries are prevalent in a variety of applications such as network traffic monitoring, task scheduling, and tweet analysis. RTJ queries are often best interpreted as top-k queries where only the best matches are returned. We show how to exploit the nature of temporal predicates and the properties of their associated scoring semantics to design TKIJ, an efficient query evaluation approach on a distributed Map-Reduce architecture. TKIJ relies on an offline statistics computation that, given a time partitioning into granules, computes the distribution of intervals' endpoints in each granule, and an online computation that generates query-dependent score bounds. Those statistics are used for workload assignment to reducers. This aims at reducing data replication, to limit I/O cost. Additionally, high-scoring results are distributed evenly to enable each reducer to prune unnecessary results. Our extensive experiments on synthetic and real datasets show that TKIJ outperforms state-of-the-art competitors and provides very good performance for n-ary RTJ queries on temporal data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3416184",
                    "name": "Julien Pilourdault"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "f73e69f2376e793590aa751effd05aef57c639ff",
            "title": "Ternary neural networks for resource-efficient AI applications",
            "abstract": "The computation and storage requirements for Deep Neural Networks (DNNs) are usually high. This issue limits their deployability on ubiquitous computing devices such as smart phones, wearables and autonomous drones. In this paper, we propose ternary neural networks (TNNs) in order to make deep learning more resource-efficient. We train these TNNs using a teacher-student approach based on a novel, layer-wise greedy methodology. Thanks to our two-stage training procedure, the teacher network is still able to use state-of-the-art methods such as dropout and batch normalization to increase accuracy and reduce training time. Using only ternary weights and activations, the student ternary network learns to mimic the behavior of its teacher network without using any multiplication. Unlike its {-1,1} binary counterparts, a ternary neural network inherently prunes the smaller weights by setting them to zero during training. This makes them sparser and thus more energy-efficient. We design a purpose-built hardware architecture for TNNs and implement it on FPGA and ASIC. We evaluate TNNs on several benchmark datasets and demonstrate up to 3.1 \u03c7 better energy efficiency with respect to the state of the art while also improving accuracy.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2065179888",
                    "name": "Hande Alemdar"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1395722868",
                    "name": "Adrien Prost-Boucle"
                },
                {
                    "authorId": "150103462",
                    "name": "F. P\u00e9trot"
                }
            ]
        },
        {
            "paperId": "33586add5d3a923ce834a588d329cb66ca74f995",
            "title": "Interactive Data-Driven Research: the place where databases and data mining research meet",
            "abstract": "Data-driven research, or the science of letting data tell us what we are looking for, is in many areas, the only viable approach to research. In some domains like adaptive clinical trials and emerging research areas such as social computing, useful results are highly dependent on the ability to observe and interactively explore large volumes of real datasets. Database management is the science of efficiently storing and retrieving data. Data mining is the science of discovering hidden correlations in data. Interactive data-driven research is a natural meeting point that presents a new research opportunity. The ability to conduct effective data-driven research requires to combine efficient indexing and querying from databases and pattern mining and classification from data mining to help analysts understand what lies behind large data volumes. In this paper, we explore key challenges and new opportunities in building robust systems for interactive data-driven research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "3359814",
                    "name": "M. Kirchgessner"
                },
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                }
            ]
        },
        {
            "paperId": "ba279f97a3d097ed8f4b1a39835b487b6b03f7ab",
            "title": "Discovering characterizing regions for consumer products",
            "abstract": "Consumer behaviour holds special importance in the retail industry. Consumer location impacts consumer behaviour by dictating purchase trends. This paper investigates the problem of examining product sales across a chain of stores to extract the geographic regions that characterize a product. Characterizing region for a product is a coherent geographic region where the consumers actively consume the said product. We introduce DICE, a diffusion-based technique to uncover all such regions for a given product, when they exist. In contrast to current state of the art, DICE involves minimal usage of parameters and shows remarkable tolerance to noise. We present experiments conducted on real datasets from a general commercial supermarket in France. Empirical evaluation and user-studies establish that the presented method significantly outperforms its natural baseline and previous state of the art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48043042",
                    "name": "Shashwati Mishra"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "bc3ba9eb70e135cf7d9f12aea9aba7cc48546298",
            "title": "Building Representative Composite Items",
            "abstract": "The problem of summarizing a large collection of homogeneous items has been addressed extensively in particular in the case of geo-tagged datasets (e.g. Flickr photos and tags). In our work, we study the problem of summarizing large collections of heterogeneous items. For example, a user planning to spend extended periods of time in a given city would be interested in seeing a map of that city with item summaries in different geographic areas, each containing a theater, a gym, a bakery, a few restaurants and a subway station. We propose to solve that problem by building representative Composite Items (CIs). To the best of our knowledge, this is the first work that addresses the problem of finding representative CIs for heterogeneous items. Our problem naturally arises when summarizing geo-tagged datasets but also in other datasets such as movie or music summarization. We formalize building representative CIs as an optimization problem and propose KFC, an extended fuzzy clustering algorithm to solve it. We show that KFC converges and run extensive experiments on a variety of real datasets that validate its effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1732180",
                    "name": "\u00c9ric Gaussier"
                },
                {
                    "authorId": "2701917",
                    "name": "Seyed Hamid Mirisaee"
                }
            ]
        },
        {
            "paperId": "f215285a89fc3facbc5d02c574225a62b05c741c",
            "title": "Data mining approach to temporal debugging of embedded streaming applications",
            "abstract": "One of the greatest challenges in the embedded systems area is to empower software developers with tools that speed up the debugging of QoS properties in applications. Typical streaming applications, such as multimedia (audio/video) decoding, fulfill the QoS properties by respecting the real-time deadlines. A perfectly functional application, when missing these deadlines, may lead to cracks in the sound or perceptible artifacts in the image. We start from the premise that most of the streaming applications that run on embedded systems can be expressed under a data ow model of computation, where the application is represented as a directed graph of the data flowing through computational units called actors. It has been shown that in order to meet real-time constraints the actors should be scheduled in a periodic manner. We exploit this property to propose SATM - a novel approach based on data mining techniques that automatically analyzes execution traces of streaming applications, and discovers significant breaks in the periodicity of actors, as well as potential causes of these breaks. We show on a real use case that our debugging approach can uncover important defects and pinpoint their location to the application developer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2309544255",
                    "name": "Oleg Iegorov"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1693038",
                    "name": "A. Termier"
                },
                {
                    "authorId": "73210645",
                    "name": "Jean-Fran\u00e7ois M\u00e9haut"
                },
                {
                    "authorId": "145264159",
                    "name": "M. Santana"
                }
            ]
        },
        {
            "paperId": "7dd184ac6bcd15b3f6fb2f8005e39a59dd147cd3",
            "title": "Proceedings of the Workshops of the EDBT/ICDT 2014 Joint Conference (EDBT/ICDT 2014), Athens, Greece, March 28, 2014",
            "abstract": "MapReduce framework is established as the standard approach for parallel processing of massive amounts of data. In this work, we extend the model of MapReduce scheduling on unrelated processors (Moseley et al., SPAA 2011) and deal with the practically important case of jobs with any number of Map and Reduce tasks. We present a polynomial-time (32 + )-approximation algorithm for minimizing the total weighted completion time in this setting. To the best of our knowledge, this is the most general setting of MapReduce scheduling for which an approximation guarantee is known. Moreover, this is the first time that a constant approximation ratio is obtained for minimizing the total weighted completion time on unrelated processors under a nontrivial class of precedence constraints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720972",
                    "name": "K. Candan"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1691736",
                    "name": "Nicole Schweikardt"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                }
            ]
        },
        {
            "paperId": "08c9add6905cfb46d9ace882cc0279670f1da4e5",
            "title": "Piggybacking on Social Networks",
            "abstract": "The popularity of social-networking sites has increased rapidly over the last decade. A basic functionalities of social-networking sites is to present users with streams of events shared by their friends. At a systems level, materialized per-user views are a common way to assemble and deliver such event streams on-line and with low latency. Access to the data stores, which keep the user views, is a major bottleneck of social-networking systems. We propose to improve the throughput of these systems by using social piggybacking, which consists of processing the requests of two friends by querying and updating the view of a third common friend. By using one such hub view, the system can serve requests of the first friend without querying or updating the view of the second. We show that, given a social graph, social piggybacking can minimize the overall number of requests, but computing the optimal set of hubs is an NP-hard problem. We propose an O(log n) approximation algorithm and a heuristic to solve the problem, and evaluate them using the full Twitter and Flickr social graphs, which have up to billions of edges. Compared to existing approaches, using social piggybacking results in similar throughput in systems with few servers, but enables substantial throughput improvements as the size of the system grows, reaching up to a 2-factor increase. We also evaluate our algorithms on a real social networking system prototype and we show that the actual increase in throughput corresponds nicely to the gain anticipated by our cost function.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1865700",
                    "name": "F. Junqueira"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "143665011",
                    "name": "M. Serafini"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                }
            ]
        },
        {
            "paperId": "0851fb3615c5a3fd5449b3b207fbb2ce8ce1908e",
            "title": "Reactive index replication for distributed search engines",
            "abstract": "Distributed search engines comprise multiple sites deployed across geographically distant regions, each site being specialized to serve the queries of local users. When a search site cannot accurately compute the results of a query, it must forward the query to other sites. This paper considers the problem of selecting the documents indexed by each site focusing on replication to increase the fraction of queries processed locally. We propose RIP, an algorithm for replicating documents and posting lists that is practical and has two important features. RIP evaluates user interests in an online fashion and uses only local data of a site. Being an online approach simplifies the operational complexity, while locality enables higher performance when processing queries and documents. The decision procedure, on top of being online and local, incorporates document popularity and user queries, which is critical when assuming a replication budget for each site. Having a replication budget reflects the hardware constraints of any given site. We evaluate RIP against the approach of replicating popular documents statically, and show that we achieve significant gains, while having the additional benefit of supporting incremental indexes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1865700",
                    "name": "F. Junqueira"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "39388165",
                    "name": "M. Morel"
                }
            ]
        },
        {
            "paperId": "bce8ebf3cbaa5f787219057f4faa4298077a07e9",
            "title": "Social piggybacking: leveraging common friends to generate event streams",
            "abstract": "Social networking systems present users with online event streams that include recent activities of their friends. Materialized per-user views are a common way to generate such event streams on-line and with low latency. We propose improving throughput by using social piggybacking: process the requests of two friends by just querying and updating the view of a third common friend. By using one such hub view, the system can serve requests of the first friend without querying nor updating the view of the second. This reduces the overall load on views, and can be implemented with minimal adaptations by existing social networking architectures. We show that, given a social graph, there exist hub-based schedules that minimize the overall number of view requests, but computing them is NP-hard. We propose a heuristic to solve the problem and evaluate it using samples from the Twitter and Flickr social graphs. The evaluation shows that existing solutions generate up to 2.4 times the amount of view requests induced by our heuristic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1682878",
                    "name": "A. Gionis"
                },
                {
                    "authorId": "1865700",
                    "name": "F. Junqueira"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "143665011",
                    "name": "M. Serafini"
                },
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                }
            ]
        },
        {
            "paperId": "d1809e7116cc259d0871097b5a49792aa1a0bd43",
            "title": "Shepherding social feed generation with Sheep",
            "abstract": "Social feeds are used in many popular Web applications. They let users produce events, as well as read feeds containing the events generated by their friends. This paper investigates the design of an in-memory platform to manage social feeds. We show that straightforward memcache implementations suffer from a low throughput due to bandwidth bottlenecks. Following these observations, we propose Sheep, a system to support applications based on social feeds that leverages data aggregation and co-location to alleviate these bottlenecks. We show experimentally that Sheep outperforms memcache implementations by a factor of 7.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1865700",
                    "name": "F. Junqueira"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "143665011",
                    "name": "M. Serafini"
                },
                {
                    "authorId": "2305624",
                    "name": "Adam Silberstein"
                }
            ]
        },
        {
            "paperId": "0bb501cf2e0157dbcbdce48f3555fb660c3aa0e5",
            "title": "Converging Quickly to Independent Uniform Random Topologies",
            "abstract": "The peer sampling service is a core building block for gossip protocols in peer-to-peer networks. Ideally, a peer sampling service continuously provides each peer with a sample of peers picked uniformly at random in the network. While empirical studies have shown that uniformity was achieved, analysis proposed so far assume strong restrictions on the topology of the overlay network it continuously generates. In this work, we analyze a Generic Random Peer Sampling Service (GRPS) that satisfies the desirable properties for any peer sampling service\u2013small views, uniform sample, load balancing, and independence\u2013 and relieve strong degree connections in the nodes assumed in previous works. The main result we prove is: starting from any simple (without loops and parallel edges) directed graph with out-degree equal to c for all nodes, and recursively applying GRPS, eventually results in a random simple directed graph with out-degree equal to c for all nodes. We test empirically convergence time and independence time for GRPS. Finally, We use this empirical evaluation to show that GRPS performs better than previously presented peer sampling services.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1723331",
                    "name": "Anne-Marie Kermarrec"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "2849050",
                    "name": "Christopher Thraves"
                }
            ]
        },
        {
            "paperId": "6e35bb49b28f8edac6ca48287cac6b5ef097b4ca",
            "title": "Assigning documents to master sites in distributed search",
            "abstract": "An appealing solution to scale Web search with the growth of the Internet is the use of distributed architectures. Distributed search engines rely on multiple sites deployed in distant regions across the world, where each site is specialized to serve queries issued by the users of its region. This paper investigates the problem of assigning each document to a master site. We show that by leveraging similarities between a document and the activity of the users, we can accurately detect which site is the most relevant to place a document. We conduct various experiments using two document assignment approaches, showing performance improvements of up to 20.8% over a baseline technique which assigns the documents to search sites based on their language.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144367841",
                    "name": "Roi Blanco"
                },
                {
                    "authorId": "1776940",
                    "name": "B. B. Cambazoglu"
                },
                {
                    "authorId": "1865700",
                    "name": "F. Junqueira"
                },
                {
                    "authorId": "2070917808",
                    "name": "I. Kelly"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                }
            ]
        },
        {
            "paperId": "8a9388a79549fab5c422bba8eeafb34e2ce085de",
            "title": "Collaborative personalized top-k processing",
            "abstract": "This article presents P4Q, a fully decentralized gossip-based protocol to personalize query processing in social tagging systems. P4Q dynamically associates each user with social acquaintances sharing similar tagging behaviors. Queries are gossiped among such acquaintances, computed on-the-fly in a collaborative, yet partitioned manner, and results are iteratively refined and returned to the querier. Analytical and experimental evaluations convey the scalability of P4Q for top-k query processing, as well its inherent ability to cope with users updating profiles and departing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3042223",
                    "name": "Xiao Bai"
                },
                {
                    "authorId": "1727558",
                    "name": "R. Guerraoui"
                },
                {
                    "authorId": "1723331",
                    "name": "Anne-Marie Kermarrec"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                }
            ]
        },
        {
            "paperId": "a043985dbee6461dc91d8bbab6ada0daf23974a8",
            "title": "Distributed social graph embedding",
            "abstract": "Distributed recommender systems are becoming increasingly important for they address both scalability and the Big Brother syndrome. Link prediction is one of the core mechanism in recommender systems and relies on extracting some notion of proximity between entities in a graph. Applied to social networks, defining a proximity metric between users enable to predict potential relevant future relationships. In this paper, we propose SoCS (Social Coordinate Systems}, a fully distributed algorithm that embeds any social graph in an Euclidean space, which can easily be used to implement link prediction. To the best of our knowledge, SoCS is the first system explicitly relying on graph embedding. Inspired by recent works on non-isomorphic embeddings, the SoCS embedding preserves the community structure of the original graph, while being easy to decentralize. Nodes thus get assigned coordinates that reflect their social position. We show through experiments on real and synthetic data sets that these coordinates can be exploited for efficient link prediction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1723331",
                    "name": "Anne-Marie Kermarrec"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "2348944",
                    "name": "Gilles Tr\u00e9dan"
                }
            ]
        },
        {
            "paperId": "0cac33b0423dfc408ab64a72b84ef100520db98e",
            "title": "Ensuring Uniformity in Random Peer Sampling Services",
            "abstract": "The peer sampling service is a core building block for gossip protocols in peer-to-peer networks. Ideally, a peer sampling service continuously provides each peer with a sample of peers picked uniformly at random in the network. While empirical studies have shown that uniformity was achieved, analysis proposed so far assume strong restrictions on the topology of the overlay network it continuously generates. In this work, we analyze a Generic Random Peer Sampling Service (GRPS) that satisfies the desirable properties for any peer sampling service \u2013small views, uniform sample, load balancing, and independence\u2013 and relieve strong degree connections in the nodes assumed in previous works. The main result we prove is: starting from any simple (without loops and parallel edges) directed graph with out-degree equal to c for all nodes, and recursively applying GRPS, eventually results in a random simple directed graph with out-degree equal to c for all nodes. We test empirically convergence time and independence time for GRPS. We use this empirical evaluation to show that GRPS performs better than previously presented peer sampling services. We also present a variant of GRPS that ensures that the in and out-degrees of nodes in the initial network are maintained in the resulting graph. Finally, we discuss on how to deal with new nodes in both settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1723331",
                    "name": "Anne-Marie Kermarrec"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1409271580",
                    "name": "Christopher Thraves-Caro"
                }
            ]
        },
        {
            "paperId": "4524672008dfac045b21758b1be9b5defacf8104",
            "title": "Cold start link prediction",
            "abstract": "In the traditional link prediction problem, a snapshot of a social network is used as a starting point to predict, by means of graph-theoretic measures, the links that are likely to appear in the future. In this paper, we introduce cold start link prediction as the problem of predicting the structure of a social network when the network itself is totally missing while some other information regarding the nodes is available. We propose a two-phase method based on the bootstrap probabilistic graph. The first phase generates an implicit social network under the form of a probabilistic graph. The second phase applies probabilistic graph-based measures to produce the final prediction. We assess our method empirically over a large data collection obtained from Flickr, using interest groups as the initial information. The experiments confirm the effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1776940",
                    "name": "B. B. Cambazoglu"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                }
            ]
        },
        {
            "paperId": "62d1d634fbdb5f46f5c8781e6357be283895159e",
            "title": "Distributing Social Applications",
            "abstract": "The so-called Web 2.0 revolution has fundamentally changed the way people interact with the Internet. The Web has turned from a read-only infrastructure to a collaborative platform. By expressing their preferences and sharing private information, the users benefit from a personalized Web experience. Yet, these systems raise several problems in terms of \\emph{privacy} and \\emph{scalability}. The social platforms use the user information for commercial needs and expose the privacy and preferences of the users. Furthermore, centralized personalized systems require costly data-centers. As a consequence, existing centralized social platforms do not exploit the full extent of the personalization possibilities. In this thesis, we consider the design of social networks and social information services in the context of \\emph{peer-to-peer} (P2P) networks. P2P networks are decentralized architecture, thus the users participates to the service and control their own data. This greatly improves the privacy of the users and the scalability of the system. Nevertheless, building social systems in a distributed context also comes with many challenges. The information is distributed among the users and the system has be able to efficiently locate relevant data. The contributions of this thesis are as follow. We define the \\emph{cold start link prediction} problem, which consists in predicting the edges of a social network solely from the social information of the users. We propose a method based on a \\emph{probabilistic graph} to solve this problem. We evaluate it on a dataset from Flickr, using the group membership as social information. Our results show that the social information indeed enables a prediction of the social network. Thus, the centralization of the information threatens the privacy of the users, hence the need for decentralized systems. We propose \\textsc{SoCS}, a \\emph{decentralized} algorithm for \\emph{link prediction}. Recommending neighbors is a central functionality in social networks, and it is therefore crucial to propose a decentralized approach as a first step towards P2P social networks. \\textsc{SoCS} relies on gossip protocols to perform a force-based embedding of the social networks. The social coordinates are then used to predict links among vertices. We show that \\textsc{SoCS} is adapted to decentralized systems at it is churn resilient and has a low bandwidth consumption. We propose \\textsc{GMIN}, a \\emph{decentralized} platform for \\emph{personalized services} based on social information. \\textsc{GMIN} provides each user with neighbors that share her interests. The clustering algorithm we propose takes care to encompass all the different interests of the user, and not only the main ones. We then propose a personalized \\emph{query expansion} algorithm (\\textsc{GQE}) that leverages the \\textsc{GMIN} neighbors. For each query, the system computes a tag centrality based on the relations between tags as seen by the user and her neighbors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                }
            ]
        },
        {
            "paperId": "713f3d4225a9716a96742f6089560cb8302b0994",
            "title": "Addressing Sparsity in Decentralized Recommender Systems through Random Walks",
            "abstract": "The need for efficient decentralized recommender systems has been appreciated for some time, both for the intrinsic advantages of decentralization and the necessity of integrating recommender systems into existing P2P applications. On the other hand, the accuracy of recommender systems is often hurt by data sparsity. In this paper, we compare different decentralized user-based and item-based Collaborative Filtering (CF) algorithms with each other, and propose a new user-based random walk approach customized for decentralized systems, specically designed to handle sparse data. We show how the application of random walks to decentralized environments is different from the centralized version. We examine the performance of our random walk approach in different settings by varying the sparsity, the similarity measure and the neighborhood size. In addition, we introduce the popularizing disadvantage of the signicance weighting term traditionally used to increase the precision of similarity measures, and elaborate how it can affect the performance of the random walk algorithm. The simulations on MovieLens 10,000,000 ratings dataset demonstrate that over a wide range of sparsity, our algorithm outperforms other decentralized CF schemes. Moreover, our results show decentralized user-based approaches perform better than their item-based counterparts in P2P recommender applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1723331",
                    "name": "Anne-Marie Kermarrec"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "3005081",
                    "name": "Afshin Moin"
                },
                {
                    "authorId": "1409271580",
                    "name": "Christopher Thraves-Caro"
                }
            ]
        },
        {
            "paperId": "982561a5525e715323108a13d4bc28395309b0af",
            "title": "D2HT: The Best of Both Worlds, Integrating RPS and DHT",
            "abstract": "Distributed Hash Tables (DHTs) and Random Peer Sampling (RPS) provide important and complementary services in the area of P2P overlay networks. DHTs achieve efficient lookup while RPS enables nodes to build and maintain connectivity in the presence of high churn. Clearly, many applications, e.g. in the area of search, would greatly benefit if both these services were available together at a reasonable cost. This paper integrates a structured P2P overlay and a Random Peer Sampling service through gossip protocols. This system called D2HT, leverages the small-world nature of DHTs and relies on two cohabiting gossip protocols maintaining the close and long-range links respectively. The long links are chosen according to a harmonic distribution, following the Kleinberg small-world model. This approach exhibits several benefits: (i) The resulting DHT is highly dynamic and self-stabilizing, changes are tracked for free through the gossip nature of the protocol. This removes the need for complex, usually disjoint, and expensive join and repair procedures. Yet, it achieves reasonable routing performance with respect to standard DHTs; (ii) The resulting peer sampling service provides a biased sampling following a harmonic distribution: this improves the routing without jeopardizing the quality of the RPS. The set of long-range links which are a source of RPS can be used independently by others applications for free. They change continuously, achieving well-balanced routing across the nodes. We perform extensive simulations and compare the performances of D2HT with Cyclon, HRing, Symphony and Pastry to demonstrate the gains achieved by the approach proposed in this paper.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1769344",
                    "name": "M. Bertier"
                },
                {
                    "authorId": "121067622",
                    "name": "Fran\u00e7ois Bonnet"
                },
                {
                    "authorId": "1723331",
                    "name": "Anne-Marie Kermarrec"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "145506228",
                    "name": "Sathya Peri"
                },
                {
                    "authorId": "143902006",
                    "name": "M. Raynal"
                }
            ]
        },
        {
            "paperId": "d2630b6ffd2d99b748cb081eb984962ae1ea9118",
            "title": "Gossiping personalized queries",
            "abstract": "This paper presents P3Q, a fully decentralized gossip-based protocol to personalize query processing in social tagging systems. P3Q dynamically associates each user with social acquaintances sharing similar tagging behaviours. Queries are gossiped among such acquaintances, computed on the fly in a collaborative, yet partitioned manner, and results are iteratively refined and returned to the querier. Analytical and experimental evaluations convey the scalability of P3Q for top-k query processing. More specifically, we show that on a 10,000-user delicious trace, with little storage at each user, the queries are accurately computed within reasonable time and bandwidth consumption. We also report on the inherent ability of P3Q to cope with users updating profiles and departing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3042223",
                    "name": "Xiao Bai"
                },
                {
                    "authorId": "1769344",
                    "name": "M. Bertier"
                },
                {
                    "authorId": "1727558",
                    "name": "R. Guerraoui"
                },
                {
                    "authorId": "1723331",
                    "name": "Anne-Marie Kermarrec"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                }
            ]
        },
        {
            "paperId": "7bd9e48736b4e0610ecc14938f605879ecd3edc6",
            "title": "Toward personalized query expansion",
            "abstract": "Social networking and tagging have taken off at an unexpected scale and speed, opening huge opportunities to enhance the user search experience. We present Gossple, a new, user-centric, approach to improve the exploration of the Internet. Underlying Gossple lies the intuition that while social networks provides news from your old buddies, you can learn a lot more from people you don't know, but with whom you share many (tagging) interests. More specifically, considering a collaborative tagging system with active taggers annotating content, Gossple expands the search query, of any user u, with tags that are considered \"close\" enough with respect to users that are \"close\" to u.\n Gossple users create their own network of social acquaintances in a gossip-based manner, by dynamically computing the estimation of a distance between taggers, based on cosine similarity between tags and items. These connections are used to feed a TagMap: our central abstraction that captures the personalised relationships between tags. The TagMap is then used by Gossple to meaningfully expand queries leveraging the personalised network. This is achieved through the TagRank algorithm, an adaptation of the celebrated pagerank algorithm, which automatically determines which tags best expand a list of tags in a given query.\n Gossple has no central authority: every user stores its own items and its tagging behaviour is stored only by its neighbours. The resulting networks are live, dynamic and do not require any underlying structure. We report on our evaluation of Gossple with CiteUlike traces, involving 33,834 users. In short, we show that, with little information stored at every peer, Gossple enables to retrieve items that cannot be retrieved with state of the art search systems (completeness).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1769344",
                    "name": "M. Bertier"
                },
                {
                    "authorId": "1727558",
                    "name": "R. Guerraoui"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                },
                {
                    "authorId": "1723331",
                    "name": "Anne-Marie Kermarrec"
                }
            ]
        },
        {
            "paperId": "decf1960945f972e798ce84a8a89e80f061896e8",
            "title": "Personalized Web Search by Gossiping with Unknown Social Acquaintances",
            "abstract": "Social networking and collaborative tagging have taken off at an unexpected scale and speed. Huge opportunities to significantly boost the search experience are out there in the Web but the amount of information to be dissected is seemingly Herculean. Moreover, users might be reluctant to publicize their profiles in order to facilitate the navigation of other users. We present Gossple, the first decentralized system to personalize the user search experience by expanding queries with information derived from anonymous social acquaintances. Underlying Gossple lies the intuition that, while social networks can provide you with news from your old buddies, you can learn a lot more from people you do not know, but with whom you share many interests. Considering a collaborative tagging system with active participants annotating content, Gossple manages each user profile and dynamically creates her personalized \"social\" network by gossiping and computing a distance between users, without revealing which profile is associated with which user. Using the information in this personalized social network, each user extracts knowledge about the relations between tags which she locally leverages to improve her own search experience through a personalized query expansion mechanism. We evaluate Gossple on traces crawled from CiteUlike and Delicious, with 33,834 and 20,000 users. We do so in a real distributed system of 170 PlanetLab nodes as well as by simulating a large-scale system involving thousands of peers. In short, we show that by sharing their tagging behaviors with small numbers of neighbors, users benefit from personalized and efficient query expansion, increasing the number of query results (recall) while significantly improving on quality (precision).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1769344",
                    "name": "M. Bertier"
                },
                {
                    "authorId": "15191012",
                    "name": "Davide Frey"
                },
                {
                    "authorId": "1727558",
                    "name": "R. Guerraoui"
                },
                {
                    "authorId": "1723331",
                    "name": "Anne-Marie Kermarrec"
                },
                {
                    "authorId": "2067225211",
                    "name": "V. Leroy"
                }
            ]
        }
    ]
}