{
    "authorId": "2167027235",
    "papers": [
        {
            "paperId": "69a86c7bd84f16137b743381441ec5ca9c026151",
            "title": "Teams of LLM Agents can Exploit Zero-Day Vulnerabilities",
            "abstract": "LLM agents have become increasingly sophisticated, especially in the realm of cybersecurity. Researchers have shown that LLM agents can exploit real-world vulnerabilities when given a description of the vulnerability and toy capture-the-flag problems. However, these agents still perform poorly on real-world vulnerabilities that are unknown to the agent ahead of time (zero-day vulnerabilities). In this work, we show that teams of LLM agents can exploit real-world, zero-day vulnerabilities. Prior agents struggle with exploring many different vulnerabilities and long-range planning when used alone. To resolve this, we introduce HPTSA, a system of agents with a planning agent that can launch subagents. The planning agent explores the system and determines which subagents to call, resolving long-term planning issues when trying different vulnerabilities. We construct a benchmark of 15 real-world vulnerabilities and show that our team of agents improve over prior work by up to 4.5$\\times$.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265752580",
                    "name": "Richard Fang"
                },
                {
                    "authorId": "2240484939",
                    "name": "R. Bindu"
                },
                {
                    "authorId": "2265943673",
                    "name": "Akul Gupta"
                },
                {
                    "authorId": "2167027235",
                    "name": "Qiusi Zhan"
                },
                {
                    "authorId": "2266051282",
                    "name": "Daniel Kang"
                }
            ]
        },
        {
            "paperId": "c8eee9766f0968e8f1b1be0731bc70b85be0ac97",
            "title": "InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents",
            "abstract": "Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative. In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4 vulnerable to attacks 24% of the time. Further investigation into an enhanced setting, where the attacker instructions are reinforced with a hacking prompt, shows additional increases in success rates, nearly doubling the attack success rate on the ReAct-prompted GPT-4. Our findings raise questions about the widespread deployment of LLM Agents. Our benchmark is available at https://github.com/uiuc-kang-lab/InjecAgent.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167027235",
                    "name": "Qiusi Zhan"
                },
                {
                    "authorId": "2290249799",
                    "name": "Zhixiang Liang"
                },
                {
                    "authorId": "2290021230",
                    "name": "Zifan Ying"
                },
                {
                    "authorId": "2266051282",
                    "name": "Daniel Kang"
                }
            ]
        },
        {
            "paperId": "e3d317fca93654ffb4af005aea35b9d42441fe38",
            "title": "LLM Agents can Autonomously Hack Websites",
            "abstract": "In recent years, large language models (LLMs) have become increasingly capable and can now interact with tools (i.e., call functions), read documents, and recursively call themselves. As a result, these LLMs can now function autonomously as agents. With the rise in capabilities of these agents, recent work has speculated on how LLM agents would affect cybersecurity. However, not much is known about the offensive capabilities of LLM agents. In this work, we show that LLM agents can autonomously hack websites, performing tasks as complex as blind database schema extraction and SQL injections without human feedback. Importantly, the agent does not need to know the vulnerability beforehand. This capability is uniquely enabled by frontier models that are highly capable of tool use and leveraging extended context. Namely, we show that GPT-4 is capable of such hacks, but existing open-source models are not. Finally, we show that GPT-4 is capable of autonomously finding vulnerabilities in websites in the wild. Our findings raise questions about the widespread deployment of LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265752580",
                    "name": "Richard Fang"
                },
                {
                    "authorId": "2240484939",
                    "name": "R. Bindu"
                },
                {
                    "authorId": "2265943673",
                    "name": "Akul Gupta"
                },
                {
                    "authorId": "2167027235",
                    "name": "Qiusi Zhan"
                },
                {
                    "authorId": "2266051282",
                    "name": "Daniel Kang"
                }
            ]
        },
        {
            "paperId": "a5ebebcf0d17d08bfa2895533b121a4411c35685",
            "title": "GLEN: General-Purpose Event Detection for Thousands of Types",
            "abstract": "The progress of event extraction research has been hindered by the absence of wide-coverage, large-scale datasets. To make event extraction systems more accessible, we build a general-purpose event detection dataset GLEN, which covers 205K event mentions with 3,465 different types, making it more than 20x larger in ontology than today's largest event dataset. GLEN is created by utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables us to use the abundant existing annotation for PropBank as distant supervision. In addition, we also propose a new multi-stage event detection model CEDAR specifically designed to handle the large ontology size in GLEN. We show that our model exhibits superior performance compared to a range of baselines including InstructGPT. Finally, we perform error analysis and show that label noise is still the largest challenge for improving performance for this new dataset. Our dataset, code, and models are released at \\url{https://github.com/ZQS1943/GLEN}.}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167027235",
                    "name": "Qiusi Zhan"
                },
                {
                    "authorId": "2109154767",
                    "name": "Sha Li"
                },
                {
                    "authorId": "39996046",
                    "name": "Kathryn Conger"
                },
                {
                    "authorId": "145755155",
                    "name": "Martha Palmer"
                },
                {
                    "authorId": "2181650518",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "ccae9fcb1f344e56a3f7cb05a4b49a6e658f9dd2",
            "title": "Removing RLHF Protections in GPT-4 via Fine-Tuning",
            "abstract": "As large language models (LLMs) have increased in their capabilities, so doestheir potential for dual use. To reduce harmful outputs, produces and vendors ofLLMs have used reinforcement learning with human feedback (RLHF). In tandem,LLM vendors have been increasingly enabling fine-tuning of their most powerfulmodels. However, concurrent work has shown that fine-tuning can remove RLHFprotections. We may expect that the most powerful models currently available(GPT-4) are less susceptible to fine-tuning attacks. In this work, we show the contrary: fine-tuning allows attackers to remove RLHFprotections with as few as 340 examples and a 95% success rate. These trainingexamples can be automatically generated with weaker models. We further show thatremoving RLHF protections does not decrease usefulness on non-censored outputs,providing evidence that our fine-tuning strategy does not decrease usefulnessdespite using weaker models to generate training data. Our results show the needfor further research on protections on LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167027235",
                    "name": "Qiusi Zhan"
                },
                {
                    "authorId": "2265752580",
                    "name": "Richard Fang"
                },
                {
                    "authorId": "2240484939",
                    "name": "R. Bindu"
                },
                {
                    "authorId": "2265943673",
                    "name": "Akul Gupta"
                },
                {
                    "authorId": "2265754270",
                    "name": "Tatsunori Hashimoto"
                },
                {
                    "authorId": "2266051282",
                    "name": "Daniel Kang"
                }
            ]
        },
        {
            "paperId": "36839dd7e08a7221d7db55f82be36237c1186e3d",
            "title": "EA2E: Improving Consistency with Event Awareness for Document-Level Argument Extraction",
            "abstract": "Events are inter-related in documents. Motivated by the one-sense-per-discourse theory, we hypothesize that a participant tends to play consistent roles across multiple events in the same document. However recent work on document-level event argument extraction models each individual event in isolation and therefore causes inconsistency among extracted arguments across events, which will further cause discrepancy for downstream applications such as event knowledge base population, question answering, and hypothesis generation. In this work, we formulate event argument consistency as the constraints from event-event relations under the document-level setting. To improve consistency we introduce the Event-Aware Argument Extraction (EA$^2$E) model with augmented context for training and inference. Experiment results on WIKIEVENTS and ACE2005 datasets demonstrate the effectiveness of EA$^2$E compared to baseline methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145653969",
                    "name": "Qi Zeng"
                },
                {
                    "authorId": "2167027235",
                    "name": "Qiusi Zhan"
                },
                {
                    "authorId": "2072975828",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "eec5b6e6a3fb3983d13ed1cf17f3160ee07a9d7e",
            "title": "ConFiguRe: Exploring Discourse-level Chinese Figures of Speech",
            "abstract": "Figures of speech, such as metaphor and irony, are ubiquitous in literature works and colloquial conversations. This poses great challenge for natural language understanding since figures of speech usually deviate from their ostensible meanings to express deeper semantic implications. Previous research lays emphasis on the literary aspect of figures and seldom provide a comprehensive exploration from a view of computational linguistics. In this paper, we first propose the concept of figurative unit, which is the carrier of a figure. Then we select 12 types of figures commonly used in Chinese, and build a Chinese corpus for Contextualized Figure Recognition (ConFiguRe). Different from previous token-level or sentence-level counterparts, ConFiguRe aims at extracting a figurative unit from discourse-level context, and classifying the figurative unit into the right figure type. On ConFiguRe, three tasks, i.e., figure extraction, figure type classification and figure recognition, are designed and the state-of-the-art techniques are utilized to implement the benchmarks. We conduct thorough experiments and show that all three tasks are challenging for existing models, thus requiring further research. Our dataset and code are publicly available at https://github.com/pku-tangent/ConFiguRe.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116276849",
                    "name": "Dawei Zhu"
                },
                {
                    "authorId": "2167027235",
                    "name": "Qiusi Zhan"
                },
                {
                    "authorId": "2157944555",
                    "name": "Zhejian Zhou"
                },
                {
                    "authorId": "2183730942",
                    "name": "Yifan Song"
                },
                {
                    "authorId": "2107975178",
                    "name": "Jiebin Zhang"
                },
                {
                    "authorId": "1695451",
                    "name": "Sujian Li"
                }
            ]
        }
    ]
}