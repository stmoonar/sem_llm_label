{
    "authorId": "2242189619",
    "papers": [
        {
            "paperId": "2fb593ca4b6d2631832d6424e238c32db3db5434",
            "title": "Factuality of Large Language Models in the Year 2024",
            "abstract": "Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2282539097",
                    "name": "Muhammad Arslan Manzoor"
                },
                {
                    "authorId": "2284540712",
                    "name": "Fei Liu"
                },
                {
                    "authorId": "2282539112",
                    "name": "Georgi Georgiev"
                },
                {
                    "authorId": "2211732585",
                    "name": "Rocktim Jyoti Das"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "8f584931615043d23ed5a1351b0fca547fbe6f16",
            "title": "Exploring the Potential of Multimodal LLM with Knowledge-Intensive Multimodal ASR",
            "abstract": "Recent advancements in multimodal large language models (MLLMs) have made significant progress in integrating information across various modalities, yet real-world applications in educational and scientific domains remain challenging. This paper introduces the Multimodal Scientific ASR (MS-ASR) task, which focuses on transcribing scientific conference videos by leveraging visual information from slides to enhance the accuracy of technical terminologies. Realized that traditional metrics like WER fall short in assessing performance accurately, prompting the proposal of severity-aware WER (SWER) that considers the content type and severity of ASR errors. We propose the Scientific Vision Augmented ASR (SciVASR) framework as a baseline method, enabling MLLMs to improve transcript quality through post-editing. Evaluations of state-of-the-art MLLMs, including GPT-4o, show a 45% improvement over speech-only baselines, highlighting the importance of multimodal information integration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "122699890",
                    "name": "Thuy-Trang Vu"
                },
                {
                    "authorId": "2888926",
                    "name": "Ehsan Shareghi"
                },
                {
                    "authorId": "2561045",
                    "name": "Gholamreza Haffari"
                }
            ]
        },
        {
            "paperId": "9741eb61739f2221c186634663876e2c1024c746",
            "title": "OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs",
            "abstract": "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. Difficulties lie in assessing the factuality of free-form responses in open domains. Also, different papers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress. To mitigate these issues, we propose OpenFactCheck, a unified factuality evaluation framework for LLMs. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims, (ii) LLMEVAL, a unified evaluation framework assesses LLM's factuality ability from various perspectives fairly, and (iii) CHECKEREVAL is an extensible solution for gauging the reliability of automatic fact-checkers' verification results using human-annotated datasets. OpenFactCheck is publicly released at https://github.com/yuxiaw/OpenFactCheck.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2300555930",
                    "name": "Hasan Iqbal"
                },
                {
                    "authorId": "2282539112",
                    "name": "Georgi Georgiev"
                },
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "aa335755b411de74ca37536f636b76ab87bcda07",
            "title": "Can Machines Resonate with Humans? Evaluating the Emotional and Empathic Comprehension of LMs",
            "abstract": "Empathy plays a pivotal role in fostering prosocial behavior, often triggered by the sharing of personal experiences through narratives. However, modeling empathy using NLP approaches remains challenging due to its deep interconnection with human interaction dynamics. Previous approaches, which involve fine-tuning language models (LMs) on human-annotated empathic datasets, have had limited success. In our pursuit of improving empathy understanding in LMs, we propose several strategies, including contrastive learning with masked LMs and supervised fine-tuning with Large Language Models (LLMs). While these methods show improvements over previous methods, the overall results remain unsatisfactory. To better understand this trend, we performed an analysis which reveals a low agreement among annotators. This lack of consensus hinders training and highlights the subjective nature of the task. We also explore the cultural impact on annotations. To study this, we meticulously collected story pairs in Urdu language and find that subjectivity in interpreting empathy among annotators appears to be independent of cultural background. The insights from our systematic exploration of LMs' understanding of empathy suggest that there is considerable room for exploration in both task formulation and modeling.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2282539097",
                    "name": "Muhammad Arslan Manzoor"
                },
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "ed24b6814946c1975b0d86736055fa528b6419c0",
            "title": "Against The Achilles' Heel: A Survey on Red Teaming for Generative Models",
            "abstract": "Generative models are rapidly gaining popularity and being integrated into everyday applications, raising concerns over their safety issues as various vulnerabilities are exposed. Faced with the problem, the field of red teaming is experiencing fast-paced growth, which highlights the need for a comprehensive organization covering the entire pipeline and addressing emerging topics for the community. Our extensive survey, which examines over 120 papers, introduces a taxonomy of fine-grained attack strategies grounded in the inherent capabilities of language models. Additionally, we have developed the searcher framework that unifies various automatic red teaming approaches. Moreover, our survey covers novel areas including multimodal attacks and defenses, risks around multilingual models, overkill of harmless queries, and safety of downstream applications. We hope this survey can provide a systematic perspective on the field and unlock new areas of research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284823063",
                    "name": "Lizhi Lin"
                },
                {
                    "authorId": "2292039878",
                    "name": "Honglin Mu"
                },
                {
                    "authorId": "51230252",
                    "name": "Zenan Zhai"
                },
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2275119551",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2215688800",
                    "name": "Renxi Wang"
                },
                {
                    "authorId": "2294509423",
                    "name": "Junjie Gao"
                },
                {
                    "authorId": "2266000475",
                    "name": "Yixuan Zhang"
                },
                {
                    "authorId": "2292032004",
                    "name": "Wanxiang Che"
                },
                {
                    "authorId": "2256987316",
                    "name": "Timothy Baldwin"
                },
                {
                    "authorId": "2110982198",
                    "name": "Xudong Han"
                },
                {
                    "authorId": "2266086775",
                    "name": "Haonan Li"
                }
            ]
        },
        {
            "paperId": "f68a7f40d755e87aa076df8ab74669e1c7cfdce3",
            "title": "Loki: An Open-Source Tool for Fact Verification",
            "abstract": "We introduce Loki, an open-source tool designed to address the growing problem of misinformation. Loki adopts a human-centered approach, striking a balance between the quality of fact-checking and the cost of human involvement. It decomposes the fact-checking task into a five-step pipeline: breaking down long texts into individual claims, assessing their check-worthiness, generating queries, retrieving evidence, and verifying the claims. Instead of fully automating the claim verification process, Loki provides essential information at each step to assist human judgment, especially for general users such as journalists and content moderators. Moreover, it has been optimized for latency, robustness, and cost efficiency at a commercially usable level. Loki is released under an MIT license and is available on GitHub. We also provide a video presenting the system and its capabilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2274084215",
                    "name": "Haonan Li"
                },
                {
                    "authorId": "2289460054",
                    "name": "Xudong Han"
                },
                {
                    "authorId": "2324499769",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2275119551",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2323789608",
                    "name": "Rui Xing"
                },
                {
                    "authorId": "2323788235",
                    "name": "Yilin Geng"
                },
                {
                    "authorId": "51230252",
                    "name": "Zenan Zhai"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2256987316",
                    "name": "Timothy Baldwin"
                }
            ]
        },
        {
            "paperId": "40dccaaf6f5a563437f26c75e92279ba415df392",
            "title": "Rethinking STS and NLI in Large Language Models",
            "abstract": "Recent years, have seen the rise of large language models (LLMs), where practitioners use task-specific prompts; this was shown to be effective for a variety of tasks. However, when applied to semantic textual similarity (STS) and natural language inference (NLI), the effectiveness of LLMs turns out to be limited by low-resource domain accuracy, model overconfidence, and difficulty to capture the disagreements between human judgements. With this in mind, here we try to rethink STS and NLI in the era of LLMs. We first evaluate the performance of STS and NLI in the clinical/biomedical domain, and then we assess LLMs\u2019 predictive confidence and their capability of capturing collective human opinions. We find that these old problems are still to be properly addressed in the era of LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2242189619",
                    "name": "Minghan Wang"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        }
    ]
}