{
    "authorId": "1390716752",
    "papers": [
        {
            "paperId": "317bc8054b054a1f1d1ed97425fecb27f35d2c28",
            "title": "FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods",
            "abstract": "This paper introduces the Fair Fairness Benchmark (\\textsf{FFB}), a benchmarking framework for in-processing group fairness methods. Ensuring fairness in machine learning is important for ethical compliance. However, there exist challenges in comparing and developing fairness methods due to inconsistencies in experimental settings, lack of accessible algorithmic implementations, and limited extensibility of current fairness packages and tools. To address these issues, we introduce an open-source standardized benchmark for evaluating in-processing group fairness methods and provide a comprehensive analysis of state-of-the-art methods to ensure different notions of group fairness. This work offers the following key contributions: the provision of flexible, extensible, minimalistic, and research-oriented open-source code; the establishment of unified fairness method benchmarking pipelines; and extensive benchmarking, which yields key insights from $\\mathbf{45,079}$ experiments, $\\mathbf{14,428}$ GPU hours. We believe that our work will significantly facilitate the growth and development of the fairness research community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50017230",
                    "name": "Xiaotian Han"
                },
                {
                    "authorId": "31357678",
                    "name": "Jianfeng Chi"
                },
                {
                    "authorId": "2179493818",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "2145778781",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "1390716752",
                    "name": "Han Zhao"
                },
                {
                    "authorId": "49648991",
                    "name": "Na Zou"
                },
                {
                    "authorId": "2193021044",
                    "name": "Xia Hu"
                }
            ]
        },
        {
            "paperId": "51484cf02592a3551f944b7c6bf94fe902c0aa66",
            "title": "Train Your Own GNN Teacher: Graph-Aware Distillation on Textual Graphs",
            "abstract": "How can we learn effective node representations on textual graphs? Graph Neural Networks (GNNs) that use Language Models (LMs) to encode textual information of graphs achieve state-of-the-art performance in many node classification tasks. Yet, combining GNNs with LMs has not been widely explored for practical deployments due to its scalability issues. In this work, we tackle this challenge by developing a Graph-Aware Distillation framework (GRAD) to encode graph structures into an LM for graph-free, fast inference. Different from conventional knowledge distillation, GRAD jointly optimizes a GNN teacher and a graph-free student over the graph's nodes via a shared LM. This encourages the graph-free student to exploit graph information encoded by the GNN teacher while at the same time, enables the GNN teacher to better leverage textual information from unlabeled nodes. As a result, the teacher and the student models learn from each other to improve their overall performance. Experiments in eight node classification benchmarks in both transductive and inductive settings showcase GRAD's superiority over existing distillation approaches for textual graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1944251405",
                    "name": "Costas Mavromatis"
                },
                {
                    "authorId": "40043851",
                    "name": "V. Ioannidis"
                },
                {
                    "authorId": "2151226309",
                    "name": "Shen Wang"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "65743795",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "1390716752",
                    "name": "Han Zhao"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "0dd31ddebbfca1fb8944f49deb1728bcef6b67b3",
            "title": "Understanding Gradual Domain Adaptation: Improved Analysis, Optimal Path and Beyond",
            "abstract": "The vast majority of existing algorithms for unsupervised domain adaptation (UDA) focus on adapting from a labeled source domain to an unlabeled target domain directly in a one-off way. Gradual domain adaptation (GDA), on the other hand, assumes a path of $(T-1)$ unlabeled intermediate domains bridging the source and target, and aims to provide better generalization in the target domain by leveraging the intermediate ones. Under certain assumptions, Kumar et al. (2020) proposed a simple algorithm, Gradual Self-Training, along with a generalization bound in the order of $e^{O(T)} \\left(\\varepsilon_0+O\\left(\\sqrt{log(T)/n}\\right)\\right)$ for the target domain error, where $\\varepsilon_0$ is the source domain error and $n$ is the data size of each domain. Due to the exponential factor, this upper bound becomes vacuous when $T$ is only moderately large. In this work, we analyze gradual self-training under more general and relaxed assumptions, and prove a significantly improved generalization bound as $\\varepsilon_0+ O \\left(T\\Delta + T/\\sqrt{n}\\right) + \\widetilde{O}\\left(1/\\sqrt{nT}\\right)$, where $\\Delta$ is the average distributional distance between consecutive domains. Compared with the existing bound with an exponential dependency on $T$ as a multiplicative factor, our bound only depends on $T$ linearly and additively. Perhaps more interestingly, our result implies the existence of an optimal choice of $T$ that minimizes the generalization error, and it also naturally suggests an optimal way to construct the path of intermediate domains so as to minimize the accumulative path length $T\\Delta$ between the source and target. To corroborate the implications of our theory, we examine gradual self-training on multiple semi-synthetic and real datasets, which confirms our findings. We believe our insights provide a path forward toward the design of future GDA algorithms.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2029142",
                    "name": "Haoxiang Wang"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "1390716752",
                    "name": "Han Zhao"
                }
            ]
        },
        {
            "paperId": "7e8e9f5ddefb025154ebbcc37b5c86302a8aea9d",
            "title": "Provable Domain Generalization via Invariant-Feature Subspace Recovery",
            "abstract": "Domain generalization asks for models trained over a set of training environments to perform well in unseen test environments. Recently, a series of algorithms such as Invariant Risk Minimization (IRM) has been proposed for domain generalization. However, Rosenfeld et al. (2021) shows that in a simple linear data model, even if non-convexity issues are ignored, IRM and its extensions cannot generalize to unseen environments with less than $d_s+1$ training environments, where $d_s$ is the dimension of the spurious-feature subspace. In this paper, we propose to achieve domain generalization with Invariant-feature Subspace Recovery (ISR). Our first algorithm, ISR-Mean, can identify the subspace spanned by invariant features from the first-order moments of the class-conditional distributions, and achieve provable domain generalization with $d_s+1$ training environments under the data model of Rosenfeld et al. (2021). Our second algorithm, ISR-Cov, further reduces the required number of training environments to $O(1)$ using the information of second-order moments. Notably, unlike IRM, our algorithms bypass non-convexity issues and enjoy global convergence guarantees. Empirically, our ISRs can obtain superior performance compared with IRM on synthetic benchmarks. In addition, on three real-world image and text datasets, we show that both ISRs can be used as simple yet effective post-processing methods to improve the worst-case accuracy of (pre-)trained models against spurious correlations and group shifts.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2029142",
                    "name": "Haoxiang Wang"
                },
                {
                    "authorId": "2151793924",
                    "name": "Haozhe Si"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "1390716752",
                    "name": "Han Zhao"
                }
            ]
        },
        {
            "paperId": "992a067ef40893607dfc5a412265bf70c4804952",
            "title": "Conditional Supervised Contrastive Learning for Fair Text Classification",
            "abstract": "Contrastive representation learning has gained much attention due to its superior performance in learning representations from both image and sequential data. However, the learned representations could potentially lead to performance disparities in downstream tasks, such as increased silencing of underrepresented groups in toxicity comment classification. In light of this challenge, in this work, we study learning fair representations that satisfy a notion of fairness known as equalized odds for text classification via contrastive learning. Specifically, we first theoretically analyze the connections between learning representations with a fairness constraint and conditional supervised contrastive objectives, and then propose to use conditional supervised contrastive objectives to learn fair representations for text classification. We conduct experiments on two text datasets to demonstrate the effectiveness of our approaches in balancing the trade-offs between task performance and bias mitigation among existing baselines for text classification. Furthermore, we also show that the proposed methods are stable in different hyperparameter settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31357678",
                    "name": "Jianfeng Chi"
                },
                {
                    "authorId": "1491747921",
                    "name": "Will Shand"
                },
                {
                    "authorId": "2142173500",
                    "name": "Yaodong Yu"
                },
                {
                    "authorId": "2782886",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "1390716752",
                    "name": "Han Zhao"
                },
                {
                    "authorId": null,
                    "name": "Yuan Tian"
                }
            ]
        },
        {
            "paperId": "9e9dc4b54b20882f871cf3b1438df33162bb5414",
            "title": "Conditional Contrastive Learning with Kernel",
            "abstract": "Conditional contrastive learning frameworks consider the conditional sampling procedure that constructs positive or negative data pairs conditioned on specific variables. Fair contrastive learning constructs negative pairs, for example, from the same gender (conditioning on sensitive information), which in turn reduces undesirable information from the learned representations; weakly supervised contrastive learning constructs positive pairs with similar annotative attributes (conditioning on auxiliary information), which in turn are incorporated into the representations. Although conditional contrastive learning enables many applications, the conditional sampling procedure can be challenging if we cannot obtain sufficient data pairs for some values of the conditioning variable. This paper presents Conditional Contrastive Learning with Kernel (CCL-K) that converts existing conditional contrastive objectives into alternative forms that mitigate the insufficient data problem. Instead of sampling data according to the value of the conditioning variable, CCL-K uses the Kernel Conditional Embedding Operator that samples data from all available data and assigns weights to each sampled data given the kernel similarity between the values of the conditioning variable. We conduct experiments using weakly supervised, fair, and hard negatives contrastive learning, showing CCL-K outperforms state-of-the-art baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145639633",
                    "name": "Yao-Hung Hubert Tsai"
                },
                {
                    "authorId": "2118909726",
                    "name": "Tianqi Li"
                },
                {
                    "authorId": "1384374825",
                    "name": "Martin Q. Ma"
                },
                {
                    "authorId": "1390716752",
                    "name": "Han Zhao"
                },
                {
                    "authorId": "2119017697",
                    "name": "Kun Zhang"
                },
                {
                    "authorId": "49933077",
                    "name": "Louis-Philippe Morency"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                }
            ]
        },
        {
            "paperId": "cd6a5a32233f3f47e8a8ea8980accbdaaa4e492f",
            "title": "Greedy modality selection via approximate submodular maximization",
            "abstract": "Multimodal learning considers learning from multi-modality data, aiming to fuse heterogeneous sources of information. However, it is not always feasible to leverage all available modalities due to memory constraints. Further, training on all the modalities may be inefficient when redundant information exists within data, such as different subsets of modalities providing similar performance. In light of these challenges, we study modality selection, intending to efficiently select the most informative and complementary modalities under certain computational constraints. We formulate a theoretical framework for optimizing modality selection in multimodal learning and introduce a utility measure to quantify the benefit of selecting a modality. For this optimization problem, we present efficient algorithms when the utility measure exhibits monotonicity and approximate submodularity. We also connect the utility measure with existing Shapley-value-based feature importance scores. Last, we demonstrate the efficacy of our algorithm on synthetic (Patch-MNIST) and two real-world (PEMS-SF, CMU-MOSI) datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51233028",
                    "name": "Runxiang Cheng"
                },
                {
                    "authorId": "1486413740",
                    "name": "Gargi Balasubramaniam"
                },
                {
                    "authorId": "2182670937",
                    "name": "Yifei He"
                },
                {
                    "authorId": "145639633",
                    "name": "Yao-Hung Hubert Tsai"
                },
                {
                    "authorId": "1390716752",
                    "name": "Han Zhao"
                }
            ]
        },
        {
            "paperId": "e1aeac9d653b4af6ee79e03671c48b20f1a67379",
            "title": "Fair and Optimal Classification via Transports to Wasserstein-Barycenter",
            "abstract": "Fairness in automated decision-making systems has gained increasing attention as their applications expand to real-world high-stakes domains. To facilitate the design of fair ML systems, it is essential to understand the potential trade-o\ufb00s between fairness and predictive power, and the construction of the optimal predictor under a given fairness constraint. In this paper, for general classi\ufb01cation problems under the group fairness criterion of demographic parity (DP), we precisely characterize the trade-o\ufb00 between DP and classi\ufb01cation accuracy, referred to as the minimum cost of fairness. Our insight comes from the key observation that \ufb01nding the optimal fair classi\ufb01er is equivalent to solving a Wasserstein-barycenter problem under (cid:96) 1 -norm restricted to the vertices of the probability simplex. Inspired by our characterization, we provide a construction of an optimal fair classi\ufb01er achieving this minimum cost via the composition of the Bayes regressor and optimal transports from its output distributions to the barycenter. Our construction naturally leads to an algorithm for post-processing any pre-trained predictor to satisfy DP fairness, complemented with \ufb01nite sample guarantees. Experiments on real-world datasets verify and demonstrate the e\ufb00ectiveness of our approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Ruicheng Xian"
                },
                {
                    "authorId": "2178278349",
                    "name": "Lang Yin"
                },
                {
                    "authorId": "1390716752",
                    "name": "Han Zhao"
                }
            ]
        },
        {
            "paperId": "014c5aa0a61489f09c224d5f43b6a0eeb47a1687",
            "title": "Invariant Information Bottleneck for Domain Generalization",
            "abstract": "Invariant risk minimization (IRM) has recently emerged as a promising alternative for domain generalization. Nevertheless, the loss function is difficult to optimize for nonlinear classifiers and the original optimization objective could fail when pseudo-invariant features and geometric skews exist. Inspired by IRM, in this paper we propose a novel formulation for domain generalization, dubbed invariant information bottleneck (IIB). IIB aims at minimizing invariant risks for nonlinear classifiers and simultaneously mitigating the impact of pseudo-invariant features and geometric skews. Specifically, we first present a novel formulation for invariant causal prediction via mutual information. Then we adopt the variational formulation of the mutual information to develop a tractable loss function for nonlinear classifiers. To overcome the failure modes of IRM, we propose to minimize the mutual information between the inputs and the corresponding representations. IIB significantly outperforms IRM on synthetic datasets, where the pseudo-invariant features and geometric skews occur, showing the effectiveness of proposed formulation in overcoming failure modes of IRM. Furthermore, experiments on DomainBed show that IIB outperforms 13 baselines by 0.9% on average across 7 real datasets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "2115383310",
                    "name": "Yifei Shen"
                },
                {
                    "authorId": "2115738764",
                    "name": "Yezhen Wang"
                },
                {
                    "authorId": "2117086566",
                    "name": "Wenzhen Zhu"
                },
                {
                    "authorId": "1718827",
                    "name": "Colorado Reed"
                },
                {
                    "authorId": "1519064904",
                    "name": "Jun Zhang"
                },
                {
                    "authorId": "2119081394",
                    "name": "Dongsheng Li"
                },
                {
                    "authorId": "1732330",
                    "name": "K. Keutzer"
                },
                {
                    "authorId": "1390716752",
                    "name": "Han Zhao"
                }
            ]
        },
        {
            "paperId": "17380e648439ca55ac948168a06b66722ba42a40",
            "title": "Understanding and Mitigating Accuracy Disparity in Regression",
            "abstract": "With the widespread deployment of large-scale prediction systems in high-stakes domains, e.g., face recognition, criminal justice, etc., disparity in prediction accuracy between different demographic subgroups has called for fundamental understanding on the source of such disparity and algorithmic intervention to mitigate it. In this paper, we study the accuracy disparity problem in regression. To begin with, we first propose an error decomposition theorem, which decomposes the accuracy disparity into the distance between marginal label distributions and the distance between conditional representations, to help explain why such accuracy disparity appears in practice. Motivated by this error decomposition and the general idea of distribution alignment with statistical distances, we then propose an algorithm to reduce this disparity, and analyze its game-theoretic optima of the proposed objective functions. To corroborate our theoretical findings, we also conduct experiments on five benchmark datasets. The experimental results suggest that our proposed algorithms can effectively mitigate accuracy disparity while maintaining the predictive power of the regression models.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "31357678",
                    "name": "Jianfeng Chi"
                },
                {
                    "authorId": "151316967",
                    "name": "Yuan Tian"
                },
                {
                    "authorId": "21889436",
                    "name": "Geoffrey J. Gordon"
                },
                {
                    "authorId": "1390716752",
                    "name": "Han Zhao"
                }
            ]
        }
    ]
}