{
    "authorId": "2258801777",
    "papers": [
        {
            "paperId": "1cc0a50089d06e6b607e5308cedaf0674edb03ee",
            "title": "Lightweight Modality Adaptation to Sequential Recommendation via Correlation Supervision",
            "abstract": "In Sequential Recommenders (SR), encoding and utilizing modalities in an end-to-end manner is costly in terms of modality encoder sizes. Two-stage approaches can mitigate such concerns, but they suffer from poor performance due to modality forgetting, where the sequential objective overshadows modality representation. We propose a lightweight knowledge distillation solution that preserves both merits: retaining modality information and maintaining high efficiency. Specifically, we introduce a novel method that enhances the learning of embeddings in SR through the supervision of modality correlations. The supervision signals are distilled from the original modality representations, including both (1) holistic correlations, which quantify their overall associations, and (2) dissected correlation types, which refine their relationship facets (honing in on specific aspects like color or shape consistency). To further address the issue of modality forgetting, we propose an asynchronous learning step, allowing the original information to be retained longer for training the representation learning module. Our approach is compatible with various backbone architectures and outperforms the top baselines by 6.8% on average. We empirically demonstrate that preserving original feature associations from modality encoders significantly boosts task-specific recommendation adaptation. Additionally, we find that larger modality encoders (e.g., Large Language Models) contain richer feature sets which necessitate more fine-grained modeling to reach their full performance potential.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258801777",
                    "name": "Hengchang Hu"
                },
                {
                    "authorId": "2244776472",
                    "name": "Qijiong Liu"
                },
                {
                    "authorId": "2240818859",
                    "name": "Chuang Li"
                },
                {
                    "authorId": "2270948852",
                    "name": "Min-Yen Kan"
                }
            ]
        },
        {
            "paperId": "4b6bcfc8ed6716deee880726eeb83a4714fd7be9",
            "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
            "abstract": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150270469",
                    "name": "Qijiong Liu"
                },
                {
                    "authorId": "2300177888",
                    "name": "Xiaoyu Dong"
                },
                {
                    "authorId": "2300433378",
                    "name": "Jiaren Xiao"
                },
                {
                    "authorId": "2257286538",
                    "name": "Nuo Chen"
                },
                {
                    "authorId": "2258801777",
                    "name": "Hengchang Hu"
                },
                {
                    "authorId": "2290237904",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2300232043",
                    "name": "Chenxu Zhu"
                },
                {
                    "authorId": "2257233277",
                    "name": "Tetsuya Sakai"
                },
                {
                    "authorId": "2265517306",
                    "name": "Xiao-Ming Wu"
                }
            ]
        },
        {
            "paperId": "53fa01ab6d73878e32940ccf9ec861b24f2ab7c8",
            "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
            "abstract": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user-item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2244776472",
                    "name": "Qijiong Liu"
                },
                {
                    "authorId": "2258801777",
                    "name": "Hengchang Hu"
                },
                {
                    "authorId": "2109164978",
                    "name": "Jiahao Wu"
                },
                {
                    "authorId": "2290237904",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2270948852",
                    "name": "Min-Yen Kan"
                },
                {
                    "authorId": "2187512110",
                    "name": "Xiao-Ming Wu"
                }
            ]
        },
        {
            "paperId": "9495cefba345dc77a84ec6411fd2e7c8434ad10a",
            "title": "Incorporating External Knowledge and Goal Guidance for LLM-based Conversational Recommender Systems",
            "abstract": "This paper aims to efficiently enable large language models (LLMs) to use external knowledge and goal guidance in conversational recommender system (CRS) tasks. Advanced LLMs (e.g., ChatGPT) are limited in domain-specific CRS tasks for 1) generating grounded responses with recommendation-oriented knowledge, or 2) proactively leading the conversations through different dialogue goals. In this work, we first analyze those limitations through a comprehensive evaluation, showing the necessity of external knowledge and goal guidance which contribute significantly to the recommendation accuracy and language quality. In light of this finding, we propose a novel ChatCRS framework to decompose the complex CRS task into several sub-tasks through the implementation of 1) a knowledge retrieval agent using a tool-augmented approach to reason over external Knowledge Bases and 2) a goal-planning agent for dialogue goal prediction. Experimental results on two multi-goal CRS datasets reveal that ChatCRS sets new state-of-the-art benchmarks, improving language quality of informativeness by 17% and proactivity by 27%, and achieving a tenfold enhancement in recommendation accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240818859",
                    "name": "Chuang Li"
                },
                {
                    "authorId": "2300030294",
                    "name": "Yang Deng"
                },
                {
                    "authorId": "2258801777",
                    "name": "Hengchang Hu"
                },
                {
                    "authorId": "2270948852",
                    "name": "Min-Yen Kan"
                },
                {
                    "authorId": "2240763421",
                    "name": "Haizhou Li"
                }
            ]
        },
        {
            "paperId": "bdf0c665aa9d3ed52fe43056900cf300755dc9c6",
            "title": "User Behavior Enriched Temporal Knowledge Graphs for Sequential Recommendation",
            "abstract": "Knowledge Graphs (KGs) enhance recommendations by providing external connectivity between items. However, there is limited research on distilling relevant knowledge in sequential recommendation, where item connections can change over time. To address this, we introduce the Temporal Knowledge Graph (TKG), which incorporates such dynamic features of user behaviors into the original KG while emphasizing sequential relationships. The TKG captures both patterns of entity dynamics (nodes) and structural dynamics (edges). Considering real-world applications with large-scale and rapidly evolving user behavior patterns, we propose an efficient two-phase framework called TKG-SRec, which strengthens Sequential Recommendation with Temporal KGs. In the first phase, we learn dynamic entity embeddings using our novel Knowledge Evolution Network (KEN) that brings together pretrained static knowledge with evolving temporal knowledge. In the second stage, downstream sequential recommender models utilize these time-specific dynamic entity embeddings with compatible neural backbones like GRUs, Transformers, and MLPs. From our extensive experiments over four datasets, TKG-SRec outperforms the current state-of-the-art by a statistically significant 5% on average. Detailed analysis validates that such filtered temporal knowledge better adapts entity embedding for sequential recommendation. In summary, TKG-SRec provides an effective and efficient approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258801777",
                    "name": "Hengchang Hu"
                },
                {
                    "authorId": "2260810851",
                    "name": "Wei Guo"
                },
                {
                    "authorId": "2293772944",
                    "name": "Xu Liu"
                },
                {
                    "authorId": "2293939763",
                    "name": "Yong Liu"
                },
                {
                    "authorId": "2284295184",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2274191373",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "2270948852",
                    "name": "Min-Yen Kan"
                }
            ]
        },
        {
            "paperId": "0790ffc8118c9ebba6a1d2b0e7f805f39faeae82",
            "title": "Leveraging Large Language Models (LLMs) to Empower Training-Free Dataset Condensation for Content-Based Recommendation",
            "abstract": "Modern techniques in Content-based Recommendation (CBR) leverage item content information to provide personalized services to users, but suffer from resource-intensive training on large datasets. To address this issue, we explore the dataset condensation for textual CBR in this paper. The goal of dataset condensation is to synthesize a small yet informative dataset, upon which models can achieve performance comparable to those trained on large datasets. While existing condensation approaches are tailored to classification tasks for continuous data like images or embeddings, direct application of them to CBR has limitations. To bridge this gap, we investigate efficient dataset condensation for content-based recommendation. Inspired by the remarkable abilities of large language models (LLMs) in text comprehension and generation, we leverage LLMs to empower the generation of textual content during condensation. To handle the interaction data involving both users and items, we devise a dual-level condensation method: content-level and user-level. At content-level, we utilize LLMs to condense all contents of an item into a new informative title. At user-level, we design a clustering-based synthesis module, where we first utilize LLMs to extract user interests. Then, the user interests and user embeddings are incorporated to condense users and generate interactions for condensed users. Notably, the condensation paradigm of this method is forward and free from iterative optimization on the synthesized dataset. Extensive empirical findings from our study, conducted on three authentic datasets, substantiate the efficacy of the proposed method. Particularly, we are able to approximate up to 97% of the original performance while reducing the dataset size by 95% (i.e., on dataset MIND).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109164978",
                    "name": "Jiahao Wu"
                },
                {
                    "authorId": "2244776472",
                    "name": "Qijiong Liu"
                },
                {
                    "authorId": "2258801777",
                    "name": "Hengchang Hu"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2152939552",
                    "name": "Shengcai Liu"
                },
                {
                    "authorId": "2254366521",
                    "name": "Qing Li"
                },
                {
                    "authorId": "2187512110",
                    "name": "Xiao-Ming Wu"
                },
                {
                    "authorId": "2253405825",
                    "name": "Ke Tang"
                }
            ]
        }
    ]
}