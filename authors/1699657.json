{
    "authorId": "1699657",
    "papers": [
        {
            "paperId": "9f1d908b8eb2cb0a676857928cf55cadfb4f9aa4",
            "title": "5th Workshop on Patent Text Mining and Semantic Technologies (PatentSemTech2024)",
            "abstract": "Information retrieval systems for the patent domain have a long history. They can support patent experts in a variety of daily tasks: from analyzing the patent landscape to support experts in the patenting process and large-scale information extraction. Advances in machine learning and natural language processing allow to further automate tasks, such as paragraph retrieval, question answering (QA) or even patent text generation. Uncovering the potential of semantic technologies for the intellectual property (IP) industry is just getting started. Investigating the use of artificial intelligence methods for the patent domain is therefore not only of academic interest, but also highly relevant for practitioners. Compared to other domains, high quality, semi-structured, annotated data is available in large volumes (a requirement for supervised machine learning models), making training large models easier. On the other hand, domain-specific challenges arise, such as very technical language or legal requirements for patent documents. With the 5th edition of this workshop we will provide a platform for researchers and industry to learn about novel and emerging technologies for semantic patent retrieval and big analytics employing sophisticated methods ranging from patent text mining, domain-specific information retrieval to large language models targeting next generation applications and use cases for the IP and related domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261751518",
                    "name": "Ralf Krestel"
                },
                {
                    "authorId": "2723448",
                    "name": "H. Aras"
                },
                {
                    "authorId": "144673977",
                    "name": "Linda Andersson"
                },
                {
                    "authorId": "3309646",
                    "name": "Florina Piroi"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                },
                {
                    "authorId": "2081565854",
                    "name": "Dean Alderucci"
                }
            ]
        },
        {
            "paperId": "4a020c1daeb52f8a9e0f80312f301b5691c4cea0",
            "title": "Outcome-based Evaluation of Systematic Review Automation",
            "abstract": "Current methods of evaluating search strategies and automated citation screening for systematic literature reviews typically rely on counting the number of relevant publications (i.e. those to be included in the review) and not relevant publications (i.e. those to be excluded). Significant importance is put into promoting the retrieval of all relevant publications through great attention to recall-oriented measures, and demoting the retrieval of non-relevant publications through precision-oriented or cost metrics. This established practice, however, does not accurately reflect the reality of conducting a systematic review, because not all included publications have the same influence on the final outcome of the systematic review. More specifically, if an important publication gets excluded or included, this might significantly change the overall review outcome, while not including or excluding less influential studies may only have a limited impact. However, in terms of evaluation measures, all inclusion and exclusion decisions are treated equally and, therefore, failing to retrieve publications with little to no impact on the review outcome leads to the same decrease in recall as failing to retrieve crucial publications. We propose a new evaluation framework that takes into account the impact of the reported study on the overall systematic review outcome. We demonstrate the framework by extracting review meta-analysis data and estimating outcome effects using predictions from ranking runs on systematic reviews of interventions from CLEF TAR 2019 shared task. We further measure how closely the obtained outcomes are to the outcomes of the original review if the arbitrary rankings were used. We evaluate 74 runs using the proposed framework and compare the results with those obtained using standard IR measures. We find that accounting for the difference in review outcomes leads to a different assessment of the quality of a system than if traditional evaluation measures were used. Our analysis provides new insights into the evaluation of retrieval results in the context of systematic review automation, emphasising the importance of assessing the usefulness of each document beyond binary relevance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50320098",
                    "name": "Wojciech Kusa"
                },
                {
                    "authorId": "1692855",
                    "name": "G. Zuccon"
                },
                {
                    "authorId": "1848764",
                    "name": "Petr Knoth"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                }
            ]
        },
        {
            "paperId": "589e2dfec431efc14785e5659937fef296a0426e",
            "title": "Statute-enhanced lexical retrieval of court cases for COLIEE 2022",
            "abstract": "We discuss our experiments for COLIEE Task 1, a court case retrieval competition using cases from the Federal Court of Canada. During experiments on the training data we observe that passage level retrieval with rank fusion outperforms document level retrieval. By explicitly adding extracted statute information to the queries and documents we can further improve the results. We submit two passage level runs to the competition, which achieve high recall but low precision.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054915166",
                    "name": "Tobias Fink"
                },
                {
                    "authorId": "3094252",
                    "name": "G\u00e1bor Recski"
                },
                {
                    "authorId": "50320098",
                    "name": "Wojciech Kusa"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                }
            ]
        },
        {
            "paperId": "5e34f0ab2724e626e2e7cbad4aacb15a5bcdf27d",
            "title": "Report on the Dagstuhl Seminar on Frontiers of Information Access Experimentation for Research and Education",
            "abstract": "This report documents the program and the outcomes of Dagstuhl Seminar 23031 \"Frontiers of Information Access Experimentation for Research and Education\", which brought together 38 participants from 12 countries. The seminar addressed technology-enhanced information access (information retrieval, recommender systems, natural language processing) and specifically focused on developing more responsible experimental practices leading to more valid results, both for research as well as for scientific education. The seminar featured a series of long and short talks delivered by participants, who helped in setting a common ground and in letting emerge topics of interest to be explored as the main output of the seminar. This led to the definition of five groups which investigated challenges, opportunities, and next steps in the following areas: reality check, i.e. conducting real-world studies, human-machine-collaborative relevance judgment frameworks, overcoming methodological challenges in information retrieval and recommender systems through awareness and education, results-blind reviewing, and guidance for authors. Date: 15--20 January 2023. Website: https://www.dagstuhl.de/23031.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052765096",
                    "name": "Christine Bauer"
                },
                {
                    "authorId": "1750995",
                    "name": "Ben Carterette"
                },
                {
                    "authorId": "2137867216",
                    "name": "Joeran Beel"
                },
                {
                    "authorId": "1491609376",
                    "name": "Timo Breuer"
                },
                {
                    "authorId": "1751287",
                    "name": "C. Clarke"
                },
                {
                    "authorId": "2815511",
                    "name": "Anita Crescenzi"
                },
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                },
                {
                    "authorId": "2051747173",
                    "name": "G. Nunzio"
                },
                {
                    "authorId": "145798497",
                    "name": "Laura Dietz"
                },
                {
                    "authorId": "80808662",
                    "name": "G. Faggioli"
                },
                {
                    "authorId": "3001795",
                    "name": "B. Ferwerda"
                },
                {
                    "authorId": "1490763899",
                    "name": "Maik Fr\u00f6be"
                },
                {
                    "authorId": "145072133",
                    "name": "Matthias Hagen"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                },
                {
                    "authorId": "2731925",
                    "name": "C. Hauff"
                },
                {
                    "authorId": "1705282",
                    "name": "D. Jannach"
                },
                {
                    "authorId": "2167781708",
                    "name": "Noriko Kando"
                },
                {
                    "authorId": "1713134",
                    "name": "E. Kanoulas"
                },
                {
                    "authorId": "2477993",
                    "name": "Bart P. Knijnenburg"
                },
                {
                    "authorId": "2993548",
                    "name": "Udo Kruschwitz"
                },
                {
                    "authorId": "1954475",
                    "name": "Maria Maistro"
                },
                {
                    "authorId": "119665711",
                    "name": "L. Michiels"
                },
                {
                    "authorId": "73425445",
                    "name": "A. Papenmeier"
                },
                {
                    "authorId": "3046200",
                    "name": "Martin Potthast"
                },
                {
                    "authorId": "143752702",
                    "name": "Paolo Rosso"
                },
                {
                    "authorId": "40404161",
                    "name": "A. Said"
                },
                {
                    "authorId": "34588911",
                    "name": "Philipp Schaer"
                },
                {
                    "authorId": "145566115",
                    "name": "C. Seifert"
                },
                {
                    "authorId": "1630446247",
                    "name": "Damiano Spina"
                },
                {
                    "authorId": "1405867539",
                    "name": "Benno Stein"
                },
                {
                    "authorId": "1803171",
                    "name": "N. Tintarev"
                },
                {
                    "authorId": "2060623050",
                    "name": "J. Urbano"
                },
                {
                    "authorId": "2626599",
                    "name": "Henning Wachsmuth"
                },
                {
                    "authorId": "1918235",
                    "name": "M. Willemsen"
                },
                {
                    "authorId": "151068958",
                    "name": "Justin W. Zobel"
                }
            ]
        },
        {
            "paperId": "6469def553f09e7f8bc3c41cac0d9c6137a704bc",
            "title": "4th Workshop on Patent Text Mining and Semantic Technologies (PatentSemTech2023)",
            "abstract": "Information retrieval systems for the patent domain have a long history. They can support patent experts in a variety of daily tasks: from analyzing the patent landscape to support experts in the patenting process and large-scale information extraction. Advances in machine learning and natural language processing allow to further automate tasks, such as paragraph retrieval or even patent text generation. Uncovering the potential of semantic technologies for the intellectual property (IP) industry is just getting started. Investigating the use of artificial intelligence methods for the patent domain is therefore not only of academic interest, but also highly relevant for practitioners. Compared to other domains, high quality, semi-structured, annotated data is available in large volumes (a requirement for supervised machine learning models), making training large models easier. On the other hand, domain-specific challenges arise, such as very technical language or legal requirements for patent documents. The focus of the 4th edition of this workshop will be on two-way communication between industry and academia from all areas of information retrieval in particular with the Asian community. We want to bring together novel research results and the latest systems and methods employed by practitioners in the field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3264110",
                    "name": "Ralf Krestel"
                },
                {
                    "authorId": "2723448",
                    "name": "H. Aras"
                },
                {
                    "authorId": "144673977",
                    "name": "Linda Andersson"
                },
                {
                    "authorId": "3309646",
                    "name": "Florina Piroi"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                },
                {
                    "authorId": "2081565854",
                    "name": "Dean Alderucci"
                }
            ]
        },
        {
            "paperId": "647cb50486f5725210dedcd44bf168ef875dce77",
            "title": "Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies are not Better than Random Selection",
            "abstract": "Search methods based on Pretrained Language Models (PLM) have demonstrated great effectiveness gains compared to statistical and early neural ranking models. However, fine-tuning PLM-based rankers requires a great amount of annotated training data. Annotating data involves a large manual effort and thus is expensive, especially in domain specific tasks. In this paper we investigate fine-tuning PLM-based rankers under limited training data and budget. We investigate two scenarios: fine-tuning a ranker from scratch, and domain adaptation starting with a ranker already fine-tuned on general data, and continuing fine-tuning on a target dataset. We observe a great variability in effectiveness when fine-tuning on different randomly selected subsets of training data. This suggests that it is possible to achieve effectiveness gains by actively selecting a subset of the training data that has the most positive effect on the rankers. This way, it would be possible to fine-tune effective PLM rankers at a reduced annotation budget. To investigate this, we adapt existing Active Learning (AL) strategies to the task of fine-tuning PLM rankers and investigate their effectiveness, also considering annotation and computational costs. Our extensive analysis shows that AL strategies do not significantly outperform random selection of training subsets in terms of effectiveness. We further find that gains provided by AL strategies come at the expense of more assessments (thus higher annotation costs) and AL strategies underperform random selection when comparing effectiveness given a fixed annotation cost. Our results highlight that \u201coptimal\u201d subsets of training data that provide high effectiveness at low annotation cost do exist, but current mainstream AL strategies applied to PLM rankers are not capable of identifying them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1620824465",
                    "name": "Sophia Althammer"
                },
                {
                    "authorId": "1692855",
                    "name": "G. Zuccon"
                },
                {
                    "authorId": "1689405303",
                    "name": "Sebastian Hofstatter"
                },
                {
                    "authorId": "2239104085",
                    "name": "Suzan Verberne"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                }
            ]
        },
        {
            "paperId": "906c2b9544c7cba662f8ec5e0a8261103ae7f7c8",
            "title": "CRUISE-Screening: Living Literature Reviews Toolbox",
            "abstract": "Keeping up with research and finding related work is still a time-consuming task for academics. Researchers sift through thousands of studies to identify a few relevant ones. Automation techniques can help by increasing the efficiency and effectiveness of this task. To this end, we developed CRUISE-Screening, a web-based application for conducting living literature reviews -- a type of literature review that is continuously updated to reflect the latest research in a particular field. CRUISE-Screening is connected to several search engines via an API, which allows for updating the search results periodically. Moreover, it can facilitate the process of screening for relevant publications by using text classification and question answering models. CRUISE-Screening can be used both by researchers conducting literature reviews and by those working on automating the citation screening process to validate their algorithms. The application is open-source, and a demo is available under this URL: https://citation-screening.ec.tuwien.ac.at.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50320098",
                    "name": "Wojciech Kusa"
                },
                {
                    "authorId": "2237802588",
                    "name": "Petr Knoth"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                }
            ]
        },
        {
            "paperId": "a1dcb31a5100a6aa98bfe94b719bfca767c01069",
            "title": "VoMBaT: A Tool for Visualising Evaluation Measure Behaviour in High-Recall Search Tasks",
            "abstract": "The objective of High-Recall Information Retrieval (HRIR) is to retrieve as many relevant documents as possible for a given search topic. One approach to HRIR is Technology-Assisted Review (TAR), which uses information retrieval and machine learning techniques to aid the review of large document collections. TAR systems are commonly used in legal eDiscovery and systematic literature reviews. Successful TAR systems are able to find the majority of relevant documents using the least number of assessments. Commonly used retrospective evaluation assumes that the system achieves a specific, fixed recall level first, and then measures the precision or work saved (e.g., precision at r% recall). This approach can cause problems related to understanding the behaviour of evaluation measures in a fixed recall setting. It is also problematic when estimating time and money savings during technology-assisted reviews. This paper presents a new visual analytics tool to explore the dynamics of evaluation measures depending on recall level. We implemented 18 evaluation measures based on the confusion matrix terms, both from general IR tasks and specific to TAR. The tool allows for a comparison of the behaviour of these measures in a fixed recall evaluation setting. It can also simulate savings in time and money and a count of manual vs automatic assessments for different datasets depending on the model quality. The tool is open-source, and the demo is available under the following URL: https://vombat.streamlit.app.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50320098",
                    "name": "Wojciech Kusa"
                },
                {
                    "authorId": "2293941214",
                    "name": "Aldo Lipani"
                },
                {
                    "authorId": "1848764",
                    "name": "Petr Knoth"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                }
            ]
        },
        {
            "paperId": "ffcca4725b4a6cf15bd7977a28b9eff31a833da8",
            "title": "HEVS-TUW at SemEval-2023 Task 8: Ensemble of Language Models and Rule-based Classifiers for Claims Identification and PICO Extraction",
            "abstract": "This paper describes the HEVS-TUW team submission to the SemEval-2023 Task 8: Causal Claims. We participated in two subtasks: (1) causal claims detection and (2) PIO identification. For subtask 1, we experimented with an ensemble of weakly supervised question detection and fine-tuned Transformer-based models. For subtask 2 of PIO frame extraction, we used a combination of deep representation learning and a rule-based approach. Our best model for subtask 1 ranks fourth with an F1-score of 65.77%. It shows moderate benefit from ensembling models pre-trained on independent categories. The results for subtask 2 warrant further investigation for improvement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1402469520",
                    "name": "Anjani Dhrangadhariya"
                },
                {
                    "authorId": "50320098",
                    "name": "Wojciech Kusa"
                },
                {
                    "authorId": "2216187671",
                    "name": "Henning M\u00fcller"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                }
            ]
        },
        {
            "paperId": "174d3defdd34e9ca41e6efbb39b2b5f3b9f72a06",
            "title": "Report on the 1st Training School on Domain Specific Systems for Information Extraction and Retrieval (DoSSIER 2022)",
            "abstract": "The DoSSIER project, an European Training Network, was kicked-off in late 2019. The first PhD candidate recruiting activities took place at the beginning of the Covid-19 pandemic and the first years of the project lived in the on-line universe. Naturally, this posed certain challenges to the cohesion of our network. With some delay due to the lock-downs and travel restrictions, our first Training School was successfully organized in September 2022 and it constituted one of the few occasions we had where the members of DoSSIER network met in person. This report gives an account on the first Summer School organized by DoSSIER, for the DoSSIER Early Stage Researchers, a school set in the picturesque village of Olympiada, which gave amply opportunity of interaction between students and senior researchers in the Information Retrieval domain. Date: 25--30 September, 2022. Website: https://dossier-project.eu/1st-dossier-training-school.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1786622",
                    "name": "M. Salampasis"
                },
                {
                    "authorId": "3309646",
                    "name": "Florina Piroi"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                }
            ]
        }
    ]
}