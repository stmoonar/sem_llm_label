{
    "authorId": "38705864",
    "papers": [
        {
            "paperId": "600f9864794d79b9801e3ebef90efb410776f606",
            "title": "Prompting Vision-Language Models For Aspect-Controlled Generation of Referring Expressions",
            "abstract": "Referring Expression Generation (REG) is the task of generating a description that unambiguously identifies a given target in the scene. Different from Image Captioning (IC), REG requires learning fine-grained characteristics of not only the scene objects but also their surrounding context. Referring expressions are usually not singular; an object can often be uniquely referenced in numerous ways, for instance, by color, by location, or by relationship with other objects. Most prior works, however, have not explored this \u2018aspect-based mul-tiplicity\u2019 of referring expressions. Hence, in this work, we focus on the Aspect-Controlled REG task, which requires generating a referring expression conditioned on the input aspect(s), where an aspect captures a style of reference. By changing the input aspect such as color, location, action etc., one can generate multiple distinct expressions per target region. To solve this new task, we first modify BLIP (Li et al., 2022a) for aligning image-regions and text-expressions. We achieve this through a novel approach for feeding the input by drawing a bounding box around the target image-region and prompting the model to generate the referring expression. Our base REG model already beats all prior works in CIDEr score. To tackle Aspect-Controlled REG, we append \u2018aspect tokens\u2019 to the prompt and show that distinct expressions can be generated by just changing the prompt. Finally, to prove the high-quality and diversity of the data generated by our proposed aspect-controlled REG model, we also perform data-augmentation-based evaluation",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2308643996",
                    "name": "Danfeng Guo"
                },
                {
                    "authorId": "2308134588",
                    "name": "Sanchit Agarwal"
                },
                {
                    "authorId": "2258670815",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "2308127367",
                    "name": "Emre Barut"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "2217929400",
                    "name": "Jing Huang"
                },
                {
                    "authorId": "2300284478",
                    "name": "Mohit Bansal"
                }
            ]
        },
        {
            "paperId": "e65f6420f0876d4681dc11a0163e12a8a236dc5a",
            "title": "Parameter-Efficient Low-Resource Dialogue State Tracking by Prompt Tuning",
            "abstract": "Dialogue state tracking (DST) is an important step in dialogue management to keep track of users' beliefs. Existing works fine-tune all language model (LM) parameters to tackle the DST task, which requires significant data and computing resources for training and hosting. The cost grows exponentially in the real-world deployment where dozens of fine-tuned LM are used for different domains and tasks. To reduce parameter size and better utilize cross-task shared information, we propose to use soft prompt token embeddings to learn task properties. Without tuning LM parameters, our method drastically reduces the number of parameters needed to less than 0.5% of prior works while achieves better low-resource DST performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "f925109f1c7591edbcd85f0df41d417bcecfc426",
            "title": "Mitigating Bias for Question Answering Models by Tracking Bias Influence",
            "abstract": "Models of various NLP tasks have been shown to exhibit stereotypes, and the bias in the question answering (QA) models is especially harmful as the output answers might be directly consumed by the end users. There have been datasets to evaluate bias in QA models, while bias mitigation technique for the QA models is still under-explored. In this work, we propose BMBI, an approach to mitigate the bias of multiple-choice QA models. Based on the intuition that a model would lean to be more biased if it learns from a biased example, we measure the bias level of a query instance by observing its influence on another instance. If the influenced instance is more biased, we derive that the query instance is biased. We then use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original QA task. We further introduce a new bias evaluation metric to quantify bias in a comprehensive and sensitive way. We show that our method could be applied to multiple QA formulations across multiple bias categories. It can significantly reduce the bias level in all 9 bias categories in the BBQ dataset while maintaining comparable QA accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "2258670815",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "2258671169",
                    "name": "Yu-Hsiang Lin"
                },
                {
                    "authorId": "2258697911",
                    "name": "Wenbo Zhao"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "2256611613",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "2257127887",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2256996328",
                    "name": "Nanyun Peng"
                }
            ]
        },
        {
            "paperId": "7e5712806ec37a554914dc8dae19fe57b9645763",
            "title": "GRAVL-BERT: Graphical Visual-Linguistic Representations for Multimodal Coreference Resolution",
            "abstract": "Learning from multimodal data has become a popular research topic in recent years. Multimodal coreference resolution (MCR) is an important task in this area. MCR involves resolving the references across different modalities, e.g., text and images, which is a crucial capability for building next-generation conversational agents. MCR is challenging as it requires encoding information from different modalities and modeling associations between them. Although significant progress has been made for visual-linguistic tasks such as visual grounding, most of the current works involve single turn utterances and focus on simple coreference resolutions. In this work, we propose an MCR model that resolves coreferences made in multi-turn dialogues with scene images. We present GRAVL-BERT, a unified MCR framework which combines visual relationships between objects, background scenes, dialogue, and metadata by integrating Graph Neural Networks with VL-BERT. We present results on the SIMMC 2.0 multimodal conversational dataset, achieving the rank-1 on the DSTC-10 SIMMC 2.0 MCR challenge with F1 score 0.783. Our code is available at https://github.com/alexa/gravl-bert.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068107537",
                    "name": "Danfeng Guo"
                },
                {
                    "authorId": "32174480",
                    "name": "Arpit Gupta"
                },
                {
                    "authorId": "150290293",
                    "name": "Sanchit Agarwal"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "2052990580",
                    "name": "Arijit Biswas"
                },
                {
                    "authorId": "2116675467",
                    "name": "Chien-Wei Lin"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                }
            ]
        },
        {
            "paperId": "2c498b6504ca7f5fb156a994aa2e68a51156237f",
            "title": "Alexa Conversations: An Extensible Data-driven Approach for Building Task-oriented Dialogue Systems",
            "abstract": "Traditional goal-oriented dialogue systems rely on various components such as natural language understanding, dialogue state tracking, policy learning and response generation. Training each component requires annotations which are hard to obtain for every new domain, limiting scalability of such systems. Similarly, rule-based dialogue systems require extensive writing and maintenance of rules and do not scale either. End-to-End dialogue systems, on the other hand, do not require module-specific annotations but need a large amount of data for training. To overcome these problems, in this demo, we present Alexa Conversations, a new approach for building goal-oriented dialogue systems that is scalable, extensible as well as data efficient. The components of this system are trained in a data-driven manner, but instead of collecting annotated conversations for training, we generate them using a novel dialogue simulator based on a few seed dialogues and specifications of APIs and entities provided by the developer. Our approach provides out-of-the-box support for natural conversational phenomenon like entity sharing across turns or users changing their mind during conversation without requiring developers to provide any such dialogue flows. We exemplify our approach using a simple pizza ordering task and showcase its value in reducing the developer burden for creating a robust experience. Finally, we evaluate our system using a typical movie ticket booking task integrated with live APIs and show that the dialogue simulator is an essential component of the system that leads to over 50% improvement in turn-level action signature prediction accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40026200",
                    "name": "Anish Acharya"
                },
                {
                    "authorId": "2025630077",
                    "name": "Suranjit Adhikari"
                },
                {
                    "authorId": "150290293",
                    "name": "Sanchit Agarwal"
                },
                {
                    "authorId": "2131423",
                    "name": "Vincent Auvray"
                },
                {
                    "authorId": "2025627813",
                    "name": "Nehal Belgamwar"
                },
                {
                    "authorId": "2052990580",
                    "name": "Arijit Biswas"
                },
                {
                    "authorId": "2059656807",
                    "name": "Shubhra Chandra"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "1399159921",
                    "name": "Maryam Fazel-Zarandi"
                },
                {
                    "authorId": "39303368",
                    "name": "Raefer Gabriel"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "2061600973",
                    "name": "Rahul Goel"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "2078502127",
                    "name": "Jan Jezabek"
                },
                {
                    "authorId": "1879594",
                    "name": "Abhay Jha"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "2068122300",
                    "name": "Prakash Krishnan"
                },
                {
                    "authorId": "2052970003",
                    "name": "Peter Ku"
                },
                {
                    "authorId": "2057019",
                    "name": "Anuj Goyal"
                },
                {
                    "authorId": "2116675467",
                    "name": "Chien-Wei Lin"
                },
                {
                    "authorId": "2116621357",
                    "name": "Qing Liu"
                },
                {
                    "authorId": "33638380",
                    "name": "Arindam Mandal"
                },
                {
                    "authorId": "47851995",
                    "name": "A. Metallinou"
                },
                {
                    "authorId": "41035810",
                    "name": "V. Naik"
                },
                {
                    "authorId": "2115430213",
                    "name": "Yi Pan"
                },
                {
                    "authorId": "31593543",
                    "name": "Shachi Paul"
                },
                {
                    "authorId": "38686805",
                    "name": "Vittorio Perera"
                },
                {
                    "authorId": "14214710",
                    "name": "Abhishek Sethi"
                },
                {
                    "authorId": "98920799",
                    "name": "Minmin Shen"
                },
                {
                    "authorId": "1805735",
                    "name": "N. Strom"
                },
                {
                    "authorId": "2087051541",
                    "name": "Eddie Wang"
                }
            ]
        },
        {
            "paperId": "cb1258f82560b67dfb9d1c7c41862f44f8d7bd94",
            "title": "Style Control for Schema-Guided Natural Language Generation",
            "abstract": "Natural Language Generation (NLG) for task-oriented dialogue systems focuses on communicating specific content accurately, fluently, and coherently. While these attributes are crucial for a successful dialogue, it is also desirable to simultaneously accomplish specific stylistic goals, such as response length, point-of-view, descriptiveness, sentiment, formality, and empathy. In this work, we focus on stylistic control and evaluation for schema-guided NLG, with joint goals of achieving both semantic and stylistic control. We experiment in detail with various controlled generation methods for large pretrained language models: specifically, conditional training, guided fine-tuning, and guided decoding. We discuss their advantages and limitations, and evaluate them with a broad range of automatic and human evaluation metrics. Our results show that while high style accuracy and semantic correctness are easier to achieve for more lexically-defined styles with conditional training, stylistic control is also achievable for more semantically complex styles using discriminator-based guided decoding methods. The results also suggest that methods that are more scalable (with less hyper-parameters tuning) and that disentangle context generation and stylistic variations are more effective at achieving semantic correctness and style accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1805974027",
                    "name": "Alicia Y. Tsai"
                },
                {
                    "authorId": "3114640",
                    "name": "Shereen Oraby"
                },
                {
                    "authorId": "38686805",
                    "name": "Vittorio Perera"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "2491862",
                    "name": "Yuheng Du"
                },
                {
                    "authorId": "35526556",
                    "name": "Anjali Narayan-Chen"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "330729fc50df37fb4a1ec560edd3008b4738a6b1",
            "title": "MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension",
            "abstract": "Machine Reading Comprehension (MRC) for question answering (QA), which aims to answer a question given the relevant context passages, is an important way to test the ability of intelligence systems to understand human language. Multiple-Choice QA (MCQA) is one of the most difficult tasks in MRC because it often requires more advanced reading comprehension skills such as logical reasoning, summarization, and arithmetic operations, compared to the extractive counterpart where answers are usually spans of text within given passages. Moreover, most existing MCQA datasets are small in size, making the task even harder. We introduce MMM, a Multi-stage Multi-task learning framework for Multi-choice reading comprehension. Our method involves two sequential stages: coarse-tuning stage using out-of-domain datasets and multi-task learning stage using a larger in-domain dataset to help model generalize better with limited data. Furthermore, we propose a novel multi-step attention network (MAN) as the top-level classifier for this task. We demonstrate MMM significantly advances the state-of-the-art on four representative MCQA datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068347799",
                    "name": "Di Jin"
                },
                {
                    "authorId": "3092863",
                    "name": "Shuyang Gao"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "2878984",
                    "name": "Tagyoung Chung"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "84c69159f4ef9f78f84f6758e554a299f80b398f",
            "title": "Graph Based Skeleton Modeling for Human Activity Analysis",
            "abstract": "Understanding human activity based on sensor information is required in many applications and has been an active research area. With the advancement of depth sensors and tracking algorithms, systems for human motion activity analysis can be built by combining off-the-shelf motion tracking systems with application-dependent learning tools to extract higher semantic level information. Many of these motion tracking systems provide raw motion data registered to the skeletal joints in the human body. In this paper, we propose novel representations for human motion data using the skeleton-based graph structure along with techniques in graph signal processing. Methods for graph construction and their corresponding basis functions are discussed. The proposed representations can achieve comparable classification performance in action recognition tasks while additionally being more robust to noise and missing data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "145029825",
                    "name": "Antonio Ortega"
                },
                {
                    "authorId": "144309297",
                    "name": "Dong Tian"
                },
                {
                    "authorId": "145730148",
                    "name": "H. Mansour"
                },
                {
                    "authorId": "1690385",
                    "name": "A. Vetro"
                }
            ]
        },
        {
            "paperId": "f185a759f00878f7fa1f945ef3482c413f6a8e0c",
            "title": "Hand Graph Representations for Unsupervised Segmentation of Complex Activities",
            "abstract": "Analysis of hand skeleton data can be used to understand patterns in manipulation and assembly tasks. This paper introduces a graph-based representation of hand skeleton data and proposes a method to perform unsupervised temporal segmentation of a sequence of sub-tasks in order to evaluate the efficiency of an assembly task. We explore the properties of different choices of hand graphs and their spectral decomposition. A comparative performance of these graphs is presented in the context of complex activity segmentation. We show that the spectral graph features extracted from 2D hand motion data outperform the direct use of motion vectors as features. We also make the collected hand position data available to the research community to facilitate further development in this direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46992288",
                    "name": "Pratyusha Das"
                },
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "145029825",
                    "name": "Antonio Ortega"
                },
                {
                    "authorId": "3329357",
                    "name": "Tomoya Sawada"
                },
                {
                    "authorId": "145730148",
                    "name": "H. Mansour"
                },
                {
                    "authorId": "1690385",
                    "name": "A. Vetro"
                },
                {
                    "authorId": "2844732",
                    "name": "Akira Minezawa"
                }
            ]
        },
        {
            "paperId": "e211e5539899773430309d83151dcb8944616132",
            "title": "Disc-GLasso: Discriminative graph learning with sparsity regularization",
            "abstract": "Learning graph topology from data is challenging. Previous work leads to learning graphs on which the graph signals used for training are smooth. In this paper, we propose an optimization framework for learning multiple graphs, each associated to a class of signals, such that representation of signals within a class and discrimination of signals in different classes are both taken into consideration. A Fisher-LDA-like term is included in the optimization objective function in addition to the conventional Gaussian ML objective. A block coordinate descent algorithm is then developed to estimate optimal graphs for different categories of signals, which are then used to efficiently classify the different signals. Experiments on synthetic data demonstrate that our proposed method can achieve better discrimination between the learned graphs, leading to improvements in subsequent classification tasks.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38705864",
                    "name": "Jiun-Yu Kao"
                },
                {
                    "authorId": "144309297",
                    "name": "Dong Tian"
                },
                {
                    "authorId": "145730148",
                    "name": "H. Mansour"
                },
                {
                    "authorId": "145029825",
                    "name": "Antonio Ortega"
                },
                {
                    "authorId": "1690385",
                    "name": "A. Vetro"
                }
            ]
        }
    ]
}