{
    "authorId": "1774783",
    "papers": [
        {
            "paperId": "5cd8f883f18a2079dee8d193cf9d14221bb7ac31",
            "title": "IW-NET BDA: A Big Data Infrastructure for Predictive and Geotemporal Analytics of Inland Waterways",
            "abstract": "The recent shift towards digitalization in traditional sectors like logistics and transportation has unlocked new avenues for gaining valuable insights and streamlining operations. This transformation is facilitated by the abundance and specificity of data now available, including fleet IoT data, transactional documents, and event notifications. These businesses leave a substantial digital footprint, ripe for analysis when combined with external data sources. However, harnessing this information requires robust computing infrastructure and adaptable software capable of handling vast amounts of data. In this paper, we introduce IW-NET BDA, a big-data analytics framework built on open-source technologies to address the storage and processing demands of massive datasets from various origins. Developed within the framework of the EU-funded research and innovation project IW-NET (Innovation driven Collaborative European Inland Waterways Transport Network), our system caters to the logistics domain but offers a versatile IT service backbone due to its agnostic design, focusing on infrastructure-as-a-service provision. Furthermore, it allows for the development and deployment of applications that encapsulate business logic, thus tailored to specific business needs. In the subsequent sections, we delve into the design principles, architectural components, and deployment possibilities of IW-NET BDA. Additionally, we present two illustrative use cases: firstly, the automated detection of areas of interest and vessel activity tracking for insightful geo-temporal data analytics along the River Weser corridor; secondly, the utilization of recurrent neural networks to forecast water levels in the Danube River corridor. These examples highlight the adaptability and efficacy of IW-NET BDA in tackling diverse challenges across different contexts, underscoring its versatility and utility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17843726",
                    "name": "Nikolaos Chalvantzis"
                },
                {
                    "authorId": "2296111987",
                    "name": "Aristotelis Vontzalidis"
                },
                {
                    "authorId": "1505809278",
                    "name": "Evdokia Kassela"
                },
                {
                    "authorId": "2296107905",
                    "name": "Aris Spyrou"
                },
                {
                    "authorId": "2296112384",
                    "name": "Nikolaos Nikitas"
                },
                {
                    "authorId": "1505758575",
                    "name": "Nikodimos Provatas"
                },
                {
                    "authorId": "1505709769",
                    "name": "I. Konstantinou"
                },
                {
                    "authorId": "1774783",
                    "name": "N. Koziris"
                }
            ]
        },
        {
            "paperId": "7029330cbcf472ce5901d65823b46c9026f3d1a1",
            "title": "A Decision Support System for Automated Configuration of Cloud Native ML Pipelines",
            "abstract": "Big Data systems like Apache Spark and Hadoop are cornerstones of large scale data processing. However, they are being utilized only on one or some steps of a larger data processing pipeline. Complex data pipelines that involve different datasets, infrastructures and programming libraries are simplified with the use of cloud-native tools like Kubernetes and KubeFlow, that model dependencies and manage the execution life cycle. Nevertheless, there are no complete solutions that are fully interoperable with Apache Spark and Kubeflow in an on-premises setup. In this work, we extend the Kubeflow Pipelines tool to support on-premises Apache Spark clusters. We experimentally evaluate our tool on an industry standard Big Data benchmark with various infrastructure and dataset configurations. We utilize the collected knowledge regarding Spark\u2019s performance to train a Decision Tree-based ML system that can detect the optimal cluster configuration according to user constraints and predict query execution time. The system can be used by non-experts through a comprehensive GUI. We finally provide open-source implementations of both Apache Spark\u2019s Kubeflow integration and the Decision Support System.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2296107905",
                    "name": "Aris Spyrou"
                },
                {
                    "authorId": "1505709769",
                    "name": "I. Konstantinou"
                },
                {
                    "authorId": "1774783",
                    "name": "N. Koziris"
                }
            ]
        },
        {
            "paperId": "aa6db7fd62f1e63d4abc64190f9838a8e610c36e",
            "title": "Architectural Support for Sharing, Isolating and Virtualizing FPGA Resources",
            "abstract": "FPGAs are increasingly popular in cloud environments for their ability to offer on-demand acceleration and improved compute efficiency. Providers would like to increase utilization, by multiplexing customers on a single device, similar to how processing cores and memory are shared. Nonetheless, multi-tenancy still faces major architectural limitations including: (a) inefficient sharing of memory interfaces across hardware tasks (HT) exacerbated by technological limitations and peculiarities, (b) insufficient solutions for performance and data isolation and high quality of service, and (c) absent or simplistic allocation strategies to effectively distribute external FPGA memory across HT. This article presents a full-stack solution for enabling multi-tenancy on FPGAs. Specifically, our work proposes an intra-fpga virtualization layer to share FPGA interfaces and its resources across tenants. To achieve efficient inter-connectivity between virtual FPGAs (vFGPAs) and external interfaces, we employ a compact network-on-chip architecture to optimize resource utilization. Dedicated memory management units implement the concept of virtual memory in FPGAs, providing mechanisms to isolate the address space and enable memory protection. We also introduce a memory segmentation scheme to effectively allocate FPGA address space and enhance isolation through hardware-software support, while preserving the efficacy of memory transactions. We assess our solution on an Alveo U250 Data Center FPGA Card, employing 10 real-world benchmarks from the Rodinia and Rosetta suites. Our framework preserves the performance of HT from a non-virtualized environment, while enhancing the device aggregate throughput through resource sharing; up to 3.96x in isolated and up to 2.31x in highly congested settings, where an external interface is shared across four vFPGAs. Finally, our work ensures high-quality of service, with HT achieving up to 0.95x of their native performance, even when resource sharing introduces interference from other accelerators.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2120212029",
                    "name": "Panagiotis Miliadis"
                },
                {
                    "authorId": "5982914",
                    "name": "D. Theodoropoulos"
                },
                {
                    "authorId": "1799344",
                    "name": "D. Pnevmatikatos"
                },
                {
                    "authorId": "1774783",
                    "name": "N. Koziris"
                }
            ]
        },
        {
            "paperId": "d506faedcf30dc47b25d715bf1c42f6bb9bf2f87",
            "title": "SmartPQ: An Adaptive Concurrent Priority Queue for NUMA Architectures",
            "abstract": "Concurrent priority queues are widely used in important workloads, such as graph applications and discrete event simulations. However, designing scalable concurrent priority queues for NUMA architectures is challenging. Even though several NUMA-oblivious implementations can scale up to a high number of threads, exploiting the potential parallelism of insert operation, NUMA-oblivious implementations scale poorly in deleteMin-dominated workloads. This is because all threads compete for accessing the same memory locations, i.e., the highest-priority element of the queue, thus incurring excessive cache coherence traffic and non-uniform memory accesses between nodes of a NUMA system. In such scenarios, NUMA-aware implementations are typically used to improve system performance on a NUMA system. In this work, we propose an adaptive priority queue, called SmartPQ. SmartPQ tunes itself by switching between a NUMA-oblivious and a NUMA-aware algorithmic mode to achieve high performance under all various contention scenarios. SmartPQ has two key components. First, it is built on top of NUMA Node Delegation (Nuddle), a generic low-overhead technique to construct efficient NUMA-aware data structures using any arbitrary concurrent NUMA-oblivious implementation as its backbone. Second, SmartPQ integrates a lightweight decision making mechanism to decide when to switch between NUMA-oblivious and NUMA-aware algorithmic modes. Our evaluation shows that, in NUMA systems, SmartPQ performs best in all various contention scenarios with 87.9% success rate, and dynamically adapts between NUMA-aware and NUMA-oblivious algorithmic mode, with negligible performance overheads. SmartPQ improves performance by 1.87x on average over SprayList, the state-of-theart NUMA-oblivious priority queue.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46175739",
                    "name": "Christina Giannoula"
                },
                {
                    "authorId": "119277247",
                    "name": "F. Strati"
                },
                {
                    "authorId": "27832849",
                    "name": "Dimitrios Siakavaras"
                },
                {
                    "authorId": "2825685",
                    "name": "G. Goumas"
                },
                {
                    "authorId": "1774783",
                    "name": "N. Koziris"
                }
            ]
        },
        {
            "paperId": "15b3523eecb9a4035324662bbb77d7d8b7185d5b",
            "title": "DaeMon: Architectural Support for Efficient Data Movement in Fully Disaggregated Systems",
            "abstract": "Resource disaggregation offers a cost effective solution to resource scaling, utilization, and failure-handling in data centers by physically separating hardware devices in a server. Servers are architected as pools of processor, memory, and storage devices, organized as independent failure-isolated components interconnected by a high-bandwidth network. A critical challenge, however, is the high performance penalty of accessing data from a remote memory module over the network. Addressing this challenge is difficult as disaggregated systems have high runtime variability in network latencies/bandwidth, and page migration can significantly delay critical path cache line accesses in other pages. This paper conducts a characterization analysis on different data movement strategies in fully disaggregated systems, evaluates their performance overheads in a variety of workloads, and introduces DaeMon, the first software-transparent mechanism to significantly alleviate data movement overheads in fully disaggregated systems. First, to enable scalability to multiple hardware components in the system, we enhance each compute and memory unit with specialized engines that transparently handle data migrations. Second, to achieve high performance and provide robustness across various network, architecture and application characteristics, we implement a synergistic approach of bandwidth partitioning, link compression, decoupled data movement of multiple granularities, and adaptive granularity selection in data movements. We evaluate DaeMon in a wide variety of workloads at different network and architecture configurations using a state-of-the-art simulator. DaeMon improves system performance and data access costs by 2.39\u00d7 and 3.06\u00d7, respectively, over the widely-adopted approach of moving data at page granularity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46175739",
                    "name": "Christina Giannoula"
                },
                {
                    "authorId": "2199125482",
                    "name": "Kailong Huang"
                },
                {
                    "authorId": "2112441580",
                    "name": "Jonathan Tang"
                },
                {
                    "authorId": "1774783",
                    "name": "N. Koziris"
                },
                {
                    "authorId": "2825685",
                    "name": "G. Goumas"
                },
                {
                    "authorId": "1696426",
                    "name": "Zeshan A. Chishti"
                },
                {
                    "authorId": "1920997",
                    "name": "Nandita Vijaykumar"
                }
            ]
        },
        {
            "paperId": "1638973b01f1819b666a2d47385a0b3530d638a5",
            "title": "PARALiA: A Performance Aware Runtime for Auto-tuning Linear Algebra on Heterogeneous Systems",
            "abstract": "Dense linear algebra operations appear very frequently in high-performance computing (HPC) applications, rendering their performance crucial to achieve optimal scalability. As many modern HPC clusters contain multi-GPU nodes, BLAS operations are frequently offloaded on GPUs, necessitating the use of optimized libraries to ensure good performance. Unfortunately, multi-GPU systems are accompanied by two significant optimization challenges: data transfer bottlenecks as well as problem splitting and scheduling in multiple workers (GPUs) with distinct memories. We demonstrate that the current multi-GPU BLAS methods for tackling these challenges target very specific problem and data characteristics, resulting in serious performance degradation for any slightly deviating workload. Additionally, an even more critical decision is omitted because it cannot be addressed using current scheduler-based approaches: the determination of which devices should be used for a certain routine invocation. To address these issues we propose a model-based approach: using performance estimation to provide problem-specific autotuning during runtime. We integrate this autotuning into an end-to-end BLAS framework named PARALiA. This framework couples autotuning with an optimized task scheduler, leading to near-optimal data distribution and performance-aware resource utilization. We evaluate PARALiA in an HPC testbed with 8 NVIDIA-V100 GPUs, improving the average performance of GEMM by 1.7\u00d7 and energy efficiency by 2.5\u00d7 over the state-of-the-art in a large and diverse dataset and demonstrating the adaptability of our performance-aware approach to future heterogeneous systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2086803781",
                    "name": "Petros Anastasiadis"
                },
                {
                    "authorId": "40395087",
                    "name": "Nikela Papadopoulou"
                },
                {
                    "authorId": "2825685",
                    "name": "G. Goumas"
                },
                {
                    "authorId": "1774783",
                    "name": "N. Koziris"
                },
                {
                    "authorId": "2240551318",
                    "name": "Dennis Hoppe"
                },
                {
                    "authorId": "2240537898",
                    "name": "Li Zhong"
                }
            ]
        },
        {
            "paperId": "34f981e0d165f2e3bf43146f1cd18467b80fd7f4",
            "title": "Architectural Support for Efficient Data Movement in Disaggregated Systems",
            "abstract": "Resource disaggregation offers a cost effective solution to resource scaling, utilization, and failure-handling in data centers by physically separating hardware devices in a server. Servers are architected as pools of processor, memory, and storage devices, organized as independent failure-isolated components interconnected by a high-bandwidth network. A critical challenge, however, is the high performance penalty of accessing data from a remote memory module over the network. Addressing this challenge is difficult as disaggregated systems have high runtime variability in network latencies/bandwidth, and page migration can significantly delay critical path cache line accesses in other pages. This paper introduces DaeMon, the first software-transparent and robust mechanism to significantly alleviate data movement overheads in fully disaggregated systems. First, to enable scalability to multiple hardware components in the system, we enhance each compute and memory unit with specialized engines that transparently handle data migrations. Second, to achieve high performance and provide robustness across various network, architecture and application characteristics, we implement a synergistic approach of bandwidth partitioning, link compression, decoupled data movement of multiple granularities, and adaptive granularity selection in data movements. We evaluate DaeMon in a wide variety of workloads at different network and architecture configurations using a state-of-the-art accurate simulator and demonstrate that DaeMon significantly improves system performance and data access costs over the widely-adopted approach of moving data at page granularity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46175739",
                    "name": "Christina Giannoula"
                },
                {
                    "authorId": "2199125482",
                    "name": "Kailong Huang"
                },
                {
                    "authorId": "2112441580",
                    "name": "Jonathan Tang"
                },
                {
                    "authorId": "1774783",
                    "name": "N. Koziris"
                },
                {
                    "authorId": "2825685",
                    "name": "G. Goumas"
                },
                {
                    "authorId": "1696426",
                    "name": "Zeshan A. Chishti"
                },
                {
                    "authorId": "1920997",
                    "name": "Nandita Vijaykumar"
                }
            ]
        },
        {
            "paperId": "36451df837c0c28c7344501cd3a6c14d63b08b9e",
            "title": "DaeMon: Architectural Support for Efficient Data Movement in Disaggregated Systems",
            "abstract": "Resource disaggregation offers a cost effective solution to resource scaling, utilization, and failure-handling in data centers by physically separating hardware devices in a server. Servers are architected as pools of processor, memory, and storage devices, organized as independent failure-isolated components interconnected by a high-bandwidth network. A critical challenge, however, is the high performance penalty of accessing data from a remote memory module over the network. Addressing this challenge is difficult as disaggregated systems have high runtime variability in network latencies/bandwidth, and page migration can significantly delay critical path cache line accesses in other pages. This paper conducts a characterization analysis on different data movement strategies in fully disaggregated systems, evaluates their performance overheads in a variety of workloads, and introduces DaeMon, the first software-transparent mechanism to significantly alleviate data movement overheads in fully disaggregated systems. First, to enable scalability to multiple hardware components in the system, we enhance each compute and memory unit with specialized engines that transparently handle data migrations. Second, to achieve high performance and provide robustness across various network, architecture and application characteristics, we implement a synergistic approach of bandwidth partitioning, link compression, decoupled data movement of multiple granularities, and adaptive granularity selection in data movements. We evaluate DaeMon in a wide variety of workloads at different network and architecture configurations using a state-of-the-art accurate simulator. DaeMon improves system performance and data access costs by 2.39$\\times$ and 3.06$\\times$, respectively, over the widely-adopted approach of moving data at page granularity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46175739",
                    "name": "Christina Giannoula"
                },
                {
                    "authorId": "2199125482",
                    "name": "Kailong Huang"
                },
                {
                    "authorId": "2112441580",
                    "name": "Jonathan Tang"
                },
                {
                    "authorId": "1774783",
                    "name": "N. Koziris"
                },
                {
                    "authorId": "2825685",
                    "name": "G. Goumas"
                },
                {
                    "authorId": "1696426",
                    "name": "Zeshan A. Chishti"
                },
                {
                    "authorId": "1920997",
                    "name": "Nandita Vijaykumar"
                }
            ]
        },
        {
            "paperId": "6504ac119f9ec23c80eb1769f9f0c60683943988",
            "title": "FaaSCell: A Case for Intra-node Resource Management: Work-In-Progress",
            "abstract": "Open-source FaaS platforms have recently shown rapid growth, which is usually manifested as extension or specialization of existing cloud-native components and systems -mainly over Kubernetes- since they are provably capable of standing their ground against production-level needs. Despite its advances, the cloud-native ecosystem has focused mostly on container-based deployments so far. FaaS workloads' need for massive colocation [1] without sacrificing security guarantees, pushes multi-tenancy to its limits.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2168303863",
                    "name": "Christos Katsakioris"
                },
                {
                    "authorId": "1654179980",
                    "name": "Chloe Alverti"
                },
                {
                    "authorId": "144704414",
                    "name": "K. Nikas"
                },
                {
                    "authorId": "2585804",
                    "name": "Stratos Psomadakis"
                },
                {
                    "authorId": "145225900",
                    "name": "Vasileios Karakostas"
                },
                {
                    "authorId": "1774783",
                    "name": "N. Koziris"
                }
            ]
        },
        {
            "paperId": "ade9cfdc6190c4a41543a004c7d92e368f2a0557",
            "title": "Feature-based SpMV Performance Analysis on Contemporary Devices",
            "abstract": "The SpMV kernel is characterized by high performance variation per input matrix and computing platform. While GPUs were considered State-of-the-Art for SpMV, with the emergence of advanced multicore CPUs and low-power FPGA accelerators, we need to revisit its performance and energy efficiency. This paper provides a high-level SpMV performance analysis based on structural features of matrices related to common bottlenecks of memory-bandwidth intensity, low ILP, load imbalance and memory latency overheads. Towards this, we create a wide artificial matrix dataset that spans these features and study the performance of different storage formats in nine modern HPC platforms; five CPUs, three GPUs and an FPGA. After validating our proposed methodology using real-world matrices, we analyze our extensive experimental results and draw key insights on the competitiveness of different target architectures for SpMV and the impact of each feature/bottleneck on its performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1654180545",
                    "name": "Panagiotis Mpakos"
                },
                {
                    "authorId": "104874513",
                    "name": "D. Galanopoulos"
                },
                {
                    "authorId": "2086803781",
                    "name": "Petros Anastasiadis"
                },
                {
                    "authorId": "40395087",
                    "name": "Nikela Papadopoulou"
                },
                {
                    "authorId": "1774783",
                    "name": "N. Koziris"
                },
                {
                    "authorId": "2825685",
                    "name": "G. Goumas"
                }
            ]
        }
    ]
}