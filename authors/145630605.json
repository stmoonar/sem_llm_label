{
    "authorId": "145630605",
    "papers": [
        {
            "paperId": "3a20d70116ffc4f8edab97b13638771a6f22f9ba",
            "title": "Data poisoning attacks on off-policy policy evaluation methods",
            "abstract": "Off-policy Evaluation (OPE) methods are a crucial tool for evaluating policies in high-stakes domains such as healthcare, where exploration is often infeasible, unethical, or expensive. However, the extent to which such methods can be trusted under adversarial threats to data quality is largely unexplored. In this work, we make the first attempt at investigating the sensitivity of OPE methods to marginal adversarial perturbations to the data. We design a generic data poisoning attack framework leveraging influence functions from robust statistics to carefully construct perturbations that maximize error in the policy value estimates. We carry out extensive experimentation with multiple healthcare and control datasets. Our results demonstrate that many existing OPE methods are highly prone to generating value estimates with large errors when subject to data poisoning attacks, even for small adversarial perturbations. These findings question the reliability of policy values derived using OPE methods and motivate the need for developing OPE methods that are statistically robust to train-time data poisoning attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35597739",
                    "name": "Elita Lobo"
                },
                {
                    "authorId": "20400898",
                    "name": "Harvineet Singh"
                },
                {
                    "authorId": "145630605",
                    "name": "Marek Petrik"
                },
                {
                    "authorId": "2060638599",
                    "name": "C. Rudin"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "d2bf697aa0f2bca7c9666e0fb871e7b2063ee186",
            "title": "Solving multi-model MDPs by coordinate ascent and dynamic programming",
            "abstract": "Multi-model Markov decision process (MMDP) is a promising framework for computing policies that are robust to parameter uncertainty in MDPs. MMDPs aim to find a policy that maximizes the expected return over a distribution of MDP models. Because MMDPs are NP-hard to solve, most methods resort to approximations. In this paper, we derive the policy gradient of MMDPs and propose CADP, which combines a coordinate ascent method and a dynamic programming algorithm for solving MMDPs. The main innovation of CADP compared with earlier algorithms is to take the coordinate ascent perspective to adjust model weights iteratively to guarantee monotone policy improvements to a local maximum. A theoretical analysis of CADP proves that it never performs worse than previous dynamic programming algorithms like WSU. Our numerical results indicate that CADP substantially outperforms existing methods on several benchmark problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3268510",
                    "name": "Xihong Su"
                },
                {
                    "authorId": "145630605",
                    "name": "Marek Petrik"
                }
            ]
        },
        {
            "paperId": "778be3bd45fb0e7eefa319af56194f1e75a493c2",
            "title": "On Dynamic Program Decompositions of Static Risk Measures",
            "abstract": "Optimizing static risk-averse objectives in Markov decision processes is challenging because they do not readily admit dynamic programming decompositions. Prior work has proposed to use a dynamic decomposition of risk measures that help to formulate dynamic programs on an augmented state space. This paper shows that several existing decompositions are inherently inexact, contradicting several claims in the literature. In particular, we give examples that show that popular decompositions for CVaR and EVaR risk measures are strict overestimates of the true risk values. However, an exact decomposition is possible for VaR, and we give a simple proof that illustrates the fundamental di\ufb00erence between VaR and CVaR dynamic programming properties.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1629211586",
                    "name": "J. Hau"
                },
                {
                    "authorId": "3203751",
                    "name": "E. Delage"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                },
                {
                    "authorId": "145630605",
                    "name": "Marek Petrik"
                }
            ]
        },
        {
            "paperId": "93c5566ab5ca596803bcaf09af1b9d62aa292628",
            "title": "Entropic Risk Optimization in Discounted MDPs",
            "abstract": "Risk-averse Markov Decision Processes (MDPs) have optimal policies that achieve high returns with low variability, but these MDPs are often difficult to solve. Only a few risk-averse objectives admit a dynamic programming (DP) formulation, which is the mainstay of most MDP and RL algorithms. We derive a new DP formulation for discounted risk-averse MDPs with En-tropic Risk Measure (ERM) and Entropic Value at Risk (EVaR) objectives. Our DP formulation for ERM, which is possible because of our novel definition of value function with time-dependent risk levels, can approximate optimal policies in a time that is polynomial in the approximation error. We then use the ERM algorithm to optimize the EVaR objective in polynomial time using an optimized discretization scheme. Our numerical results show the viability of our formulations and algorithms in discounted MDPs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1629211586",
                    "name": "J. Hau"
                },
                {
                    "authorId": "145630605",
                    "name": "Marek Petrik"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                }
            ]
        },
        {
            "paperId": "acc9730b0291d0d56b2080e084038fcf8f035fc9",
            "title": "Bayesian Regret Minimization in Offline Bandits",
            "abstract": "We study how to make decisions that minimize Bayesian regret in offline linear bandits. Prior work suggests that one must take actions with maximum lower confidence bound (LCB) on their reward. We argue that the reliance on LCB is inherently flawed in this setting and propose a new algorithm that directly minimizes upper bounds on the Bayesian regret using efficient conic optimization solvers. Our bounds build heavily on new connections to monetary risk measures. Proving a matching lower bound, we show that our upper bounds are tight, and by minimizing them we are guaranteed to outperform the LCB approach. Our numerical results on synthetic domains confirm that our approach is superior to LCB.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                },
                {
                    "authorId": "145630605",
                    "name": "Marek Petrik"
                },
                {
                    "authorId": "29978064",
                    "name": "Guy Tennenholtz"
                }
            ]
        },
        {
            "paperId": "eeb7dd8eb56c910089e9ffb1164724a91e371898",
            "title": "On Dynamic Programming Decompositions of Static Risk Measures in Markov Decision Processes",
            "abstract": "Optimizing static risk-averse objectives in Markov decision processes is difficult because they do not admit standard dynamic programming equations common in Reinforcement Learning (RL) algorithms. Dynamic programming decompositions that augment the state space with discrete risk levels have recently gained popularity in the RL community. Prior work has shown that these decompositions are optimal when the risk level is discretized sufficiently. However, we show that these popular decompositions for Conditional-Value-at-Risk (CVaR) and Entropic-Value-at-Risk (EVaR) are inherently suboptimal regardless of the discretization level. In particular, we show that a saddle point property assumed to hold in prior literature may be violated. However, a decomposition does hold for Value-at-Risk and our proof demonstrates how this risk measure differs from CVaR and EVaR. Our findings are significant because risk-averse algorithms are used in high-stake environments, making their correctness much more critical.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1629211586",
                    "name": "J. Hau"
                },
                {
                    "authorId": "3203751",
                    "name": "E. Delage"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                },
                {
                    "authorId": "145630605",
                    "name": "Marek Petrik"
                }
            ]
        },
        {
            "paperId": "a8bffbe10134b53c6182556afe688fe41d32feb3",
            "title": "On the convex formulations of robust Markov decision processes",
            "abstract": "Robust Markov decision processes (MDPs) are used for applications of dynamic optimization in uncertain environments and have been studied extensively. Many of the main properties and algorithms of MDPs, such as value iteration and policy iteration, extend directly to RMDPs. Surprisingly, there is no known analog of the MDP convex optimization formulation for solving RMDPs. This work describes the first convex optimization formulation of RMDPs under the classical sa-rectangularity and s-rectangularity assumptions. By using entropic regularization and exponential change of variables, we derive a convex formulation with a number of variables and constraints polynomial in the number of states and actions, but with large coefficients in the constraints. We further simplify the formulation for RMDPs with polyhedral, ellipsoidal, or entropy-based uncertainty sets, showing that, in these cases, RMDPs can be reformulated as conic programs based on exponential cones, quadratic cones, and nonnegative orthants. Our work opens a new research direction for RMDPs and can serve as a first step toward obtaining a tractable convex formulation of RMDPs. Funding: The work in the paper was supported, in part, by NSF [Grants 2144601 and 1815275]; and Agence Nationale de la Recherche [Grant 11-LABX-0047].",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1417807513",
                    "name": "Julien Grand-Cl\u00e9ment"
                },
                {
                    "authorId": "145630605",
                    "name": "Marek Petrik"
                }
            ]
        },
        {
            "paperId": "ac3b964d8649ec6a6bf2be5ba9fad5373fcef401",
            "title": "Policy Gradient in Robust MDPs with Global Convergence Guarantee",
            "abstract": "Robust Markov decision processes (RMDPs) provide a promising framework for computing reliable policies in the face of model errors. Many successful reinforcement learning algorithms build on variations of policy-gradient methods, but adapting these methods to RMDPs has been challenging. As a result, the applicability of RMDPs to large, practical domains remains limited. This paper proposes a new Double-Loop Robust Policy Gradient (DRPG), the first generic policy gradient method for RMDPs. In contrast with prior robust policy gradient algorithms, DRPG monotonically reduces approximation errors to guarantee convergence to a globally optimal policy in tabular RMDPs. We introduce a novel parametric transition kernel and solve the inner loop robust policy via a gradient-based method. Finally, our numerical results demonstrate the utility of our new algorithm and confirm its global convergence properties.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116274979",
                    "name": "Qiuhao Wang"
                },
                {
                    "authorId": "20780816",
                    "name": "C. Ho"
                },
                {
                    "authorId": "145630605",
                    "name": "Marek Petrik"
                }
            ]
        },
        {
            "paperId": "c75fabdc880b4048ed452ed32d6e1ef0d40c24d1",
            "title": "RASR: Risk-Averse Soft-Robust MDPs with EVaR and Entropic Risk",
            "abstract": "Prior work on safe Reinforcement Learning (RL) has studied risk-aversion to randomness in dynamics (aleatory) and to model uncertainty (epistemic) in isolation. We propose and analyze a new framework to jointly model the risk associated with epistemic and aleatory uncertainties in finite-horizon and discounted infinite-horizon MDPs. We call this framework that combines Risk-Averse and Soft-Robust methods RASR. We show that when the risk-aversion is defined using either EVaR or the entropic risk, the optimal policy in RASR can be computed efficiently using a new dynamic program formulation with a time-dependent risk level. As a result, the optimal risk-averse policies are deterministic but time-dependent, even in the infinite-horizon discounted setting. We also show that particular RASR objectives reduce to risk-averse RL with mean posterior transition probabilities. Our empirical results show that our new algorithms consistently mitigate uncertainty as measured by EVaR and other standard risk measures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1629211586",
                    "name": "J. Hau"
                },
                {
                    "authorId": "145630605",
                    "name": "Marek Petrik"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                },
                {
                    "authorId": "10766679",
                    "name": "R. Russel"
                }
            ]
        },
        {
            "paperId": "cea5646c0c24eb903c724ed31173a09f8d06134f",
            "title": "On the Convergence of Policy Gradient in Robust MDPs",
            "abstract": "Robust Markov decision processes (RMDPs) are promising models that provide reliable policies under ambiguities in model parameters. As opposed to nominal Markov decision processes (MDPs), however, the state-of-the-art solution methods for RMDPs are limited to value-based methods, such as value iteration and policy iteration. This paper proposes Double-Loop Robust Policy Gradient (DRPG), the first generic policy gradient method for RMDPs with a global convergence guarantee in tabular problems. Unlike value-based methods, DRPG does not rely on dynamic programming techniques. In particular, the inner-loop robust policy evaluation problem is solved via projected gradient descent. Finally, our experimental results demonstrate the performance of our algorithm and verify our theoretical guarantees.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116274979",
                    "name": "Qiuhao Wang"
                },
                {
                    "authorId": "20780816",
                    "name": "C. Ho"
                },
                {
                    "authorId": "145630605",
                    "name": "Marek Petrik"
                }
            ]
        }
    ]
}