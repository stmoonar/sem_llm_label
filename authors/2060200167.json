{
    "authorId": "2060200167",
    "papers": [
        {
            "paperId": "27e45a8aecc1fec246fd70c80d8f5104807cf0dd",
            "title": "VIT-LENS: Towards Omni-modal Representations",
            "abstract": "Aiming to advance AI agents, large foundation models significantly improve reasoning and instruction execution, yet the current focus on vision and language neglects the potential of perceiving diverse modalities in open-world environments. However, the success of data-driven vision and language models is costly or even infeasible to be reproduced for rare modalities. In this paper, we present Vit-lens that facilitates efficient omni-modal representation learning by perceiving novel modalities with a pretrained- ViT and aligning them to a pre-defined space. Specifically, the modality-specific lens is tuned to project any-modal signals to an intermediate embedding space, which are then processed by a strong ViT with pre-trained visual knowledge. The encoded representations are optimized toward aligning with the modal-independent space, pre-defined by off-the-shelf foundation models. Vit-lensprovides a unified solution for representation learning of increasing modalities with two appealing advantages: (i) Unlocking the great potential of pretrained- ViTs to novel modalities effectively with efficient parameters and data regime; (ii) Enabling emergent down- stream capabilities through modality alignment and shared ViT parameters. We tailor Vit-lensto learn representations for 3D point cloud, depth, audio, tactile and EEG, and set new state-of-the-art results across various understanding tasks, such as zero-shot classification. By seamlessly integrating Vit-lensinto Multimodal Foundation Models, we enable Any-modality to Text and Image Generation in a zero-shot manner. Code and models are available at https://github.com/TencentARC/ViT-Lens.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1716845065",
                    "name": "Weixian Lei"
                },
                {
                    "authorId": "152988335",
                    "name": "Yixiao Ge"
                },
                {
                    "authorId": "2107968449",
                    "name": "Jianfeng Zhang"
                },
                {
                    "authorId": "1481092824",
                    "name": "Dylan Sun"
                },
                {
                    "authorId": "2060200167",
                    "name": "Kun Yi"
                },
                {
                    "authorId": "1387190008",
                    "name": "Ying Shan"
                },
                {
                    "authorId": "2047358650",
                    "name": "Mike Zheng Shou"
                }
            ]
        },
        {
            "paperId": "5becc0d68692f36b7d87aab3f1c172a1df370670",
            "title": "A Survey on Deep Learning based Time Series Analysis with Frequency Transformation",
            "abstract": "Recently, frequency transformation (FT) has been increasingly incorporated into deep learning models to significantly enhance state-of-the-art accuracy and efficiency in time series analysis. The advantages of FT, such as high efficiency and a global view, have been rapidly explored and exploited in various time series tasks and applications, demonstrating the promising potential of FT as a new deep learning paradigm for time series analysis. Despite the growing attention and the proliferation of research in this emerging field, there is currently a lack of a systematic review and in-depth analysis of deep learning-based time series models with FT. It is also unclear why FT can enhance time series analysis and what its limitations in the field are. To address these gaps, we present a comprehensive review that systematically investigates and summarizes the recent research advancements in deep learning-based time series analysis with FT. Specifically, we explore the primary approaches used in current models that incorporate FT, the types of neural networks that leverage FT, and the representative FT-equipped models in deep time series analysis. We propose a novel taxonomy to categorize the existing methods in this field, providing a structured overview of the diverse approaches employed in incorporating FT into deep learning models for time series analysis. Finally, we highlight the advantages and limitations of FT for time series modeling and identify potential future research directions that can further contribute to the community of time series analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060200167",
                    "name": "Kun Yi"
                },
                {
                    "authorId": "47835189",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2116951322",
                    "name": "Shoujin Wang"
                },
                {
                    "authorId": "2107477853",
                    "name": "Hui He"
                },
                {
                    "authorId": "2062835",
                    "name": "Guodong Long"
                },
                {
                    "authorId": "8253080",
                    "name": "Zhendong Niu"
                }
            ]
        },
        {
            "paperId": "d17771d1e83304b0072ebc7536b638c89efe0f95",
            "title": "Learning Informative Representation for Fairness-Aware Multivariate Time-Series Forecasting: A Group-Based Perspective",
            "abstract": "Multivariate time series (MTS) forecasting penetrates various aspects of our economy and society, whose roles become increasingly recognized. However, often MTS forecasting is unfair, not only degrading their practical benefits but even incurring potential risk. Unfair MTS forecasting may be attributed to disparities relating to advantaged and disadvantaged variables, which has rarely been studied in the MTS forecasting. In this work, we formulate the MTS fairness modeling problem as learning informative representations attending to both advantaged and disadvantaged variables. Accordingly, we propose a novel framework, named <italic>FairFor</italic>, for fairness-aware MTS forecasting, i.e., <italic>fair MTS forecasting</italic>. <italic>FairFor</italic> uses adversarial learning to generate both group-irrelevant and -relevant representations for downstream forecasting. <italic>FairFor</italic> first adopts recurrent graph convolution to capture spatio-temporal variable correlations and to group variables by leveraging a spectral relaxation of the K-means objective. Then, it utilizes a novel filtering <inline-formula><tex-math notation=\"LaTeX\">$ \\& $</tex-math><alternatives><mml:math><mml:mo>&</mml:mo></mml:math><inline-graphic xlink:href=\"zhang-ieq1-3323956.gif\"/></alternatives></inline-formula> fusion module to filter group-relevant information and generate group-irrelevant representations by orthogonality regularization. The group-irrelevant and -relevant representations form highly informative representations, facilitating to share the knowledge from advantaged variables to disadvantaged variables and guarantee the fairness of forecasting. Extensive experiments on four public datasets demonstrate the <italic>FairFor</italic> effectiveness for fair forecasting and significant performance improvement.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2107477853",
                    "name": "Hui He"
                },
                {
                    "authorId": "2145908596",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2116951322",
                    "name": "Shoujin Wang"
                },
                {
                    "authorId": "2060200167",
                    "name": "Kun Yi"
                },
                {
                    "authorId": "8253080",
                    "name": "Zhendong Niu"
                },
                {
                    "authorId": "2184095458",
                    "name": "Longbin Cao"
                }
            ]
        },
        {
            "paperId": "d6b1d1bad791f5b80244b2c015c817ef53829182",
            "title": "RILS: Masked Visual Reconstruction in Language Semantic Space",
            "abstract": "Both masked image modeling (MIM) and natural language supervision have facilitated the progress of transferable visual pre-training. In this work, we seek the synergy between two paradigms and study the emerging properties when MIM meets natural language supervision. To this end, we present a novel masked visual Reconstruction In Language semantic Space (RILS) pre-training framework, in which sentence representations, encoded by the text encoder, serve as prototypes to transform the vision-only signals into patch-sentence probabilities as semantically meaningful MIM reconstruction targets. The vision models can therefore capture useful components with structured information by predicting proper semantic of masked tokens. Better visual representations could, in turn, improve the text encoder via the image-text alignment objective, which is essential for the effective MIM target transformation. Extensive experimental results demonstrate that our method not only enjoys the best of previous MIM and CLIP but also achieves further improvements on various tasks due to their mutual benefits. RILS exhibits advanced transferability on downstream classification, detection, and segmentation, especially for low-shot regimes. Code is available at https://github.com/hustvl/RILS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Shusheng Yang"
                },
                {
                    "authorId": "152988335",
                    "name": "Yixiao Ge"
                },
                {
                    "authorId": "2060200167",
                    "name": "Kun Yi"
                },
                {
                    "authorId": "2151495740",
                    "name": "Dian Li"
                },
                {
                    "authorId": "1387190008",
                    "name": "Ying Shan"
                },
                {
                    "authorId": "3284850",
                    "name": "Xiaohu Qie"
                },
                {
                    "authorId": "2443233",
                    "name": "Xinggang Wang"
                }
            ]
        },
        {
            "paperId": "0c92781eee76517abb9f308f6f52885ef63407a5",
            "title": "PENCIL: Deep Learning with Noisy Labels",
            "abstract": "Deep learning has achieved excellent performance in various computer vision tasks, but requires a lot of training examples with clean labels. It is easy to collect a dataset with noisy labels, but such noise makes networks overfit seriously and accuracies drop dramatically. To address this problem, we propose an end-to-end framework called PENCIL, which can update both network parameters and label estimations as label distributions. PENCIL is independent of the backbone network structure and does not need an auxiliary clean dataset or prior information about noise, thus it is more general and robust than existing methods and is easy to apply. PENCIL can even be used repeatedly to obtain better performance. PENCIL outperforms previous state-of-the-art methods by large margins on both synthetic and real-world datasets with different noise types and noise rates. And PENCIL is also effective in multi-label classification tasks through adding a simple attention structure on backbone networks. Experiments show that PENCIL is robust on clean datasets, too.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060200167",
                    "name": "Kun Yi"
                },
                {
                    "authorId": "50248762",
                    "name": "G. Wang"
                },
                {
                    "authorId": "1808816",
                    "name": "Jianxin Wu"
                }
            ]
        },
        {
            "paperId": "1dfc7c6cb0f19d3384eaeb98698d64a67162c99c",
            "title": "mc-BEiT: Multi-choice Discretization for Image BERT Pre-training",
            "abstract": "Image BERT pre-training with masked image modeling (MIM) becomes a popular practice to cope with self-supervised representation learning. A seminal work, BEiT, casts MIM as a classification task with a visual vocabulary, tokenizing the continuous visual signals into discrete vision tokens using a pre-learned dVAE. Despite a feasible solution, the improper discretization hinders further improvements of image pre-training. Since image discretization has no ground-truth answers, we believe that the masked patch should not be assigned with a unique token id even if a better tokenizer can be obtained. In this work, we introduce an improved BERT-style image pre-training method, namely mc-BEiT, which performs MIM proxy tasks towards eased and refined multi-choice training objectives. Specifically, the multi-choice supervision for the masked image patches is formed by the soft probability vectors of the discrete token ids, which are predicted by the off-the-shelf image tokenizer and further refined by high-level inter-patch perceptions resorting to the observation that similar patches should share their choices. Extensive experiments on classification, segmentation, and detection tasks demonstrate the superiority of our method, e.g., the pre-trained ViT-B achieves 84.1% top-1 fine-tuning accuracy on ImageNet-1K classification, 49.2% AP^b and 44.0% AP^m of object detection and instance segmentation on COCO, 50.8% mIOU on ADE20K semantic segmentation, outperforming the competitive counterparts. The code will be available at https://github.com/lixiaotong97/mc-BEiT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108689869",
                    "name": "Xiaotong Li"
                },
                {
                    "authorId": "152988335",
                    "name": "Yixiao Ge"
                },
                {
                    "authorId": "2060200167",
                    "name": "Kun Yi"
                },
                {
                    "authorId": "1557412467",
                    "name": "Zixuan Hu"
                },
                {
                    "authorId": "1387190008",
                    "name": "Ying Shan"
                },
                {
                    "authorId": "7667912",
                    "name": "Ling-yu Duan"
                }
            ]
        },
        {
            "paperId": "32e606846f5396162294055fafd3632757e35ba2",
            "title": "Masked Image Modeling with Denoising Contrast",
            "abstract": "Since the development of self-supervised visual representation learning from contrastive learning to masked image modeling (MIM), there is no significant difference in essence, that is, how to design proper pretext tasks for vision dictionary look-up. MIM recently dominates this line of research with state-of-the-art performance on vision Transformers (ViTs), where the core is to enhance the patch-level visual context capturing of the network via denoising auto-encoding mechanism. Rather than tailoring image tokenizers with extra training stages as in previous works, we unleash the great potential of contrastive learning on denoising auto-encoding and introduce a pure MIM method, ConMIM, to produce simple intra-image inter-patch contrastive constraints as the sole learning objectives for masked patch prediction. We further strengthen the denoising mechanism with asymmetric designs, including image perturbations and model progress rates, to improve the network pre-training. ConMIM-pretrained models with various scales achieve competitive results on downstream image classification, semantic segmentation, object detection, and instance segmentation tasks, e.g., on ImageNet-1K classification, we achieve 83.9% top-1 accuracy with ViT-Small and 85.3% with ViT-Base without extra data for pre-training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060200167",
                    "name": "Kun Yi"
                },
                {
                    "authorId": "152988335",
                    "name": "Yixiao Ge"
                },
                {
                    "authorId": "2108689869",
                    "name": "Xiaotong Li"
                },
                {
                    "authorId": null,
                    "name": "Shusheng Yang"
                },
                {
                    "authorId": "2151495740",
                    "name": "Dian Li"
                },
                {
                    "authorId": "2158877245",
                    "name": "Jianping Wu"
                },
                {
                    "authorId": "1387190008",
                    "name": "Ying Shan"
                },
                {
                    "authorId": "3284850",
                    "name": "Xiaohu Qie"
                }
            ]
        },
        {
            "paperId": "4a282742bb264677a17687988fc22fce1eba128b",
            "title": "Edge-Varying Fourier Graph Networks for Multivariate Time Series Forecasting",
            "abstract": "The key problem in multivariate time series (MTS) analysis and forecasting aims to disclose the underlying couplings between variables that drive the co-movements. Considerable recent successful MTS methods are built with graph neural networks (GNNs) due to their essential capacity for relational modeling. However, previous work often used a static graph structure of time-series variables for modeling MTS failing to capture their ever-changing correlations over time. To this end, a fully-connected supra-graph connecting any two variables at any two timestamps is adaptively learned to capture the high-resolution variable dependencies via an efficient graph convolutional network. Specifically, we construct the Edge-Varying Fourier Graph Networks (EV-FGN) equipped with Fourier Graph Shift Operator (FGSO) which efficiently performs graph convolution in the frequency domain. As a result, a high-efficiency scale-free parameter learning scheme is derived for MTS analysis and forecasting according to the convolution theorem. Extensive experiments show that EV-FGN outperforms state-of-the-art methods on seven real-world MTS datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060200167",
                    "name": "Kun Yi"
                },
                {
                    "authorId": "47835189",
                    "name": "Qi Zhang"
                }
            ]
        },
        {
            "paperId": "57ba68e866ce4eca123b9afd13d9b32acfed348e",
            "title": "Distributional Drift Adaptation with Temporal Conditional Variational Autoencoder for Multivariate Time Series Forecasting",
            "abstract": "Due to the nonstationary nature, the distribution of real-world multivariate time series (MTS) changes over time, which is known as distribution drift. Most existing MTS forecasting models greatly suffer from distribution drift and degrade the forecasting performance over time. Existing methods address distribution drift via adapting to the latest arrived data or self-correcting per the meta knowledge derived from future data. Despite their great success in MTS forecasting, these methods hardly capture the intrinsic distribution changes, especially from a distributional perspective. Accordingly, we propose a novel framework temporal conditional variational autoencoder (TCVAE) to model the dynamic distributional dependencies over time between historical observations and future data in MTSs and infer the dependencies as a temporal conditional distribution to leverage latent variables. Specifically, a novel temporal Hawkes attention (THA) mechanism represents temporal factors that subsequently fed into feedforward networks to estimate the prior Gaussian distribution of latent variables. The representation of temporal factors further dynamically adjusts the structures of Transformer-based encoder and decoder to distribution changes by leveraging a gated attention mechanism (GAM). Moreover, we introduce conditional continuous normalization flow (CCNF) to transform the prior Gaussian to a complex and form-free distribution to facilitate flexible inference of the temporal conditional distribution. Extensive experiments conducted on six real-world MTS datasets demonstrate the TCVAE's superior robustness and effectiveness over the state-of-the-art MTS forecasting baselines. We further illustrate the TCVAE applicability through multifaceted case studies and visualization in real-world scenarios.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2107477853",
                    "name": "Hui He"
                },
                {
                    "authorId": "49346854",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2060200167",
                    "name": "Kun Yi"
                },
                {
                    "authorId": "102590513",
                    "name": "Kaize Shi"
                },
                {
                    "authorId": "8253080",
                    "name": "Zhendong Niu"
                },
                {
                    "authorId": "2184095458",
                    "name": "Longbin Cao"
                }
            ]
        },
        {
            "paperId": "58b4cf057bdd361be289601ef3dd69b4efbef83e",
            "title": "Probabilistic End-To-End Noise Correction for Learning With Noisy Labels",
            "abstract": "Deep learning has achieved excellent performance in various computer vision tasks, but requires a lot of training examples with clean labels. It is easy to collect a dataset with noisy labels, but such noise makes networks overfit seriously and accuracies drop dramatically. To address this problem, we propose an end-to-end framework called PENCIL, which can update both network parameters and label estimations as label distributions. PENCIL is independent of the backbone network structure and does not need an auxiliary clean dataset or prior information about noise, thus it is more general and robust than existing methods and is easy to apply. PENCIL outperformed previous state-of-the-art methods by large margins on both synthetic and real-world datasets with different noise types and noise rates. Experiments show that PENCIL is robust on clean datasets, too.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060200167",
                    "name": "Kun Yi"
                },
                {
                    "authorId": "1808816",
                    "name": "Jianxin Wu"
                }
            ]
        }
    ]
}