{
    "authorId": "2365952",
    "papers": [
        {
            "paperId": "c2578f5ced46da44eb68253293d8e6ce2397f043",
            "title": "Measuring Distributional Shifts in Text: The Advantage of Language Model-Based Embeddings",
            "abstract": "An essential part of monitoring machine learning models in production is measuring input and output data drift. In this paper, we present a system for measuring distributional shifts in natural language data and highlight and investigate the potential advantage of using large language models (LLMs) for this problem. Recent advancements in LLMs and their successful adoption in different domains indicate their effectiveness in capturing semantic relationships for solving various natural language processing problems. The power of LLMs comes largely from the encodings (embeddings) generated in the hidden layers of the corresponding neural network. First we propose a clustering-based algorithm for measuring distributional shifts in text data by exploiting such embeddings. Then we study the effectiveness of our approach when applied to text embeddings generated by both LLMs and classical embedding algorithms. Our experiments show that general-purpose LLM-based embeddings provide a high sensitivity to data drift compared to other embedding methods. We propose drift sensitivity as an important evaluation metric to consider when comparing language models. Finally, we present insights and lessons learned from deploying our framework as part of the Fiddler ML Monitoring platform over a period of 18 months.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2269733839",
                    "name": "Gyandev Gupta"
                },
                {
                    "authorId": "2365952",
                    "name": "Bashir Rastegarpanah"
                },
                {
                    "authorId": "2269735028",
                    "name": "Amalendu Iyer"
                },
                {
                    "authorId": "2269735459",
                    "name": "Joshua Rubin"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                }
            ]
        },
        {
            "paperId": "efb5c95a45747db1e2eaf7515639eb05a9e9c75e",
            "title": "Auditing Black-Box Prediction Models for Data Minimization Compliance",
            "abstract": "In this paper, we focus on auditing black-box prediction models for compliance with the GDPR\u2019s data minimization principle . This principle restricts prediction models to use the minimal information that is necessary for performing the task at hand. Given the challenge of the black-box setting, our key idea is to check if each of the prediction model\u2019s input features is individually necessary by assigning it some constant value (i.e., applying a simple imputation) across all prediction instances, and measuring the extent to which the model outcomes would change. We introduce a metric for data minimization that is based on model instability under simple imputations. We extend the applicability of this metric from a \ufb01nite sample model to a distributional setting by introducing a probabilistic data minimization guarantee, which we derive using a Bayesian approach. Furthermore, we address the auditing problem under a constraint on the number of queries to the prediction system. We formulate the problem of allocating a budget of system queries to feasible simple imputations (for investigating model instability) as a multi-armed bandit framework with probabilistic success metrics. We de\ufb01ne two bandit problems for providing a probabilistic data minimization guarantee at a given con\ufb01dence level: a decision problem given a data minimization level, and a measurement problem given a \ufb01xed query budget. We design ef\ufb01cient algorithms for these auditing problems using novel exploration strategies that expand classical bandit strategies. Our experiments with real-world prediction systems show that our auditing algorithms signi\ufb01cantly outperform simpler benchmarks in both measurement and decision problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2365952",
                    "name": "Bashir Rastegarpanah"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                },
                {
                    "authorId": "1685445",
                    "name": "M. Crovella"
                }
            ]
        },
        {
            "paperId": "8d8c833902a91e18e535ec0a3cce3125e27e8357",
            "title": "Fair Inputs and Fair Outputs: The Incompatibility of Fairness in Privacy and Accuracy",
            "abstract": "Fairness concerns about algorithmic decision-making systems have been mainly focused on the outputs (e.g., the accuracy of a classifier across individuals or groups). However, one may additionally be concerned with fairness in the inputs. In this paper, we propose and formulate two properties regarding the inputs of (features used by) a classifier. In particular, we claim that fair privacy (whether individuals are all asked to reveal the same information) and need-to-know (whether users are only asked for the minimal information required for the task at hand) are desirable properties of a decision system. We explore the interaction between these properties and fairness in the outputs (fair prediction accuracy). We show that for an optimal classifier these three properties are in general incompatible, and we explain what common properties of data make them incompatible. Finally we provide an algorithm to verify if the trade-off between the three properties exists in a given dataset, and use the algorithm to show that this trade-off is common in real data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2365952",
                    "name": "Bashir Rastegarpanah"
                },
                {
                    "authorId": "1685445",
                    "name": "M. Crovella"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                }
            ]
        },
        {
            "paperId": "ff8538bcaa9540e8c1a0e902cb05ddb51967f219",
            "title": "Fighting Fire with Fire: Using Antidote Data to Improve Polarization and Fairness of Recommender Systems",
            "abstract": "The increasing role of recommender systems in many aspects of society makes it essential to consider how such systems may impact social good. Various modifications to recommendation algorithms have been proposed to improve their performance for specific socially relevant measures. However, previous proposals are often not easily adapted to different measures, and they generally require the ability to modify either existing system inputs, the system's algorithm, or the system's outputs. As an alternative, in this paper we introduce the idea of improving the social desirability of recommender system outputs by adding more data to the input, an approach we view as as providing 'antidote' data to the system. We formalize the antidote data problem, and develop optimization-based solutions. We take as our model system the matrix factorization approach to recommendation, and we propose a set of measures to capture the polarization or fairness of recommendations. We then show how to generate antidote data for each measure, pointing out a number of computational efficiencies, and discuss the impact on overall system accuracy. Our experiments show that a modest budget for antidote data can lead to significant improvements in the polarization or fairness of recommendations.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2365952",
                    "name": "Bashir Rastegarpanah"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                },
                {
                    "authorId": "1685445",
                    "name": "M. Crovella"
                }
            ]
        },
        {
            "paperId": "1234096290c679612068e69e67ad444b216aedce",
            "title": "Collective attention to social media evolves according to diffusion models",
            "abstract": "We investigate patterns of adoption of 175 social media services and Web businesses using data from Google Trends. For each service, we collect aggregated search frequencies from 45 countries as well as global averages. This results in more than 8.000 time series which we analyze using economic diffusion models. The models are found to provide accurate and statistically significant fits to the data and show that collective attention to social media grows and subsides in a highly regular manner. Regularities persist across regions, cultures, and topics and thus hint at general mechanisms that govern the adoption of Web-based services.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1692283",
                    "name": "C. Bauckhage"
                },
                {
                    "authorId": "1746871",
                    "name": "K. Kersting"
                },
                {
                    "authorId": "2365952",
                    "name": "Bashir Rastegarpanah"
                }
            ]
        }
    ]
}