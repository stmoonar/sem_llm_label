{
    "authorId": "3417475",
    "papers": [
        {
            "paperId": "3c743f9aeddc8a076f90b924bfb3bd0b9d6870ee",
            "title": "Semi-Supervised Graph Ultra-Sparsifier Using Reweighted \u21131 Optimization",
            "abstract": "Graph representation learning with the family of graph convolution networks (GCN) provides powerful tools for prediction on graphs. As graphs grow with more edges, the GCN family suffers from sub-optimal generalization performance due to task-irrelevant connections. Recent studies solve this problem by using graph sparsification in neural networks. However, graph sparsification cannot generate ultra-sparse graphs while simultaneously maintaining the performance of the GCN family. To address this problem, we propose Graph Ultra-sparsifier, a semi-supervised graph sparsifier with dynamically-updated regularization terms based on the graph convolution. The graph ultra-sparsifier can generate ultra-sparse graphs while maintaining the performance of the GCN family with the ultra-sparse graphs as inputs. In the experiments, when compared to the state-of-the-art graph sparsifiers, our graph ultra-sparsifier generates ultra-sparse graphs and these ultra-sparse graphs can be used as inputs to maintain the performance of GCN and its variants in node classification tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46277026",
                    "name": "Jiayu Li"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "28044622",
                    "name": "Shengmin Jin"
                },
                {
                    "authorId": "2281410",
                    "name": "R. Zafarani"
                }
            ]
        },
        {
            "paperId": "74dea43b8cbf75907a34059ba3cf43d1c680625a",
            "title": "Automatic Subnetwork Search Through Dynamic Differentiable Neuron Pruning",
            "abstract": "Locating and pruning redundant neurons from deep neural networks (DNNs) is the focal point of DNN subnetwork search. Recent advance mainly targets at pruning neuron through heuristic \"hard\" constraints or through penalizing neurons. However, these two methods heavily rely on expert knowledge in designing model-and-task-specific constraints and penalization, which prohibits easily applying pruning to general models. In this paper, we propose an automatic non-expert-friendly differentiable subnetwork search algorithm which dynamically adjusts the layer-wise neuron-pruning penalty based on sensitivity of Lagrangian multipliers. The idea is to introduce \"soft\" neuron-cardinality layer-wise constraints and then relax them through Lagrangian multipliers. The sensitivity nature of the multipliers is then exploited to iteratively determine the appropriate pruning penalization hyper-parameters during the differentiable neuron pruning procedure. In this way, the model weight, model subnetwork and layer-wise penalty hyper-parameters are simultaneously learned, relieving the prior knowledge requirements and reducing the time for trail-and-error. Results show that our method can select the state-of-the-art slim subnetwork architecture. For VGG-like on CIFAR10, more than 6\u00d7 neuron compression rate is achieved without accuracy drop and without retraining. Accuracy rates of 66.3% and 57.8% are achieved for 150M and 50M FLOPs for MobileNetV1, and accuracy rates of 73.46% and 66.94% are achieved for 200M and 100M FLOPs for MobileNetV2, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117414364",
                    "name": "Zigeng Wang"
                },
                {
                    "authorId": "2145729424",
                    "name": "Bingbing Li"
                },
                {
                    "authorId": "144613373",
                    "name": "Xia Xiao"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "2418402",
                    "name": "Mikhail A. Bragin"
                },
                {
                    "authorId": "2218208575",
                    "name": "Bing Yan"
                },
                {
                    "authorId": "2881873",
                    "name": "Caiwen Ding"
                },
                {
                    "authorId": "143856869",
                    "name": "S. Rajasekaran"
                }
            ]
        },
        {
            "paperId": "93a9663ace0cc16e9ab5691b41457783104ae1c2",
            "title": "Defense against Adversarial Cloud Attack on Remote Sensing Salient Object Detection",
            "abstract": "Detecting the salient objects in a remote sensing image has wide applications. Many existing deep learning methods have been proposed for Salient Object Detection (SOD) in remote sensing images with remarkable results. However, the recent adversarial attack examples, generated by changing a few pixel values on the original image, could result in a collapse for the well-trained deep learning model. Different with existing methods adding perturbation to original images, we propose to jointly tune adversarial exposure and additive perturbation for attack and constrain image close to cloudy image as Adversarial Cloud. Cloud is natural and common in remote sensing images, however, camouflaging cloud based adversarial attack and defense for remote sensing images are not well studied before. Furthermore, we design DefenseNet as a learnable pre-processing to the adversarial cloudy images to preserve the performance of the deep learning based remote sensing SOD model, without tuning the already deployed deep SOD model. By considering both regular and generalized adversarial examples, the proposed DefenseNet can defend the proposed Adversarial Cloud in white-box setting and other attack methods in black-box setting. Experimental results on a synthesized benchmark from the public remote sensing dataset (EORSSD) show the promising defense against adversarial cloud attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47217872",
                    "name": "Huiming Sun"
                },
                {
                    "authorId": "2088894449",
                    "name": "Lan Fu"
                },
                {
                    "authorId": "2125035893",
                    "name": "Jinlong Li"
                },
                {
                    "authorId": "2153929458",
                    "name": "Qing Guo"
                },
                {
                    "authorId": "151481595",
                    "name": "Zibo Meng"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "2933674",
                    "name": "Yuewei Lin"
                },
                {
                    "authorId": "1730682",
                    "name": "Hongkai Yu"
                }
            ]
        },
        {
            "paperId": "2255c2ffb11ef4aa5e5a2da76ec58ec07fd5da70",
            "title": "BLCR: Towards Real-time DNN Execution with Block-based Reweighted Pruning",
            "abstract": "Accelerating DNN execution on resource-limited computing platforms has been a long-standing problem. Prior works utilize \u21131-based group lasso or dynamic regularization such as ADMM to perform structured pruning on DNN models to leverage the parallel computing architectures. However, both of the pruning schemes and pruning methods lack universality, which leads to degraded performance and limited applicability. Considering mobile devices are becoming an important carrier for deep learning tasks, current approaches are not ideal for fully exploiting mobile parallelism while achieving high inference accuracy. To solve the problem, we propose BLCR, a novel block-based pruning framework that comprises a general and flexible structured pruning scheme that enjoys higher flexibility while exploiting full on-device parallelism, as well as a powerful and efficient reweighted regularization method to achieve the proposed sparsity scheme. Our framework is universal, which can be applied to both CNNs and RNNs, implying complete support for the two major kinds of computation-intensive layers (i.e., CONV and FC layers). To complete all aspects of the pruning-for-acceleration task, we also integrate compiler-based code optimization into our framework that can perform DNN inference on mobile devices in real-time. To the best of our knowledge, it is the first time that the weight pruning framework achieves universal coverage for both CNNs and RNNs with real-time mobile acceleration and no accuracy compromise.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151480882",
                    "name": "Xiaolong Ma"
                },
                {
                    "authorId": "9347641",
                    "name": "Geng Yuan"
                },
                {
                    "authorId": "48459506",
                    "name": "Z. Li"
                },
                {
                    "authorId": "2114981469",
                    "name": "Yifan Gong"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "48643324",
                    "name": "Wei Niu"
                },
                {
                    "authorId": "2949135",
                    "name": "Zheng Zhan"
                },
                {
                    "authorId": "31643513",
                    "name": "Pu Zhao"
                },
                {
                    "authorId": "2152354569",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "2115854503",
                    "name": "Jian Tang"
                },
                {
                    "authorId": "2143778722",
                    "name": "Xue Lin"
                },
                {
                    "authorId": "153108488",
                    "name": "Bin Ren"
                },
                {
                    "authorId": "2136922252",
                    "name": "Yanzhi Wang"
                }
            ]
        },
        {
            "paperId": "b9f20ca9aa392714876bcede05fe976e1d584d68",
            "title": "Loss Attitude Aware Energy Management for Signal Detection",
            "abstract": "This work considers a Bayesian signal processing problem where increasing the power of the probing signal may cause risks or undesired consequences. We employ a market based approach to solve energy management problems for signal detection while balancing multiple objectives. In particular, the optimal amount of resource consumption is determined so as to maximize a profit-loss based expected utility function. Next, we study the human behavior of resource consumption while taking individuals' behavioral disparity into account. Unlike rational decision makers who consume the amount of resource to maximize the expected utility function, human decision makers act to maximize their subjective utilities. We employ prospect theory to model humans' loss aversion towards a risky event. The amount of resource consumption that maximizes the humans' subjective utility is derived to characterize the actual behavior of humans. It is shown that loss attitudes may lead the human to behave quite differently from a rational decision maker.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144176883",
                    "name": "Baocheng Geng"
                },
                {
                    "authorId": "2058865328",
                    "name": "Chen Quan"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "1925309",
                    "name": "P. Varshney"
                }
            ]
        },
        {
            "paperId": "cc5e7bf0432543d842badab51b002fcee913c406",
            "title": "AdverSparse: An Adversarial Attack Framework for Deep Spatial-Temporal Graph Neural Networks",
            "abstract": "Spatial-temporal graph have been widely observed in various domains such as neuroscience, climate research, and transportation engineering. The state-of-the-art models of spatialtemporal graphs rely on Graph Neural Networks (GNNs) to obtain explicit representations for such networks and to discover hidden spatial dependencies in them. These models have demonstrated superior performance in various tasks. In this paper, we propose a sparse adversarial attack framework AdverSparse to illustrate that when only a few key connections are removed in such graphs, hidden spatial dependencies learned by such spatial-temporal models are significantly impacted, leading to various issues such as increasing prediction errors. We formulate the adversarial attack as an optimization problem and solve it by the Alternating Direction Method of Multipliers (ADMM). Experiments show that AdverSparse can find and remove key connections in these graphs, leading to malfunctioning models, even in models capable of learning hidden spatial dependencies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46277026",
                    "name": "Jiayu Li"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "28044622",
                    "name": "Shengmin Jin"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "2281410",
                    "name": "R. Zafarani"
                }
            ]
        },
        {
            "paperId": "3c40774aedede4605ad4e5ce88484a441e9136f8",
            "title": "Compact Multi-level Sparse Neural Networks with Input Independent Dynamic Rerouting",
            "abstract": "Deep neural networks (DNNs) have shown to provide superb performance in many real life applications, but their large computation cost and storage requirement have prevented them from being deployed to many edge and internet-of-things (IoT) devices. Sparse deep neural networks, whose majority weight parameters are zeros, can substantially reduce the computation complexity and memory consumption of the models. In real-use scenarios, devices may suffer from large fluctuations of the available computation and memory resources under different environment, and the quality of service (QoS) is difficult to maintain due to the long tail inferences with large latency. Facing the real-life challenges, we propose to train a sparse model that supports multiple sparse levels. That is, a hierarchical structure of weights are satisfied such that the locations and the values of the non-zero parameters of the more-sparse sub-model are a subset of the less-sparse sub-model. In this way, one can dynamically select the appropriate sparsity level during inference, while the storage cost is capped by the least sparse sub-model. We have verified our methodologies on a variety of DNN models and tasks, including the ResNet-50, PointNet++, GNMT, and graph attention networks. We obtain sparse sub-models with an average of 13.38% weights and 14.97% FLOPs, while the accuracies are as good as their dense counterparts. More-sparse sub-models with 5.38% weights and 4.47% of FLOPs, which are subsets of the less-sparse ones, can be obtained with only 3.25% relative accuracy loss. In addition, our proposed hierarchical model structure supports the mechanism to inference the first part of the model with less sparsity, and dynamically reroute to the more-sparse level if the real-time latency constraint is estimated to be violated. Preliminary analysis shows that we can improve the QoS by one or two nines depending on the task and the computation-memory resources of the inference engine.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39449475",
                    "name": "Minghai Qin"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "2075373569",
                    "name": "Fei Sun"
                },
                {
                    "authorId": "123331823",
                    "name": "Yen-kuang Chen"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                },
                {
                    "authorId": "2154871317",
                    "name": "Yuan Xie"
                }
            ]
        },
        {
            "paperId": "55f2d654de4cc3822549c1f6335e5316b99d7765",
            "title": "StructADMM: Achieving Ultrahigh Efficiency in Structured Pruning for DNNs",
            "abstract": "Weight pruning methods of deep neural networks (DNNs) have been demonstrated to achieve a good model pruning rate without loss of accuracy, thereby alleviating the significant computation/storage requirements of large-scale DNNs. Structured weight pruning methods have been proposed to overcome the limitation of irregular network structure and demonstrated actual GPU acceleration. However, in prior work, the pruning rate (degree of sparsity) and GPU acceleration are limited (to less than 50%) when accuracy needs to be maintained. In this work, we overcome these limitations by proposing a unified, systematic framework of structured weight pruning for DNNs. It is a framework that can be used to induce different types of structured sparsity, such as filterwise, channelwise, and shapewise sparsity, as well as nonstructured sparsity. The proposed framework incorporates stochastic gradient descent (SGD; or ADAM) with alternating direction method of multipliers (ADMM) and can be understood as a dynamic regularization method in which the regularization target is analytically updated in each iteration. Leveraging special characteristics of ADMM, we further propose a progressive, multistep weight pruning framework and a network purification and unused path removal procedure, in order to achieve higher pruning rate without accuracy loss. Without loss of accuracy on the AlexNet model, we achieve <inline-formula> <tex-math notation=\"LaTeX\">$2.58\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$3.65\\times $ </tex-math></inline-formula> average measured speedup on two GPUs, clearly outperforming the prior work. The average speedups reach <inline-formula> <tex-math notation=\"LaTeX\">$3.15\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$8.52\\times $ </tex-math></inline-formula> when allowing a moderate accuracy loss of 2%. In this case, the model compression for convolutional layers is <inline-formula> <tex-math notation=\"LaTeX\">$15.0\\times $ </tex-math></inline-formula>, corresponding to <inline-formula> <tex-math notation=\"LaTeX\">$11.93\\times $ </tex-math></inline-formula> measured CPU speedup. As another example, for the ResNet-18 model on the CIFAR-10 data set, we achieve an unprecedented <inline-formula> <tex-math notation=\"LaTeX\">$54.2\\times $ </tex-math></inline-formula> structured pruning rate on CONV layers. This is <inline-formula> <tex-math notation=\"LaTeX\">$32\\times $ </tex-math></inline-formula> higher pruning rate compared with recent work and can further translate into <inline-formula> <tex-math notation=\"LaTeX\">$7.6\\times $ </tex-math></inline-formula> inference time speedup on the Adreno 640 mobile GPU compared with the original, unpruned DNN model. We share our codes and models at the link <uri>http://bit.ly/2M0V7DO</uri>.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "35539001",
                    "name": "Shaokai Ye"
                },
                {
                    "authorId": "1992710",
                    "name": "Xiaoyu Feng"
                },
                {
                    "authorId": "151480882",
                    "name": "Xiaolong Ma"
                },
                {
                    "authorId": "145054784",
                    "name": "Kaiqi Zhang"
                },
                {
                    "authorId": "48459506",
                    "name": "Z. Li"
                },
                {
                    "authorId": "2115854503",
                    "name": "Jian Tang"
                },
                {
                    "authorId": "143743061",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "1662772707",
                    "name": "Xue Lin"
                },
                {
                    "authorId": "2442306",
                    "name": "Yongpan Liu"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                }
            ]
        },
        {
            "paperId": "851ad21f48e738eabf9ff0f809d48139ddcb6cad",
            "title": "Achieving on-Mobile Real-Time Super-Resolution with Neural Architecture and Pruning Search",
            "abstract": "Though recent years have witnessed remarkable progress in single image super-resolution (SISR) tasks with the prosperous development of deep neural networks (DNNs), the deep learning methods are confronted with the computation and memory consumption issues in practice, especially for resource-limited platforms such as mobile devices. To overcome the challenge and facilitate the real-time deployment of SISR tasks on mobile, we combine neural architecture search with pruning search and propose an automatic search framework that derives sparse super-resolution (SR) models with high image quality while satisfying the real-time inference requirement. To decrease the search cost, we leverage the weight sharing strategy by introducing a supernet and decouple the search problem into three stages, including supernet construction, compiler-aware architecture and pruning search, and compiler-aware pruning ratio search. With the proposed framework, we are the first to achieve real-time SR inference (with only tens of milliseconds per frame) for implementing 720p resolution with competitive image quality (in terms of PSNR and SSIM) on mobile platforms (Samsung Galaxy S20).",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2949135",
                    "name": "Zheng Zhan"
                },
                {
                    "authorId": "2114981469",
                    "name": "Yifan Gong"
                },
                {
                    "authorId": "31643513",
                    "name": "Pu Zhao"
                },
                {
                    "authorId": "9347641",
                    "name": "Geng Yuan"
                },
                {
                    "authorId": "48643324",
                    "name": "Wei Niu"
                },
                {
                    "authorId": "2048040696",
                    "name": "Yushu Wu"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "40371678",
                    "name": "Malith Jayaweera"
                },
                {
                    "authorId": "2081923342",
                    "name": "D. Kaeli"
                },
                {
                    "authorId": "2042633100",
                    "name": "Bin Ren"
                },
                {
                    "authorId": "1662772707",
                    "name": "Xue Lin"
                },
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                }
            ]
        },
        {
            "paperId": "9b74b1400c4cb009d0dbcd505f86a7f5eb0c94b7",
            "title": "Towards AQFP-Capable Physical Design Automation",
            "abstract": "Adiabatic Quantum-Flux-Parametron (AQFP) superconducting technology exhibits a high energy efficiency among superconducting electronics, however lacks effective design automation tools. In this work, we develop the first, efficient placement and routing framework for AQFP circuits considering the unique features and constraints, using MIT-LL technology as an example. Our proposed placement framework iteratively executes a fixed-order, row-wise placement algorithm, where the row-wise algorithm derives optimal solution with polynomial-time complexity. To address the maximum wirelength constraint issue in AQFP circuits, a whole row of buffers (or even more rows) is inserted. A* routing algorithm is adopted as the backbone algorithm, incorporating dynamic step size and net negotiation process to reduce the computational complexity accounting for AQFP characteristics, improving overall routability. Extensive experimental results demonstrate the effectiveness of our proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1391217910",
                    "name": "Hongjia Li"
                },
                {
                    "authorId": null,
                    "name": "Mengshu Sun"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "145390775",
                    "name": "O. Chen"
                },
                {
                    "authorId": "144255570",
                    "name": "N. Yoshikawa"
                },
                {
                    "authorId": "143676008",
                    "name": "Bei Yu"
                },
                {
                    "authorId": "2143431864",
                    "name": "Yanzhi Wang"
                },
                {
                    "authorId": "1787759",
                    "name": "Yibo Lin"
                }
            ]
        }
    ]
}