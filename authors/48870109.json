{
    "authorId": "48870109",
    "papers": [
        {
            "paperId": "7c8a6552fe0e3b33456afdac79614e7dc04f0b4d",
            "title": "ChemReasoner: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback",
            "abstract": "The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and reaction energy barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automatically guide the exploration without human input, providing competitive performance against expert-enumerated chemical descriptor-based implementations. By integrating language-guided reasoning with computational chemistry feedback, our work pioneers AI-accelerated, trustworthy catalyst discovery.",
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2190281477",
                    "name": "Henry W Sprueill"
                },
                {
                    "authorId": "48870109",
                    "name": "Carl N. Edwards"
                },
                {
                    "authorId": "39073194",
                    "name": "Khushbu Agarwal"
                },
                {
                    "authorId": "15751638",
                    "name": "Mariefel V. Olarte"
                },
                {
                    "authorId": "37794737",
                    "name": "Udishnu Sanyal"
                },
                {
                    "authorId": "2284668995",
                    "name": "Conrad Johnston"
                },
                {
                    "authorId": "2284730320",
                    "name": "Hongbin Liu"
                },
                {
                    "authorId": "2264201811",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "7617146",
                    "name": "Sutanay Choudhury"
                }
            ]
        },
        {
            "paperId": "d36a247eec7e1e48de618993c9272af92aeeca36",
            "title": "GLaD: Synergizing Molecular Graphs and Language Descriptors for Enhanced Power Conversion Efficiency Prediction in Organic Photovoltaic Devices",
            "abstract": "This paper presents a novel approach for predicting Power Conversion Efficiency (PCE) of Organic Photovoltaic (OPV) devices, called GLaD: synergizing molecular Graphs and Language Descriptors for enhanced PCE prediction. Due to the lack of high-quality experimental data, we collect a dataset consisting of 500 pairs of OPV donor and acceptor molecules along with their corresponding PCE values, which we utilize as the training data for our predictive model. In this low-data regime, GLaD leverages properties learned from large language models (LLMs) pretrained on extensive scientific literature to enrich molecular structural representations, allowing for a multimodal representation of molecules. GLaD achieves precise predictions of PCE, thereby facilitating the synthesis of new OPV molecules with improved efficiency. Furthermore, GLaD showcases versatility, as it applies to a range of molecular property prediction tasks (BBBP, BACE, ClinTox, and SIDER), not limited to those concerning OPV materials. Especially, GLaD proves valuable for tasks in low-data regimes within the chemical space, as it enriches molecular representations by incorporating molecular property descriptions learned from large-scale pretraining. This capability is significant in real-world scientific endeavors like drug and material discovery, where access to comprehensive data is crucial for informed decision-making and efficient exploration of the chemical space.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2302813601",
                    "name": "Thao Nguyen"
                },
                {
                    "authorId": "2302796463",
                    "name": "Tiara Torres-Flores"
                },
                {
                    "authorId": "2082458918",
                    "name": "Changhyun Hwang"
                },
                {
                    "authorId": "48870109",
                    "name": "Carl N. Edwards"
                },
                {
                    "authorId": "2302799040",
                    "name": "Ying Diao"
                },
                {
                    "authorId": "2290170233",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "db8b335f5286f9173e4d2315044575f6af9ded68",
            "title": "Geometry Informed Tokenization of Molecules for Language Model Generation",
            "abstract": "We consider molecule generation in 3D space using language models (LMs), which requires discrete tokenization of 3D molecular geometries. Although tokenization of molecular graphs exists, that for 3D geometries is largely unexplored. Here, we attempt to bridge this gap by proposing the Geo2Seq, which converts molecular geometries into $SE(3)$-invariant 1D discrete sequences. Geo2Seq consists of canonical labeling and invariant spherical representation steps, which together maintain geometric and atomic fidelity in a format conducive to LMs. Our experiments show that, when coupled with Geo2Seq, various LMs excel in molecular geometry generation, especially in controlled generation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118053386",
                    "name": "Xiner Li"
                },
                {
                    "authorId": "2109120459",
                    "name": "Limei Wang"
                },
                {
                    "authorId": "2004524780",
                    "name": "Youzhi Luo"
                },
                {
                    "authorId": "48870109",
                    "name": "Carl N. Edwards"
                },
                {
                    "authorId": "1914700964",
                    "name": "Shurui Gui"
                },
                {
                    "authorId": "2107966079",
                    "name": "Yu-Ching Lin"
                },
                {
                    "authorId": "2316534140",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2279225650",
                    "name": "Shuiwang Ji"
                }
            ]
        },
        {
            "paperId": "e855963bdaa7a3b46d9a79707d80795f06fbf1ed",
            "title": "L+M-24: Building a Dataset for Language+Molecules @ ACL 2024",
            "abstract": "Language-molecule models have emerged as an exciting direction for molecular discovery and understanding. However, training these models is challenging due to the scarcity of molecule-language pair datasets. At this point, datasets have been released which are 1) small and scraped from existing databases, 2) large but noisy and constructed by performing entity linking on the scientific literature, and 3) built by converting property prediction datasets to natural language using templates. In this document, we detail the L+M-24 dataset, which has been created for the Language + Molecules Workshop shared task at ACL 2024. In particular, L+M-24 is designed to focus on three key benefits of natural language in molecule design: compositionality, functionality, and abstraction",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "48870109",
                    "name": "Carl N. Edwards"
                },
                {
                    "authorId": "1786863",
                    "name": "Qingyun Wang"
                },
                {
                    "authorId": "2289900563",
                    "name": "Lawrence Zhao"
                },
                {
                    "authorId": "2290170233",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "2129c6edc2593bf4adb5bc2772fdb042bdf14070",
            "title": "Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design",
            "abstract": "Discovering novel catalysts requires complex reasoning involving multiple chemical properties and resultant trade-offs, leading to a combinatorial growth in the search space. While large language models (LLM) have demonstrated novel capabilities for chemistry through complex instruction following capabilities and high quality reasoning, a goal-driven combinatorial search using LLMs has not been explored in detail. In this work, we present a Monte Carlo Tree Search-based approach that improves beyond state-of-the-art chain-of-thought prompting variants to augment scientific reasoning. We introduce two new reasoning datasets: 1) a curation of computational chemistry simulations, and 2) diverse questions written by catalysis researchers for reasoning about novel chemical conversion processes. We improve over the best baseline by 25.8\\% and find that our approach can augment scientist's reasoning and discovery process with novel insights.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2190281477",
                    "name": "Henry W Sprueill"
                },
                {
                    "authorId": "48870109",
                    "name": "Carl N. Edwards"
                },
                {
                    "authorId": "15751638",
                    "name": "Mariefel V. Olarte"
                },
                {
                    "authorId": "37794737",
                    "name": "Udishnu Sanyal"
                },
                {
                    "authorId": "2264201811",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "7617146",
                    "name": "Sutanay Choudhury"
                }
            ]
        },
        {
            "paperId": "316a011bf461d3a96965fb9f69398888da19bd9f",
            "title": "SynerGPT: In-Context Learning for Personalized Drug Synergy Prediction and Drug Design",
            "abstract": "Predicting synergistic drug combinations can help accelerate discovery of cancer treatments, particularly therapies personalized to a patient\u2019s specific tumor via biopsied cells. In this paper, we propose a novel setting and models for in-context drug synergy learning. We are given a small \u201cpersonalized dataset\u201d of 10-20 drug synergy relationships in the context of specific cancer cell targets. Our goal is to predict additional drug synergy relationships in that context. Inspired by recent work that pre-trains a GPT language model (LM) to \u201cin-context learn\u201d common function classes, we devise novel pre-training schemes that enable a GPT model to in-context learn \u201cdrug synergy functions\u201d. Our model\u2014which does not use any textual corpora, molecular fingerprints, protein interaction or any other domain-specific knowledge\u2014 is able to achieve competitive results. We further integrate our in-context approach with a genetic algorithm to optimize model prompts and select synergy candidates to test after conducting a patient biopsy. Finally, we explore a novel task of inverse drug design which can potentially enable the design of drugs that synergize specifically to target a given patient\u2019s \u201cpersonalized dataset\u201d. Our findings can potentially have an important impact on precision cancer medicine, and also raise intriguing questions on non-textual pre-training for LMs.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "48870109",
                    "name": "Carl N. Edwards"
                },
                {
                    "authorId": "23175870",
                    "name": "Aakanksha Naik"
                },
                {
                    "authorId": "2236429",
                    "name": "Tushar Khot"
                },
                {
                    "authorId": "2199258773",
                    "name": "Martin Burke"
                },
                {
                    "authorId": "2181650518",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2041698667",
                    "name": "Tom Hope"
                }
            ]
        },
        {
            "paperId": "5402c22369d0190d0a002b7a1222d403edae010a",
            "title": "Defining a New NLP Playground",
            "abstract": "The recent explosion of performance of large language models (LLMs) has changed the field of Natural Language Processing (NLP) more abruptly and seismically than any other shift in the field's 80-year history. This has resulted in concerns that the field will become homogenized and resource-intensive. The new status quo has put many academic researchers, especially PhD students, at a disadvantage. This paper aims to define a new NLP playground by proposing 20+ PhD-dissertation-worthy research directions, covering theoretical analysis, new and challenging problems, learning paradigms, and interdisciplinary applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2263798673",
                    "name": "Sha Li"
                },
                {
                    "authorId": "2118642562",
                    "name": "Chi Han"
                },
                {
                    "authorId": "144808890",
                    "name": "Pengfei Yu"
                },
                {
                    "authorId": "48870109",
                    "name": "Carl N. Edwards"
                },
                {
                    "authorId": "2118482058",
                    "name": "Manling Li"
                },
                {
                    "authorId": "2144803999",
                    "name": "Xingyao Wang"
                },
                {
                    "authorId": "51135899",
                    "name": "Y. Fung"
                },
                {
                    "authorId": "2263891451",
                    "name": "Charles Yu"
                },
                {
                    "authorId": "2249230172",
                    "name": "Joel R. Tetreault"
                },
                {
                    "authorId": "2257189311",
                    "name": "Eduard H. Hovy"
                },
                {
                    "authorId": "2264201811",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "d1f49ea7287120bb7a1e214862388222ce76d9e8",
            "title": "Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems",
            "abstract": "Advances in artificial intelligence (AI) are fueling a new paradigm of discoveries in natural sciences. Today, AI has started to advance natural sciences by improving, accelerating, and enabling our understanding of natural phenomena at a wide range of spatial and temporal scales, giving rise to a new area of research known as AI for science (AI4Science). Being an emerging research paradigm, AI4Science is unique in that it is an enormous and highly interdisciplinary area. Thus, a unified and technical treatment of this field is needed yet challenging. This work aims to provide a technically thorough account of a subarea of AI4Science; namely, AI for quantum, atomistic, and continuum systems. These areas aim at understanding the physical world from the subatomic (wavefunctions and electron density), atomic (molecules, proteins, materials, and interactions), to macro (fluids, climate, and subsurface) scales and form an important subarea of AI4Science. A unique advantage of focusing on these areas is that they largely share a common set of challenges, thereby allowing a unified and foundational treatment. A key common challenge is how to capture physics first principles, especially symmetries, in natural systems by deep learning methods. We provide an in-depth yet intuitive account of techniques to achieve equivariance to symmetry transformations. We also discuss other common technical challenges, including explainability, out-of-distribution generalization, knowledge transfer with foundation and large language models, and uncertainty quantification. To facilitate learning and education, we provide categorized lists of resources that we found to be useful. We strive to be thorough and unified and hope this initial effort may trigger more community interests and efforts to further advance AI4Science.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2155975186",
                    "name": "Xuan Zhang"
                },
                {
                    "authorId": "2109120459",
                    "name": "Limei Wang"
                },
                {
                    "authorId": "2211730572",
                    "name": "Jacob Helwig"
                },
                {
                    "authorId": "2004524780",
                    "name": "Youzhi Luo"
                },
                {
                    "authorId": "2084647086",
                    "name": "Cong Fu"
                },
                {
                    "authorId": "14629242",
                    "name": "Yaochen Xie"
                },
                {
                    "authorId": "38813990",
                    "name": "Meng Liu"
                },
                {
                    "authorId": "2107966079",
                    "name": "Yu-Ching Lin"
                },
                {
                    "authorId": "2115510017",
                    "name": "Zhao Xu"
                },
                {
                    "authorId": "1879114760",
                    "name": "Keqiang Yan"
                },
                {
                    "authorId": "95261912",
                    "name": "Keir Adams"
                },
                {
                    "authorId": "47927975",
                    "name": "Maurice Weiler"
                },
                {
                    "authorId": "2118053386",
                    "name": "Xiner Li"
                },
                {
                    "authorId": "2427076",
                    "name": "Tianfan Fu"
                },
                {
                    "authorId": "2204444421",
                    "name": "Yucheng Wang"
                },
                {
                    "authorId": "2119316118",
                    "name": "Haiyang Yu"
                },
                {
                    "authorId": "2118597356",
                    "name": "Yuqing Xie"
                },
                {
                    "authorId": "2119032930",
                    "name": "Xiang Fu"
                },
                {
                    "authorId": "103618574",
                    "name": "A. Strasser"
                },
                {
                    "authorId": "50433504",
                    "name": "Shenglong Xu"
                },
                {
                    "authorId": "2153630672",
                    "name": "Yi Liu"
                },
                {
                    "authorId": "93584228",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "2223648674",
                    "name": "Alexandra Saxton"
                },
                {
                    "authorId": "2055943899",
                    "name": "Hongyi Ling"
                },
                {
                    "authorId": "46877850",
                    "name": "Hannah Lawrence"
                },
                {
                    "authorId": "2124211700",
                    "name": "Hannes St\u00e4rk"
                },
                {
                    "authorId": "1914700964",
                    "name": "Shurui Gui"
                },
                {
                    "authorId": "48870109",
                    "name": "Carl N. Edwards"
                },
                {
                    "authorId": "2068203542",
                    "name": "Nicholas Gao"
                },
                {
                    "authorId": "2036602766",
                    "name": "A. Ladera"
                },
                {
                    "authorId": "3716141",
                    "name": "Tailin Wu"
                },
                {
                    "authorId": "146613894",
                    "name": "E. Hofgard"
                },
                {
                    "authorId": "90258499",
                    "name": "A. M. Tehrani"
                },
                {
                    "authorId": "2151036085",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "102488653",
                    "name": "Ameya Daigavane"
                },
                {
                    "authorId": "2223648322",
                    "name": "Montgomery Bohde"
                },
                {
                    "authorId": "2179190141",
                    "name": "Jerry Kurtin"
                },
                {
                    "authorId": "2111287017",
                    "name": "Qiang Huang"
                },
                {
                    "authorId": "2223648653",
                    "name": "Tuong Phung"
                },
                {
                    "authorId": "1490886784",
                    "name": "Minkai Xu"
                },
                {
                    "authorId": "38009979",
                    "name": "Chaitanya K. Joshi"
                },
                {
                    "authorId": "2054235048",
                    "name": "Simon V. Mathis"
                },
                {
                    "authorId": "3371922",
                    "name": "K. Azizzadenesheli"
                },
                {
                    "authorId": "2030142647",
                    "name": "Ada Fang"
                },
                {
                    "authorId": "1422193589",
                    "name": "A. Aspuru\u2010Guzik"
                },
                {
                    "authorId": "2231179",
                    "name": "E. Bekkers"
                },
                {
                    "authorId": "2149583375",
                    "name": "Michael M. Bronstein"
                },
                {
                    "authorId": "2095762",
                    "name": "M. Zitnik"
                },
                {
                    "authorId": "2047844",
                    "name": "Anima Anandkumar"
                },
                {
                    "authorId": "2490652",
                    "name": "Stefano Ermon"
                },
                {
                    "authorId": "2075355155",
                    "name": "Pietro Lio'"
                },
                {
                    "authorId": "2151886670",
                    "name": "Rose Yu"
                },
                {
                    "authorId": "51249380",
                    "name": "Stephan Gunnemann"
                },
                {
                    "authorId": "1702139",
                    "name": "J. Leskovec"
                },
                {
                    "authorId": "144016781",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "49991208",
                    "name": "Jimeng Sun"
                },
                {
                    "authorId": "1741283",
                    "name": "R. Barzilay"
                },
                {
                    "authorId": "35132120",
                    "name": "T. Jaakkola"
                },
                {
                    "authorId": "13027820",
                    "name": "Connor W. Coley"
                },
                {
                    "authorId": "2067730514",
                    "name": "Xiaoning Qian"
                },
                {
                    "authorId": "2198715748",
                    "name": "Xiaofeng Qian"
                },
                {
                    "authorId": "5485763",
                    "name": "T. Smidt"
                },
                {
                    "authorId": "1743600",
                    "name": "Shuiwang Ji"
                }
            ]
        },
        {
            "paperId": "3b9b1aba877ecd3f7e508cbc78a41b623349902b",
            "title": "Translation between Molecules and Natural Language",
            "abstract": "We present MolT5 - a self-supervised learning framework for pretraining models on a vast amount of unlabeled natural language text and molecule strings. MolT5 allows for new, useful, and challenging analogs of traditional vision-language tasks, such as molecule captioning and text-based de novo molecule generation (altogether: translation between molecules and language), which we explore for the first time. Since MolT5 pretrains models on single-modal data, it helps overcome the chemistry domain shortcoming of data scarcity. Furthermore, we consider several metrics, including a new cross-modal embedding-based metric, to evaluate the tasks of molecule captioning and text-based molecule generation. Our results show that MolT5-based models are able to generate outputs, both molecules and captions, which in many cases are high quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48870109",
                    "name": "Carl N. Edwards"
                },
                {
                    "authorId": "145242558",
                    "name": "T. Lai"
                },
                {
                    "authorId": "79769007",
                    "name": "Kevin Ros"
                },
                {
                    "authorId": "3359924",
                    "name": "Garrett Honke"
                },
                {
                    "authorId": "2072975661",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "6780df51300f81e355f4d00db7589b8d89ef9525",
            "title": "Semi-supervised New Event Type Induction and Description via Contrastive Loss-Enforced Batch Attention",
            "abstract": "Most event extraction methods have traditionally relied on an annotated set of event types. However, creating event ontologies and annotating supervised training data are expensive and time-consuming. Previous work has proposed semi-supervised approaches which leverage seen (annotated) types to learn how to automatically discover new event types. State-of-the-art methods, both semi-supervised or fully unsupervised, use a form of reconstruction loss on specific tokens in a context. In contrast, we present a novel approach to semi-supervised new event type induction using a masked contrastive loss, which learns similarities between event mentions by enforcing an attention mechanism over the data minibatch. We further disentangle the discovered clusters by approximating the underlying manifolds in the data, which allows us to achieve an adjusted rand index score of 48.85%. Building on these clustering results, we extend our approach to two new tasks: predicting the type name of the discovered clusters and linking them to FrameNet frames.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48870109",
                    "name": "Carl N. Edwards"
                },
                {
                    "authorId": "2113323573",
                    "name": "Heng Ji"
                }
            ]
        }
    ]
}