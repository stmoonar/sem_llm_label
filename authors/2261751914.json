{
    "authorId": "2261751914",
    "papers": [
        {
            "paperId": "28d6073418c75c23c110e9afaa9f82c66d61e84b",
            "title": "EmpathyEar: An Open-source Avatar Multimodal Empathetic Chatbot",
            "abstract": "This paper introduces EmpathyEar, a pioneering open-source, avatar-based multimodal empathetic chatbot, to fill the gap in traditional text-only empathetic response generation (ERG) systems. Leveraging the advancements of a large language model, combined with multimodal encoders and generators, EmpathyEar supports user inputs in any combination of text, sound, and vision, and produces multimodal empathetic responses, offering users, not just textual responses but also digital avatars with talking faces and synchronized speeches. A series of emotion-aware instruction-tuning is performed for comprehensive emotional understanding and generation capabilities. In this way, EmpathyEar provides users with responses that achieve a deeper emotional resonance, closely emulating human-like empathy. The system paves the way for the next emotional intelligence, for which we open-source the code for public access.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2303470150",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2307939465",
                    "name": "Han Zhang"
                },
                {
                    "authorId": "2256856981",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2261751914",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "2307418239",
                    "name": "Qian Liu"
                },
                {
                    "authorId": "2297194681",
                    "name": "Erik Cambria"
                }
            ]
        },
        {
            "paperId": "48aaa0155ecc9bdd5971ced96e5693b2cbc40edd",
            "title": "Actively Learn from LLMs with Uncertainty Propagation for Generalized Category Discovery",
            "abstract": "Generalized category discovery faces a key issue: the lack of supervision for new and unseen data categories. Traditional methods typically combine supervised pretraining with self-supervised learning to create models, and then employ clustering for category identification. However, these approaches tend to become overly tailored to known categories, failing to fully resolve the core issue. Hence, we propose to integrate the feedback from LLMs into an active learning paradigm. Specifically, our method innovatively employs uncertainty propagation to select data samples from high-uncertainty regions, which are then labeled using LLMs through a comparison-based prompting scheme. This not only eases the labeling task but also enhances accuracy in identifying new categories. Additionally, a soft feedback propagation mechanism is introduced to minimize the spread of inaccurate feedback. Experiments on various datasets demonstrate our framework\u2019s efficacy and generalizability, significantly improving baseline models at a nominal average cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2205539393",
                    "name": "Jinggui Liang"
                },
                {
                    "authorId": "2261751914",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "2294780338",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2132446579",
                    "name": "Bobo Li"
                },
                {
                    "authorId": "2294818776",
                    "name": "Jing Jiang"
                }
            ]
        },
        {
            "paperId": "48ebf17b6ec5e55f66b6053265761805130ea7cd",
            "title": "Synergizing Large Language Models and Pre-Trained Smaller Models for Conversational Intent Discovery",
            "abstract": "In Conversational Intent Discovery (CID), Small Language Models (SLMs) struggle with overfitting to familiar intents and fail to label newly discovered ones. This issue stems from their limited grasp of semantic nuances and their intrinsically discriminative framework. Therefore, we propose Syn ergizing Large Language Models (LLMs) with pre-trained SLMs for CID ( SynCID ). It harnesses the profound semantic comprehension of LLMs alongside the operational agility of SLMs. By utilizing LLMs to refine both utterances and existing intent labels, SynCID significantly enhances the semantic depth, subsequently realigning these enriched descriptors within the SLMs\u2019 feature space to correct cluster distortion and promote robust learning of representations. A key advantage is its capacity for the early identification of new intents, a critical aspect for deploying conversational agents successfully. Additionally, SynCID leverages the in-context learning strengths of LLMs to generate labels for new intents. Thorough evaluations across a wide array of datasets have demonstrated its superior performance over traditional CID methods. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2205539393",
                    "name": "Jinggui Liang"
                },
                {
                    "authorId": "2261751914",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "2316926663",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2294818776",
                    "name": "Jing Jiang"
                }
            ]
        },
        {
            "paperId": "7d77fb19298e39a5d88c224b1df8e5eba0be99d4",
            "title": "Harnessing Holistic Discourse Features and Triadic Interaction for Sentiment Quadruple Extraction in Dialogues",
            "abstract": "Dialogue Aspect-based Sentiment Quadruple (DiaASQ) is a newly-emergent task aiming to extract the sentiment quadruple (i.e., targets, aspects, opinions, and sentiments) from conversations. While showing promising performance, the prior DiaASQ approach unfortunately falls prey to the key crux of DiaASQ, including insufficient modeling of discourse features, and lacking quadruple extraction, which hinders further task improvement. To this end, we introduce a novel framework that not only capitalizes on comprehensive discourse feature modeling, but also captures the intrinsic interaction for optimal quadruple extraction. On the one hand, drawing upon multiple discourse features, our approach constructs a token-level heterogeneous graph and enhances token interactions through a heterogeneous attention network. We further propose a novel triadic scorer, strengthening weak token relations within a quadruple, thereby enhancing the cohesion of the quadruple extraction. Experimental results on the DiaASQ benchmark showcase that our model significantly outperforms existing baselines across both English and Chinese datasets. Our code is available at https://bit.ly/3v27pqA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2132446579",
                    "name": "Bobo Li"
                },
                {
                    "authorId": "46959445",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2261751914",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "2293562548",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2159755533",
                    "name": "Fangfang Su"
                },
                {
                    "authorId": "2109530930",
                    "name": "Fei Li"
                },
                {
                    "authorId": "2237222852",
                    "name": "Donghong Ji"
                }
            ]
        },
        {
            "paperId": "06673e37f8d8a515d6fe26511dd1fbae69a9237d",
            "title": "Partial Annotation-based Video Moment Retrieval via Iterative Learning",
            "abstract": "Given a descriptive language query, Video Moment Retrieval (VMR) aims to seek the corresponding semantic-consistent moment clip in the video, which is represented as a pair of the start and end timestamps. Although current methods have achieved satisfying performance, training these models heavily relies on the fully-annotated VMR datasets. Nonetheless, precise video temporal annotations are extremely labor-intensive and ambiguous due to the diverse preferences of different annotators. Although there are several works trying to explore weakly supervised VMR tasks with scattered annotated frames as labels, there is still much room to improve in terms of accuracy. Therefore, we design a new setting of VMR where users can easily point to small segments of non-controversy video moments and our proposed method can automatically fill in the remaining parts based on the video and query semantics. To support this, we propose a new framework named Video Moment Retrieval via Iterative Learning (VMRIL). It treats the partial temporal region as the seed, then expands the pseudo label by iterative training. In order to restrict the expansion with reasonable boundaries, we utilize a pretrained video action localization model to provide coarse guidance of potential video segments. Compared with other VMR methods, our VMRIL achieves a trade-off between satisfying performance and annotation efficiency. Experimental results show that our proposed method can achieve the SOTA performance in the weakly supervised VMR setting, and are even comparable with some fully-supervised VMR methods but with much less annotation cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261791648",
                    "name": "Wei Ji"
                },
                {
                    "authorId": "2261748535",
                    "name": "Renjie Liang"
                },
                {
                    "authorId": "2261751914",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "2261750082",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "2fec56baadace9788b6a814a55e6193086c9f72d",
            "title": "ClusterPrompt: Cluster Semantic Enhanced Prompt Learning for New Intent Discovery",
            "abstract": "The discovery of new intent categories from user utterances is a crucial task in expanding agent skills. The key lies in how to efficiently solicit semantic evidence from utterances and properly transfer knowledge from existing intents to new intents. However, previous methods laid too much emphasis on relations among utterances or clusters for transfer learning, while paying less attention to the usage of semantics. As a result, these methods suffer from in-domain over-fitting and often generate meaningless new intent clusters due to data distortion. In this paper, we present a novel approach called Cluster Semantic Enhanced Prompt Learning (CsePL) for discovering new intents. Our method leverages two-level contrastive learning with label semantic alignment to learn meaningful representations of intent clusters. These learned intent representations are then utilized as soft prompt initializations for discriminating new intents, reducing the dominance of existing intents. Extensive experiments conducted on three public datasets demonstrate the superiority of our proposed method. It not only outperforms existing meth-ods but also suggests meaningful intent labels and enables early detection of new intents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2205539393",
                    "name": "Jinggui Liang"
                },
                {
                    "authorId": "2261751914",
                    "name": "Lizi Liao"
                }
            ]
        },
        {
            "paperId": "9085e72053b8e1295e1641276682f0bc1463d8b1",
            "title": "Reverse Multi-Choice Dialogue Commonsense Inference with Graph-of-Thought",
            "abstract": "With the proliferation of dialogic data across the Internet, the Dialogue Commonsense Multi-choice Question Answering (DC-MCQ) task has emerged as a response to the challenge of comprehending user queries and intentions.\nAlthough prevailing methodologies exhibit effectiveness in addressing single-choice questions, they encounter difficulties in handling multi-choice queries due to the heightened intricacy and informational density. \nIn this paper, inspired by the human cognitive process of progressively excluding options, we propose a three-step Reverse Exclusion Graph-of-Thought (ReX-GoT) framework, including Option Exclusion, Error Analysis, and Combine Information.\nSpecifically, our ReX-GoT mimics human reasoning by gradually excluding irrelevant options and learning the reasons for option errors to choose the optimal path of the GoT and ultimately infer the correct answer.\nBy progressively integrating intricate clues, our method effectively reduces the difficulty of multi-choice reasoning and provides a novel solution for DC-MCQ.\nExtensive experiments on the CICERO and CICERO_v2 datasets validate the significant improvement of our approach on DC-MCQ task.\nOn zero-shot setting, our model outperform the best baseline by 17.67% in terms of F1 score for the multi-choice task.\nMost strikingly, our GPT3.5-based ReX-GoT framework achieves a remarkable 39.44% increase in F1 score.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149972134",
                    "name": "Limin Zheng"
                },
                {
                    "authorId": "46959445",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2266836320",
                    "name": "Fei Li"
                },
                {
                    "authorId": "2132446579",
                    "name": "Bobo Li"
                },
                {
                    "authorId": "2261751914",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "2237222852",
                    "name": "Donghong Ji"
                },
                {
                    "authorId": "1679617",
                    "name": "Chong Teng"
                }
            ]
        }
    ]
}