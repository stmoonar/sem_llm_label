{
    "authorId": "2110797784",
    "papers": [
        {
            "paperId": "4df8f1b032ca763c9dc73ec45a2d9c57571eb76f",
            "title": "Detecting Frames in News Headlines and Lead Images in U.S. Gun Violence Coverage",
            "abstract": "News media structure their reporting of events or issues using certain perspectives. When describing an incident involving gun violence, for example, some journalists may focus on mental health or gun regulation, while others may emphasize the discussion of gun rights. Such perspectives are called \\say{frames} in communication research. We study, for the first time, the value of combining lead images and their contextual information with text to identify the frame of a given news article. We observe that using multiple modes of information(article- and image-derived features) improves prediction of news frames over any single mode of information when the images are relevant to the frames of the headlines. We also observe that frame image relevance is related to the ease of conveying frames via images, which we call frame concreteness. Additionally, we release the first multimodal news framing dataset related to gun violence in the U.S., curated and annotated by communication researchers. The dataset will allow researchers to further examine the use of multiple information modalities for studying media framing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72437140",
                    "name": "Isidora Chara Tourni"
                },
                {
                    "authorId": "2110797784",
                    "name": "Lei Guo"
                },
                {
                    "authorId": "2140488680",
                    "name": "T. Daryanto"
                },
                {
                    "authorId": "2140490515",
                    "name": "Fabian Zhafransyah"
                },
                {
                    "authorId": "2065594140",
                    "name": "Edward Edberg Halim"
                },
                {
                    "authorId": "47801182",
                    "name": "Mona Jalal"
                },
                {
                    "authorId": "2882606",
                    "name": "Boqi Chen"
                },
                {
                    "authorId": "98229497",
                    "name": "Shan-Ching Lai"
                },
                {
                    "authorId": "2142605154",
                    "name": "Hengchang Hu"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                },
                {
                    "authorId": "1756038",
                    "name": "P. Ishwar"
                },
                {
                    "authorId": "2129412",
                    "name": "D. Wijaya"
                }
            ]
        },
        {
            "paperId": "91b0b1c7b4378a7a5a21bf88fa86b1fefe80c4e3",
            "title": "DA-DAN: A Dual Adversarial Domain Adaption Network for Unsupervised Non-overlapping Cross-domain Recommendation",
            "abstract": "Unsupervised Non-overlapping Cross-domain Recommendation (UNCR) is the task that recommends source domain items to the target domain users, which is more challenging as the users are non-overlapped, and its learning process is unsupervised. Unsupervised Non-overlapping Cross-domain Recommendation UNCR is still unsolved due to the following: (1) Previous studies need extra auxiliary information to learn transferable features when aligning two domains, which is unrealistic and hard to obtain due to privacy concerns. (2) Since the adoption of the shared network, existing works cannot well eliminate the domain-specific features in the common feature space, which may incorporate domain noise and harm the cross-domain recommendation. In this work, we propose a domain adaption-based method, namely DA-DAN, to address the above challenges. Specifically, to let DA-DAN be free of auxiliary information, we learn users\u2019 preferences by only exploring their sequential patterns, and propose an improved self-attention layer to model them. To well eliminate the domain-specific features from the common feature space, we resort to a dual generative adversarial network with a multi-target adversarial loss, where two generators and discriminators are leveraged to model each domain separately. Experimental results on three real-world datasets demonstrate the advantage of DA-DAN compared with the state-of-the-art recommendation baselines. Moreover, our source codes have been publicly released.1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110797784",
                    "name": "Lei Guo"
                },
                {
                    "authorId": "2143855553",
                    "name": "Hao Liu"
                },
                {
                    "authorId": "145081293",
                    "name": "Lei Zhu"
                },
                {
                    "authorId": "15880069",
                    "name": "Weili Guan"
                },
                {
                    "authorId": "13167100",
                    "name": "Zhiyong Cheng"
                }
            ]
        },
        {
            "paperId": "11b41e95a4a9368588a42643fccfb08de03049cb",
            "title": "BU-NEmo: an Affective Dataset of Gun Violence News",
            "abstract": "Given our society\u2019s increased exposure to multimedia formats on social media platforms, efforts to understand how digital content impacts people\u2019s emotions are burgeoning. As such, we introduce a U.S. gun violence news dataset that contains news headline and image pairings from 840 news articles with 15K high-quality, crowdsourced annotations on emotional responses to the news pairings. We created three experimental conditions for the annotation process: two with a single modality (headline or image only), and one multimodal (headline and image together). In contrast to prior works on affectively-annotated data, our dataset includes annotations on the dominant emotion experienced with the content, the intensity of the selected emotion and an open-ended, written component. By collecting annotations on different modalities of the same news content pairings, we explore the relationship between image and text influence on human emotional response. We offer initial analysis on our dataset, showing the nuanced affective differences that appear due to modality and individual factors such as political leaning and media consumption habits. Our dataset is made publicly available to facilitate future research in affective computing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2174214381",
                    "name": "Carley Reardon"
                },
                {
                    "authorId": "2132933809",
                    "name": "Sejin Paik"
                },
                {
                    "authorId": "2154652618",
                    "name": "Ge Gao"
                },
                {
                    "authorId": "114769212",
                    "name": "Meet Parekh"
                },
                {
                    "authorId": "2174367978",
                    "name": "Yanling Zhao"
                },
                {
                    "authorId": "2110797784",
                    "name": "Lei Guo"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                },
                {
                    "authorId": "2129412",
                    "name": "D. Wijaya"
                }
            ]
        },
        {
            "paperId": "51d62559e8ef363865f4c171bb3f929e312ffd4b",
            "title": "Prediction of People\u2019s Emotional Response towards Multi-modal News",
            "abstract": "We aim to develop methods for understanding how multimedia news exposure can affect people\u2019s emotional responses, and we especially focus on news content related to gun violence, a very important yet polarizing issue in the U.S. We created the dataset NEmo+ by significantly extending the U.S. gun violence news-to-emotions dataset, BU-NEmo, from 320 to 1,297 news headline and lead image pairings and collecting 38,910 annotations in a large crowdsourcing experiment. In curating the NEmo+ dataset, we developed methods to identify news items that will trigger similar versus divergent emotional responses. For news items that trigger similar emotional responses, we compiled them into the NEmo+-Consensus dataset. We benchmark models on this dataset that predict a person\u2019s dominant emotional response toward the target news item (single-label prediction). On the full NEmo+ dataset, containing news items that would lead to both differing and similar emotional responses, we also benchmark models for the novel task of predicting the distribution of evoked emotional responses in humans when presented with multi-modal news content. Our single-label and multi-label prediction models outperform baselines by large margins across several metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2154652618",
                    "name": "Ge Gao"
                },
                {
                    "authorId": "2132933809",
                    "name": "Sejin Paik"
                },
                {
                    "authorId": "2174214381",
                    "name": "Carley Reardon"
                },
                {
                    "authorId": "2174367978",
                    "name": "Yanling Zhao"
                },
                {
                    "authorId": "2110797784",
                    "name": "Lei Guo"
                },
                {
                    "authorId": "1756038",
                    "name": "P. Ishwar"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                },
                {
                    "authorId": "2129412",
                    "name": "D. Wijaya"
                }
            ]
        },
        {
            "paperId": "ef1254bd0630b3d826371f941d4dfc9382e9a071",
            "title": "An Unsupervised Approach to Discover Media Frames",
            "abstract": "Media framing refers to highlighting certain aspect of an issue in the news to promote a particular interpretation to the audience. Supervised learning has often been used to recognize frames in news articles, requiring a known pool of frames for a particular issue, which must be identified by communication researchers through thorough manual content analysis. In this work, we devise an unsupervised learning approach to discover the frames in news articles automatically. Given a set of news articles for a given issue, e.g., gun violence, our method first extracts frame elements from these articles using related Wikipedia articles and the Wikipedia category system. It then uses a community detection approach to identify frames from these frame elements. We discuss the effectiveness of our approach by comparing the frames it generates in an unsupervised manner to the domain-expert-derived frames for the issue of gun violence, for which a supervised learning model for frame recognition exists.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1998921",
                    "name": "Sha Lai"
                },
                {
                    "authorId": "144824029",
                    "name": "Yanru Jiang"
                },
                {
                    "authorId": "2110797784",
                    "name": "Lei Guo"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                },
                {
                    "authorId": "1756038",
                    "name": "P. Ishwar"
                },
                {
                    "authorId": "2129412",
                    "name": "D. Wijaya"
                }
            ]
        },
        {
            "paperId": "22b3ac617e9f094cc54ba1c650382ae06be3b11f",
            "title": "Hierarchical Hyperedge Embedding-Based Representation Learning for Group Recommendation",
            "abstract": "Group recommendation aims to recommend items to a group of users. In this work, we study group recommendation in a particular scenario, namely occasional group recommendation, where groups are formed ad hoc and users may just constitute a group for the first time\u2014that is, the historical group-item interaction records are highly limited. Most state-of-the-art works have addressed the challenge by aggregating group members\u2019 personal preferences to learn the group representation. However, the representation learning for a group is most complex beyond the aggregation or fusion of group member representation, as the personal preferences and group preferences may be in different spaces and even orthogonal. In addition, the learned user representation is not accurate due to the sparsity of users\u2019 interaction data. Moreover, the group similarity in terms of common group members has been overlooked, which, however, has the great potential to improve the group representation learning. In this work, we focus on addressing the aforementioned challenges in the group representation learning task, and devise a hierarchical hyperedge embedding-based group recommender, namely HyperGroup. Specifically, we propose to leverage the user-user interactions to alleviate the sparsity issue of user-item interactions, and design a graph neural network-based representation learning network to enhance the learning of individuals\u2019 preferences from their friends\u2019 preferences, which provides a solid foundation for learning groups\u2019 preferences. To exploit the group similarity (i.e., overlapping relationships among groups) to learn a more accurate group representation from highly limited group-item interactions, we connect all groups as a network of overlapping sets (a.k.a. hypergraph), and treat the task of group preference learning as embedding hyperedges (i.e., user sets/groups) in a hypergraph, where an inductive hyperedge embedding method is proposed. To further enhance the group-level preference modeling, we develop a joint training strategy to learn both user-item and group-item interactions in the same process. We conduct extensive experiments on two real-world datasets, and the experimental results demonstrate the superiority of our proposed HyperGroup in comparison to the state-of-the-art baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110797784",
                    "name": "Lei Guo"
                },
                {
                    "authorId": "2416851",
                    "name": "Hongzhi Yin"
                },
                {
                    "authorId": "1490931831",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2928371",
                    "name": "Xiangliang Zhang"
                },
                {
                    "authorId": "144297772",
                    "name": "Kai Zheng"
                }
            ]
        },
        {
            "paperId": "36ad8bfd23f619e3ad844218c0766463787121ca",
            "title": "Double-Scale Self-Supervised Hypergraph Learning for Group Recommendation",
            "abstract": "With the prevalence of social media, there has recently been a proliferation of recommenders that shift their focus from individual modeling to group recommendation. Since the group preference is a mixture of various predilections from group members, the fundamental challenge of group recommendation is to model the correlations among members. Existing methods mostly adopt heuristic or attention-based preference aggregation strategies to synthesize group preferences. However, these models mainly focus on the pairwise connections of users and ignore the complex high-order interactions within and beyond groups. Besides, group recommendation suffers seriously from the problem of data sparsity due to severely sparse group-item interactions. In this paper, we propose a self-supervised hypergraph learning framework for group recommendation to achieve two goals: (1) capturing the intra- and inter-group interactions among users; (2) alleviating the data sparsity issue with the raw data itself. Technically, for (1), a hierarchical hypergraph convolutional network based on the user- and group-level hypergraphs is developed to model the complex tuplewise correlations among users within and beyond groups. For (2), we design a double-scale node dropout strategy to create self-supervision signals that can regularize user representations with different granularities against the sparsity issue. The experimental analysis on multiple benchmark datasets demonstrates the superiority of the proposed model and also elucidates the rationality of the hypergraph modeling and the double-scale self-supervision.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1519050253",
                    "name": "Junwei Zhang"
                },
                {
                    "authorId": "48082691",
                    "name": "Min Gao"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                },
                {
                    "authorId": "2110797784",
                    "name": "Lei Guo"
                },
                {
                    "authorId": "1737121128",
                    "name": "Jundong Li"
                },
                {
                    "authorId": "2416851",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "9fdd454443164ebfec0e9e48209720bf2ce543c8",
            "title": "OpenFraming: Open-sourced Tool for Computational Framing Analysis of Multilingual Data",
            "abstract": "When journalists cover a news story, they can cover the story from multiple angles or perspectives. These perspectives are called \u201cframes,\u201d and usage of one frame or another may influence public perception and opinion of the issue at hand. We develop a web-based system for analyzing frames in multilingual text documents. We propose and guide users through a five-step end-to-end computational framing analysis framework grounded in media framing theory in communication research. Users can use the framework to analyze multilingual text data, starting from the exploration of frames in user\u2019s corpora and through review of previous framing literature (step 1-3) to frame classification (step 4) and prediction (step 5). The framework combines unsupervised and supervised machine learning and leverages a state-of-the-art (SoTA) multilingual language model, which can significantly enhance frame prediction performance while requiring a considerably small sample of manual annotations. Through the interactive website, anyone can perform the proposed computational framing analysis, making advanced computational analysis available to researchers without a programming background and bridging the digital divide within the communication research discipline in particular and the academic community in general. The system is available online at http://www.openframing.org, via an API http://www.openframing.org:5000/docs/, or through our GitHub page https://github.com/vibss2397/openFraming.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2140439642",
                    "name": "Vibhu Bhatia"
                },
                {
                    "authorId": "1896157633",
                    "name": "V. Akavoor"
                },
                {
                    "authorId": "2132933809",
                    "name": "Sejin Paik"
                },
                {
                    "authorId": "2110797784",
                    "name": "Lei Guo"
                },
                {
                    "authorId": "47801182",
                    "name": "Mona Jalal"
                },
                {
                    "authorId": "2116708494",
                    "name": "Alyssa Smith"
                },
                {
                    "authorId": "1396663967",
                    "name": "D. Tofu"
                },
                {
                    "authorId": "2065594140",
                    "name": "Edward Edberg Halim"
                },
                {
                    "authorId": "2108938465",
                    "name": "Yimeng Sun"
                },
                {
                    "authorId": "1723703",
                    "name": "Margrit Betke"
                },
                {
                    "authorId": "1756038",
                    "name": "P. Ishwar"
                },
                {
                    "authorId": "2129412",
                    "name": "D. Wijaya"
                }
            ]
        },
        {
            "paperId": "bcbc348ba3902df71d932f2b07973b255dd35a66",
            "title": "Tri-Branch Convolutional Neural Networks for Top-k Focused Academic Performance Prediction",
            "abstract": "Academic performance prediction aims to leverage student-related information to predict their future academic outcomes, which is beneficial to numerous educational applications, such as personalized teaching and academic early warning. In this article, we reveal the students\u2019 behavior trajectories by mining campus smartcard records, and capture the characteristics inherent in trajectories for academic performance prediction. Particularly, we carefully design a tri-branch convolutional neural network (CNN) architecture, which is equipped with rowwise, columnwise, and depthwise convolutions and attention operations, to effectively capture the persistence, regularity, and temporal distribution of student behavior in an end-to-end manner, respectively. However, different from existing works mainly targeting at improving the prediction performance for the whole students, we propose to cast academic performance prediction as a top- $k$ ranking problem, and introduce a top- $k$ focused loss to ensure the accuracy of identifying academically at-risk students. Extensive experiments were carried out on a large-scale real-world dataset, and we show that our approach substantially outperforms recently proposed methods for academic performance prediction. For the sake of reproducibility, our codes have been released at https://github.com/ZongJ1111/Academic-Performance-Prediction.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35099667",
                    "name": "C. Cui"
                },
                {
                    "authorId": "2000457661",
                    "name": "Jian Zong"
                },
                {
                    "authorId": "1998921670",
                    "name": "Yuling Ma"
                },
                {
                    "authorId": "2108062520",
                    "name": "Xinhua Wang"
                },
                {
                    "authorId": "2110797784",
                    "name": "Lei Guo"
                },
                {
                    "authorId": "2108610760",
                    "name": "Meng Chen"
                },
                {
                    "authorId": "102446355",
                    "name": "Yilong Yin"
                }
            ]
        },
        {
            "paperId": "c6b8b32a0e8a20bb454344b5706c777bf93b2de0",
            "title": "DA-GCN: A Domain-aware Attentive Graph Convolution Network for Shared-account Cross-domain Sequential Recommendation",
            "abstract": "Shared-account Cross-domain Sequential Recommendation (SCSR) is the task of recommending the next item based on a sequence of recorded user behaviors, where multiple users share a single account, and their behaviours are available in multiple domains. Existing work on solving SCSR mainly relies on mining sequential patterns via RNN-based models, which are not expressive enough to capture the relationships among multiple entities. Moreover, all existing algorithms try to bridge two domains via knowledge transfer in the latent space, and the explicit cross-domain graph structure is unexploited. In this work, we propose a novel graph-based solution, namely DA-GCN, to address the above challenges. Specifically, we first link users and items in each domain as a graph. Then, we devise a domain-aware graph convolution network to learn user-specific node representations. To fully account for users' domain-specific preferences on items, two novel attention mechanisms are further developed to selectively guide the message passing process. Extensive experiments on two real-world datasets are conducted to demonstrate the superiority of our DA-GCN method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110797784",
                    "name": "Lei Guo"
                },
                {
                    "authorId": "2153720615",
                    "name": "Li Tang"
                },
                {
                    "authorId": "1490931831",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "2112583494",
                    "name": "Lei Zhu"
                },
                {
                    "authorId": "144133815",
                    "name": "Q. Nguyen"
                },
                {
                    "authorId": "2416851",
                    "name": "Hongzhi Yin"
                }
            ]
        }
    ]
}