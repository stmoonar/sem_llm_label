{
    "authorId": "2104141869",
    "papers": [
        {
            "paperId": "792e94e6880692307e71e4c2d2ea20ecb77b8241",
            "title": "Understanding Modality Preferences in Search Clarification",
            "abstract": "This study is the first attempt to explore the impact of clarification question modality on user preference in search engines. We introduce the multi-modal search clarification dataset, MIMICS-MM, containing clarification questions with associated expert-collected and model-generated images. We analyse user preferences over different clarification modes of text, image, and combination of both through crowdsourcing by taking into account image and text quality, clarity, and relevance. Our findings demonstrate that users generally prefer multi-modal clarification over uni-modal approaches. We explore the use of automated image generation techniques and compare the quality, relevance, and user preference of model-generated images with human-collected ones. The study reveals that text-to-image generation models, such as Stable Diffusion, can effectively generate multi-modal clarification questions. By investigating multi-modal clarification, this research establishes a foundation for future advancements in search systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2104141869",
                    "name": "Leila Tavakoli"
                },
                {
                    "authorId": "2196542565",
                    "name": "Giovanni Castiglia"
                },
                {
                    "authorId": "2196591796",
                    "name": "Federica Cal\u00f2"
                },
                {
                    "authorId": "2614755",
                    "name": "Yashar Deldjoo"
                },
                {
                    "authorId": "2291137161",
                    "name": "Hamed Zamani"
                },
                {
                    "authorId": "2528063",
                    "name": "Johanne R. Trippas"
                }
            ]
        },
        {
            "paperId": "dd8a811c1e5cc033849ea2dcdc648af48bb82bef",
            "title": "Online and Offline Evaluation in Search Clarification",
            "abstract": "The effectiveness of clarification question models in engaging users within search systems is currently constrained, casting doubt on their overall usefulness. To improve the performance of these models, it is crucial to employ assessment approaches that encompass both real-time feedback from users (online evaluation) and the characteristics of clarification questions evaluated through human assessment (offline evaluation). However, the relationship between online and offline evaluations has been debated in information retrieval. This study aims to investigate how this discordance holds in search clarification. We use user engagement as ground truth and employ several offline labels to investigate to what extent the offline ranked lists of clarification resemble the ideal ranked lists based on online user engagement. Contrary to the current understanding that offline evaluations fall short of supporting online evaluations, we indicate that when identifying the most engaging clarification questions from the user\u2019s perspective, online and offline evaluations correspond with each other. We show that the query length does not influence the relationship between online and offline evaluations, and reducing uncertainty in online evaluation strengthens this relationship. We illustrate that an engaging clarification needs to excel from multiple perspectives, and SERP quality and characteristics of the clarification are equally important. We also investigate if human labels can enhance the performance of Large Language Models (LLMs) and Learning-to-Rank (LTR) models in identifying the most engaging clarification questions from the user\u2019s perspective by incorporating offline evaluations as input features. Our results indicate that Learning-to-Rank models do not perform better than individual offline labels. However, GPT, an LLM, emerges as the standout performer, surpassing all Learning-to-Rank models and offline labels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2104141869",
                    "name": "Leila Tavakoli"
                },
                {
                    "authorId": "2528063",
                    "name": "Johanne R. Trippas"
                },
                {
                    "authorId": "2291137161",
                    "name": "Hamed Zamani"
                },
                {
                    "authorId": "2256152195",
                    "name": "Falk Scholer"
                },
                {
                    "authorId": "2270077823",
                    "name": "Mark Sanderson"
                }
            ]
        },
        {
            "paperId": "9e58888b1779bc63dff1c3c623cdf226e321c652",
            "title": "MIMICS-Duo: Offline & Online Evaluation of Search Clarification",
            "abstract": "Asking clarification questions is an active area of research; however, resources for training and evaluating search clarification methods are not sufficient. To address this issue, we describe MIMICS-Duo, a new freely available dataset of 306 search queries with multiple clarifications (a total of 1,034 query-clarification pairs). MIMICS-Duo contains fine-grained annotations on clarification questions and their candidate answers and enhances the existing MIMICS datasets by enabling multi-dimensional evaluation of search clarification methods, including online and offline evaluation. We conduct extensive analysis to demonstrate the relationship between offline and online search clarification datasets and outline several research directions enabled by MIMICS-Duo. We believe that this resource will help researchers better understand clarification in search.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2104141869",
                    "name": "Leila Tavakoli"
                },
                {
                    "authorId": "2528063",
                    "name": "Johanne R. Trippas"
                },
                {
                    "authorId": "2499986",
                    "name": "Hamed Zamani"
                },
                {
                    "authorId": "1732541",
                    "name": "Falk Scholer"
                },
                {
                    "authorId": "144721996",
                    "name": "M. Sanderson"
                }
            ]
        },
        {
            "paperId": "837e433105f2288f4f7a7be4bfaf19498c973c1f",
            "title": "An Intent Taxonomy for Questions Asked in Web Search",
            "abstract": "We present a new, multi-faceted taxonomy to classify questions asked in web search engines based on the question intent, types of entities mentioned, types of question words, and granularity of the expected answer. Built based on the inspection of 1,000 real-life questions issued to a web search engine, the taxonomy reflects the recent search behavior of users and enables deep understanding of user intents, goals, and expected answers. This taxonomy is more fine-grained than previous query taxonomies, and is designed with the ultimate goal of reducing the inherent ambiguity in determining the intent of questions. In addition, we describe the formal procedure for conducting an editorial study of the taxonomy including its evaluation. The adopted procedure aims to increase assessor agreement without incurring too much overhead. Our results demonstrate that, despite being more fine-grained, the proposed intent categories result in higher agreement between assessors compared to an existing, commonly used taxonomy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1776940",
                    "name": "B. B. Cambazoglu"
                },
                {
                    "authorId": "2104141869",
                    "name": "Leila Tavakoli"
                },
                {
                    "authorId": "1732541",
                    "name": "Falk Scholer"
                },
                {
                    "authorId": "144721996",
                    "name": "M. Sanderson"
                },
                {
                    "authorId": "144456145",
                    "name": "W. Bruce Croft"
                }
            ]
        },
        {
            "paperId": "cd4b0d5c8ebdf262c82f9d5eeb2e55338a3dfdb3",
            "title": "Quantifying Human-Perceived Answer Utility in Non-factoid Question Answering",
            "abstract": "Taking a user-centric approach, we study the features that render an answer to a non-factoid question useful in the eyes of the person who asked that question. An editorial study, where participants assess the usefulness of the answers they received in response to their questions, as well as 12 different aspects associated with the answers, indicates considerable correlation between certain aspects such as relevance, correctness, and completeness with the user-perceived usefulness of answers. Moreover, we investigate the effectiveness of some commonly used answer quality measures, such as ROGUE, BLEU, METEOR, and BERTScore, demonstrating that these measures are limited in their ability to capture the aspects of usefulness and have room for improvement. The question answering dataset created in our work was made publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1776940",
                    "name": "B. B. Cambazoglu"
                },
                {
                    "authorId": "145286681",
                    "name": "Valeriia Bolotova-Baranova"
                },
                {
                    "authorId": "1732541",
                    "name": "Falk Scholer"
                },
                {
                    "authorId": "144721996",
                    "name": "M. Sanderson"
                },
                {
                    "authorId": "2104141869",
                    "name": "Leila Tavakoli"
                },
                {
                    "authorId": "144456145",
                    "name": "W. Bruce Croft"
                }
            ]
        },
        {
            "paperId": "7cd2ca293197e8471584d76271a6b5a79d613c58",
            "title": "Generating Clarifying Questions in Conversational Search Systems",
            "abstract": "Asking a clarifying question can be a key element improving the performance of information seeking systems, particularly conversational search systems due to their limited bandwidth interfaces. While generating and asking clarifying questions is important; get-ting an answer for the clarifying question is also essential as a clarifying question without an answer is useless. Therefore, as the first step in current research, we analysed human-generated clarifying questions in a Community Question Answering website as a sample of conversation. This helped us to gain a better insight into how users interact with clarification. We investigated the clarifying questions in terms of whether they add any information to the question and the accepted answer. We further discovered the patterns and types of such clarifying questions. The next phase of this research will then generate clarifying questions in conversational search systems. We will then employ neural network models to generate clarifying questions to maximise clarification in questions. The proposed model will be trained using the MIMICS data collection in addition to our collected dataset. We will also attempt to consider the recognised patterns from the analysis conducted in the first step to enhance the chance that a user will interact with the clarifying questions. Finally, we will aim to minimise the interaction between the search system and the user to reduce the risk of dropping the conversation by the user due to asking too many clarifying questions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2104141869",
                    "name": "Leila Tavakoli"
                }
            ]
        }
    ]
}