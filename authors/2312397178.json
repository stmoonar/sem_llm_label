{
    "authorId": "2312397178",
    "papers": [
        {
            "paperId": "af034d8aee20d34dc471c00ddb95ca71c2038651",
            "title": "AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations",
            "abstract": "For many use-cases, it is often important to explain the prediction of a black-box model by identifying the most influential training data samples. Existing approaches lack customization for user intent and often provide a homogeneous set of explanation samples, failing to reveal the model's reasoning from different angles. In this paper, we propose AIDE, an approach for providing antithetical (i.e., contrastive), intent-based, diverse explanations for opaque and complex models. AIDE distinguishes three types of explainability intents: interpreting a correct, investigating a wrong, and clarifying an ambiguous prediction. For each intent, AIDE selects an appropriate set of influential training samples that support or oppose the prediction either directly or by contrast. To provide a succinct summary, AIDE uses diversity-aware sampling to avoid redundancy and increase coverage of the training data. We demonstrate the effectiveness of AIDE on image and text classification tasks, in three ways: quantitatively, assessing correctness and continuity; qualitatively, comparing anecdotal evidence from AIDE and other example-based approaches; and via a user study, evaluating multiple aspects of AIDE. The results show that AIDE addresses the limitations of existing methods and exhibits desirable traits for an explainability method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2312397178",
                    "name": "Ikhtiyor Nematov"
                },
                {
                    "authorId": "1760642",
                    "name": "Dimitris Sacharidis"
                },
                {
                    "authorId": "47125649",
                    "name": "Tomer Sagi"
                },
                {
                    "authorId": "47061445",
                    "name": "K. Hose"
                }
            ]
        },
        {
            "paperId": "f2c5e65ee7783731490c78ced65d3cbb62ce3d3e",
            "title": "The Susceptibility of Example-Based Explainability Methods to Class Outliers",
            "abstract": "This study explores the impact of class outliers on the effectiveness of example-based explainability methods for black-box machine learning models. We reformulate existing explainability evaluation metrics, such as correctness and relevance, specifically for example-based methods, and introduce a new metric, distinguishability. Using these metrics, we highlight the shortcomings of current example-based explainability methods, including those who attempt to suppress class outliers. We conduct experiments on two datasets, a text classification dataset and an image classification dataset, and evaluate the performance of four state-of-the-art explainability methods. Our findings underscore the need for robust techniques to tackle the challenges posed by class outliers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2312397178",
                    "name": "Ikhtiyor Nematov"
                },
                {
                    "authorId": "1760642",
                    "name": "Dimitris Sacharidis"
                },
                {
                    "authorId": "47125649",
                    "name": "Tomer Sagi"
                },
                {
                    "authorId": "47061445",
                    "name": "K. Hose"
                }
            ]
        }
    ]
}