{
    "authorId": "34741133",
    "papers": [
        {
            "paperId": "0042c4f35d4a3ea947de8374e298099d476dad7f",
            "title": "PIVOINE: Instruction Tuning for Open-world Information Extraction",
            "abstract": "We consider the problem of Open-world Information Extraction (Open-world IE), which extracts comprehensive entity profiles from unstructured texts. Different from the conventional closed-world setting of Information Extraction (IE), Open-world IE considers a more general situation where entities and relations could be beyond a predefined ontology. More importantly, we seek to develop a large language model (LLM) that is able to perform Open-world IE to extract desirable entity profiles characterized by (possibly fine-grained) natural language instructions. We achieve this by finetuning LLMs using instruction tuning. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction tuning dataset for Open-world IE enriched with a comprehensive corpus, extensive annotations, and diverse instructions. We finetune the pretrained BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world IE with strong instruction-following capabilities. Our experiments demonstrate that PIVOINE significantly outperforms traditional closed-world methods and other LLM baselines, displaying impressive generalization capabilities on both unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as a promising solution to tackle the open-world challenge in IE effectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1515662094",
                    "name": "K. Lu"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "50982080",
                    "name": "Kaiqiang Song"
                },
                {
                    "authorId": "49723569",
                    "name": "Hongming Zhang"
                },
                {
                    "authorId": "144580027",
                    "name": "Dong Yu"
                },
                {
                    "authorId": "2108276402",
                    "name": "Jianshu Chen"
                }
            ]
        },
        {
            "paperId": "447bbdbeb5dfa9252b51a833eafe5e8f4d3b632e",
            "title": "Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models",
            "abstract": "We investigate how to elicit compositional generalization capabilities in large language models (LLMs). Compositional generalization empowers LLMs to solve complex problems by combining foundational skills, a critical reasoning ability akin to human intelligence. However, even the most advanced LLMs currently struggle with this form of reasoning. We examine this problem within the framework of in-context learning and find that demonstrating both foundational skills and compositional examples grounded in these skills within the same prompt context is crucial. We refer to this prompt structure as skills-in-context (SKiC). With as few as two exemplars, this in-context learning structure enables LLMs to tackle more challenging problems requiring innovative skill combinations, achieving near-perfect systematic generalization across a broad range of tasks. Intriguingly, SKiC also unlocks the latent potential of LLMs, allowing them to more actively utilize pre-existing internal skills acquired during earlier pretraining stages to solve complex reasoning problems. The SKiC structure is robust across different skill constructions and exemplar choices and demonstrates strong transferability to new tasks. Finally, inspired by our in-context learning study, we show that fine-tuning LLMs with SKiC-style data can elicit zero-shot weak-to-strong generalization, enabling the models to solve much harder problems directly with standard prompting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47739850",
                    "name": "Jiaao Chen"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "41190054",
                    "name": "Dian Yu"
                },
                {
                    "authorId": "50982080",
                    "name": "Kaiqiang Song"
                },
                {
                    "authorId": "48631781",
                    "name": "Xiaoyang Wang"
                },
                {
                    "authorId": "144580027",
                    "name": "Dong Yu"
                },
                {
                    "authorId": "2108276402",
                    "name": "Jianshu Chen"
                }
            ]
        },
        {
            "paperId": "8afe3743913bc8743c75fcb9d8bab5d0c489a6aa",
            "title": "How do Words Contribute to Sentence Semantics? Revisiting Sentence Embeddings with a Perturbation Method",
            "abstract": "Understanding sentence semantics requires an interpretation of the main information from a concrete context. To investigate how individual word contributes to sentence semantics, we propose a perturbation method for unsupervised semantic analysis. We next re-examine SOTA sentence embedding models\u2019 ability to capture the main semantics of a sentence by developing a new evaluation metric to adapt sentence compression datasets for automatic evaluation. Results on three datasets show that unsupervised discourse relation recognition can serve as a general inference task that can more effectively aggregate information to essential contents than several SOTA unsupervised sentence embedding models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087264100",
                    "name": "Wenlin Yao"
                },
                {
                    "authorId": "50496698",
                    "name": "Lifeng Jin"
                },
                {
                    "authorId": "49723569",
                    "name": "Hongming Zhang"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "50982080",
                    "name": "Kaiqiang Song"
                },
                {
                    "authorId": "41190054",
                    "name": "Dian Yu"
                },
                {
                    "authorId": "144580027",
                    "name": "Dong Yu"
                },
                {
                    "authorId": "2108276402",
                    "name": "Jianshu Chen"
                }
            ]
        },
        {
            "paperId": "c15e57285a25c5fb941269bb55d91dce628c2ba1",
            "title": "OpenFact: Factuality Enhanced Open Knowledge Extraction",
            "abstract": "We focus on the factuality property during the extraction of an OpenIE corpus named OpenFact, which contains more than 12 million high-quality knowledge triplets. We break down the factuality property into two important aspects\u2014expressiveness and groundedness\u2014and we propose a comprehensive framework to handle both aspects. To enhance expressiveness, we formulate each knowledge piece in OpenFact based on a semantic frame. We also design templates, extra constraints, and adopt human efforts so that most OpenFact triplets contain enough details. For groundedness, we require the main arguments of each triplet to contain linked Wikidata1 entities. A human evaluation suggests that the OpenFact triplets are much more accurate and contain denser information compared to OPIEC-Linked (Gashteovski et al., 2019), one recent high-quality OpenIE corpus grounded to Wikidata. Further experiments on knowledge base completion and knowledge base question answering show the effectiveness of OpenFact over OPIEC-Linked as supplementary knowledge to Wikidata as the major KG.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50258954",
                    "name": "Linfeng Song"
                },
                {
                    "authorId": "1754106063",
                    "name": "Ante Wang"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "49723569",
                    "name": "Hongming Zhang"
                },
                {
                    "authorId": "41190054",
                    "name": "Dian Yu"
                },
                {
                    "authorId": "50496698",
                    "name": "Lifeng Jin"
                },
                {
                    "authorId": "2013337",
                    "name": "Haitao Mi"
                },
                {
                    "authorId": "34739384",
                    "name": "Jinsong Su"
                },
                {
                    "authorId": "39939186",
                    "name": "Yue Zhang"
                },
                {
                    "authorId": "2111505433",
                    "name": "Dong Yu"
                }
            ]
        },
        {
            "paperId": "c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36",
            "title": "MinT: Boosting Generalization in Mathematical Reasoning via Multi-view Fine-tuning",
            "abstract": "Reasoning in mathematical domains remains a significant challenge for relatively small language models (LMs). Many current methods focus on specializing LMs in mathematical reasoning and rely heavily on distilling knowledge from powerful yet inefficient large LMs (LLMs). In this work, we explore a new direction that avoids over-reliance on LLM teachers, introducing a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles. Our approach uniquely considers the various annotation formats as different \u201cviews\u201d that may help each other and leverage them in training the model. By postpending distinct instructions to input questions, models can learn to generate solutions in diverse formats in a flexible manner. Experimental results show that our strategy enables relatively small LMs to outperform prior approaches that heavily rely on knowledge distillation, as well as carefully established baselines. Additionally, the proposed method grants the models promising generalization ability across various views and datasets, and the capability to learn from inaccurate or incomplete noisy data. We hope our multi-view training paradigm could inspire future studies in other machine reasoning domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151474408",
                    "name": "Zhenwen Liang"
                },
                {
                    "authorId": "41190054",
                    "name": "Dian Yu"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "2087264100",
                    "name": "Wenlin Yao"
                },
                {
                    "authorId": "1694209",
                    "name": "Qingkai Zeng"
                },
                {
                    "authorId": "2928371",
                    "name": "Xiangliang Zhang"
                },
                {
                    "authorId": "144580027",
                    "name": "Dong Yu"
                }
            ]
        },
        {
            "paperId": "facbd5cbc3456639e01f685bdc233e469f5ac30f",
            "title": "Thrust: Adaptively Propels Large Language Models with External Knowledge",
            "abstract": "Although large-scale pre-trained language models (PTLMs) are shown to encode rich knowledge in their model parameters, the inherent knowledge in PTLMs can be opaque or static, making external knowledge necessary. However, the existing information retrieval techniques could be costly and may even introduce noisy and sometimes misleading knowledge. To address these challenges, we propose the instance-level adaptive propulsion of external knowledge (IAPEK), where we only conduct the retrieval when necessary. To achieve this goal, we propose measuring whether a PTLM contains enough knowledge to solve an instance with a novel metric, Thrust, which leverages the representation distribution of a small number of seen instances. Extensive experiments demonstrate that thrust is a good measurement of PTLM models' instance-level knowledgeability. Moreover, we can achieve significantly higher cost-efficiency with the Thrust score as the retrieval indicator than the naive usage of external knowledge on 88% of the evaluated tasks with 26% average performance improvement. Such findings shed light on the real-world practice of knowledge-enhanced LMs with a limited knowledge-seeking budget due to computation latency or costs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500662261",
                    "name": "Xinran Zhao"
                },
                {
                    "authorId": "49723569",
                    "name": "Hongming Zhang"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "2087264100",
                    "name": "Wenlin Yao"
                },
                {
                    "authorId": "144580027",
                    "name": "Dong Yu"
                },
                {
                    "authorId": "2108276402",
                    "name": "Jianshu Chen"
                }
            ]
        },
        {
            "paperId": "6e8a1f2337f5888360287cc902d561852882177a",
            "title": "C-MORE: Pretraining to Answer Open-Domain Questions by Consulting Millions of References",
            "abstract": "We consider the problem of pretraining a two-stage open-domain question answering (QA) system (retriever + reader) with strong transfer capabilities. The key challenge is how to construct a large amount of high-quality question-answer-context triplets without task-specific annotations. Specifically, the triplets should align well with downstream tasks by: (i) covering a wide range of domains (for open-domain applications), (ii) linking a question to its semantically relevant context with supporting evidence (for training the retriever), and (iii) identifying the correct answer in the context (for training the reader). Previous pretraining approaches generally fall short of one or more of these requirements. In this work, we automatically construct a large-scale corpus that meets all three criteria by consulting millions of references cited within Wikipedia. The well-aligned pretraining signals benefit both the retriever and the reader significantly. Our pretrained retriever leads to 2%-10% absolute gains in top-20 accuracy. And with our pretrained reader, the entire system improves by up to 4% in exact match.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144065082",
                    "name": "Xiang Yue"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "2087264100",
                    "name": "Wenlin Yao"
                },
                {
                    "authorId": "41190054",
                    "name": "Dian Yu"
                },
                {
                    "authorId": "144580027",
                    "name": "Dong Yu"
                },
                {
                    "authorId": "2108276402",
                    "name": "Jianshu Chen"
                }
            ]
        },
        {
            "paperId": "7ffb3a27a2a4da5c35472bd3a3a4dee8d40a6d86",
            "title": "Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models",
            "abstract": "Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training. In this paper, we develop a novel semi-parametric language model architecture, Knowledge-in-Context (KiC), which empowers a parametric text-to-text language model with a knowledge-rich external memory. Specifically, the external memory contains six different types of knowledge: entity, dictionary, commonsense, event, script, and causality knowledge. For each input instance, the KiC model adaptively selects a knowledge type and retrieves the most helpful pieces of knowledge. The input instance along with its knowledge augmentation is fed into a text-to-text model (e.g., T5) to generate the output answer, where both the input and the output are in natural language forms after prompting. Interestingly, we find that KiC can be identified as a special mixture-of-experts (MoE) model, where the knowledge selector plays the role of a router that is used to determine the sequence-to-expert assignment in MoE. This key observation inspires us to develop a novel algorithm for training KiC with an instance-adaptive knowledge selector. As a knowledge-rich semi-parametric language model, KiC only needs a much smaller parametric part to achieve superior zero-shot performance on unseen tasks. By evaluating on 40+ different tasks, we show that KiC_Large with 770M parameters easily outperforms large language models (LMs) that are 4-39x larger by a large margin. We also demonstrate that KiC exhibits emergent abilities at a much smaller model scale compared to the fully-parametric models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "2087264100",
                    "name": "Wenlin Yao"
                },
                {
                    "authorId": "49723569",
                    "name": "Hongming Zhang"
                },
                {
                    "authorId": "41190054",
                    "name": "Dian Yu"
                },
                {
                    "authorId": "144580027",
                    "name": "Dong Yu"
                },
                {
                    "authorId": "2108276402",
                    "name": "Jianshu Chen"
                }
            ]
        },
        {
            "paperId": "8c1fb4831e697b42d8cb41558f71c4d0dd5fb561",
            "title": "ZeroKBC: A Comprehensive Benchmark for Zero-Shot Knowledge Base Completion",
            "abstract": "Knowledge base completion (KBC) aims to predict the missing links in knowledge graphs. Previous KBC tasks and approaches mainly focus on the setting where all test entities and relations have appeared in the training set. However, there has been limited research on the zero-shot KBC settings, where we need to deal with unseen entities and relations that emerge in a constantly growing knowledge base. In this work, we systematically examine different possible scenarios of zero-shot KBC and develop a comprehensive benchmark, ZeroKBC, that covers these scenarios with diverse types of knowledge sources. Our systematic analysis reveals several missing yet important zero-shot KBC settings. Experimental results show that canonical and state-of-the-art KBC systems cannot achieve satisfactory performance on this challenging benchmark. By analyzing the strength and weaknesses of these systems on solving ZeroKBC, we further present several important observations and promising future directions.11Work was done during the internship at Tencent AI lab. The data and code are available at: https://github.com/brickee/ZeroKBC",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2901524",
                    "name": "Pei Chen"
                },
                {
                    "authorId": "2087264100",
                    "name": "Wenlin Yao"
                },
                {
                    "authorId": "48212577",
                    "name": "Hongming Zhang"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "41190054",
                    "name": "Dian Yu"
                },
                {
                    "authorId": "144580027",
                    "name": "Dong Yu"
                },
                {
                    "authorId": "2108276402",
                    "name": "Jianshu Chen"
                }
            ]
        },
        {
            "paperId": "d2fe7536b347a7039a241bb60d507880ada686e8",
            "title": "OASum: Large-Scale Open Domain Aspect-based Summarization",
            "abstract": "Aspect or query-based summarization has recently caught more attention, as it can generate differentiated summaries based on users' interests. However, the current dataset for aspect or query-based summarization either focuses on specific domains, contains relatively small-scale instances, or includes only a few aspect types. Such limitations hinder further explorations in this direction. In this work, we take advantage of crowd-sourcing knowledge on Wikipedia.org and automatically create a high-quality, large-scale open-domain aspect-based summarization dataset named OASum, which contains more than 3.7 million instances with around 1 million different aspects on 2 million Wikipedia pages. We provide benchmark results on OASum and demonstrate its ability for diverse aspect-based summarization generation. To overcome the data scarcity problem on specific domains, we also perform zero-shot, few-shot, and fine-tuning on seven downstream datasets. Specifically, zero/few-shot and fine-tuning results show that the model pre-trained on our corpus demonstrates a strong aspect or query-focused generation ability compared with the backbone model. Our dataset and pre-trained checkpoints are publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145170944",
                    "name": "Xianjun Yang"
                },
                {
                    "authorId": "50982080",
                    "name": "Kaiqiang Song"
                },
                {
                    "authorId": "2173531",
                    "name": "Sangwoo Cho"
                },
                {
                    "authorId": "48631781",
                    "name": "Xiaoyang Wang"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "21038849",
                    "name": "Linda Petzold"
                },
                {
                    "authorId": "144580027",
                    "name": "Dong Yu"
                }
            ]
        }
    ]
}