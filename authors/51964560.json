{
    "authorId": "51964560",
    "papers": [
        {
            "paperId": "3dae9d5ff0717281d1f1c6c37274ffe72e6bda3d",
            "title": "TIGQA: An Expert-Annotated Question-Answering Dataset in Tigrinya",
            "abstract": "The absence of explicitly tailored, accessible annotated datasets for educational purposes presents a notable obstacle for NLP tasks in languages with limited resources. This study initially explores the feasibility of using machine translation (MT) to convert an existing dataset into a Tigrinya dataset in SQuAD format. As a result, we present TIGQA, an expert-annotated dataset containing 2,685 question-answer pairs covering 122 diverse topics such as climate, water, and traffic. These pairs are from 537 context paragraphs in publicly accessible Tigrinya and Biology books. Through comprehensive analyses, we demonstrate that the TIGQA dataset requires skills beyond simple word matching, requiring both single-sentence and multiple-sentence inference abilities. We conduct experiments using state-of-the-art MRC methods, marking the first exploration of such models on TIGQA. Additionally, we estimate human performance on the dataset and juxtapose it with the results obtained from pre-trained models. The notable disparities between human performance and the best model performance underscore the potential for fu- ture enhancements to TIGQA through continued research. Our dataset is freely accessible via the provided link to encourage the research community to address the challenges in the Tigrinya MRC. Keywords: Tigrinya QA dataset, Low resource QA dataset, domain specific QA",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298756123",
                    "name": "Hailay Teklehaymanot"
                },
                {
                    "authorId": "2205658867",
                    "name": "Dren Fazlija"
                },
                {
                    "authorId": "2297771227",
                    "name": "Niloy Ganguly"
                },
                {
                    "authorId": "51964560",
                    "name": "Gourab K. Patro"
                },
                {
                    "authorId": "2237628085",
                    "name": "Wolfgang Nejdl"
                }
            ]
        },
        {
            "paperId": "faf261c4dc30f71ab3c147e5fee070f780572cd6",
            "title": "A Review of the Role of Causality in Developing Trustworthy AI Systems",
            "abstract": "State-of-the-art AI models largely lack an understanding of the cause-effect relationship that governs human understanding of the real world. Consequently, these models do not generalize to unseen data, often produce unfair results, and are difficult to interpret. This has led to efforts to improve the trustworthiness aspects of AI models. Recently, causal modeling and inference methods have emerged as powerful tools. This review aims to provide the reader with an overview of causal methods that have been developed to improve the trustworthiness of AI models. We hope that our contribution will motivate future research on causality-based solutions for trustworthy AI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4213990",
                    "name": "Niloy Ganguly"
                },
                {
                    "authorId": "2205658867",
                    "name": "Dren Fazlija"
                },
                {
                    "authorId": "51192415",
                    "name": "Maryam Badar"
                },
                {
                    "authorId": "2140238020",
                    "name": "M. Fisichella"
                },
                {
                    "authorId": "2632448",
                    "name": "Sandipan Sikdar"
                },
                {
                    "authorId": "2164815337",
                    "name": "J. Schrader"
                },
                {
                    "authorId": "1999172945",
                    "name": "Jonas Wallat"
                },
                {
                    "authorId": "2042376",
                    "name": "Koustav Rudra"
                },
                {
                    "authorId": "1746733",
                    "name": "Manolis Koubarakis"
                },
                {
                    "authorId": "51964560",
                    "name": "Gourab K. Patro"
                },
                {
                    "authorId": "2142447153",
                    "name": "W. Z. E. Amri"
                },
                {
                    "authorId": "1744808",
                    "name": "W. Nejdl"
                }
            ]
        },
        {
            "paperId": "44070f32949ca85c6bd4e1b2e2764e3409a63156",
            "title": "Fairness Implications of Encoding Protected Categorical Attributes",
            "abstract": "Past research has demonstrated that the explicit use of protected attributes in machine learning can improve both performance and fairness. Many machine learning algorithms, however, cannot directly process categorical attributes, such as country of birth or ethnicity. Because protected attributes frequently are categorical, they must be encoded as features that can be input to a chosen machine learning algorithm, e.g. support vector machines, gradient boosting decision trees or linear models. Thereby, encoding methods influence how and what the machine learning algorithm will learn, affecting model performance and fairness. This work compares the accuracy and fairness implications of the two most well-known encoding methods: one-hot encoding and target encoding. We distinguish between two types of induced bias that may arise from these encoding methods and may lead to unfair models. The first type, irreducible bias, is due to direct group category discrimination and the second type, reducible bias, is due to the large variance in statistically underrepresented groups. We investigate the interaction between categorical encodings and target encoding regularization methods that reduce unfairness. Furthermore, we consider the problem of intersectional unfairness that may arise when machine learning best practices improve performance measures by encoding several categorical attributes into a high-cardinality feature.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "115400334",
                    "name": "Carlos Mougan"
                },
                {
                    "authorId": "2974008",
                    "name": "J. \u00c1lvarez"
                },
                {
                    "authorId": "51964560",
                    "name": "Gourab K. Patro"
                },
                {
                    "authorId": "2325428",
                    "name": "S. Ruggieri"
                },
                {
                    "authorId": "1752093",
                    "name": "Steffen Staab"
                }
            ]
        },
        {
            "paperId": "4fc6929590ccddf6d79cef4e52fb08e1c5e72586",
            "title": "Fair ranking: a critical review, challenges, and future directions",
            "abstract": "Ranking, recommendation, and retrieval systems are widely used in online platforms and other societal systems, including e-commerce, media-streaming, admissions, gig platforms, and hiring. In the recent past, a large \u201cfair ranking\u201d research literature has been developed around making these systems fair to the individuals, providers, or content that are being ranked. Most of this literature defines fairness for a single instance of retrieval, or as a simple additive notion for multiple instances of retrievals over time. This work provides a critical overview of this literature, detailing the often context-specific concerns that such approaches miss: the gap between high ranking placements and true provider utility, spillovers and compounding effects over time, induced strategic incentives, and the effect of statistical uncertainty. We then provide a path forward for a more holistic and impact-oriented fair ranking research agenda, including methodological lessons from other fields and the role of the broader stakeholder community in overcoming data bottlenecks and designing effective regulatory environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51964560",
                    "name": "Gourab K. Patro"
                },
                {
                    "authorId": "74248595",
                    "name": "Lorenzo Porcaro"
                },
                {
                    "authorId": "2152051189",
                    "name": "Laura Mitchell"
                },
                {
                    "authorId": "51456850",
                    "name": "Qiuyue Zhang"
                },
                {
                    "authorId": "41154657",
                    "name": "Meike Zehlike"
                },
                {
                    "authorId": "2779427",
                    "name": "Nikhil Garg"
                }
            ]
        },
        {
            "paperId": "89470938662408d91f6165966cf47d47e1501fab",
            "title": "Scheduling Virtual Conferences Fairly: Achieving Equitable Participant and Speaker Satisfaction",
            "abstract": "Recently, almost all conferences have moved to virtual mode due to the pandemic-induced restrictions on travel and social gathering. Contrary to in-person conferences, virtual conferences face the challenge of efficiently scheduling talks, accounting for the availability of participants from different timezones and their interests in attending different talks. A natural objective for conference organizers is to maximize efficiency, e.g., total expected audience participation across all talks. However, we show that optimizing for efficiency alone can result in an unfair virtual conference schedule, where individual utilities for participants and speakers can be highly unequal. To address this, we formally define fairness notions for participants and speakers, and derive suitable objectives to account for them. As the efficiency and fairness objectives can be in conflict with each other, we propose a joint optimization framework that allows conference organizers to design schedules that balance (i.e., allow trade-offs) among efficiency, participant fairness and speaker fairness objectives. While the optimization problem can be solved using integer programming to schedule smaller conferences, we provide two scalable techniques to cater to bigger conferences. Extensive evaluations over multiple real-world datasets show the efficacy and flexibility of our proposed approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51964560",
                    "name": "Gourab K. Patro"
                },
                {
                    "authorId": "47487418",
                    "name": "Prithwish Jana"
                },
                {
                    "authorId": "2676839",
                    "name": "Abhijnan Chakraborty"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                },
                {
                    "authorId": "4213990",
                    "name": "Niloy Ganguly"
                }
            ]
        },
        {
            "paperId": "97fdcd32fde4b43b82b57977110bb4a95d25cf38",
            "title": "Fairness in Agreement With European Values: An Interdisciplinary Perspective on AI Regulation",
            "abstract": "With increasing digitalization, Artificial Intelligence (AI) is becoming ubiquitous. AI-based systems to identify, optimize, automate, and scale solutions to complex economic and societal problems are being proposed and implemented. This has motivated regulation efforts, including the Proposal of an EU AI Act. This interdisciplinary position paper considers various concerns surrounding fairness and discrimination in AI, and discusses how AI regulations address them, focusing on (but not limited to) the Proposal. We first look at AI and fairness through the lenses of law, (AI) industry, sociotechnology, and (moral) philosophy, and present various perspectives. Then, we map these perspectives along three axes of interests: (i) Standardization vs. Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential vs. Deontological ethics which leads us to identify a pattern of common arguments and tensions between these axes. Positioning the discussion within the axes of interest and with a focus on reconciling the key tensions, we identify and propose the roles AI Regulation should take to make the endeavor of the AI Act a success in terms of AI fairness concerns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "104924516",
                    "name": "A. Colmenarejo"
                },
                {
                    "authorId": "116344774",
                    "name": "L. Nannini"
                },
                {
                    "authorId": "26755992",
                    "name": "Alisa Rieger"
                },
                {
                    "authorId": "2171370539",
                    "name": "Kristen M. Scott"
                },
                {
                    "authorId": "2048363867",
                    "name": "Xuan Zhao"
                },
                {
                    "authorId": "51964560",
                    "name": "Gourab K. Patro"
                },
                {
                    "authorId": "1686448",
                    "name": "Gjergji Kasneci"
                },
                {
                    "authorId": "1404596968",
                    "name": "K. Kinder-Kurlanda"
                }
            ]
        },
        {
            "paperId": "87c63358bc5b5547bf4d41144ea8743c29e1106b",
            "title": "Toward Fair Recommendation in Two-sided Platforms",
            "abstract": "Many online platforms today (such as Amazon, Netflix, Spotify, LinkedIn, and AirBnB) can be thought of as two-sided markets with producers and customers of goods and services. Traditionally, recommendation services in these platforms have focused on maximizing customer satisfaction by tailoring the results according to the personalized preferences of individual customers. However, our investigation reinforces the fact that such customer-centric design of these services may lead to unfair distribution of exposure to the producers, which may adversely impact their well-being. However, a pure producer-centric design might become unfair to the customers. As more and more people are depending on such platforms to earn a living, it is important to ensure fairness to both producers and customers. In this work, by mapping a fair personalized recommendation problem to a constrained version of the problem of fairly allocating indivisible goods, we propose to provide fairness guarantees for both sides. Formally, our proposed FairRec algorithm guarantees Maxi-Min Share of exposure for the producers, and Envy-Free up to One Item fairness for the customers. Extensive evaluations over multiple real-world datasets show the effectiveness of FairRec in ensuring two-sided fairness while incurring a marginal loss in overall recommendation quality. Finally, we present a modification of FairRec (named as FairRecPlus) that at the cost of additional computation time, improves the recommendation performance for the customers, while maintaining the same fairness guarantees.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2690810",
                    "name": "Arpita Biswas"
                },
                {
                    "authorId": "51964560",
                    "name": "Gourab K. Patro"
                },
                {
                    "authorId": "4213990",
                    "name": "Niloy Ganguly"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                },
                {
                    "authorId": "2676839",
                    "name": "Abhijnan Chakraborty"
                }
            ]
        },
        {
            "paperId": "20aa000524b9bee0d81758806979b76839eb66fe",
            "title": "FairRec: Two-Sided Fairness for Personalized Recommendations in Two-Sided Platforms",
            "abstract": "We investigate the problem of fair recommendation in the context of two-sided online platforms, comprising customers on one side and producers on the other. Traditionally, recommendation services in these platforms have focused on maximizing customer satisfaction by tailoring the results according to the personalized preferences of individual customers. However, our investigation reveals that such customer-centric design may lead to unfair distribution of exposure among the producers, which may adversely impact their well-being. On the other hand, a producer-centric design might become unfair to the customers. Thus, we consider fairness issues that span both customers and producers. Our approach involves a novel mapping of the fair recommendation problem to a constrained version of the problem of fairly allocating indivisible goods. Our proposed FairRec algorithm guarantees at least Maximin Share (MMS) of exposure for most of the producers and Envy-Free up to One Good (EF1) fairness for every customer. Extensive evaluations over multiple real-world datasets show the effectiveness of FairRec in ensuring two-sided fairness while incurring a marginal loss in the overall recommendation quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51964560",
                    "name": "Gourab K. Patro"
                },
                {
                    "authorId": "2690810",
                    "name": "Arpita Biswas"
                },
                {
                    "authorId": "4213990",
                    "name": "Niloy Ganguly"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                },
                {
                    "authorId": "2676839",
                    "name": "Abhijnan Chakraborty"
                }
            ]
        },
        {
            "paperId": "24af653b0fe8007ea7fa8fa1a13a312d4e86a77c",
            "title": "Towards Safety and Sustainability: Designing Local Recommendations for Post-pandemic World",
            "abstract": "The COVID-19 pandemic has made it paramount to maintain social distance to limit the viral transmission probability. At the same time, local businesses (e.g., restaurants, cafes, stores, malls) need to operate to ensure their economic sustainability. Considering the wide usage of local recommendation platforms like Google Local and Yelp by customers to choose local businesses, we propose to design local recommendation systems which can help in achieving both safety and sustainability goals. Our investigation of existing local recommendation systems shows that they can lead to overcrowding at some businesses compromising customer safety, and very low footfall at other places threatening their economic sustainability. On the other hand, naive ways of ensuring safety and sustainability can cause significant loss in recommendation utility for the customers. Thus, we formally express the problem as a multi-objective optimization problem and solve by innovatively mapping it to a bipartite matching problem with polynomial time solutions. Extensive experiments over multiple real-world datasets reveal the efficacy of our approach along with the three-way control over sustainability, safety, and utility goals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51964560",
                    "name": "Gourab K. Patro"
                },
                {
                    "authorId": "2676839",
                    "name": "Abhijnan Chakraborty"
                },
                {
                    "authorId": "1949238793",
                    "name": "Ashmi Banerjee"
                },
                {
                    "authorId": "4213990",
                    "name": "Niloy Ganguly"
                }
            ]
        },
        {
            "paperId": "26edb738c7d2134e05a1015edcb02ce1398c1a4f",
            "title": "Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness",
            "abstract": "Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7233152",
                    "name": "J. Finocchiaro"
                },
                {
                    "authorId": "51131672",
                    "name": "R. Maio"
                },
                {
                    "authorId": "1396810650",
                    "name": "F. Monachou"
                },
                {
                    "authorId": "51964560",
                    "name": "Gourab K. Patro"
                },
                {
                    "authorId": "38009222",
                    "name": "Manish Raghavan"
                },
                {
                    "authorId": "5182072",
                    "name": "Ana-Andreea Stoica"
                },
                {
                    "authorId": "1491821380",
                    "name": "Stratis Tsirtsis"
                }
            ]
        }
    ]
}