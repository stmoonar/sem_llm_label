{
    "authorId": "3201827",
    "papers": [
        {
            "paperId": "9969fd03fa53d126124edc00fad3a23dd6a77ee3",
            "title": "SCOUT: A Situated and Multi-Modal Human-Robot Dialogue Corpus",
            "abstract": "We introduce the Situated Corpus Of Understanding Transactions (SCOUT), a multi-modal collection of human-robot dialogue in the task domain of collaborative exploration. The corpus was constructed from multiple Wizard-of-Oz experiments where human participants gave verbal instructions to a remotely-located robot to move and gather information about its surroundings. SCOUT contains 89,056 utterances and 310,095 words from 278 dialogues averaging 320 utterances per dialogue. The dialogues are aligned with the multi-modal data streams available during the experiments: 5,785 images and 30 maps. The corpus has been annotated with Abstract Meaning Representation and Dialogue-AMR to identify the speaker\u2019s intent and meaning within an utterance, and with Transactional Units and Relations to track relationships between utterances to reveal patterns of the Dialogue Structure. We describe how the corpus and its annotations have been used to develop autonomous human-robot systems and enable research in open questions of how humans speak to robots. We release this corpus to accelerate progress in autonomous, situated, human-robot dialogue, especially in the context of navigation tasks where details about the environment need to be discovered.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2003313",
                    "name": "S. Lukin"
                },
                {
                    "authorId": "3202888",
                    "name": "C. Bonial"
                },
                {
                    "authorId": "2372948",
                    "name": "M. Marge"
                },
                {
                    "authorId": "2185648281",
                    "name": "Taylor Hudson"
                },
                {
                    "authorId": "145454795",
                    "name": "C. Hayes"
                },
                {
                    "authorId": "2256514948",
                    "name": "Kimberly A. Pollard"
                },
                {
                    "authorId": "2278070028",
                    "name": "Anthony L. Baker"
                },
                {
                    "authorId": "5743783",
                    "name": "A. Foots"
                },
                {
                    "authorId": "122518132",
                    "name": "Ron Artstein"
                },
                {
                    "authorId": "4137017",
                    "name": "Felix Gervits"
                },
                {
                    "authorId": "2266063209",
                    "name": "Mitchell Abrams"
                },
                {
                    "authorId": "49914007",
                    "name": "Cassidy Henry"
                },
                {
                    "authorId": "2292305658",
                    "name": "Lucia Donatelli"
                },
                {
                    "authorId": "3201827",
                    "name": "Anton Leuski"
                },
                {
                    "authorId": "2301581663",
                    "name": "Susan G. Hill"
                },
                {
                    "authorId": "2251500479",
                    "name": "David Traum"
                },
                {
                    "authorId": "2285477225",
                    "name": "Clare R. Voss"
                }
            ]
        },
        {
            "paperId": "6ea1afbc09f471f2afb2e2da34a9fd6149ae6e2b",
            "title": "Comparing Statistical Models for Retrieval based Question-answering Dialogue: BERT vs Relevance Models",
            "abstract": "In this paper, we compare the performance of four models in a retrieval based question answering dialogue task on two moderately sized corpora (~ 10,000 utterances). One model is a statistical model and uses cross language relevance while the others are deep neural networks utilizing the BERT architecture along with different retrieval methods. The statistical model has previously outperformed LSTM based neural networks in a similar task whereas BERT has been proven to perform well on a variety of NLP tasks, achieving state-of-the-art results in many of them. Results show that the statistical cross language relevance model outperforms the BERT based architectures in learning question-answer mappings. BERT achieves better results by mapping new questions to existing questions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1994186471",
                    "name": "Debaditya Pal"
                },
                {
                    "authorId": "3201827",
                    "name": "Anton Leuski"
                },
                {
                    "authorId": "144518646",
                    "name": "D. Traum"
                }
            ]
        },
        {
            "paperId": "bcd7fe4f2da8054fc660e9aee094758b04a5915a",
            "title": "DIVIS: Digital Interactive Victim Intake Simulator",
            "abstract": "The Digital Interactive Victim Intake Simulator (\"DIVIS\") is an interactive, agent-based simulated training tool that has been deployed at the U.S. Army's Sexual Harassment/Assault Response Prevention Program (\"SHARP\") Academy since May 2021. The system allows student Sexual Assault Response Coordinators (\"SARCs\") and Victim Advocates (\"VAs\") to practice critical interpersonal intake skills needed when conducting the initial interview of a survivor of military sexual assault. Currently the system includes two scenarios -- one with a male victim and a second with a female victim -- with two more scenarios under development. Each victim exhibits one of a possible three different emotional vectors, (e.g., angry, ashamed or defensive). Scenarios can run multiple times, giving trainees the ability to navigate through various potential story paths based on how they engage with the victim during each session. After any given session, the system provides an after-action review (\"AAR\") illustrated by an interface that allows a video replay of the trainee's practice session with a timeline that is flagged to highlight relevant moments within the training scenario, based on verbal and nonverbal data collected by the system during the interaction. In addition, key topics that the trainee should cover in the course of an ideal intake interview are either checked or left unchecked in a \"Key Topics\" key on the interface.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49406416",
                    "name": "Alesia Gainer"
                },
                {
                    "authorId": "2275618745",
                    "name": "Allison Aptaker"
                },
                {
                    "authorId": "2038490",
                    "name": "Ron Artstein"
                },
                {
                    "authorId": "2275621149",
                    "name": "David Cobbins"
                },
                {
                    "authorId": "2275633634",
                    "name": "Mark Core"
                },
                {
                    "authorId": "2275601240",
                    "name": "Carla Gordon"
                },
                {
                    "authorId": "3201827",
                    "name": "Anton Leuski"
                },
                {
                    "authorId": "2256580500",
                    "name": "Zongjian Li"
                },
                {
                    "authorId": "2275633677",
                    "name": "Chirag Merchant"
                },
                {
                    "authorId": "2112886499",
                    "name": "David Nelson"
                },
                {
                    "authorId": "2256153190",
                    "name": "Mohammad Soleymani"
                },
                {
                    "authorId": "2251500479",
                    "name": "David Traum"
                }
            ]
        },
        {
            "paperId": "44e08d3ba95032ea9a2b185e15bd34740093f579",
            "title": "TaskMAD: A Platform for Multimodal Task-Centric Knowledge-Grounded Conversational Experimentation",
            "abstract": "The role of conversational assistants continues to evolve, beyond simple voice commands to ones that support rich and complex tasks in the home, car, and even virtual reality. Going beyond simple voice command and control requires agents and datasets blending structured dialogue, information seeking, grounded reasoning, and contextual question-answering in a multimodal environment with rich image and video content. In this demo, we introduce Task-oriented Multimodal Agent Dialogue (TaskMAD), a new platform that supports the creation of interactive multimodal and task-centric datasets in a Wizard-of-Oz experimental setup. TaskMAD includes support for text and voice, federated retrieval from text and knowledge bases, and structured logging of interactions for offline labeling. Its architecture supports a spectrum of tasks that span open-domain exploratory search to traditional frame-based dialogue tasks. It's open-source and offers rich capability as a platform used to collect data for the Amazon Alexa Prize Taskbot challenge, TREC Conversational Assistance track, undergraduate student research, and others. TaskMAD is distributed under the MIT license.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2175276993",
                    "name": "Alessandro Speggiorin"
                },
                {
                    "authorId": "145269114",
                    "name": "Jeffrey Dalton"
                },
                {
                    "authorId": "3201827",
                    "name": "Anton Leuski"
                }
            ]
        },
        {
            "paperId": "135edd73dbed8a50d0075416cdc403c7fb8b1b5c",
            "title": "Human swarm interaction using plays, audibles, and a virtual spokesperson",
            "abstract": "This study explores two hypotheses about human-agent teaming: 1. Real-time coordination among a large set of autonomous robots can be achieved using predefined \"plays\" which define how to execute a task, and \"audibles\" which modify the play on the fly. 2. A spokesperson agent can serve as a representative for a group of robots, relaying information between the robots and human teammates. These hypotheses are tested in a simulated game environment: a human participant leads a search-and-rescue operation to evacuate a town threatened by an approaching wildfire, with the object of saving as many lives as possible. The participant communicates verbally with a virtual agent controlling a team of ten aerial robots and one ground vehicle, while observing a live map display with real-time location of the fire and identified survivors. Since full automation is not currently possible, two human controllers control the agent's speech and actions, and input parameters to the robots, which then operate autonomously until the parameters are changed. Designated plays include monitoring the spread of fire, searching for survivors, broadcasting warnings, guiding residents to safety, and sending the rescue vehicle. A successful evacuation of all the residents requires personal intervention in some cases (e.g., stubborn residents) while delegating other responsibilities to the spokesperson agent and robots, all in a rapidly changing scene. The study records the participants' verbal and nonverbal behavior in order to identify strategies people use when communicating with robotic swarms, and to collect data for eventual automation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50629399",
                    "name": "P. Chaffey"
                },
                {
                    "authorId": "122518132",
                    "name": "Ron Artstein"
                },
                {
                    "authorId": "3194430",
                    "name": "Kallirroi Georgila"
                },
                {
                    "authorId": "1852064",
                    "name": "Kimberly A. Pollard"
                },
                {
                    "authorId": "40353538",
                    "name": "Setareh Nasihati Gilani"
                },
                {
                    "authorId": "46622929",
                    "name": "D. Krum"
                },
                {
                    "authorId": "2112886499",
                    "name": "David Nelson"
                },
                {
                    "authorId": "103328848",
                    "name": "K. Huynh"
                },
                {
                    "authorId": "49406416",
                    "name": "Alesia Gainer"
                },
                {
                    "authorId": "143654999",
                    "name": "S. Alavi"
                },
                {
                    "authorId": "2860406",
                    "name": "Rhys Yahata"
                },
                {
                    "authorId": "3201827",
                    "name": "Anton Leuski"
                },
                {
                    "authorId": "1388211500",
                    "name": "V. Yanov"
                },
                {
                    "authorId": "144518646",
                    "name": "D. Traum"
                }
            ]
        },
        {
            "paperId": "5d90ac5958c7f5f86b2bffc87fcc5f85ac8badd6",
            "title": "Evaluation of Off-the-shelf Speech Recognizers Across Diverse Dialogue Domains",
            "abstract": "We evaluate several publicly available off-the-shelf (commercial and research) automatic speech recognition (ASR) systems across diverse dialogue domains (in US-English). Our evaluation is aimed at non-experts with limited experience in speech recognition. Our goal is not only to compare a variety of ASR systems on several diverse data sets but also to measure how much ASR technology has advanced since our previous large-scale evaluations on the same data sets. Our results show that the performance of each speech recognizer can vary significantly depending on the domain. Furthermore, despite major recent progress in ASR technology, current state-of-the-art speech recognizers perform poorly in domains that require special vocabulary and language models, and under noisy conditions. We expect that our evaluation will prove useful to ASR consumers and dialogue system designers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3194430",
                    "name": "Kallirroi Georgila"
                },
                {
                    "authorId": "3201827",
                    "name": "Anton Leuski"
                },
                {
                    "authorId": "1388211500",
                    "name": "V. Yanov"
                },
                {
                    "authorId": "144518646",
                    "name": "D. Traum"
                }
            ]
        },
        {
            "paperId": "b5e68bc3dc714ab0fe049dd180f2dc326b40fb90",
            "title": "Agent Dialogue: A Platform for Conversational Information Seeking Experimentation",
            "abstract": "Conversational Information Seeking (CIS) is an emerging area of Information Retrieval focused on interactive search systems. As a result there is a need for new benchmark datasets and tools to enable their creation. In this demo we present the Agent Dialogue (AD) platform, an open-source system developed for researchers to perform Wizard-of-Oz CIS experiments. AD is a scalable cloud-native platform developed with Docker and Kubernetes with a flexible and modular micro-service architecture built on production-grade state-of-the-art open-source tools (Kubernetes, gRPC streaming, React, and Firebase). It supports varied front-ends and has the ability to interface with multiple existing agent systems, including Google Assistant and open-source search libraries. It includes support for centralized structure logging as well as offline relevance annotation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2061425801",
                    "name": "A. Czyzewski"
                },
                {
                    "authorId": "145269114",
                    "name": "Jeffrey Dalton"
                },
                {
                    "authorId": "3201827",
                    "name": "Anton Leuski"
                }
            ]
        },
        {
            "paperId": "f1fd7b0d4a1b208610eec5079cc80f4f869672cc",
            "title": "Which Model Should We Use for a Real-World Conversational Dialogue System? a Cross-Language Relevance Model or a Deep Neural Net?",
            "abstract": "We compare two models for corpus-based selection of dialogue responses: one based on cross-language relevance with a cross-language LSTM model. Each model is tested on multiple corpora, collected from two different types of dialogue source material. Results show that while the LSTM model performs adequately on a very large corpus (millions of utterances), its performance is dominated by the cross-language relevance model for a more moderate-sized corpus (ten thousands of utterances).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143654999",
                    "name": "S. Alavi"
                },
                {
                    "authorId": "3201827",
                    "name": "Anton Leuski"
                },
                {
                    "authorId": "144518646",
                    "name": "D. Traum"
                }
            ]
        },
        {
            "paperId": "17f0919ed5576c38cca46aa9c5e2eaba5a71cc75",
            "title": "UBeBot: voice-driven, personalized, avatar-based communicative video content in A/R",
            "abstract": "UBeBot allows a mobile user to create a 3D avatar of themselves using a photo, as well as dress and style the avatar. Users then record their voice, allowing the avatar to act our the content of the utterance, including lip sync, facial expressions, gestures and other body language. This animated performance is generated automatically by analyzing the recorded voice signal, and does not require any camera tracking. The 3D avatar can then be placed in augmented reality (A/R) and saved to a video for sharing on social media. A mobile user is thus able to create their own personalized, animated and voiced 3D A/R content. Performances can be saved and triggered to create the appearance of interactive conversations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145109163",
                    "name": "Ari Shapiro"
                },
                {
                    "authorId": "3201827",
                    "name": "Anton Leuski"
                },
                {
                    "authorId": "1788771",
                    "name": "S. Marsella"
                }
            ]
        },
        {
            "paperId": "775bba31770cebbf8a81ac08e34f6c3471388361",
            "title": "PRIMER: An Emotionally Aware Virtual Agent",
            "abstract": "PRIMER is a proof-of-concept system designed to show the potential of immersive dialogue agents and virtual environments that adapt and respond to both direct verbal input and indirect emotional input. The system has two novel interfaces: (1) for the user, an immersive VR environment and an animated virtual agent both of which adapt and react to the user\u2019s direct input as well as the user\u2019s perceived emotional state, and (2) for an observer, an interface that helps track the perceived emotional state of the user, with visualizations to provide insight into the system\u2019s decision making process. While the basic system architecture can be adapted for many potential real world applications, the initial version of this system was designed to assist clinical social workers in helping children cope with bullying. The virtual agent produces verbal and non-verbal behaviors guided by a plan for the counseling session, based on in-depth discussions with experienced counselors, but is also reactive to both initiatives that the user takes, e.g. asking their own questions, and the user\u2019s perceived emotional state.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40325729",
                    "name": "Carla Gordon"
                },
                {
                    "authorId": "3201827",
                    "name": "Anton Leuski"
                },
                {
                    "authorId": "31624455",
                    "name": "G. Benn"
                },
                {
                    "authorId": "144984764",
                    "name": "E. Klassen"
                },
                {
                    "authorId": "2432742",
                    "name": "Edward Fast"
                },
                {
                    "authorId": "2138666",
                    "name": "Matt Liewer"
                },
                {
                    "authorId": "1705118",
                    "name": "Arno Hartholt"
                },
                {
                    "authorId": "144518646",
                    "name": "D. Traum"
                }
            ]
        }
    ]
}