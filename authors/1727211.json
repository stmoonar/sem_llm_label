{
    "authorId": "1727211",
    "papers": [
        {
            "paperId": "25d5f9ef2e479ea3ad7f0a97bb0c9e21f5e88110",
            "title": "AI-Resilient Interfaces",
            "abstract": "AI is powerful, but it can make choices that result in objective errors, contextually inappropriate outputs, and disliked options. We need AI-resilient interfaces that help people be resilient to the AI choices that are not right, or not right for them. To support this goal, interfaces need to help users notice and have the context to appropriately judge those AI choices. Existing human-AI interaction guidelines recommend efficient user dismissal, modification, or otherwise efficient recovery from AI choices that a user does not like. However, in order to recover from AI choices, the user must notice them first. This can be difficult. For example, when generating summaries of long documents, a system's exclusion of a detail that is critically important to the user is hard for the user to notice. That detail can be hiding in a wall of text in the original document, and the existence of a summary may tempt the user not to read the original document as carefully. Once noticed, judging AI choices well can also be challenging. The interface may provide very little information that contextualizes the choices, and the user may fall back on assumptions when deciding whether to dismiss, modify, or otherwise recover from an AI choice. Building on prior work, this paper defines key aspects of AI-resilient interfaces, illustrated with examples. Designing interfaces for increased AI-resilience of users will improve AI safety, usability, and utility. This is especially critical where AI-powered systems are used for context- and preference-dominated open-ended AI-assisted tasks, like ideating, summarizing, searching, sensemaking, and the reading and writing of text or code.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257972100",
                    "name": "Elena L. Glassman"
                },
                {
                    "authorId": "2280066148",
                    "name": "Ziwei Gu"
                },
                {
                    "authorId": "1727211",
                    "name": "Jonathan K. Kummerfeld"
                }
            ]
        },
        {
            "paperId": "26b2adbe089ea36617c3ec0aa009319929da0550",
            "title": "A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity",
            "abstract": "While alignment algorithms are now commonly used to tune pre-trained language models towards a user's preferences, we lack explanations for the underlying mechanisms in which models become ``aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in a pre-trained language model, GPT2-medium. We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting model averts toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the model, reverting it back to its toxic behavior.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237957247",
                    "name": "Andrew Lee"
                },
                {
                    "authorId": "2277817046",
                    "name": "Xiaoyan Bai"
                },
                {
                    "authorId": "2277740563",
                    "name": "Itamar Pres"
                },
                {
                    "authorId": "2237803620",
                    "name": "Martin Wattenberg"
                },
                {
                    "authorId": "1727211",
                    "name": "Jonathan K. Kummerfeld"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                }
            ]
        },
        {
            "paperId": "36c0b8f26c1c05fb8b250877d5c75f1040942504",
            "title": "Supporting Sensemaking of Large Language Model Outputs at Scale",
            "abstract": "Large language models (LLMs) are capable of generating multiple responses to a single prompt, yet little effort has been expended to help end-users or system designers make use of this capability. In this paper, we explore how to present many LLM responses at once. We design five features, which include both pre-existing and novel methods for computing similarities and differences across textual documents, as well as how to render their outputs. We report on a controlled user study (n=24) and eight case studies evaluating these features and how they support users in different tasks. We find that the features support a wide variety of sensemaking tasks and even make tasks tractable that our participants previously considered to be too difficult to attempt. Finally, we present design guidelines to inform future explorations of new LLM interfaces.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2134414724",
                    "name": "Katy Ilonka Gero"
                },
                {
                    "authorId": "40964240",
                    "name": "Chelse Swoopes"
                },
                {
                    "authorId": "2280066148",
                    "name": "Ziwei Gu"
                },
                {
                    "authorId": "1727211",
                    "name": "Jonathan K. Kummerfeld"
                },
                {
                    "authorId": "2257972100",
                    "name": "Elena L. Glassman"
                }
            ]
        },
        {
            "paperId": "538b6f1d0ce39e568aaf53a7f49812e693f1a0cb",
            "title": "Do Text-to-Vis Benchmarks Test Real Use of Visualisations?",
            "abstract": "Large language models are able to generate code for visualisations in response to user requests. This is a useful application, and an appealing one for NLP research because plots of data provide grounding for language. However, there are relatively few benchmarks, and it is unknown whether those that exist are representative of what people do in practice. This paper aims to answer that question through an empirical study comparing benchmark datasets and code from public repositories. Our findings reveal a substantial gap in datasets, with evaluations not testing the same distribution of chart types, attributes, and the number of actions. The only representative dataset requires modification to become an end-to-end and practical benchmark. This shows that new, more benchmarks are needed to support the development of systems that truly address users' visualisation needs. These observations will guide future data creation, highlighting which features hold genuine significance for users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2313695874",
                    "name": "Hy Nguyen"
                },
                {
                    "authorId": "2313698231",
                    "name": "Xuefei He"
                },
                {
                    "authorId": "2240091332",
                    "name": "Andrew Reeson"
                },
                {
                    "authorId": "2313642394",
                    "name": "Cecile Paris"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                },
                {
                    "authorId": "1727211",
                    "name": "Jonathan K. Kummerfeld"
                }
            ]
        },
        {
            "paperId": "67f211fa66335dc32be2ec630b85a6f9026fa9de",
            "title": "What Can Natural Language Processing Do for Peer Review?",
            "abstract": "The number of scientific articles produced every year is growing rapidly. Providing quality control over them is crucial for scientists and, ultimately, for the public good. In modern science, this process is largely delegated to peer review -- a distributed procedure in which each submission is evaluated by several independent experts in the field. Peer review is widely used, yet it is hard, time-consuming, and prone to error. Since the artifacts involved in peer review -- manuscripts, reviews, discussions -- are largely text-based, Natural Language Processing has great potential to improve reviewing. As the emergence of large language models (LLMs) has enabled NLP assistance for many new tasks, the discussion on machine-assisted peer review is picking up the pace. Yet, where exactly is help needed, where can NLP help, and where should it stand aside? The goal of our paper is to provide a foundation for the future efforts in NLP for peer-reviewing assistance. We discuss peer review as a general process, exemplified by reviewing at AI conferences. We detail each step of the process from manuscript submission to camera-ready revision, and discuss the associated challenges and opportunities for NLP assistance, illustrated by existing work. We then turn to the big challenges in NLP for peer review as a whole, including data acquisition and licensing, operationalization and experimentation, and ethical issues. To help consolidate community efforts, we create a companion repository that aggregates key datasets pertaining to peer review. Finally, we issue a detailed call for action for the scientific community, NLP and AI researchers, policymakers, and funding bodies to help bring the research in NLP for peer review forward. We hope that our work will help set the agenda for research in machine-assisted scientific quality control in the age of AI, within the NLP community and beyond.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145775250",
                    "name": "Ilia Kuznetsov"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2301019670",
                    "name": "Koen Dercksen"
                },
                {
                    "authorId": "1740602147",
                    "name": "Nils Dycke"
                },
                {
                    "authorId": "2061487530",
                    "name": "Alexander Goldberg"
                },
                {
                    "authorId": "2301019717",
                    "name": "Tom Hope"
                },
                {
                    "authorId": "2324600714",
                    "name": "Dirk Hovy"
                },
                {
                    "authorId": "1727211",
                    "name": "Jonathan K. Kummerfeld"
                },
                {
                    "authorId": "29891652",
                    "name": "Anne Lauscher"
                },
                {
                    "authorId": "2066411743",
                    "name": "Kevin Leyton-Brown"
                },
                {
                    "authorId": "2237947104",
                    "name": "Sheng Lu"
                },
                {
                    "authorId": "2188374527",
                    "name": "Mausam"
                },
                {
                    "authorId": "2921990",
                    "name": "Margot Mieskes"
                },
                {
                    "authorId": "2190281078",
                    "name": "Aur'elie N'ev'eol"
                },
                {
                    "authorId": "2064506371",
                    "name": "Danish Pruthi"
                },
                {
                    "authorId": "2301070055",
                    "name": "Lizhen Qu"
                },
                {
                    "authorId": "2287451699",
                    "name": "Roy Schwartz"
                },
                {
                    "authorId": "2288092270",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "1794626",
                    "name": "T. Solorio"
                },
                {
                    "authorId": "2301091804",
                    "name": "Jingyan Wang"
                },
                {
                    "authorId": "2279988869",
                    "name": "Xiaodan Zhu"
                },
                {
                    "authorId": "2301020698",
                    "name": "Anna Rogers"
                },
                {
                    "authorId": "2266853981",
                    "name": "Nihar B. Shah"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                }
            ]
        },
        {
            "paperId": "889326d971894a8f3a3fb515655b8b3740b3609e",
            "title": "More Victories, Less Cooperation: Assessing Cicero's Diplomacy Play",
            "abstract": "The boardgame Diplomacy is a challenging setting for communicative and cooperative artificial intelligence. The most prominent communicative Diplomacy AI, Cicero, has excellent strategic abilities, exceeding human players. However, the best Diplomacy players master communication, not just tactics, which is why the game has received attention as an AI challenge. This work seeks to understand the degree to which Cicero succeeds at communication. First, we annotate in-game communication with abstract meaning representation to separate in-game tactics from general language. Second, we run two dozen games with humans and Cicero, totaling over 200 human-player hours of competition. While AI can consistently outplay human players, AI-Human communication is still limited because of AI's difficulty with deception and persuasion. This shows that Cicero relies on strategy and has not yet reached the full promise of communicative and cooperative AI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2179109975",
                    "name": "Wichayaporn Wongkamjan"
                },
                {
                    "authorId": "2305486610",
                    "name": "Feng Gu"
                },
                {
                    "authorId": "2305557360",
                    "name": "Yanze Wang"
                },
                {
                    "authorId": "2305485992",
                    "name": "Ulf Hermjakob"
                },
                {
                    "authorId": "2305486683",
                    "name": "Jonathan May"
                },
                {
                    "authorId": "2305486572",
                    "name": "Brandon M. Stewart"
                },
                {
                    "authorId": "1727211",
                    "name": "Jonathan K. Kummerfeld"
                },
                {
                    "authorId": "2221286885",
                    "name": "Denis Peskoff"
                },
                {
                    "authorId": "2267532838",
                    "name": "Jordan L. Boyd-Graber"
                }
            ]
        },
        {
            "paperId": "b1027a2cfb8305c76f97cf719f09023132ceeed2",
            "title": "An AI-Resilient Text Rendering Technique for Reading and Skimming Documents",
            "abstract": "Readers find text difficult to consume for many reasons. Summarization can address some of these difficulties, but introduce others, such as omitting, misrepresenting, or hallucinating information, which can be hard for a reader to notice. One approach to addressing this problem is to instead modify how the original text is rendered to make important information more salient. We introduce Grammar-Preserving Text Saliency Modulation (GP-TSM), a text rendering method with a novel means of identifying what to de-emphasize. Specifically, GP-TSM uses a recursive sentence compression method to identify successive levels of detail beyond the core meaning of a passage, which are de-emphasized by rendering words in successively lighter but still legible gray text. In a lab study (n=18), participants preferred GP-TSM over pre-existing word-level text rendering methods and were able to answer GRE reading comprehension questions more efficiently.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280066148",
                    "name": "Ziwei Gu"
                },
                {
                    "authorId": "2243191212",
                    "name": "Ian Arawjo"
                },
                {
                    "authorId": "2280327413",
                    "name": "Kenneth Li"
                },
                {
                    "authorId": "1727211",
                    "name": "Jonathan K. Kummerfeld"
                },
                {
                    "authorId": "2257972100",
                    "name": "Elena L. Glassman"
                }
            ]
        },
        {
            "paperId": "55739eaba9f3f954996bd72a27ac5ee0cfec8520",
            "title": "Leveraging Similar Users for Personalized Language Modeling with Limited Data",
            "abstract": "Personalized language models are designed and trained to capture language patterns specific to individual users. This makes them more accurate at predicting what a user will write. However, when a new user joins a platform and not enough text is available, it is harder to build effective personalized language models. We propose a solution for this problem, using a model trained on users that are similar to a new user. In this paper, we explore strategies for finding the similarity between new users and existing ones and methods for using the data from existing users who are a good match. We further explore the trade-off between available data for new users and how well their language can be modeled.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145645240",
                    "name": "Charles F Welch"
                },
                {
                    "authorId": "2053399628",
                    "name": "Chenxi Gu"
                },
                {
                    "authorId": "1727211",
                    "name": "Jonathan K. Kummerfeld"
                },
                {
                    "authorId": "1396239754",
                    "name": "Ver\u00f3nica P\u00e9rez-Rosas"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                }
            ]
        },
        {
            "paperId": "c25bd2ded301e9c501d9d08428087f6e05cf4b0d",
            "title": "Using Paraphrases to Study Properties of Contextual Embeddings",
            "abstract": "We use paraphrases as a unique source of data to analyze contextualized embeddings, with a particular focus on BERT. Because paraphrases naturally encode consistent word and phrase semantics, they provide a unique lens for investigating properties of embeddings. Using the Paraphrase Database\u2019s alignments, we study words within paraphrases as well as phrase representations. We find that contextual embeddings effectively handle polysemous words, but give synonyms surprisingly different representations in many cases. We confirm previous findings that BERT is sensitive to word order, but find slightly different patterns than prior work in terms of the level of contextualization across BERT\u2019s layers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491971694",
                    "name": "Laura Burdick"
                },
                {
                    "authorId": "1727211",
                    "name": "Jonathan K. Kummerfeld"
                },
                {
                    "authorId": "145557251",
                    "name": "Rada Mihalcea"
                }
            ]
        },
        {
            "paperId": "40a37bc3de182713c81c702d2e0b687a87609863",
            "title": "Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health",
            "abstract": "Many statistical models have high accuracy on test benchmarks, but are not explainable, struggle in low-resource scenarios, cannot be reused for multiple tasks, and cannot easily integrate domain expertise. These factors limit their use, particularly in settings such as mental health, where it is difficult to annotate datasets and model outputs have significant impact. We introduce a micromodel architecture to address these challenges. Our approach allows researchers to build interpretable representations that embed domain knowledge and provide explanations throughout the model's decision process. We demonstrate the idea on multiple mental health tasks: depression classification, PTSD classification, and suicidal risk assessment. Our systems consistently produce strong results, even in low-resource scenarios, and are more interpretable than alternative methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116598740",
                    "name": "Andrew Lee"
                },
                {
                    "authorId": "1727211",
                    "name": "Jonathan K. Kummerfeld"
                },
                {
                    "authorId": "2113905187",
                    "name": "Lawrence C. An"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                }
            ]
        }
    ]
}