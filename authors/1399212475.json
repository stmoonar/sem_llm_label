{
    "authorId": "1399212475",
    "papers": [
        {
            "paperId": "6cb99af50b91fed276b7b6cee4c91cea5d222ae3",
            "title": "PriMeSRL-Eval: A Practical Quality Metric for Semantic Role Labeling Systems Evaluation",
            "abstract": "Semantic role labeling (SRL) identifies the predicate-argument structure in a sentence. This task is usually accomplished in four steps: predicate identification, predicate sense disambiguation, argument identification, and argument classification. Errors introduced at one step propagate to later steps. Unfortunately, the existing SRL evaluation scripts do not consider the full effect of this error propagation aspect. They either evaluate arguments independent of predicate sense (CoNLL09) or do not evaluate predicate sense at all (CoNLL05), yielding an inaccurate SRL model performance on the argument classification task. In this paper, we address key practical issues with existing evaluation scripts and propose a more strict SRL evaluation metric PriMeSRL. We observe that by employing PriMeSRL, the quality evaluation of all SoTA SRL models drops significantly, and their relative rankings also change. We also show that PriMeSRLsuccessfully penalizes actual failures in SoTA SRL models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144377686",
                    "name": "Ishan Jindal"
                },
                {
                    "authorId": "13836343",
                    "name": "Alexandre Rademaker"
                },
                {
                    "authorId": "1399212475",
                    "name": "Khoi-Nguyen Tran"
                },
                {
                    "authorId": "2115718238",
                    "name": "Huaiyu Zhu"
                },
                {
                    "authorId": "35562751",
                    "name": "H. Kanayama"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                },
                {
                    "authorId": "1718694",
                    "name": "Yunyao Li"
                }
            ]
        },
        {
            "paperId": "83e3b3b19bbe92a85265bcee6634738206662f03",
            "title": "Universal Proposition Bank 2.0",
            "abstract": "Semantic role labeling (SRL) represents the meaning of a sentence in the form of predicate-argument structures. Such shallow semantic analysis is helpful in a wide range of downstream NLP tasks and real-world applications. As treebanks enabled the development of powerful syntactic parsers, the accurate predicate-argument analysis demands training data in the form of propbanks. Unfortunately, most languages simply do not have corresponding propbanks due to the high cost required to construct such resources. To overcome such challenges, Universal Proposition Bank 1.0 (UP1.0) was released in 2017, with high-quality propbank data generated via a two-stage method exploiting monolingual SRL and multilingual parallel data. In this paper, we introduce Universal Proposition Bank 2.0 (UP2.0), with significant enhancements over UP1.0: (1) propbanks with higher quality by using a state-of-the-art monolingual SRL and improved auto-generation of annotations; (2) expanded language coverage (from 7 to 9 languages); (3) span annotation for the decoupling of syntactic analysis; and (4) Gold data for a subset of the languages. We also share our experimental results that confirm the significant quality improvements of the generated propbanks. In addition, we present a comprehensive experimental evaluation on how different implementation choices impact the quality of the resulting data. We release these resources to the research community and hope to encourage more research on cross-lingual SRL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144377686",
                    "name": "Ishan Jindal"
                },
                {
                    "authorId": "13836343",
                    "name": "Alexandre Rademaker"
                },
                {
                    "authorId": "2185433526",
                    "name": "Micha\u0142 Ulewicz"
                },
                {
                    "authorId": "2180951336",
                    "name": "Linh H. Ha"
                },
                {
                    "authorId": "145199659",
                    "name": "Huyen Nguyen"
                },
                {
                    "authorId": "1399212475",
                    "name": "Khoi-Nguyen Tran"
                },
                {
                    "authorId": "2115718238",
                    "name": "Huaiyu Zhu"
                },
                {
                    "authorId": "1718694",
                    "name": "Yunyao Li"
                }
            ]
        },
        {
            "paperId": "7fbbee7b183660cf7d742997341ffae57f3e525f",
            "title": "Cognitive Compliance: Assessing Regulatory Risk in Financial Advice Documents",
            "abstract": "This paper describes Cognitive Compliance - a solution that automates the complex manual process of assessing regulatory compliance of personal financial advice. The solution uses natural language processing (NLP), machine learning and deep learning to characterise the regulatory risk status of personal financial advice documents with traffic light rating for various risk factors. This enables comprehensive coverage of the review and rapid identification of documents at high risk of non-compliance with government regulations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2002492",
                    "name": "W. Sherchan"
                },
                {
                    "authorId": "2111241206",
                    "name": "S. A. Chen"
                },
                {
                    "authorId": "2113974447",
                    "name": "Simon Harris"
                },
                {
                    "authorId": "2065635804",
                    "name": "Nebula Alam"
                },
                {
                    "authorId": "1399212475",
                    "name": "Khoi-Nguyen Tran"
                },
                {
                    "authorId": "2056211505",
                    "name": "Christopher J. Butler"
                }
            ]
        },
        {
            "paperId": "fce58b33858ccd5be54e631dabbf0e4795ce869d",
            "title": "Assessing Regulatory Risk in Personal Financial Advice Documents: a Pilot Study",
            "abstract": "Assessing regulatory compliance of personal financial advice is currently a complex manual process. In Australia, only 5%- 15% of advice documents are audited annually and 75% of these are found to be non-compliant(ASI 2018b). This paper describes a pilot with an Australian government regulation agency where Artificial Intelligence (AI) models based on techniques such natural language processing (NLP), machine learning and deep learning were developed to methodically characterise the regulatory risk status of personal financial advice documents. The solution provides traffic light rating of advice documents for various risk factors enabling comprehensive coverage of documents in the review and allowing rapid identification of documents that are at high risk of non-compliance with government regulations. This pilot serves as a case study of public-private partnership in developing AI systems for government and public sector.",
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "authors": [
                {
                    "authorId": "2002492",
                    "name": "W. Sherchan"
                },
                {
                    "authorId": "2113974447",
                    "name": "Simon Harris"
                },
                {
                    "authorId": "2111241206",
                    "name": "S. A. Chen"
                },
                {
                    "authorId": "2065635804",
                    "name": "Nebula Alam"
                },
                {
                    "authorId": "1399212475",
                    "name": "Khoi-Nguyen Tran"
                },
                {
                    "authorId": "3120277",
                    "name": "Adam J. Makarucha"
                },
                {
                    "authorId": "2056211505",
                    "name": "Christopher J. Butler"
                }
            ]
        },
        {
            "paperId": "0a279710ad0d5270590961b0e002b05fa9effbf5",
            "title": "Document Chunking and Learning Objective Generation for Instruction Design",
            "abstract": "Instructional Systems Design is the practice of creating of instructional experiences that make the acquisition of knowledge and skill more efficient, effective, and appealing. Specifically in designing courses, an hour of training material can require between 30 to 500 hours of effort in sourcing and organizing reference data for use in just the preparation of course material. In this paper, we present the first system of its kind that helps reduce the effort associated with sourcing reference material and course creation. We present algorithms for document chunking and automatic generation of learning objectives from content, creating descriptive content metadata to improve content-discoverability. Unlike existing methods, the learning objectives generated by our system incorporate pedagogically motivated Bloom's verbs. We demonstrate the usefulness of our methods using real world data from the banking industry and through a live deployment at a large pharmaceutical company.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1399212475",
                    "name": "Khoi-Nguyen Tran"
                },
                {
                    "authorId": "1800564",
                    "name": "Jey Han Lau"
                },
                {
                    "authorId": "2075459",
                    "name": "Danish Contractor"
                },
                {
                    "authorId": "2058888214",
                    "name": "Utkarsh Gupta"
                },
                {
                    "authorId": "1776870",
                    "name": "Bikram Sengupta"
                },
                {
                    "authorId": "2056211505",
                    "name": "Christopher J. Butler"
                },
                {
                    "authorId": "1732053",
                    "name": "M. Mohania"
                }
            ]
        },
        {
            "paperId": "35bb846d8fb687db93837b9886ec17bab0837d47",
            "title": "Agent-based model of WiFi tracking system in urban environment (Version 1)",
            "abstract": "The model (written in basic Matlab, convertable to Octave) provides proof-of-concept for a sensor network system for tracking WiFi users in outdoor urban environments. Sensors are fixed, and are capable of measuring signal power from users\u2019 WiFi devices. The location algorithm takes into account the effects of power control, and is self-calibrating in that the signal power model used by the algorithm is adjusted and improved as part of the operation of the\u00a0network.\n\nThe Matlab code includes testbench files to generate simulation results which verify the system\u2019s performance. The simulation scenario is based on a 1.5 km2 area of lower Manhattan, The self-calibration mechanism was verified for initial rms (root mean square) errors of up to 12 dB in the channel power estimates: rms errors were reduced by over 60% in 300 track-hours, in systems with limited power control. Under typical operating conditions with (without) power control, location rms errors are about 8.5 (5) meters with 90% accuracy within 9 (13) meters, for both pedestrian and vehicular users. The distance error distributions for smaller distances (<30 m) are well-approximated by an exponential distribution, while the distributions for large distance errors have fat\u00a0tails.\n\nAlso included is an Octave program which utilizes an integer linear programming algorithm to determine sensor placement for networks with reduced number of sensors. In our test case, the algorithm produces a network with 18.5% fewer sensors with comparable accuracy estimation\u00a0performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1399212475",
                    "name": "Khoi-Nguyen Tran"
                }
            ]
        },
        {
            "paperId": "89bcae74fbf0413bfd1b88a3b25c4c4a106aed86",
            "title": "End-to-end Network for Twitter Geolocation Prediction and Hashing",
            "abstract": "We propose an end-to-end neural network to predict the geolocation of a tweet. The network takes as input a number of raw Twitter metadata such as the tweet message and associated user account information. Our model is language independent, and despite minimal feature engineering, it is interpretable and capable of learning location indicative words and timing patterns. Compared to state-of-the-art systems, our model outperforms them by 2%-6%. Additionally, we propose extensions to the model to compress representation learnt by the network into binary codes. Experiments show that it produces compact codes compared to benchmark hashing algorithms. An implementation of the model is released publicly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800564",
                    "name": "Jey Han Lau"
                },
                {
                    "authorId": "1754670",
                    "name": "Lianhua Chi"
                },
                {
                    "authorId": "1399212475",
                    "name": "Khoi-Nguyen Tran"
                },
                {
                    "authorId": "143620680",
                    "name": "Trevor Cohn"
                }
            ]
        },
        {
            "paperId": "62fae25bd03797eda3f2d70d262f6156e750d9d9",
            "title": "Automatic Discovery of Abnormal Values in Large Textual Databases",
            "abstract": "Textual databases are ubiquitous in many application domains. Examples of textual data range from names and addresses of customers to social media posts and bibliographic records. With online services, individuals are increasingly required to enter their personal details for example when purchasing products online or registering for government services, while many social network and e-commerce sites allow users to post short comments. Many online sites leave open the possibility for people to enter unintended or malicious abnormal values, such as names with errors, bogus values, profane comments, or random character sequences. In other applications, such as online bibliographic databases or comparative online shopping sites, databases are increasingly populated in (semi-) automatic ways through Web crawls. This practice can result in low quality data being added automatically into a database. In this article, we develop three techniques to automatically discover abnormal (unexpected or unusual) values in large textual databases. Following recent work in categorical outlier detection, our assumption is that \u201cnormal\u201d values are those that occur frequently in a database, while an individual abnormal value is rare. Our techniques are unsupervised and address the challenge of discovering abnormal values as an outlier detection problem. Our first technique is a basic but efficient q-gram set based technique, the second is based on a probabilistic language model, and the third employs morphological word features to train a one-class support vector machine classifier. Our aim is to investigate and develop techniques that are fast, efficient, and automatic. The output of our techniques can help in the development of rule-based data cleaning and information extraction systems, or be used as training data for further supervised data cleaning procedures. We evaluate our techniques on four large real-world datasets from different domains: two US voter registration databases containing personal details, the 2013 KDD Cup dataset of bibliographic records, and the SNAP Memetracker dataset of phrases from social networking sites. Our results show that our techniques can efficiently and automatically discover abnormal textual values, allowing an organization to conduct efficient data exploration, and improve the quality of their textual databases without the need of requiring explicit training data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145188508",
                    "name": "P. Christen"
                },
                {
                    "authorId": "3051216",
                    "name": "Ross W. Gayler"
                },
                {
                    "authorId": "1399212475",
                    "name": "Khoi-Nguyen Tran"
                },
                {
                    "authorId": "2072758735",
                    "name": "Jeffrey Fisher"
                },
                {
                    "authorId": "1736048",
                    "name": "Dinusha Vatsalan"
                }
            ]
        },
        {
            "paperId": "52019c64adf89cfe2568b0f1722638374e1d59bc",
            "title": "Cross-Language Learning from Bots and Users to Detect Vandalism on Wikipedia",
            "abstract": "Vandalism, the malicious modification of articles, is a serious problem for open access encyclopedias such as Wikipedia. The use of counter-vandalism bots is changing the way Wikipedia identifies and bans vandals, but their contributions are often not considered nor discussed. In this paper, we propose novel text features capturing the invariants of vandalism across five languages to learn and compare the contributions of bots and users in the task of identifying vandalism. We construct computationally efficient features that highlight the contributions of bots and users, and generalize across languages. We evaluate our proposed features through classification performance on revisions of five Wikipedia languages, totaling over 500 million revisions of over nine million articles. As a comparison, we evaluate these features on the small PAN Wikipedia vandalism data sets, used by previous research, which contain approximately 62,000 revisions. We show differences in the performance of our features on the PAN and the full Wikipedia data set. With the appropriate text features, vandalism bots can be effective across different languages while learning from only one language. Our ultimate aim is to build the next generation of vandalism detection bots based on machine learning approaches that can work effectively across many languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1399212475",
                    "name": "Khoi-Nguyen Tran"
                },
                {
                    "authorId": "145188508",
                    "name": "P. Christen"
                }
            ]
        },
        {
            "paperId": "55edd4757ae4275df1760616d0da7a421ed71025",
            "title": "Detecting Vandalism on Wikipedia across Multiple Languages",
            "abstract": "Vandalism, the malicious modification or editing of articles, is a serious problem for free and open access online encyclopedias such as Wikipedia. Over the 13 year lifetime of Wikipedia, editors have identified and repaired vandalism in 1.6% of more than 500 million revisions of over 9 million English articles, but smaller manually inspected sets of revisions for research show vandalism may appear in 7% to 11% of all revisions of English Wikipedia articles. The persistent threat of vandalism has led to the development of automated programs (bots) and editing assistance programs to help editors detect and repair vandalism. Research into improving vandalism detection through application of machine learning techniques have shown significant improvements to detection rates of a wider variety of vandalism. However, the focus of research is often only on the English Wikipedia, which has led us to develop a novel research area of cross-language vandalism detection (CLVD). CLVD provides a solution to detecting vandalism across several languages through the development of language-independent machine learning models. These models can identify undetected vandalism cases across languages that may have insufficient identified cases to build learning models. The two main challenges of CLVD are (1) identifying language-independent features of vandalism that are common to multiple languages, and (2) extensibility of vandalism detection models trained in one language to other languages without significant loss in detection rate. In addition, other important challenges of vandalism detection are (3) high detection rate of a variety of known vandalism types, (4) scalability to the size of Wikipedia in the number of revisions, and (5) ability to incorporate and generate multiple types of data that characterise vandalism. In this thesis, we present our research into CLVD on Wikipedia, where we identify gaps and problems in existing vandalism detection techniques. To begin our thesis, we introduce the problem of vandalism on Wikipedia with motivating examples, and then present a review of the literature. From this review, we identify and address the following research gaps. First, we propose techniques for summarising the user activity of articles and comparing the knowledge coverage of articles across languages. Second, we investigate CLVD using the metadata of article revisions together with article views to learn vandalism models and classify incoming revisions. Third, we propose new text features that are more suitable for CLVD than text features from the literature. Fourth, we propose a novel context-aware vandalism detection technique for sneaky types of vandalism that may not be detectable through constructing features. Finally, to show that our techniques of detecting malicious activities are not limited to Wikipedia, we apply our feature sets to detecting malicious attachments and URLs in spam emails. Overall, our ultimate aim is to build the next generation of vandalism detection bots that can learn and detect vandalism from multiple languages and extend their usefulness to other language editions of Wikipedia.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1399212475",
                    "name": "Khoi-Nguyen Tran"
                }
            ]
        }
    ]
}