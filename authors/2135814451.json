{
    "authorId": "2135814451",
    "papers": [
        {
            "paperId": "317e524f5280e7cfdb52bddfa0536b9ee36bbfd1",
            "title": "SHEVA: A Visual Analytics System for Statistical Hypothesis Exploration",
            "abstract": "We demonstrate SHEVA, a System for Hypothesis Exploration with Visual Analytics. SHEVA adopts an Exploratory Data Analysis (EDA) approach to discovering statistically-sound insights from large datasets. The system addresses three longstanding challenges in Multiple Hypothesis Testing: (i) the likelihood of rejecting the null hypothesis by chance, (ii) the pitfall of not being representative of the input data, and (iii) the ability to navigate among many data regions while preserving the user's train of thought. To address (i) & (ii), SHEVA implements significance adjustment methods that account for data-informed properties such as coverage and novelty. To address (iii), SHEVA proposes to guide users by recommending one-sample and two-sample hypotheses in a stepwise fashion following a data hierarchy. Users may choose from a collection of pre-trained hypothesis exploration policies and let SHEVA guide them through the most significant hypotheses in the data, or intervene to override suggested hypotheses. Furthermore, SHEVA relies on data-to-visual element mappings to convey hypothesis testing results in an interpretable fashion, and allows hypothesis pipelines to be stored and retrieved later to be tested on new datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257369136",
                    "name": "Vicente Nejar de Almeida"
                },
                {
                    "authorId": "2239823093",
                    "name": "Eduardo Ribeiro"
                },
                {
                    "authorId": "2135814451",
                    "name": "Nassim Bouarour"
                },
                {
                    "authorId": "1737524",
                    "name": "J. Comba"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "de4f38ff2aa49a1eca93f08680a4d5aafb4d45f6",
            "title": "Adaptive Test Recommendation for Mastery Learning",
            "abstract": "We tackle the problem of recommending tests to learners to achieve upskilling. Our work is grounded in two learning theories: mastery learning, an instructional strategy that guides learners by providing them tests of increasing difficulty, reviewing their test results, and iterating until they reach a level of mastery; Flow Theory, which identifies different test zones, frustration, learnable, flow and boredom zones, to determine the best k tests to recommend to a learner. We formalize the AdUp Problem and develop a multi-objective optimization solution that adapts the difficulty of recommended tests to the learner's predicted performance, aptitude, and skill gap. We leverage existing models to simulate learner behavior and run experiments to demonstrate that our formalization is best to attain skill mastery. We discuss open research directions including the applicability of reinforcement learning and the recommendation of peers in collaborative projects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135814451",
                    "name": "Nassim Bouarour"
                },
                {
                    "authorId": "2038836",
                    "name": "Idir Benouaret"
                },
                {
                    "authorId": "2080883170",
                    "name": "C. D'Ham"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "7ea8005fecec0d57419aaf66a8f4b9da38a2e8e4",
            "title": "Learning Diversity Attributes in Multi-Session Recommendations",
            "abstract": "Diversity in recommendation has been studied extensively. It has been shown that maximizing diversity subject to constrained relevance yields high user engagement over time. Existing work largely relies on setting some attributes that are used to craft an item similarity function and diversify results. In this paper, we examine the question of learning diversity attributes. That is particularly important when users receive recommendations over multiple sessions. We devise two main approaches to look for the best diversity attribute in each session: the first is a generalization of traditional diversity algorithms and the second is based on reinforcement learning. We implement both approaches and run extensive experiments on a semi-synthetic dataset. Our results demonstrate that learning diversity attributes yields a higher overall diversity than traditional diversity algorithms. We also find that training policies using reinforcement learning is more efficient in terms of response time, in particular for high dimensional data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135814451",
                    "name": "Nassim Bouarour"
                },
                {
                    "authorId": "2038836",
                    "name": "Idir Benouaret"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "d9e92f34332742e6fe268f08cb2cb4a9a809448a",
            "title": "Significance and Coverage in Group Testing on the Social Web",
            "abstract": "We tackle the longstanding question of checking hypotheses on the social Web. In particular, we address the challenges that arise in the context of testing an input hypothesis on many data samples, in our case, user groups. This is referred to as Multiple Hypothesis Testing, a method of choice for data-driven discoveries. Ensuring sound discoveries in large datasets poses two challenges: the likelihood of accepting a hypothesis by chance, i.e., returning false discoveries, and the pitfall of not being representative of the input data. We develop GroupTest, a framework for group testing that addresses both challenges. We formulate CoverTest, a generic top-n problem that seeks n user groups satisfying one-sample, two-sample, or multiple-sample tests, and maximizing data coverage. We show the hardness of CoverTest and develop a greedy algorithm with a provable approximation guarantee as well as a faster heuristic-based algorithm based on \u03b1-investing. Our extensive experiments on four real-world datasets demonstrate the necessity to optimize coverage for sound data-driven discoveries, and the efficiency of our heuristic-based algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135814451",
                    "name": "Nassim Bouarour"
                },
                {
                    "authorId": "2038836",
                    "name": "Idir Benouaret"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "5480019f7a5f9cabad878f76e06dced41b5b7ad7",
            "title": "DashBot: An ML-Guided Dashboard Generation System",
            "abstract": "Data summarization provides a bird's eye view of data and groupby queries have been the method of choice for data summarization. Such queries provide the ability to group by some attributes and aggregate by others, and their results can be coupled with a visualization to convey insights. The number of possible groupbys that can be computed over a dataset is quite large which naturally calls for developing approaches to aid users in choosing which groupbys best summarize data. We demonstrate DashBot, a system that leverages Machine Learning to guide users in generating data-driven and customized dashboards. A dashboard contains a set of panels, each of which is a groupby query. DashBot iteratively recommends the most relevant panel while ensuring coverage. Relevance is computed based on intrinsic measures of the dataset and coverage aims to provide comprehensive summaries. DashBot relies on a Multi-Armed Bandits (MABs) approach to balance exploitation of relevance and exploration of different regions of the data to achieve coverage. Users can provide feedback and explanations to customize recommended panels. We demonstrate the utility and features of DashBot on different datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "102397366",
                    "name": "S. D. Col"
                },
                {
                    "authorId": "1850588",
                    "name": "Radu Ciucanu"
                },
                {
                    "authorId": "144004707",
                    "name": "Marta Soare"
                },
                {
                    "authorId": "2135814451",
                    "name": "Nassim Bouarour"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "971b23ba60ac0fb6cd8ce6ae9f96374388e21146",
            "title": "How Useful is Meta-Recommendation? An Empirical Investigation",
            "abstract": "Despite the proliferation of recommendation algorithms, the question of which recommender works best for which user-item instance remains widely open. In this paper, we develop a meta-learning approach that chooses among several recommendation algorithms, which one is best suited for predicting the preference of a user for an item. We propose an empirical investigation of the meta-learner when applied to implicit and explicit datasets. The meta-learner is trained using four classifiers/regressors: logistic regression, decision trees, stochastic gradient descent, and gradient boosting. We run extensive experiments on four real datasets: RETAIL, a proprietary implicit dataset provided by our industrial partner, TAFENG, a publicly available grocery shopping dataset and two publicly available AMAZON datasets with explicit preferences. Results show that using a meta-learner yields higher accuracy than single recommendation algorithms for explicit datasets when compared to state-of-the-art ensemble-learned models and factorization machines. This work is an ongoing collaboration with the marketing department of a major industrial partner to test promotional offers for different customer segments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135814451",
                    "name": "Nassim Bouarour"
                },
                {
                    "authorId": "2038836",
                    "name": "Idir Benouaret"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        }
    ]
}