{
    "authorId": "2218044799",
    "papers": [
        {
            "paperId": "14daac32518c1bd872a53cb5b4fb38aed92f31b8",
            "title": "Beyond Individual Facts: Investigating Categorical Knowledge Locality of Taxonomy and Meronomy Concepts in GPT Models",
            "abstract": "The location of knowledge within Generative Pre-trained Transformer (GPT)-like models has seen extensive recent investigation. However, much of the work is focused towards determining locations of individual facts, with the end goal being the editing of facts that are outdated, erroneous, or otherwise harmful, without the time and expense of retraining the entire model. In this work, we investigate a broader view of knowledge location, that of concepts or clusters of related information, instead of disparate individual facts. To do this, we first curate a novel dataset, called DARC, that includes a total of 34 concepts of ~120K factual statements divided into two types of hierarchical categories, namely taxonomy and meronomy. Next, we utilize existing causal mediation analysis methods developed for determining regions of importance for individual facts and apply them to a series of related categories to provide detailed investigation into whether concepts are associated with distinct regions within these models. We find that related categories exhibit similar areas of importance in contrast to less similar categories. However, fine-grained localization of individual category subsets to specific regions is not apparent.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218044799",
                    "name": "Christopher Burger"
                },
                {
                    "authorId": "2308045416",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "2258868683",
                    "name": "Thai Le"
                }
            ]
        },
        {
            "paperId": "c313716d8e19157f369f30bb71f3a8454dc1debb",
            "title": "The Effect of Similarity Measures on Accurate Stability Estimates for Local Surrogate Models in Text-based Explainable AI",
            "abstract": "Recent work has investigated the vulnerability of local surrogate methods to adversarial perturbations on a machine learning (ML) model's inputs, where the explanation is manipulated while the meaning and structure of the original input remains similar under the complex model. While weaknesses across many methods have been shown to exist, the reasons behind why still remain little explored. Central to the concept of adversarial attacks on explainable AI (XAI) is the similarity measure used to calculate how one explanation differs from another A poor choice of similarity measure can result in erroneous conclusions on the efficacy of an XAI method. Too sensitive a measure results in exaggerated vulnerability, while too coarse understates its weakness. We investigate a variety of similarity measures designed for text-based ranked lists including Kendall's Tau, Spearman's Footrule and Rank-biased Overlap to determine how substantial changes in the type of measure or threshold of success affect the conclusions generated from common adversarial attack processes. Certain measures are found to be overly sensitive, resulting in erroneous estimates of stability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218044799",
                    "name": "Christopher Burger"
                },
                {
                    "authorId": "2308032442",
                    "name": "Charles Walter"
                },
                {
                    "authorId": "2258868683",
                    "name": "Thai Le"
                }
            ]
        },
        {
            "paperId": "0d791a32241198d2f9ee2ca33aa719b5484785b6",
            "title": "\"Are Your Explanations Reliable?\" Investigating the Stability of LIME in Explaining Text Classifiers by Marrying XAI and Adversarial Attack",
            "abstract": "LIME has emerged as one of the most commonly referenced tools in explainable AI (XAI) frameworks that is integrated into critical machine learning applications--e.g., healthcare and finance. However, its stability remains little explored, especially in the context of text data, due to the unique text-space constraints. To address these challenges, in this paper, we first evaluate the inherent instability of LIME on text data to establish a baseline, and then propose a novel algorithm XAIFooler to perturb text inputs and manipulate explanations that casts investigation on the stability of LIME as a text perturbation optimization problem. XAIFooler conforms to the constraints to preserve text semantics and original prediction with small perturbations, and introduces Rank-biased Overlap (RBO) as a key part to guide the optimization of XAIFooler that satisfies all the requirements for explanation similarity measure. Extensive experiments on real-world text datasets demonstrate that XAIFooler significantly outperforms all baselines by large margins in its ability to manipulate LIME's explanations with high semantic preservability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218044799",
                    "name": "Christopher Burger"
                },
                {
                    "authorId": "2258910885",
                    "name": "Lingwei Chen"
                },
                {
                    "authorId": "2258868683",
                    "name": "Thai Le"
                }
            ]
        },
        {
            "paperId": "70ec04a269e2b72536dda8da1cd7cac71545d384",
            "title": "Are Your Explanations Reliable? Investigating the Stability of LIME in Explaining Textual Classification Models via Adversarial Perturbation",
            "abstract": "Local Surrogate models have increased in popularity for use in explaining complex black-box models for diverse types of data, including text, tabular, and image. One particular algo-rithm, LIME, continues to see use within the \ufb01eld of machine learning due to its inherently interpretable explanations and model-agnostic behavior. But despite continued use, questions about the stability of LIME persist. Stability, a property where similar instances result in similar explanations, has been shown to be lacking in explanations generated for tabular and image data, both of which are continuous domains. Here we explore the stability of LIME\u2019s explanations generated on textual data and con\ufb01rm the trend of instability shown in previous research for other data types.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218044799",
                    "name": "Christopher Burger"
                },
                {
                    "authorId": "2119322767",
                    "name": "Ling Chen"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                }
            ]
        }
    ]
}