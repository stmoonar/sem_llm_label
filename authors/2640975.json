{
    "authorId": "2640975",
    "papers": [
        {
            "paperId": "a404d4a2f742cd189de4cef1d990d98cda5f6226",
            "title": "Participatory Research as a Path to Community-Informed, Gender-Fair Machine Translation",
            "abstract": "Recent years have seen a strongly increased visibility of non-binary people in public discourse. Accordingly, considerations of gender-fair language go beyond a binary conception of male/female. However, language technology, especially machine translation (MT), still suffers from binary gender bias. Proposing a solution for gender-fair MT beyond the binary from a purely technological perspective might fall short to accommodate different target user groups and in the worst case might lead to misgendering. To address this challenge, we propose a method and case study building on participatory action research to include experiential experts, i.e., queer and non-binary people, translators, and MT experts, in the MT design process. The case study focuses on German, where central findings are the importance of context dependency to avoid identity invalidation and a desire for customizable MT solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2640975",
                    "name": "Dagmar Gromann"
                },
                {
                    "authorId": "2184245750",
                    "name": "Manuel Lardelli"
                },
                {
                    "authorId": "40976817",
                    "name": "Katta Spiel"
                },
                {
                    "authorId": "1932196676",
                    "name": "Sabrina Burtscher"
                },
                {
                    "authorId": "3006074",
                    "name": "Lukas Daniel Klausner"
                },
                {
                    "authorId": "66621878",
                    "name": "A. Mettinger"
                },
                {
                    "authorId": "1863321",
                    "name": "I. Miladinovic"
                },
                {
                    "authorId": "1399578344",
                    "name": "Sigrid Schefer-Wenzl"
                },
                {
                    "authorId": "15557993",
                    "name": "D. Duh"
                },
                {
                    "authorId": "2220098235",
                    "name": "Katharina Buhn"
                }
            ]
        },
        {
            "paperId": "ad95d5646e06db26f53673ddf6c099e1cc8c5295",
            "title": "Gender-Fair Post-Editing: A Case Study Beyond the Binary",
            "abstract": "Machine Translation (MT) models are well-known to suffer from gender bias, especially for gender beyond a binary conception. Due to the multiplicity of language-specific strategies for gender representation beyond the binary, debiasing MT is extremely challenging. As an alternative, we propose a case study on gender-fair post-editing. In this study, six professional translators each post-edited three English to German machine translations. For each translation, participants were instructed to use a different gender-fair language strategy, that is, gender-neutral rewording, gender-inclusive characters, and a neosystem. The focus of this study is not on translation quality but rather on the ease of integrating gender-fair language into the post-editing process. Findings from non-participant observation and interviews show clear differences in temporal and cognitive effort between participants and strategy as well as in the success of using gender-fair language.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2184245750",
                    "name": "Manuel Lardelli"
                },
                {
                    "authorId": "2640975",
                    "name": "Dagmar Gromann"
                }
            ]
        },
        {
            "paperId": "b31fb03a86cd44860f1c38e5c7032d9aed10d2f2",
            "title": "Does GPT-3 Grasp Metaphors? Identifying Metaphor Mappings with Generative Language Models",
            "abstract": "Conceptual metaphors present a powerful cognitive vehicle to transfer knowledge structures from a source to a target domain. Prior neural approaches focus on detecting whether natural language sequences are metaphoric or literal. We believe that to truly probe metaphoric knowledge in pre-trained language models, their capability to detect this transfer should be investigated. To this end, this paper proposes to probe the ability of GPT-3 to detect metaphoric language and predict the metaphor\u2019s source domain without any pre-set domains. We experiment with different training sample configurations for fine-tuning and few-shot prompting on two distinct datasets. When provided 12 few-shot samples in the prompt, GPT-3 generates the correct source domain for a new sample with an accuracy of 65.15% in English and 34.65% in Spanish. GPT\u2019s most common error is a hallucinated source domain for which no indicator is present in the sentence. Other common errors include identifying a sequence as literal even though a metaphor is present and predicting the wrong source domain based on specific words in the sequence that are not metaphorically related to the target domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2029658284",
                    "name": "Lennart Wachowiak"
                },
                {
                    "authorId": "2640975",
                    "name": "Dagmar Gromann"
                }
            ]
        },
        {
            "paperId": "03f487649f57177a33a61c79ad2dd74d8022f247",
            "title": "Cross-Lingual Link Discovery for Under-Resourced Languages",
            "abstract": "In this paper, we provide an overview of current technologies for cross-lingual link discovery, and we discuss challenges, experiences and prospects of their application to under-resourced languages. We rst introduce the goals of cross-lingual linking and associated technologies, and in particular, the role that the Linked Data paradigm (Bizer et al., 2011) applied to language data can play in this context. We de ne under-resourced languages with a speci c focus on languages actively used on the internet, i.e., languages with a digitally versatile speaker community, but limited support in terms of language technology. We argue that languages for which considerable amounts of textual data and (at least) a bilingual word list are available, techniques for cross-lingual linking can be readily applied, and that these enable the implementation of downstream applications for under-resourced languages via the localisation and adaptation of existing technologies and resources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35041118",
                    "name": "M. Rosner"
                },
                {
                    "authorId": "35183492",
                    "name": "Sina Ahmadi"
                },
                {
                    "authorId": "3202565",
                    "name": "E. Apostol"
                },
                {
                    "authorId": "1409496551",
                    "name": "Julia Bosque-Gil"
                },
                {
                    "authorId": "1723161",
                    "name": "C. Chiarcos"
                },
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "2598237",
                    "name": "K. Gkirtzou"
                },
                {
                    "authorId": "143708147",
                    "name": "J. Gracia"
                },
                {
                    "authorId": "2640975",
                    "name": "Dagmar Gromann"
                },
                {
                    "authorId": "2601200",
                    "name": "Chaya Liebeskind"
                },
                {
                    "authorId": "118562165",
                    "name": "G. Ole\u0161kevi\u010dien\u0117"
                },
                {
                    "authorId": "1753940",
                    "name": "Gilles S\u00e9rasset"
                },
                {
                    "authorId": "39812162",
                    "name": "Ciprian-Octavian Truic\u0103"
                }
            ]
        },
        {
            "paperId": "0966200c19c90116deee80ca10425171fb171610",
            "title": "The Racing Mind and the Path of Love: automatic extraction of image schematic triggers in knowledge graphs generated from natural language",
            "abstract": "Embodied Cognition and Cognitive Metaphors Theory take their origin from our use of language: sensorimotor triggers are disseminated in our daily communication, expression and commonsense knowledge. We propose, in this work, a first attempt of image-schematic triggers automatic extraction, starting from knowledge graphs automatically generated from natural language. The methodology proposed here is conceived as a modular addition integrated in the FRED tool, able to generate knowledge graphs from natural language, while it has its foundation in querying ImageSchemaNet, the Image Schematic layer developed on top of FrameNet and integrated in the Framester resource. This methodology allows the extraction of sensorimotor triggers from WordNet, VerbNet, MetaNet, BabelNet and many more. PATH, and GOAL spatial primitives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2040275084",
                    "name": "S. D. Giorgis"
                },
                {
                    "authorId": "2420171",
                    "name": "Aldo Gangemi"
                },
                {
                    "authorId": "2640975",
                    "name": "Dagmar Gromann"
                }
            ]
        },
        {
            "paperId": "248e06328df631f389887329de374d22b56d46c9",
            "title": "Introducing ISAAC: The Image Schema Abstraction And Cognition Modular Ontology",
            "abstract": "Embodied cognition and the theory of cognitive metaphors ground our commonsense reasoning ability in language, linking subjective perception of the external world with cognitive inferential patterns. Fur-thermore, commonsense reasoning is linked to human sense-making, pattern recognition and knowledge framing abilities. This work presents ISAAC, the Image Schema Abstraction And Cognition modular ontology, a new resource that formalizes the cognitive theory of Image Schemas. Image Schemas are conceptual dynamic building blocks originated by recurring sensorimotor interactions with the physical world. These experiential patterns provide coherence and structure to entities, sequences of events and situations we experience everyday. ISAAC ontology provides a formalization of theoretical state of the art literature background and integration of different theories regarding linguistic and factual entities already operationalised in ontological modules like ImageSchemaNet, the image-schematic layer built on top of FrameNet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2040275084",
                    "name": "S. D. Giorgis"
                },
                {
                    "authorId": "2420171",
                    "name": "Aldo Gangemi"
                },
                {
                    "authorId": "2640975",
                    "name": "Dagmar Gromann"
                }
            ]
        },
        {
            "paperId": "964b613686e1c533c8994c78a952ffe823101578",
            "title": "Systematic Analysis of Image Schemas in Natural Language through Explainable Multilingual Neural Language Processing",
            "abstract": "In embodied cognition, physical experiences are believed to shape abstract cognition, such as natural language and reasoning. Image schemas were introduced as spatio-temporal cognitive building blocks that capture these recurring sensorimotor experiences. The few existing approaches for automatic detection of image schemas in natural language rely on specific assumptions about word classes as indicators of spatio-temporal events. Furthermore, the lack of sufficiently large, annotated datasets makes evaluation and supervised learning difficult. We propose to build on the recent success of large multilingual pretrained language models and a small dataset of examples from image schema literature to train a supervised classifier that classifies natural language expressions of varying lengths into image schemas. Despite most of the training data being in English with few examples for German, the model performs best in German. Additionally, we analyse the model\u2019s zero-shot performance in Russian, French, and Mandarin. To further investigate the model\u2019s behaviour, we utilize local linear approximations for prediction probabilities that indicate which words in a sentence the model relies on for its final classification decision. Code and dataset are publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2029658284",
                    "name": "Lennart Wachowiak"
                },
                {
                    "authorId": "2640975",
                    "name": "Dagmar Gromann"
                }
            ]
        },
        {
            "paperId": "066f6096e566f0317737a69d3e7d60cb259930d3",
            "title": "Transforming Term Extraction: Transformer-Based Approaches to Multilingual Term Extraction Across Domains",
            "abstract": "Automated Term Extraction (ATE), even though well-investigated, continues to be a challenging task. Approaches conventionally extract terms on corpus or document level and the bene\ufb01ts of neural models still remain un-derexplored with very few exceptions. We introduce three transformer-based term extraction models operating on sentence level: a language model for token classi\ufb01cation, one for sequence classi\ufb01cation, and an innovative use of Neural Machine Translation (NMT), which learns to reduce sentences to terms. All three models are trained and tested on the dataset of the ATE challenge TermEval 2020 in English, French, and Dutch across four specialized domains. The two best performing approaches are also evaluated on the ACL RD-TEC 2.0 dataset. Our models outperform previous base-lines, one of which is BERT-based, by a substantial margin, with the token-classi\ufb01er language model performing best.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055383275",
                    "name": "Christian Lang"
                },
                {
                    "authorId": "2029658284",
                    "name": "Lennart Wachowiak"
                },
                {
                    "authorId": "152530262",
                    "name": "B. Heinisch"
                },
                {
                    "authorId": "2640975",
                    "name": "Dagmar Gromann"
                }
            ]
        },
        {
            "paperId": "d541387292a0c465ee966a65a081e1e2610bc88a",
            "title": "Neuro-Symbolic Semantic Reasoning",
            "abstract": "Humans have astounding reasoning capabilities. They can learn from very few examples while providing explanations for their decision-making process. In contrast, deep learning techniques\u2013even though robust to noise and very effective in generalizing across several fields including machine vision, natural language understanding, speech recognition, etc. \u2013require large amounts of data and are mostly unable to provide explanations for their decisions. Attaining human-level robust reasoning requires combining sound symbolic reasoning with robust connectionist learning. However, connectionist learning uses low-level representations\u2013such as embeddings\u2013rather than symbolic representations. This challenge constitutes what is referred to as the Neuro-Symbolic gap. A field of study to bridge this gap between the two paradigms has been called neuro-symbolic integration or neuro-symbolic computing. This chapter aims to present approaches that contribute towards bridging the Neuro-Symbolic gap specifically in the Semantic Web field, RDF Schema (RDFS) and EL+ reasoning and to discuss the benefits and shortcomings of neuro-symbolic reasoning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2542374",
                    "name": "B. Makni"
                },
                {
                    "authorId": "7893587",
                    "name": "Monireh Ebrahimi"
                },
                {
                    "authorId": "2640975",
                    "name": "Dagmar Gromann"
                },
                {
                    "authorId": "51144460",
                    "name": "Aaron Eberhart"
                }
            ]
        },
        {
            "paperId": "e6a86ff40fc4c7932f986d2ef77b330defb917e8",
            "title": "Towards Learning Terminological Concept Systems from Multilingual Natural Language Text",
            "abstract": "Terminological Concept Systems (TCS) provide a means of organizing, structuring and representing domain-specific multilingual information and are important to ensure terminological consistency in many tasks, such as translation and cross-border communication. While several approaches to (semi-)automatic term extraction exist, learning their interrelations is vastly underexplored. We propose an automated method to extract terms and relations across natural languages and specialized domains. To this end, we adapt pretrained multilingual neural language models, which we evaluate on term extraction standard datasets with best performing results and a combination of relation extraction standard datasets with competitive results. Code and dataset are publicly available",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2029658284",
                    "name": "Lennart Wachowiak"
                },
                {
                    "authorId": "2055383275",
                    "name": "Christian Lang"
                },
                {
                    "authorId": "152530262",
                    "name": "B. Heinisch"
                },
                {
                    "authorId": "2640975",
                    "name": "Dagmar Gromann"
                }
            ]
        }
    ]
}