{
    "authorId": "2153422494",
    "papers": [
        {
            "paperId": "203574cdbb7ae35a44c40abeed414093a58b74bf",
            "title": "Prospect Personalized Recommendation on Large Language Model-based Agent Platform",
            "abstract": "The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A preliminary study involving several cases of Rec4Agentverse validates its significant potential for application. Lastly, we discuss potential issues and promising directions for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2188063534",
                    "name": "Keqin Bao"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2145957648",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2153422494",
                    "name": "Wentao Shi"
                },
                {
                    "authorId": "2288043800",
                    "name": "Wanhong Xu"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "2add25de304fbf12b75f3209532bf685972f29b9",
            "title": "Debias Can be Unreliable: Mitigating Bias Issue in Evaluating Debiasing Recommendation",
            "abstract": "Recent work has improved recommendation models remarkably by equipping them with debiasing methods. Due to the unavailability of fully-exposed datasets, most existing approaches resort to randomly-exposed datasets as a proxy for evaluating debiased models, employing traditional evaluation scheme to represent the recommendation performance. However, in this study, we reveal that traditional evaluation scheme is not suitable for randomly-exposed datasets, leading to inconsistency between the Recall performance obtained using randomly-exposed datasets and that obtained using fully-exposed datasets. Such inconsistency indicates the potential unreliability of experiment conclusions on previous debiasing techniques and calls for unbiased Recall evaluation using randomly-exposed datasets. To bridge the gap, we propose the Unbiased Recall Evaluation (URE) scheme, which adjusts the utilization of randomly-exposed datasets to unbiasedly estimate the true Recall performance on fully-exposed datasets. We provide theoretical evidence to demonstrate the rationality of URE and perform extensive experiments on real-world datasets to validate its soundness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2320452720",
                    "name": "Chengbing Wang"
                },
                {
                    "authorId": "2153422494",
                    "name": "Wentao Shi"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2230045941",
                    "name": "Hang Pan"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "3b5778b0aec88c1921fee7538400e49bed42a5fa",
            "title": "Direct Multi-Turn Preference Optimization for Language Agents",
            "abstract": "Adapting Large Language Models (LLMs) for agent tasks is critical in developing language agents. Direct Preference Optimization (DPO) is a promising technique for this adaptation with the alleviation of compounding errors, offering a means to directly optimize Reinforcement Learning (RL) objectives. However, applying DPO to multi-turn tasks presents challenges due to the inability to cancel the partition function. Overcoming this obstacle involves making the partition function independent of the current state and addressing length disparities between preferred and dis-preferred trajectories. In this light, we replace the policy constraint with the state-action occupancy measure constraint in the RL objective and add length normalization to the Bradley-Terry model, yielding a novel loss function named DMPO for multi-turn agent tasks with theoretical explanations. Extensive experiments on three multi-turn agent task datasets confirm the effectiveness and superiority of the DMPO loss.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153422494",
                    "name": "Wentao Shi"
                },
                {
                    "authorId": "2307996050",
                    "name": "Mengqi Yuan"
                },
                {
                    "authorId": "2260622979",
                    "name": "Junkang Wu"
                },
                {
                    "authorId": "2260433198",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2271382818",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "40fd47de355722f7968b2bc1fca8a69d5bbfa047",
            "title": "Improving Prostate Cancer Risk Prediction through Partial AUC Optimization",
            "abstract": "Prostate cancer risk prediction (PCRP) is crucial in guiding clinical decision-making and ensuring accurate diagnoses. The area under the receiver operating characteristic curve (AUC) is typically used for the evaluation of PCRP models. However, AUC considers regions with high false positive rates (FPRs), which are not applicable in clinical practice. To address this concern, we propose to use partial AUC (pAUC) as a more clinically meaningful metric which evaluates PCRP models with restricted FPR. Moreover, we propose a new PCRP framework named pAUCP, which optimizes pAUC to train PCRP models and adopts model ensemble to further enhance its usability. We construct clinical datasets obtained from two medical centers over an extended period to evaluate the proposed pAUCP framework. Extensive experiments demonstrate the rationality and superiority of the pAUCP framework, especially the cross-time and cross-center transferability of the obtained PCRP model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2165424018",
                    "name": "Xinyuan Zhu"
                },
                {
                    "authorId": "2302133000",
                    "name": "Xiaohan Ren"
                },
                {
                    "authorId": "2153422494",
                    "name": "Wentao Shi"
                },
                {
                    "authorId": "2267729999",
                    "name": "Changming Wang"
                },
                {
                    "authorId": "2246739128",
                    "name": "Xuehan Liu"
                },
                {
                    "authorId": "2301208091",
                    "name": "Yuqing Liu"
                },
                {
                    "authorId": "2267470873",
                    "name": "Tao Tao"
                },
                {
                    "authorId": "2275173839",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "97bcbd60591daf9557312c205e0e2790d2aa6b30",
            "title": "Towards Reliable and Efficient Long-Term Recommendation with Large Foundation Models",
            "abstract": "Prioritizing long-term engagement rather than immediate benefits has garnered increasing attention in recent years. However, current research on long-term recommendation faces substantial challenges in terms of model evaluation and design: 1) Traditional evaluation approaches suffer from limitations due to the sparsity and bias in the offline data and fail to capture user psychological influences. 2) Existing recommenders based on Reinforcement Learning (RL) are entirely data-driven and constrained by sparse and long-tail distributed offline data. Fortunately, recent advancements in Large Foundation Models (LFMs), characterized by remarkable simulation and planning capacity, offer significant opportunities for long-term recommendation. Despite potential, due to the substantial scenario divergence between LFM pre-training and recommendation, employing LFMs in long-term recommendation still faces certain challenges. To this end, this research focuses on adapting the remarkable capabilities of LFMs to long-term recommendations to devise reliable evaluation schemes and efficient recommenders.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153422494",
                    "name": "Wentao Shi"
                }
            ]
        },
        {
            "paperId": "ada30783ffb6e221430b0e0e9e21fe8af0b596e1",
            "title": "Lower-Left Partial AUC: An Effective and Efficient Optimization Metric for Recommendation",
            "abstract": "Optimization metrics are crucial for building recommendation systems at scale. However, an effective and efficient metric for practical use remains elusive. While Top-K ranking metrics are the gold standard for optimization, they suffer from significant computational overhead. Alternatively, the more efficient accuracy and AUC metrics often fall short of capturing the true targets of recommendation tasks, leading to suboptimal performance. To overcome this dilemma, we propose a new optimization metric, Lower-Left Partial AUC (LLPAUC), which is computationally efficient like AUC but strongly correlates with Top-K ranking metrics. Compared to AUC, LLPAUC considers only the partial area under the ROC curve in the Lower-Left corner to push the optimization focus on Top-K. We provide theoretical validation of the correlation between LLPAUC and Top-K ranking metrics and demonstrate its robustness to noisy user feedback. We further design an efficient point-wise recommendation loss to maximize LLPAUC and evaluate it on three datasets, validating its effectiveness and robustness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153422494",
                    "name": "Wentao Shi"
                },
                {
                    "authorId": "2289863291",
                    "name": "Chenxu Wang"
                },
                {
                    "authorId": "2275173839",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2145957648",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2260622979",
                    "name": "Junkang Wu"
                },
                {
                    "authorId": "2239071206",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "d45874b9af5e067961aa9dcdc264db280c67eb67",
            "title": "Uplift Modeling for Target User Attacks on Recommender Systems",
            "abstract": "Recommender systems are vulnerable to injective attacks, which inject limited fake users into the platforms to manipulate the exposure of target items to all users. In this work, we identify that conventional injective attackers overlook the fact that each item has its unique potential audience, and meanwhile, the attack difficulty across different users varies. Blindly attacking all users will result in a waste of fake user budgets and inferior attack performance. To address these issues, we focus on an under-explored attack task called target user attacks, aiming at promoting target items to a particular user group. In addition, we formulate the varying attack difficulty as heterogeneous treatment effects through a causal lens and propose an Uplift-guided Budget Allocation (UBA) framework. UBA estimates the treatment effect on each target user and optimizes the allocation of fake user budgets to maximize the attack performance. Theoretical and empirical analysis demonstrates the rationality of treatment effect estimation methods of UBA. By instantiating UBA on multiple attackers, we conduct extensive experiments on three datasets under various settings with different target items, target users, fake user budgets, victim models, and defense models, validating the effectiveness and robustness of UBA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2239056233",
                    "name": "Changsheng Wang"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2153422494",
                    "name": "Wentao Shi"
                },
                {
                    "authorId": "27029227",
                    "name": "Daizong Ding"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "f9db375824ca667897229f570ceb699ab05f1483",
            "title": "Large Language Models are Learnable Planners for Long-Term Recommendation",
            "abstract": "Planning for both immediate and long-term benefits becomes increasingly important in recommendation. Existing methods apply Reinforcement Learning (RL) to learn planning capacity by maximizing cumulative reward for long-term recommendation. However, the scarcity of recommendation data presents challenges such as instability and susceptibility to overfitting when training RL models from scratch, resulting in sub-optimal performance. In this light, we propose to leverage the remarkable planning capabilities over sparse data of Large Language Models (LLMs) for long-term recommendation. The key to achieving the target lies in formulating a guidance plan following principles of enhancing long-term engagement and grounding the plan to effective and executable actions in a personalized manner. To this end, we propose a Bi-level Learnable LLM Planner framework, which consists of a set of LLM instances and breaks down the learning process into macro-learning and micro-learning to learn macro-level guidance and micro-level personalized recommendation policies, respectively. Extensive experiments validate that the framework facilitates the planning ability of LLMs for long-term recommendation. Our code and data can be found at https://github.com/jizhi-zhang/BiLLP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153422494",
                    "name": "Wentao Shi"
                },
                {
                    "authorId": "2239071206",
                    "name": "Xiangnan He"
                },
                {
                    "authorId": "2145957648",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2265123543",
                    "name": "Chongming Gao"
                },
                {
                    "authorId": "2289866905",
                    "name": "Xinyue Li"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2260433198",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2275173839",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "28fa51f1b70869c1c8f610b25ddb841c83a8de2b",
            "title": "Discriminative-Invariant Representation Learning for Unbiased Recommendation",
            "abstract": "Selection bias hinders recommendation models from learning unbiased user preference. Recent works empirically reveal that pursuing invariant user and item representation across biased and unbiased data is crucial for counteracting selection bias. However, our theoretical analysis reveals that simply optimizing representation invariance is insufficient for addressing the selection bias \u2014 recommendation performance is bounded by both representation invariance and discriminability. Worse still, current invariant representation learning methods in recommendation neglect even hurt the representation discriminability due to data sparsity and label shift. In this light, we propose a new Discriminative-Invariant Representation Learning framework for unbiased recommendation, which incorporates label-conditional clustering and prior-guided contrasting into conventional invariant representation learning to mitigate the impact of data sparsity and label shift, respectively. We conduct extensive experiments on three real-world datasets, validating the rationality and effectiveness of the proposed framework. Code and supplementary materials are available at: https://github.com/HungPaan/DIRL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2230045941",
                    "name": "Hang Pan"
                },
                {
                    "authorId": "1452347263",
                    "name": "Jiawei Chen"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2153422494",
                    "name": "Wentao Shi"
                },
                {
                    "authorId": "8612672",
                    "name": "Junkang Wu"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "2a00328722f7f700bd2a5325d599e73d46a2cf01",
            "title": "BSL: Understanding and Improving Softmax Loss for Recommendation",
            "abstract": "Loss functions steer the optimization direction of recommendation models and are critical to model performance, but have received relatively little attention in recent recommendation research. Among various losses, we find Softmax loss (SL) stands out for not only achieving remarkable accuracy but also better robustness and fairness. Nevertheless, the current literature lacks a comprehensive explanation for the efficacy of SL. Toward addressing this research gap, we conduct theoretical analyses on SL and uncover three insights: 1) Optimizing SL is equivalent to performing Distributionally Robust Optimization (DRO) on the negative data, thereby learning against perturbations on the negative distribution and yielding robustness to noisy negatives. 2) Comparing with other loss functions, SL implicitly penalizes the prediction variance, resulting in a smaller gap between predicted values and and thus producing fairer results. Building on these insights, we further propose a novel loss function Bilateral SoftMax Loss (BSL) that extends the advantage of SL to both positive and negative sides. BSL augments SL by applying the same Log-Expectation-Exp structure to positive examples as is used for negatives, making the model robust to the noisy positives as well. Remarkably, BSL is simple and easy-to-implement - requiring just one additional line of code compared to SL. Experiments on four real-world datasets and three representative backbones demonstrate the effectiveness of our proposal. The code is available at https://github.com/junkangwu/BSL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260622979",
                    "name": "Junkang Wu"
                },
                {
                    "authorId": "2259837759",
                    "name": "Jiawei Chen"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2153422494",
                    "name": "Wentao Shi"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                }
            ]
        }
    ]
}