{
    "authorId": "2238627171",
    "papers": [
        {
            "paperId": "260282639dce1984c1a065aa4feae41cea0fed06",
            "title": "Bilingual Adaptation of Monolingual Foundation Models",
            "abstract": "We present an efficient method for adapting a monolingual Large Language Model (LLM) to another language, addressing challenges of catastrophic forgetting and tokenizer limitations. We focus this study on adapting Llama 2 to Arabic. Our two-stage approach begins with expanding the vocabulary and training only the embeddings matrix, followed by full model continual pre-training on a bilingual corpus. By continually pre-training on a mix of Arabic and English corpora, the model retains its proficiency in English while acquiring capabilities in Arabic. Our approach results in significant improvements in Arabic and slight enhancements in English, demonstrating cost-effective cross-lingual transfer. We perform ablations on embedding initialization techniques, data mix ratios, and learning rates and release a detailed training recipe. To demonstrate generalizability of this approach we also adapted Llama 3 8B to Arabic and Llama 2 13B to Hindi.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2238627171",
                    "name": "Gurpreet Gosal"
                },
                {
                    "authorId": "2312005159",
                    "name": "Yishi Xu"
                },
                {
                    "authorId": "2311889065",
                    "name": "Gokul Ramakrishnan"
                },
                {
                    "authorId": "2311894591",
                    "name": "Rituraj Joshi"
                },
                {
                    "authorId": "2311892291",
                    "name": "Avraham Sheinin"
                },
                {
                    "authorId": "2311991181",
                    "name": "Zhiming Chen"
                },
                {
                    "authorId": "2311891083",
                    "name": "Biswajit Mishra"
                },
                {
                    "authorId": "2243228182",
                    "name": "Natalia Vassilieva"
                },
                {
                    "authorId": "3130228",
                    "name": "Joel Hestness"
                },
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "2311888768",
                    "name": "Sunil Kumar Sahu"
                },
                {
                    "authorId": "2087720002",
                    "name": "Bokang Jia"
                },
                {
                    "authorId": "2311887646",
                    "name": "Onkar Pandit"
                },
                {
                    "authorId": "2235818050",
                    "name": "Satheesh Katipomu"
                },
                {
                    "authorId": "2203791403",
                    "name": "Samta Kamboj"
                },
                {
                    "authorId": "2313594182",
                    "name": "Samujjwal Ghosh"
                },
                {
                    "authorId": "2235794681",
                    "name": "Rahul Pal"
                },
                {
                    "authorId": "2311889115",
                    "name": "Parvez Mullah"
                },
                {
                    "authorId": "2311887505",
                    "name": "Soundar Doraiswamy"
                },
                {
                    "authorId": "2311887344",
                    "name": "Mohamed El Karim Chami"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "2ddef4301dc9f9ef0f36e111e83cf8428716c562",
            "title": "Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches",
            "abstract": "This study presents a comprehensive analysis and comparison of two predominant fine-tuning methodologies - full-parameter fine-tuning and parameter-efficient tuning - within the context of medical Large Language Models (LLMs). We developed and refined a series of LLMs, based on the Llama-2 architecture, specifically designed to enhance medical knowledge retrieval, reasoning, and question-answering capabilities. Our experiments systematically evaluate the effectiveness of these tuning strategies across various well-known medical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of 72% on the US Medical Licensing Examination (USMLE) datasets, setting a new standard in performance for openly available medical LLMs. Through this comparative analysis, we aim to identify the most effective and efficient method for fine-tuning LLMs in the medical domain, thereby contributing significantly to the advancement of AI-driven healthcare applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2297849088",
                    "name": "Cl'ement Christophe"
                },
                {
                    "authorId": "2297848890",
                    "name": "Praveen K Kanithi"
                },
                {
                    "authorId": "102615578",
                    "name": "Prateek Munjal"
                },
                {
                    "authorId": "1840492875",
                    "name": "Tathagata Raha"
                },
                {
                    "authorId": "2297848425",
                    "name": "Nasir Hayat"
                },
                {
                    "authorId": "2297849270",
                    "name": "Ronnie Rajan"
                },
                {
                    "authorId": "2297848914",
                    "name": "Ahmed Al-Mahrooqi"
                },
                {
                    "authorId": "2298005616",
                    "name": "Avani Gupta"
                },
                {
                    "authorId": "2297848569",
                    "name": "Muhammad Umar Salman"
                },
                {
                    "authorId": "2238627171",
                    "name": "Gurpreet Gosal"
                },
                {
                    "authorId": "9645616",
                    "name": "Bhargav Kanakiya"
                },
                {
                    "authorId": "2297853602",
                    "name": "Charles Chen"
                },
                {
                    "authorId": "2243228182",
                    "name": "Natalia Vassilieva"
                },
                {
                    "authorId": "2125606",
                    "name": "B. Amor"
                },
                {
                    "authorId": "2308995611",
                    "name": "M. A. Pimentel"
                },
                {
                    "authorId": "2298728972",
                    "name": "Shadab Khan"
                }
            ]
        },
        {
            "paperId": "3b5968a3f33fe3c7a14d8691c6f6151bd633203d",
            "title": "Improving Resnet-9 Generalization Trained on Small Datasets",
            "abstract": "This paper presents our proposed approach that won the first prize at the ICLR competition on Hardware Aware Efficient Training. The challenge is to achieve the highest possible accuracy in an image classification task in less than 10 minutes. The training is done on a small dataset of 5000 images picked randomly from CIFAR-10 dataset. The evaluation is performed by the competition organizers on a secret dataset with 1000 images of the same size. Our approach includes applying a series of technique for improving the generalization of ResNet-9 including: sharpness aware optimization, label smoothing, gradient centralization, input patch whitening as well as metalearning based training. Our experiments show that the ResNet-9 can achieve the accuracy of 88% while trained only on a 10% subset of CIFAR-10 dataset in less than 10 minuets",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152432835",
                    "name": "Omar Mohamed Awad"
                },
                {
                    "authorId": "3401413",
                    "name": "Habib Hajimolahoseini"
                },
                {
                    "authorId": "2238630803",
                    "name": "Michael Lim"
                },
                {
                    "authorId": "2238627171",
                    "name": "Gurpreet Gosal"
                },
                {
                    "authorId": "2238208401",
                    "name": "Walid Ahmed"
                },
                {
                    "authorId": "2238404323",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2238625388",
                    "name": "Gordon Deng"
                }
            ]
        }
    ]
}