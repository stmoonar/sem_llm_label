{
    "authorId": "7337091",
    "papers": [
        {
            "paperId": "0cb137e11dec0fda0634d054afa7b2251a572326",
            "title": "Learned Selection Strategy for Lightweight Integer Compression Algorithms",
            "abstract": "Data compression has recently experienced a revival in the domain of in-memory column stores. In this field, a large corpus of lightweight integer compression algorithms plays a dominant role since all columns are typically encoded as sequences of integer values. Unfortunately, there is no single-best integer compression algorithm and the best algorithm depends on data and hardware properties. For this reason, selecting the best-fitting integer compression algorithm becomes more important and is an interesting tuning knob for optimization. However, traditional selection strategies require a profound knowledge of the (de-)compression algorithms for decision-making. This limits the broad applicability of the selection strategies. To counteract this, we propose a novel learned selection strategy by considering integer compression algorithms as independent black boxes. This black-box approach ensures broad applicability and requires machine learning-based methods to model the required knowledge for decision-making. Most importantly, we show that a local approach, where every algorithm is modeled individually, plays a crucial role. Moreover, our learned selection strategy is generalized by user-data-independence. Finally, we evaluate our approach and compare our approach against existing selection strategies to show the benefits of our learned selection strategy .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41065728",
                    "name": "Lucas Woltmann"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "31f76a40aa81e54b0e2a0f889ac8f35751c38c42",
            "title": "Enhanced Featurization of Queries with Mixed Combinations of Predicates for ML-based Cardinality Estimation",
            "abstract": "Estimating query result sizes is a critical task in areas like query optimization. For some years now it has been popular to apply machine learning to this problem. However, surprisingly, there has been very little research yet on how to present queries to a machine learning model. Machine learning models do not simply consume SQL strings. Instead, a SQL string is transformed into a numerical representation. This transformation is called query fea-turization and is defined by a query featurization technique (QFT). This paper is concerned with QFTs for queries with many selection predicates. In particular, we consider queries that contain both predicates over different attributes and multiple predicates per attribute. We identify a desired property of query featuriza-tion and present three novel QFTs. To the best of our knowledge, we are the first to featurize queries with mixed combinations of predicates, i.e., containing both conjunctions and disjunctions. Our QFTs are model-independent and can serve as the query featurization layer for different machine learning model types. In our evaluation, we combine our QFTs with three different machine learning models. We demonstrate that the estimation accuracy of machine learning models significantly depends on the QFT used. In addition, we compare our best combination of QFT and machine learning model to various existing cardinality estimators.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116237589",
                    "name": "Magnus M\u00fcller"
                },
                {
                    "authorId": "41065728",
                    "name": "Lucas Woltmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "3bc12e44211c6b74145064c19cf5bc3516fb83d7",
            "title": "Simplicity done right for SIMDified query processing on CPU and FPGA",
            "abstract": "We present a simple but effective solution idea to port SIMDified query processing code to Intel\u00ae FPGA cards for acceleration. The main advantage of our approach is the seamless integration with existing SIMD abstraction libraries originally developed to overcome SIMD heterogeneity on x86-processors. Moreover, our approach has the practical benefit to be straightforwardly implemented in C++ without the necessity of complex FPGA-specific programming. Our initial results are very promising, demonstrating a novel approach to comprehensively integrate Intel\u00ae FPGAs into the prevailing SIMDified processing on the CPU with reasonable effort.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "9187969",
                    "name": "Alexander Krause"
                },
                {
                    "authorId": "33232107",
                    "name": "Johannes Pietrzyk"
                },
                {
                    "authorId": "2243362571",
                    "name": "Christian Faerber"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "3f40ddeb29317ddc40a030a81009ca9c5267fc16",
            "title": "PostBOUND: PostgreSQL with Upper Bound SPJ Query Optimization",
            "abstract": ": A variety of query optimization papers have shown the disastrous effect of poor cardinality estimates on the overall runtime for arbitrary select-project-join (SPJ) queries. Especially, underestimating join cardinalities for multi-joins can lead to catastrophic join orderings. A promising solution to overcome this problem is query optimization based on upper bounds for the join cardinalities. In this domain, our proposed UES concept is presently the most efficient technique featuring a simple, yet effective upper bound for an arbitrary number of joins. To foster research in that direction, we introduce PostBOUND , our generalized framework to seamlessly integrate upper bound SPJ query optimization in PostgreSQL. PostBOUND provides abstractions to calculate arbitrary upper bounds, to model joins required by an SPJ query and to iteratively construct an optimized join order. To highlight the extensibility of PostBOUND , and to show the research potential, we additionally present two tighter upper bound UES variants using top-k statistics in this paper. In our evaluation, we show the efficiency and applicability of PostBOUND on different workloads as well as using different PostgreSQL versions. Additionally, we evaluate both presented tighter upper bound variant ideas.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3316581",
                    "name": "Rico Bergmann"
                },
                {
                    "authorId": "1491355603",
                    "name": "Axel Hertzschuch"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "540dff94488e08e3021ff52e7b13351df38654c0",
            "title": "Pipeline Group Optimization on Disaggregated Systems",
            "abstract": "While hardware disaggregation is considered the \"next big thing\" providing unique opportunities for database systems, the pipeline-based execution model is state-of-the-art in modern query engines on monolithic systems. Within this paper, we propose a lightweight way of adapting this pipeline-based model to disaggregated memory systems to soften the inherent overhead induced by arbitrary memory accesses. Instead of executing pipelines in strict isolation including a pipeline-local data transfer, we group pipelines with similar data access characteristics of concurrently running queries into pipeline groups . Each such pipeline group is then executed sep-arately, but shared data across pipelines within each group is only transferred once from memory resources to compute resources and potentially re-used multiple times. This method dramatically re-duces redundant data transfers and \u2013 in combination with a suitable caching strategy as well as a fast communication layer \u2013 increases the performance significantly in comparison to traditional pipeline-based execution of multiple queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145363671",
                    "name": "Andrea M. Geyer"
                },
                {
                    "authorId": "9187969",
                    "name": "Alexander Krause"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "7bddbd1d6750f50016a27755b14fa6a7bc990555",
            "title": "KeRRaS: Sort-Based Database Query Processing on\u00a0Wide\u00a0Tables\u00a0Using\u00a0FPGAs",
            "abstract": "Sorting is an important operation in database query processing. Complex pipeline-breaking operators (e.g., aggregation and equi-join) become single-pass algorithms on sorted tables. Therefore, sort-based query processing is a popular method for FPGA-based database system acceleration. However, most accelerators have a limit on the table width or the number of columns they can sort. This limit is often set by the width of the data path or the amount of BRAM present on the FPGA. In this paper we propose KeRRaS, an abstract sorting algorithm that enables existing sort-based query processors to support arbitrarily wide tables while offering scalability, preserving modularity, and having low resource overhead. Moreover, we present an implementation of KeRRaS based on morphing sort-merge, a resource-efficient FPGA-based query accelerator. The implementation behaves similarly to morphing sort-merge on narrow tables, and scales well as the number of key columns increases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738666964",
                    "name": "Mehdi Moghaddamfar"
                },
                {
                    "authorId": "2071266998",
                    "name": "Christian F\u00e4rber"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "12364057",
                    "name": "Akash Kumar"
                }
            ]
        },
        {
            "paperId": "7bef0e334d14f164ff3996092c183e0238d0ab6d",
            "title": "A Study of Early Aggregation in Database Query Processing on FPGAs",
            "abstract": "In database query processing, aggregation is an operator by which data with a common property is grouped and expressed in a summary form. Early aggregation is a popular method for improving the performance of the aggregation operator. In this paper, we study early aggregation algorithms in the context of query processing acceleration in database systems on FPGAs. The comparative study leads us to set-associative caches with a low inter-reference recency set (LIRS) replacement policy. They show both great performance and modest implementation complexity compared to some of the most prominent early aggregation algorithms. We also present a novel application-specific architecture for implementing set-associative caches. Benchmarks of our implementation show speedups of up to 3x for end-to-end aggregation compared to a state-of-the-art FPGA-based query engine.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738666964",
                    "name": "Mehdi Moghaddamfar"
                },
                {
                    "authorId": "1766564",
                    "name": "Norman May"
                },
                {
                    "authorId": "2071266998",
                    "name": "Christian F\u00e4rber"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2143184651",
                    "name": "Akash Kumar"
                }
            ]
        },
        {
            "paperId": "be099f36e054fd9efb9b8e48d03729a2a0f0b249",
            "title": "Working with Disaggregated Systems. What are the Challenges and Opportunities of RDMA and CXL?",
            "abstract": "The usage of disaggregated systems in large scale data-centers offers a lot of flexibility and easy scalability in comparison to the traditional statically configured scale-up and scale-out systems. Disaggregated architectures allow for the creation of software composable systems [Li17, Li18]. On the one hand, this allows seamless integration of specialized hardware like FPGAs or GPUs as well as a high degree of elasticity to scale a system with its workload by dynamically adding and removing resources via software. On the other hand, however, it also brings several challenges like an additional communication overhead for memory accesses, which is especially critical for In-Memory databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145363671",
                    "name": "Andrea M. Geyer"
                },
                {
                    "authorId": "144405144",
                    "name": "Daniel Ritter"
                },
                {
                    "authorId": "2115475788",
                    "name": "Dong Hun Lee"
                },
                {
                    "authorId": "2450376",
                    "name": "Minseon Ahn"
                },
                {
                    "authorId": "33232107",
                    "name": "Johannes Pietrzyk"
                },
                {
                    "authorId": "9187969",
                    "name": "Alexander Krause"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "dbeb781a01d672c6641a7d8734f0d46d1d73de59",
            "title": "FASTgres: Making Learned Query Optimizer Hinting Effective",
            "abstract": "\n The traditional and well-established cost-based query optimizer approach enumerates different execution plans for each query, assesses each plan with costs, and selects the plan that promises the lowest costs for execution. However, the optimal execution plan is not always selected. To steer the optimizer in the right direction, many query optimizers provide configuration parameters called query optimizer hints. These hints can be set for every single query separately. To show the great potential of these hints for the optimization of analytical queries, we present results of a comprehensive and in-depth evaluation using three benchmarks and two different versions of the open-source database system PostgreSQL. In particular, we highlight that query optimizer hinting is a non-trivial challenge. To solve this challenge, we propose\n FASTgres\n , a learning-based context-aware classification strategy for hint set prediction. Compared to related work,\n FASTgres\n provides transparent and direct hint set predictions with consistent performance improvements. In our end-to-end evaluation, we demonstrate that\n FASTgres\n effectively reduces benchmark runtimes by a factor of up to 3.25x with only steering the cost-based optimizer.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41065728",
                    "name": "Lucas Woltmann"
                },
                {
                    "authorId": "2209414836",
                    "name": "Jerome Thiessat"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "1b6bd24e2ce6bac1270450c42dacf2a32f0758db",
            "title": "Accelerating Parallel Operation for Compacting Selected Elements on GPUs",
            "abstract": "Compacting is a common and heavily used operation in different application areas such as statistics, database systems, simulations, and artificial intelligence. The task of this operation is to write selected elements of an input array contiguously back to a new output array, producing a smaller (i.e., compact) array. The selected elements are usually defined by means of a bit mask. With the always increasing amount of data to be processed in different application areas, better performance becomes a key factor for this operation. Thus, exploiting the parallel capabilities of GPUs to speed up the compacting operation is of great interest. We introduce smart partitioning for GPU compaction (SPACE ) as a set of different optimization approaches for GPUs. A detailed guide about setting up and using the software is found in Fett et al. (n.d.). Fett et al. (2022) is a preprint of the published Euro-Par 2022 paper, in which SPACE is described in great detail.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113619044",
                    "name": "Johannes Fett"
                },
                {
                    "authorId": "2178896886",
                    "name": "Urs Kober"
                },
                {
                    "authorId": "144258182",
                    "name": "Christian Schwarz"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "3c3f8344e58ff7a8ea9c924e428bda5b8c775df6",
            "title": "DAPHNE: An Open and Extensible System Infrastructure for Integrated Data Analysis Pipelines",
            "abstract": "Integrated data analysis (IDA) pipelines\u2014that combine data management (DM) and query processing, high-performance computing (HPC)",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "2135591133",
                    "name": "Marius Birkenbach"
                },
                {
                    "authorId": "67078377",
                    "name": "C. Bitsakos"
                },
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "1748177",
                    "name": "Philippe Bonnet"
                },
                {
                    "authorId": "1764998",
                    "name": "F. Ciorba"
                },
                {
                    "authorId": "33641807",
                    "name": "Mark Dokter"
                },
                {
                    "authorId": "2155231332",
                    "name": "Pawel Dowgiallo"
                },
                {
                    "authorId": "9076323",
                    "name": "A. Eleliemy"
                },
                {
                    "authorId": "2243362571",
                    "name": "Christian Faerber"
                },
                {
                    "authorId": "2825685",
                    "name": "G. Goumas"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "1491355697",
                    "name": "Niclas Hedam"
                },
                {
                    "authorId": "2155231377",
                    "name": "Marlies Hofer"
                },
                {
                    "authorId": "32347753",
                    "name": "W. Huang"
                },
                {
                    "authorId": "1388063032",
                    "name": "Kevin Innerebner"
                },
                {
                    "authorId": "145225900",
                    "name": "Vasileios Karakostas"
                },
                {
                    "authorId": "144361874",
                    "name": "Roman Kern"
                },
                {
                    "authorId": "8964295",
                    "name": "T. Kosar"
                },
                {
                    "authorId": "9187969",
                    "name": "Alexander Krause"
                },
                {
                    "authorId": "2037739392",
                    "name": "D. Krems"
                },
                {
                    "authorId": "2176399730",
                    "name": "Andreas Laber"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "9539656",
                    "name": "Eric Mier"
                },
                {
                    "authorId": "2587068",
                    "name": "M. Paradies"
                },
                {
                    "authorId": "3147468",
                    "name": "B. Peischl"
                },
                {
                    "authorId": "9265354",
                    "name": "Gabrielle Poerwawinata"
                },
                {
                    "authorId": "2585804",
                    "name": "Stratos Psomadakis"
                },
                {
                    "authorId": "1731210",
                    "name": "T. Rabl"
                },
                {
                    "authorId": "3250930",
                    "name": "P. Ratuszniak"
                },
                {
                    "authorId": "47723840",
                    "name": "Pedro J. N. Silva"
                },
                {
                    "authorId": "1394964950",
                    "name": "N. Skuppin"
                },
                {
                    "authorId": "2830933",
                    "name": "Andreas Starzacher"
                },
                {
                    "authorId": "2079467",
                    "name": "Benjamin Steinwender"
                },
                {
                    "authorId": "72671874",
                    "name": "Ilin Tolovski"
                },
                {
                    "authorId": "1843945",
                    "name": "P\u0131nar T\u00f6z\u00fcn"
                },
                {
                    "authorId": "98260585",
                    "name": "W. Ulatowski"
                },
                {
                    "authorId": "2146024306",
                    "name": "Yuanyuan Wang"
                },
                {
                    "authorId": "1994724443",
                    "name": "Izajasz P. Wrosz"
                },
                {
                    "authorId": "2343857",
                    "name": "A. Zamuda"
                },
                {
                    "authorId": "2154976782",
                    "name": "Ce Zhang"
                },
                {
                    "authorId": "2125159330",
                    "name": "Xiao Xiang Zhu"
                }
            ]
        },
        {
            "paperId": "6d629af3fff9a3ee99a2d6568cb0b2bc33bfc206",
            "title": "Towards A General SIMD Concurrent Approach to Accelerating Integer Compression Algorithms",
            "abstract": "Integer compression algorithms play an important role in column-oriented data systems. Previous research has shown that the vec-torized implementation of these algorithms based on the Single Instruction Multiple Data (SIMD) parallel paradigm can multiply the compression as well as decompression speeds. While a scalar compression algorithm usually compresses a block of \ud835\udc41 consecutive integers, the state-of-the-art SIMD implementation scales the block size to \ud835\udc58 \u2217 \ud835\udc41 with \ud835\udc58 as the number of elements which could be simultaneously processed in a SIMD register. However, this means that as the SIMD register size increases, the block of integer values for compression also grows, which can have a negative effect on the compression ratio. In this paper, we analyze this effect and present an idea for a novel general approach for the SIMD implementation of integer compression algorithms to overcome that effect. Our novel idea is to concurrently compress \ud835\udc58 different blocks of size \ud835\udc41 within SIMD registers. To show the applicability of our idea, we present initial evaluation results for a heavily used compression algorithm and show that our approach can lead to more responsible usage of main memory resources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27226616",
                    "name": "Juliana Hildebrandt"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "708e6a90db9436af6fde0e5f9100a8624de4457a",
            "title": "Ingredient-based Forecast of Sold Dish Portions in Campus Canteen Kitchens",
            "abstract": "In the catering industry, one major challenge is the unknown short-term demand for dish portions. Customers want to avoid queuing and desire their favorite dish according to their preferences. Meeting these demands is important for the industry but predicting future sales is a challenging task. Often, the predictions are derived manually and automated approaches are rarely applied in practice. This paper presents an ML-based forecast model using a set of derived features to predict shares and absolute numbers of dish portions per day. In particular, these features include text-based extractions of ingredients, calendar effects to model time dependencies, and favorite features to model customers' preferences. As the detailed real world evaluation shows, our approach achieves a relative model error of 15% for the prediction of dishes. Furthermore, we discuss the influence of beneficial features and assess their influence on the overall prediction quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41065728",
                    "name": "Lucas Woltmann"
                },
                {
                    "authorId": "2175528321",
                    "name": "Jonathan Drechsel"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "7c3af255d08c33581ebe4bb267212ca25fe51565",
            "title": "Publication Culture and Review Processes in the Data Management Community: An Open Discussion",
            "abstract": "The Data Management community has explored many options in recent years to improve our publication culture and review processes, ranging from innovative journal-conference hybrids that decouple publication from presentation, incorporating journal-style reviewing for conference-style papers, requesting code reproducibility and code/data availability, multiple submission deadlines in a year, new categories of papers, informal shepherding processes, guidelines for diversity and inclusion, automated COI check, and so on. This panel seeks to examine our many experiments, comparing them with other CS disciplines, and help determine (i) have our experiments worked? (ii) what has their impact been? and (iii) can we do better?",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1730344",
                    "name": "S. Bhowmick"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "9b10d779b7e9d93eda950ccc45ec060438aa85ac",
            "title": "To use or not to use the SIMD gather instruction?",
            "abstract": "The Single Instruction Multiple Data (SIMD) paradigm became a core principle for optimizing query operators. Until now, only the LOAD/STORE instructions are considered to be efficient enough to achieve the expected speedups, while avoiding GATHER/SCATTER is considered almost imperative. However, the GATHER instruction offers a very flexible way to populate SIMD registers with data elements coming from non-consecutive memory locations. As we will show within the paper, the GATHER instruction can achieve the same performance as the LOAD instruction, if applied properly. To enable the proper usage, we outline a novel access pattern which then allows fine-grained, partition-based SIMD implementations using the GATHER instruction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "33232107",
                    "name": "Johannes Pietrzyk"
                },
                {
                    "authorId": "9187969",
                    "name": "Alexander Krause"
                },
                {
                    "authorId": "27226616",
                    "name": "Juliana Hildebrandt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "c7fcb1d43dc5b594289603f017a87b9a02f4b1c7",
            "title": "Memory Efficient Scheduling of Query Pipeline Execution",
            "abstract": "State-of-the-art query engines pursue a pipeline-based query execution model. Using such a model, a pipeline computes a query plan fragment up to a pipeline breaker resulting in an intermediate result, which will be consumed by subsequent pipelines. Interestingly, the ordering of execution of such pipelines poses an opportunity for memory savings. Within this paper, we tackle the challenge to compute an optimal schedule of the individual pipelines with respect to minimizing the memory consumption needed for a particular query execution plan. We therefore will precisely state the problem and show the potential of an optimal pipeline execution ordering. We will then provide a formal model to describe the search space and propose four different algorithms to identify optimal/near-optimal schedules. The paper also presents insights into our implementation within a prototypical query execution engine and reports results relying on the Join Order Benchmark scenarios. Specifically, the experimental evaluation focuses on the identified memory savings and the stability of the query runtime behavior. Furthermore, the evaluation reports on the small overhead of the proposed search algorithms during the planning time, thus emphasizing the practical applicability of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171362657",
                    "name": "Lukas Landgraf"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "49803096",
                    "name": "Florian Wolf"
                },
                {
                    "authorId": "144879374",
                    "name": "Alexander B\u00f6hm"
                }
            ]
        },
        {
            "paperId": "d42685edf4b624ae0fe60bfb960307428f660939",
            "title": "Turbo-Charging SPJ Query Plans with Learned Physical Join Operator Selections",
            "abstract": "\n The optimization of select-project-join (SPJ) queries entails two major challenges: (i) finding a good join order and (ii) selecting the best-fitting physical join operator for each single join within the chosen join order. Previous work mainly focuses on the computation of a good join order, but leaves open to which extent the physical join operator selection accounts for plan quality. Our analysis using different query optimizers indicates that physical join operator selection is crucial and that none of the investigated query optimizers reaches the full potential of optimal operator selections. To unlock this potential, we propose\n TONIC\n , a novel cardinality estimation-free extension for generic SPJ query optimizers in this paper.\n TONIC\n follows a\n learning-based\n approach and revises operator decisions for arbitrary join paths based on learned query feedback. To continuously capture and reuse optimal operator selections, we introduce a lightweight yet powerful\n Query Execution Plan Synopsis\n (\n QEP-S\n ). In comparison to related work,\n TONIC\n enables transparent planning decisions with consistent performance improvements. Using two real-life benchmarks, we demonstrate that extending existing optimizers with\n TONIC\n substantially reduces query response times with a\n cumulative\n speedup of up to 2.8x.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491355603",
                    "name": "Axel Hertzschuch"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "ec1b23c44dd812234e38eb00590d8d8f3dee0338",
            "title": "Transformer-Encoder and Decoder Models for Questions on Math",
            "abstract": "This work summarizes our submission to ARQMath-3. We pre-trained Transformer-Encoder-based Language Models for the task of mathematical answer retrieval and employed a Transformer-Decoder Model for the generation of answers given a question from a mathematical domain. In comparison to our submission to ARQmath-2, we could improve the performance of our models regarding all three metrics nDGC\u2019, mAP\u2019 and p\u2019@10 by refined pre-training and enlarged fine-tuning data. In addition, we improved our p\u2019@10 results even further by additionally fine-tuning on annotated test data from ARQMath-2. In summary, our findings confirm that Transformer-based models benefit from domain adaptive pre-training in the mathematical domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2105728025",
                    "name": "Anja Reusch"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "f976b59766aa525a35ce609ce9697f1b75198309",
            "title": "FPGA-Based Database Query Processing on Arbitrarily Wide Tables",
            "abstract": "Thanks to the flexibility of FPGAs and their widespread adoption in the cloud, they have become attractive solutions for the acceleration of resource- and memory-intensive database workloads. Complex pipeline-breaking operators (e.g., aggregation, join) often constitute most of the execution time of the queries involved in these workloads. A popular approach in processing these operators is by pre-sorting the input, as they become single-pass algorithms on sorted tables [1] .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738666964",
                    "name": "Mehdi Moghaddamfar"
                },
                {
                    "authorId": "2071266998",
                    "name": "Christian F\u00e4rber"
                },
                {
                    "authorId": "1766564",
                    "name": "Norman May"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2143184651",
                    "name": "Akash Kumar"
                }
            ]
        },
        {
            "paperId": "084a75894f9b6d312f017f07a9325f244e41fd68",
            "title": "Towards Porting Hardware-Oblivious Vectorized Query Operators to GPUs",
            "abstract": "Nowadays, query processing in column-store database systems is highly tuned to the underlying (co-)processors. This approach works very well from a performance perspective, but has several shortcomings from a conceptual perspective. For example, this tuning introduces high implementation as well as maintenance cost and one implementation cannot be ported to other (co-)processors. To overcome that, we developed a column-store speci\ufb01c abstraction layer for hardware-driven vectorization based on the Single Instruction Multiple Data (SIMD) parallel paradigm. Thus, we are able to implement vectorized query operators in a hardware-oblivious manner, which can be specialized to di\ufb00erent SIMD instruction set extensions of modern x86-processors. To soften the limitation to x86-processors, we describe our vision to integrate GPUs in our abstraction layer by interpreting GPUs as virtual vector engines in this paper. Moreover, we present some initial evaluation results to determine a reasonable virtual vector size. We conclude the paper with an outlook on our ongoing research in that direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113619044",
                    "name": "Johannes Fett"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "12938ed592b0e26263a64bdf52868b3cf16c71a6",
            "title": "Accurate and Efficient Time Series Matching by Season- and Trend-aware Symbolic Approximation - Extended Version Including Additional Evaluation and Proofs",
            "abstract": "Processing and analyzing time series data\\-sets have become a central issue in many domains requiring data management systems to support time series as a native data type. A crucial prerequisite of these systems is time series matching, which still is a challenging problem. A time series is a high-dimensional data type, its representation is storage-, and its comparison is time-consuming. Among the representation techniques that tackle these challenges, the symbolic aggregate approximation (SAX) is the current state of the art. This technique reduces a time series to a low-dimensional space by segmenting it and discretizing each segment into a small symbolic alphabet. However, SAX ignores the deterministic behavior of time series such as cyclical repeating patterns or trend component affecting all segments and leading to a distortion of the symbolic distribution. In this paper, we present a season- and a trend-aware symbolic approximation. We show that this improves the symbolic distribution and increase the representation accuracy without increasing its memory footprint. Most importantly, this enables a more efficient time series matching by providing a match up to three orders of magnitude faster than SAX.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30389017",
                    "name": "Lars Kegel"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2a91e58740cce6fa88d59a22400effdf6d4618d8",
            "title": "LCTL: Lightweight Compression Template Library",
            "abstract": "For fast and efficient data processing, a common approach in many application domains is to mainly store and process data in form of arrays of integers offering several benefits. For example, with the help of some additional lightweight computations for lossless integer compression, the necessary memory space can be dramatically reduced. Moreover, compressed integer values offer advantages for data processing such as increasing the effective bandwidth to reduce the memory wall effect. Thus, a large corpus of lightweight integer compression formats has been developed. Unfortunately, there is no approach available that allows to define a specific format in a consistent way and to generate executable code for compression and decompression from it. To overcome that, we present the Lightweight Compression Template Library (LCTL) in this paper. As we are going to show, LCTL allows (i) the implementation of a variety of compression formats in an abstract way, (ii) the generation of efficient executable code for compression, and (iii) the automatic derivation of the decompression routines out of a compression format. Thus, LCTL offers a unique approach to comprehensively and systematically implement the large corpus of lightweight integer compression formats.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27226616",
                    "name": "Juliana Hildebrandt"
                },
                {
                    "authorId": "2151280885",
                    "name": "Andr\u00e9 Berthold"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2a9f27f2c38356cc5d4df73d9ef3199d9e0ae8c8",
            "title": "TU_DBS in the ARQMath Lab 2021, CLEF",
            "abstract": "Mathematical Information Retrieval (MIR) deals with the task of finding relevant documents that contain text and mathematical formulas. Therefore, retrieval systems should not only be able to process natural language, but also mathematical and scientific notation to retrieve documents. The goal of this work is to review the participation of our team in the ARQMath 2021 Lab where two different approaches based on ALBERT and ColBERT were applied to a Question Answer Retrieval task and a Formula Similarity task. The ALBERT-based classification approach received competitive results for the first task. We found that by pre-training on data separated in chunks of text and formulas, the model performed better on formula data. This way of pre-training could also be beneficial for the Formula Search task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2105728025",
                    "name": "Anja Reusch"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "35f5efba0c067cbe033db6bdfc2c12e90860388a",
            "title": "SIMD-MIMD cocktail in a hybrid memory glass: shaken, not stirred",
            "abstract": "Hybrid memory systems consisting of DRAM and NVRAM offer a great opportunity for column-oriented data systems to persistently store and to efficiently process columnar data completely in main memory. While vectorization (SIMD) of query operators is state-of-the-art to increase the single-thread performance, it has to be combined with thread-level parallelism (MIMD) to satisfy growing needs for higher performance and scalability. However, it is not well investigated how such a SIMD-MIMD interplay could be leveraged efficiently in hybrid memory systems. On the one hand, we deliver an extensive experimental evaluation of typical workloads on columnar data in this paper. We reveal that the choice of the most performant SIMD version differs greatly for both memory types. Moreover, we show that the throughput of concurrent queries can be boosted (up to 2x) when combining various SIMD flavors in a multi-threaded execution. On the other hand, to enable that optimization, we propose an adaptive SIMD-MIMD cocktail approach incurring only a negligible runtime overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1730399016",
                    "name": "M. Zarubin"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "9187969",
                    "name": "Alexander Krause"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "3fb3c52fd461037823ad433dd87777cd2c6563eb",
            "title": "Mastering the NEC Vector Engine Accelerator for Analytical Query Processing",
            "abstract": "NEC Corporation offers a vector engine as a specialized co-processor having two unique features. On the one hand, it operates on vector registers multiple times wider than those of recent mainstream x86-processors. On the other hand, this accelerator provides a memory bandwidth of up to 1.2TB/s for 48GB of main memory. Both features are interesting for analytical query processing: First, vectorization based on the Single Instruction Multiple Data (SIMD) paradigm is a state-of-the-art technique to improve the query performance on x86-processors. Thus, for this accelerator we are able to use the same programming, processing, and optimization concepts as for the host x86-processor. Second, this vector engine is an optimal platform for investigating the efficient vector processing on wide vector registers. To achieve that, we describe an approach to master this co-processor for analytical query processing using a column-store specific abstraction layer for vectorization in this paper. We also detail on selected evaluation results to show the benefits and shortcomings of our approach as well as of the coprocessor compared to x86-processors. We conclude the paper with a discussion on interesting future research activities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "2105752304",
                    "name": "Lennart Schmidt"
                },
                {
                    "authorId": "33232107",
                    "name": "Johannes Pietrzyk"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "8b27d909c3002f12455db7ed09e29e99f779430a",
            "title": "An ALBERT-based Similarity Measure for Mathematical Answer Retrieval",
            "abstract": "Mathematical Language Processing (MLP) deals with the automated processing and analysis of mathematical documents and relies heavily on good representations of mathematical symbols and texts. The aim of this work is to explore the modeling capabilities of state-of-the-art unsupervised deep learning methods to create such representations. Therefore, we pre-trained different instances of an ALBERT model on Mathematics StackExchange data and fine-tuned it on the task of Mathematical Answer Retrieval. Our evaluation shows that ALBERT outperforms all previous systems and is on par with current state-of-the-art systems for math retrieval indicating strong capabilities of modeling mathematical posts. This implies that our approach can also be beneficial to various other tasks in MLP such as automatic proof checking or summarization of scientific texts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2105728025",
                    "name": "Anja Reusch"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "9c15ab0bd20dd6811b6dc77b1352d9aa065b5069",
            "title": "ImitAL: Learning Active Learning Strategies from Synthetic Data",
            "abstract": "One of the biggest challenges that complicates applied supervised machine learning is the need for huge amounts of labeled data. Active Learning (AL) is a well-known standard method for efficiently obtaining labeled data by first labeling the samples that contain the most information based on a query strategy. Although many methods for query strategies have been proposed in the past, no clear superior method that works well in general for all domains has been found yet. Additionally, many strategies are computationally expensive which further hinders the widespread use of AL for large-scale annotation projects. We, therefore, propose ImitAL, a novel query strategy, which encodes AL as a learning-to-rank problem. For training the underlying neural network we chose Imitation Learning. The required demonstrative expert experience for training is generated from purely synthetic data. To show the general and superior applicability of \\ImitAL{}, we perform an extensive evaluation comparing our strategy on 15 different datasets, from a wide range of domains, with 10 different state-of-the-art query strategies. We also show that our approach is more runtime performant than most other strategies, especially on very large datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "80599770",
                    "name": "Julius Gonsior"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "a48e73d1c337023de79aac0db16144ad385559b9",
            "title": "The Case for SIMDified Analytical Query Processing on GPUs",
            "abstract": "Data-level parallelism (DLP) is a heavily used hardware-driven parallelization technique to optimize the analytical query processing, especially in in-memory column stores. This kind of parallelism is characterized by executing essentially the same operation on different data elements simultaneously. Besides Single Instruction Multiple Data (SIMD) extensions on common x86-processors, GPUs also provide DLP but with a different execution model called Single Instruction Multiple Threads (SIMT), where multiple scalar threads are executed in a SIMD manner. Unfortunately, a complete GPU-specific implementation of all query operators has to be set up, since the state of the vectorized implementations cannot be ported from x86-processors to GPUs right now. To avoid this implementation effort, we present our vision to virtualize GPUs as virtual vector engines with software-defined SIMD instructions and to specialize hardware-oblivious vectorized operators to GPUs using our Template Vector Library (TVL) in this paper.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113619044",
                    "name": "Johannes Fett"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "a57e8a2bc662622e89e61f6dce8ecfb197562b8c",
            "title": "RecipeGM: A Hierarchical Recipe Generation Model",
            "abstract": "This paper demonstrates the application of hierarchical convolutional neural networks using self-attention mechanisms for the task of generating recipes given a set of ingredients the recipe should contain. We compare this model, RECIPEGM, to an LSTM baseline and RecipeGPT using several metrics and show that our model is able to outperform even RecipeGPT in some cases. Furthermore, this work discusses suitable evaluation techniques for recipe generation and highlights weak points of some current in use metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2105728025",
                    "name": "Anja Reusch"
                },
                {
                    "authorId": "2105730345",
                    "name": "Alexander Weber"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "a95f50bd55eef66991338f59b1da7f488d65d92c",
            "title": "Small Selectivities Matter: Lifting the Burden of Empty Samples",
            "abstract": "Every year more and more advanced approaches to cardinality estimation are published, using learned models or other data and workload specific synopses. In contrast, the majority of commercial in-memory systems still relies on sampling. It is arguably the most general and easiest estimator to implement. While most methods do not seem to improve much over sampling-based estimators in the presence of non-selective queries, sampling struggles with highly selective queries due to limitations of the sample size. Especially in situations where no sample tuple qualifies, optimizers fall back to basic heuristics that ignore attribute correlations and lead to large estimation errors. In this work, we present a novel approach, dealing with these 0-Tuple Situations. It is ready to use in any DBMS capable of sampling, showing a negligible impact on optimization time. Our experiments on real world and synthetic data sets demonstrate up to two orders of magnitude reduced estimation errors. Enumerating single filter predicates according to our estimates reveals 1.3 to 1.8 times faster query responses for complex filters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491355603",
                    "name": "Axel Hertzschuch"
                },
                {
                    "authorId": "1809585",
                    "name": "G. Moerkotte"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1766564",
                    "name": "Norman May"
                },
                {
                    "authorId": "49803096",
                    "name": "Florian Wolf"
                },
                {
                    "authorId": "2113618344",
                    "name": "Lars Fricke"
                }
            ]
        },
        {
            "paperId": "bfcac51d045d80c1963ab8e7c792b00de3bada64",
            "title": "Pre-Trained Web Table Embeddings for Table Discovery",
            "abstract": "Pre-trained word embedding models have become the de-facto standard to model text in state-of-the-art analysis tools and frameworks. However, while there are massive amounts of textual data stored in tables, word embedding models are usually pre-trained on large documents. This mismatch can lead to narrowed performance on tasks where text values in tables are analyzed. To improve analysis and retrieval tasks working with tabular data, we propose a novel embedding technique to be pre-trained directly on a large Web table corpus. In an experimental evaluation, we employ our models for various data analysis tasks on different data sources. Our evaluation shows that models using pre-trained Web table embeddings outperform the same models when applied to embeddings pre-trained on text. Moreover, we show that by using Web table embeddings state-of-the-art models for the investigated tasks can be outperformed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143655513",
                    "name": "Michael G\u00fcnther"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "80599770",
                    "name": "Julius Gonsior"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "ddc66a6964979904a6deab37ba65cb406e2c07f2",
            "title": "PostCENN: PostgreSQL with Machine Learning Models for Cardinality Estimation",
            "abstract": "In this demo, we present PostCENN , an enhanced PostgreSQL data-base system with an end-to-end integration of machine learning (ML) models for cardinality estimation. In general, cardinality estimation is a topic with a long history in the database community. While traditional models like histograms are extensively used, recent works mainly focus on developing new approaches using ML models. However, traditional as well as ML models have their own advantages and disadvantages. With PostCENN , we aim to combine both to maximize their potentials for cardinality estimation by introducing ML models as a novel means to increase the accuracy of the cardinality estimation for certain parts of the database schema. To achieve this, we integrate ML models as first class citizen in PostgreSQL with a well-defined end-to-end life cycle. This life cycle consists of creating ML models for different sub-parts of the database schema, triggering the training, using ML models within the query optimizer in a transparent way, and deleting ML models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41065728",
                    "name": "Lucas Woltmann"
                },
                {
                    "authorId": "1396373351",
                    "name": "Dominik Olwig"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "e22e3a1edef8c24fca68e17dc970093b7f3d9916",
            "title": "Resource-Efficient Database Query Processing on FPGAs",
            "abstract": "FPGA technology has introduced new ways to accelerate database query processing, that often result in higher performance and energy efficiency. This is thanks to the unique architecture of FPGAs using reconfigurable resources to behave like an application-specific integrated circuit upon programming. The limited amount of these resources restricts the number and type of modules that an FPGA can simultaneously support. In this paper, we propose \"morphing sort-merge\": a set of run-time configurable FPGA modules that achieves resource efficiency by reusing the FPGA's resources to support different pipeline-breaking database operators, namely sort, aggregation, and equi-join. The proposed modules use dynamic optimization mechanisms that adapt the implementation to the distribution of data at run-time, thus resulting in higher performance. Our benchmarks show that morphing sort-merge reaches an average speedup of 5x compared to MonetDB.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738666964",
                    "name": "Mehdi Moghaddamfar"
                },
                {
                    "authorId": "2071266998",
                    "name": "Christian F\u00e4rber"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1766564",
                    "name": "Norman May"
                },
                {
                    "authorId": "143906106",
                    "name": "Akash Kumar"
                }
            ]
        },
        {
            "paperId": "faf5ff2e28be1fd982a5159eba3c14829591f515",
            "title": "Simplicity Done Right for Join Ordering",
            "abstract": "In this paper, we propose a simple, yet fast and e\ufb00ective approach to determine good join orders for arbitrary select-project-join queries. Our scheme comprises three building blocks: (i) a simple upper bound for arbitrary multi-joins, (ii) appropriate join enumeration according to the upper bound, and (iii) sampling as query execution to provide fast and near-exact estimates for complex conjunctive \ufb01lters. As we are going to show, using the Join-Order-Benchmark (JOB), our simple approach provides better join orderings with signi\ufb01cantly less optimization overhead, resulting in a substantially faster response time for all 113 JOB queries compared to state-of-the-art and recent approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491355603",
                    "name": "Axel Hertzschuch"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "10adb513cb357e7ff621991a9fe0bcf95ce3f959",
            "title": "Hardware-Oblivious SIMD Parallelism for In-Memory Column-Stores",
            "abstract": "Vectorization based on the Single Instruction Multiple Data (SIMD) parallel paradigm is a core technique to improve query processing performance especially in state-of-the-art in-memory column-stores. In mainstream CPUs, vectorization is offered by a large number of powerful SIMD extensions growing not only in vector size but also in terms of complexity of the provided instruction sets. However, programming with vector extensions is a non-trivial task and currently accomplished in a hardware-conscious way. This implementation process is not only error-prone but also connected with quite some effort for embracing new vector extensions or porting to other vector extensions. To overcome that, we present a Template Vector Library (TVL) as a hardware-oblivious concept in this paper. We will show that our single source hardware-oblivious implementation runs efficiently on different SIMD extensions as well as on a pure vector engine. Moreover, we demonstrate that several new optimization opportunities are possible, which are difficult to realize without a hardware-oblivious approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "33232107",
                    "name": "Johannes Pietrzyk"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "9187969",
                    "name": "Alexander Krause"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2878377",
                    "name": "E. Focht"
                }
            ]
        },
        {
            "paperId": "21dc269346e2c579d006c1238fa4e32d9d67e58e",
            "title": "Sharing Opportunities for OLTP Workloads in Different Isolation Levels",
            "abstract": "OLTP applications are usually executed by a high number of clients in parallel and are typically faced with high throughput demand as well as a constraint latency requirement for individual statements. Interestingly, OLTP workloads are often read-heavy and comprise similar query patterns, which provides a potential to share work of statements belonging to different transactions. Consequently, OLAP techniques for sharing work have started to be applied also to OLTP workloads, lately.\n In this paper, we present an approach for merging read statements within interactively submitted multi-statement transactions consisting of reads and writes. We first define a formal framework for merging transactions running under a given isolation level and provide insights into a prototypical implementation of merging within a commercial database system. In our experimental evaluation, we show that, depending on the isolation level, the load in the system and the read-share of the workload, an improvement of the transaction throughput by up to a factor of 2.5X is possible without compromising the transactional semantics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2568755",
                    "name": "Robin Rehrmann"
                },
                {
                    "authorId": "2691974",
                    "name": "Carsten Binnig"
                },
                {
                    "authorId": "144879374",
                    "name": "Alexander B\u00f6hm"
                },
                {
                    "authorId": "94622503",
                    "name": "Kihong Kim"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "391041eb316643609bc6af56dea3ae602b26f9d0",
            "title": "Active Learning for Spreadsheet Cell Classification",
            "abstract": "Spreadsheets are mainly the most successful content generation tools, used in almost every enterprise to create a plethora of semi-structured data. However, this information is often intermingled with various formatting, layout, and textual metadata, making it hard to identify and extract the actual tabularly structured payload. For this reason, automated information extraction from spreadsheets is a challenging task. Previous papers proposed cell classification as a first step of the table extraction process, which, however, requires a substantial amount of labeled training data, that is expensive to obtain. Therefore, in this paper we investigate a semi-supervised approach called Active Learning (AL), that can be used to train classification models by selecting only the most informative examples from an unlabeled dataset. In detail, we implement an AL cycle for spreadsheet cell classification by investigating different selection strategies and stopping criteria. We compare the performance of various AL strategies and derive guidelines for semi-supervised cell classification. Our experiments show, that by implementing AL for cell classification, we are able to reduce the amount of training data by 90% without any accuracy losses compared to a passive classifier.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "80599770",
                    "name": "Julius Gonsior"
                },
                {
                    "authorId": "1491425973",
                    "name": "Josephine Rehak"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "1757263",
                    "name": "Elvis Koci"
                },
                {
                    "authorId": "143655513",
                    "name": "Michael G\u00fcnther"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "4c5363dbcd097bd8795e2a95dcd3413501967356",
            "title": "MorphStore",
            "abstract": "In this paper, we present MorphStore, an open-source in-memory columnar analytical query engine with a novel holistic compression-enabled processing model. Basically, compression using lightweight integer compression algorithms already plays an important role in existing in-memory column-store database systems, but mainly for base data. In particular, during query processing, these systems only keep the data compressed until an operator cannot process the compressed data directly, whereupon the data is decompressed, but not recompressed. Thus, the full potential of compression during query processing is not exploited. To overcome that, we developed a novel compression-enabled processing model as presented in this paper. As we are going to show, the continuous usage of compression for all base data and all intermediates is very beneficial to reduce the overall memory footprint as well as to improve the query performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "33232107",
                    "name": "Johannes Pietrzyk"
                },
                {
                    "authorId": "9187969",
                    "name": "Alexander Krause"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "5aa2f2f9f19c93ad40d9316c540cb857416f771d",
            "title": "Best of both worlds: combining traditional and machine learning models for cardinality estimation",
            "abstract": "Cardinality estimation is a high-profile technique in database management systems with a serious impact on query performance. Thus, a lot of traditional approaches such as histograms-based or sampling-based methods have been developed over the last decades. With the advance of Machine Learning (ML) into the database world, cardinality estimation profits from several methods improving its quality as shown in different recent papers. However, neither an ML model nor a traditional approach meets all requirements for cardinality estimation, so that a one size fits all approach is difficult to imagine. For that reason, we advocate a better interlacing of ML models and traditional approaches for cardinality estimation and thoroughly consider their potential, advantages, and disadvantages in this paper. We start by proposing a classification of different estimation techniques and their usability for cardinality estimation. Then, we motivate a novel hybrid approach as the core proof of concept of this paper which uses the best of both worlds: ML models and the proven histogram approach. For this, we show in which cases it is beneficial to use ML models or when we can trust the traditional estimators. We evaluate our hybrid approach on two real-world data sets and conclude what can be done to improve the coexistence of traditional and ML approaches in DBMS. With all our proposals, we use ML to improve DBMS without abandoning years of valuable research in cardinality estimation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41065728",
                    "name": "Lucas Woltmann"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "646266ec0f1c506d8d102f546b94cb9371a82fd7",
            "title": "Learning from Textual Data in Database Systems",
            "abstract": "Relational database systems hold massive amounts of text, valuable for many machine learning (ML) tasks. Since ML techniques depend on numerical input representations, pre-trained word embeddings are increasingly utilized to convert text values into meaningful numbers. However, a na\u00efve one-to-one mapping of each word in a database to a word embedding vector misses incorporating rich context information given by the database schema. Thus, we propose a novel relational retrofitting framework Retro to learn numerical representations of text values in databases, capturing the rich information encoded by pre-trained word embedding models as well as context information provided by tabular and foreign key relations in the database. We defined relation retrofitting as an optimization problem, present an efficient algorithm solving it, and investigate the influence of various hyperparameters. Further, we develop simple feed-forward and complex graph convolutional neural network architectures to operate on those representations. Our evaluation shows that the proposed embeddings and models are ready-to-use for many ML tasks, such as text classification, imputation, and link prediction, and even outperform state-of-the-art techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143655513",
                    "name": "Michael G\u00fcnther"
                },
                {
                    "authorId": "2000983253",
                    "name": "Philipp Oehme"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "6678569076bf2d472c3d7bd42fca5234626eb8aa",
            "title": "Enabling low tail latency on multicore key-value stores",
            "abstract": "Modern applications employ key-value stores (KVS) in at least some point of their software stack, often as a caching system or a storage manager. Many of these applications also require a high degree of responsiveness and performance predictability. However, most KVS have similar design decisions which focus on improving throughput metrics, at times by sacrificing latency. While latency can be occasionally reduced by over provisioning hardware, this entails significant increase in costs. In this paper we present RStore, a KVS which focus on low tail latency as its primary goal, while also enabling efficient usage of hardware resources. To that aim, we argue in favor of techniques such as an asynchronous programming model, message-passing communication, and log-structured storage on modern hardware. Throughout the paper we discuss these and other design decisions of RStore that differ from those of more traditional systems. Our evaluation shows that RStore scales its throughput with an increasing number of cores while maintaining a robust behavior with low and predictable latency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143708490",
                    "name": "L. Lersch"
                },
                {
                    "authorId": "3369609",
                    "name": "I. Schr\u00e9ter"
                },
                {
                    "authorId": "2742360",
                    "name": "Ismail Oukid"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "682e10ad9c4092c962e2cb77726f33c9e3cc7132",
            "title": "Integrating Lightweight Compression Capabilities into Apache Arrow",
            "abstract": "With the ongoing shift to a data-driven world in almost all application domains, the management and in particular the analytics of large amounts of data gain in importance. For that reason, a variety of new big data systems has been developed in recent years. Aside from that, a revision of the data organization and formats has been initiated as a foundation for these big data systems. In this context, Apache Arrow is a novel cross-language development platform for in-memory data with a standardized language-independent columnar memory format. The data is organized for efficient analytic operations on modern hardware, whereby Apache Arrow only supports dictionary encoding as a specific compression approach. However, there exists a large corpus of lightweight compression algorithms for columnar data which helps to reduce the necessary memory space as well as to increase the processing performance. Thus, we present a flexible and language-independent approach integrating lightweight compression algorithms into the Apache Arrow framework in this paper. With our so-called ArrowComp approach, we preserve the unique properties of Apache Arrow, but enhance the platform with a large variety of lightweight compression capabilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27226616",
                    "name": "Juliana Hildebrandt"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "683c7bc3e26ae62fde359bb5a34cd457ee0fbecc",
            "title": "Polymorphic Compressed Replication of Columnar Data in Scale-Up Hybrid Memory Systems",
            "abstract": "In-memory database systems adopting a columnar storage model play a crucial role with respect to data analytics. While data is completely kept in-memory by these systems for efficiency, data has to be stored on a non-volatile medium for persistence and fault tolerance as well. Traditionally, slow block-level devices like HDDs or SSDs are used which, however, can be replaced by fast byte-addressable NVRAM nowadays. Thus, hybrid memory systems consisting of DRAM and NVRAM offer a great opportunity for column-oriented database systems to persistently store and to efficiently process columnar data exclusively in main-memory. However, possible DRAM and NVRAM failures still necessitate the protection of primary data. While data replication is a suitable means, it increases the NVRAM endurance problem through increased write activities. To tackle that challenge and to reduce the overhead of replication, we propose a novel Polymorphic Compressed Replication (PCR) mechanism representing replicas using lightweight compression algorithms to reduce NVRAM writes, while supporting different compressed formats for the replicas of one column to facilitate different database operations during query processing. To show the feasibility and applicability, we developed an inmemory column-store prototype transparently employing PCR through an abstract user-space library. Based on this prototype, our conducted experiments show the effectiveness of our proposed PCR mechanism.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1730399016",
                    "name": "M. Zarubin"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "7aa3bcf22f1a34168a6b66d21c0ebd82a952d10d",
            "title": "RetroLive: Analysis of Relational Retrofitted Word Embeddings",
            "abstract": "Text values are valuable information in relational database systems for analysis and machine learning (ML) tasks. Since ML techniques depend on numerical input representations, word embeddings are increasingly utilized to convert symbolic representations such as text into meaningful numbers. However, those models do not incorporate the context-specific semantics of text values in the database. To significantly improve the representation of text values occurring in DBMS, we propose a novel retrofitting approach called Retro which considers both, the semantics of the word embedding and the relational schema. Based on this, we developed RetroLive, an interactive system, that allows exploring how the retrofitted embeddings improve the performance for various ML and integration tasks. Moreover, the demo includes several interactive visualizations to explore the characteristics of the adapted vectors and their connection to the relational database.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143655513",
                    "name": "Michael G\u00fcnther"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "1586561666",
                    "name": "Erik Nikulski"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "976666adcaeb5ae7af0cfc35de755f22601ee5c8",
            "title": "Feature-aware forecasting of large-scale time series data sets",
            "abstract": "Abstract The Internet of Things (IoT) sparks a revolution in time series forecasting. Traditional techniques forecast time series individually, which becomes unfeasible when the focus changes to thousands of time series exhibiting anomalies like noise and missing values. This work presents CSAR, a technique forecasting a set of time series with only one model, and a feature-aware partitioning applying CSAR on subsets of similar time series. These techniques provide accurate forecasts a hundred times faster than traditional techniques, preparing forecasting for the arising challenges of the IoT era.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "30389017",
                    "name": "Lars Kegel"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "a819794e1067bb5b01a44fdc942fa967e1289c61",
            "title": "Comparative analysis of OpenCL and RTL for sort-merge primitives on FPGA",
            "abstract": "As a result of recent improvements in FPGA technology, their benefits for highly efficient data processing pipelines are becoming more and more apparent. However, traditional RTL methods for programming FPGAs require knowledge of digital design and hardware description languages. OpenCL\u2122 provides software developers with a C-based platform for implementing their applications without deep knowledge of digital design. In this paper, we conduct a comparative analysis of OpenCL and RTL-based implementations of a novel heapsort with merging sorted runs. In particular, we quantitatively compare their performance, FPGA resource utilization, and development effort. Our results show that while requiring comparable development effort, RTL implementations of critical primitives used in the algorithm achieve 4X better performance while using half as much the FPGA resources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738666964",
                    "name": "Mehdi Moghaddamfar"
                },
                {
                    "authorId": "2071266998",
                    "name": "Christian F\u00e4rber"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1766564",
                    "name": "Norman May"
                }
            ]
        },
        {
            "paperId": "ad2e4b6a38fde9b00d7673f155ee9512031d2655",
            "title": "Workload merging potential in SAP Hybris",
            "abstract": "OLTP DBMSs in enterprise scenarios are often facing the challenge to deal with workload peaks resulting from events such as Cyber Monday or Black Friday. The traditional solution to prevent running out of resources and thus coping with such workload peaks is to use a significant over-provisioning of the underlying infrastructure. Another direction to cope with such peak scenarios is to apply resource sharing. In a recent work, we showed that merging read statements in OLTP scenarios offers the opportunity to maintain low latency for systems under heavy load without over-provisioning. In this paper, we analyze a real enterprise OLTP workload --- SAP Hybris --- with respect to statements types, complexity, and hot-spot statements to find potential candidates for workload sharing in OLTP. We additionally share work of the Hybris workload in our system OLTPShare and report on savings with respect to CPU consumption. Another interesting effect we show is that with OLTPShare, we can increase the SAP Hybris throughput by 20%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2568755",
                    "name": "Robin Rehrmann"
                },
                {
                    "authorId": "51933312",
                    "name": "Martin Keppner"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2691974",
                    "name": "Carsten Binnig"
                },
                {
                    "authorId": "48069347",
                    "name": "Arne Schwarz"
                }
            ]
        },
        {
            "paperId": "b8a3f696171c09c0732cf598e63100a0d8d5f360",
            "title": "Configuring Parallelism for Hybrid Layouts Using Multi-Objective Optimization",
            "abstract": "Modern organizations typically store their data in a raw format in data lakes. These data are then processed and usually stored under hybrid layouts, because they allow projection and selection operations. Thus, they allow (when required) to read less data from the disk. However, this is not very well exploited by distributed processing frameworks (e.g., Hadoop, Spark) when analytical queries are posed. These frameworks divide the data into multiple partitions and then process each partition in a separate task, consequently creating tasks based on the total file size and not the actual size of the data to be read. This typically leads to launching more tasks than needed, which, in turn, increases the query execution time and induces significant waste of computing resources. To allow a more efficient use of resources and reduce the query execution time, we propose a method that decides the number of tasks based on the data being read. To this end, we first propose a cost-based model for estimating the size of data read in hybrid layouts. Next, we use the estimated reading size in a multi-objective optimization method to decide the number of tasks and computational resources to be used. We prototyped our solution for Apache Parquet and Spark and found that our estimations are highly correlated (0.96) with the real executions. Further, using TPC-H we show that our recommended configurations are only 5.6% away from the Pareto front and provide 2.1\u2009\u00d7\u2009speedup compared with default solutions.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240624",
                    "name": "Rana Faisal Munir"
                },
                {
                    "authorId": "48843545",
                    "name": "A. Abell\u00f3"
                },
                {
                    "authorId": "144149891",
                    "name": "Oscar Romero"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "cfc0d20e8fb0bfb0ab012e92da69ff5747e508b8",
            "title": "FacetE: exploiting web tables for domain-specific word embedding evaluation",
            "abstract": "Today's natural language processing and information retrieval systems heavily depend on word embedding techniques to represent text values. However, given a specific task deciding for a word embedding dataset is not trivial. Current word embedding evaluation methods mostly provide only a one-dimensional quality measure, which does not express how knowledge from different domains is represented in the word embedding models. To overcome this limitation, we provide a new evaluation data set called FacetE derived from 125M Web tables, enabling domain-sensitive evaluation. We show that FacetE can effectively be used to evaluate word embedding models. The evaluation of common general-purpose word embedding models suggests that there is currently no best word embedding for every domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143655513",
                    "name": "Michael G\u00fcnther"
                },
                {
                    "authorId": "1716207533",
                    "name": "Paul Sikorski"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "04c7b202491a6dae810209a58e32372c5e2b8079",
            "title": "A Genetic-Based Search for Adaptive Table Recognition in Spreadsheets",
            "abstract": "Spreadsheets are very successful content generation tools, used in almost every enterprise to create a wealth of information. However, this information is often intermingled with various formatting, layout, and textual metadata, making it hard to identify and interpret the tabular payload. Previous works proposed to solve this problem by mainly using heuristics. Although fast to implement, these approaches fail to capture the high variability of user-generated spreadsheet tables. Therefore, in this paper, we propose a supervised approach that is able to adapt to arbitrary spreadsheet datasets. We use a graph model to represent the contents of a sheet, which carries layout and spatial features. Subsequently, we apply genetic-based approaches for graph partitioning, to recognize the parts of the graph corresponding to tables in the sheet. The search for tables is guided by an objective function, which is tuned to match the specific characteristics of a given dataset. We present the feasibility of this approach with an experimental evaluation, on a large, real-world spreadsheet corpus.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1757263",
                    "name": "Elvis Koci"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "144149891",
                    "name": "Oscar Romero"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "107e08508e47d0056db83a07a942e13b1848203f",
            "title": "Persistent Buffer Management with Optimistic Consistency",
            "abstract": "Finding the best way to leverage non-volatile memory (NVM) on modern database systems is still an open problem. The answer is far from trivial since the clear boundary between memory and storage present in most systems seems to be incompatible with the intrinsic memory-storage duality of NVM. Rather than treating NVM either solely as memory or solely as storage, in this work we propose how NVM can be simultaneously used as both in the context of modern database systems. We design a persistent buffer pool on NVM, enabling pages to be directly read/written by the CPU (like memory) while recovering corrupted pages after a failure (like storage). The main benefits of our approach are an easy integration in the existing database architectures, reduced costs (by replacing DRAM with NVM), and faster peak-performance recovery.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143708490",
                    "name": "L. Lersch"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2742360",
                    "name": "Ismail Oukid"
                }
            ]
        },
        {
            "paperId": "10ff63cc209135b0d8a728ec087502f79b5d5b62",
            "title": "First Investigations of the Vector Supercomputer SX-Aurora TSUBASA as a Co-Processor for Database Systems",
            "abstract": "The hardware landscape is currently changing from homogeneous multi-core systems towards heterogeneous systems with many different computing units, each with their own characteristics. This trend is a great opportunity for database systems to increase the overall performance if the heterogeneous resources can be utilized efficiently. Following that trend, NEC cooperation has recently introduced a novel heterogeneous hardware system called SX-Aurora TSUBASA. This novel heterogeneous system features a strong vector engine as a (co-)processor providing world\u2019s highest memory bandwidth of 1.2TB/s per vector processor. From a database system perspective, where many operations are memory bound, this bandwidth is very interesting. Thus, we describe the unique architecture and properties of this novel heterogeneous system in this paper. Moreover, we present first database-specific evaluation results to show the benefit of this system to increase the query performance. We conclude the paper with an outlook on our ongoing research activities in this direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33232107",
                    "name": "Johannes Pietrzyk"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "27ea31d82b91f0b77c88ed99465eb13bed9dd7d7",
            "title": "DataCalc: Ad-hoc Analyses on Heterogeneous Data Sources",
            "abstract": "Storing and processing data at different locations using a heterogeneous set of formats and data managements systems is state-of-the-art in many organizations. However, data analyses can often provide better insight when data from several sources is integrated into a combined perspective. In this paper we present an overview of our data integration system DataCalc. DataCalc is an extensible integration platform that executes adhoc analytical queries on a set of heterogeneous data processors. Our novel platform uses an expressive function shipping interface that promotes local computation and reduces data movement between processors. In this paper, we provide a discussion of the overall architecture and the main components of DataCalc. Moreover, we discuss the cost of integrating additional processors and evaluate the overall performance of the platform.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3469702",
                    "name": "Johannes Luong"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2b519f3ec796f6fb5cad8d766e57e3fcf7ac8d7b",
            "title": "Conjunctive Queries with Theta Joins Under Updates",
            "abstract": "Modern application domains such as Composite Event Recognition (CER) and real-time Analytics require the ability to dynamically refresh query results under high update rates. Traditional approaches to this problem are based either on the materialization of subresults (to avoid their recomputation) or on the recomputation of subresults (to avoid the space overhead of materialization). Both techniques have recently been shown suboptimal: instead of materializing results and subresults, one can maintain a data structure that supports efficient maintenance under updates and can quickly enumerate the full query output, as well as the changes produced under single updates. Unfortunately, these data structures have been developed only for aggregate-join queries composed of equi-joins, limiting their applicability in domains such as CER where temporal joins are commonplace. In this paper, we present a new approach for dynamically evaluating queries with multi-way theta-joins under updates that is effective in avoiding both materialization and recomputation of results, while supporting a wide range of applications. To do this we generalize Dynamic Yannakakis, an algorithm for dynamically processing acyclic equi-join queries. In tandem, and of independent interest, we generalize the notions of acyclicity and free-connexity to arbitrary theta-joins and show how to compute corresponding join trees. We instantiate our framework to the case where theta-joins are only composed of equalities and inequalities and experimentally compare our algorithm to state of the art CER systems as well as incremental view maintenance engines. Our approach performs consistently better than the competitor systems with up to two orders of magnitude improvements in both time and memory consumption.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059322882",
                    "name": "Muhammad Idris"
                },
                {
                    "authorId": "144739409",
                    "name": "M. Ugarte"
                },
                {
                    "authorId": "1709642",
                    "name": "Stijn Vansummeren"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2d86338973469a5edaf6eb8b54cdd4166924296d",
            "title": "Fighting the Duplicates in Hashing: Conflict Detection-aware Vectorization of Linear Probing",
            "abstract": "Hash tables are a core data structure in database systems, because they are fundamental for many database operators like hash-based join and aggregation. In recent years, the efficient vectorized implementation using SIMD (Single Instruction Multiple Data) instructions has attracted a lot of attention. Generally, all hash table implementations need to address what happens when collisions occur. In order to do that, the collisions have to be detected first. There are two types of collisions: (i) key duplicates and (ii) hash value duplicates. The second type is more complicated than the first type. In this paper, we investigate linear probing as a heavily applied hash table implementation and we present an extension of the state-of-the-art vectorized implementation with a hardware-supported duplicate or collision detection. For that, we use novel SIMD instructions which have been introduced with Intel\u2019s SIMD instruction set extension AVX-512. As we are going to show, our approach outperforms the state-of-the-art vectorized version for the key handling, but introduces novel challenges for the value handling. We conclude the paper with some ideas how to tackle that challenge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33232107",
                    "name": "Johannes Pietrzyk"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "5410f3fc1ebec51099c31c52007b7034572d26ed",
            "title": "Trading Memory versus Workload Overhead in Graph Pattern Matching on Multiprocessor Systems",
            "abstract": "Graph pattern matching (GPM) is a core primitive in graph analysis with many applications. Efficient processing of GPM on modern NUMA systems poses several challenges, such as an intelligent storage of the graph itself or keeping track of vertex locality information. During query processing, intermediate results need to be communicated, but target partitions are not always directly identifiable, which requires all workers to scan for requested vertices. To optimize this performance bottleneck, we introduce a Bloom filter based workload reduction approach and discuss the benefits and drawbacks of different implementations. Furthermore, we show the trade-offs between invested memory and performance gain, compared to fully redundant storage.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9187969",
                    "name": "Alexander Krause"
                },
                {
                    "authorId": "40287401",
                    "name": "Frank Ebner"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "62edf462519f494a81c383c2c6802ace421d294b",
            "title": "Architecture and Advanced Electronics Pathways Toward Highly Adaptive Energy- Efficient Computing",
            "abstract": "With the explosion of the number of compute nodes, the bottleneck of future computing systems lies in the network architecture connecting the nodes. Addressing the bottleneck requires replacing current backplane-based network topologies. We propose to revolutionize computing electronics by realizing embedded optical waveguides for onboard networking and wireless chip-to-chip links at 200-GHz carrier frequency connecting neighboring boards in a rack. The control of novel rate-adaptive optical and mm-wave transceivers needs tight interlinking with the system software for runtime resource management.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145570132",
                    "name": "G. Fettweis"
                },
                {
                    "authorId": "1691157",
                    "name": "Meik D\u00f6rpinghaus"
                },
                {
                    "authorId": "144195575",
                    "name": "J. Castrill\u00f3n"
                },
                {
                    "authorId": "143906106",
                    "name": "Akash Kumar"
                },
                {
                    "authorId": "5403194",
                    "name": "C. Baier"
                },
                {
                    "authorId": "50275025",
                    "name": "K. Bock"
                },
                {
                    "authorId": "3097477",
                    "name": "F. Ellinger"
                },
                {
                    "authorId": "4009619",
                    "name": "A. Fery"
                },
                {
                    "authorId": "8387960",
                    "name": "F. Fitzek"
                },
                {
                    "authorId": "1731688",
                    "name": "Hermann H\u00e4rtig"
                },
                {
                    "authorId": "3002608",
                    "name": "K. Jamshidi"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "3267486",
                    "name": "M. Mertig"
                },
                {
                    "authorId": "1781970",
                    "name": "W. Nagel"
                },
                {
                    "authorId": "144060080",
                    "name": "Giang T. Nguyen"
                },
                {
                    "authorId": "1678946",
                    "name": "D. Plettemeier"
                },
                {
                    "authorId": "31188628",
                    "name": "M. Schr\u00f6ter"
                },
                {
                    "authorId": "143850196",
                    "name": "T. Strufe"
                }
            ]
        },
        {
            "paperId": "84892ebc3e052d33191889c60bfb877c34722450",
            "title": "Assessing the Impact of Driving Bans with Data Analysis",
            "abstract": "Suspended particulate matter (SPM) is a significant problem discussed in current environmental research with an impact on the every-day life of many people. Our goal for the BTW 2019 Data Science Challenge (DSC) is to leverage information from available sensor data about SPM and assess the benefits and disadvantages of driving bans. Our application builds upon data of 57 sensors in the city of Dresden and 338 sensors in the city of Stuttgart. Each sensor tracks particle concentration, temperature, and humidity. Stuttgart has a particular interesting situation because of the driving ban for outdated diesel engines on roads in the inner city introduced in January 2019. This gives us the possibility to compare the effectiveness of driving bans not only over time but also between two cities. While we only analyze two cities exemplary in this report, we see high potential of applying our tools to other cities and scenarios. We think, this universality of our approach is an important factor in knowledge transfer. The applications are not limited to SPM analyses but can be extended for example to weather and climate research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41065728",
                    "name": "Lucas Woltmann"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "896d48e25148570cf8f1b6685d3289cf298ebf96",
            "title": "Explore FREDDY: Fast Word Embeddings in Database Systems",
            "abstract": "Word embeddings encode a lot of semantic as well as syntactic features and therefore are useful in many tasks especially in Natural Language Processing and Information Retrieval. FREDDY (Fast woRd EmbedDings Database sYstems), an extended PostgreSQL database system, allowing the user to analyze structured knowledge in the database relations together with unstructured text corpora encoded as word embedding by introducing novel operations for similarity calculation and analogy inference. Approximation techniques support these operations to perform fast similarity computations on high-dimensional vector spaces. This demo allows exploring the powerful query capabilities of FREDDY on different database schemes and a variety of word embeddings generated on different text corpora. From a systems perspective, the user is able to examine the impact of multiple approximation techniques and their parameters for similarity search on query execution time and precision.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143655513",
                    "name": "Michael G\u00fcnther"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "93128044",
                    "name": "Zdravko Yanakiev"
                }
            ]
        },
        {
            "paperId": "898ec8666572418dbb94a1280d18805432db8fbe",
            "title": "A Technical Perspective of DataCalc \u2014 Ad-hoc Analyses on Heterogeneous Data Sources",
            "abstract": "Many organizations store and process data at different locations using a heterogeneous set of formats and data management systems. However, data analyses can often provide better insight when data from several sources is integrated into a combined perspective. DataCalc is an extensible data integration platform that executes ad-hoc analytical queries on a set of heterogeneous data processors. The platform uses an expressive function shipping interface that promotes local computation and reduces data movement between processors. In this paper, we provide a detailed discussion of the architecture and implementation of DataCalc. We introduce data processors for plain files, JDBC, the MongoDB document store, and a custom in memory system. Finally, we discuss the cost of integrating additional processors and evaluate the overall performance of the platform. Our main contribution is the specification and evaluation of the DataCalc code delegation interface.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3469702",
                    "name": "Johannes Luong"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "8cb621e003ba8ff6de8c2c9f15adeccf67d6ca05",
            "title": "XLIndy: Interactive Recognition and Information Extraction in Spreadsheets",
            "abstract": "Over the years, spreadsheets have established their presence in many domains, including business, government, and science. However, challenges arise due to spreadsheets being partially-structured and carrying implicit (visual and textual) information. This translates into a bottleneck, when it comes to automatic analysis and extraction of information. Therefore, we present XLIndy, a Microsoft Excel add-in with a machine learning back-end, written in Python. It showcases our novel methods for layout inference and table recognition in spreadsheets. For a selected task and method, users can visually inspect the results, change configurations, and compare different runs. This enables iterative fine-tuning. Additionally, users can manually revise the predicted layout and tables, and subsequently save them as annotations. The latter is used to measure performance and (re-)train classifiers. Finally, data in the recognized tables can be extracted for further processing. XLIndy supports several standard formats, such as CSV and JSON.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1757263",
                    "name": "Elvis Koci"
                },
                {
                    "authorId": "1389838356",
                    "name": "Dana Kuban"
                },
                {
                    "authorId": "2065963951",
                    "name": "Nico Luettig"
                },
                {
                    "authorId": "1396373351",
                    "name": "Dominik Olwig"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "80599770",
                    "name": "Julius Gonsior"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "144149891",
                    "name": "Oscar Romero"
                }
            ]
        },
        {
            "paperId": "90d7f008e6ceb4131160ca4ec4a4f1bb72a37b79",
            "title": "MorphStore - In-Memory Query Processing based on Morphing Compressed Intermediates LIVE",
            "abstract": "In this demo, we present MorphStore, an in-memory column store with a novel compression-aware query processing concept. Basically, compression using lightweight integer compression algorithms already plays an important role in existing in-memory column stores, but mainly for base data. The continuous handling of compression from the base data to the intermediate results during query processing has already been discussed, but not investigated in detail since the computational effort for compression as well as decompression is often assumed to exceed the benefits of a reduced transfer cost between CPU and main memory. However, this argument increasingly loses its validity as we are going to show in our demo. Generally, our novel compression-aware query processing concept is characterized by the fact that we are able to speed up the query execution by morphing compressed intermediate results from one scheme to another scheme to dynamically adapt to the changing data characteristics during query processing. Our morphing decisions are made using a cost-based approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "33232107",
                    "name": "Johannes Pietrzyk"
                },
                {
                    "authorId": "9187969",
                    "name": "Alexander Krause"
                },
                {
                    "authorId": "27226616",
                    "name": "Juliana Hildebrandt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "94564565865abd743479ec32bf30945ce8096324",
            "title": "Integer Compression in NVRAM-centric Data Stores: Comparative Experimental Analysis to DRAM",
            "abstract": "Lightweight integer compression algorithms play an important role in in-memory database systems to tackle the growing gap between processor speed and main memory bandwidth. Thus, there is a large number of algorithms to choose from, while different algorithms are tailored to different data characteristics. As we show in this paper, with the availability of byte-addressable non-volatile random-access memory (NVRAM), a novel type of main memory with specific characteristics increases the overall complexity in this domain. In particular, we provide a detailed evaluation of state-of-the-art lightweight integer compression schemes and database operations on NVRAM and compare it with DRAM. Furthermore, we reason about possible deployments of middle- and heavyweight approaches for better adaptation to NVRAM characteristics. Finally, we investigate a combined approach where both volatile and non-volatile memories are used in a cooperative fashion that is likely to be the case for hybrid and NVRAM-centric database systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1730399016",
                    "name": "M. Zarubin"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "3250632",
                    "name": "Thomas Willhalm"
                }
            ]
        },
        {
            "paperId": "9aded16eb506f2559324ea67f532b6ab39ece3ef",
            "title": "RETRO: Relation Retrofitting For In-Database Machine Learning on Textual Data",
            "abstract": "There are massive amounts of textual data residing in databases, valuable for many machine learning (ML) tasks. Since ML techniques depend on numerical input representations, word embeddings are increasingly utilized to convert symbolic representations such as text into meaningful numbers. However, a naive one-to-one mapping of each word in a database to a word embedding vector is not sufficient and would lead to poor accuracies in ML tasks. Thus, we argue to additionally incorporate the information given by the database schema into the embedding, e.g. which words appear in the same column or are related to each other. In this paper, we propose RETRO (RElational reTROfitting), a novel approach to learn numerical representations of text values in databases, capturing the best of both worlds, the rich information encoded by word embeddings and the relational information encoded by database tables. We formulate relation retrofitting as a learning problem and present an efficient algorithm solving it. We investigate the impact of various hyperparameters on the learning problem and derive good settings for all of them. Our evaluation shows that the proposed embeddings are ready-to-use for many ML tasks such as classification and regression and even outperform state-of-the-art techniques in integration tasks such as null value imputation and link prediction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143655513",
                    "name": "Michael G\u00fcnther"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "9c7891fdbd214fff3d514b5b9c35b2e7b7cefe44",
            "title": "NeMeSys - A Showcase of Data Oriented Near Memory Graph Processing",
            "abstract": "NeMeSys is a NUMA-aware graph pattern processing engine, which uses the Near Memory Processing paradigm to allow for high scalability. With modern server systems incorporating an increasing amount of main memory, we can store graphs and compute analytical graph algorithms like graph pattern matching completely in-memory. Our system blends state-of-the-art approaches from the transactional database world together with graph processing principles. We demonstrate, that graph pattern processing - standalone and workloads - can be controlled by leveraging different partitioning strategies, applying Bloom filter based messaging optimization and, given performance constraints, can save energy by applying frequency scaling of CPU cores.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9187969",
                    "name": "Alexander Krause"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "a00a21ccff9f005e6bdcc57542507f594a72a1bd",
            "title": "Fast Approximated Nearest Neighbor Joins For Relational Database Systems",
            "abstract": "K nearest neighbor search (kNN-Search) is a universal data processing technique and a fundamental operation for word embeddings trained by word2vec or related approaches. The benefits of operations on dense vectors like word embeddings for analytical functionalities of RDBMSs motivate an integration of kNN-Joins. However, kNN-Search, as well as kNN-Joins, have barely been integrated into relational database systems so far. In this paper, we develop an index structure for approximated kNN-Joins working well on high-dimensional data and provide an integration into PostgreSQL. The novel index structure is efficient for different cardinalities of the involved join partners. An evaluation of the system based on applications on word embeddings shows the benefits of such an integrated kNN-Join operation and the performance of the proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143655513",
                    "name": "Michael G\u00fcnther"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "bc253dfe5f024164be880ca3bde8e51cc9b76dbf",
            "title": "NeMeSys - Energy Adaptive Graph Pattern Matching on NUMA-based Multiprocessor Systems",
            "abstract": ": NeMeSys is a NUMA-aware graph pattern processing engine, which leverages intelligent resource management for energy adaptive processing. With modern server systems incorporating an increasing amount of main memory, we can store graphs and compute analytical graph algorithms like graph pattern matching completely in-memory. Such server systems usually contain several powerful multiprocessors, which come with a high demand for energy. We demonstrate, that graph patterns can be processed in given performance constraints while saving energy, which would be wasted without proper controlling.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9187969",
                    "name": "Alexander Krause"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "c7e2952d91f5d7fbc2e800b5e8c10e2b9a1b5ac5",
            "title": "DECO: A Dataset of Annotated Spreadsheets for Layout and Table Recognition",
            "abstract": "This paper presents DECO (Dresden Enron COrpus), a dataset of spreadsheet files, annotated on the basis of layout and contents. It comprises of 1,165 files, extracted from the Enron corpus. Three different annotators (judges) assigned layout roles (e.g., Header, Data, and Notes) to non-empty cells and marked the borders of tables. Files that do not contain tables were flagged using categories such as Template, Form, and Report. Subsequently, a thorough analysis is performed to uncover the characteristics of the overall dataset and specific annotations. The results are discussed in this paper, providing several takeaways for future works. Furthermore, this work describes in detail the annotation methodology, going through the individual steps. The dataset, methodology, and tools are made publicly available, so that they can be adopted for further studies. DECO is available at: https://wwwdb.inf.tu-dresden.de/research-projects/deexcelarator/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1757263",
                    "name": "Elvis Koci"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "1491425973",
                    "name": "Josephine Rehak"
                },
                {
                    "authorId": "144149891",
                    "name": "Oscar Romero"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "d067849111f53293750193bee3a48adf4b4571e1",
            "title": "High-Throughput BitPacking Compression",
            "abstract": "To efficiently support analytical applications from a data management perspective, in-memory column store database systems are state-of-the art. In this kind of database system, lossless lightweight integer compression schemes are crucial to keep the memory storage as low as possible and to speedup query processing. In this specific compression domain, BitPacking is one of the most frequently applied compression scheme. However, (de) compression should not come with any additional cost during run time, but should be provided transparently without compromising the overall system performance. To achieve that, we focus on acceleration of BitPacking using Field Programmable Gate Arrays (FPGAs). Therefore, we outline several FPGA designs for BitPacking in this paper. As we are going to show in our evaluation, our specific designs provide the BitPacking compression scheme with high-throughput.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1851810",
                    "name": "N. J. Lisa"
                },
                {
                    "authorId": "123233140",
                    "name": "T. D. A. Nguyen"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "143906106",
                    "name": "Akash Kumar"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "d3fa2bf700fabe308acae987f78a55bd959b804c",
            "title": "From a Comprehensive Experimental Survey to a Cost-based Selection Strategy for Lightweight Integer Compression Algorithms",
            "abstract": "Lightweight integer compression algorithms are frequently applied in in-memory database systems to tackle the growing gap between processor speed and main memory bandwidth. In recent years, the vectorization of basic techniques such as delta coding and null suppression has considerably enlarged the corpus of available algorithms. As a result, today there is a large number of algorithms to choose from, while different algorithms are tailored to different data characteristics. However, a comparative evaluation of these algorithms with different data and hardware characteristics has never been sufficiently conducted in the literature. To close this gap, we conducted an exhaustive experimental survey by evaluating several state-of-the-art lightweight integer compression algorithms as well as cascades of basic techniques. We systematically investigated the influence of data as well as hardware properties on the performance and the compression rates. The evaluated algorithms are based on publicly available implementations as well as our own vectorized reimplementations. We summarize our experimental findings leading to several new insights and to the conclusion that there is no single-best algorithm. Moreover, in this article, we also introduce and evaluate a novel cost model for the selection of a suitable lightweight integer compression algorithm for a given dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "27226616",
                    "name": "Juliana Hildebrandt"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "e09e4d81ba38de21cc7e7e9a3133bc38b132408c",
            "title": "Cardinality estimation with local deep learning models",
            "abstract": "Cardinality estimation is a fundamental task in database query processing and optimization. Unfortunately, the accuracy of traditional estimation techniques is poor resulting in non-optimal query execution plans. With the recent expansion of machine learning into the field of data management, there is the general notion that data analysis, especially neural networks, can lead to better estimation accuracy. Up to now, all proposed neural network approaches for the cardinality estimation follow a global approach considering the whole database schema at once. These global models are prone to sparse data at training leading to misestimates for queries which were not represented in the sample space used for generating training queries. To overcome this issue, we introduce a novel local-oriented approach in this paper, therefore the local context is a specific sub-part of the schema. As we will show, this leads to better representation of data correlation and thus better estimation accuracy. Compared to global approaches, our novel approach achieves an improvement by two orders of magnitude in accuracy and by a factor of four in training time performance for local models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41065728",
                    "name": "Lucas Woltmann"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "e69fcf24e2f3cdebf2f9df7b9b16da2e4e3948b4",
            "title": "Efficient Query Processing for Dynamically Changing Datasets",
            "abstract": "The ability to efficiently analyze changing data is a key requirement of many real-time analytics applications. Traditional approaches to this problem were developed around the notion of Incremental View Maintenance (IVM), and are based either on the materialization of subresults (to avoid their recomputation) or on the recomputation of subresults (to avoid the space overhead of materialization). Both techniques are suboptimal: instead of materializing results and subresults, one may also maintain a data structure that supports efficient maintenance under updates and from which the full query result can quickly be enumerated. In two previous articles, we have presented algorithms for dynamically evaluating queries that are easy to implement, efficient, and can be naturally extended to evaluate queries from a wide range of application domains. In this paper, we discuss our algorithm and its complexity, explaining the main components behind its efficiency. Finally, we show experiments that compare our algorithm to a state-of-the-art (Higher-order) IVM engine, as well as to a prominent complex event recognition engine. Our approach outperforms the competitor systems by up to two orders of magnitude in processing time, and one order in memory consumption.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059322882",
                    "name": "Muhammad Idris"
                },
                {
                    "authorId": "144739409",
                    "name": "M. Ugarte"
                },
                {
                    "authorId": "1709642",
                    "name": "Stijn Vansummeren"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "fcb2ca153101d53c9f5f6a3269bb4f567b5d1b97",
            "title": "Graph Traversals for Regular Path Queries",
            "abstract": "Regular Path Queries (RPQs) are at the core of many recent declarative graph pattern matching languages. They leverage the compactness and expressiveness of regular expressions for matching recursive path structures. Unfortunately, most prior works on RPQs only consider breadth-first search as traversal strategy, neglecting other possible graph traversals like depth-first search or a combination of both. Within this paper, we conduct an analysis of graph traversals for RPQs by introducing a generalized graph traversal frame-work subsuming breadth-first search and depth-first search as extreme cases and thus opening up a new design space for graph traversals algorithms. We outline the underlying principles as well as provide comprehensive experimental evaluation using implementations which yield beneficial results regarding evaluation time and peak memory consumption.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "15152499",
                    "name": "Frank Tetzel"
                },
                {
                    "authorId": "1951237",
                    "name": "Romans Kasperovics"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "30fa2b09a4a459f315703a838a7460fe70f9af5d",
            "title": "Intermediate Results Materialization Selection and Format for Data-Intensive Flows",
            "abstract": ". Data-intensive \ufb02ows deploy a variety of complex data transformations to build information pipelines from data sources to different end users. As data are processed, these work\ufb02ows generate large intermediate results, typically pipelined from one operator to the following ones. Materializing intermediate results, shared among multiple \ufb02ows, brings bene\ufb01ts not only in terms of performance but also in resource usage and consistency. Similar ideas have been proposed in the context of data warehouses, which are studied under the materialized view selection problem. With the rise of Big Data systems, new challenges emerge due to new quality metrics captured by service level agreements which must be taken into account. Moreover, the way such results are stored must be reconsidered, as different data layouts can be used to reduce the I/O cost. In this paper, we propose a novel approach for automatic selection of multi-objective materialization of intermediate results in data-intensive \ufb02ows, which can tackle multiple and con\ufb02icting quality objectives. In addition, our approach chooses the optimal storage data format for selected materialized intermediate results based on subsequent access patterns. The experimental results show that our approach provides 40% better average speedup with respect to the current state-of-the-art, as well as an improvement on disk access time of 18% as compared to \ufb01xed format solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240624",
                    "name": "Rana Faisal Munir"
                },
                {
                    "authorId": "36761081",
                    "name": "S. Nadal"
                },
                {
                    "authorId": "144149891",
                    "name": "Oscar Romero"
                },
                {
                    "authorId": "48843545",
                    "name": "A. Abell\u00f3"
                },
                {
                    "authorId": "47378028",
                    "name": "P. Jovanovic"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "3b215a0dd571bb689598c02914af649394e18030",
            "title": "Analysis of Data Structures Involved in RPQ Evaluation",
            "abstract": "A fundamental ingredient of declarative graph query languages are regular path queries (RPQs). They provide an expressive yet compact way to match long and complex paths in a data graph by utilizing regular expressions. In this paper, we systematically explore and analyze the design space for the data structures involved in automaton-based RPQ evaluation. We consider three fundamental data structures used during RPQ processing: adjacency lists for quick neighborhood exploration, visited data structure for cycle detection, and the representation of intermediate results. We conduct an extensive experimental evaluation on realistic graph data sets and systematically investigate various alternative data structure representations and implementation variants. We show that carefully crafted data structures which exploit the access pattern of RPQs lead to reduced peak memory consumption and evaluation time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "15152499",
                    "name": "Frank Tetzel"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "2587068",
                    "name": "M. Paradies"
                },
                {
                    "authorId": "1951237",
                    "name": "Romans Kasperovics"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "41a8a600c0a08d0f58024f9c90eea251a21bfe96",
            "title": "AHEAD: Adaptable Data Hardening for On-the-Fly Hardware Error Detection during Database Query Processing",
            "abstract": "We have already known for a long time that hardware components are not perfect and soft errors in terms of single bit flips happen all the time. Up to now, these single bit flips are mainly addressed in hardware using general-purpose protection techniques. However, recent studies have shown that all future hardware components become less and less reliable in total and multi-bit flips are occurring regularly rather than exceptionally. Additionally, hardware aging effects will lead to error models that change during run-time. Scaling hardware-based protection techniques to cover changing multi-bit flips is possible, but this introduces large performance, chip area, and power overheads, which will become non-affordable in the future. To tackle that, an emerging research direction is employing protection techniques in higher software layers like compilers or applications. The available knowledge at these layers can be efficiently used to specialize and adapt protection techniques. Thus, we propose a novel adaptable and on-the-fly hardware error detection approach called AHEAD for database systems in this paper. AHEAD provides configurable error detection in an end-to-end fashion and reduces the overhead (storage and computation) compared to other techniques at this level. Our approach uses an arithmetic error coding technique which allows query processing to completely work on hardened data on the one hand. On the other hand, this enables on-the-fly detection during query processing of (i) errors that modify data stored in memory or transferred on an interconnect and (ii) errors induced during computations. Our exhaustive evaluation clearly shows the benefits of our AHEAD approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2881245",
                    "name": "Till Kolditz"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2055596828",
                    "name": "Matthias Werner"
                },
                {
                    "authorId": "46236813",
                    "name": "Stefan T. J. de Bruijn"
                }
            ]
        },
        {
            "paperId": "45f89f79620384a7de0cfb80504f21a92348b950",
            "title": "Column Scan Optimization by Increasing Intra-Instruction Parallelism",
            "abstract": "The key objective of database systems is to reliably manage data, whereby high query throughput and low query latency are core requirements. To satisfy these requirements for analytical query workloads, in-memory column store database systems are state-of-the-art. In these systems, relational tables are organized by column rather than by row, so that a full column scan is a fundamental key operation and thus, the optimization of the key operation is very crucial. For this reason, we investigated the optimization of a well-known scan technique using SIMD (Single Instruction Multiple Data) vectorization as well as using Field Programmable Gate Arrays (FPGA). In this paper, we present both optimization approaches with the goal to increase the intra-instruction execution parallelism to process more columns values in a single instruction simultaneously. For both, we present selective results of our exhaustive evaluation. Based on this evaluation, we draw some lessons learned for our ongoing research activities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1851810",
                    "name": "N. J. Lisa"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "123233140",
                    "name": "T. D. A. Nguyen"
                },
                {
                    "authorId": "143906106",
                    "name": "Akash Kumar"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "64cf01935eb034653601871e5113d212abecb1bb",
            "title": "Seamless Database Evolution for Cloud Applications",
            "abstract": "We present DECA (Database Evolution in Cloud Applications), a framework that facilitates fast, robust, and agile database evolution in cloud applications. Successful business requires to react to changing wishes and requirements of customers instantly. In today\u2019s realities, this often boils down to adding new features or fixing bugs in a software system. We focus on cloud application platforms that allow seamless development and deployment of applications by operating both the old and the new version in parallel for the time of development/deployment. This does not work if a common database is involved, since they cannot co-exist in multiple versions. To overcome this limitation, we apply current advances in the field of agile database evolution. DECA equips developers with an intuitive Database Evolution Language to create a new co-existing schema version for development and testing. Meanwhile, users can continuously use the old version. With the click of a button, we migrate the database to the new version and move all the users without unpredictable downtime and without the risk of corrupting our data. So, DECA speeds up the evolution of information systems to the pace of modern business.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51226730",
                    "name": "A. Mohapatra"
                },
                {
                    "authorId": "40466466",
                    "name": "K. Herrmann"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "153887725",
                    "name": "Simon L\u00fcders"
                },
                {
                    "authorId": "16566674",
                    "name": "Tsvetan Tsokov"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "6c6ae6c9d013e97caf452a285a47d510dc3cbb35",
            "title": "Feature-based comparison and generation of time series",
            "abstract": "For more than three decades, researchers have been developping generation methods for the weather, energy, and economic domain. These methods provide generated datasets for reasons like system evaluation and data availability. However, despite the variety of approaches, there is no comparative and cross-domain assessment of generation methods and their expressiveness. We present a similarity measure that analyzes generation methods regarding general time series features. By this means, users can compare generation methods and validate whether a generated dataset is considered similar to a given dataset. Moreover, we propose a feature-based generation method that evolves cross-domain time series datasets. This method outperforms other generation methods regarding the feature-based similarity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30389017",
                    "name": "Lars Kegel"
                },
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "72babebcd6e61988d6caa1fb2e98a26a642862d3",
            "title": "Table Recognition in Spreadsheets via a Graph Representation",
            "abstract": "Spreadsheet software are very popular data management tools. Their ease of use and abundant functionalities equip novices and professionals alike with the means to generate, transform, analyze, and visualize data. As a result, spreadsheets are a great resource of factual and structured information. This accentuates the need to automatically understand and extract their contents. In this paper, we present a novel approach for recognizing tables in spreadsheets. Having inferred the layout role of the individual cells, we build layout regions. We encode the spatial interrelations between these regions using a graph representation. Based on this, we propose Remove and Conquer (RAC), an algorithm for table recognition that implements a list of carefully curated rules. An extensive experimental evaluation shows that our approach is viable. We achieve significant accuracy in a dataset of real spreadsheets from various domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1757263",
                    "name": "Elvis Koci"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "144149891",
                    "name": "Oscar Romero"
                }
            ]
        },
        {
            "paperId": "757d72ef41da2705eb38605c8361bee790ad8932",
            "title": "A Cost-based Storage Format Selector for Materialization in Big Data Frameworks",
            "abstract": "Modern big data frameworks (such as Hadoop and Spark) allow multiple users to do large-scale analysis simultaneously. Typically, users deploy Data-Intensive Workflows (DIWs) for their analytical tasks. These DIWs of different users share many common parts (i.e, 50-80%), which can be materialized to reuse them in future executions. The materialization improves the overall processing time of DIWs and also saves computational resources. Current solutions for materialization store data on Distributed File Systems (DFS) by using a fixed data format. However, a fixed choice might not be the optimal one for every situation. For example, it is well-known that different data fragmentation strategies (i.e., horizontal, vertical or hybrid) behave better or worse according to the access patterns of the subsequent operations. \nIn this paper, we present a cost-based approach which helps deciding the most appropriate storage format in every situation. A generic cost-based storage format selector framework considering the three fragmentation strategies is presented. Then, we use our framework to instantiate cost models for specific Hadoop data formats (namely SequenceFile, Avro and Parquet), and test it with realistic use cases. Our solution gives on average 33% speedup over SequenceFile, 11% speedup over Avro, 32% speedup over Parquet, and overall, it provides upto 25% performance gain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240624",
                    "name": "Rana Faisal Munir"
                },
                {
                    "authorId": "48843545",
                    "name": "A. Abell\u00f3"
                },
                {
                    "authorId": "144149891",
                    "name": "Oscar Romero"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "7718bbba237c1164cce73f5b08549e354ec34848",
            "title": "Column Scan Acceleration in Hybrid CPU-FPGA Systems",
            "abstract": "Nowadays, in-memory column store database systems are state-of-the-art for analytical workloads. In these column stores, a full column scan is a fundamental key operation and thus, the optimization of this primitive is very crucial from a performance perspective. For this optimization, advances in hardware are always an interesting opportunity, but represent also a major challenge. At the moment, hardware systems are more and more changing from homogeneous CPU systems towards hybrid systems with di\ufb00erent computing units. Based on that, we focus on column scan acceleration for hybrid hardware systems incorporating a Field Programmable Gate Array (FPGA) and a CPU into a single system in this paper. The advantage of those hybrid systems is that the FPGA has usually direct access to the main memory of the CPU avoiding data copy which is a necessary procedure in other hybrid systems like CPU-GPU architectures. Thus, we present several FPGA designs for a recent column scan technique to fully o\ufb04oad the scan operation to the FPGA. In detail, we present our basic FPGA de-sign and di\ufb00erent optimization techniques. Then, we present selective results of our exhaustive evaluation showing the bene\ufb01t of our FPGA acceleration. As we are going to show, we achieve a maximum speedup of factor 7 compared to a single-threaded CPU scan execution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1851810",
                    "name": "N. J. Lisa"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "123233140",
                    "name": "T. D. A. Nguyen"
                },
                {
                    "authorId": "143906106",
                    "name": "Akash Kumar"
                }
            ]
        },
        {
            "paperId": "8955c1062c5db19426b92340ffe6f6c1d16a8eef",
            "title": "Modeling Customers and Products with Word Embeddings from Receipt Data",
            "abstract": "For many tasks in market research it is important to model customers and products as comparable instances. Usually, the integration of customers and products into one model is not trivial. In this paper, we will detail an approach for a combined vector space of customers and products based on word embeddings learned from receipt data. To highlight the strengths of this approach we propose four different applications: recommender systems, customer and product segmentation and purchase prediction. Experimental results on a real-world dataset with 200M order receipts for 2M customers show that our word embedding approach is promising and helps to improve the quality in these applications scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41065728",
                    "name": "Lucas Woltmann"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "8e2809c72d89a3476bdea29be4832f62c62dd582",
            "title": "A Hardware/Software Stack for Heterogeneous Systems",
            "abstract": "Plenty of novel emerging technologies are being proposed and evaluated today, mostly at the device and circuit levels. It is unclear what the impact of different new technologies at the system level will be. What is clear, however, is that new technologies will make their way into systems and will increase the already high complexity of heterogeneous parallel computing platforms, making it ever so difficult to program them. This paper discusses a programming stack for heterogeneous systems that combines and adapts well-understood principles from different areas, including capability-based operating systems, adaptive application runtimes, dataflow programming models, and model checking. We argue why we think that these principles built into the stack and the interfaces among the layers will also be applicable to future systems that integrate heterogeneous technologies. The programming stack is evaluated on a tiled heterogeneous multicore.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144195575",
                    "name": "J. Castrill\u00f3n"
                },
                {
                    "authorId": "30247428",
                    "name": "Matthias Lieber"
                },
                {
                    "authorId": "1840345",
                    "name": "Sascha Kl\u00fcppelholz"
                },
                {
                    "authorId": "1720944",
                    "name": "M. V\u00f6lp"
                },
                {
                    "authorId": "2495306",
                    "name": "Nils Asmussen"
                },
                {
                    "authorId": "143681675",
                    "name": "U. Assmann"
                },
                {
                    "authorId": "1710233",
                    "name": "F. Baader"
                },
                {
                    "authorId": "5403194",
                    "name": "C. Baier"
                },
                {
                    "authorId": "145570132",
                    "name": "G. Fettweis"
                },
                {
                    "authorId": "145284133",
                    "name": "J. Fr\u00f6hlich"
                },
                {
                    "authorId": "2132535685",
                    "name": "Andr\u00e9s Goens"
                },
                {
                    "authorId": "1605950053",
                    "name": "Sebastian Haas"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "1731688",
                    "name": "Hermann H\u00e4rtig"
                },
                {
                    "authorId": "144202399",
                    "name": "Mattis Hasler"
                },
                {
                    "authorId": "2070377",
                    "name": "I. Huismann"
                },
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "3307482",
                    "name": "Sven Karol"
                },
                {
                    "authorId": "143906106",
                    "name": "Akash Kumar"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2395105",
                    "name": "Linda Leuschner"
                },
                {
                    "authorId": "40404655",
                    "name": "Siqi Ling"
                },
                {
                    "authorId": "2735651",
                    "name": "Steffen M\u00e4rcker"
                },
                {
                    "authorId": "2008512095",
                    "name": "Christian Menard"
                },
                {
                    "authorId": "5747726",
                    "name": "Johannes Mey"
                },
                {
                    "authorId": "1781970",
                    "name": "W. Nagel"
                },
                {
                    "authorId": "51292635",
                    "name": "Benedikt N\u00f6then"
                },
                {
                    "authorId": "144138457",
                    "name": "R. Pe\u00f1aloza"
                },
                {
                    "authorId": "1862588",
                    "name": "Michael Raitza"
                },
                {
                    "authorId": "144760860",
                    "name": "J. Stiller"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "145650681",
                    "name": "A. Voigt"
                },
                {
                    "authorId": "36350092",
                    "name": "Sascha Wunderlich"
                }
            ]
        },
        {
            "paperId": "8e5596441f7e13ad9f0f9f482e58b8eefe492df6",
            "title": "Design of a Portable Programming Abstraction for Data Transformations",
            "abstract": "Novel data intensive applications and the diversification of data processing platforms have changed data management significantly over the last decade. In this changed environment, the expressiveness of the traditional relational algebra is often insufficient and data management systems have started to provide more powerful special purpose programming languages. However, these languages create a tight coupling between applications and specific systems that can hinder further development on both sides of the equation. The goal of this article is to start a discussion on the future of platform independent programming models for data processing that re-establish the separation of application logic and implementation details that used to be a cornerstone of data management systems. As a guide for that discussion, we introduce several recent related works on that topic and also outline our own contribution, the Analytical Calculus.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3469702",
                    "name": "Johannes Luong"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "98622b7ef3babd993349fccc28b13b9e3cc090b5",
            "title": "Adaptive Energy-Control for In-Memory Database Systems",
            "abstract": "The ever-increasing demand for scalable database systems is limited by their energy consumption, which is one of the major challenges in research today. While existing approaches mainly focused on transaction-oriented disk-based database systems, we are investigating and optimizing the energy consumption and performance of data-oriented scale-up in-memory database systems that make heavy use of the main power consumers, which are processors and main memory. We give an in-depth energy analysis of a current mainstream server system and show that modern processors provide a rich set of energy-control features, but lack the capability of controlling them appropriately, because of missing application-specific knowledge. Thus, we propose the Energy-Control Loop (ECL) as an DBMS-integrated approach for adaptive energy-control on scale-up in-memory database systems that obeys a query latency limit as a soft constraint and actively optimizes energy efficiency and performance of the DBMS. The ECL relies on adaptive workload-dependent energy profiles that are continuously maintained at runtime. In our evaluation, we observed energy savings ranging from 20% to 40% for a real-world load profile.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "9921c0a98a18ba16be4eab3e162123d25e243a52",
            "title": "Reliable In-Memory Data Management on Unreliable Hardware",
            "abstract": "The key objective of database systems is to reliably manage data, whereby high query throughput and low query latency are core requirements. To satisfy these requirements, database systems constantly adapt to novel hardware features. Although it has been intensively studied and commonly accepted that hardware error rates in terms of bit flips increase dramatically with the decrease of the underlying chip structures, most database system research activities neglected this fact, leaving error (bit flip) detection as well as correction to the underlying hardware. Especially for memory, silent data corruption (SDC) as a result of transient bit flips leading to faulty data is mainly detected and corrected at the DRAM and memory-controller layer. However, since future hardware becomes less reliable and error detection as well as correction by hardware becomes more expensive, this free ride will come to an end in the near future. To further provide a reliable data management, an emerging research direction will be employing specific and tailored protection techniques at the database system level. Following that, we are currently developing and implementing an adopted system design for state-of-the-art in-memory column stores. In this position paper, we summarize our vision, the current state and outline future work of our research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2881245",
                    "name": "Till Kolditz"
                },
                {
                    "authorId": "27226616",
                    "name": "Juliana Hildebrandt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "9a2cfd7ff0b0f1e56e4337a633e0758798edd9ea",
            "title": "OLTPShare: The Case for Sharing in OLTP Workloads",
            "abstract": "In the past, resource sharing has been extensively studied for OLAP workloads. Naturally, the question arises, why studies mainly focus on OLAP and not on OLTP workloads? At first sight, OLTP queries - due to their short runtime - may not have enough potential for the additional overhead. In addition, OLTP workloads do not only execute read operations but also updates. In this paper, we address query sharing for OLTP workloads. We first analyze the sharing potential in real-world OLTP workloads. Based on those findings, we then present an execution strategy, called OLTPShare that implements a novel batching scheme for OLTP workloads. We analyze the sharing benefits by integrating OLTPShare into a prototype version of the commercial database system SAP HANA. Our results show for different OLTP workloads that OLTPShare enables SAP HANA to provide a significant throughput increase in high-load scenarios compared to the conventional execution strategy without sharing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2568755",
                    "name": "Robin Rehrmann"
                },
                {
                    "authorId": "2691974",
                    "name": "Carsten Binnig"
                },
                {
                    "authorId": "144879374",
                    "name": "Alexander B\u00f6hm"
                },
                {
                    "authorId": "94622503",
                    "name": "Kihong Kim"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "145799243",
                    "name": "Amr Rizk"
                }
            ]
        },
        {
            "paperId": "a84238d867ddf33282393fda387d94ab5b5a02f9",
            "title": "Energy-Utility Function-Based Resource Control for In-Memory Database Systems LIVE",
            "abstract": "The ever-increasing demand for scalable database systems is limited by their energy consumption, which is one of the major challenges in research today. While existing approaches mainly focused on transaction-oriented disk-based database systems, we are investigating and optimizing the energy consumption and performance of data-oriented scale-up in-memory database systems that make heavy use of the main power consumers, which are processors and main memory. In this demo, we present energy-utility functions as an approach for enabling the operating system to improve the energy efficiency of scalable in-memory database systems. Our highly interactive demo setup mainly allows attendees to switch between multiple DBMS workloads and watch in detail how the system responds by adapting the hardware configuration appropriately.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "33601888",
                    "name": "Marcus H\u00e4hnel"
                },
                {
                    "authorId": "39348488",
                    "name": "Till Smejkal"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "1731688",
                    "name": "Hermann H\u00e4rtig"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "ac0db1a1ee1a2f88f4d90659190b675239a5883a",
            "title": "Beyond Straightforward Vectorization of Lightweight Data Compression Algorithms for Larger Vector Sizes",
            "abstract": "Data as well as hardware characteristics are two key aspects for efficient data management. This holds in particular for the field of in-memory data processing. Aside from increasing main memory capacities, efficient in-memory processing benefits from novel processing concepts based on lightweight compressed data. Thus, an active research field deals with the adaptation of new hardware features such as vectorization using SIMD instructions to speedup lightweight data compression algorithms. Most of the vectorized implementations have been proposed for 128-bit vector registers. A straightforward transformation to wider vector sizes is possible. However, this straightforward way does not exploit the capabilities of newer SIMD extensions to the maximum extent as we will show in this paper. On the one hand, we present a novel implementation concept for run-length encoding using conflict-detection operations which have been introduced in Intel\u2019s AVX-512 SIMD extension. On the other hand, we investigate different data layouts for vectorization and their impact on wider vector sizes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33232107",
                    "name": "Johannes Pietrzyk"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "cc79e529e32feedabafa403213f16295fecfdbde",
            "title": "Make Larger Vector Register Sizes New Challenges?: Lessons Learned from the Area of Vectorized Lightweight Compression Algorithms",
            "abstract": "The exploitation of data as well as hardware properties is a core aspect for efficient data management. This holds in particular for the field of in-memory data processing. Aside from increasing main memory capacities, in-memory data processing also benefits from novel processing concepts based on lightweight compressed data. To speed up compression as well as decompression, an active research field deals with the specialization of these algorithms to hardware features such as vectorization using SIMD instructions. Most of the vectorized implementations have been proposed for 128 bit vector registers. However, hardware vendors still increase the vector register sizes, whereby a straightforward transformation to these wider vector sizes is possible in most-cases. Thus, we systematically investigated the impact of different SIMD instruction set extensions with wider vector sizes on the behavior of straightforward transformed implementations. In this paper, we will describe our evaluation methodology and present selective results of our exhaustive evaluation. In particular, we will highlight some challenges and present first approaches to tackle them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "dd3e116a97ed6afaa90efdb5db4ab82342d682f3",
            "title": "Conflict Detection-Based Run-Length Encoding - AVX-512 CD Instruction Set in Action",
            "abstract": "Data as well as hardware characteristics are two key aspects for efficient data management. This holds in particular for the field of in-memory data processing. Aside from increasing main memory capacities, efficient in-memory processing benefits from novel processing concepts based on lightweight compressed data. Thus, an active research field deals with the adaptation of new hardware features such as vectorization using SIMD instructions to speedup lightweight data compression algorithms. Following this trend, we propose a novel approach for run-length encoding, a well-known and often applied lightweight compression technique. Our novel approach is based on newly introduced conflict detection (CD) instructions in Intel's AVX-512 instruction set extension. As we are going to show, our CD-based approach has unique properties and outperforms the state-of-the-art RLE approach for data sets with small run lengths.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "33232107",
                    "name": "Johannes Pietrzyk"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "efbde7aeccd0777537663a775c0fe9a02acae736",
            "title": "Conjunctive Queries with Inequalities Under Updates",
            "abstract": "Modern application domains such as Composite Event Recognition (CER) and real-time Analytics require the ability to dynamically refresh query results under high update rates. Traditional approaches to this problem are based either on the materialization of subresults (to avoid their recomputation) or on the recomputation of subresults (to avoid the space overhead of materialization). Both techniques have recently been shown suboptimal: instead of materializing results and subresults, one can maintain a data structure that supports efficient maintenance under updates and can quickly enumerate the full query output, as well as the changes produced under single updates. Unfortunately, these data structures have been developed only for aggregate-join queries composed of equi-joins, limiting their applicability in domains such as CER where temporal joins are commonplace. In this paper, we present a new approach for dynamically evaluating queries with multi-way \u03b8-joins under updates that is effective in avoiding both materialization and recomputation of results, while supporting a wide range of applications. To do this we generalize Dynamic Yannakakis, an algorithm for dynamically processing acyclic equi-join queries. In tandem, and of independent interest, we generalize the notions of acyclicity and free-connexity to arbitrary \u03b8-joins. We instantiate our framework to the case where \u03b8-joins are only composed of equalities and inequalities ( , \u2265) and experimentally compare this algorithm, called IEDyn, to state of the art CER systems as well as incremental view maintenance engines. IEDyn performs consistently better than the competitor systems with up to two orders of magnitude improvements in both time and memory consumption.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059322882",
                    "name": "Muhammad Idris"
                },
                {
                    "authorId": "144739409",
                    "name": "M. Ugarte"
                },
                {
                    "authorId": "1709642",
                    "name": "Stijn Vansummeren"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "f25cc72178d2d202a8d23cf4e68151a4abdb7262",
            "title": "Lower Bound-oriented Parameter Calculation for AN Coding",
            "abstract": "The hardware as well as software communities have recently experienced a shift towards mitigating bit flips issues in software, rather than completely mitigating only in hardware. For this software error mitigation, arithmetic error coding schemes like AN coding are increasingly applied because arithmetic operations can be directly executed without decoding and bit flip detection is provided in an end-to-end fashion. In this case, each encoded data word is computed by multiplying the original data word with a constant integer value A. To reliably detect b bit flips in each code word, the value A has to be well-chosen, so that a minimum Hamming distance of b + 1 can be guaranteed. However, the value A depends on the data word length as well as on the desired minimum Hamming distance. Up to now, a very expensive brute force approach for computation of the value for A is applied. To tackle that in a more efficient way, we present a lower bound-oriented approach for this calculation in this paper.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27226616",
                    "name": "Juliana Hildebrandt"
                },
                {
                    "authorId": "2881245",
                    "name": "Till Kolditz"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "f360c4439de50822810bc1ceaf3ff4b65476efe7",
            "title": "Teaching In-Memory Database Systems the Detection of Hardware Errors",
            "abstract": "The key objective of database systems is to reliably manage data, whereby high query throughput and low query latency are core requirements. To satisfy these requirements, database systems constantly adapt to novel hardware features. Although it has been intensively studied and commonly accepted that hardware error rates in terms of bit flips increase dramatically with the decrease of the underlying chip structures, most database system research activities neglected this fact, leaving error (bit flip) detection as well as correction to the underlying hardware. Especially for main memory, silent data corruption (SDC) as a result of transient bit flips leading to faulty data is mainly detected and corrected at the DRAM and memory-controller layer. However, since future hardware becomes less reliable and error detection as well as correction by hardware becomes more expensive, this free ride will come to an end in the near future. To further provide a reliable data management, an emerging research direction is employing specific and tailored protection techniques at the database system level. Following that, we are currently developing and implementing an adopted system design for state-of-the-art in-memory column stores. In our lightning talk, we will summarize our current state and outline future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2881245",
                    "name": "Till Kolditz"
                }
            ]
        },
        {
            "paperId": "0f10a89d56453377786632b26b609b436d5f7900",
            "title": "Memory Management Techniques for Large-Scale Persistent-Main-Memory Systems",
            "abstract": "Storage Class Memory (SCM) is a novel class of memory technologies that promise to revolutionize database architectures. SCM is byte-addressable and exhibits latencies similar to those of DRAM, while being non-volatile. Hence, SCM could replace both main memory and storage, enabling a novel single-level database architecture without the traditional I/O bottleneck. Fail-safe persistent SCM allocation can be considered conditio sine qua non for enabling this novel architecture paradigm for database management systems. In this paper we present PAllocator, a fail-safe persistent SCM allocator whose design emphasizes high concurrency and capacity scalability. Contrary to previous works, PAllocator thoroughly addresses the important challenge of persistent memory fragmentation by implementing an efficient defragmentation algorithm. We show that PAllocator outperforms state-of-the-art persistent allocators by up to one order of magnitude, both in operation throughput and recovery time, and enables up to 2.39x higher operation throughput on a persistent B-Tree.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2742360",
                    "name": "Ismail Oukid"
                },
                {
                    "authorId": "2077127667",
                    "name": "Daniel Booss"
                },
                {
                    "authorId": "3410409",
                    "name": "Adrien Lespinasse"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "3250632",
                    "name": "Thomas Willhalm"
                },
                {
                    "authorId": "25021579",
                    "name": "G. Gomes"
                }
            ]
        },
        {
            "paperId": "14b9b036dd56e4798b3215d1936842a19bb8d541",
            "title": "Feature-driven Time Series Generation",
            "abstract": "Time series data are an ubiquitous and important data source in many domains. Most companies and organizations rely on this data for critical tasks like decision-making, planning, and analytics in general. Usually, all these tasks focus on actual data representing organization and business processes. In order to assess the robustness of current systems and methods, it is also desirable to focus on time-series scenarios which represent speci\ufb01c time-series features. This work presents a generally applicable and easy-to-use method for the feature-driven generation of time series data. Our approach extracts descriptive features of a data set and allows the construction of a speci\ufb01c version by means of the modi-\ufb01cation of these features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30389017",
                    "name": "Lars Kegel"
                },
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "188c54098e36be073bc6bce540251f95ad4e6123",
            "title": "Hardware-Accelerated Memory Operations on Large-Scale NUMA Systems",
            "abstract": "As NUMA systems grow in complexity, the average distance of memory increases. Because the number of parallel read requests is limited, this reduces the bandwidth available to the CPUs. Also, the cost of cache coherency increases as atomic operations are synced across multiple hops. We explore how these problems can be alleviated by offloading NUMA accesses to the interconnect hardware and show how databases can profit. For cross-NUMA table scans, we report a performance improvement of up to 30%; for atomic increments as used for transaction sequencing up to 10x, and for latches up to 8x. These experiments were performed on an SGI UV300 system but demonstrate the general value of explicit memory instructions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2378601",
                    "name": "Markus Dreseler"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "3467398",
                    "name": "Timo Dj\u00fcrken"
                },
                {
                    "authorId": "46174428",
                    "name": "Eric L\u00fcbke"
                },
                {
                    "authorId": "3151537",
                    "name": "M. Uflacker"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "1810943",
                    "name": "H. Plattner"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "1b0734d32328c49b5961116842b62ab99b1a0dc9",
            "title": "Energy Elasticity on Heterogeneous Hardware using Adaptive Resource Reconfiguration",
            "abstract": "Energy awareness of database systems has emerged as a critical research topic, because energy consumption is becoming a major factor. Recent energy-related hardware developments tend towards offering more and more configuration opportunities for the software to control its own energy-based behavior. Existing research within the DB community so far mainly focused on leveraging this configuration spectrum to identify the most energy-efficient configuration for specific operators or entire queries. In [Un16], we introduced the concept of energy elasticity and proposed the energy-control loop as an implementation of this concept. Energy elasticity refers to the ability of software to behave energy-proportional and energy-efficient at the same time while maintaining a certain quality of service.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "1414019323",
                    "name": "Willi-Wolfram Mentzel"
                },
                {
                    "authorId": "9539656",
                    "name": "Eric Mier"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "1fb45fcf0ca7151840c1b63bfc0840c5dc9a8d6d",
            "title": "Overview on Hardware Optimizations for Database Engines",
            "abstract": ": The key objective of database systems is to e \ufffd ciently manage an always increasing amount of data. Thereby, a high query throughput and a low query latency are core requirements. To satisfy these requirements, database engines are highly adapted to the given hardware by using all features of modern processors. Apart from this software optimization, even tailor-made processing circuits running on FGPAs are built to run mostly stateless query plans with a high throughput. A similar approach, which was already investigated three decades ago, is to build customized hardware like a database processor. Tailor-made hardware allows to achieve performance numbers that cannot be reached with software running on general-purpose CPUs, while at the same time, addressing the dark silicon problem. The main disadvantage of custom hardware is the high development cost that comes with designing and verifying a new processor, as well as building respective drivers and the software stack. However, there is actually no need to build a fully-\ufb02edged processor from scratch. In this paper, we present our conducted as well as our ongoing research e \ufffd orts in the direction of customizing hardware for databases. In detail, we illustrate the potential of instruction set extensions of processors as well as of optimizing memory access by o \ufffd oading logic to the main memory controller.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "1605950053",
                    "name": "Sebastian Haas"
                },
                {
                    "authorId": "9539656",
                    "name": "Eric Mier"
                },
                {
                    "authorId": "145570132",
                    "name": "G. Fettweis"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "26d00fe71ae527cebac04a68616dfeddb1f4c6e1",
            "title": "The Dresden Database Systems Group",
            "abstract": "The Dresden Database Systems Group focuses on the advancement of data management techniques from a system level as well as information management perspective. With more than 15 PhD students the research group is involved in a variety of larger research projects ranging from activities to exploit modern hardware for scalable storage engines to advancing statistical methods for large-scale time series management. The group is visible at an international level as well as actively involved in cooperations with national and regional research partners",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "3b0143befcc8a02cb4745f47260402d2ff47843c",
            "title": "AL: unified analytics in domain specific terms",
            "abstract": "Data driven organizations gather information on various aspects of their endeavours and analyze that information to gain valuable insights or to increase automatization. Today, these organizations can choose from a wealth of specialized analytical libraries and platforms to meet their functional and non-functional requirements. Indeed, many common application scenarios involve the combination of multiple such libraries and platforms in order to provide a holistic perspective. Due to the scattered landscape of specialized analytical tools, this integration can result in complex and hard to evolve applications. In addition, the necessary movement of data between tools and formats can introduce a serious performance penalty. In this article we present a unified programming environment for analytical applications. The environment includes AL, a programming language that combines concepts of various common analytical domains. Further, the environment also includes a flexible compilation system that uses a language-, domain-, and platform independent program intermediate representation to separate high level application logic and physical organisation. We provide a detailed introduction of AL, establish our program intermediate representation as a generally useful abstraction, and give a detailed explanation of the translation of AL programs into workloads for our experimental shared-memory processing engine.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3469702",
                    "name": "Johannes Luong"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "3d367dc1595902ac7b4cdfe486d96af1bd64cbea",
            "title": "CSAR: The Cross-Sectional Autoregression Model",
            "abstract": "The forecasting of time series data is an integral component for management, planning, and decision making. Following the Big Data trend, large amounts of time series data are available in many application domains. The highly dynamic and often noisy character of these domains in combination with the logistic problems of collecting data from a large number of data sources, imposes new requirements on the forecasting process. A constantly increasing number of time series has to be forecasted, preferably with low latency AND high accuracy. This is almost impossible, when keeping the traditional focus on creating one forecast model for each individual time series. In addition, often used forecasting approaches like ARIMA need complete historical data to train forecast models and fail if time series are intermittent. A method that addresses all these new requirements is the cross-sectional forecasting approach. It utilizes available data from many time series of the same domain in one single model, thus, missing values can be compensated and accurate forecast results can be calculated quickly. However, this approach is limited by a rigid training data selection and existing forecasting methods show that adaptability of the model to the data increases the forecast accuracy. Therefore, in this paper we present CSAR a model that extends the cross-sectional paradigm by adding more flexibility and allowing fine grained adaptations to the analyzed data. In this way, we achieve an increased forecast accuracy and thus a wider applicability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "564f2cdd6bc530a17da4149a1e032a5a6967986c",
            "title": "Lightweight Data Compression Algorithms: An Experimental Survey (Experiments and Analyses)",
            "abstract": "Lightweight data compression algorithms are frequently applied in in-memory database systems to tackle the growing gap between processor speed and main memory bandwidth. In recent years, the vectorization of basic techniques such as delta coding and null suppression has considerably enlarged the corpus of available algorithms. As a result, today there is a large number of algorithms to choose from, while different algorithms are tailored to different data characteristics. However, a comparative evaluation of these algorithms under different data characteristics has never been sufficiently conducted in the literature. To close this gap, we conducted an exhaustive experimental survey by evaluating several state-of-the-art compression algorithms as well as cascades of basic techniques. We systematically investigated the influence of the data properties on the performance and the compression rates. The evaluated algorithms are based on publicly available implementations as well as our own vectorized reimplementations. We summarize our experimental findings leading to several new insights and to the conclusion, that there is no single-best algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "27226616",
                    "name": "Juliana Hildebrandt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "57de9d74f82fbf718c086fad10a14e32c66c4891",
            "title": "Data Structure Engineering For Byte-Addressable Non-Volatile Memory",
            "abstract": "Storage Class Memory (SCM) is emerging as a viable alternative to traditional DRAM, alleviating its scalability limits, both in terms of capacity and energy consumption, while being non-volatile. Hence, SCM has the potential to become a universal memory, blurring well-known storage hierarchies. However, along with opportunities, SCM brings many challenges. In this tutorial we will dissect SCM challenges and provide an in-depth view of existing programming models that circumvent them, as well as novel data structures that stem from these models. We will also elaborate on fail-safety testing challenges -- an often overlooked, yet important topic. Finally, we will discuss SCM emulation techniques for end-to-end testing of SCM-based software components. In contrast to surveys investigating the use of SCM in database systems, this tutorial is designed as a programming guide for researchers and professionals interested in leveraging SCM in database systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2742360",
                    "name": "Ismail Oukid"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "5d60affb64ce43ff0c3b969845d88e3b850f5fc5",
            "title": "SAP HANA - The Evolution of an In-Memory DBMS from Pure OLAP Processing Towards Mixed Workloads",
            "abstract": "The journey of SAP HANA started as an in-memory appliance for complex, analytical applications. The success of the system quickly motivated SAP to broaden the scope from the OLAP workloads the system was initially architected for to also handle transactional workloads, in particular to support its Business Suite flagship product. In this paper, we highlight some of the core design changes to evolve an in-memory column store system towards handling OLTP workloads. We also discuss the challenges of running mixed workloads with low-latency OLTP queries and complex analytical queries in the context of the same database management system and give an outlook on the future database interaction patterns of modern business applications we see emerging currently.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1766564",
                    "name": "Norman May"
                },
                {
                    "authorId": "144879374",
                    "name": "Alexander B\u00f6hm"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "5db52c0fcd47c94c3ebd2363cf3312a248040bf2",
            "title": "Metamodeling Lightweight Data Compression Algorithms and its Application Scenarios",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27226616",
                    "name": "Juliana Hildebrandt"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "1820834438",
                    "name": "Thomas K\u00fchn"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "6be81623671f373da282e59e6cb02f264f589075",
            "title": "Insights into the Comparative Evaluation of Lightweight Data Compression Algorithms",
            "abstract": "Lightweight data compression is frequently applied in inmemory database systems to tackle the growing gap between processor speed and main memory bandwidth. In recent years, the number of available compression algorithms has grown considerably. Since the correct choice of one of these algorithms requires understanding of their performance behavior, we systematically evaluated several stateof-the-art compression algorithms on a multitude of different data characteristics. In this demonstration, the attendee will learn our findings in an interactive tour through our obtained measurements. The most important insight is that there is no single-best algorithm, but that the choice depends on the data characteristics and is non-trivial.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "27226616",
                    "name": "Juliana Hildebrandt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "86776eb5f4bfc2882ffd93d3f7162dbc97fcd932",
            "title": "Context Similarity for Retrieval-Based Imputation",
            "abstract": "Completeness as one of the four major dimensions of data quality is a pervasive issue in modern databases. Although data imputation has been studied extensively in the literature, most of the research is focused on inference-based approach. We propose to harness Web tables as an external data source to effectively and efficiently retrieve missing data while taking into account the inherent uncertainty and lack of veracity that they contain. Existing approaches mostly rely on standard retrieval techniques and out-of-the-box matching methods which result in a very low precision, especially when dealing with numerical data. We, therefore, propose a novel data imputation approach by applying numerical context similarity measures which results in a significant increase in the precision of the imputation procedure, by ensuring that the imputed values are of the same domain and magnitude as the local values, thus resulting in an accurate imputation. We use Dresden Web Table Corpus which is comprised of more than 125 million web tables extracted from the Common Crawl as our knowledge source. The comprehensive experimental results demonstrate that the proposed method well outperforms the default out-of-the-box retrieval approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49497567",
                    "name": "Ahmad Ahmadov"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1789686",
                    "name": "R. Wrembel"
                }
            ]
        },
        {
            "paperId": "9551541b0055461510986566a359830a41a1204c",
            "title": "Adaptive Recovery for SCM-Enabled Databases",
            "abstract": "Storage Class Memory (SCM) has the potential to drastically change the database system landscape \u2013 much like high core count CPUs and large DRAM capacities spurred a shift to in-memory databases a decade ago. One of the possibilities provided by SCM is to significantly improve restart performance. SCM-enabled databases can evince a single-level main-memory architecture that stores, accesses, and modifies data directly in SCM, removing the traditional recovery bottleneck for main-memory databases: reloading data from durable media to main memory. Almost instantaneous recovery is possible, but at the cost of reduced throughput as latency sensitive data structures such as indexes need to be kept on SCM whose read and write latencies are projected to be noticeably slower than those of DRAM. We can regain this throughput by fully storing secondary data structures in DRAM and rebuilding them after restart, concurrently with processing incoming requests. While these data structures are being rebuilt, request throughput will be reduced not only because of the rebuilding overhead but, more significantly, because their (temporary) absence will result in sub-optimal access plans. Hence, rebuilding DRAM-based data structures becomes the new bottleneck for SCM-enabled databases recovery. In this paper, we address this bottleneck and describe Adaptive Recovery, a novel recovery technique that significantly reduces the impact on request throughput during the recovery period over na\u0131\u0308ve approaches such as rebuilding data structures when first referenced.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2742360",
                    "name": "Ismail Oukid"
                },
                {
                    "authorId": "1700004",
                    "name": "A. Nica"
                },
                {
                    "authorId": "46181593",
                    "name": "D. Bossle"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2921307",
                    "name": "P. Bumbulis"
                },
                {
                    "authorId": "3250632",
                    "name": "Thomas Willhalm"
                }
            ]
        },
        {
            "paperId": "9c6bddcc4c04009abdafc4b3340868975552e1d1",
            "title": "An Analysis of the Feasibility of Graph Compression Techniques for Indexing Regular Path Queries",
            "abstract": "Regular path queries (RPQs) are a fundamental part of recent graph query languages like SPARQL and PGQL. They allow the definition of recursive path structures through regular expressions in a declarative pattern matching environment. We study the use of the K2-tree graph compression technique to materialize RPQ results with low memory consumption for indexing. Compact index representations enable the efficient storage of multiple indexes for varying RPQs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "15152499",
                    "name": "Frank Tetzel"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "2587068",
                    "name": "M. Paradies"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "aef49c955060e0f4f9a288b23c42789c80aa937d",
            "title": "Generating What-If Scenarios for Time Series Data",
            "abstract": "Time series data has become a ubiquitous and important data source in many application domains. Most companies and organizations strongly rely on this data for critical tasks like decision-making, planning, predictions, and analytics in general. While all these tasks generally focus on actual data representing organization and business processes, it is also desirable to apply them to alternative scenarios in order to prepare for developments that diverge from expectations or assess the robustness of current strategies. When it comes to the construction of such what-if scenarios, existing tools either focus on scalar data or they address highly specific scenarios. In this work, we propose a generally applicable and easy-to-use method for the generation of what-if scenarios on time series data. Our approach extracts descriptive features of a data set and allows the construction of an alternate version by means of filtering and modification of these features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30389017",
                    "name": "Lars Kegel"
                },
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "bba4d39eddaa225d7f1584b4786c3cd31f9dede2",
            "title": "The Data Center under your Desk - How Disruptive is Modern Hardware for DB System Design?",
            "abstract": "While we are already used to see more than 1,000 cores within a single machine, the next processing platforms for database engines will be heterogeneous with built-in GPU-style processors as well as specialized FPGAs or chips with domain-specific instruction sets. Moreover, the traditional volatile as well as the upcoming non-volatile RAM with capacities in the 100s of TBytes per machine will provide great opportunities for storage engines but also call for radical changes on the architecture of such systems. Finally, the emergence of economically affordable, high-speed/low-latency interconnects as a basis for rack-scale computing is questioning long-standing folklore algorithmic assumptions but will certainly play an important role in the big picture of building modern data management platforms. In this talk, we will try to classify and review existing approaches from a performance, robustness, as well as energy efficiency perspective and pinpoint interesting starting points for further research activities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "bf52eff9b323629d0274cc8ecdb597fb43c15865",
            "title": "Big data causing big (TLB) problems: taming random memory accesses on the GPU",
            "abstract": "GPUs are increasingly adopted for large-scale database processing, where data accesses represent the major part of the computation. If the data accesses are irregular, like hash table accesses or random sampling, the GPU performance can suffer. Especially when scaling such accesses beyond 2GB of data, a performance decrease of an order of magnitude is encountered. This paper analyzes the source of the slowdown through extensive micro-benchmarking, attributing the root cause to the Translation Lookaside Buffer (TLB). Using the micro-benchmarks, the TLB hierarchy and structure are fully analyzed on two different GPU architectures, identifying never-before-published TLB sizes that can be used for efficient large-scale application tuning. Based on the gained knowledge, we propose a TLB-conscious approach to mitigate the slowdown for algorithms with irregular memory access. The proposed approach is applied to two fundamental database operations - random sampling and hash-based grouping - showing that the slowdown can be dramatically reduced, and resulting in a performance increase of up to 13\u00d7.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "1402921119",
                    "name": "Tal Ben-Nun"
                },
                {
                    "authorId": "2055596828",
                    "name": "Matthias Werner"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "c5c27f620785ef0e1ccb79f950d3d675ef099de5",
            "title": "SPARQLytics: Multidimensional Analytics for RDF",
            "abstract": "With the rapid growth of open RDF data in recent years, being able to perform multidimensional analytics with it has become more and more important, in particular for the data analyst performing explorative business intelligence tasks. Existing analytic approaches are often not \u0106exible enough to address the needs of data analysts and enthusiasts with iterative exploratory work\u0106ows. In this paper we propose SPARQLytics, a tool that exposes the concepts of multidimensional graph analytics by ofering standard OLAP cube operations and generating SPARQL queries. Our evaluation shows that SPARQLytics unburdens data analysts from writing many lines of SPARQL code in iterative data explorations and at the same time it does not impose any overhead to query execution. SPARQLytics \u0104ts well with interactive computing tools, such as Jupyter, providing data enthusiasts with a familiar work environment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053142642",
                    "name": "Michael Rudolf"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "d726ef99c5a5fbe3b0c16e632c2392093f384335",
            "title": "An analysis of LSM caching in NVRAM",
            "abstract": "The rise of NVRAM technologies promises to change the way we think about system architectures. In order to fully exploit its advantages, it is required to develop systems specially tailored for NVRAM devices. Not only this imposes great challenges, but developing full system architectures from scratch is undesirable in many scenarios due to prohibitive development costs. Instead, we analyze in this paper the behavior of an existing log-structured persistent key-value store, namely LevelDB, when run on top of an emulated NVRAM device. We investigate initial opportunities for improvement when adapting a system tailored for HDD/SSDs to run on top of an NVRAM environment. Furthermore, we analyze the behavior of the legacy DRAM caching component of LevelDB and whether more suitable caching policies are required.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143708490",
                    "name": "L. Lersch"
                },
                {
                    "authorId": "2742360",
                    "name": "Ismail Oukid"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "3369609",
                    "name": "I. Schr\u00e9ter"
                }
            ]
        },
        {
            "paperId": "dfede9963a1cfe5cb6178cbc573cae48b1af0869",
            "title": "Adaptive Work Placement for Query Processing on Heterogeneous Computing Resources",
            "abstract": "The hardware landscape is currently changing from homogeneous multi-core systems towards heterogeneous systems with many different computing units, each with their own characteristics. This trend is a great opportunity for data-base systems to increase the overall performance if the heterogeneous resources can be utilized efficiently. To achieve this, the main challenge is to place the right work on the right computing unit. Current approaches tackling this placement for query processing assume that data cardinalities of intermediate results can be correctly estimated. However, this assumption does not hold for complex queries. To overcome this problem, we propose an adaptive placement approach being independent of cardinality estimation of intermediate results. Our approach is incorporated in a novel adaptive placement sequence. Additionally, we implement our approach as an extensible virtualization layer, to demonstrate the broad applicability with multiple database systems. In our evaluation, we clearly show that our approach significantly improves OLAP query processing on heterogeneous hardware, while being adaptive enough to react to changing cardinalities of intermediate query results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "eb90e7d610350ecd339eacad510498d8398c219f",
            "title": "Hardware Based Databases",
            "abstract": "Recent development on the hardware sidewill have amassive impact on the software stack of a computer system. While the software side enjoyed a \u201cfree ride\u201d with ever increasing clock cycles for many decades, the time has finally come to adjust principles of software architecture in order to exploit the opportunities provided by modern hardware components. Within this special issue of it \u2013 Information Technology, we are perceiving data management systems as one of the most performance hungry software applications giving evidence of the longstanding strive to directly control the hardware layer. The collection of articles addresses this topic from multiple directions \u2013 advances on the memory as well as on the processor side; low-level considerations versus a more global perspective in hybrid cloud environments aswell as academic perspective versus industrial contributions from SAP as well as IBM. In more detail, the first article addresses opportunities as well as challenges in the context of the upcoming non-volatile memory technology. This contribution is provided by Ismail Oukid et al. from the SAP HANA Database Campus inWalldorf and outlines the characteristics of this novel type of memory technology like byte-addressability, low-latency as well as energy-efficiency. This paper also sketches the requirements from a database system perspective to achieve transactional consistency as well as fast recovery. The second paper of this series addresses the combination of memory and compute elements by focusing on the perspective of data (as well as operator) placement within a heterogeneous system setup. This research direction will be extremely relevant in the context of \u201cdark silicon\u201d providing more opportunities for specialized processor elements and \u2013 as a consequence \u2013 a necessity to deploy a database query plan optimally onto the different processor elements. The paper states the overall problem",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "16791bbf794c86e635a3b6ead5a48769cab798cd",
            "title": "Model Kit for Lightweight Data Compression Algorithms",
            "abstract": "Modern database systems are very often in the position to store and efficiently process their entire data in main memory. Aside from increased main memory capacities, a further driver for in-memory database systems has been the shift to a column-oriented storage format in combination with lightweight data compression techniques. In recent years, a lot of lightweight data compression algorithms have been developed to efficiently support different data characteristics. Therefore, database systems should include a large number of these algorithms. To enable this, we introduce our novel modularization concept including our model kit implementation for lightweight data compression algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27226616",
                    "name": "Juliana Hildebrandt"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2098d55da06f1ed889dc211322d4229099c69120",
            "title": "Web-based Benchmarks for Forecasting Systems: The ECAST Platform",
            "abstract": "The role of precise forecasts in the energy domain has changed dramatically. New supply forecasting methods are developed to better address this challenge, but meaningful benchmarks are rare and time-intensive. We propose the ECAST online platform in order to solve that problem. The system's capability is demonstrated on a real-world use case by comparing the performance of different prediction tools.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266405",
                    "name": "R. Ulbricht"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "2366481",
                    "name": "Hilko Donker"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "21ef32caa5f53b7c6501f8fb3429766dc4209105",
            "title": "Energy Elasticity on Heterogeneous Hardware using Adaptive Resource Reconfiguration LIVE",
            "abstract": "Energy awareness of database systems has emerged as a critical research topic, since energy consumption is becoming a major limiter for their scalability. Recent energy-related hardware developments trend towards offering more and more configuration opportunities for the software to control its own energy consumption. Existing research so far mainly focused on leveraging this configuration spectrum to find the most energy-efficient configuration for specific operators or entire queries. In this demo, we introduce the concept of energy elasticity and propose the energy-control loop as an implementation of this concept. Energy elasticity refers to the ability of software to behave energy-proportional and energy-efficient at the same time while maintaining a certain quality of service. Thus, our system does not draw the least energy possible but the least energy necessary to still perform reasonably. We demonstrate our overall approach using a rich interactive GUI to give attendees the opportunity to learn more about our concept.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "1414019323",
                    "name": "Willi-Wolfram Mentzel"
                },
                {
                    "authorId": "9539656",
                    "name": "Eric Mier"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "28afcfdb558cd3e5a1f9dc6014f2317b6264f420",
            "title": "Template-based Time Series Generation with Loom",
            "abstract": "Time series analysis and forecasting are important techniques for decision-making in many domains. They are typically evaluated on given sets of time series that have a constant size and specied characteristics. Synthetic datasets are relevant because they are exible in both size and characteristics. In this demo, we present our prototype Loom, that generates datasets with respect to the user\u2019s conguration of categorical information and time series characteristics. The prototype allows for comparison of dierent analysis techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30389017",
                    "name": "Lars Kegel"
                },
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "3684cd59d3e65b96fdbf7504ee4e99a075a0911d",
            "title": "Topology-aware optimization of big sparse matrices and matrix multiplications on main-memory systems",
            "abstract": "Since data sizes of analytical applications are continuously growing, many data scientists are switching from customized micro-solutions to scalable alternatives, such as statistical and scientific databases. However, many algorithms in data mining and science are expressed in terms of linear algebra, which is barely supported by major database vendors and big data solutions. On the other side, conventional linear algebra algorithms and legacy matrix representations are often not suitable for very large matrices. We propose a strategy for large matrix processing on modern multicore systems that is based on a novel, adaptive tile matrix representation (AT MATRIX). Our solution utilizes multiple techniques inspired from database technology, such as multidimensional data partitioning, cardinality estimation, indexing, dynamic rewrites, and many more in order to optimize the execution time. Based thereon we present a matrix multiplication operator ATMULT, which outperforms alternative approaches. The aim of our solution is to overcome the burden for data scientists of selecting appropriate algorithms and matrix storage representations. We evaluated AT MATRIX together with ATMULT on several real-world and synthetic random matrices.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2132632",
                    "name": "D. Kernert"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2064738044",
                    "name": "F. K\u00f6hler"
                }
            ]
        },
        {
            "paperId": "3f38a7a77f865b786821fdfdfd2e6266a95ff155",
            "title": "DebEAQ - debugging empty-answer queries on large data graphs",
            "abstract": "The large volume of freely available graph data sets impedes the users in analyzing them. For this purpose, they usually pose plenty of pattern matching queries and study their answers. Without deep knowledge about the data graph, users can create `failing' queries, which deliver empty answers. Analyzing the causes of these empty answers is a time-consuming and complicated task especially for graph queries. To help users in debugging these `failing' queries, there are two common approaches: one is focusing on discovering missing subgraphs of a data graph, the other one tries to rewrite the queries such that they deliver some results. In this demonstration, we will combine both approaches and give the users an opportunity to discover why empty results were delivered by the requested queries. Therefore, we propose DebEAQ, a debugging tool for pattern matching queries, which allows to compare both approaches and also provides functionality to debug queries manually.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145410888",
                    "name": "E. Vasilyeva"
                },
                {
                    "authorId": "122233317",
                    "name": "Thomas S. Heinze"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "4974924bf648a56d555079c37774dd31649a1145",
            "title": "InVerDa - co-existing schema versions made foolproof",
            "abstract": "In modern software landscapes multiple applications usually share one database as their single point of truth. All these applications will evolve over time by their very nature. Often former versions need to stay available, so database developers find themselves maintaining co-existing schema version of multiple applications in multiple versions. This is highly error-prone and accounts for significant costs in software projects, as developers realize the translation of data accesses between schema versions with hand-written delta code. In this demo, we showcase INVERDA, a tool for integrated, robust, and easy to use database versioning. We rethink the way of specifying the evolution to new schema versions. Using the richer semantics of a descriptive database evolution language, we generate all required artifacts automatically and make database versioning foolproof.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40466466",
                    "name": "K. Herrmann"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "3419131",
                    "name": "Thorsten Seyschab"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "586dad39791ea4c0bf7c557b32f831a5d66eb2ae",
            "title": "Model-Driven Integration of Compression Algorithms in Column-Store Database Systems",
            "abstract": ". Modern database systems are very often in the position to store their entire data in main memory. Aside from increased main memory capacities, a further driver for in-memory database systems was the shift to a decomposition storage model in combination with lightweight data compression algorithms. Using both mentioned storage design concepts, large datasets can be held and processed in main memory with a low memory footprint. In recent years, a large corpus of lightweight data compression algorithms has been developed to e\ufb03ciently support di\ufb00er-ent data characteristics. In this paper, we present our novel model-driven concept to integrate this large and evolving corpus of lightweight data compression algorithms in column-store database systems. Core components of our concept are (i) a uni\ufb01ed conceptual model for lightweight compression algorithms, (ii) specifying algorithms as platform-independ-ent model instances, (iii) transforming model instances into low-level system code, and (iv) integrating low-level system code into a storage layer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27226616",
                    "name": "Juliana Hildebrandt"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "66fedd8464c74e63b4b8b333916b6087b463cf8a",
            "title": "Big by blocks: modular analytics",
            "abstract": "Abstract Big Data and Big Data analytics have attracted major interest in research and industry and continue to do so. The high demand for capable and scalable analytics in combination with the ever increasing number and volume of application scenarios and data has lead to a large and intransparent landscape full of versions, variants and individual algorithms. As this zoo of methods lacks a systematic way of description, understanding is almost impossible which severely hinders effective application and efficient development of analytic algorithms. To solve this issue we propose our concept of modular analytics that abstracts the essentials of an analytic domain and turns them into a set of universal building blocks. As arbitrary algorithms can be created from the same set of blocks, understanding is eased and development benefits from reusability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "30389017",
                    "name": "Lars Kegel"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "686426d2d639156d6f4757e9be630f9c0b6b2fab",
            "title": "A Machine Learning Approach for Layout Inference in Spreadsheets",
            "abstract": "Spreadsheet applications are one of the most used tools for content generation and presentation in industry and the Web. In spite of this success, there does not exist a comprehensive approach to automatically extract and reuse the richness of data maintained in this format. The biggest obstacle is the lack of awareness about the structure of the data in spreadsheets, which otherwise could provide the means to automatically understand and extract knowledge from these files. In this paper, we propose a classification approach to discover the layout of tables in spreadsheets. Therefore, we focus on the cell level, considering a wide range of features not covered before by related work. We evaluated the performance of our classifiers on a large dataset covering three different corpora from various domains. Finally, our work includes a novel technique for detecting and repairing incorrectly classified cells in a post-processing step. The experimental results show that our approach delivers very high accuracy bringing us a crucial step closer towards automatic table extraction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1757263",
                    "name": "Elvis Koci"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "144149891",
                    "name": "Oscar Romero"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "718d533b84515054652205ba737caffd7d47b5e6",
            "title": "A database accelerator for energy-efficient query processing and optimization",
            "abstract": "Data processing on a continuously growing amount of information and the increasing power restrictions have become an ubiquitous challenge in our world today. Besides parallel computing, a promising approach to improve the energy efficiency of current systems is to integrate specialized hardware. This paper presents a Tensilica RISC processor extended with an instruction set to accelerate basic database operators frequently used in modern database systems. The core was taped out in a 28 nm SLP CMOS technology and allows energy-efficient query processing as well as query optimization by applying selectivity estimation techniques. Our chip measurements show an 1000x energy improvement on selected database operators compared to state-of-the-art systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1605950053",
                    "name": "Sebastian Haas"
                },
                {
                    "authorId": "48730774",
                    "name": "Oliver Arnold"
                },
                {
                    "authorId": "2274439",
                    "name": "Stefan Scholze"
                },
                {
                    "authorId": "3145457",
                    "name": "S. H\u00f6ppner"
                },
                {
                    "authorId": "2456940",
                    "name": "G. Ellguth"
                },
                {
                    "authorId": "3405503",
                    "name": "Andreas Dixius"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "9539656",
                    "name": "Eric Mier"
                },
                {
                    "authorId": "51292635",
                    "name": "Benedikt N\u00f6then"
                },
                {
                    "authorId": "144845338",
                    "name": "E. Mat\u00fas"
                },
                {
                    "authorId": "2214282",
                    "name": "S. Schiefer"
                },
                {
                    "authorId": "2198499",
                    "name": "Love Cederstroem"
                },
                {
                    "authorId": "2083798148",
                    "name": "Fabian Pilz"
                },
                {
                    "authorId": "1713846",
                    "name": "C. Mayr"
                },
                {
                    "authorId": "1700081",
                    "name": "R. Sch\u00fcffny"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "145570132",
                    "name": "G. Fettweis"
                }
            ]
        },
        {
            "paperId": "72eb22e1f79850dae8f8c7ce1385b228dd34df4f",
            "title": "Logical Data Independence in the 21st Century - Co-Existing Schema Versions with InVerDa",
            "abstract": "We present InVerDa, a tool for end-to-end support of co-existing schema versions within one database. While it is state of the art to run multiple versions of a continuously developed application concurrently, the same is hard for databases. In order to keep multiple co-existing schema versions alive, that all access the same data set, developers usually employ handwritten delta code (e.g. views and triggers in SQL). This delta code is hard to write and hard to maintain: if a database administrator decides to adapt the physical table schema, all handwritten delta code needs to be adapted as well, which is expensive and error-prone in practice. With InVerDa, developers use a simple bidirectional database evolution language in the first place that carries enough information to generate all the delta code automatically. Without additional effort, new schema versions become immediately accessible and data changes in any version are visible in all schema versions at the same time. We formally validate the correctness of this propagation. InVerDa also allows for easily changing the physical table designs without affecting the availability of co-existing schema versions. This greatly increases robustness (264 times less lines of code) and allows for significant performance optimization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40466466",
                    "name": "K. Herrmann"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "3193518",
                    "name": "Andreas Behrend"
                },
                {
                    "authorId": "2765285",
                    "name": "Jonas Rausch"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "73f785c10a97ce8758a3dc86e64e4badb5c4eb88",
            "title": "On testing persistent-memory-based software",
            "abstract": "Leveraging Storage Class Memory (SCM) as a universal memory--i.e. as memory and storage at the same time--has deep implications on database architectures. It becomes possible to store a single copy of the data in SCM and directly operate on it at a fine granularity. However, exposing the whole database with direct access to the application dramatically increases the risk of data corruption. In this paper we propose a lightweight on-line testing framework that helps find and debug SCM-related errors that can occur upon software or power failures. Our testing framework simulates failures in critical code paths and achieves fast code coverage by leveraging call stack information to limit duplicate testing. It also partially covers the errors that might arise as a result of reordered memory operations. We show through an experimental evaluation that our testing framework is fast enough to be used with large software systems and discuss its use during the development of our in-house persistent SCM allocator.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2742360",
                    "name": "Ismail Oukid"
                },
                {
                    "authorId": "2077127667",
                    "name": "Daniel Booss"
                },
                {
                    "authorId": "3410409",
                    "name": "Adrien Lespinasse"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "ac6db2054957cd48a3f1dffa5bc552a544e47dee",
            "title": "Penalized Graph Partitioning Based Allocation Strategy for Database-as-a-Service Systems",
            "abstract": "Databases as a service (DBaaS) transfer the advantages of cloud computing to data management systems, which is important for the big data era. The allocation in a DBaaS system, i.e., the mapping from databases to nodes of the infrastructure, influences performance, utilization, and cost-effectiveness of the system. Modeling databases and the underlying infrastructure as weighted graphs and using graph partitioning and mapping algorithms yields an allocation strategy. However, graph partitioning assumes that individual vertex weights add up (linearly) to partition weights. In reality, performance does usually not scale linearly with the amount of work due to contention on the hardware, on operating system resources, or on DBMS components. To overcome this issue, we propose an allocation strategy based on penalized graph partitioning in this paper. We show how existing algorithms can be modified for graphs with non-linear partition weights, i.e., vertex weights that do not sum up linearly to partition weights. We experimentally evaluate our allocation strategy in a DBaaS system with 1,000 databases on 32 nodes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32019086",
                    "name": "Tim Kiefer"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "b1a67db44428a6e48ee7ffb54c6f60ea56e258c2",
            "title": "The Orchestration Stack: The Impossible Task of Designing Software for Unknown Future Post-CMOS Hardware",
            "abstract": "\u2014Future systems based on post-CMOS technologies will be wildly heterogeneous, with properties largely unknown today. This paper presents our design of a new hardware/software stack to address the challenge of preparing software development for such systems. It combines well-understood technologies from different areas, e.g., network-on-chips, capability operating systems, \ufb02exible programming models and model checking. We describe our approach and provide details on key technologies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31472436",
                    "name": "Marcus Volp"
                },
                {
                    "authorId": "1840345",
                    "name": "Sascha Kl\u00fcppelholz"
                },
                {
                    "authorId": "144195575",
                    "name": "J. Castrill\u00f3n"
                },
                {
                    "authorId": "1731688",
                    "name": "Hermann H\u00e4rtig"
                },
                {
                    "authorId": "2495306",
                    "name": "Nils Asmussen"
                },
                {
                    "authorId": "143681675",
                    "name": "U. Assmann"
                },
                {
                    "authorId": "1710233",
                    "name": "F. Baader"
                },
                {
                    "authorId": "5403194",
                    "name": "C. Baier"
                },
                {
                    "authorId": "145570132",
                    "name": "G. Fettweis"
                },
                {
                    "authorId": "145284133",
                    "name": "J. Fr\u00f6hlich"
                },
                {
                    "authorId": "32708115",
                    "name": "Andr\u00e9s Goens"
                },
                {
                    "authorId": "1605950053",
                    "name": "Sebastian Haas"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "144202399",
                    "name": "Mattis Hasler"
                },
                {
                    "authorId": "2070377",
                    "name": "I. Huismann"
                },
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "3307482",
                    "name": "Sven Karol"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2395105",
                    "name": "Linda Leuschner"
                },
                {
                    "authorId": "30247428",
                    "name": "Matthias Lieber"
                },
                {
                    "authorId": "40404655",
                    "name": "Siqi Ling"
                },
                {
                    "authorId": "2735651",
                    "name": "Steffen M\u00e4rcker"
                },
                {
                    "authorId": "5747726",
                    "name": "Johannes Mey"
                },
                {
                    "authorId": "1781970",
                    "name": "W. Nagel"
                },
                {
                    "authorId": "31314164",
                    "name": "Benedikt Nothen"
                },
                {
                    "authorId": "144138457",
                    "name": "R. Pe\u00f1aloza"
                },
                {
                    "authorId": "1862588",
                    "name": "Michael Raitza"
                },
                {
                    "authorId": "144760860",
                    "name": "J. Stiller"
                },
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "145650681",
                    "name": "A. Voigt"
                }
            ]
        },
        {
            "paperId": "cc2bc224cc1c8ef3d5e644f971b02ea30e1a4ca6",
            "title": "Efficient Approximate OLAP Querying Over Time Series",
            "abstract": "The ongoing trend for data gathering not only produces larger volumes of data, but also increases the variety of recorded data types. Out of these, especially time series, e.g. various sensor readings, have attracted attention in the domains of business intelligence and decision making. As OLAP queries play a major role in these domains, it is desirable to also execute them on time series data. While this is not a problem on the conceptual level, it can become a bottleneck with regards to query run-time. In general, processing OLAP queries gets more computationally intensive as the volume of data grows. This is a particular problem when querying time series data, which generally contains multiple measures recorded at fine time granularities. Usually, this issue is addressed either by scaling up hardware or by employing workload based query optimization techniques. However, these solutions are either costly or require continuous maintenance. In this paper we propose an approach for approximate OLAP querying of time series that offers constant latency and is maintenance-free. To achieve this, we identify similarities between aggregation cuboids and propose algorithms that eliminate the redundancy these similarities present. In doing so, we can achieve compression rates of up to 80% while maintaining low average errors in the query results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38606508",
                    "name": "K. Perera"
                },
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1731453",
                    "name": "T. Pedersen"
                },
                {
                    "authorId": "146439944",
                    "name": "Christian Thomsen"
                }
            ]
        },
        {
            "paperId": "d217f9be0379e658bebbb85ea9248906dd9f9a48",
            "title": "Living in Parallel Realities: Co-Existing Schema Versions with a Bidirectional Database Evolution Language",
            "abstract": "We introduce end-to-end support of co-existing schema versions within one database. While it is state of the art to run multiple versions of a continuously developed application concurrently, it is hard to do the same for databases. In order to keep multiple co-existing schema versions alive -- which are all accessing the same data set -- developers usually employ handwritten delta code (e.g. views and triggers in SQL). This delta code is hard to write and hard to maintain: if a database administrator decides to adapt the physical table schema, all handwritten delta code needs to be adapted as well, which is expensive and error-prone in practice. In this paper, we present InVerDa: developers use the simple bidirectional database evolution language BiDEL, which carries enough information to generate all delta code automatically. Without additional effort, new schema versions become immediately accessible and data changes in any version are visible in all schema versions at the same time. InVerDa also allows for easily changing the physical table design without affecting the availability of co-existing schema versions. This greatly increases robustness (orders of magnitude less lines of code) and allows for significant performance optimization. A main contribution is the formal evaluation that each schema version acts like a common full-fledged database schema independently of the chosen physical table design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40466466",
                    "name": "K. Herrmann"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "2249189141",
                    "name": "Andreas Behrend"
                },
                {
                    "authorId": "2765285",
                    "name": "Jonas Rausch"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "e613f5c47f30d098005bd10ccb8cfb82df991294",
            "title": "Challenges for Context-Driven Time Series Forecasting",
            "abstract": "Predicting time series is a crucial task for organizations, since decisions are often based on uncertain information. Many forecasting models are designed from a generic statistical point of view. However, each real-world application requires domain-specific adaptations to obtain high-quality results. All such specifics are summarized by the term of context. In contrast to current approaches, we want to integrate context as the primary driver in the forecasting process. We introduce context-driven time series forecasting focusing on two exemplary domains: renewable energy and sparse sales data. In view of this, we discuss the challenge of context integration in the individual process steps.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266405",
                    "name": "R. Ulbricht"
                },
                {
                    "authorId": "2366481",
                    "name": "Hilko Donker"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "ed250441c1c9a97afecc74553e3e3a9fd4966b33",
            "title": "FPTree: A Hybrid SCM-DRAM Persistent and Concurrent B-Tree for Storage Class Memory",
            "abstract": "The advent of Storage Class Memory (SCM) is driving a rethink of storage systems towards a single-level architecture where memory and storage are merged. In this context, several works have investigated how to design persistent trees in SCM as a fundamental building block for these novel systems. However, these trees are significantly slower than DRAM-based counterparts since trees are latency-sensitive and SCM exhibits higher latencies than DRAM. In this paper we propose a novel hybrid SCM-DRAM persistent and concurrent B-Tree, named Fingerprinting Persistent Tree (FPTree) that achieves similar performance to DRAM-based counterparts. In this novel design, leaf nodes are persisted in SCM while inner nodes are placed in DRAM and rebuilt upon recovery. The FPTree uses Fingerprinting, a technique that limits the expected number of in-leaf probed keys to one. In addition, we propose a hybrid concurrency scheme for the FPTree that is partially based on Hardware Transactional Memory. We conduct a thorough performance evaluation and show that the FPTree outperforms state-of-the-art persistent trees with different SCM latencies by up to a factor of 8.2. Moreover, we show that the FPTree scales very well on a machine with 88 logical cores. Finally, we integrate the evaluated trees in memcached and a prototype database. We show that the FPTree incurs an almost negligible performance overhead over using fully transient data structures, while significantly outperforming other persistent trees.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2742360",
                    "name": "Ismail Oukid"
                },
                {
                    "authorId": "3416311",
                    "name": "Johan Lasperas"
                },
                {
                    "authorId": "1700004",
                    "name": "A. Nica"
                },
                {
                    "authorId": "3250632",
                    "name": "Thomas Willhalm"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "f61830a75745b8672bc4dd74b0052ef5770675bf",
            "title": "Putting Web Tables into Context",
            "abstract": "Web tables are a valuable source of information used in many application areas. However, to exploit Web \n \ntables it is necessary to understand their content and intention which is impeded by their ambiguous semantics \n \nand inconsistencies. Therefore, additional context information, e.g. text in which the tables are embedded, \n \nis needed to support the table understanding process. In this paper, we propose a novel contextualization \n \napproach that 1) splits the table context in topically coherent paragraphs, 2) provides a similarity measure \n \nthat is able to match each paragraph to the table in question and 3) ranks these paragraphs according to their \n \nrelevance. Each step is accompanied by an experimental evaluation on real-world data showing that our \n \napproach is feasible and effectively identifies the most relevant context for a given Web table.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3176379",
                    "name": "Katrin Braunschweig"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "1757263",
                    "name": "Elvis Koci"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "f9dd1dd5c5649fd6d52b2a26ce50e64c2d5bde21",
            "title": "HW/SW-database-codesign for compressed bitmap index processing",
            "abstract": "Compressed bitmap indices are heavily used in scientific and commercial database systems because they largely improve query performance for various workloads. Early research focused on finding tailor-made index compression schemes that are amenable for modern processors. Improving performance further typically comes at the expense of a lower compression rate, which is in many applications not acceptable because of memory limitations. Alternatively, tailor-made hardware allows to achieve a performance that can only hardly be reached with software running on general-purpose CPUs. In this paper, we will show how to create a custom instruction set framework for compressed bitmap processing that is generic enough to implement most of the major compressed bitmap indices. For evaluation, we implemented WAH, PLWAH, and COMPAX operations using our framework and compared the resulting implementation to multiple state-of-the-art processors. We show that the custom-made bitmap processor achieves speedups of up to one order of magnitude by also using two orders of magnitude less energy compared to a modern energy-efficient Intel processor. Finally, we discuss how to embed our processor with database-specific instruction sets into database system environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1605950053",
                    "name": "Sebastian Haas"
                },
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "48730774",
                    "name": "Oliver Arnold"
                },
                {
                    "authorId": "7422541",
                    "name": "Erik Laux"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "145570132",
                    "name": "G. Fettweis"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "fbd9d555a1113e558fd870c00e01e57c1d3c911b",
            "title": "HUGS - A Lightweight Graph Partitioning Approach",
            "abstract": "The growing interest in graph data lead to increasingly more research in the field of graph data management and graph analytics. Nowadays, even large graphs of upto a size of billions of vertices and edges fit into main memory of big modern multisocket machines, making them a first-grade platform for graph management and graph analytics. High performance data management solutions have to be aware of the NUMA properties of such big machines. A dataoriented architecture (DORA) is a particular solution to that. However, it requires partitioning the data in a way such that inter-partition communication can be avoided. Graph partitioning is a long studied problem and stateof-the-art solutions, such as multilevel k-way partitioning and recursive bisection achieve good results in feasible time. Integrating such solution is a rather difficult task, though. In this paper, we present a more lightweight approach called Hugs. The key idea of Hugs is to reuse the BFS routine present in a graph data management system anyway, since it is the foundation of many analytical graph algorithms. Hugs is not meant to produce a good general-purpose graph partitioning but good runtimes of BFS graph traversals such as reachability queries on DORA systems. In our experiments Hugs showed capable of finding good graph partitionings faster than state-of-the-art approaches. The partitioning found by Hugs also showed shorter runtimes for reachability queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9187969",
                    "name": "Alexander Krause"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "febc1a6ac3e6db1a87572a3a50b4ef9ffb700213",
            "title": "Architecture of a Multi-domain Processing and Storage Engine",
            "abstract": "In today\u00e2\u0080\u0099s data-driven world, economy and research depend on the analysis of empirical datasets to guide decision \n \nmaking. These applications often encompass a rich variety of data types and special purpose processing \n \nmodels. We believe, the database system of the future will integrate flexible processing and storage of a variety \n \nof data types in a scalable and integrated end-to-end solution. In this paper, we propose a database system \n \narchitecture that is designed from the core to support these goals. In the discussion we will especially focus on \n \nthe multi-domain programming concept of the proposed architecture that exploits domain specific knowledge \n \nto guide compiler based optimization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3469702",
                    "name": "Johannes Luong"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "0b5ca1131d91a4ad4eb2e97d9de92703513bacc7",
            "title": "Relaxation of subgraph queries delivering empty results",
            "abstract": "Graph databases with the property graph model are used in multiple domains including social networks, biology, and data integration. They provide schema-flexible storage for data of a different degree of a structure and support complex, expressive queries such as subgraph isomorphism queries. The exibility and expressiveness of graph databases make it difficult for the users to express queries correctly and can lead to unexpected query results, e.g. empty results. Therefore, we propose a relaxation approach for subgraph isomorphism queries that is able to automatically rewrite a graph query, such that the rewritten query is similar to the original query and returns a non-empty result set. In detail, we present relaxation operations applicable to a query, cardinality estimation heuristics, and strategies for prioritizing graph query elements to be relaxed. To determine the similarity between the original query and its relaxed variants, we propose a novel cardinality-based graph edit distance. The feasibility of our approach is shown by using real-world queries from the DBpedia query log.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145410888",
                    "name": "E. Vasilyeva"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "40462818",
                    "name": "Adrian Mocan"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "1576ae275432fc261f7d7835a15562c43cceeb7d",
            "title": "DrillBeyond: processing multi-result open world SQL queries",
            "abstract": "In a traditional relational database management system, queries can only be defined over attributes defined in the schema, but are guaranteed to give single, definitive answer structured exactly as specified in the query. In contrast, an information retrieval system allows the user to pose queries without knowledge of a schema, but the result will be a top-k list of possible answers, with no guarantees about the structure or content of the retrieved documents. In this paper, we present DrillBeyond, a novel IR/RDBMS hybrid system, in which the user seamlessly queries a relational database together with a large corpus of tables extracted from a web crawl. The system allows full SQL queries over the relational database, but additionally allows the user to use arbitrary additional attributes in the query that need not to be defined in the schema. The system then processes this semi-specified query by computing a top-k list of possible query evaluations, each based on different candidate web data sources, thus mixing properties of RDBMS and IR systems. We design a novel plan operator that encapsulates a web data retrieval and matching system and allows direct integration of such systems into relational query processing. We then present methods for efficiently processing multiple variants of a query, by producing plans that are optimized for large invariant intermediate results that can be reused between multiple query evaluations. We demonstrate the viability of the operator and our optimization strategies by implementing them in PostgreSQL and evaluating on a standard benchmark by adding arbitrary attributes to its queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2590616",
                    "name": "Julian Eberius"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "3176379",
                    "name": "Katrin Braunschweig"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "15ff34155d0c653ab44cb9be39b0392a8112ed19",
            "title": "Towards Programmability of a NUMA-Aware Storage Engine",
            "abstract": "The SQL database language was originally intended for ap- plication programmers. However, after more than 20 years of language extensions, SQL can only be generated by software components and is no longer suitable for an increasing user base like knowledge workers or data scientists, who want to work with data in an interactive fash- ion. The original idea of declarative query languages, telling the system what information to retrieve and not how to retrieve it, is still rele- vant. However, procedural elements are extremely worthwhile and have to be part of a next generation database programming language without compromising performance and scalability. To tackle this challenge, we are going to present our overall approach consisting of a highly-scalable NUMA-aware storage engine ERIS and a novel appropriate procedural programming approach on top of ERIS in this paper.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "1950850",
                    "name": "Johannes Schad"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "16e02929bc217601f4b801332b5dd2fbb4915537",
            "title": "Relationships for Dynamic Data Types in RSQL",
            "abstract": "Currently, there is a mismatch between the conceptual model of an information system and its implementation in a database management system (DBMS). Most of the conceptual modeling languages relate their conceptual entities with relationships, but relational database management systems solely rely on the notion of relations to model both, entities and relationships. To make things worse, real world objects are not static as assumed in such modeling languages, but change over time. Thus, modeling languages were enriched to model those scenarios, as well. However, mapping these models onto relational databases requires the use of object-relational mapping engines, which in turn hide the semantics of the conceptual model from the DBMS. Consequently, traditional relational database systems cannot directly ensure specific consistency constraints and thus lose their meaning as single point of truth for highly distributed information systems. To overcome these issues we have proposed RSQL, a data model and query language introducing role-based data structures in DBMSs. Despite the fact that RSQL is able to handle complex objects, it does not support relationships between those objects. Therefore, this work adds relationships to RSQL by augmenting the data model and extending its query language. As a result, this extension allows for the direct representation of conceptual models with complex objects and relationships in the DBMS. Thus, relationships can be directly addressed in queries and the DBMS automatically ensures relationship consistency constraints as well as cardinality. In sum, a DBMS equipped with the extended RSQL is apt for storing and querying conceptual models and thus regains its rightful position as the single point of truth for highly distributed information systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21658827",
                    "name": "Tobias J\u00e4kel"
                },
                {
                    "authorId": "1820834438",
                    "name": "Thomas K\u00fchn"
                },
                {
                    "authorId": "35443626",
                    "name": "Stefan Hinkel"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2230fc14c7da92963316ca1143ca62d8d3cd816d",
            "title": "Database Evolution for Software Product Lines",
            "abstract": "Software product lines (SPLs) allow creating a multitude of individual but similar products based on one \n \ncommon software model. Software components can be developed independently and new products can be \n \ngenerated easily. Inevitably, software evolves, a new version has to be deployed, and the data already existing \n \nin the database has to be transformed accordingly. As independently developed components are compiled \n \ninto an individual SPL product, the local evolution script of every involved component has to be weaved into \n \na single global database evolution script for the product. In this paper, we report on the database evolution \n \ntoolkit DAVE in the context of an industry project. DAVE solves the weaving problem and provides a feasible \n \nsolution for database evolution in SPLs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40466466",
                    "name": "K. Herrmann"
                },
                {
                    "authorId": "2371688",
                    "name": "Jan Reimann"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "2968777",
                    "name": "B. Demuth"
                },
                {
                    "authorId": "39259372",
                    "name": "Stefan Fromm"
                },
                {
                    "authorId": "2819080",
                    "name": "Robert Stelzmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2b2d791bacb6376c327a9ca503ea14e9ed4ec20c",
            "title": "POIESIS: a Tool for Quality-aware ETL Process Redesign",
            "abstract": "We present a tool, called POIESIS, for automatic ETL process enhancement. ETL processes are essential data-centric activities in modern business intelligence environments and they need to be examined through a viewpoint that concerns their quality characteristics (e.g., data quality, performance, manageability) in the era of Big Data. POIESIS responds to this need by providing a user-centered environment for quality-aware analysis and redesign of ETL flows. It generates thousands of alternative flows by adding flow patterns to the initial flow, in varying positions and combinations, thus creating alternative design options in a multidimensional space of di\u21b5erent quality attributes. Through the demonstration of POIESIS we introduce the tool\u2019s capabilities and highlight its eciency, usability and modifiability, thanks to its polymorphic design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3054847",
                    "name": "V. Theodorou"
                },
                {
                    "authorId": "48843545",
                    "name": "A. Abell\u00f3"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2ecd723060666fca72ea7a7b317510bf2c2f3bb2",
            "title": "Optimierung der Anfrageverarbeitung mittels Kompression der Zwischenergebnisse",
            "abstract": "In Hauptspeicher-zentrischen Architekturans\u00e4tzen f\u00fcr Datenbanksysteme m\u00fcssen f\u00fcr die Anfrageverarbeitung sowohl die Basisrelationen als auch die Zwischenergebnisse im Hauptspeicher gehalten werden. Des Weiteren ist der Aufwand, um Zwischenergebnisse zu generieren, \u00e4quivalent zum Aufwand, um \u00c4nderungen an den Basisrelationen durchzuf\u00fchren. Daher sollten zur effizienten Anfrageverarbeitung die Zwischenergebnisse so organisiert werden, dass eine effiziente Verarbeitung im Anfrageplan m\u00f6glich ist. F\u00fcr diesen Bereich schlagen wir den durchg\u00e4ngigen Einsatz leichtgewichtiger Kompressionsverfahren f\u00fcr Zwischenergebnisse vor und haben das Ziel, eine ausgewogene Anfrageverarbeitung auf Basis komprimierter Zwischenergebnisse zu entwickeln. Dieser Artikel gibt einen \u00dcberblick \u00fcber unser Konzept und die wissenschaftlichen Herausforderungen. Dar\u00fcber hinaus f\u00fchren wir erste Ans\u00e4tze zur Optimierung von leichtgewichtigen Kompressionsund Transformationsverfahren ein, die aus Sicht der Anfrageverarbeitung eventuell sinnvoll sind, um einen effizienten Wechsel des Kompressionsverfahrens durchzuf\u00fchren.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072699345",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2059177263",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2f9752da8c1b7c39d6d56d049db735761c6bc99b",
            "title": "Cache-Efficient Aggregation: Hashing Is Sorting",
            "abstract": "For decades researchers have studied the duality of hashing and sorting for the implementation of the relational operators, especially for efficient aggregation. Depending on the underlying hardware and software architecture, the specifically implemented algorithms, and the data sets used in the experiments, different authors came to different conclusions about which is the better approach. In this paper we argue that in terms of cache efficiency, the two paradigms are actually the same. We support our claim by showing that the complexity of hashing is the same as the complexity of sorting in the external memory model. Furthermore we make the similarity of the two approaches obvious by designing an algorithmic framework that allows to switch seamlessly between hashing and sorting during execution. The fact that we mix hashing and sorting routines in the same algorithmic framework allows us to leverage the advantages of both approaches and makes their similarity obvious. On a more practical note, we also show how to achieve very low constant factors by tuning both the hashing and the sorting routines to modern hardware. Since we observe a complementary dependency of the constant factors of the two routines to the locality of the input, we exploit our framework to switch to the faster routine where appropriate. The result is a novel relational aggregation algorithm that is cache-efficient---independently and without prior knowledge of input skew and output cardinality---, highly parallelizable on modern multi-core systems, and operating at a speed close to the memory bandwidth, thus outperforming the state-of-the-art by up to 3.7x.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068635149",
                    "name": "Ingo M\u00fcller"
                },
                {
                    "authorId": "144376533",
                    "name": "P. Sanders"
                },
                {
                    "authorId": "1735329",
                    "name": "Arnaud Lacurie"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1708337",
                    "name": "Franz F\u00e4rber"
                }
            ]
        },
        {
            "paperId": "465a5a66092127a902c1d05ccce76a51746ef156",
            "title": "From Static to Agile - Interactive Particle Physics Analysis in the SAP HANA DB",
            "abstract": "In order to confirm their theoretical assumptions, physicists employ Monte-Carlo generators to produce millions \n \nof simulated particle collision events and compare them with the results of the detector experiments. The \n \ntraditional, static analysis workflow of physicists involves creating and compiling a C++ program for each \n \nstudy, and loading large data files for every run of their program. To make this process more interactive and \n \nagile, we created an application that loads the data into the relational in-memory column store DBMS SAP \n \nHANA, exposes raw particle data as database views and offers an interactive web interface to explore this data. \n \nWe expressed common particle physics analysis algorithms using SQL queries to benefit from the inherent \n \nscalability and parallelization of the DBMS. In this paper we compare the two approaches, i.e. manual analysis \n \nwith C++ programs and interactive analysis with SAP HANA. We demonstrate the tuning of the physical \n \ndatabase schema and the SQL queries used for the application. Moreover, we show the web-based interface that \n \nallows for interactive analysis of the simulation data generated by the EPOS Monte-Carlo generator, which is \n \ndeveloped in conjunction with the ALICE experiment at the Large Hadron Collider (LHC), CERN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2132632",
                    "name": "D. Kernert"
                },
                {
                    "authorId": "1766564",
                    "name": "Norman May"
                },
                {
                    "authorId": "101372754",
                    "name": "M. Hladik"
                },
                {
                    "authorId": "97777953",
                    "name": "K. Werner"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "50ea3b8009a1c69263068064415bd0879c6ce364",
            "title": "Considering User Intention in Differential Graph Queries",
            "abstract": "Empty answers are a major problem by processing pattern matching queries in graph databases. Especially, there can be multiple reasons why a query failed. To support users in such situations, differential queries can be used that deliver missing parts of a graph query. Multiple heuristics are proposed for differential queries, which reduce the search space. Although they are successful in increasing the performance, they can discard query subgraphs relevant to a user. To address this issue, the authors extend the concept of differential queries and introduce top-k differential queries that calculate the ranking based on users' preferences and significantly support the users' understanding of query database management systems. A user assigns relevance weights to elements of a graph query that steer the search and are used for the ranking. In this paper the authors propose different strategies for selection of relevance weights and their propagation. As a result, the search is modelled along the most relevant paths. The authors evaluate their solution and both strategies on the DBpedia data graph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145410888",
                    "name": "E. Vasilyeva"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "1698512",
                    "name": "Christof Bornh\u00f6vd"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "54248c32b823d0cb2db083852f3785e294889928",
            "title": "GraphVista: Interactive Exploration Of Large Graphs",
            "abstract": "The potential to gain business insights from graph-structured data through graph analytics is increasingly attracting companies from a variety of industries, ranging from web companies to traditional enterprise businesses. To analyze a graph, a user often executes isolated graph queries using a dedicated interface---a procedural graph programming interface or a declarative graph query language. The results are then returned and displayed using a specific visualization technique. This follows the classical ad-hoc Query$\\rightarrow$Result interaction paradigm and often requires multiple query iterations until an interesting aspect in the graph data is identified. This is caused on the one hand by the schema flexibility of graph data and on the other hand by the intricacies of declarative graph query languages. To lower the burden for the user to explore an unknown graph without prior knowledge of a graph query language, visual graph exploration provides an effective and intuitive query interface to navigate through the graph interactively. \nWe demonstrate GRAPHVISTA, a graph visualization and exploration tool that can seamlessly combine ad-hoc querying and interactive graph exploration within the same query session. In our demonstration, conference attendees will see GRAPHVISTA running against a large real-world graph data set. They will start by identifying entry points of interest with the help of ad-hoc queries and will then discover the graph interactively through visual graph exploration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2587068",
                    "name": "M. Paradies"
                },
                {
                    "authorId": "2053142642",
                    "name": "Michael Rudolf"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "564ef76fb210b7aa7b852bd431f9849efc8daa7b",
            "title": "Building the Dresden Web Table Corpus: A Classification Approach",
            "abstract": "In recent years, researchers have recognized relational tables on the Web as an important source of information. To assist this research we developed the Dresden Web Tables Corpus (DWTC), a collection of about 125 million data tables extracted from the Common Crawl (CC) which contains 3.6 billion web pages and is 266TB in size. As the vast majority of HTML tables are used for layout purposes and only a small share contains genuine tables with different surface forms, accurate table detection is essential for building a large-scale Web table corpus. Furthermore, correctly recognizing the table structure (e.g. horizontal listings, matrices) is important in order to understand the role of each table cell, distinguishing between label and data cells. In this paper, we present an extensive table layout classification that enables us to identify the main layout categories of Web tables with very high precision. We therefore identify and develop a plethora of table features, different feature selection techniques and several classification algorithms. We evaluate the effectiveness of the selected features and compare the performance of various state-of-the-art classification algorithms. Finally, the winning approach is employed to classify millions of tables resulting in the Dresden Web Table Corpus (DWTC).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2590616",
                    "name": "Julian Eberius"
                },
                {
                    "authorId": "3176379",
                    "name": "Katrin Braunschweig"
                },
                {
                    "authorId": "39759325",
                    "name": "Markus Hentsch"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "49497567",
                    "name": "Ahmad Ahmadov"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "56eea83908e518eeffe8230eb8272e50f8bd3233",
            "title": "Plan Operator Specialization using Reflective Compiler Techniques",
            "abstract": "Query-specific code generation has become aw ell-established approach to speed up query execution. However, this approach has twom ajor drawbacks: (1) code generators are in general hard to write and maintain, (2) code generators lack the ability to deal with custom operators. To overcome these limitations, we suggest to return to the traditional execution approach with precompiled generic operators which are parametrized and composed to query plans at query compile time. Nevertheless, to optimize such plan operators and speed up their execution, we introduce an ovel specialization approach using reflective compiler techniques. Employing code annotations and an additional compiler pass, we are able to track and replace low-levelload instructions that refer to operator parameters which remain constant during execution time. By dissolving such up-to-nowu nknown constant variables, the compiler can further optimize the code and is able to determine query-specific optimized operators out of generic operator code. In our evaluation, we showthat our approach speeds up the execution of the traditional generic operator approach in terms of execution time without facing the drawbacks of code generators. Due to the tremendous increase in the amount of data managed by today'sdatabase systems , query optimization is still one of the most challenging issues in database research. In disk-based database systems most of the time is spent for waiting on data from disk and thus, the effective CPU computation time is more or less anegligible factor.H owever,the ongoing trend towards in-memory databases shifted the bottleneck from disk access to the main memory access, resulting in ahigher bandwidth and alower latencywhen accessing data objects. Due to these decreased data access times, the effective CPU computation time nowtakes asignificant share of the overall query execution time in such in-memory database systems. Therefore, besides determining an optimal query execution plan, the efficient execution of query plans including all operators attracted attention in the domain of in-memory database systems. The goal of this research is to execute query plans and its operators with less instructions to save as manyCPU cycles as possible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46459866",
                    "name": "Carl-Philip H\u00e4nsch"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "5a1bae409e984970e1a91c8c8beed639f5fe136e",
            "title": "SAP HANA - From Relational OLAP Database to Big Data Infrastructure",
            "abstract": "SAP HANA started as one of the best-performing database engines for OLAP workloads strictly pursuing a main-memory centric ar- chitecture and exploiting hardware developments like large number of cores and main memories in the TByte range. Within this pa- per, we outline the steps from a traditional relational database en- gine to a Big Data infrastructure comprising different methods to handle data of different volume, coming in with different velocity, and showing a fairly large degree of variety. In order to make the presentation of this transformation process more tangible, we dis- cuss two major technical topics-HANA native integration points as well as extension points for collaboration with Hadoop-based data management infrastructures. The overall of goal of this paper is to (a) review current application patterns and resulting technical challenges as well as to (b) paint the big picture for upcoming ar- chitectural designs with SAP HANA database as the core of a SAP Big Data infrastructure.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1766564",
                    "name": "Norman May"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2991909",
                    "name": "P. ShahulHameed"
                },
                {
                    "authorId": "35663146",
                    "name": "Nitesh Maheshwari"
                },
                {
                    "authorId": "2098823676",
                    "name": "Carsten M\u00fcller"
                },
                {
                    "authorId": "40283219",
                    "name": "Sudipto Chowdhuri"
                },
                {
                    "authorId": "2053886348",
                    "name": "Anil K. Goel"
                }
            ]
        },
        {
            "paperId": "64e18c2fd12c209512f1a0be9f0dd963a2512e99",
            "title": "Query processing on low-energy many-core processors",
            "abstract": "Aside from performance, energy efficiency is an increasing challenge in database systems. To tackle both aspects in an integrated fashion, we pursue a hardware/software co-design approach. To fulfill the energy requirement from the hardware perspective, we utilize a low-energy processor design offering the possibility to us to place hundreds to millions of chips on a single board without any thermal restrictions. Furthermore, we address the performance requirement by the development of several database-specific instruction set extensions to customize each core, whereas each core does not have all extensions. Therefore, our hardware foundation is a low-energy processor consisting of a high number of heterogeneous cores. In this paper, we introduce our hardware setup on a system level and present several challenges for query processing. Based on these challenges, we describe two implementation concepts and a comparison between these concepts. Finally, we conclude the paper with some lessons learned and an outlook on our upcoming research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2600997",
                    "name": "A. Ungeth\u00fcm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2495306",
                    "name": "Nils Asmussen"
                },
                {
                    "authorId": "1720944",
                    "name": "M. V\u00f6lp"
                },
                {
                    "authorId": "2324833",
                    "name": "Benedikt Noethen"
                },
                {
                    "authorId": "145570132",
                    "name": "G. Fettweis"
                }
            ]
        },
        {
            "paperId": "6e51d35d51b08b252931e8ae4c8b140b11cd0cfb",
            "title": "Column-specific context extraction for web tables",
            "abstract": "Relational Web tables have become an important resource for applications such as factual search and entity augmentation. A major challenge for an automatic identification of relevant tables on the Web is the fact that many of these tables have missing or non-informative column labels. Research has focused largely on recovering the meaning of columns by inferring class labels from the instances using external knowledge bases. The table context, which often contains additional information on the table's content, is frequently considered as an indicator for the general content of a table, but not as a source for column-specific details. In this paper, we propose a novel approach to identify and extract column-specific information from the context of Web tables. In our extraction framework, we consider different techniques to extract directly as well as indirectly related phrases. We perform a number of experiments on Web tables extracted from Wikipedia. The results show that column-specific information extracted using our simple heuristic significantly boost precision and recall for table and column search.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3176379",
                    "name": "Katrin Braunschweig"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "2590616",
                    "name": "Julian Eberius"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "70b6a8df71b46fcedebb1d3635a0a608aa56894f",
            "title": "Modularisierung leichtgewichtiger Kompressionsalgorithmen",
            "abstract": "Im Kontext von In-Memory Datenbanksystemen nehmen leichtgewichtige Kompressionsalgorithmen eine entscheidende Rolle ein, um eine eziente Speicherung und Verarbeitung groser Datenmengen im Hauptspeicher zu realisieren. Verglichen mit klassischen Komprimierungstechniken wie z.B. Human erzielen leichtgewichtige Kompressionsalgorithm en vergleichbare Kompressionsraten aufgrund der Einbeziehung von Kontextwissen und erlauben eine schnellere Kompression und Dekompression. Die Vielfalt der leichtgewichtigen Kompressionsalgorithmen hat in den letzten Jahren zuge",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065333054",
                    "name": "J. Hildebrandt"
                },
                {
                    "authorId": "2072699345",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2059177263",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "7cd150c5e35a65854572159d14b6dab42da0df0e",
            "title": "Local vs. Global Optimization: Operator Placement Strategies in Heterogeneous Environments",
            "abstract": "In several parts of query optimization, like join enumeration or physical operator selection, there is always the question of how much optimization is needed and how large the performance benefits are. In particular, a decision for either global optimization (e.g., during query optimization) or local optimization (during query execution) has to be taken. In this way, heterogeneity in the hardware environment is adding a further optimization aspect while it is yet unknown, how much optimization is actually required for that aspect. Generally, several papers have shown that heterogeneous hardware environments can be used eciently by applying operator placement for OLAP queries. However, whether it is better to apply this placement in a local or global optimization strategy is still an open question. To tackle this challenge, we examine both strategies for a column-store database system in this paper. Aside from describing local and global placement in detail, we conduct an exhaustive evaluation to draw some conclusions. For the global placement strategy, we also propose a novel approach to address the challenge of an exploding search space together with discussing wellknown solutions for improving cardinality estimation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "80a5d4e06df061c4df9de846e5d4c9e9b4185d25",
            "title": "Online Bit Flip Detection for In-Memory B-Trees Live!",
            "abstract": "Hardware vendors constantly decrease the feature sizes of integrated circuits to obtain higher performance and energy efficiency. As a side-effect, integrated circuits \u00b1 like CPUs and main memory \u00b1 become more and more vulnerable to external influences and thus unreliable, which results in increasing numbers of (multi-) bit flips. From a database perspective bit flip errors in main memory will become a major challenge for modern in-memory database systems, which keep all their enterprise data in volatile, unreliable main memory. Existing hardware error control techniques like ECC-DRAM are able to detect and correct memory errors, but their detection and correction capabilities are limited and come along with several downsides. To underline this we heat up RAM live on-site to show possible error rates of future hardware. We previously presented various techniques for the B-Tree \u00b1 as a wide-spread index structure \u00b1 for online error detection and thus increase its overall reliability [Kea14b]. We also show live performance comparisons in terms of throughput and error detection rates between several bit flip detecting B-Tree variants. By that, we demonstrate the tradeoff between detection accuracy and index throughput. Furthermore, we show annotated structural information about the trees like corrupted nodes and inaccessible sub-trees.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2881245",
                    "name": "Till Kolditz"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "87ffc4a3dcfd52a136361a1c10302bfb2781c10b",
            "title": "Towards a Hybrid Imputation Approach Using Web Tables",
            "abstract": "Data completeness is one of the most important data quality dimensions and an essential premise in data analytics. With new emerging Big Data trends such as the data lake concept, which provides a low cost data preparation repository instead of moving curated data into a data warehouse, the problem of data completeness is additionally reinforced. While traditionally the process of filling in missing values is addressed by the data imputation community using statistical techniques, we complement these approaches by using external data sources from the data lake or even the Web to lookup missing values. In this paper we propose a novel hybrid data imputation strategy that, takes into account the characteristics of an incomplete dataset and based on that chooses the best imputation approach, i.e. either a statistical approach such as regression analysis or a Web-based lookup or a combination of both. We formalize and implement both imputation approaches, including a Web table retrieval and matching system and evaluate them extensively using a corpus with 125M Web tables. We show that applying statistical techniques in conjunction with external data sources will lead to a imputation system which is robust, accurate, and has high coverage at the same time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49497567",
                    "name": "Ahmad Ahmadov"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "2590616",
                    "name": "Julian Eberius"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1789686",
                    "name": "R. Wrembel"
                }
            ]
        },
        {
            "paperId": "a554fe7c49837e2d2d995e00fd3b62a6ca5650f2",
            "title": "Top-k entity augmentation using consistent set covering",
            "abstract": "Entity augmentation is a query type in which, given a set of entities and a large corpus of possible data sources, the values of a missing attribute are to be retrieved. State of the art methods return a single result that, to cover all queried entities, is fused from a potentially large set of data sources. We argue that queries on large corpora of heterogeneous sources using information retrieval and automatic schema matching methods can not easily return a single result that the user can trust, especially if the result is composed from a large number of sources that user has to verify manually. We therefore propose to process these queries in a Top-k fashion, in which the system produces multiple minimal consistent solutions from which the user can choose to resolve the uncertainty of the data sources and methods used. In this paper, we introduce and formalize the problem of consistent, multi-solution set covering, and present algorithms based on a greedy and a genetic optimization approach. We then apply these algorithms to Web table-based entity augmentation. The publication further includes a Web table corpus with 100M tables, and a Web table retrieval and matching system in which these algorithms are implemented. Our experiments show that the consistency and minimality of the augmentation results can be improved using our set covering approach, without loss of precision or coverage and while producing multiple alternative query results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2590616",
                    "name": "Julian Eberius"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "3176379",
                    "name": "Katrin Braunschweig"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "a6530f5cc3295c79305b874c160794bd0c41ca7d",
            "title": "Resiliency-aware Data Compression for In-memory Database Systems",
            "abstract": "Nowadays, database systems pursuit a main memory-centric architecture, where the entire business-related \n \ndata is stored and processed in a compressed form in main memory. In this case, the performance gain is \n \nmassive because database operations can benefit from its higher bandwidth and lower latency. However, \n \ncurrent main memory-centric database systems utilize general-purpose error detection and correction solutions \n \nto address the emerging problem of increasing dynamic error rate of main memory. The costs of these generalpurpose \n \nmethods dramatically increases with increasing error rates. To reduce these costs, we have to exploit \n \ncontext knowledge of database systems for resiliency. Therefore, we introduce our vision of resiliency-aware \n \ndata compression in this paper, where we want to exploit the benefits of both fields in an integrated approach \n \nwith low performance and memory overhead. In detail, we present and evaluate a first approach using AN \n \nencoding and two different compression schemes to show the potentials and challenges of our vision.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2881245",
                    "name": "Till Kolditz"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "3193053",
                    "name": "Dmitrii Kuvaiskii"
                },
                {
                    "authorId": "145650767",
                    "name": "O. Oleksenko"
                },
                {
                    "authorId": "145494211",
                    "name": "C. Fetzer"
                }
            ]
        },
        {
            "paperId": "a65e31a1ba7d2749e095882215052776711b72e5",
            "title": "Exploiting big data in time series forecasting: A cross-sectional approach",
            "abstract": "Forecasting time series data is an integral component for management, planning and decision making. Following the Big Data trend, large amounts of time series data are available from many heterogeneous data sources in more and more applications domains. The highly dynamic and often fluctuating character of these domains in combination with the logistic problems of collecting such data from a variety of sources, imposes new challenges to forecasting. Traditional approaches heavily rely on extensive and complete historical data to build time series models and are thus no longer applicable if time series are short or, even more important, intermittent. In addition, large numbers of time series have to be forecasted on different aggregation levels with preferably low latency, while forecast accuracy should remain high. This is almost impossible, when keeping the traditional focus on creating one forecast model for each individual time series. In this paper we tackle these challenges by presenting a novel forecasting approach called cross-sectional forecasting. This method is especially designed for Big Data sets with a multitude of time series. Our approach breaks with existing concepts by creating only one model for a whole set of time series and requiring only a fraction of the available data to provide accurate forecasts. By utilizing available data from all time series of a data set, missing values can be compensated and accurate forecasting results can be calculated quickly on arbitrary aggregation levels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "46554189",
                    "name": "Frank Rosenthal"
                }
            ]
        },
        {
            "paperId": "b1e1334cc8ec03b4b198611a7d2196b1827525a8",
            "title": "SpMacho - Optimizing Sparse Linear Algebra Expressions with Probabilistic Density Estimation",
            "abstract": "In the age of statistical and scientific databases, there is an emerging trend of integrating analytical algorithms into database systems. Many of these algorithms are based on linear algebra with large, sparse matrices. However, linear algebra expressions often contain multiplications of more then two matrices. The execution of sparse matrix chains is nontrivial, since the runtime depends on the parenthesization and on physical properties of intermediate results. Our approach targets to overcome the burden for data scientists of selecting appropriate algorithms, matrix storage representations, and execution paths. In this paper, we present a sparse matrix chain optimizer (SpMachO) that creates an execution plan, which is composed of multiplication operators and transformations between sparse and dense matrix storage representations. We introduce a comprehensive cost model for sparse-, dense- and hybrid multiplication kernels. Moreover, we propose a sparse matrix product density estimator (SpProdest) for intermediate result matrices. We evaluated SpMachO and SpProdest using real-world matrices and random matrix chains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2132632",
                    "name": "D. Kernert"
                },
                {
                    "authorId": "2064738044",
                    "name": "F. K\u00f6hler"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "b5e86f72db56d61ca3554d0f3aa794f48c3a4419",
            "title": "Big Data-Zentren - Vorstellung und Panel",
            "abstract": "Zur Erforschung der verschiedenen Facetten von \u201dBig Data\u201c wurden jungst drei Zentren gegrundet. Hierbei handelt es sich um die vom Bundesministerium fur Bildung und Forschung (BMBF) geforderten Kompetenzzentren BBDC (Berlin Big Data Center, Leitung TU Berlin) und ScaDS (Competence Center for Scalable Data Services and Solutions, Leitung TU Dresden und Uni Leipzig) sowie das in Zusammenarbeit von Industrie und Forschung eingerichtete SDIL (Smart Data Innovation Lab, Leitung KIT). Diese drei Zentren werden zunachst in Kurzvortragen vorgestellt. Eine sich anschliesende PanelDiskussion arbeitet Gemeinsamkeiten, spezifische Anspruche und Kooperationsmoglichkeiten heraus.",
            "fieldsOfStudy": [
                "Computer Science",
                "Political Science"
            ],
            "authors": [
                {
                    "authorId": "2067222755",
                    "name": "Volker Markl"
                },
                {
                    "authorId": "2064926840",
                    "name": "Erhard Rahm"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2065264360",
                    "name": "Michael Beigl"
                },
                {
                    "authorId": "1732690",
                    "name": "T. Seidl"
                }
            ]
        },
        {
            "paperId": "b73f3ec2bf87edbc2ab0fc5b95011f42106df970",
            "title": "Towards a web-scale data management ecosystem demonstrated by SAP HANA",
            "abstract": "Over the years, data management has diversified and moved into multiple directions, mainly caused by a significant growth in the application space with different usage patterns, a massive change in the underlying hardware characteristics, and-last but not least-growing data volumes to be processed. A solution matching these constraints has to cope with a multidimensional problem space including techniques dealing with a large number of domain-specific data types, data and consistency models, deployment scenarios, and processing, storage, and communication infrastructures on a hardware level. Specialized database engines are available and are positioned in the market optimizing a particular dimension on the one hand while relaxing other aspects (e.g. web-scale deployment with relaxed consistency). Today it is common sense, that there is no single engine which can handle all the different dimensions equally well and therefore we have very good reasons to tackle this problem and optimize the dimensions with specialized approaches in a first step. However, we argue for a second step (reflecting in our opinion on the even harder problem) of a deep integration of individual engines into a single coherent and consistent data management ecosystem providing not only shared components but also a common understanding of the overall business semantics. More specifically, a data management ecosystem provides common \u201cinfrastructure\u201d for software and data life cycle management, backup/recovery, replication and high availability, accounting and monitoring, and many other operational topics, where administrators and users expect a harmonized experience. More importantly from an application perspective however, customer experience teaches us to provide a consistent business view across all different components and the ability to seamlessly combine different capabilities. For example, within recent customer-based Internet of Things scenarios, a huge potential exists in combining graph-processing functionality with temporal and geospatial information and keywords extracted from high-throughput twitter streams. Using SAP HANA as the running example, we want to demonstrate what moving a set of individual engines and infra-structural components towards a holistic but also flexible data management ecosystem could look like. Although there are some solutions for some problems already visible on the horizon, we encourage the database research community in general to focus more on the Big Picture providing a holistic/integrated approach to efficiently deal with different types of data, with different access methods, and different consistency requirements-research in this field would push the envelope far beyond the traditional notion of data management.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708337",
                    "name": "Franz F\u00e4rber"
                },
                {
                    "authorId": "37664530",
                    "name": "J. Dees"
                },
                {
                    "authorId": "2097023626",
                    "name": "Martin Weidner"
                },
                {
                    "authorId": "2631971",
                    "name": "Stefan B\u00e4uerle"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "cf855ba4a09c2181d0166705717b5788454fcfa5",
            "title": "Towards Scalable Real-time Analytics: An Architecture for Scale-out of OLxP Workloads",
            "abstract": "We present an overview of our work on the SAP HANA Scale-out Extension, a novel distributed database architecture designed to support large scale analytics over real-time data. This platform permits high performance OLAP with massive scale-out capabilities, while concurrently allowing OLTP workloads. This dual capability enables analytics over real-time changing data and allows fine grained user-specified service level agreements (SLAs) on data freshness. We advocate the decoupling of core database components such as query processing, concurrency control, and persistence, a design choice made possible by advances in high-throughput low-latency networks and storage devices. We provide full ACID guarantees and build on a logical timestamp mechanism to provide MVCC-based snapshot isolation, while not requiring synchronous updates of replicas. Instead, we use asynchronous update propagation guaranteeing consistency with timestamp validation. \n \nWe provide a view into the design and development of a large scale data management platform for real-time analytics, driven by the needs of modern enterprise customers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053886348",
                    "name": "Anil K. Goel"
                },
                {
                    "authorId": "32546616",
                    "name": "Jeffrey Pound"
                },
                {
                    "authorId": "2812826",
                    "name": "Nathan Auch"
                },
                {
                    "authorId": "2921307",
                    "name": "P. Bumbulis"
                },
                {
                    "authorId": "2076065883",
                    "name": "Scott MacLean"
                },
                {
                    "authorId": "1708337",
                    "name": "Franz F\u00e4rber"
                },
                {
                    "authorId": "2388284",
                    "name": "Francis Gropengie\u00dfer"
                },
                {
                    "authorId": "30004169",
                    "name": "Christian Mathis"
                },
                {
                    "authorId": "50386129",
                    "name": "Thomas Bodner"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "db256e22623b148c1d490e9e5a7b36492e8f5d49",
            "title": "Instant Recovery for Main Memory Databases",
            "abstract": "With the emergence of new hardware technologies, new opportunities arise and existing database architectures have to be rethought to fully exploit them. In particular, recovery mechanisms of current main-memory database systems are tuned to efficiently work on block-oriented, high-latency storage devices. These devices create a bottleneck during transaction processing. In this paper, we investigate the opportunities given by the upcoming Storage Class Memory (SCM) technology for database system recovery mechanisms. In contrast to traditional block-oriented devices, SCM is byte-addressable and offers a latency close to that of DRAM. We propose a novel main-memory database architecture that directly operates in SCM, eliminates the need for logging mechanisms, and provides a way to trade recovery time with the overall query performance. We implemented these concepts in our prototype SOFORT. Our evaluation shows that we are able to achieve instant recovery of the DBMS while removing the need for transaction rollbacks after failure.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2742360",
                    "name": "Ismail Oukid"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "3250632",
                    "name": "Thomas Willhalm"
                },
                {
                    "authorId": "2921307",
                    "name": "P. Bumbulis"
                }
            ]
        },
        {
            "paperId": "e81ab87b6b75860eec0c85f6373b4b47fc855cad",
            "title": "Enjoy FRDM - play with a schema-flexible RDBMS",
            "abstract": "Relational database management systems build on the closed world assumption requiring upfront modeling of a usually stable schema. However, a growing number of today's database applications are characterized by self-descriptive data. The schema of self-descriptive data is very dynamic and prone to frequent changes; a situation which is always troublesome to handle in relational systems. This demo presents the relational database management system FRDM. With flexible relational tables FRDM greatly simplifies the management of self-descriptive data in a relational database system. Self-descriptive data can reside directly next to traditionally modeled data and both can be queried together using SQL. This demo presents the various features of FRDM and provides first-hand experience of the newly gained freedom in relational database systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "f83e9c45cf3891ff4ec2912765ce9ee0aa2c5c3d",
            "title": "Message from the ICDE 2015 Program Committee and general chairs",
            "abstract": "Since its inception in 1984, the IEEE International Conference on Data Engineering (ICDE) has become a premier forum for the exchange and dissemination of data management research results among researchers, users, practitioners, and developers. Continuing this long-standing tradition, the 31st ICDE will be hosted this year in Seoul, South Korea, from April 13 to April 17, 2015. It is our great pleasure to welcome you to ICDE 2015 and to present its proceedings to you.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143614516",
                    "name": "J. Gehrke"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "144232630",
                    "name": "Kyuseok Shim"
                },
                {
                    "authorId": "2237996",
                    "name": "S. Cha"
                },
                {
                    "authorId": "1793770",
                    "name": "G. Lohman"
                }
            ]
        },
        {
            "paperId": "03b179226ddb944b1e67a5062508650b59ebb4fc",
            "title": "ERIS live: a NUMA-aware in-memory storage engine for tera-scale multiprocessor systems",
            "abstract": "The ever-growing demand for more computing power forces hardware vendors to put an increasing number of multiprocessors into a single server system, which usually exhibits a non-uniform memory access (NUMA). In-memory database systems running on NUMA platforms face several issues such as the increased latency and the decreased bandwidth when accessing remote main memory. To cope with these NUMA-related issues, a DBMS has to allow flexible data partitioning and data placement at runtime. In this demonstration, we present ERIS, our NUMA-aware in-memory storage engine. ERIS uses an adaptive partitioning approach that exploits the topology of the underlying NUMA platform and significantly reduces NUMA-related issues. We demonstrate throughput numbers and hardware performance counter evaluations of ERIS and a NUMA-unaware index for different workloads and configurations. All experiments are conducted on a standard server system as well as on a system consisting of 64 multiprocessors, 512 cores, and 8 TBs main memory.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32019086",
                    "name": "Tim Kiefer"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2863294",
                    "name": "Daniel Molka"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "09cb54cad4fd84c24cb40716e2ed4b5a45463c8c",
            "title": "Improving in-memory database index performance with Intel\u00ae Transactional Synchronization Extensions",
            "abstract": "The increasing number of cores every generation poses challenges for high-performance in-memory database systems. While these systems use sophisticated high-level algorithms to partition a query or run multiple queries in parallel, they also utilize low-level synchronization mechanisms to synchronize access to internal database data structures. Developers often spend significant development and verification effort to improve concurrency in the presence of such synchronization. The Intel\u00ae Transactional Synchronization Extensions (Intel\u00ae TSX) in the 4th Generation Core\u2122 Processors enable hardware to dynamically determine whether threads actually need to synchronize even in the presence of conservatively used synchronization. This paper evaluates the effectiveness of such hardware support in a commercial database. We focus on two index implementations: a B+Tree Index and the Delta Storage Index used in the SAP HANA\u00ae database system. We demonstrate that such support can improve performance of database data structures such as index trees and presents a compelling opportunity for the development of simpler, scalable, and easy-to-verify algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "2303659485",
                    "name": "Roman Dementiev"
                },
                {
                    "authorId": "2391490",
                    "name": "Ravi Rajwar"
                },
                {
                    "authorId": "35136689",
                    "name": "K. Lai"
                },
                {
                    "authorId": "1888037",
                    "name": "T. Legler"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "1fb82149f9a70e00188745a85b3d1d2e60626cc9",
            "title": "Energy-Efficient Databases Using Sweet Spot Frequencies",
            "abstract": "Database management systems (DBMS) are typically tuned for high performance and scalability. Nevertheless, carbon footprint and energy efficiency are also becoming increasing concerns. Unfortunately, existing studies mainly present theoretical contributions but fall short on proposing practical techniques. These could be used by administrators or query optimizers to increase the energy efficiency of the DBMS. Thus, this paper explores the effect of so-called sweet spots, which are energy-efficient CPU frequencies, on the energy required to execute queries. From our findings, we derive the Sweet Spot Technique, which relies on identifying energy-efficient sweet spots and the optimal number of threads that minimizes energy consumption for a query or an entire database workload. The technique is simple and has a practical implementation leading to energy savings of up to 50% compared to using the nominal frequency and maximum number of threads.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "11139427",
                    "name": "Sebastian G\u00f6tz"
                },
                {
                    "authorId": "2889090",
                    "name": "Thomas Ilsche"
                },
                {
                    "authorId": "2053183119",
                    "name": "Jorge S. Cardoso"
                },
                {
                    "authorId": "1765470",
                    "name": "Josef Spillner"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "143681675",
                    "name": "U. Assmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1781970",
                    "name": "W. Nagel"
                },
                {
                    "authorId": "145417024",
                    "name": "A. Schill"
                }
            ]
        },
        {
            "paperId": "31317cb2751efab67eda3b20f856726c56c00a4e",
            "title": "Online bit flip detection for in-memory B-trees on unreliable hardware",
            "abstract": "Hardware vendors constantly decrease the feature sizes of integrated circuits to obtain better performance and energy efficiency. Due to cosmic rays, low voltage or heat dissipation, hardware -- both processors and memory -- becomes more and more unreliable as the error rate increases. From a database perspective bit flip errors in main memory will become a major challenge for modern in-memory database systems, which keep all their enterprise data in volatile, unreliable main memory. Although existing hardware error control techniques like ECC-DRAM are able to detect and correct memory errors, their detection and correction capabilities are limited. Moreover, hardware error correction faces major drawbacks in terms of acquisition costs, additional memory utilization, and latency. In this paper, we argue that slightly increasing data redundancy at the right places by incorporating context knowledge already increases error detection significantly. We use the B-Tree -- as a widespread index structure -- as an example and propose various techniques for online error detection and thus increase its overall reliability. In our experiments, we found that our techniques can detect more errors in less time on commodity hardware compared to non-resilient B-Trees running in an ECC-DRAM environment. Our techniques can further be easily adapted for other data structures and are a first step in the direction of resilient database systems which can cope with unreliable hardware.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2881245",
                    "name": "Till Kolditz"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "3c6d63a02008c86ee7c0872e704a00dcfa86906b",
            "title": "Towards Optimal Execution of Density-based Clustering on Heterogeneous Hardware",
            "abstract": "Data Clustering is an important and highly utilized data mining technique in various application domains. With ever increasing data volumes in the era of big data, the efficient execution of clustering algorithms is a fundamental prerequisite to gain understanding and acquire novel, previously unknown knowledge from data. To establish an efficient execution, the clustering algorithms have to be re-engineered to fully exploit the provided hardware capabilities. Shared-memory multiprocessor systems like graphics processing units (GPUs) provide extremely high parallelism combined with a high bandwidth transfer at low cost. The availability of such computing units increases with upcoming processors, where a common CPU and various computing units, like GPU, are tightly coupled using a unified shared memory hierarchy. In this paper, we consider density-based clustering for such heterogeneous systems. In particular, we optimize the configuration of CUDA-DClust - a density-based clustering algorithm for GPUs - and show that our configuration approach enables an efficient and deterministic execution. Our configuration approach is based on data as well as hardware properties, so that we are able to adjust the algorithm execution in both directions. In our evaluation, we show the applicability of our approach and present open challenges which have to be solved next.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "3033342",
                    "name": "Stefanie Gahrig"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "41bda2aba4696243aae3f60a1bd2859eb8238f55",
            "title": "Online horizontal partitioning of heterogeneous data",
            "abstract": "Abstract In an increasing number of use cases, databases face the challenge of managing heterogeneous data. Heterogeneous data is characterized by a quickly evolving variety of entities without a common set of attributes. These entities do not show enough regularity to be captured in a traditional database schema. A common solution is to centralize the diverse entities in a universal table. Usually, this leads to a very sparse table. Although today's techniques allow efficient storage of sparse universal tables, query efficiency is still a problem. Queries that address only a subset of attributes have to read the whole universal table including many irrelevant entities. A solution is to use a partitioning of the table, which allows pruning partitions of irrelevant entities before they are touched. Creating and maintaining such a partitioning manually is very laborious or even infeasible, due to the enormous complexity. Thus an autonomous solution is desirable. In this article, we define the Online Partitioning Problem for heterogeneous data. We sketch how an optimal solution for this problem can be determined based on hypergraph partitioning. Although it leads to the optimal partitioning, the hypergraph approach is inappropriate for an implementation in a database system. We present Cinderella, an autonomous online algorithm for horizontal partitioning of heterogeneous entities in universal tables. Cinderella is designed to keep its overhead low by operating online; it incrementally assigns entities to partition while they are touched anyway during modifications. This enables a reasonable physical database design at runtime instead of static modeling.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40466466",
                    "name": "K. Herrmann"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "479b4996462acc48039d065357af5b209a1f7d0f",
            "title": "A Framework for User-Centered Declarative ETL",
            "abstract": "As business requirements evolve with increasing information density and velocity, there is a growing need for efficiency and automation of Extract-Transform-Load (ETL) processes. Current approaches for the modeling and optimization of ETL processes provide platform-independent optimization solutions for the (semi-)automated transition among different abstraction levels, focusing on cost and performance. However, the suggested representations are not abstract enough to communicate business requirements and the role of the process quality in a user-centered perspective has not yet been adequately examined. In this paper, we introduce a novel methodology for the end-to-end design of ETL processes that takes under consideration both functional and non-functional requirements. Based on existing work, we raise the level of abstraction for the conceptual representation of ETL operations and we show how process quality characteristics can generate specific patterns on the process design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3054847",
                    "name": "V. Theodorou"
                },
                {
                    "authorId": "48843545",
                    "name": "A. Abell\u00f3"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "5460bd8379855877af410256d660f35a060c6f69",
            "title": "SOFORT: a hybrid SCM-DRAM storage engine for fast data recovery",
            "abstract": "Storage Class Memory (SCM) has the potential to significantly improve database performance. This potential has been well documented for throughput [4] and response time [25, 22]. In this paper we show that SCM has also the potential to significantly improve restart performance, a shortcoming of traditional main memory database systems. We present SOFORT, a hybrid SCM-DRAM storage engine that leverages full capabilities of SCM by doing away with a traditional log and updating the persisted data in place in small increments. We show that we can achieve restart times of a few seconds independent of instance size and transaction volume without significantly impacting transaction throughput.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2742360",
                    "name": "Ismail Oukid"
                },
                {
                    "authorId": "2077127667",
                    "name": "Daniel Booss"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2921307",
                    "name": "P. Bumbulis"
                },
                {
                    "authorId": "3250632",
                    "name": "Thomas Willhalm"
                }
            ]
        },
        {
            "paperId": "54bc7784665341d8ae5ec04e060503e2ab66c634",
            "title": "HASHI: An Application Specific Instruction Set Extension for Hashing",
            "abstract": "Hashing is one of the most relevant operations within query processing. Almost all core database operators like groupby, selections, or different join implementations rely on highly efficient hash implementations. In this paper, we present a way to significantly improve performance and energy efficiency of hash operations using specialized instruction set extensions for the Tensilica Xtensa LX5 core. To show the applicability of instruction set extensions, we implemented a bit extraction hashing scheme for 32-bit integer keys as well as the CityHash function for string values. We identify the individual parts of the algorithms required to be optimized, we describe our hashing-specific instruction set, and finally give a comprehensive experimental evaluation. We observed that the hash implementation using the hashing-specific instruction set (1) is up to two orders of magnitudes faster than the basic core without extensions, (2) exhibits always better performance compared to handtuned code running on modern high-end general purpose CPUs, and (3) has a significantly better footprint with respect to energy consumption as well as chip area. Especially the third observation has the potential for a higher packing density and therefore a significantly better overall system performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48730774",
                    "name": "Oliver Arnold"
                },
                {
                    "authorId": "1605950053",
                    "name": "Sebastian Haas"
                },
                {
                    "authorId": "145570132",
                    "name": "G. Fettweis"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "6e85582c72b60d68a608000eec736ef6837ea15c",
            "title": "Quality measures for ETL processes: from goals to implementation",
            "abstract": "Extraction transformation loading (ETL) processes play an increasingly important role for the support of modern business operations. These business processes are centred around artifacts with high variability and diverse lifecycles, which correspond to key business entities. The apparent complexity of these activities has been examined through the prism of business process management, mainly focusing on functional requirements and performance optimization. However, the quality dimension has not yet been thoroughly investigated, and there is a need for a more human\u2010centric approach to bring them closer to business\u2010users requirements. In this paper, we take a first step towards this direction by defining a sound model for ETL process quality characteristics and quantitative measures for each characteristic, based on existing literature. Our model shows dependencies among quality characteristics and can provide the basis for subsequent analysis using goal modeling techniques. We showcase the use of goal modeling for ETL process design through a use case, where we employ the use of a goal model that includes quantitative components (i.e., indicators) for evaluation and analysis of alternative design decisions. Copyright \u00a9 2015 John Wiley & Sons, Ltd.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3054847",
                    "name": "V. Theodorou"
                },
                {
                    "authorId": "48843545",
                    "name": "A. Abell\u00f3"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "6fa0dca9460e7fac20524efce32c6acd7ea8b2b2",
            "title": "ECAST: A Benchmark Framework for Renewable Energy Forecasting Systems",
            "abstract": "The increasing capacities of renewable energy sources and the opportunities emerging from the smart grid technology lead to new challenges for energy forecasters. Energy output fluctuates stronger compared to conventional power produc- tion. More time series data is available through the usage of sensor technology. New supply forecasting approaches are developed to better address those characteristics, but meaningful benchmarks of such solutions are rare. Conduct- ing detailed evaluations is time-intensive and unattractive to customers as this is mostly handwork. We define and discuss requirements for ecient and reliable benchmarks of renewable energy supply forecasting tools. To cope with those requirements, we introduce the automated benchmark framework ECAST as our proposed solution. The system's capability is demonstrated on a real-world scenario compar- ing the performance of di\u21b5erent prediction tools against a naive method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266405",
                    "name": "R. Ulbricht"
                },
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "30389017",
                    "name": "Lars Kegel"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2366481",
                    "name": "Hilko Donker"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "75d5e0a55d1c3b7acc6725fbfafc3a02b6386c22",
            "title": "Report on the second international workshop on energy data management (EnDM 2013)",
            "abstract": "The energy sector is in transition\u2013being forced to rethink the current practice and apply data-management based IT solutions to provide a scalable and sustainable supply and distribution of energy. Novel challenges range from renewable energy production over energy distribution and monitoring to controlling and moving energy consumption. Huge amounts of \u201cBig Energy Data,\u201d i.e., data from smart meters, new renewable energy sources (RES\u2013such as wind, solar, hydro, thermal, etc), novel distributions mechanisms (Smart Grid), and novel types of consumers and devices, e.g., electric cars, are being collected and must be managed and analyzed to yield their potential. Energy is at the top of the worldwide political agenda. For example, The European Union has stated the \u201c2020-20 goals\u201d (20% renewable energy, 20% better energy efficiency, and 20% CO2 reduction by 2020). Even more ambitious goals are set for 2030 and 2050. This situation is reflected in research funding schemes such as the EU Horizon 2020 Framework program as well as national programs. Increasingly, such programs include joint calls involving both energy and IT partners. Data management is at the heart of this development, as witnessed by the following story headlines from key players: \u201cThe Smart Grid Data Deluge\u201d (O\u2019Reilly Radar); \u201cBig data for the Smart Grid\u201d (theenergycollective); \u201cThe Coming Smart Grid Data Surge\u201d (SmartGridNews.com). Thus, data management within the energy domain becomes increasingly important. The International Workshop on Energy Data Management (EnDM) focuses on conceptual and system architecture issues related to the management of very large-scale data sets specifically in the context of the energy domain. The overall goal of the EmDM workshop is a) to bridge the gap between domain experts and data management scientists and b) to create awareness of this emerging and very challenging application area. For the workshop\u2019s research program, the organizers especially try to attract contributions that push the envelope towards novel schemes for large-scale data processing with special focus on energy data management. The Second International Workshop on Energy Data Management (EnDM\u201913) was held in conjunction with",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1731453",
                    "name": "T. Pedersen"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "800ef36302fd4dc947494f06ba01f55dbc555b49",
            "title": "RSQL - a query language for dynamic data types",
            "abstract": "Database Management Systems (DBMS) are used by software applications, to store, manipulate, and retrieve large sets of data. However, the requirements of current software systems pose various challenges to established DBMS. First, most software systems organize their data by means of objects rather than relations leading to increased maintenance, redundancy, and transformation overhead when persisting objects to relational databases. Second, complex objects are separated into several objects resulting in Object Schizophrenia and hard to persist Distributed State. Last but not least, current software systems have to cope with increased complexity and changes. These challenges have lead to a general paradigm shift in the development of software systems. Unfortunately, classical DBMS will become intractable, if they are not adapted to the new requirements imposed by these software systems. As a result, we propose an extension of DBMS with roles to represent complex objects within a relational database and support the flexibility required by current software systems. To achieve this goal, we introduces RSQL, an extension to SQL with the concept of objects playing roles when interacting with other objects. Additionally, we present a formal model for the logical representation of roles in the extended DBMS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21658827",
                    "name": "Tobias J\u00e4kel"
                },
                {
                    "authorId": "1820834438",
                    "name": "Thomas K\u00fchn"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "8a7b25886ee436e2e3654f1d330152d5bb9e7ba7",
            "title": "GraphMCS: Discover the Unknown in Large Data Graphs",
            "abstract": "Graph databases implementing the property graph model provide schema-flexible storage and support complex, expressive queries like shortest path, reachability, and graph isomorphism queries. However, both the flexibility and expressiveness in these queries come with additional costs: queries can result in an unexpected, empty answer. To understand the reason of an empty answer, a user normally has to create alternative queries, which is a cumbersome and time-consuming task. To address this, we introduce di\u21b5-queries, a new kind of graph queries, that give an answer about which part of a query graph is represented in a data graph and which part is missing. We propose a new algorithm for processing di\u21b5-queries, which detects maximum common subgraphs between a query graph and a data graph and computes the di\u21b5erence between them. In addition, we present several extensions and optimizations for an established maximum common subgraph algorithm for processing property graphs, which are the foundation of state of the art graph databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145410888",
                    "name": "E. Vasilyeva"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "1698512",
                    "name": "Christof Bornh\u00f6vd"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "90dfabb00c42231fd762ef93413aa21ae46bb4dd",
            "title": "Dynamic fine-grained scheduling for energy-efficient main-memory queries",
            "abstract": "Power and cooling costs are some of the highest costs in data centers today, which make improvement in energy efficiency crucial. Energy efficiency is also a major design point for chips that power whole ranges of computing devices. One important goal in this area is energy proportionality, arguing that the system's power consumption should be proportional to its performance. Currently, a major trend among server processors, which stems from the design of chips for mobile devices, is the inclusion of advanced power management techniques, such as dynamic voltage-frequency scaling, clock gating, and turbo modes. A lot of recent work on energy efficiency of database management systems is focused on coarse-grained power management at the granularity of multiple machines and whole queries. These techniques, however, cannot efficiently adapt to the frequently fluctuating behavior of contemporary workloads. In this paper, we argue that databases should employ a fine-grained approach by dynamically scheduling tasks using precise hardware models. These models can be produced by calibrating operators under different combinations of scheduling policies, parallelism, and memory access strategies. The models can be employed at run-time for dynamic scheduling and power management in order to improve the overall energy efficiency. We experimentally show that energy efficiency can be improved by up to 4x for fundamental memory-intensive database operations, such as scans.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3175804",
                    "name": "Iraklis Psaroudakis"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "1686933",
                    "name": "Danica Porobic"
                },
                {
                    "authorId": "2889090",
                    "name": "Thomas Ilsche"
                },
                {
                    "authorId": "2548295",
                    "name": "Erietta Liarou"
                },
                {
                    "authorId": "1843945",
                    "name": "P\u0131nar T\u00f6z\u00fcn"
                },
                {
                    "authorId": "1728318",
                    "name": "A. Ailamaki"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "ada1cdff02a7821eb5fc161a5580ecbe91188cd6",
            "title": "Cinderella \u2014 Adaptive online partitioning of irregularly structured data",
            "abstract": "In an increasing number of use cases, databases face the challenge of managing irregularly structured data. Irregularly structured data is characterized by a quickly evolving variety of entities without a common set of attributes. These entities do not show enough regularity to be captured in a traditional database schema. A common solution is to centralize the diverse entities in a universal table. Usually, this leads to a very sparse table. Although today's techniques allow efficient storage of sparse universal tables, query efficiency is still a problem. Queries that reference only a subset of attributes have to read the whole universal table including many irrelevant entities. One possible solution is to use a partitioning of the table, which allows pruning partitions of irrelevant entities before they are touched. Creating and maintaining such a partitioning manually is very laborious or even infeasible, due to the enormous complexity. Thus an autonomous solution is desirable. In this paper, we define the Online Partitioning Problem for irregularly structured data and present Cinderella. Cinderella is an autonomous online algorithm for horizontal partitioning of irregularly structured entities in universal tables. It is designed to keep its overhead low by incrementally assigning entities to partitions while they are touched anyway during modifications. The achieved partitioning allows queries that retrieve only entities with a subset of attributes easily pruning partitions of irrelevant entities. Cinderella increases the locality of queries and reduces query execution cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40466466",
                    "name": "K. Herrmann"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "b79555c9d9818d23cbe83a580b0bd8c7b5163f12",
            "title": "Demonstrating efficient query processing in heterogeneous environments",
            "abstract": "The increasing heterogeneity in hardware systems gives developers many opportunities to add more functionality and computational power to the system. As a consequence, modern database systems will need to be able to adapt to a wide variety of heterogeneous architectures. While porting single operators to accelerator architectures is well-understood, a more generic approach is needed for the whole database system. In prior work, we presented a generic hardware-oblivious database system, where the operators can be executed on the main processor as well as on a large number of accelerator architectures. However, to achieve fully heterogeneous query processing, placement decisions are needed for the database operators. We enhance the presented system with heterogeneity-aware operator placement (HOP) to take a major step towards designing a database system that can efficiently exploit highly heterogeneous hardware environments. In this demonstration, we are focusing on the placement-integration aspect as well as presenting the resulting database system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "47703286",
                    "name": "M. Hille"
                },
                {
                    "authorId": "2053224778",
                    "name": "M. Ludwig"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "3121130",
                    "name": "Max Heimel"
                },
                {
                    "authorId": "1733290",
                    "name": "V. Markl"
                }
            ]
        },
        {
            "paperId": "c5dbe5e1ef758351054fbad48a4fad0413fbb735",
            "title": "SLACID - sparse linear algebra in a column-oriented in-memory database system",
            "abstract": "Scientific computations and analytical business applications are often based on linear algebra operations on large, sparse matrices. With the hardware shift of the primary storage from disc into memory it is now feasible to execute linear algebra queries directly in the database engine. This paper presents and compares different approaches of storing sparse matrices in an in-memory column-oriented database system. We show that a system layout derived from the compressed sparse row representation integrates well with a columnar database design and that the resulting architecture is moreover amenable to a wide range of non-numerical use cases when dictionary encoding is used. Dynamic matrix manipulation operations, like online insertion or deletion of elements, are not covered by most linear algebra frameworks. Therefore, we present a hybrid architecture that consists of a read-optimized main and a write-optimized delta structure and evaluate the performance for dynamic sparse matrix workloads by applying workflows of nuclear science and network graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2132632",
                    "name": "D. Kernert"
                },
                {
                    "authorId": "2064738044",
                    "name": "F. K\u00f6hler"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "cb2dddec0c905282f713d0c19d033855ca9e6bde",
            "title": "An application-specific instruction set for accelerating set-oriented database primitives",
            "abstract": "The key task of database systems is to efficiently manage large amounts of data. A high query throughput and a low query latency are essential for the success of a database system. Lately, research focused on exploiting hardware features like superscalar execution units, SIMD, or multiple cores to speed up processing. Apart from these software optimizations for given hardware, even tailor-made processing circuits running on FPGAs are built to run mostly stateless query plans with incredibly high throughput. A similar idea, which was already considered three decades ago, is to build tailor-made hardware like a database processor. Despite their superior performance, such application-specific processors were not considered to be beneficial because general-purpose processors eventually always caught up so that the high development costs did not pay off. In this paper, we show that the development of a database processor is much more feasible nowadays through the availability of customizable processors. We illustrate exemplarily how to create an instruction set extension for set-oriented database primitives. The resulting application-specific processor provides not only a high performance but it also enables very energy-efficient processing. Our processor requires in various configurations more than 960x less energy than a high-end x86 processor while providing the same performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48730774",
                    "name": "Oliver Arnold"
                },
                {
                    "authorId": "1605950053",
                    "name": "Sebastian Haas"
                },
                {
                    "authorId": "145570132",
                    "name": "G. Fettweis"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "d04d896b115663e0fc4722d44e8c85b21f0eb943",
            "title": "GRATIN: Accelerating Graph Traversals in Main-Memory Column Stores",
            "abstract": "Native graph query and processing capabilities have become indispensable for modern business applications in enterprise-critical operations on data that is stored in relational database management systems. Traversal operations are a basic ingredient of graph algorithms and graph queries. As a consequence, they are fundamental for querying graph data in a relational database management system. In this paper we present gratin, a concise secondary index structure to speedup graph traversals in main-memory column stores. Conventional approaches for graph traversals rely on repeated full column scans, making it an inefficient approach for deep traversals on very large graphs. To tackle this challenge, we devise a novel and adaptive block-based index to handle graphs efficiently. Most importantly, gratin is updateable in constant time and allows supporting evolving graphs with frequent updates to the graph topology. We conducted an extensive evaluation on real-world data sets from different domains for a large variety of traversal queries. Our experiments show improvements of up to an order of magnitude compared to a scan-based traversal algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2587068",
                    "name": "M. Paradies"
                },
                {
                    "authorId": "2053142642",
                    "name": "Michael Rudolf"
                },
                {
                    "authorId": "1698512",
                    "name": "Christof Bornh\u00f6vd"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "e07e63c53585c4109e2e888f132a99bcb66f88a6",
            "title": "Modular Data Clustering - Algorithm Design beyond MapReduce",
            "abstract": "In the context of Big Data, flexible and adjustable data analytics become more and more important, whereas an efficient, scalable and fault-tolerant execution is required as well. To fulfill the flexibility as well as the execution requirements, the specification of the analysis methods have to be in an appropriate and easy adjustable manner. The MapReduce approach has demonstrated that such flexible specification as well as scalable execution is possible and applicable. However, the MapReduce programming model is too generic and complicates the specification from a data analysis point of view. Therefore, we propose a novel programming approach using well-defined modular building blocks for a specific and highly utilized data analysis domain named data clustering in this paper. Our approach offers many advantages: (i) a unified and specific instruction set for data clustering which eases understanding and algorithm adaptation in an abstract way, and (ii) enables an efficient and scalable execution of all data clustering algorithms based on an efficient mapping of the unified instruction set to a specific target environment is possible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "ec02fc78f7ec0d9cbef609f4386143eaa84d4ae5",
            "title": "ERIS: A NUMA-Aware In-Memory Storage Engine for Analytical Workload",
            "abstract": "The ever-growing demand for more computing power forces hardware vendors to put an increasing number of multiprocessors into a single server system, which usually exhibits a non-uniform memory access (NUMA). In-memory database systems running on NUMA platforms face several issues such as the increased latency and the decreased bandwidth when accessing remote main memory. To cope with these NUMA-related issues, NUMA-awareness has to be considered as a major design principle for the fundamental architecture of a database system. In this paper we present ERIS, a NUMA-aware inmemory storage engine that is based on a data-oriented architecture. In contrast to existing approaches that focus on transactional workloads on a disk-based DBMS, ERIS aims at tera-scale analytical workloads that are executed entirely in main memory. ERIS uses an adaptive partitioning approach that exploits the topology of the underlying NUMA platform and significantly reduces NUMA-related issues. We evaluate ERIS on widespread standard server systems as well as on a system consisting of 64 multiprocessors and 512 cores. On these platforms, we achieve a more than linear speedup for index lookups and scalable parallel scan operations that are only limited by the available local bandwidth of the multiprocessor. Moreover, we measured a performance gain of up to 200% (index lookups) respectively 660% (column scans) in the memory-bound case compared to a NUMA-agnostic storage subsystem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "32019086",
                    "name": "Tim Kiefer"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2863294",
                    "name": "Daniel Molka"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "fea79e295090cb5041025e1368122d2d847ee70b",
            "title": "GRAPHITE: an extensible graph traversal framework for relational database management systems",
            "abstract": "Graph traversals are a basic but fundamental ingredient for a variety of graph algorithms and graph-oriented queries. To achieve the best possible query performance, they need to be implemented at the core of a database management system that aims at storing, manipulating, and querying graph data. Increasingly, modern business applications demand native graph query and processing capabilities for enterprise-critical operations on data stored in relational database management systems. In this paper we propose an extensible graph traversal framework (GRAPHITE) as a central graph processing component on a common storage engine inside a relational database management system. We study the influence of the graph topology on the execution time of graph traversals and derive two traversal algorithm implementations specialized for different graph topologies and traversal queries. We conduct extensive experiments on GRAPHITE for a large variety of real-world graph data sets and input configurations. Our experiments show that the proposed traversal algorithms differ by up to two orders of magnitude for different input configurations and therefore demonstrate the need for a versatile framework to efficiently process graph traversals on a wide range of different graph topologies and types of queries. Finally, we highlight that the query performance of our traversal implementations is competitive with those of two native graph database management systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2587068",
                    "name": "M. Paradies"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1698512",
                    "name": "Christof Bornh\u00f6vd"
                }
            ]
        },
        {
            "paperId": "0d05b30fb74b0cf17662c18d47eb0b06f00f54c8",
            "title": "DrillBeyond: Open-World SQL Queries Using Web Tables",
            "abstract": "The Web consists of a huge number of documents, but also large amounts structured information, for example in the form of HTML tables containing relationalstyle data. One typical usage scenario for this kind of data is their integration into a database or data warehouse in order to apply data analytics. However, in today\u2019s business intelligence tools there is an evident lack of support for so-called situational or ad-hoc data integration. In this demonstration we will therefore present DrillBeyond, a novel database and information retrieval engine which allows users to query a local database as well as the web datasets in a seamless and integrated way with standard SQL. The audience will be able to pose queries to our DrillBeyond system which will be answered partly from local data in the database and partly from datasets that originate from the Web of Data. We will demonstrate the integration of the web tables back into the DBMS in order to apply its analytical features. 1 Open-World SQL Queries The system we want to demonstrate offers a novel way of integrating web tables into regular query processing in a relational database. We present a modified RDBMS that is able to answer so-called open-world queries which are not restricted to the schema of the local database. Instead the user is allowed to use arbitrary attribute names that do not appear in the original schema. Consider the following running example query: SELECT p o p u l a t i o n , n_name , AVG( o _ t o t a l p r i c e ) FROM n a t i o n JOIN r e g i o n ON n _ r e g i o n k e y = r _ r e g i o n k e y JOIN c u s t o m e r ON n _ n a t i o n k e y = c _ n a t i o n k e y JOIN o r d e r s ON c _ c u s t k e y = o _ c u s t k e y WHERE r_name = \u2019AMERICA \u2019 GROUP BY p o p u l a t i o n , n_name ORDER BY p o p u l a t i o n The population attribute which is used in the SELECT and ORDER BY clauses is not part of the TPC-H schema and therefore requires special processing. In the DrillBeyond system, missing attributes are translated into keyword queries that are run against an index of open datasets on the web. It will answer the query by substituting the missing attribute",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2590616",
                    "name": "Julian Eberius"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "3176379",
                    "name": "Katrin Braunschweig"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "1678406ae271c6a421510a3cf811e95978dc51ae",
            "title": "Stream Join Processing on Heterogeneous Processors",
            "abstract": "The window-based stream join is an important operator in all data streaming systems. It has often high resource requirements so that many efficient sequential as well as parallel versions of it were proposed in the literature. The parallel stream join operators recently gain increasing interest because hardware is getting more and more parallel. Most of these operators, however, are only optimized for processors with homogeneous execution units (e.g., multi-core processors). Newly available processors with heterogeneous execution units cannot be exploited whereas such processors provide typically a very high peak performance. In this paper, we propose an initial variant of a window-based stream join operator that is optimized for processors with heterogeneous execution units. We provide an efficient load balancing approach to utilize all available execution units of a processor and further provide highly-optimized kernels that run on them. On our test machine with a 4-core CPU and an integrated graphics processor, our operator achieves a speedup of 69.2x compared to our single-threaded implementation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "1889dfb8f55b586c01399a5d81f37f3add41c639",
            "title": "Experimental Evaluation of NUMA Effects on Database Management Systems",
            "abstract": "NUMA systems with multiple CPUs and large main memories are common today. Consequently, database management systems (DBMSs) in data centers are deployed on NUMA systems. They serve a wide range of database use-cases, single large applications having high performance needs as well as many small applications that are consolidated on one machine to save resources and increase utilization. Database servers often show a natural partitioning in the data that is accessed, e.g., caused by multiple applications accessing only their data. Knowledge about these partitions can be used to allocate a database\u2019s memory on the different nodes accordingly: a strategy that increases memory locality and reduces expensive communication between CPUs. In this work, we show that partitioning a database\u2019s memory with respect to the data\u2019s access patterns can improve the query performance by as much as 75%. The allocation strategy is enabled by knowledge that is available only inside the DBMS. Additionally, we show that grouping database worker threads on CPUs, based on their data partitions, improves cache behavior, which in turn improves query performance. We use a self-developed synthetic, low-level benchmark as well as a real database benchmark executed on the MySQL DBMS to verify our hypotheses. We also give an outlook on how our findings can be used to improve future DBMS performance on NUMA systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32019086",
                    "name": "Tim Kiefer"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "1da651bff65512ad17b2e54eb436f3de1911a8d8",
            "title": "Research challenges for energy data management (panel)",
            "abstract": "This panel paper aims at initiating discussion at the Second International Workshop on Energy Data Management (EnDM 2013) about the important research challenges within Energy Data Management. The authors are the panel organizers, extra panelists will be recruited before the workshop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1731453",
                    "name": "T. Pedersen"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "1fcc9f235e98d9cbf9e9a18f59b3e9708ad9e4cf",
            "title": "Leveraging flexible data management with graph databases",
            "abstract": "Integrating up-to-date information into databases from different heterogeneous data sources is still a time-consuming and mostly manual job that can only be accomplished by skilled experts. For this reason, enterprises often lack information regarding the current market situation, preventing a holistic view that is needed to conduct sound data analysis and market predictions. Ironically, the Web consists of a huge and growing number of valuable information from diverse organizations and data providers, such as the Linked Open Data cloud, common knowledge sources like Freebase, and social networks. One desirable usage scenario for this kind of data is its integration into a single database in order to apply data analytics. However, in today's business intelligence tools there is an evident lack of support for so-called situational or ad-hoc data integration. What we need is a system which 1) provides a flexible storage of heterogeneous information of different degrees of structure in an ad-hoc manner, and 2) supports mass data operations suited for data analytics. In this paper, we will provide our vision of such a system and describe an extension of the well-studied property graph model that allows to \"integrate and analyze as you go\" external data exposed in the RDF format in a seamless manner. The proposed integration approach extends the internal graph model with external data from the Linked Open Data cloud, which stores over 31 billion RDF triples (September 2011) from a variety of domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145410888",
                    "name": "E. Vasilyeva"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "1698512",
                    "name": "Christof Bornh\u00f6vd"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "360cbec96bccda6e225cc7e2193a870cc280173f",
            "title": "SAP HANA: The Evolution from a Modern Main-Memory Data Platform to an Enterprise Application Platform",
            "abstract": "SAP HANA is a pioneering, and one of the best performing, data platform designed from the grounds up to heavily exploit modern hardware capabilities, including SIMD, and large memory and CPU footprints. As a comprehensive data management solution, SAP HANA supports the complete data life cycle encompassing modeling, provisioning, and consumption. This extended abstract outlines the vision and planned next step of the SAP HANA evolution growing from a core data platform into an innovative enterprise application platform as the foundation for current as well as novel business applications in both on-premise and on-demand scenarios. We argue that only a holistic system design rigorously applying co-design at different levels may yield a highly optimized and sustainable platform for modern enterprise applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2613970",
                    "name": "Vishal Sikka"
                },
                {
                    "authorId": "1708337",
                    "name": "Franz F\u00e4rber"
                },
                {
                    "authorId": "2053886348",
                    "name": "Anil K. Goel"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "419f0b6979c4b952b104db7c91ce267a6750f4e8",
            "title": "pcApriori: scalable apriori for multiprocessor systems",
            "abstract": "Frequent-itemset mining is an important part of data mining. It is a computational and memory intensive task and has a large number of scientific and statistical application areas. In many of them, the datasets can easily grow up to tens or even several hundred gigabytes of data. Hence, efficient algorithms are required to process such amounts of data. In the recent years, there have been proposed many efficient sequential mining algorithms, which however cannot exploit current and future systems providing large degrees of parallelism. Contrary, the number of parallel frequent-itemset mining algorithms is rather small and most of them do not scale well as the number of threads is largely increased. In this paper, we present a highly-scalable mining algorithm that is based on the well-known Apriori algorithm; it is optimized for processing very large datasets on multiprocessor systems. The key idea of pcApriori is to employ a modified producer--consumer processing scheme, which partitions the data during processing and distributes it to the available threads. We conduct many experiments on large datasets. pcApriori scales almost linear on our test system comprising 32 cores.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "32019086",
                    "name": "Tim Kiefer"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "44eb5692fddc4fce7f31bc40f286ef0ea1d79ee8",
            "title": "SMIX: self-managing indexes for dynamic workloads",
            "abstract": "As databases accumulate growing amounts of data at an increasing rate, adaptive indexing becomes more and more important. At the same time, applications and their use get more agile and flexible, resulting in less steady and less predictable workload characteristics. Being inert and coarse-grained, state-of-the-art index tuning techniques become less useful in such environments. Especially the full-column indexing paradigm results in many indexed but never queried records and prohibitively high storage and maintenance costs. In this paper, we present Self-Managing Indexes, a novel, adaptive, fine-grained, autonomous indexing infrastructure. In its core, our approach builds on a novel access path that automatically collects useful index information, discards useless index information, and competes with its kind for resources to host its index information. Compared to existing technologies for adaptive indexing, we are able to dynamically grow and shrink our indexes, instead of incrementally enhancing the index granularity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "4c2ce39145ca7340cdab108ca3fc6815baf80e51",
            "title": "Pack Indexing for Time-Constrained In-Memory Query Processing",
            "abstract": "Main memory databases management systems are used more often and in a wide spread of application scenarios. To take significant advantage of the main memory read performance, most techniques known from traditional disk-centric database systems have to be adapted and re-designed. In the field of indexing, many mainmemory-optimized index structures have been proposed. Most of these works aim at primary indexing. Secondary indexes are rarely considered in the context of main memory databases. Either query performance is sufficiently good without secondary indexing or main memory is a resource too scarce to invest in huge secondary indexes. A more subtle trade between benefit and costs of secondary indexing has not been considered so far. In this paper we present Pack Indexing, a secondary indexing technique for main memory databases that allows a precise trade-off between the benefit in query execution time gained with a secondary index and main memory invested for that index. Compared to traditional indexing, Pack Indexing achieves this by varying the granularity of indexing. We discuss the Pack Indexing concept in detail and describe how the concept can be implemented. To demonstrate the usefulness and the effectiveness of our approach, we present several experiments with different datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21658827",
                    "name": "Tobias J\u00e4kel"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "55950b833e8b325f1ff5ec8e844b417387a81973",
            "title": "Forecasting the data cube: A model configuration advisor for multi-dimensional data sets",
            "abstract": "Forecasting time series data is crucial in a number of domains such as supply chain management and display advertisement. In these areas, the time series data to forecast is typically organized along multiple dimensions leading to a high number of time series that need to be forecasted. Most current approaches focus only on selection and optimizing a forecast model for a single time series. In this paper, we explore how we can utilize time series at different dimensions to increase forecast accuracy and, optionally, reduce model maintenance overhead. Solving this problem is challenging due to the large space of possibilities and possible high model creation costs. We propose a model configuration advisor that automatically determines the best set of models, a model configuration, for a given multi-dimensional data set. Our approach is based on a general process that iteratively examines more and more models and simultaneously controls the search space depending on the data set, model type and available hardware. The final model configuration is integrated into F2DB, an extension of PostgreSQL, that processes forecast queries and maintains the configuration as new data arrives. We comprehensively evaluated our approach on real and synthetic data sets. The evaluation shows that our approach significantly increases forecast query accuracy while ensuring low model costs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "108111460",
                    "name": "Chris Schildt"
                },
                {
                    "authorId": "31715175",
                    "name": "Claudio Hartmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "6db952f15a5df759949902d21accb2e2b75d0f58",
            "title": "Optimized renewable energy forecasting in local distribution networks",
            "abstract": "The integration of renewable energy sources (RES) into local energy distribution networks becomes increasingly important. Renewable energy highly depends on weather conditions, making it difficult to maintain stability in such networks. To still enable efficient planning and balancing, forecasts of energy supply are essential. However, typical distribution networks contain a variety of heterogeneous RES installations (e.g. wind, solar, water), each providing different characteristics and weather dependencies. Additionally, advanced meters, which allow the communication of final-granular production curves to the network operator, are not available at all RES sites. Despite these heterogeneities and missing measurements, reliable forecasts over the whole local distribution network have to be provided. This poses high challenges on choosing the right input parameters, statistical models and forecasting granularity (e.g. single RES installations vs. aggregated data). In this paper, we will discuss such problems in energy supply forecasting using a real-world scenario. Subsequently, we introduce our idea of a generalized optimization approach that determines the best forecasting strategy for a given scenario and sketch research challenges we are planning to investigate in future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266405",
                    "name": "R. Ulbricht"
                },
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2366481",
                    "name": "Hilko Donker"
                }
            ]
        },
        {
            "paperId": "7e6e641e17568d9fd3e1e48b64ebac5ec902ee49",
            "title": "The HELLS-join: a heterogeneous stream join for extremely large windows",
            "abstract": "Upcoming processors are combining different computing units in a tightly-coupled approach using a unified shared memory hierarchy. This tightly-coupled combination leads to novel properties with regard to cooperation and interaction. This paper demonstrates the advantages of those processors for a stream-join operator as an important data-intensive example. In detail, we propose our HELLS-Join approach employing all heterogeneous devices by outsourcing parts of the algorithm on the appropriate device. Our HELLS-Join performs better than CPU stream joins, allowing wider time windows, higher stream frequencies, and more streams to be joined as before.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "83d9254379c982c4aa4ab47e26a1887c8fbab870",
            "title": "Enhancing Named Entity Extraction by Effectively Incorporating the Crowd",
            "abstract": "Named entity extraction is an established research area in the field of information extraction. When tailored to a specific domain and with sufficient pre-labeled training data, state-of-the-art extraction algorithms have achieved near human performance. However, when presented with semi-structured data, informal text or unknown domains where training data is not available, extraction results can deteriorate significantly. Recent research has focused on crowdsourcing as an alternative to automatic named entity extraction or as a tool to generate the required training data. While humans easily adapt to semi-structured data and informal style, a crowd-based approach also introduces new issues due to monetary costs or spamming. We address these issues by combining automatic named entity extraction algorithms with crowdsourcing into a hybrid approach. We have conducted a wide range of experiments on real world data to identify a set of subtasks or operators, that can be performed either by the crowd or automatically. Results show that a meaningful combination of these operators into complex processing pipelines can significantly enhance the quality of named entity extraction in challenging scenarios, while at the same time reducing the monetary costs of crowdsourcing and the risk of misuse.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3176379",
                    "name": "Katrin Braunschweig"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "2590616",
                    "name": "Julian Eberius"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "8d34349148ab58a09cda483b31f84844291c21f4",
            "title": "Energy-efficient in-memory database computing",
            "abstract": "The efficient and flexible management of large datasets is one of the core requirements of modern business applications. Having access to consistent and up-to-date information is the foundation for operational, tactical, and strategic decision making. Within the last few years, the database community sparked a large number of extremely innovative research projects to push the envelope in the context of modern database system architectures. In this paper, we outline requirements and influencing factors to identify some of the hot research topics in database management systems. We argue that\u2014even after 30 years of active database research\u2014the time is right to rethink some of the core architectural principles and come up with novel approaches to meet the requirements of the next decades in data management. The sheer number of diverse and novel (e.g., scientific) application areas, the existence of modern hardware capabilities, and the need of large data centers to become more energy-efficient will be the drivers for database research in the years to come.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "968b986cb149eec3bd9fc667ba4946e095defa2f",
            "title": "BUZZARD: a NUMA-aware in-memory indexing system",
            "abstract": "With the availability of large main memory capacities, in-memory index structures have become an important component of modern data management platforms. Current research even suggests index-based query processing as an alternative or supplement for traditional tuple-at-a-time processing models. However, while simple sequential scan operations can fully exploit the high bandwidth provided by main memory, indexes are mainly latency bound and spend most of their time waiting for memory accesses.\n Considering current hardware trends, the problem of high memory latency is further exacerbated as modern shared-memory multiprocessors with non-uniform memory access (NUMA) become increasingly common. On those NUMA platforms, the execution time of index operations is dominated by memory access latency that increases dramatically when accessing memory on remote sockets. Therefore, good index performance can only be achieved through careful optimization of the index structure to the given topology.\n BUZZARD is a NUMA-aware in-memory indexing system. Using adaptive data partitioning techniques, BUZZARD distributes a prefix-tree-based index across the NUMA system and hands off incoming requests to worker threads located on each partition's respective NUMA node. This approach reduces the number of remote memory accesses to a minimum and improves cache utilization. In addition, all indexes inside BUZZARD are only accessed by their respective owner, eliminating the need for synchronization primitives like compare-and-swap.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32544076",
                    "name": "Lukas M. Maas"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "ae3b0c99e81635b44af6078ffc00a526f2158b24",
            "title": "QPPT: Query Processing on Prefix Trees",
            "abstract": "Modern database systems have to process huge amounts of data and should provide results with low latency at the same time. To achieve this, data is nowadays typically hold completely in main memory, to benefit of its high bandwidth and low access latency that could never be reached with disks. Current in-memory databases are usually columnstores that exchange columns or vectors between operators and suer from a high tuple reconstruction overhead. In this paper, we present the indexed table-at-a-time processing model that makes indexes the first-class citizen of the database system. The processing model comprises the concepts of intermediate indexed tables and cooperative operators, which make indexes the common data exchange format between plan operators. To keep the intermediate index materialization costs low, we employ optimized prefix trees that oer a balanced read/write performance. The indexed tableat-a-time processing model allows the ecient construction of composed operators like the multi-way-select-join-group. Such operators speed up the processing of complex OLAP queries so that our approach outperforms state-of-the-art in-memory databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "b0c903bd6bbca2e2ff12570203222610e1416a4c",
            "title": "pEDM: online-forecasting for smart energy analytics",
            "abstract": "Continuous balancing of energy demand and supply is a fundamental prerequisite for the stability of energy grids and requires accurate forecasts of electricity consumption and production at any point in time. Today's Energy Data Management (EDM) systems already provide accurate predictions, but typically employ a very time-consuming and inflexible forecasting process. However, emerging trends such as intra-day trading and an increasing share of renewable energy sources need a higher forecasting efficiency. Additionally, the wide variety of applications in the energy domain pose different requirements with respect to runtime and accuracy and thus, require flexible control of the forecasting process. To solve this issue, we introduce our novel online forecasting process as part of our EDM system called pEDM. The online forecasting process rapidly provides forecasting results and iteratively refines them over time. Thus, we avoid long calculation times and allow applications to adapt the process to their needs. Our evaluation shows that our online forecasting process offers a very efficient and flexible way of providing forecasts to the requesting applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2715207",
                    "name": "Lars Dannecker"
                },
                {
                    "authorId": "2932139",
                    "name": "Philipp J. R\u00f6sch"
                },
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "2296355",
                    "name": "Gordon Gaumnitz"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "3068811",
                    "name": "Gregor Hackenbroich"
                }
            ]
        },
        {
            "paperId": "b7bcbec4f3991c7ccec7fcd129c21a052c7284b0",
            "title": "SAP HANA distributed in-memory database system: Transaction, session, and metadata management",
            "abstract": "One of the core principles of the SAP HANA database system is the comprehensive support of distributed query facility. Supporting scale-out scenarios was one of the major design principles of the system from the very beginning. Within this paper, we first give an overview of the overall functionality with respect to data allocation, metadata caching and query routing. We then dive into some level of detail for specific topics and explain features and methods not common in traditional disk-based database systems. In summary, the paper provides a comprehensive overview of distributed query processing in SAP HANA database to achieve scalability to handle large databases and heterogeneous types of workloads.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2784494",
                    "name": "Juchang Lee"
                },
                {
                    "authorId": "2382269",
                    "name": "Y. Kwon"
                },
                {
                    "authorId": "1708337",
                    "name": "Franz F\u00e4rber"
                },
                {
                    "authorId": "2103914839",
                    "name": "Michael Muehle"
                },
                {
                    "authorId": "2118658862",
                    "name": "Chulwon Lee"
                },
                {
                    "authorId": "2010879",
                    "name": "Christian Bensberg"
                },
                {
                    "authorId": "2108648932",
                    "name": "Joo-Yeon Lee"
                },
                {
                    "authorId": "7728591",
                    "name": "Arthur H. Lee"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "be055e407b6314e9bc6c31713398cfcf2f8e90d3",
            "title": "Query processing on prefix trees live",
            "abstract": "Modern database systems have to process huge amounts of data and should provide results with low latency at the same time. To achieve this, data is nowadays typically hold completely in main memory, to benefit of its high bandwidth and low access latency that could never be reached with disks. Current in-memory databases are usually column-stores that exchange columns or vectors between operators and suffer from a high tuple reconstruction overhead. In this demonstration proposal, we present DexterDB, which implements our novel prefix tree-based processing model that makes indexes the first-class citizen of the database system. The core idea is that each operator takes a set of indexes as input and builds a new index as output that is indexed on the attribute requested by the successive operator. With that, we are able to build composed operators, like the multi-way-select-join-group. Such operators speed up the processing of complex OLAP queries so that DexterDB outperforms state-of-the-art in-memory databases. Our demonstration focuses on the different optimization options for such query plans. Hence, we built an interactive GUI that connects to a DexterDB instance and allows the manipulation of query optimization parameters. The generated query plans and important execution statistics are visualized to help the visitor to understand our processing model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "c72b7197ebf03b747e73a4c0e4899c30a65ee81d",
            "title": "Rethinking Energy Data Management: Trends and Challenges in Today's Transforming Markets",
            "abstract": "The energy market domain is subject to a continuous transformation process, mostly driven by governmental regulations. To efficiently handle the large amounts of data and the communication processes between market participants, specialized database applications have been developed. In this paper, we present the energy data management system (EDMS) as a standard software solution, describing its core components and typical system integration aspects. However, current market topics like smart metering, energy saving, forecasting for renewable energy sources, mobile consumption and smart grids lead to new database challenges. We provide an overview of these trends and discuss their impact on existing information systems, focusing on the technical challenges of data integration, data storage, data analytics and scalability. As energy data management has to match those new requirements, promising research opportunities are offered to the database community.",
            "fieldsOfStudy": [
                "Computer Science",
                "Political Science"
            ],
            "authors": [
                {
                    "authorId": "2266405",
                    "name": "R. Ulbricht"
                },
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2366481",
                    "name": "Hilko Donker"
                }
            ]
        },
        {
            "paperId": "d14fedae7e1fe90704c8a8dfbf99d37d3310f2d7",
            "title": "Report on the first international workshop on energy data management (EnDM 2012)",
            "abstract": "The energy sector is in transition\u2013being forced to rethink the current practice and apply data-management based IT solutions to provide a scalable and sustainable supply and distribution of energy. Novel challenges range from renewable energy production over energy distribution and monitoring to controlling and moving energy consumption. Huge amounts of \u201cBig Energy Data,\u201d i.e., data from smart meters, new renewable energy sources (RES\u2013such as wind, solar, hydro, thermal, etc), novel distributions mechanisms (Smart Grid), and novel types of consumers and devices, e.g., electric cars, are being collected and must be managed and analyzed to yield their potential. Energy is at the top of the worldwide political agenda. For example, The European Union has stated the \u201c2020-20 goals\u201d (20% renewable energy, 20% better energy efficiency, and 20% CO2 reduction by 2020). Even more ambitious goals are set for 2030 and 2050. This situation is reflected in research funding schemes such as the EU Horizon 2020 Framework program as well as national programs. Increasingly, such programs include joint calls involving both energy and IT partners. Data management is at the heart of this development, as witnessed by the following story headlines from key players: \u201cThe Smart Grid Data Deluge\u201d (O\u2019Reilly Radar); \u201cBig data for the Smart Grid\u201d (theenergycollective); \u201cThe Coming Smart Grid Data Surge\u201d (SmartGridNews.com). Thus, data management within the energy domain becomes increasingly important. The International Workshop on Energy Data Management (EnDM) focuses on conceptual and system architecture issues related to the management of very large-scale data sets specifically in the context of the energy domain. The overall goal of the EmDM workshop is a) to bridge the gap between domain experts and data management scientists and b) to create awareness of this emerging and very challenging application area. For the workshop\u2019s research program, the organizers especially try to attract contributions that push the envelope towards novel schemes for large-scale data processing with special focus on energy data management. The Second International Workshop on Energy Data Management (EnDM\u201913) was held in conjunction with",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1731453",
                    "name": "T. Pedersen"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "3068811",
                    "name": "Gregor Hackenbroich"
                }
            ]
        },
        {
            "paperId": "d23f6db0b0333656704f850a426c9e9fa0263798",
            "title": "Forecasting in hierarchical environments",
            "abstract": "Forecasting is an important data analysis technique and serves as the basis for business planning in many application areas such as energy, sales and traffic management. The currently employed statistical models already provide very accurate predictions, but the forecasting calculation process is very time consuming. This is especially true since many application domains deal with hierarchically organized data. Forecasting in these environments is especially challenging due to ensuring forecasting consistency between hierarchy levels, which leads to an increased data processing and communication effort. For this purpose, we introduce our novel hierarchical forecasting approach, where we propose to push forecast models to the entities on the lowest hierarch level and reuse these models to efficiently create forecast models on higher hierarchical levels. With that we avoid the time-consuming parameter estimation process and allow an almost instant calculation of forecasts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48260655",
                    "name": "R. Lorenz"
                },
                {
                    "authorId": "2715207",
                    "name": "Lars Dannecker"
                },
                {
                    "authorId": "2932139",
                    "name": "Philipp J. R\u00f6sch"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "3068811",
                    "name": "Gregor Hackenbroich"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                }
            ]
        },
        {
            "paperId": "e1d816470d86a3f9aa407f9497f440cc9163a407",
            "title": "Optimizing Sample Design for Approximate Query Processing",
            "abstract": "The rapid increase of data volumes makes sampling a crucial component of modern data management systems. Although there is a large body of work on database sampling, the problem of automatically determine the optimal sample for a given query remained almost unaddressed. To tackle this problem the authors propose a sample advisor based on a novel cost model. Primarily designed for advising samples of a few queries specified by an expert, the authors additionally propose two extensions of the sample advisor. The first extension enhances the applicability by utilizing recorded workload information and taking memory bounds into account. The second extension increases the effectiveness by merging samples in case of overlapping pieces of sample advice. For both extensions, the authors present exact and heuristic solutions. Within their evaluation, the authors analyze the properties of the cost model and demonstrate the effectiveness and the efficiency of the heuristic solutions with a variety of experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2932139",
                    "name": "Philipp J. R\u00f6sch"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "ec6a60a384f5d6eadc3bbe73395d4ab5d996a06d",
            "title": "SynopSys: large graph analytics in the SAP HANA database through summarization",
            "abstract": "Graph-structured data is ubiquitous and with the advent of social networking platforms has recently seen a significant increase in popularity amongst researchers. However, also many business applications deal with this kind of data and can therefore benefit greatly from graph processing functionality offered directly by the underlying database. This paper summarizes the current state of graph data processing capabilities in the SAP HANA database and describes our efforts to enable large graph analytics in the context of our research project SynopSys. With powerful graph pattern matching support at the core, we envision OLAP-like evaluation functionality exposed to the user in the form of easy-to-apply graph summarization templates. By combining them, the user is able to produce concise summaries of large graph-structured datasets. We also point out open questions and challenges that we plan to tackle in the future developments on our way towards large graph analytics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053142642",
                    "name": "Michael Rudolf"
                },
                {
                    "authorId": "2587068",
                    "name": "M. Paradies"
                },
                {
                    "authorId": "1698512",
                    "name": "Christof Bornh\u00f6vd"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "ee67861a293418973003555d36440abab2db898c",
            "title": "Efficient forecasting for hierarchical time series",
            "abstract": "Forecasting is used as the basis for business planning in many application areas such as energy, sales and traffic management. Time series data used in these areas is often hierarchically organized and thus, aggregated along the hierarchy levels based on their dimensional features. Calculating forecasts in these environments is very time consuming, due to ensuring forecasting consistency between hierarchy levels. To increase the forecasting efficiency for hierarchically organized time series, we introduce a novel forecasting approach that takes advantage of the hierarchical organization. There, we reuse the forecast models maintained on the lowest level of the hierarchy to almost instantly create already estimated forecast models on higher hierarchical levels. In addition, we define a hierarchical communication framework, increasing the communication flexibility and efficiency. Our experiments show significant runtime improvements for creating a forecast model at higher hierarchical levels, while still providing a very high accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2715207",
                    "name": "Lars Dannecker"
                },
                {
                    "authorId": "48260655",
                    "name": "R. Lorenz"
                },
                {
                    "authorId": "2932139",
                    "name": "Philipp J. R\u00f6sch"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "3068811",
                    "name": "Gregor Hackenbroich"
                }
            ]
        },
        {
            "paperId": "ef9040b10adb1eb4c25f9b3fe646784bd5f86aa1",
            "title": "The Graph Story of the SAP HANA Database",
            "abstract": "Many traditional and new business applications work with inherently graphstructured data and therefore benefit from graph abstractions and operations provided in the data management layer. The property graph data model not only offers schema flexibility but also permits managing and processing data and metadata jointly. By having typical graph operations implemented directly in the database engine and exposing them both in the form of an intuitive programming interface and a declarative language, complex business application logic can be expressed more easily and executed very efficiently. In this paper we describe our ongoing work to extend the SAP HANA database with built-in graph data support. We see this as a next step on the way to provide an efficient and intuitive data management platform for modern business applications with SAP HANA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053142642",
                    "name": "Michael Rudolf"
                },
                {
                    "authorId": "2587068",
                    "name": "M. Paradies"
                },
                {
                    "authorId": "1698512",
                    "name": "Christof Bornh\u00f6vd"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "f14fc1c5dc0b0773e80e4a20b1651c885bd8c901",
            "title": "Scalable frequent itemset mining on many-core processors",
            "abstract": "Frequent-itemset mining is an essential part of the association rule mining process, which has many application areas. It is a computation and memory intensive task with many opportunities for optimization. Many efficient sequential and parallel algorithms were proposed in the recent years. Most of the parallel algorithms, however, cannot cope with the huge number of threads that are provided by large multiprocessor or many-core systems. In this paper, we provide a highly parallel version of the well-known Eclat algorithm. It runs on both, multiprocessor systems and many-core coprocessors, and scales well up to a very large number of threads---244 in our experiments. To evaluate mcEclat's performance, we conducted many experiments on realistic datasets. mcEclat achieves high speedups of up to 11.5x and 100x on a 12-core multiprocessor system and a 61-core Xeon Phi many-core coprocessor, respectively. Furthermore, mcEclat is competitive with highly optimized existing frequent-itemset mining implementations taken from the FIMI repository.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "2482934",
                    "name": "Tomas Karnagel"
                },
                {
                    "authorId": "32019086",
                    "name": "Tim Kiefer"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "f25ea5d6c7fd3b2ea17a1b349c82893070560367",
            "title": "Publish-time data integration for open data platforms",
            "abstract": "Platforms for publication and collaborative management of data, such as Data.gov or Google Fusion Tables, are a new trend on the web. They manage very large corpora of datasets, but often lack an integrated schema, ontology, or even just common publication standards. This results in inconsistent names for attributes of the same meaning, which constrains the discovery of relationships between datasets as well as their reusability. Existing data integration techniques focus on reuse-time, i.e., they are applied when a user wants to combine a specific set of datasets or integrate them with an existing database. In contrast, this paper investigates a novel method of data integration at publish-time, where the publisher is provided with suggestions on how to integrate the new dataset with the corpus as a whole, without resorting to a manually created mediated schema or ontology for the platform. We propose data-driven algorithms that propose alternative attribute names for a newly published dataset based on attribute- and instance statistics maintained on the corpus. We evaluate the proposed algorithms using real-world corpora based on the Open Data Platform opendata.socrata.com and relational data extracted from Wikipedia. We report on the system's response time, and on the results of an extensive crowdsourcing-based evaluation of the quality of the generated attribute names alternatives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2590616",
                    "name": "Julian Eberius"
                },
                {
                    "authorId": "3069231",
                    "name": "Patrick Damme"
                },
                {
                    "authorId": "3176379",
                    "name": "Katrin Braunschweig"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "f4e447253d8c04facc9756fed8b1b0e29408b3d4",
            "title": "DeExcelerator: a framework for extracting relational data from partially structured documents",
            "abstract": "Of the structured data published on the web, for instance as datasets on Open Data Platforms such as data.gov, but also in the form of HTML tables on the general web, only a small part is in a relational form. Instead the data is intermingled with formatting, layout and textual metadata, i.e., it is contained in partially structured documents. This makes transformation into a true relational form necessary, which is a precondition for most forms of data analysis and data integration. Studying data.gov as an example source for partially structured documents, we present a classification of typical normalization problems. We then present the DeExcelerator, which is a framework for extracting relations from partially structured documents such as spreadsheets and HTML tables.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2590616",
                    "name": "Julian Eberius"
                },
                {
                    "authorId": "39113490",
                    "name": "Christoper Werner"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "3176379",
                    "name": "Katrin Braunschweig"
                },
                {
                    "authorId": "2715207",
                    "name": "Lars Dannecker"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "fabc9725a66dcd393116066f088b4e91884eafba",
            "title": "Advanced Analytics with the SAP HANA Database",
            "abstract": "MapReduce as a programming paradigm provides a simple-to-use yet very powerful abstraction encapsulated in two second-order functions: Map and Reduce. As such, they allow defining single sequentially processed tasks while at the same time hiding many of the framework details about how those tasks are parallelized and scaled out. In this paper we discuss four processing patterns in the context of the distributed SAP HANA database that go beyond the classic MapReduce paradigm. We illustrate them using some typical Machine Learning algorithms and present experimental results that demonstrate how the data flows scale out with the number of parallel tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057183670",
                    "name": "Philippe Grosse"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1766564",
                    "name": "Norman May"
                }
            ]
        },
        {
            "paperId": "1be04eef05d2547199ed787125f38610a2838658",
            "title": "KISS-Tree: smart latch-free in-memory indexing on modern architectures",
            "abstract": "Growing main memory capacities and an increasing number of hardware threads in modern server systems led to fundamental changes in database architectures. Most importantly, query processing is nowadays performed on data that is often completely stored in main memory. Despite of a high main memory scan performance, index structures are still important components, but they have to be designed from scratch to cope with the specific characteristics of main memory and to exploit the high degree of parallelism. Current research mainly focused on adapting block-optimized B+-Trees, but these data structures were designed for secondary memory and involve comprehensive structural maintenance for updates.\n In this paper, we present the KISS-Tree, a latch-free in-memory index that is optimized for a minimum number of memory accesses and a high number of concurrent updates. More specifically, we aim for the same performance as modern hash-based algorithms but keeping the order-preserving nature of trees. We achieve this by using a prefix tree that incorporates virtual memory management functionality and compression schemes. In our experiments, we evaluate the KISS-Tree on different workloads and hardware platforms and compare the results to existing in-memory indexes. The KISS-Tree offers the highest reported read performance on current architectures, a balanced read/write performance, and has a low memory footprint.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "1d1cc5e35022ac6910ef32657d9ecb83c9322fa4",
            "title": "Adaptive Index Buffer",
            "abstract": "With rapidly increasing datasets and more dynamic workloads, adaptive partial indexing becomes an important way to keep indexing efficiently. During times of changing workloads, the query performance suffers from inefficient tables scans while the index tuning mechanism adapts the partial index. In this paper we present the Adaptive Index Buffer. The Adaptive Index Buffer reduces the cost of table scans by quickly indexing tuples in memory until the partial index has adapted to the workload again. We explain the basic operating mode of an Index Buffer and discuss how it adapts to changing workload situations. Further, we present three experiments that show the Index Buffer at work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "21658827",
                    "name": "Tobias J\u00e4kel"
                },
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2f52cbef51a6a8a2a74119ad821526f9e0b57b39",
            "title": "SAP HANA database: data management for modern business applications",
            "abstract": "The SAP HANA database is positioned as the core of the SAP HANA Appliance to support complex business analytical processes in combination with transactionally consistent operational workloads. Within this paper, we outline the basic characteristics of the SAP HANA database, emphasizing the distinctive features that differentiate the SAP HANA database from other classical relational database management systems. On the technical side, the SAP HANA database consists of multiple data processing engines with a distributed query processing environment to provide the full spectrum of data processing -- from classical relational data supporting both row- and column-oriented physical representations in a hybrid engine, to graph and text processing for semi- and unstructured data management within the same system.\n From a more application-oriented perspective, we outline the specific support provided by the SAP HANA database of multiple domain-specific languages with a built-in set of natively implemented business functions. SQL -- as the lingua franca for relational database systems -- can no longer be considered to meet all requirements of modern applications, which demand the tight interaction with the data management layer. Therefore, the SAP HANA database permits the exchange of application semantics with the underlying data management platform that can be exploited to increase query expressiveness and to reduce the number of individual application-to-database round trips.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708337",
                    "name": "Franz F\u00e4rber"
                },
                {
                    "authorId": "2237996",
                    "name": "S. Cha"
                },
                {
                    "authorId": "1706006",
                    "name": "J\u00fcrgen Primsch"
                },
                {
                    "authorId": "1698512",
                    "name": "Christof Bornh\u00f6vd"
                },
                {
                    "authorId": "32566776",
                    "name": "Stefan Sigg"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "4c527d77ff3edc876bbd5f61062679d195605017",
            "title": "Flexible Information Management, Exploration and Analysis in SAP HANA",
            "abstract": "Data management is not limited anymore to towering data silos full of perfectly structured, well integrated data. Today, we need to process and make sense of data from diverse sources (public and on-premise), in different application contexts, with different schemas, and with varying degrees of structure and quality. Because of the necessity to define a rigid data schema upfront, fixed-schema database systems are not a good fit for these new scenarios. However, schema is still essential to give data meaning and to process data purposefully. In this paper, we describe a schema-flexible database system that combines a flexible data model with a powerful data query, analysis, and manipulation language that provides both required schema information and the flexibility required for modern information processing and decision support.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1698512",
                    "name": "Christof Bornh\u00f6vd"
                },
                {
                    "authorId": "1700343",
                    "name": "Robert Kubis"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "2069400403",
                    "name": "Horst Werner"
                }
            ]
        },
        {
            "paperId": "519b4a9bbbc5ac5dd180972088bfa7e335049514",
            "title": "SMIX Live -- A Self-Managing Index Infrastructure for Dynamic Workloads",
            "abstract": "As databases accumulate growing amounts of data at an increasing rate, adaptive indexing becomes more and more important. At the same time, applications and their use get more agile and flexible, resulting in less steady and less predictable workload characteristics. Being inert and coarse-grained, state-of-the-art index tuning techniques become less useful in such environments. Especially the full-column indexing paradigm results in lot of indexed but never queried data and prohibitively high memory and maintenance costs. In our demonstration, we present Self-Managing Indexes, a novel, adaptive, fine-grained, autonomous indexing infrastructure. In its core, our approach builds on a novel access path that automatically collects useful index information, discards useless index information, and competes with its kind for resources to host its index information. Compared to existing technologies for adaptive indexing, we are able to dynamically grow and shrink our indexes, instead of incrementally enhancing the index granularity. In the demonstration, we visualize performance and system measures for different scenarios and allow the user to interactively change several system parameters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "5b85ec42128e4310465e0c6eda4505585739e867",
            "title": "Data management in the MIRABEL smart grid system",
            "abstract": "Nowadays, Renewable Energy Sources (RES) are attracting more and more interest. Thus, many countries aim to increase the share of green energy and have to face with several challenges (e.g., balancing, storage, pricing). In this paper, we address the balancing challenge and present the MIRABEL project which aims to prototype an Energy Data Management System (EDMS) which takes benefit of flexibilities to efficiently balance energy demand and supply. The EDMS consists of millions of heterogeneous nodes that each incorporates advanced components (e.g., aggregation, forecasting, scheduling, negotiation). We describe each of these components and their interaction. Preliminary experimental results confirm the feasibility of our EDMS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "2715207",
                    "name": "Lars Dannecker"
                },
                {
                    "authorId": "1714006",
                    "name": "Andreas Doms"
                },
                {
                    "authorId": "2790548",
                    "name": "E. Dovgan"
                },
                {
                    "authorId": "1691028",
                    "name": "B. Filipi\u010d"
                },
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1731453",
                    "name": "T. Pedersen"
                },
                {
                    "authorId": "2919587",
                    "name": "Y. Pitarch"
                },
                {
                    "authorId": "2622931",
                    "name": "Laurynas Siksnys"
                },
                {
                    "authorId": "1972464",
                    "name": "Tea Tu\u0161ar"
                }
            ]
        },
        {
            "paperId": "7c104054f316cf3176a3a4388be9786223a8c6f8",
            "title": "Model-based Integration of Past & Future in TimeTravel",
            "abstract": "We demonstrate TimeTravel, an efficient DBMS system for seamless integrated querying of past and (forecasted) future values of time series, allowing the user to view past and future values as one joint time series. This functionality is important for advanced application domain like energy. The main idea is to compactly represent time series as models. By using models, the TimeTravel system answers queries approximately on past and future data with error guarantees (absolute error and confidence) one order of magnitude faster than when accessing the time series directly. In addition, it efficiently supports exact historical queries by only accessing relevant portions of the time series. This is unlike existing approaches, which access the entire time series to exactly answer the query. \n \nTo realize this system, we propose a novel hierarchical model index structure. As real-world time series usually exhibits seasonal behavior, models in this index incorporate seasonality. To construct a hierarchical model index, the user specifies seasonality period, error guarantees levels, and a statistical forecast method. As time proceeds, the system incrementally updates the index and utilizes it to answer approximate and exact queries. TimeTravel is implemented into PostgreSQL, thus achieving complete user transparency at the query level. In the demo, we show the easy building of a hierarchical model index for a real-world time series and the effect of varying the error guarantees on the speed up of approximate and exact queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7625436",
                    "name": "Mohamed E. Khalefa"
                },
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "1731453",
                    "name": "T. Pedersen"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "856da0d2913bbc73e7f12db9a6a95d7240487d72",
            "title": "Identifying and weighting integration hypotheses on open data platforms",
            "abstract": "Open data platforms such as data.gov or opendata.socrata. com provide a huge amount of valuable information, publicly available to anyone. This data has the potential to drive innovation and lead to a more democratic and transparent society. Still, the platforms it is offered on have some unique problems: Their free-for-all nature, the lack of publishing standards and the multitude of domains and authors represented on these platforms lead to new integration and standardization problems, such as duplicated or partitioned datasets.\n At the same time, crowd-based data integration techniques are emerging as new way of dealing with data integration problems. However, these methods still require input in form of specific questions or tasks that can be passed to the crowd. This paper identifies several classes of integration problems on Open Data Platforms, and proposes a method for identifying and ranking potential them in this context. In this method, an Open Data Platform is modeled as a graph of datasets, so that potentital integration problems, called integration hypotheses, can be identified by analyzing the graph for specific patterns.\n The paper concludes with a comprehensive evaluation using one of the largest Open Data platforms, opendata.socrata.com.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2590616",
                    "name": "Julian Eberius"
                },
                {
                    "authorId": "3176379",
                    "name": "Katrin Braunschweig"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "8e73647a723862082544244ab986ac58d57ec3c6",
            "title": "Sample-based forecasting exploiting hierarchical time series",
            "abstract": "Time series forecasting is challenging as sophisticated forecast models are computationally expensive to build. Recent research has addressed the integration of forecasting inside a DBMS. One main benefit is that models can be created once and then repeatedly used to answer forecast queries. Often forecast queries are submitted on higher aggregation levels, e. g., forecasts of sales over all locations. To answer such a forecast query, we have two possibilities. First, we can aggregate all base time series (sales in Austria, sales in Belgium...) and create only one model for the aggregate time series. Second, we can create models for all base time series and aggregate the base forecast values. The second possibility might lead to a higher accuracy but it is usually too expensive due to a high number of base time series. However, we actually do not need all base models to achieve a high accuracy, a sample of base models is enough. With this approach, we still achieve a better accuracy than an aggregate model, very similar to using all models, but we need less models to create and maintain in the database. We further improve this approach if new actual values of the base time series arrive at different points in time. With each new actual value we can refine the aggregate forecast and eventually converge towards the real actual value. Our experimental evaluation using several real-world data sets, shows a high accuracy of our approaches and a fast convergence towards the optimal value with increasing sample sizes and increasing number of actual values respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "46554189",
                    "name": "Frank Rosenthal"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "abc15fb972ec38f9dd74b723fb9956008f6d2bc0",
            "title": "DrillBeyond: Enabling Business Analysts to Explore the Web of Open Data",
            "abstract": "Following the Open Data trend, governments and public agencies have started making their data available on the Web and established platforms such as data.gov or data.un.org. These Open Data platforms provide a huge amount of data for various topics such as demographics, transport, finance or health in various data formats. One typical usage scenario for this kind of data is their integration into a database or data warehouse in order to apply data analytics. However, in today's business intelligence tools there is an evident lack of support for so-called situational or ad-hoc data integration. In this demonstration we will therefore present DrillBeyond, a novel database and information retrieval engine which allows users to query a local database as well as the Web of Open Data in a seamless and integrated way with standard SQL. The audience will be able to pose queries to our DrillBeyond system which will be answered partly from local data in the database and partly from datasets that originate from the Web of Data. \n \nWe will show how such queries are divided into known and unknown parts and how missing attributes are mapped to open datasets. We will demonstrate the integration of the open datasets back into the DBMS in order to apply its analytical features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2590616",
                    "name": "Julian Eberius"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "3176379",
                    "name": "Katrin Braunschweig"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "afda6470dd16dc0a865dbb6fc291e5806132379b",
            "title": "The SAP HANA Database -- An Architecture Overview",
            "abstract": "Requirements of enterprise applications have become much more demanding. They require the computation of complex reports on transactional data while thousands of users may read or update records of the same data. The goal of the SAP HANA database is the integration of transactional and analytical workload within the same database management system. To achieve this, a columnar engine exploits modern hardware (multiple CPU cores, large main memory, and caches), compression of database content, maximum parallelization in the database kernel, and database extensions required by enterprise applications, e.g., specialized data structures for hierarchies or support for domain specific languages. In this paper we highlight the architectural concepts employed in the SAP HANA database. We also report on insights gathered with the SAP HANA database in real-world enterprise application scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708337",
                    "name": "Franz F\u00e4rber"
                },
                {
                    "authorId": "1766564",
                    "name": "Norman May"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2057183670",
                    "name": "Philippe Grosse"
                },
                {
                    "authorId": "2068635149",
                    "name": "Ingo M\u00fcller"
                },
                {
                    "authorId": "2698309",
                    "name": "Hannes Rauhe"
                },
                {
                    "authorId": "37664530",
                    "name": "J. Dees"
                }
            ]
        },
        {
            "paperId": "bff4570b76500e0d94a114b283345133cb443f53",
            "title": "Pathways to servers of the future",
            "abstract": "The Special Session on \u201cPathways to Servers of the Future\u201d outlines a new research program set up at Technische Universita\u0308t Dresden addressing the increasing energy demand of global internet usage and the resulting ecological impact of it. The program pursues a novel holistic approach that considers hardware as well as software adaptivity to significantly increase energy efficiency, while suitably addressing application demands. The session presents the research challenges and industry perspective.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145570132",
                    "name": "G. Fettweis"
                },
                {
                    "authorId": "1781970",
                    "name": "W. Nagel"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "d9a9ffc51ffff010272c2f9b1a123c8aedecd9d5",
            "title": "A high-throughput in-memory index, durable on flash-based SSD: insights into the winning solution of the SIGMOD programming contest 2011",
            "abstract": "Growing memory capacities and the increasing number of cores on modern hardware enforces the design of new in-memory indexing structures that reduce the number of memory transfers and minimizes the need for locking to allow massive parallel access. However, most applications depend on hard durability constraints requiring a persistent medium like SSDs, which shorten the latency and throughput gap between main memory and hard disks. In this paper, we present our winning solution of the SIGMOD Programming Contest 2011. It consists of an in-memory indexing structure that provides a balanced read/write performance as well as non-blocking reads and single-lock writes. Complementary to this index, we describe an SSD-optimized logging approach to fit hard durability requirements at a high throughput rate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143839880",
                    "name": "T. Kissinger"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "da7cea1393b4f60ef62999254c8a741d361479e6",
            "title": "F2DB: The Flash-Forward Database System",
            "abstract": "Forecasts are important to decision-making and risk assessment in many domains. Since current database systems do not provide integrated support for forecasting, it is usually done outside the database system by specially trained experts using forecast models. However, integrating model-based forecasting as a first-class citizen inside a DBMS speeds up the forecasting process by avoiding exporting the data and by applying database-related optimizations like reusing created forecast models. It especially allows subsequent processing of forecast results inside the database. In this demo, we present our prototype F2DB based on PostgreSQL, which allows for transparent processing of forecast queries. Our system automatically takes care of model maintenance when the underlying dataset changes. In addition, we offer optimizations to save maintenance costs and increase accuracy by using derivation schemes for multidimensional data. Our approach reduces the required expert knowledge by enabling arbitrary users to apply forecasting in a declarative way.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "46554189",
                    "name": "Frank Rosenthal"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "fee43ab69a8f12d23d0d9a5d2747cc6edb3b656f",
            "title": "Enabling Real-Time Business Intelligence: 5th International Workshop, BIRTE 2011, Held at the 37th International Conference on Very Large Databases",
            "abstract": "This book constitutes the thoroughly refereed conference proceedings of the 5th International Workshop on Business Intelligence for the Real-Time Enterprise, BIRTE 2011, held in Seattle, WA, USA, in September 2011, in conjunction with VLDB 2011, the International Conference on Very Large Data Bases. The series of BIRTE workshops aims to provide a forum for researchers to discuss and advance the foundational science and engineering required to enable real-time business intelligence as well as novel applications and solutions based on these foundational techniques.The volume contains 6 research papers, which have been carefully reviewed and selected from 12 submissions, plus the 3 keynotes presented at the workshop. The topics cover all stages of the business intelligence cycle, including capturing of real-time data, handling of temporal or uncertain data, performance issues, event management, and the optimization of complex ETL workflows. The volume contains 6 research papers, which have been carefully reviewed and selected from 12 submissions, plus the 3 keynotes presented at the workshop. The topics cover all stages of the business intelligence cycle, including capturing of real-time data, handling of temporal or uncertain data, performance issues, event management, and the optimization of complex ETL workflows.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144016128",
                    "name": "M. Castellanos"
                },
                {
                    "authorId": "1725067",
                    "name": "U. Dayal"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "09af915b0a7c26c2a9486c96e93ff4bba5c05179",
            "title": "Resiliency-aware data management",
            "abstract": "Computing architectures change towards massively parallel environments with increasing numbers of heterogeneous components. The large scale in combination with decreasing feature sizes leads to dramatically increasing error rates. The heterogeneity further leads to new error types. Techniques for ensuring resiliency in terms of robustness regarding these errors are typically applied at hardware abstraction and operating system levels. However, as errors become the normal case, we observe increasing costs in terms of computation overhead for ensuring robustness. In this paper, we argue that ensuring resiliency on the data management level can reduce the required overhead by exploiting context knowledge of query processing and data storage. Apart from reacting on already detected errors, this was mostly neglected in database research so far. We therefore give a broad overview of the background of resilient computing and existing techniques from the database perspective. Based on the lack of existing techniques on data management level, we raise three fundamental challenges of resiliency-aware data management and present example use cases. Finally, our vision of resiliency-aware data management opens many directions of future work. Fundamental research, including the partial reuse of underlying mechanisms, would allow data management systems to cope with future hardware characteristics by effectively and efficiently ensuring resiliency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "145494211",
                    "name": "C. Fetzer"
                }
            ]
        },
        {
            "paperId": "182ca938338b40023e27a585f3152360ec62e36a",
            "title": "Memory-efficient frequent-itemset mining",
            "abstract": "Efficient discovery of frequent itemsets in large datasets is a key component of many data mining tasks. In-core algorithms---which operate entirely in main memory and avoid expensive disk accesses---and in particular the prefix tree-based algorithm FP-growth are generally among the most efficient of the available algorithms. Unfortunately, their excessive memory requirements render them inapplicable for large datasets with many distinct items and/or itemsets of high cardinality. To overcome this limitation, we propose two novel data structures---the CFP-tree and the CFP-array---, which reduce memory consumption by about an order of magnitude. This allows us to process significantly larger datasets in main memory than previously possible. Our data structures are based on structural modifications of the prefix tree that increase compressability, an optimized physical representation, lightweight compression techniques, and intelligent node ordering and indexing. Experiments with both real-world and synthetic datasets show the effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "1777107",
                    "name": "Rainer Gemulla"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2c165906474b780a55bc587fa45008c84d921839",
            "title": "Next Generation Database Programming and Execution Environment",
            "abstract": "The database research is always on the move. In order to integrate novel concepts, the significance of the database programmability aspect more and more increases. The programmability aspect focuses on internal components as well as on principle to push-down application logic to the database system. In this paper, we propose a novel database programming model and a corresponding database architecture framework enabling extensibility and a better integration of application code into DBMS. In detail, we present a scripting language pyDBL which is unified utilizable to implement physical database operators, query plans and even complete applications. We demonstrate the applicability of our approach in terms of a moderate performance overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2f117defaf936b53ee00ebcfa69897fedfb1350b",
            "title": "Bridging two worlds with RICE",
            "abstract": "The growing need to use large amounts of data as the basis for sophisticated business analysis conflicts with the current capabilities of statistical software systems as well as the functions provided by most modern databases. We developed two novel approaches towards a solution for this basic conflict, based on the widely-used statistical software package R and the SAP In-Memory Computing Engine (IMCE). We thereby propose an alternative data exchange mechanism with R. Instead of using standard SQL interfaces like JDBC or ODBC we introduced SQL-SHM, a shared memory-based data exchange to incorporate R's vertical data structure. Furthermore, we extended this approach to R-Op introducing R scripts equivalent to native database operations like join or aggregation within the execution plans. With the calculation engine, IMCE provides a framework to model logical execution plans and thereby offers a convenient way to use the full functionality of R via SQL interface. Moreover, this enables us to run R scripts in parallel without the necessity of extending the R interpreter itself.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057183670",
                    "name": "Philippe Grosse"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "40312856",
                    "name": "T. Weichert"
                },
                {
                    "authorId": "1708337",
                    "name": "Franz F\u00e4rber"
                },
                {
                    "authorId": "2108717697",
                    "name": "Wen-Syan Li"
                }
            ]
        },
        {
            "paperId": "3afdc9f3ff5ce37bf204a4f92f4ab1a1bd0e7b5f",
            "title": "Efficient In-Memory Indexing with Generalized Prefix Trees",
            "abstract": "Efficient data structures for in-memory indexing gain in importance due to (1) the exponentially increasing amount of data, (2) the growing main-memory capacity, and (3) the gap between main-memory and CPU speed. In consequence, there are high performance demands for in-memory data structures. Such index structures are used\u2014with minor changes\u2014as primary or secondary indices in almost every DBMS. Typically, tree-based or hash-based structures are used, while structures based on prefix-trees (tries) are neglected in this context. For tree-based and hash-based structures, the major disadvantages are inherently caused by the need for reorganization and key comparisons. In contrast, the major disadvantage of trie-based structures in terms of high memory consumption (created and accessed nodes) could be improved. In this paper, we argue for reconsidering prefix trees as in-memory index structures and we present the generalized trie, which is a prefix tree with variable prefix length for indexing arbitrary data types of fixed or variable length. The variable prefix length enables the adjustment of the trie height and its memory consumption. Further, we introduce concepts for reducing the number of created and accessed trie levels. This trie is order-preserving and has deterministic trie paths for keys, and hence, it does not require any dynamic reorganization or key comparisons. Finally, the generalized trie yields improvements compared to existing in-memory index structures, especially for skewed data. In conclusion, the generalized trie is applicable as general-purpose in-memory index structure in many different OLTP or hybrid (OLTP and OLAP) data management systems that require balanced read/write performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "2060654334",
                    "name": "P. Volk"
                },
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "3e18257b92c31454e654391a2c0faf3c4dafdf23",
            "title": "Browsing Robust Clustering-Alternatives",
            "abstract": "In the last years, new clustering approaches utilizing the notion of multiple clusterings have gained attention. Two general directions -- each with its individual benefits -- are identifiable: (i) extraction of multiple alternative clustering solutions from one dataset and (ii) combination of multiple clusterings of a dataset into one robust consensussolution. In this paper, we propose a novel hybrid approach to generate and browse robust, alternative clustering results. Our hybrid approach is based on frequent-groupings as specialization of frequent-itemset mining. In this way, the different benefits of the existing directions are combined, offering new opportunities for knowledge extraction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "44680ac375502a3f250aea019b7083eca7b92c8b",
            "title": "Data-Warehousing 3.0 - Die Rolle von Data-Warehouse-Systemen auf Basis von In-Memory Technologie",
            "abstract": "In diesem Beitrag widmen wir uns der Frage, welche Rolle aktuelle Trends der Hardund Software f\u00fcr Datenbanksysteme spielen, um als Enabler f\u00fcr neuartige Konzepte im Umfeld des Data-Warehousing zu dienen. Als zentraler Schritt der Evolution im Kontext des Data-Warehousing wird dabei die enge Kopplung zu operativen Systemen gesehen, um eine direkte R\u00fcckkopplung bzw. Einbettung in operationale Gesch\u00e4ftsprozesse zu realisieren. In diesem Papier diskutieren wir die Fragen, wie In-Memory-Technologie das Konzept von Echtzeit-DWH-Systemen unterst\u00fctzt bzw. erm\u00f6glicht. Dazu stellen wir zum einen eine Referenzarchitektur f\u00fcr DWH-Systeme vor, die insbesondere pushund pullbasierte Datenversorgung ber\u00fccksichtigt. Zum anderen diskutieren wir die konkrete Rolle von In-Memory-Systemen mit Blick auf konkrete Aspekte wie der Frage optionaler Persistenzschichten, Reduktion der Batchgr\u00f6\u00dfe, Positionierung von In-Memory-Techniken f\u00fcr den Aufbau eines Corporate Memorys und die schnelle Bereitstellung externer Datenbest\u00e4nde zur Unterst\u00fctzung situativer BISzenarien.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35248967",
                    "name": "M. Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2072699345",
                    "name": "Dirk Habich"
                }
            ]
        },
        {
            "paperId": "6522da31d114b60e9fcbb44cbe8e477e3c63f463",
            "title": "Cost-based Business Process Deployment Advisor",
            "abstract": "Service-oriented environments increasingly become the central backbone of a company\u2019s business processes. Thereby, services and business process flows are loosely coupled components that are distributed over many server nodes and communicate via messages. Unfortunately, communication between SOA components using the XML format is still the most resourceand time-consuming activity within a traditional business process flow. This paper describes an approach that considers these transfer-costs and advises a process and service placement for a given set of server nodes to partially eliminate remote message transfers. Furthermore, experiments have been conducted to show its feasibility.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1732902",
                    "name": "Steffen Preissler"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "7acf96333366ff630ea29b0b0f8eb93283170097",
            "title": "Fast Sorted-Set Intersection using SIMD Instructions",
            "abstract": "In this paper, we focus on sorted-set intersection which is an important part in many algorithms, e.g., RID-list inter-section, inverted indexes, and others. In contrast to traditional scalar sorted-set intersection algorithms that try to reduce the number of comparisons, we propose a parallel algorithm that relies on speculative execution of comparisons. In general, our algorithm requires more comparisons but less instructions than scalar algorithms that translates into a better overall speed. We achieve this by utilizing ef-\ufb01cient single-instruction-multiple-data (SIMD) instructions that are available in many processors. We provide di\ufb00erent sorted-set intersection algorithms for di\ufb00erent integer data types. We propose versions that use uncompressed integer values as input and output as well as a version that uses a tailor-made data layout for even faster intersections. In our experiments, we achieve speedups up to 5 . 3x compared to popular fast scalar algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "3250632",
                    "name": "Thomas Willhalm"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "8de44c5faa642b38bd01ffff11b7b7dd670fe010",
            "title": "Offline Design Tuning for Hierarchies of Forecast Models",
            "abstract": "Forecasting of time series data is crucial for decision-making processes in many domains as it allows the prediction of future behavior. In this context, a model is fit to the observed data points of the time series by estimating the model parameters. The computed parameters are then utilized to forecast future points in time. Existing approaches integrate forecasting into traditional relational query processing, where a forecast query requests the creation of a forecast model. Models of continued interest should be deployed only once and used many times afterwards. This however leads to additional maintenance costs as models need to be kept up-to-date. Costs can be reduced by choosing a well-defined subset of models and answering queries using derivation schemes. In contrast to materialized view selection, model selection opens a whole new problem area as results are approximate. A derivation schema might increase or decrease the accuracy of a forecast query. Thus, a two-dimensional optimization problem of minimizing the model cost and model usage error is introduced in this paper. Our solution consists of a greedy enumeration approach that empirically evaluates different configurations of forecast models. In our experimental evaluation, with data sets from different domains, we show the superiority of our approach over traditional approaches from forecasting literature.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "936bcfd7a99a9daec7bada25094d3a7adda64969",
            "title": "Private Table Database Virtualization for DBaaS",
            "abstract": "Growing number of applications store data in relational databases. Moving database applications to the cloud faces challenges related to flexible and scalable management of data. The obvious strategy of hosting legacy database management systems (DMBSs) on virtualized cloud resources leads to sub optimal utilization and performance. However, the layered architecture inside the DBMS allows for virtualization and consolidation above the OS level which can lead to significantly better system utilization and application performance. Finding an optimal database cloud solution requires finding an assignment from virtual to physical resources as well as configurations for all components. Our goal is to provide a virtualization advisor that aids in setting up and operating a database cloud. By formulating analytic cost, workload, and resource models performance of cloud-hosted relational database services can be significantly improved.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32019086",
                    "name": "Tim Kiefer"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "a90e1718c2fbe6d1c619ae1404e1e28c85daa028",
            "title": "Report of the symposium \"Lernen, Wissen, Adaptivit\u00e4t 2011\" of the GI special interest groups KDML, IR and WM, LWA 2011, Magdeburg, 28.-30.September 2011",
            "abstract": "We present a novel approach in machine learning by combining na\u0131\u0308ve Bayes classifiers with tree kernels. Tree kernel methods produce promising results in machine learning tasks containing treestructured attribute values. These kernel methods are used to compare two tree-structured attribute values recursively. Up to now tree kernels are only used in kernel machines like Support Vector Machines or Perceptrons. In this paper, we show that tree kernels can be utilized in a na\u0131\u0308ve Bayes classifier enabling the classifier to handle tree-structured values. We evaluate our approach on three datasets containing tree-structured values. We show that our approach using tree-structures delivers significantly better results in contrast to approaches using non-structured (flat) features extracted from the tree. Additionally, we show that our approach is significantly faster than comparable kernel machines in several settings which makes it more useful in resource-aware settings like mobile devices. Na\u0131\u0308ve Bayes Classifier; Tree Kernel; Lazy Learning; Tree-structured Values",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1685612",
                    "name": "M. Spiliopoulou"
                },
                {
                    "authorId": "1759689",
                    "name": "A. N\u00fcrnberger"
                },
                {
                    "authorId": "145737032",
                    "name": "R. Schult"
                },
                {
                    "authorId": "1689519",
                    "name": "B. Preim"
                },
                {
                    "authorId": "5912522",
                    "name": "S. Oeltze"
                },
                {
                    "authorId": "2983978",
                    "name": "Sebastian Stober"
                },
                {
                    "authorId": "1972250",
                    "name": "Alexander Hinneburg"
                },
                {
                    "authorId": "2160730",
                    "name": "Iris Ad\u00e4"
                },
                {
                    "authorId": "1683418",
                    "name": "M. Berthold"
                },
                {
                    "authorId": "143601244",
                    "name": "C. Makris"
                },
                {
                    "authorId": "2216501",
                    "name": "S. Stamou"
                },
                {
                    "authorId": "1817592",
                    "name": "E. Theodoridis"
                },
                {
                    "authorId": "2279741",
                    "name": "Paraskevi Tzekou"
                },
                {
                    "authorId": "2287942",
                    "name": "A. Busche"
                },
                {
                    "authorId": "1728442",
                    "name": "A. Nanopoulos"
                },
                {
                    "authorId": "1388781075",
                    "name": "L. Schmidt-Thieme"
                },
                {
                    "authorId": "1965643",
                    "name": "Daniel Klan"
                },
                {
                    "authorId": "2522511",
                    "name": "Stefan Hagedorn"
                },
                {
                    "authorId": "2098602342",
                    "name": "Steffen Hirte"
                },
                {
                    "authorId": "2232254",
                    "name": "H. Betz"
                },
                {
                    "authorId": "39554201",
                    "name": "F. Kassem"
                },
                {
                    "authorId": "1716839",
                    "name": "K. Sattler"
                },
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "3242843",
                    "name": "A. Tehrani"
                },
                {
                    "authorId": "49767752",
                    "name": "Weiwei Cheng"
                },
                {
                    "authorId": "1836428",
                    "name": "K. Dembczynski"
                },
                {
                    "authorId": "1691955",
                    "name": "Eyke H\u00fcllermeier"
                }
            ]
        },
        {
            "paperId": "f07c4afa4cb06e3cbeabd072b693624773a9c0e1",
            "title": "Sprachraumerstellung als Bestandteil der Gesch\u00e4ftsprozessmodellierung in SOA",
            "abstract": "Fur die Implementierung der annotationsbasierten Geschaftsmodellierung ist es notwendig, die erforderlichen Bedingungen nach bestimmten Kriterien zu erstellen. Grundlage dafur ist ein Sprachraum, aus welchem die in den Bedingungssatzen enthaltenen Begriffe entnommen werden. Dieser Sprachraum besteht im Wesentlichen aus Konzepten und Klassen, die diese Konzepte reprasentieren, sowie Fakttypen. Die Vorgehensweise der Erstellung eines Sprachraums fur das Precision Dairy Farming ist Gegenstand dieses Artikels.",
            "fieldsOfStudy": [
                "Computer Science",
                "Political Science"
            ],
            "authors": [
                {
                    "authorId": "3042936",
                    "name": "Franziska Gietl"
                },
                {
                    "authorId": "2094551795",
                    "name": "J. Spilke"
                },
                {
                    "authorId": "2072699345",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "f6f27d3ff28ca1544b7c25f7e6348756eedb842b",
            "title": "Touch it, Mine it, View it, Shape it",
            "abstract": "To benefit from the large amounts of data, gathered in more and more application domains, analysis techniques like clustering have become a necessity. As their application expands, a lot of unacquainted users come into contact with these techniques. Unfortunately, most clustering approaches are complex and/or scenario specific, which makes clustering a challenging domain to access. In this demonstration, we want to present a clustering process, that can be used in a hands-on way.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "0ba0bcd6b1df479f662bf3a7dcff92878d77fc42",
            "title": "Ein begriffsbasierter Ansatz zur semantischen Extraktion von Datenbankschemata",
            "abstract": "Die durch das rasante Anwachsen digitaler Datenbest\u00e4nde in Volumen und Vielfalt notwendig gewordene effiziente Verwaltung der erhobenen Datenbest\u00e4nde, bringt herk\u00f6mmliche Datenbankmethoden an ihre Grenzen. Ein modelliertes Datenbankschema zur Grundstrukturierung der Datenbank kann l\u00e4ngst nicht mehr statisch rigide modelliert werden. Vielmehr werden schemaflexible Datenbanken ben\u00f6tigt, die ihr Schema entsprechend an \u00c4nderungen im Datenbestand anpassen k\u00f6nnen. Da das Datenbankschema basierend auf einer konzeptuellen Datenbanksicht modelliert wird, pr\u00e4sentieren wir einen Ansatz, der die Formale Begriffsanalyse als Modellierungsmethode einsetzt. Die Formale Begriffsanalyse greift genau diese begriffsorientierte Weltsicht auf. Damit k\u00f6nnen wir Schemaextraktion und weiterf\u00fchrende Problemstellungen mit wohl verstandenen und gut untersuchten Mechanismen behandeln. Im Rahmen dieses Beitrages stellen wir ein begriffsbasiertes Verfahren zur Schemaextraktion vor, das sich genau diese konzeptuelle Weltsicht zu Nutze macht.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "90837001",
                    "name": "H. M\u00fchle"
                },
                {
                    "authorId": "50213741",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "21ae4b8227bfd4ac11cd3fa1ef24b8a20cd3b7b7",
            "title": "Database as a service (DBaaS)",
            "abstract": "Modern Web or \u201cEternal-Beta\u201d applications necessitate a flexible and easy-to-use data management platform that allows the evolutionary development of databases and applications. The classical approach of relational database systems following strictly the ACID properties has to be extended by an extensible and easy-to-use persistency layer with specialized DB features. Using the underlying concept of Software as a Service (SaaS) also enables an economic advantage based on the \u201ceconomy of the scale\u201c, where application and system environments only need to be provided once but can be used by thousands of users. Within this tutorial, we are looking at the current state-of-the-art from different perspectives. We outline foundations and techniques to build database services based on the SaaS-paradigm. We discuss requirements from a programming perspective, show different dimensions in the context of consistency and reliability, and also describe different non-functional properties under the umbrella of Service-Level agreements (SLA).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1716839",
                    "name": "K. Sattler"
                }
            ]
        },
        {
            "paperId": "3c0494558d3f1d6b91e27f342c8ec23e595082b6",
            "title": "How to juggle columns: an entropy-based approach for table compression",
            "abstract": "Many relational databases exhibit complex dependencies between data attributes, caused either by the nature of the underlying data or by explicitly denormalized schemas. In data warehouse scenarios, calculated key figures may be materialized or hierarchy levels may be held within a single dimension table. Such column correlations and the resulting data redundancy may result in additional storage requirements. They may also result in bad query performance if inappropriate independence assumptions are made during query compilation. In this paper, we tackle the specific problem of detecting functional dependencies between columns to improve the compression rate for column-based database systems, which both reduces main memory consumption and improves query performance. Although a huge variety of algorithms have been proposed for detecting column dependencies in databases, we maintain that increased data volumes and recent developments in hardware architectures demand novel algorithms with much lower runtime overhead and smaller memory footprint. Our novel approach is based on entropy estimations and exploits a combination of sampling and multiple heuristics to render it applicable for a wide range of use cases. We demonstrate the quality of our approach by means of an implementation within the SAP NetWeaver Business Warehouse Accelerator. Our experiments indicate that our approach scales well with the number of columns and produces reliable dependence structure information. This both reduces memory consumption and improves performance for nontrivial queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2587068",
                    "name": "M. Paradies"
                },
                {
                    "authorId": "12714015",
                    "name": "Christian Lemke"
                },
                {
                    "authorId": "1810943",
                    "name": "H. Plattner"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1716839",
                    "name": "K. Sattler"
                },
                {
                    "authorId": "1746379",
                    "name": "A. Zeier"
                },
                {
                    "authorId": "145216039",
                    "name": "Jens Kr\u00fcger"
                }
            ]
        },
        {
            "paperId": "3ea8a93b2fa8ad3e4935e0243e6e322299fac1dc",
            "title": "Fast integer compression using SIMD instructions",
            "abstract": "We study algorithms for efficient compression and decompression of a sequence of integers on modern hardware. Our focus is on universal codes in which the codeword length is a monotonically non-decreasing function of the uncompressed integer value; such codes are widely used for compressing \"small integers\". In contrast to traditional integer compression, our algorithms make use of the SIMD capabilities of modern processors by encoding multiple integer values at once. More specifically, we provide SIMD versions of both null suppression and Elias gamma encoding. Our experiments show that these versions provide a speedup from 1.5x up to 6.7x for decompression, while maintaining a similar compression performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "1777107",
                    "name": "Rainer Gemulla"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "5242cbb61a61779592e0f0f983a779636a8c58f3",
            "title": "Cherry picking in database languages",
            "abstract": "To avoid expensive round-trips between the application layer and the database layer it is crucial that data-intensive processing and calculations happen close to where the data resides -- ideally within the database engine. However, each application has its own domain and provides domain-specific languages (DSL) as a user interface to keep interactions confined within the well-known metaphors of the respective domain. Revealing the innards of the underlying data layer by forcing users to formulate problems in terms of a general database language is often not an option. To bridge that gap, we propose an approach to transform and directly compile a DSL into a general database execution plan using graph transformations. We identify the commonalities and mismatches between different models and show which parts can be cherry-picked for direct translation. Finally, we argue that graph transformations can be used in general to translate a DSL into an executable plan for a database.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2864302",
                    "name": "Bernhard J\u00e4cksch"
                },
                {
                    "authorId": "1708337",
                    "name": "Franz F\u00e4rber"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "5d8776d6e350688f7739a83b25b0fc08f6758301",
            "title": "Multi-process Optimization Via Horizontal Message Queue Partitioning",
            "abstract": "Message-oriented integration platforms execute integration processes\u2014in the sense of workflow-based process specifications of integration tasks\u2014in order to exchange data between heterogeneous systems and applications. The overall optimization objective is throughput maximization, i.e., maximizing the number of processed messages per time period. Here, moderate latency time of single messages is acceptable. The efficiency of the central integration platform is crucial for enterprise data management because both the data consistency between operational systems and the up-to-dateness of analytical query results depend on it. With the aim of integration process throughput maximization, we propose the concept of multi-process optimization (MPO). In this approach, messages are collected during a waiting period and executed in batches to optimize sequences of process instances of a single process plan. We introduce a horizontal\u2014and thus, valuebased\u2014partitioning approach for message batch creation and show how to compute the optimal waiting time with regard to throughput maximization. This approach significantly reduces the total processing time of a message sequence and hence, it maximizes the throughput while accepting moderate latency time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "68702f053917f26ada0a6c8873e0bf031ca65115",
            "title": "Evolving Ensemble-Clustering to a Feedback-Driven Process",
            "abstract": "Data clustering is a highly used knowledge extraction technique and is applied in more and more application domains. Over the last years, a lot of algorithms have been proposed that are often complicated and/or tailored to specific scenarios. As a result, clustering has become a hardly accessible domain for non-expert users, who face major difficulties like algorithm selection and parameterization. To overcome this issue, we develop a novel feedback-driven clustering process using a new perspective of clustering. By substituting parameterization with user-friendly feedback and providing support for result interpretation, clustering becomes accessible and allows the step-by-step construction of a satisfying result through iterative refinement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "688acd44370e0391bbf391b2471875c46c624f59",
            "title": "Process-based Data Streaming in Service-oriented Environments - Application and Technique",
            "abstract": "Service-oriented environments increasingly become the central point for enterprise-related workflows. This also holds for data-intensive service applications, where such process types encounter performance and resource issues. To tackle these issues on a more conceptual level, we propose a stream-based process execution that is inspired by the typical execution semantics in data management environments. More specifically, we present a data and process model including a generalized concept for stream-based services. In our evaluation, we show that our approach outperforms the execution model of current service-oriented process environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1732902",
                    "name": "Steffen Preissler"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "6a9d50b60992a25f1862d858db9aaaf0904160e5",
            "title": "Serviceorientierte Architektur im Precision Dairy Farming aus der Perspektive der Informatik",
            "abstract": "Als Basis f\u00fcr ein effizientes Informationsmanagement im Umfeld des Precision Dairy Farming (PDF) bietet sich der Einsatz einer serviceorientierten Architektur (SOA) an. Der konsequente Einsatz einer serviceorientierten Architektur bringt jedoch nicht nur Vorteile mit sich, sondern verursacht auch einen erheblichen organisatorischen Aufwand. In diesem Artikel zeigen wir die Vorteile f\u00fcr den Einsatz einer SOA innerhalb des PDF auf und pr\u00e4sentieren einen Ansatz, um den organisatorischen Aufwand effizient von Beginn an zu minimieren.",
            "fieldsOfStudy": [
                "Computer Science",
                "Geography"
            ],
            "authors": [
                {
                    "authorId": "2072699345",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "7b88070e636f015afd1e9cbaf329328acaf1f101",
            "title": "GPU-Based Speculative Query Processing for Database Operations",
            "abstract": "With an increasing amount of data and user demands for fast query processing, the optimization of database operations continues to be a challenging task. A common optimization method is to leverage parallel hardware architectures. With the introduction of general-purpose GPU computing, massively parallel hardware has become available within commodity hardware. To efficiently exploit this technology, we introduce the method of speculative query processing. This speculative query processing works on, but is not limited to, a prefix tree structure to efficiently support heavily used database index operations. Fundamentally, our developed approach traverse a prefix tree structure in a speculative, parallel way instead of a step-by-step traversing. To show the benefits and opportunities of our novel approach, we present an exhaustive evaluation on a graphical processing unit.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060654334",
                    "name": "P. Volk"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "7fd8ee6c4426f440f441e63e2ecf630c8caf3cf5",
            "title": "A plan for OLAP",
            "abstract": "So far, data warehousing has often been discussed in the light of complex OLAP queries and as reporting facility for operative data. We argue that business planning as a means to generate plan data is an equally important cornerstone of a data warehouse system, and we propose it to be a first-class citizen within an OLAP engine. We introduce an abstract model describing relevant aspects of the planning process in general and the requirements it poses to a planning engine. Furthermore, we show that business planning lends itself well to parallelization and benefits from a column-store much like traditional OLAP does. We then develop a physical model specifically targeted at a highly parallel column-store, and with our implementation, we show nearly linear scaling behavior.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2864302",
                    "name": "Bernhard J\u00e4cksch"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1708337",
                    "name": "Franz F\u00e4rber"
                }
            ]
        },
        {
            "paperId": "9fa52249fb5c02f45a963298da8193aea640750e",
            "title": "Using Cloud Technologies to Optimize Data-Intensive Service Applications",
            "abstract": "The role of data analytics increases in several application domains to cope with the large amount of captured data. Generally, data analytics are data-intensive processes, whose efficient execution is a challenging task. Each process consists of a collection of related structured activities, where huge data sets have to be exchanged between several loosely coupled services. The implementation of such processes in a service-oriented environment offers some advantages, but the efficient realization of data flows is difficult. Therefore, we use this paper to propose a novel SOA-aware approach with a special focus on the data flow. The tight interaction of new cloud technologies with SOA technologies enables us to optimize the execution of data-intensive service applications by reducing the data exchange tasks to a minimum. Fundamentally, our core concept to optimize the data flows is found in data clouds. Moreover, we can exploit our approach to derive efficient process execution strategies regarding different optimization objectives for the data flows.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2625926",
                    "name": "Sebastian Richly"
                },
                {
                    "authorId": "143681675",
                    "name": "U. Assmann"
                }
            ]
        },
        {
            "paperId": "a44c6dce47c21961a1428ad7321ccd66f8dcc905",
            "title": "Listen to the customer: model-driven database design",
            "abstract": "In modern IT landscapes, databases are subject to a major role change. Especially in Service-Oriented Architectures, databases are more and more frequently dedicated to a single application. Therefore, it is even more important to reflect the application requirements in their design. Software developers and application experts formulate application requirements in software models. Hence, we obviously need to bridge the gap to the software world and directly derive a database design from the software models used in application development and maintenance. We introduce this concept as model-driven database design. In this paper, we present the architecture principles of a model-driven database design tool and details on the enumeration and evaluation of logical database designs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "40466466",
                    "name": "K. Herrmann"
                },
                {
                    "authorId": "32019086",
                    "name": "Tim Kiefer"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "afeb0df30879fe3ca07b06e1aa6c70e7edd06895",
            "title": "Pairwise Element Computation with MapReduce",
            "abstract": "In this paper, we present a parallel method to evaluate functions on pairs of elements. It is a challenge to partition the Cartesian product of a set with itself in order to parallelize the function evaluation on all pairs. Our solution uses (a) replication of set elements to allow for partitioning and (b) aggregation of the results gathered for different copies of an element. Based on an execution model with nodes that execute tasks on local data without online communication, we present a generic algorithm and show how it can be implemented with MapReduce. Three different distribution schemes that define the partitioning of the Cartesian product are introduced, compared, and evaluated. Any one of the distribution schemes can be used to derive and implement a specific algorithm for parallel pairwise element computation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32019086",
                    "name": "Tim Kiefer"
                },
                {
                    "authorId": "2060654334",
                    "name": "P. Volk"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "cafb9d7cd7983d24efa1fb3cccd584ef4c6e151d",
            "title": "One Clustering Process Fits All - A Visually Guided Ensemble Approach",
            "abstract": "Looking back on the past decade of research on clustering algorithms, we witness two major and apparent trends: 1) The already vast amount of existing clustering algorithms, is continuously broadened and 2) clustering algorithms in general, are becoming more and more adapted to specific application domains with very particular assumptions. As a result, algorithms have grown complicated and/or very scenariodependent, which made clustering a hardly accessible domain for non-expert users. This is an especially critical development, since, due to increasing data gathering, the need for analysis techniques like clustering emerges in many application domains. In this paper, we oppose the current focus on specialization, by proposing our vision of a usable, guided and universally applicable clustering process. In detail, we are going to describe our already conducted work and present our future research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "d5261f50e24f36abed0020ddbe34017195e0580d",
            "title": "Indexing forecast models for matching and maintenance",
            "abstract": "Forecasts are important to decision-making and risk assessment in many domains. There has been recent interest in integrating forecast queries inside a DBMS. Answering a forecast query requires the creation of forecast models. Creating a forecast model is an expensive process and may require several scans over the base data as well as expensive operations to estimate model parameters. However, if forecast queries are issued repeatedly, answer times can be reduced significantly if forecast models are reused. Due to the possibly high number of forecast queries, existing models need to be found quickly. Therefore, we propose a model index that efficiently stores forecast models and allows for the efficient reuse of existing ones. Our experiments illustrate that the model index shows a negligible overhead for update transactions, but it yields significant improvements during query execution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "46554189",
                    "name": "Frank Rosenthal"
                },
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "ed2278a421e493736574c08f1ab7001c4995be6f",
            "title": "Annotationsbasierte Prozessmodellierung in SOA - dargestellt an einem Beispiel aus dem Precision Dairy Farming",
            "abstract": "Bei der Entwicklung einer serviceorientierten Architektur im Bereich des Precision Dairy Farmings haben wir uns mit der Modellierung unternehmensubergreifender Prozesse mit Hilfe der Business Process Modeling Notation (BPMN) beschaftigt. Da diese Modellierung stellenweise sehr abstrakt ist, schlagen wir einen angepassten Modellierungsansatz unter der Verwendung von Annotationen vor. Damit konnen notwendige Bedingungen direkt dem betreffenden Objekt zugeordnet werden, wodurch die Modellierung fachbezogener und damit fur den Nutzer transparenter wird.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3042936",
                    "name": "Franziska Gietl"
                },
                {
                    "authorId": "2094551795",
                    "name": "J. Spilke"
                },
                {
                    "authorId": "2072699345",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "0384cc415fb64233886557599edba22358e082bf",
            "title": "Innovative Process Execution in Service-oriented Environments",
            "abstract": "Today\u2019s information systems are often built on the foundation of service-oriented environments. Although the fundamental purpose of an information system is the processing of data and information, the service-oriented architecture (SOA) does not treat data as a core first class citizen. Current SOA technologies support neither the explicit modeling of data flows in common business process modeling languages (such as BPMN) nor the usage of specialized data transformation and propagation technologies (for instance ETL-tools) on the process execution layer (BPEL). In this paper, we introduce our data-aware approach on the execution perspective as well as on the modeling perspective of business processes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "1732902",
                    "name": "Steffen Preissler"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "11f952b58b2af03232a49ef2949668a4b94b15d0",
            "title": "Sample synopses for approximate answering of group-by queries",
            "abstract": "With the amount of data in current data warehouse databases growing steadily, random sampling is continuously gaining in importance. In particular, interactive analyses of large datasets can greatly benefit from the significantly shorter response times of approximate query processing. Typically, those analytical queries partition the data into groups and aggregate the values within the groups. Further, with the commonly used roll-up and drill-down operations a broad range of group-by queries is posed to the system, which makes the construction of highly-specialized synopses difficult.\n In this paper, we propose a general-purpose sampling scheme that is biased in order to answer group-by queries with high accuracy. While existing techniques focus on the size of the group when computing its sample size, our technique is based on its standard deviation. The basic idea is that the more homogeneous a group is, the less representatives are required in order to give a good estimate. With an extensive set of experiments, we show that our approach reduces both the estimation error and the construction cost compared to existing techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2932139",
                    "name": "Philipp J. R\u00f6sch"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "1b725632f0162c6a687138c8fe92eca6456bf8b7",
            "title": "Representing Data Quality in Sensor Data Streaming Environments",
            "abstract": "Sensors in smart-item environments capture data about product conditions and usage to support business decisions as well as production automation processes. A challenging issue in this application area is the restricted quality of sensor data due to limited sensor precision and sensor failures. Moreover, data stream processing to meet resource constraints in streaming environments introduces additional noise and decreases the data quality. In order to avoid wrong business decisions due to dirty data, quality characteristics have to be captured, processed, and provided to the respective business task. However, the issue of how to efficiently provide applications with information about data quality is still an open research problem.\n In this article, we address this problem by presenting a flexible model for the propagation and processing of data quality. The comprehensive analysis of common data stream processing operators and their impact on data quality allows a fruitful data evaluation and diminishes incorrect business decisions. Further, we propose the data quality model control to adapt the data quality granularity to the data stream interestingness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064804569",
                    "name": "Anja Klein"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2774c78814d3cc6c359a70dfe8bf7c6722a6080c",
            "title": "Robust Distributed Top-N Frequent Pattern Mining Using the SAP BW Accelerator",
            "abstract": "Mining for association rules and frequent patterns is a central activity in data mining. However, most existing algorithms are only moderately suitable for real-world scenarios. Most strategies use parameters like minimum support, for which it can be very difficult to define a suitable value for unknown datasets. Since most untrained users are unable or unwilling to set such technical parameters, we address the problem of replacing the minimum-support parameter with top-n strategies. In our paper, we start by extending a top-n implementation of the ECLAT algorithm to improve its performance by using heuristic search strategy optimizations. Also, real-world datasets are often distributed and modern database architectures are switching from expensive SMPs to cheaper shared-nothing blade servers. Thus, most mining queries require distribution handling. Since partitioning can be forced by user-defined semantics, it is often forbidden to transform the data. Therefore, we developed an adaptive top-n frequent-pattern mining algorithm that simplifies the mining process on real distributions by relaxing some requirements on the results. We first combine the PARTITION and the TPUT algorithms to handle distributed top-n frequent-pattern mining. Then, we extend this new algorithm for distributions with real-world data characteristics. For frequent-pattern mining algorithms, equal distributions are important conditions, and tiny partitions can cause performance bottlenecks. Hence, we implemented an approach called MAST that defines a minimum absolute-support threshold. MAST prunes patterns with low chances of reaching the global top-n result set and high computing costs. In total, our approach simplifies the process of frequent-pattern mining for real customer scenarios and data sets. This may make frequent-pattern mining accessible for very new user groups. Finally, we present results of our algorithms when run on the SAP NetWeaver BW Acceleratorwith standard and real business datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1888037",
                    "name": "T. Legler"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1953738",
                    "name": "J. Schaffner"
                },
                {
                    "authorId": "2245043670",
                    "name": "J. Kr\u00fcger"
                }
            ]
        },
        {
            "paperId": "4385b6eec8b47da562fe291c866be00867fc8b71",
            "title": "Streaming Web Services and Standing Processes",
            "abstract": "A pipelined digital computer processor system (10, FIG. 1) is provided comprising an instruction prefetch unit (IPU,2) for prefetching instructions and an arithmetic logic processing unit (ALPU, 4) for executing instructions. The IPU (2) has associated with it a high speed instruction cache (6), and the ALPU (4) has associated with it a high speed operand cache (8). Each cache comprises a data store (84, 94, FIG. 3) for storing frequently accessed data, and a tag store (82, 92, FIG. 3) for indicating which main memory locations are contained in the respective cache. The IPU and ALPU processing units (2, 4) may access their associated caches independently under most conditions. When the ALPU performs a write operation to main memory, it also updates the corresponding data in the operand cache and, if contained therein, in the instruction cache permitting the use of self-modifying code. The IPU does not write to either cache. Provision is made for clearing the caches on certain conditions when their contents become invalid.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1732902",
                    "name": "Steffen Preissler"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "43a7241296c0b2c0eb365ab9253a3a2dffd081c3",
            "title": "k-ary search on modern processors",
            "abstract": "This paper presents novel tree-based search algorithms that exploit the SIMD instructions found in virtually all modern processors. The algorithms are a natural extension of binary search: While binary search performs one comparison at each iteration, thereby cutting the search space in two halves, our algorithms perform k comparisons at a time and thus cut the search space into k pieces. On traditional processors, this so-called k-ary search procedure is not beneficial because the cost increase per iteration offsets the cost reduction due to the reduced number of iterations. On modern processors, however, multiple scalar operations can be executed simultaneously, which makes k-ary search attractive. In this paper, we provide two different search algorithms that differ in terms of efficiency and memory access patterns. Both algorithms are first described in a platform independent way and then evaluated on various state-of-the-art processors. Our experiments suggest that k-ary search provides significant performance improvements (factor two and more) on most platforms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "1777107",
                    "name": "Rainer Gemulla"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "4d836bf4603359928fa2b31ae85bd4f5be17b768",
            "title": "Anfragegetriebene Indizierung r\u00e4umlicher Daten",
            "abstract": "Mit der zunehmenden Verbreitung von GPSund internetfahigen Smartphones werden ortsbezogene Informationsdienste immer beliebter. Zur Sicherung einer hohen Dienstqualitat werden die zugrundeliegenden Ortsinformationen indiziert. Bekannte Indexstrukturen fur raumliche Daten teilen diese gemas ihrer Verteilung auf, wodurch alle Anfragen gleich behandelt werden. Mochte man haufige Anfrage durch eine genauere Indizierung besonders unterstutzen, so muss sich die Aufteilung der Daten nicht an der Datenverteilung, sondern an der Anfrageverteilung orientieren. In diesem Papier stellen wir das QD-Grid vor, eine raumliche Indexstruktur, deren Indizierung sich inkrementell mit den gestellten Anfragen aufbaut. Zusatzlich prasentieren wir Evaluationsergebnisse.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50213741",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "2069977368",
                    "name": "Steffen Preissler"
                },
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "6770777847cd82345475d36d96bc694d6060ba98",
            "title": "Clustering Uncertain Data with Possible Worlds",
            "abstract": "The topic of managing uncertain data has been explored in many ways. Different methodologies for data storage and query processing have been proposed. As the availability of management systems grows, the research on analytics of uncertain data is gaining in importance. Similar to the challenges faced in the field of data management, algorithms for uncertain data mining also have a high performance degradation compared to their certain algorithms. To overcome the problem of performance degradation, the MCDB approach was developed for uncertain data management based on the possible world scenario. As this methodology shows significant performance and scalability enhancement, we adopt this method for the field of mining on uncertain data. In this paper, we introduce a clustering methodology for uncertain data and illustrate current issues with this approach within the field of clustering uncertain data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060654334",
                    "name": "P. Volk"
                },
                {
                    "authorId": "46554189",
                    "name": "Frank Rosenthal"
                },
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "69f756c4c5d99970ada53b493ec2f2a3e1e15eeb",
            "title": "Cardinality estimation in ETL processes",
            "abstract": "The cardinality estimation in ETL processes is particularly difficult. Aside from the well-known SQL operators, which are also used in ETL processes, there are a variety of operators without exact counterparts in the relational world. In addition to those, we find operators that support very specific data integration aspects. For such operators, there are no well-examined statistic approaches for cardinality estimations. Therefore, we propose a black-box approach and estimate the cardinality using a set of statistic models for each operator. We discuss different model granularities and develop an adaptive cardinality estimation framework for ETL processes. We map the abstract model operators to specific statistic learning approaches (regression, decision trees, support vector machines, etc.) and evaluate our cardinality estimations in an extensive experimental study.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "32019086",
                    "name": "Tim Kiefer"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "80246ec890913df49aab7d65a8079b6292d0510a",
            "title": "GCIP: exploiting the generation and optimization of integration processes",
            "abstract": "As a result of the changing scope of data management towards the management of highly distributed systems and applications, integration processes have gained in importance. Such integration processes represent an abstraction of workflow-based integration tasks. In practice, integration processes are pervasive and the performance of complete IT infrastructures strongly depends on the performance of the central integration platform that executes the specified integration processes. In this area, the three major problems are: (1) significant development efforts, (2) low portability, and (3) inefficient execution. To overcome those problems, we follow a model-driven generation approach for integration processes. In this demo proposal, we want to introduce the so-called GCIP Framework (Generation of Complex Integration Processes) which allows the modeling of integration process and the generation of different concrete integration tasks. The model-driven approach opens opportunities for rule-based and workload-based optimization techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "2776810",
                    "name": "Uwe Wloka"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "98c0765d8af3c09f112bcc65dc857fa47906b0c0",
            "title": "Stream-Based Web Service Invocation",
            "abstract": "A large document scanner has a transparent top plate glass plate large enough to support an E size drawing. The base of the cabinet supports a trapezoidal mirror tilted up from the bottom with the larger parallel side above the bottom. The image of a document on the transparent plate is reflected by the trapezoidal mirror to a digital imaging camera system having a mechanically displaced electronically scanned array located inside the cabinet to the side. Tubular lamps located adjacent to bowed out upper sides between upper and lower planar reflectors illuminate the document on the transparent top glass plate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1732902",
                    "name": "Steffen Preissler"
                },
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "b278956dfb4ee080c1dc1a1e271ce3c3b2161e37",
            "title": "How to Screen a Data Stream - Quality-Driven Load Shedding in Sensor Data Streams",
            "abstract": "As most data stream sources exhibit bursty data rates, data stream management systems must recurrently cope with load spikes that exceed the average workload to a considerable degree. To guarantee low-latency processing results, load has to be shed from the stream, when data rates overstress system resources. There exist numerous load shedding strategies to delete excess data. However, the consequent data loss leads to incomplete and/or inaccurate results during the ongoing stream processing. In this paper, we present a novel quality-driven load shedding approach that screens the data stream to find and discard data items of minor quality. The data quality of stream processing results is maximized under the adverse condition of data overload. After an introduction to data quality management in data streams, we define three data quality-driven load shedding algorithms, which minimize the approximation error of aggregations and maximize the completeness of join processing results, respectively. Finally, we demonstrate their superiority over existing load shedding techniques at real-life weather data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064804569",
                    "name": "Anja Klein"
                },
                {
                    "authorId": "3068811",
                    "name": "Gregor Hackenbroich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "b4a3342db7e61d8ff56475997edfa3a1cdcceab4",
            "title": "Global Slope Change Synopses for Measurement Maps",
            "abstract": "Quality control using scalar quality measures is standard practice in manufacturing. However, there are also quality measures that are determined at a large number of positions on a product, since the spatial distribution is important. We denote such a mapping of local coordinates on the product to values of a measure as a measurement map. In this paper, we examine how measurement maps can be clustered according to a novel notion of similarity\u2014mapscape similarity\u2014that considers the overall course of the measure on the map. We present a class of synopses called global slope change that uses the profile of the measure along several lines from a reference point to different points on the borders to represent a measurement map. We conduct an evaluation of global slope change using a real-world data set from manufacturing and demonstrate its superiority over other synopses.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "46554189",
                    "name": "Frank Rosenthal"
                },
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "2060654334",
                    "name": "P. Volk"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "da391d2e799b77b74a80ab3bf66273b84549d0f4",
            "title": "System\u00fcbergreifende Kostennormalisierung f\u00fcr Integrationsprozesse",
            "abstract": "Zusammenfassung Auf Grundlage der Vielzahl propriet\u00e4rer Integrationssysteme ist zunehmend die Entwicklung von Ans\u00e4tzen zur modellbasierten Generierung von Integrationsprozessen zu beobachten. Eine derartige Generierung bietet weiterhin die M\u00f6glichkeit der Auswahl des optimalen Integrationssystems, welche ein hohes Optimierungspotenzial in sich birgt. Die Grundlage f\u00fcr eine solche Entscheidung ist jedoch eine integrationssystem\u00fcbergreifende Kostennormalisierung, um die Vergleichbarkeit von Verarbeitungsstatistiken zu erm\u00f6glichen. Basierend auf einem plattformunabh\u00e4ngigen Kostenmodell und der system\u00fcbergreifenden Kostennormalisierung kann eine kostenbasierte Optimalit\u00e4tsentscheidung hinsichtlich des effizientesten Integrationssystems getroffen werden. Folglich k\u00f6nnen hierbei ver\u00e4nderliche Workload-Charakteristika in die Betrachtung einbezogen werden. In diesem Papier zeigen wir zun\u00e4chst die Probleme der system\u00fcbergreifenden Kostenmodellierung und Kostennormalisierung auf, welche bislang noch nicht betrachtet wurden. Darauf aufbauend diskutieren wir die plattforminvariante Kostenmodellierung und die Erhebung von Verarbeitungsstatistiken. Weiterhin f\u00fchren wir drei Algorithmen zur Kostennormalisierung ein, mit denen die Vergleichbarkeit von Verarbeitungsstatistiken sichergestellt wird. Als Konsequenz kann das vorgestellte Konzept in beliebigen Ans\u00e4tzen der Generierung von Integrationsprozessen zum Einsatz kommen.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "2072699345",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "72337459",
                    "name": "U. Wloka"
                }
            ]
        },
        {
            "paperId": "df90141688c9474faead5cc92db7dde5ea968836",
            "title": "Standing Processes in Service-Oriented Environments",
            "abstract": "Current realization techniques for service-oriented architectures (SOA) and business process management (BPM) cannot be efficiently applied to any kind of application scenario. For example, an important requirement in the finance sector is the continuous evaluation of stock prices to automatically trigger business processes--e.g. the buying or selling of stocks--with regard to several strategies. In this paper, we address the continuous evaluation of message streams within BPM to establish a common environment for stream-based message processing and traditional business processes. In detail, we propose the notion of standing processes as (i) a process-centric concept for the interpretation of message streams, and (ii) a trigger element for subsequent business processes. The demonstration system focuses on the execution of standing processes and the smooth interaction with the traditional business process environment.",
            "fieldsOfStudy": [
                "Materials Science",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1732902",
                    "name": "Steffen Preissler"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "e37901d90c9cff649167c03dc539931e734c9553",
            "title": "How to Optimize the Quality of Sensor Data Streams",
            "abstract": "Present data stream management systems allow the automatic recording and processing of huge data volumes to guide any kind of process control or business decision. However, a crucial problem is posed by data quality deficiencies due to imprecise sensors, environmental influences, transfer failures, etc. If not handled carefully, they lead to misguided decisions and inappropriate actions. In this paper, we present the quality-driven optimization of stream processing to improve the resulting quality of data and service. First, we present the optimization objectives and discuss the parameterization of stream processing operators to define the underlying optimization problem. We develop the generic optimization framework and present the quality-driven evolution strategy (QES). Finally, we show that the designed optimization scales very well with regard to processing complexity and reduces numerical errors in the contact lens production monitoring.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064804569",
                    "name": "Anja Klein"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "0a3588f06d80d7f509a5eea69f16597a2df3c154",
            "title": "Pre analysis and clustering of uncertain data from manufacturing processes",
            "abstract": "With increasing complexity of manufacturing processes, the volume of data that has to be evaluated rises accordingly. The complexity and data volume make any kind of manual data analysis infeasable. At this point, data mining techniques become interesting. The application of current techniques is of complex nature because most of the data is captured by sensor measurement tools. Therefore, every measured value contains a specific error. In this paper, we propose an erroraware extension of the density-based algorithm DBSCAN. Furthermore, we discuss some quality measures that could be utilized for further interpretations of the determined clustering results. Additionally, we introduce the concept of pre-analysis during a necessary data integration step for the proposed algorithm. With this concept, the runtime of the error-aware clustering algorithm can be optimized and the integration of data mining in the overall software landscape can be promoted further .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060654334",
                    "name": "P. Volk"
                },
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "35e2534d93f9005499a57d5f6c59563017052fe1",
            "title": "Exploiting Graphic Card Processor Technology to Accelerate Data Mining Queries in SAP NetWeaver BIA",
            "abstract": "Within business Intelligence contexts, the importance of data mining algorithms is continuously increasing, particularly from the perspective of applications and users that demand novel algorithms on the one hand and an efficient implementation exploiting novel system architectures on the other hand. Within this paper, we focus on the latter issue and report our experience with the exploitation of graphic card processor technology within the SAP NetWeaver business intelligence accelerator (BIA). The BIA represents a highly distributed analytical engine that supports OLAP and data mining processing primitives. The system organizes data entities in column-wise fashion and its operation is completely main-memory-based. Since case studies have shown that classic data mining queries spend a large portion of their runtime on scanning and filtering the data as a necessary prerequisite to the actual mining step, our main goal was to speed up this expensive scanning and filtering process. In a first step, the paper outlines the basic data mining processing techniques within SAP NetWeaver BIA and illustrates the implementation of scans and filters. In a second step, we give insight into the main features of a hybrid system architecture design exploiting graphic card processor technology. Finally, we sketch the implementation and give details of our vast evaluations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2720173",
                    "name": "Christoph Weyerhaeuser"
                },
                {
                    "authorId": "3170577",
                    "name": "Tobias Mindnich"
                },
                {
                    "authorId": "1708337",
                    "name": "Franz F\u00e4rber"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "3eec39d28023de623429d5938290bdb538f4601c",
            "title": "Message Indexing for Document-Oriented Integration Processes",
            "abstract": "The integration of heterogeneous systems is still an evolving research area. Due to the complexity of integration processes, there are challenges for the optimization of integration processes. Message-based integration systems, like EAI servers and workflow process engines, are mostly document-oriented, using XML technologies to achieve suitable data independence from the different and particular proprietary data representations of the supported external systems. However, such an approach causes large costs for single-value evaluations within the integration processes. At this point, message indexing, adapting extended database technologies, can be applied in order to achieve better performance. In this paper, we introduce our message indexing structure MIX and discuss and evaluate immediate as well as deferred indexing concepts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "2776810",
                    "name": "Uwe Wloka"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "479e0ef4538aff1495abface6cd1eaef4cedc0f3",
            "title": "DIPBench Toolsuite: A Framework for Benchmarking Integration Systems",
            "abstract": "So far the optimization of integration processes between heterogeneous data sources is still an open challenge. A first step towards sufficient techniques was the specification of a universal benchmark for integration systems. This DIPBench allows to compare solutions under controlled conditions and would help generate interest in this research area. However, we see the requirement for providing a sophisticated toolsuite in order to minimize the effort for benchmark execution. This demo illustrates the use of the DIPBench toolsuite. We show the macro-architecture as well as the micro-architecture of each tool. Furthermore, we also present the first reference benchmark implementation using a federated DBMS. Thereby, we discuss the impact of the defined benchmark scale factors. Finally, we want to give guidance on how to benchmark other integration systems and how to extend the toolsuite with new distribution functions or other functionalities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2776810",
                    "name": "Uwe Wloka"
                }
            ]
        },
        {
            "paperId": "5a3c7805debdfe59d0184e1128893c5a572e8248",
            "title": "DIPBench: An independent benchmark for Data-Intensive Integration Processes",
            "abstract": "The integration of heterogeneous data sources is one of the main challenges within the area of data engineering. Due to the absence of an independent and universal benchmark for data-intensive integration processes, we propose a scalable benchmark, called DIPBench (Data intensive integration Process Benchmark), for evaluating the performance of integration systems. This benchmark could be used for subscription systems, like replication servers, distributed and federated DBMS or message-oriented middleware platforms like Enterprise Application Integration (EAI) servers and Extraction Transformation Loading (ETL) tools. In order to reach the mentioned universal view for integration processes, the benchmark is designed in a conceptual, process-driven way. The benchmark comprises 15 integration process types. We specify the source and target data schemas and provide a toolsuite for the initialization of the external systems, the execution of the benchmark and the monitoring of the integration system's performance. The core benchmark execution may be influenced by three scale factors. Finally, we discuss a metric unit used for evaluating the measured integration system's performance, and we illustrate our reference benchmark implementation for federated DBMS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2776810",
                    "name": "Uwe Wloka"
                }
            ]
        },
        {
            "paperId": "6334f0439aeca428dbc35bad06cadb58b1db8d2a",
            "title": "Designing Random Sample Synopses with Outliers",
            "abstract": "Random sampling is one of the most widely used means to build synopses of large datasets because random samples can be used for a wide range of analytical tasks. Unfortunately, the quality of the estimates derived from a sample is negatively affected by the presence of \"outliers\" in the data. In this paper, we show how to circumvent this shortcoming by constructing outlier-aware sample synopses. Our approach extends the well-known outlier indexing scheme to multiple aggregation columns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2932139",
                    "name": "Philipp J. R\u00f6sch"
                },
                {
                    "authorId": "1777107",
                    "name": "Rainer Gemulla"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "7f352795042c2a6234fb5438b495aa32ed77a490",
            "title": "Model-Driven Generation and Optimization of Complex Integration Processes",
            "abstract": "The integration of heterogeneous systems is still one of the main challenges in the area of data management. Its importance is based on the trend towards heterogeneous system environments, where the different levels of integration approaches result in a large number of different integration systems. Due to these proprietary solutions and the lack of a standard for data-intensive integration processes, the model-driven development\u2014 following the paradigm of the Model-Driven Architecture (MDA)\u2014is advantageous. This paper contributes to the model-driven development of complex and data-intensive integration processes. In addition, we illustrate optimization possibilities offered by this model-driven approach and discuss first evaluation results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "2776810",
                    "name": "Uwe Wloka"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "8ee9ec165064c5886a5a594272265c5952cecfad",
            "title": "Poster session: Constrained dynamic physical database design",
            "abstract": "Physical design has always been an important part of database administration. Today's commercial database management systems offer physical design tools, which recommend a physical design for a given workload. However, these tools work only with static workloads and ignore the fact that workloads, and physical designs, may change over time. Research has now begun to focus on dynamic physical design, which can account for time-varying workloads. In this paper, we consider a dynamic but constrained approach to physical design. The goal is to recommend dynamic physical designs that reflect major workload trends but that are not tailored too closely to the details of the input workloads. To achieve this, we constrain the number of changes that are permitted in the recommended design. In this paper we present our definition of the constrained dynamic physical design problem and discuss several techniques for solving it.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143842962",
                    "name": "H. Voigt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "145527641",
                    "name": "K. Salem"
                }
            ]
        },
        {
            "paperId": "936cc20fe8619d0d1439c241f467379d1455e45f",
            "title": "Quality of service and predictability in DBMS",
            "abstract": "DBMS are a ubiquitous building block of the software stack in many complex applications. Middleware technologies, application servers and mapping approaches hide the core database technologies just like power, networking infrastructure and operating system services. Furthermore, many enterprise-critical applications demand a certain degree of quality of service (QoS) or guarantees, e.g. wrt. response time, transaction throughput, latency but also completeness or more generally quality of results. Examples of such applications are billing systems in telecommunication, where each telephone call has to be monitored and registered in a database, Ecommerce applications where orders have to be accepted even in times of heavy load and the waiting time of customers should not exceed a few seconds, ERP systems processing a large number of transactions in parallel, or systems for processing streaming or sensor data in realtime, e.g. in process automation of traffic control. As part of complex multilevel software stack, database systems have to share or contribute to these QoS requirements, which means that guarantees have to be given by the DBMS, too, and that the processing of database requests is predictable. Todays mainstream DBMS typically follow a best effort approach: requests are processed as fast as possible without any guarantees: the optimization goal of query optimizers and tuning approaches is rather to minimize resource consumption instead of just fulfilling given service level agreements. However, motivated by the situation described above there is an emerging need for database services providing guarantees or simply behave in a predictable manner and at the same time interact with other components of the software stack in order to fulfill the requirements. This is also driven by the paradigm of service-oriented architectures widely discussed in industry. Currently, this is addressed only by very specialized solutions. Nevertheless, database researchers have developed several techniques contributing to the goal of QoS-aware database systems. The purpose of the tutorial is to introduce database researchers and practitioners to the scope, the challenges and the available techniques to the problem of predictability and QoS agreements in DBMS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1716839",
                    "name": "K. Sattler"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "9415bb7625adf6e8f3204515aaf088dafc28c86f",
            "title": "Improving Data Independence, Efficiency and Functional Flexibility of Integration Platforms",
            "abstract": "The concept of Enterprise Application Integration (EAI) is widely used for integrating heterogeneous applications and systems via message-based communication. Typically, EAI servers provide a huge set of specific inbound and outbound adapters used for interacting with the external systems and for converting proprietary message formats. However, the main problems in currently available products are the monolithic design of these adapters and performance deficits caused by the need for data independence. First, we classify and discuss these open problems. Second, we introduce our model-driven DIEFOS (data independence, efficiency and functional flexibility using feature-oriented software engineering) approach and show how the feature-based generation of dynamic adapters can improve data independence, efficiency and functional flexibility. Finally, we analyze open research challenges we see in this context.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "2059398737",
                    "name": "J\u00fcrgen Bittner"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2776810",
                    "name": "Uwe Wloka"
                }
            ]
        },
        {
            "paperId": "b092c13496f537a38895b84403b73c2514b609e5",
            "title": "An Advanced Transaction Model for Recovery Processing of Integration Processes",
            "abstract": "Integration processes are increasingly used in order to integrate distributed and heterogeneous systems. Although transactional behavior of workflows has been discussed extensively, recovery processing has been disregarded so far. Due to the huge number of different integration systems and models, there are also different transaction concepts with overlapping functionalities available. However, there is the need for a discussion of problem categories and guarantees in order to consolidate the existing transaction concepts. In this paper, we survey possible anomalies of recovery processing in message-oriented middleware and\u2014in conclusion\u2014we define the comprehensive transaction model SIR for data-intensive\u2014but instance-based\u2014integration processes. This model includes the definition of specific transaction levels, which are the precondition for the integration process optimization under transactional restrictions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2776810",
                    "name": "Uwe Wloka"
                }
            ]
        },
        {
            "paperId": "bbef28f26923945f4aa0835c08a9ceddae1504c1",
            "title": "Sampling time-based sliding windows in bounded space",
            "abstract": "Random sampling is an appealing approach to build synopses of large data streams because random samples can be used for a broad spectrum of analytical tasks. Users are often interested in analyzing only the most recent fraction of the data stream in order to avoid outdated results. In this paper, we focus on sampling schemes that sample from a sliding window over a recent time interval; such windows are a popular and highly comprehensible method to model recency. In this setting, the main challenge is to guarantee an upper bound on the space consumption of the sample while using the allotted space efficiently at the same time. The difficulty arises from the fact that the number of items in the window is unknown in advance and may vary significantly over time, so that the sampling fraction has to be adjusted dynamically. We consider uniform sampling schemes, which produce each sample of the same size with equal probability, and stratified sampling schemes, in which the window is divided into smaller strata and a uniform sample is maintained per stratum. For uniform sampling, we prove that it is impossible to guarantee a minimum sample size in bounded space. We then introduce a novel sampling scheme called bounded priority sampling (BPS), which requires only bounded space. We derive a lower bound on the expected sample size and show that BPS quickly adapts to changing data rates. For stratified sampling, we propose a merge-based stratification scheme (MBS), which maintains strata of approximately equal size. Compared to naive stratification, MBS has the advantage that the sample is evenly distributed across the window, so that no part of the window is over- or underrepresented. We conclude the paper with a feasibility study of our algorithms on large real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1777107",
                    "name": "Rainer Gemulla"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "caf2ca37f840b70ee263efb61d59c1970094f1ce",
            "title": "RiTE: Providing On-Demand Data for Right-Time Data Warehousing",
            "abstract": "Data warehouses (DWs) have traditionally been loaded with data at regular time intervals, e.g., monthly, weekly, or daily, using fast bulk loading techniques. Recently, the trend is to insert all (or only some) new source data very quickly into DWs, called near-realtime DWs (right-time DWs). This is done using regular INSERT statements, resulting in too low insert speeds. There is thus a great need for a solution that makes inserted data available quickly, while still providing bulk-load insert speeds. This paper presents RiTE (\"Right-Time ETL\"), a middleware system that provides exactly that. A data producer (ETL) can insert data that becomes available to data consumers on demand. RiTE includes an innovative main-memory based catalyst that provides fast storage and offers concurrency control. A number of policies controlling the bulk movement of data based on user requirements for persistency, availability, freshness, etc. are supported. The system works transparently to both producer and consumers. The system is integrated with an open source DBMS, and experiments show that it provides \"the best of both worlds\", i.e., INSERT-like data availability, but with bulk-load speeds (up to 10 times faster).",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12489091",
                    "name": "Christian Thomsen"
                },
                {
                    "authorId": "1731453",
                    "name": "T. Pedersen"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "cda00dad4e7ac4d9adfe7aaff223dd52a9b91a74",
            "title": "Workload-based optimization of integration processes",
            "abstract": "The efficient execution of integration processes between distributed, heterogeneous data sources and applications is a challenging research area of data management. These integration processes are an abstraction for workflow-based integration tasks, used in EAI servers and WfMS. The major problem are significant workload changes during runtime. The performance of integration processes strongly depends on those dynamic workload characteristics, and hence workload-based optimization is important. However, existing approaches of workflow optimization only address the rule-based optimization and disregard changing workload characteristics. To overcome the problem of inefficient process execution in the presence of workload shifts, here, we present an approach for the workload-based optimization of instance-based integration processes and show that significant execution time reductions are possible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "2776810",
                    "name": "Uwe Wloka"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "0270d2044ade041add8ffeb971c622bd8ecc7b23",
            "title": "Data-aware SOA for Gene Expression Analysis Processes",
            "abstract": "In the context of genome research, the method of gene expression analysis has been used for several years. Related microarray experiments are conducted all over the world, and consequently, a vast amount of microarray data sets are produced. Having access to this variety of repositories, researchers would like to incorporate this data in their analyses processes to increase the statistical significance of their results. Such analyses processes are typical examples of data-intensive processes. In general, data-intensive processes are characterized by (i) a sequence of functional operations processing large amount of data and (ii) the transportation and transformation of huge data sets between the functional operations. To support data-intensive processes, an efficient and scalable environment is required, since the performance is a key factor today. The service-oriented architecture (SOA) is beneficial in this area according to process orchestration and execution. However, the current realization of SOA with Web services and BPEL includes some drawbacks with regard to the performance of the data propagation between Web services. Therefore, we present in this paper our data-aware service-oriented approach to efficiently support such data-intensive processes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2625926",
                    "name": "Sebastian Richly"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "143681675",
                    "name": "U. Assmann"
                },
                {
                    "authorId": "2867481",
                    "name": "M. Grasselt"
                },
                {
                    "authorId": "144579342",
                    "name": "A. Maier"
                },
                {
                    "authorId": "47619614",
                    "name": "C. Pilarsky"
                }
            ]
        },
        {
            "paperId": "1214e0a40fb963749497338ee1959c4c7ef4795c",
            "title": "Der Einfluss der Datenverteilung auf die Performanz eines Data Warehouse",
            "abstract": "Dieses Papier befasst sich mit einer Studie uber die Optimierungsmoglichkeiten von Anfragen auf verteilten Data Warehouse Architekturen mittels verschiedenartiger Verteilungsstrategien der beteiligten Tabellen am Beispiel SAP NetWeaver BI.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2066137775",
                    "name": "T. Legler"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2068943216",
                    "name": "Andrew Ross"
                }
            ]
        },
        {
            "paperId": "2635302ddfaf132eb58274c904f8164d5c7410f6",
            "title": "Representing Data Quality for Streaming and Static Data",
            "abstract": "In smart item environments, multitude of sensors are applied to capture data about product conditions and usage to guide business decisions as well as production automation processes. A big issue in this application area is posed by the restricted quality of sensor data due to limited sensor precision as well as sensor failures and malfunctions. Decisions derived on incorrect or misleading sensor data are likely to be faulty. The issue of how to efficiently provide applications with information about data quality (DQ) is still an open research problem. In this paper, we present a flexible model for the efficient transfer and management of data quality for streaming as well as static data. We propose a data stream metamodel to allow for the propagation of data quality from the sensors up to the respective business application without a significant overhead of data. Furthermore, we present the extension of the traditional RDBMS metamodel to permit the persistent storage of data quality information in a relational database. Finally, we demonstrate a data quality metadata mapping to close the gap between the streaming environment and the target database. Our solution maintains a flexible number of DQ dimensions and supports applications directly consuming streaming data or processing data filed in a persistent database.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064804569",
                    "name": "Anja Klein"
                },
                {
                    "authorId": "36851470",
                    "name": "H. Do"
                },
                {
                    "authorId": "3068811",
                    "name": "Gregor Hackenbroich"
                },
                {
                    "authorId": "1734216",
                    "name": "Marcel Karnstedt"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "2dc0cd878ca031325cb5840f17f04c75792eb5cb",
            "title": "Maintaining bernoulli samples over evolving multisets",
            "abstract": "Random sampling has become a crucial component of modern data management systems. Although the literature on database sampling is large, there has been relatively little work on the problem of maintaining a sample in the presence of arbitrary insertions and deletions to the underlying dataset. Most existing maintenance techniques apply either to the insert-only case or to datasets that do not contain duplicates. In this paper, we provide a scheme that maintains a Bernoulli sample of an underlying multiset in the presence of an arbitrary stream of updates, deletions, and insertions. Importantly, the scheme never needs to access the underlying multiset. Such Bernoulli samples are easy to manipulate, and are well suited to parallel processing environments. Our method can be viewed as an enhancement of the \"counting sample\" scheme developed by Gibbons and Matias for estimating the frequency of highly frequent items. We show how the \"tracking counters\" used by our maintenance scheme can be exploited to estimate population frequencies, sums, and averages in an unbiased manner, with lower variance than the usual estimators based on a Bernoulli sample. The number of distinct items in the multiset can also be estimated without bias. Finally, we discuss certain problems of subsampling and merging that a rise in systems with limited memory resources or distributed processing, respectively.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1777107",
                    "name": "Rainer Gemulla"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "37810307",
                    "name": "P. Haas"
                }
            ]
        },
        {
            "paperId": "38187c680ed38cac43ee3ce43c5696a381e333dc",
            "title": "Parameterfreies Clustering mittels Multi-Level Ansatz",
            "abstract": "A cooling jacket for an ingot mold for the continuous casting of metal comprises a tubular element and a grid of stiffening ribs projecting from the outer surface of the tubular element, in which the grid is provided at the intersection of the stiffening ribs with openings or bores which pass also through the tubular element for the passage of fastening elements in form of tie rods therethrough by which an inner tube for the passage of the metal to be cast can be fastened to the cooling jacket with radial clearance. The upright ingot mold in which the cooling jacket is used and which is especially employed for the casting of steel, includes, besides the cooling jacket and the inner tube, an outer shell surrounding the cooling jacket with considerable clearance. The outer shell is closed by plates at opposite ends to which opposite ends of the inner tube are respectively fastened and these plates are provided with openings aligned with the opposite open ends of the inner tube. The space between the tubular element of the cooling jacket and the outer shell is divided by a transverse wall in a lower chamber with which the open lower end of the tubular element of the cooling jacket communicates and a longer upper chamber with which the open upper end of the tubular element communicates. A cooling water inlet communicates with the lower chamber adjacent the separating wall and a cooling water outlet communicates with the upper compartment at the other side and adjacent to the separating wall. A polyphase magnetic inductor is located in the upper compartment around the cooling jacket.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "3108625",
                    "name": "M. Hahmann"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "54f80c7d1d06a47181b8b6f4ba45fae70e7e562e",
            "title": "Data-Grey-BoxWeb Services in Data-Centric Environments",
            "abstract": "In data-centric environments, for example, in the field of scientific computing, the transmission of large amount of structured data to Web services is required. In service-oriented environments (SOA), the Simple Object Access Protocol (SOAP) is commonly used as the main transport protocol. However, the resulting 'by value' data transmission approach is not efficiently applicable in data-centric environments. One challenging bottleneck of SOAP arises from the XML serialization and deserialization when processing large SOAP messages. In this paper, we present an extended Web service framework which explicitly considers the data aspects of functional Web services. Aside from the possibility to integrate specialized data transfer methods in SOA, this framework allows the efficient and scalable data handling and processing within Web services. In this case, we combine the advantages of the functional perspective (SOA) and the data perspective to efficiently support data-centric environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "1732902",
                    "name": "Steffen Preissler"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2625926",
                    "name": "Sebastian Richly"
                },
                {
                    "authorId": "143681675",
                    "name": "U. Assmann"
                },
                {
                    "authorId": "2867481",
                    "name": "M. Grasselt"
                },
                {
                    "authorId": "144579342",
                    "name": "A. Maier"
                }
            ]
        },
        {
            "paperId": "5c043f7e41c98569df810b02f368bb63e439bca7",
            "title": "Towards Self-Optimization of Message Transformation Processes",
            "abstract": "The Message Transformation Model (MTM), for modeling complex message transformation processes in data centric application scenarios, provides strong capabilities for describing the data and con- trol flow, transactional behavior and even the interaction with external systems. Thus, this general model could be used for dierent integration platforms like EAI-Servers, Message-Brokers and Subscription-Systems, as well as for ETL-Tools. In this paper, we describe self-optimization strategies for MTM processes to determine an optimal executable pro- cess. Our proposed strategies can be distinguished into rule-based and workload-based techniques. Aside from theoretical consideration, we de- scribe implementation aspects within the integration platform Trans- Connect R . Furthermore, we present some first evaluation results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40237241",
                    "name": "Matthias Boehm"
                },
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2776810",
                    "name": "Uwe Wloka"
                },
                {
                    "authorId": "2059398737",
                    "name": "J\u00fcrgen Bittner"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "5dee08488e3cfafc7646b1d0b2164e13b27bb87f",
            "title": "Euro-Par 2006 Workshops : parallel processing : CoreGRID 2006, UNICORE Summit 2006, Petascale Computational Biology and Bioinformatics Dresden, Germany, August 29-September 1, 2006 : revised selected papers",
            "abstract": "CoreGRID Workshop on GRID Middleware.- Architecture of a Network Monitoring Element.- Support for Automatic Diagnosis and Dynamic Configuration of Scalable Storage Systems.- Adding Dynamism to OGSA-DQP: Incorporating the DynaSOAr Framework in Distributed Query Processing.- Review of Security Models Applied to Distributed Data Access.- Security Requirements Analysis for Large-Scale Distributed File Systems.- Coupling Contracts for Deployment on Alien Grids.- A Transparent Framework for Hierarchical Master-Slave Grid Computing.- A Multi-level Scheduler for the Grid Computing YML Framework.- Virtual Environments - Framework for Virtualized Resource Access in the Grid.- Grid Meta-Broker Architecture: Towards an Interoperable Grid Resource Brokering Service.- A Super-Peer Model for Multiple Job Submission on a Grid.- A Scheduling Algorithm for High Performance Peer-to-Peer Platform.- Brokering Multi-grid Workflows in the P-GRADE Portal.- Diet: New Developments and Recent Results.- Execution Support of High Performance Heterogeneous Component-Based Applications on the Grid.- Towards a Grid Information Knowledge Base.- UNICORE Summit 2006.- A Versatile Execution Management System for Next-Generation UNICORE Grids.- Towards More Flexible and Increased Security and Privacy in Grids.- Integration of Grid Cost Model into ISS/VIOLA Meta-scheduler Environment.- A One-Stop, Fire-and-(Almost)Forget, Dropping-Off and Rendezvous Point.- Grid-Based Processing of High-Volume Meteorological Data Sets.- BLAST Application on the GPE/UnicoreGS Grid.- Job Management Enterprise Application.- UNICORE Deployment Within the DEISA Supercomputing Grid Infrastructure.- Petascale Computational Biology and Bioinformatics.- Progress in Scaling Biomolecular Simulations to Petaflop Scale Platforms.- Progress Towards Petascale Applications in Biology: Status in 2006.- Toward a Solution of the Reverse Engineering Problem Using FPGAs.- Two Challenges in Genomics That Can Benefit from Petascale Platforms.- High Throughput Image Analysis on PetaFLOPS Systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69882288",
                    "name": "CoreGRID"
                },
                {
                    "authorId": "69896852",
                    "name": "Unicore Summit"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "901f2e4bdd05cbd082657ca166dc567e72bda9e1",
            "title": "Partition-based workload scheduling in living data warehouse environments",
            "abstract": "The demand for so-called living or real-time data warehouses is increasing in many application areas such as manufacturing, event monitoring and telecommunications. In these fields users usually expect short response times for their queries and high freshness for the requested data. However, meeting these fundamental requirements is challenging due to the high loads and the continuous flow of write-only updates and read-only queries, which may be in conflict with each other. Therefore, we present the concept of Workload Balancing by Election (WINE), which allows users to express their individual demands on the Quality of Service and the Quality of Data respectively. WINE applies this information to balance and prioritize over both types of transactions -- queries and update -- according to the varying user needs. A simulation study shows that our proposed algorithm outperforms competitor baseline algorithms over the entire spectrum of workloads and user requirements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2405035",
                    "name": "Maik Thiele"
                },
                {
                    "authorId": "153705371",
                    "name": "Ulrike Fischer"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "b33c72c2ed0f9099e3c1c6b42d6d1a7d66c3c77b",
            "title": "Exploiting self-monitoring sample views for cardinality estimation",
            "abstract": "Good cardinality estimates are critical for generating good execution plans during query optimization. Complex predicates, correlations between columns, and user-defined functions are extremely hard to handle when using the traditional histogram approach. This demo illustrates the use of sample views for cardinality estimations as prototyped in Microsoft SQL Server. We show the creation of sample views, discuss how they are exploited during query optimization, and explain their potential effect on query plans. In addition, we also show our implementation of maintenance policies using statistical quality control techniques based on query feedback.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144014124",
                    "name": "P. Larson"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1709595",
                    "name": "Jingren Zhou"
                },
                {
                    "authorId": "1701270",
                    "name": "P. Zabback"
                }
            ]
        },
        {
            "paperId": "c278b575cd3ddf9c625695bd2b03d655797d1eb2",
            "title": "QDM: A Generic QoS-Aware Data Model for Real-Time Data Stream Processing",
            "abstract": "Data stream processing addresses a huge variety of application, ranging from processing complex-structured business events to high-volume RFID tag info streams. Some application areas, especially in telecommunication or manufacturing, require guarantees with regard to pre-defined service constraints. In this paper, we outline the cornerstones of a QoS-aware data model comprising functional as well as non-functional properties to describe quality constraints. We will look at the model from a structural point of view as well as from the operational perspective. Since the proposed model is specific with regard to QoS constraint and generic with regard to specific operators, it may serve as a blue-print for a broad spectrum of data stream systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49924300",
                    "name": "S. Schmidt"
                },
                {
                    "authorId": "2395322",
                    "name": "B. Schlegel"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "c6be2d9f88becfea1b8e4d348864387a332e3b5c",
            "title": "Error-Aware Density-Based Clustering of Imprecise Measurement Values",
            "abstract": "Manufacturing process development is under constant pressure to achieve a good yield for stable processes. The development of new technologies, especially in the field of photomask and semiconductor development, is at its phys- ical limits. In this area, data, e.g. sensor data, has to be collected and analyzed for each process in order to ensure process quality. With increasing complexity of manufactur- ing processes, the volume of data that has to be evaluated rises accordingly. The complexity and data volume exceeds the possibility of a manual data analysis. At this point, data mining techniques become interesting. The application of current techniques is complex because most of the data is captured with sensor measurement tools. Therefore, every measured value contains a specific error. In this paper we propose an error-aware extension of the density-based al- gorithm DBSCAN. Furthermore, we present some quality measures which could be utilized for further interpretation of the determined clustering results. With this new cluster algorithm, we can ensure that masks are classified into the correct cluster with respect to the measurement errors, thus ensuring a more likely correlation between the masks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "2060654334",
                    "name": "P. Volk"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2068588646",
                    "name": "Ralf Dittmann"
                },
                {
                    "authorId": "50424955",
                    "name": "C. Utzny"
                }
            ]
        },
        {
            "paperId": "cb4e9a7b4bd12d2998e4c498fa0c15c07c6840a0",
            "title": "Feeding roller for processing a core wire for a web of staple fibers.",
            "abstract": "Feed roller for processing a core wire for a web of staple fibers. Feed roller (11) for processing a core thread (12) for a fiber (2) cut processing in the drawbench (1) of a textile machine with a groove roller (13) for the thread soul (12). The feed roller (11) can be used especially for draw-benches (1) of textile staple fiber processing machines. The technical problem to be solved is the design of a roller (11) for feeding the core thread (12) to a drawbench (1) of textile machines so that can be manufactured with reduced costs. This problem is solved by producing first and second individual parts (18, 19) to be mounted below to form the roll (11).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "2116645600",
                    "name": "Michael Smith"
                }
            ]
        },
        {
            "paperId": "cea5f764b4fbe08670a3bdb45e06c1dbe21d7ef8",
            "title": "Cardinality estimation using sample views with quality assurance",
            "abstract": "Accurate cardinality estimation is critically important to high-quality query optimization. It is well known that conventional cardinality estimation based on histograms or similar statistics may produce extremely poor estimates in a variety of situations, for example, queries with complex predicates, correlation among columns, or predicates containing user-defined functions. In this paper, we propose a new, general cardinality estimation technique that combines random sampling and materialized view technology to produce accurate estimates even in these situations. As a major innovation, we exploit feedback information from query execution and process control techniques to assure that estimates remain statistically valid when the underlying data changes. Experimental results based on a prototype implementation in Microsoft SQL Server demonstrate the practicality of the approach and illustrate the dramatic effects improved cardinality estimates may have.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144014124",
                    "name": "P. Larson"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "1709595",
                    "name": "Jingren Zhou"
                },
                {
                    "authorId": "1701270",
                    "name": "P. Zabback"
                }
            ]
        },
        {
            "paperId": "e808e9ea24fbea5b8842b13a1902a5979f7b458a",
            "title": "Efficient exploitation of similar subexpressions for query processing",
            "abstract": "Complex queries often contain common or similar subexpressions, either within a single query or among multiple queries submitted as a batch. If so, query execution time can be improved by evaluating a common subexpression once and reusing the result in multiple places. However, current query optimizers do not recognize and exploit similar subexpressions, even within the same query.\n We present an efficient, scalable, and principled solution to this long-standing optimization problem. We introduce a light-weight and effective mechanism to detect potential sharing opportunities among expressions. Candidate covering subexpressions are constructed and optimization is resumed to determine which, if any, such subexpressions to include in the final query plan. The chosen subexpression(s) are computed only once and the results are reused to answer other parts of queries. Our solution automatically applies to optimization of query batches, nested queries, and maintenance of multiple materialized views. It is the first comprehensive solution covering all aspects of the problem: detection, construction, and cost-based optimization. Experiments on Microsoft SQL Server show significant performance improvements with minimal overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709595",
                    "name": "Jingren Zhou"
                },
                {
                    "authorId": "144014124",
                    "name": "P. Larson"
                },
                {
                    "authorId": "1751739",
                    "name": "J. Freytag"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        },
        {
            "paperId": "ebad4777938bc8415bc7bea390cce12c113658b3",
            "title": "A Message Transformation Model for Data-Centric Service Integration Processes",
            "abstract": "The horizontal integration of systems by message-based communication via middleware products is a widespread method of application integration to ensure an adequate loose coupling of participating systems and applications. For the description of such service integration processes, the use of functionally oriented process description languages, like WSBPEL, is gaining in importance. However, these languages reveal deficits when describing data-centric application scenarios. Due to these deficits and the lack of a model for service integration processes, this paper contributes to the systematic modeling of complex message transformations in data-centric integration processes. The practicability of the model is shown with a prototypical implementation within the service integration platform TransConnect R of the SQL GmbH Dresden.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694689",
                    "name": "Dirk Habich"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                }
            ]
        }
    ]
}