{
    "authorId": "1759772",
    "papers": [
        {
            "paperId": "0cb2ca7ab5145c078ecae88ce0dab28acb559767",
            "title": "Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2291065818",
                    "name": "Juan Manuel Zambrano Chaves"
                },
                {
                    "authorId": "2267360148",
                    "name": "Shih-Cheng Huang"
                },
                {
                    "authorId": "2279740020",
                    "name": "Yanbo Xu"
                },
                {
                    "authorId": "2263936549",
                    "name": "Hanwen Xu"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "2267010012",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2267154244",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2291048834",
                    "name": "Yujia Xie"
                },
                {
                    "authorId": "2268760479",
                    "name": "Mahmoud Khademi"
                },
                {
                    "authorId": "2291073936",
                    "name": "Ziyi Yang"
                },
                {
                    "authorId": "3032929",
                    "name": "H. Awadalla"
                },
                {
                    "authorId": "2268383237",
                    "name": "Julia Gong"
                },
                {
                    "authorId": "2291084567",
                    "name": "Houdong Hu"
                },
                {
                    "authorId": "2279705714",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "2109738542",
                    "name": "Chun-yue Li"
                },
                {
                    "authorId": "2256227183",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "2260374573",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "2264107059",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                },
                {
                    "authorId": "2188270295",
                    "name": "M. Lungren"
                },
                {
                    "authorId": "2275638250",
                    "name": "Serena Yeung-Levy"
                },
                {
                    "authorId": "2249123829",
                    "name": "Curtis P. Langlotz"
                },
                {
                    "authorId": "2302416483",
                    "name": "Sheng Wang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "0f4be00866b3de3fdf04afe5e8b62f6fc98182bb",
            "title": "BiomedParse: a biomedical foundation model for image parsing of everything everywhere all at once",
            "abstract": "Biomedical image analysis is fundamental for biomedical discovery in cell biology, pathology, radiology, and many other biomedical domains. Holistic image analysis comprises interdependent subtasks such as segmentation, detection, and recognition of relevant objects. Here, we propose BiomedParse, a biomedical foundation model for imaging parsing that can jointly conduct segmentation, detection, and recognition for 82 object types across 9 imaging modalities. Through joint learning, we can improve accuracy for individual tasks and enable novel applications such as segmenting all relevant objects in an image through a text prompt, rather than requiring users to laboriously specify the bounding box for each object. We leveraged readily available natural-language labels or descriptions accompanying those datasets and use GPT-4 to harmonize the noisy, unstructured text information with established biomedical object ontologies. We created a large dataset comprising over six million triples of image, segmentation mask, and textual description. On image segmentation, we showed that BiomedParse is broadly applicable, outperforming state-of-the-art methods on 102,855 test image-mask-label triples across 9 imaging modalities (everything). On object detection, which aims to locate a specific object of interest, BiomedParse again attained state-of-the-art performance, especially on objects with irregular shapes (everywhere). On object recognition, which aims to identify all objects in a given image along with their semantic types, we showed that BiomedParse can simultaneously segment and label all biomedical objects in an image (all at once). In summary, BiomedParse is an all-in-one tool for biomedical image analysis by jointly solving segmentation, detection, and recognition for all major biomedical image modalities, paving the path for efficient and accurate image-based biomedical discovery.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220964895",
                    "name": "Theodore Zhao"
                },
                {
                    "authorId": "2260374573",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "2279705714",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "2268317049",
                    "name": "Ho Hin Lee"
                },
                {
                    "authorId": "2264107059",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "2265287355",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "2264916917",
                    "name": "Angela Crabtree"
                },
                {
                    "authorId": "2264514939",
                    "name": "B. Piening"
                },
                {
                    "authorId": "2298702328",
                    "name": "Carlo Bifulco"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "2302416483",
                    "name": "Sheng Wang"
                }
            ]
        },
        {
            "paperId": "388309f1e772001a28ec95d19816dfdc6a3a63b8",
            "title": "From Introspection to Best Practices: Principled Analysis of Demonstrations in Multimodal In-Context Learning",
            "abstract": "Motivated by in-context learning (ICL) capabilities of Large Language models (LLMs), multimodal LLMs with additional visual modality are also exhibited with similar ICL abilities when multiple image-text pairs are provided as demonstrations. However, relatively less work has been done to investigate the principles behind how and why multimodal ICL works. We conduct a systematic and principled evaluation of multimodal ICL for models of different scales on a broad spectrum of new yet critical tasks. Through perturbations over different modality information, we show that modalities matter differently across tasks in multimodal ICL. Considering such modality impact, we further utilize modality-driven demonstration strategies to boost ICL performance. We also identify that demonstration selection is closely related to the models' ability to capture task inductive biases from multimodal ICL. Our principled analysis provides a comprehensive way of understanding the role of demonstrations in multimodal in-context learning, and sheds light on effectively improving multimodal ICL on a wide range of tasks even if those tasks are not seen in or even contradict pretraining data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257007579",
                    "name": "Nan Xu"
                },
                {
                    "authorId": "2267154244",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2267010012",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                }
            ]
        },
        {
            "paperId": "5982c074413f0a73b4b72db6f01cd6b9b909a6d0",
            "title": "Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries",
            "abstract": "Summarizing clinical text is crucial in health decision-support and clinical research. Large language models (LLMs) have shown the potential to generate accurate clinical text summaries, but still struggle with issues regarding grounding and evaluation, especially in safety-critical domains such as health. Holistically evaluating text summaries is challenging because they may contain unsubstantiated information. Here, we explore a general mitigation framework using Attribute Structuring (AS), which structures the summary evaluation process. It decomposes the evaluation process into a grounded procedure that uses an LLM for relatively simple structuring and scoring tasks, rather than the full task of holistic summary evaluation. Experiments show that AS consistently improves the correspondence between human annotations and automated metrics in clinical text summarization. Additionally, AS yields interpretations in the form of a short text span corresponding to each output, which enables efficient human auditing, paving the way towards trustworthy evaluation of clinical information in resource-constrained scenarios. We release our code, prompts, and an open-source benchmark at https://github.com/microsoft/attribute-structuring.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1395101702",
                    "name": "Zelalem Gero"
                },
                {
                    "authorId": "2261286637",
                    "name": "Chandan Singh"
                },
                {
                    "authorId": "2267021210",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2264107059",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "2256227183",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "72010b7fe48301a38e8063109b8ef8fcfd573e05",
            "title": "Offset Unlearning for Large Language Models",
            "abstract": "Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, harmful, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose $\\delta$-unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, $\\delta$-unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that $\\delta$-unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. $\\delta$-unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110302673",
                    "name": "James Y. Huang"
                },
                {
                    "authorId": "2203076",
                    "name": "Wenxuan Zhou"
                },
                {
                    "authorId": "2267154244",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2258550405",
                    "name": "Fred Morstatter"
                },
                {
                    "authorId": "2267010012",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                }
            ]
        },
        {
            "paperId": "ab88cdefc888fb102b91140bd6e6f8eafef3d135",
            "title": "Towards a clinically accessible radiology foundation model: open-access and lightweight, with automated evaluation",
            "abstract": "The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world clinics. Frontier general-domain models such as GPT-4V still have significant performance gaps in multimodal biomedical applications. More importantly, less-acknowledged pragmatic issues, including accessibility, model cost, and tedious manual evaluation make it hard for clinicians to use state-of-the-art large models directly on private patient data. Here, we explore training open-source small multimodal models (SMMs) to bridge competency gaps for unmet clinical needs in radiology. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on training a lightweight adapter to ground each modality to the text embedding space, as exemplified by LLaVA-Med. For training, we assemble a large dataset of over 697 thousand radiology image-text pairs. For evaluation, we propose CheXprompt, a GPT-4-based metric for factuality evaluation, and demonstrate its parity with expert evaluation. For best practice, we conduct a systematic ablation study on various choices in data engineering and multimodal training. The resulting LlaVA-Rad (7B) model attains state-of-the-art results on standard radiology tasks such as report generation and cross-modal retrieval, even outperforming much larger models such as GPT-4V and Med-PaLM M (84B). The inference of LlaVA-Rad is fast and can be performed on a single V100 GPU in private settings, offering a promising state-of-the-art tool for real-world clinical applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2291065818",
                    "name": "Juan Manuel Zambrano Chaves"
                },
                {
                    "authorId": "2267360148",
                    "name": "Shih-Cheng Huang"
                },
                {
                    "authorId": "2279740020",
                    "name": "Yanbo Xu"
                },
                {
                    "authorId": "2263936549",
                    "name": "Hanwen Xu"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "2267154244",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2291048834",
                    "name": "Yujia Xie"
                },
                {
                    "authorId": "2268760479",
                    "name": "Mahmoud Khademi"
                },
                {
                    "authorId": "2291073936",
                    "name": "Ziyi Yang"
                },
                {
                    "authorId": "3032929",
                    "name": "H. Awadalla"
                },
                {
                    "authorId": "2268383237",
                    "name": "Julia Gong"
                },
                {
                    "authorId": "2291084567",
                    "name": "Houdong Hu"
                },
                {
                    "authorId": "2279705714",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "2244470181",
                    "name": "Chunyuan Li"
                },
                {
                    "authorId": "2256227183",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "2260374573",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "2264107059",
                    "name": "Tristan Naumann"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                },
                {
                    "authorId": "2188270295",
                    "name": "M. Lungren"
                },
                {
                    "authorId": "2275638250",
                    "name": "Serena Yeung-Levy"
                },
                {
                    "authorId": "2249123829",
                    "name": "Curtis P. Langlotz"
                },
                {
                    "authorId": "2261294491",
                    "name": "Sheng Wang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "d8ff39b155f71a0391b5e13c7eb4a16d3e31e55f",
            "title": "Foundation Models for Biomedical Image Segmentation: A Survey",
            "abstract": "Recent advancements in biomedical image analysis have been significantly driven by the Segment Anything Model (SAM). This transformative technology, originally developed for general-purpose computer vision, has found rapid application in medical image processing. Within the last year, marked by over 100 publications, SAM has demonstrated its prowess in zero-shot learning adaptations for medical imaging. The fundamental premise of SAM lies in its capability to segment or identify objects in images without prior knowledge of the object type or imaging modality. This approach aligns well with tasks achievable by the human visual system, though its application in non-biological vision contexts remains more theoretically challenging. A notable feature of SAM is its ability to adjust segmentation according to a specified resolution scale or area of interest, akin to semantic priming. This adaptability has spurred a wave of creativity and innovation in applying SAM to medical imaging. Our review focuses on the period from April 1, 2023, to September 30, 2023, a critical first six months post-initial publication. We examine the adaptations and integrations of SAM necessary to address longstanding clinical challenges, particularly in the context of 33 open datasets covered in our analysis. While SAM approaches or achieves state-of-the-art performance in numerous applications, it falls short in certain areas, such as segmentation of the carotid artery, adrenal glands, optic nerve, and mandible bone. Our survey delves into the innovative techniques where SAM's foundational approach excels and explores the core concepts in translating and applying these models effectively in diverse medical imaging scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268317049",
                    "name": "Ho Hin Lee"
                },
                {
                    "authorId": "2260374573",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "2220964895",
                    "name": "Theodore Zhao"
                },
                {
                    "authorId": "2279740020",
                    "name": "Yanbo Xu"
                },
                {
                    "authorId": "2279705714",
                    "name": "Jianwei Yang"
                },
                {
                    "authorId": "2637252",
                    "name": "Naoto Usuyama"
                },
                {
                    "authorId": "2109566188",
                    "name": "Cliff Wong"
                },
                {
                    "authorId": "2072847758",
                    "name": "Mu-Hsin Wei"
                },
                {
                    "authorId": "2239208043",
                    "name": "Bennett A. Landman"
                },
                {
                    "authorId": "34430081",
                    "name": "Yuankai Huo"
                },
                {
                    "authorId": "2264626868",
                    "name": "Alberto Santamar\u00eda-Pang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                }
            ]
        },
        {
            "paperId": "fd655992d1b0220e16004ca39774e9390fb28cee",
            "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language Models",
            "abstract": "Direct preference optimization (DPO) has shown to be an effective method for large language model (LLM) alignment. Recent works have attempted to apply DPO to multimodal scenarios but have found it challenging to achieve consistent improvement. Through a comparative experiment, we identify the unconditional preference problem in multimodal preference optimization, where the model overlooks the image condition. To address this problem, we propose mDPO, a multimodal DPO objective that prevents the over-prioritization of language-only preferences by also optimizing image preference. Moreover, we introduce a reward anchor that forces the reward to be positive for chosen responses, thereby avoiding the decrease in their likelihood -- an intrinsic problem of relative preference optimization. Experiments on two multimodal LLMs of different sizes and three widely used benchmarks demonstrate that mDPO effectively addresses the unconditional preference problem in multimodal preference optimization and significantly improves model performance, particularly in reducing hallucination.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267154244",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2203076",
                    "name": "Wenxuan Zhou"
                },
                {
                    "authorId": "2110302673",
                    "name": "James Y. Huang"
                },
                {
                    "authorId": "2257007579",
                    "name": "Nan Xu"
                },
                {
                    "authorId": "2267010012",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                }
            ]
        },
        {
            "paperId": "fdb3864b5c8914adedfce0245648604191e6e38e",
            "title": "MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding",
            "abstract": "We introduce MuirBench, a comprehensive benchmark that focuses on robust multi-image understanding capabilities of multimodal LLMs. MuirBench consists of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that involve 10 categories of multi-image relations (e.g., multiview, temporal relations). Comprising 11,264 images and 2,600 multiple-choice questions, MuirBench is created in a pairwise manner, where each standard instance is paired with an unanswerable variant that has minimal semantic differences, in order for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our results reveal that even the best-performing models like GPT-4o and Gemini Pro find it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy. Open-source multimodal LLMs trained on single images can hardly generalize to multi-image questions, hovering below 33.3% in accuracy. These results highlight the importance of MuirBench in encouraging the community to develop multimodal LLMs that can look beyond a single image, suggesting potential pathways for future improvements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267154244",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2078360",
                    "name": "Xingyu Fu"
                },
                {
                    "authorId": "2110302673",
                    "name": "James Y. Huang"
                },
                {
                    "authorId": "2306133127",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2267021590",
                    "name": "Qin Liu"
                },
                {
                    "authorId": "2257096300",
                    "name": "Xiaogeng Liu"
                },
                {
                    "authorId": "144592155",
                    "name": "Mingyu Derek Ma"
                },
                {
                    "authorId": "2257007579",
                    "name": "Nan Xu"
                },
                {
                    "authorId": "2203076",
                    "name": "Wenxuan Zhou"
                },
                {
                    "authorId": "145086492",
                    "name": "Kai Zhang"
                },
                {
                    "authorId": "2284682152",
                    "name": "Tianyi Yan"
                },
                {
                    "authorId": "2266978162",
                    "name": "W. Mo"
                },
                {
                    "authorId": "2306480671",
                    "name": "Hsiang-Hui Liu"
                },
                {
                    "authorId": "2887562",
                    "name": "Pan Lu"
                },
                {
                    "authorId": "2287283699",
                    "name": "Chunyuan Li"
                },
                {
                    "authorId": "2256992327",
                    "name": "Chaowei Xiao"
                },
                {
                    "authorId": "2278984743",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2297187000",
                    "name": "Dan Roth"
                },
                {
                    "authorId": "2267010012",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "1998918",
                    "name": "Muhao Chen"
                }
            ]
        },
        {
            "paperId": "fe8527541afd578326a51b885aef82da6bb32e95",
            "title": "T-Rex: Text-assisted Retrosynthesis Prediction",
            "abstract": "As a fundamental task in computational chemistry, retrosynthesis prediction aims to identify a set of reactants to synthesize a target molecule. Existing template-free approaches only consider the graph structures of the target molecule, which often cannot generalize well to rare reaction types and large molecules. Here, we propose T-Rex, a text-assisted retrosynthesis prediction approach that exploits pre-trained text language models, such as ChatGPT, to assist the generation of reactants. T-Rex first exploits ChatGPT to generate a description for the target molecule and rank candidate reaction centers based both the description and the molecular graph. It then re-ranks these candidates by querying the descriptions for each reactants and examines which group of reactants can best synthesize the target molecule. We observed that T-Rex substantially outperformed graph-based state-of-the-art approaches on two datasets, indicating the effectiveness of considering text information. We further found that T-Rex outperformed the variant that only use ChatGPT-based description without the re-ranking step, demonstrate how our framework outperformed a straightforward integration of ChatGPT and graph information. Collectively, we show that text generated by pre-trained language models can substantially improve retrosynthesis prediction, opening up new avenues for exploiting ChatGPT to advance computational chemistry. And the codes can be found at https://github.com/lauyikfung/T-Rex.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2264342410",
                    "name": "Yifeng Liu"
                },
                {
                    "authorId": "2263936549",
                    "name": "Hanwen Xu"
                },
                {
                    "authorId": "2264489734",
                    "name": "Tangqi Fang"
                },
                {
                    "authorId": "2281644064",
                    "name": "Haocheng Xi"
                },
                {
                    "authorId": "2265222599",
                    "name": "Zixuan Liu"
                },
                {
                    "authorId": "72655349",
                    "name": "Sheng Zhang"
                },
                {
                    "authorId": "1759772",
                    "name": "Hoifung Poon"
                },
                {
                    "authorId": "2257325568",
                    "name": "Sheng Wang"
                }
            ]
        }
    ]
}