{
    "authorId": "2170344171",
    "papers": [
        {
            "paperId": "2235f0df7efa6571007c33c3a5f3ea4286be1b9a",
            "title": "Local-Global Defense against Unsupervised Adversarial Attacks on Graphs",
            "abstract": "Unsupervised pre-training algorithms for graph representation learning are vulnerable to adversarial attacks, such as first-order perturbations on graphs, which will have an impact on particular downstream applications. Designing an effective representation learning strategy against white-box attacks remains a crucial open topic. Prior research attempts to improve representation robustness by maximizing mutual information between the representation and the perturbed graph, which is sub-optimal because it does not adapt its defense techniques to the severity of the attack. To address this issue, we propose an unsupervised defense method that combines local and global defense to improve the robustness of representation. Note that we put forward the Perturbed Edges Harmfulness (PEH) metric to determine the riskiness of the attack. Thus, when the edges are attacked, the model can automatically identify the risk of attack. We present a method of attention-based protection against high-risk attacks that penalizes attention coefficients of perturbed edges to encoders. Extensive experiments demonstrate that our strategies can enhance the robustness of representation against various adversarial attacks on three benchmark graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152284414",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2222464835",
                    "name": "Bingdao Feng"
                },
                {
                    "authorId": "2148931772",
                    "name": "Siqi Guo"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2170344171",
                    "name": "Jianguo Wei"
                },
                {
                    "authorId": "2118453036",
                    "name": "Zhen Wang"
                }
            ]
        },
        {
            "paperId": "69b6b6f82eeaca06eddb632c92db09ba0d5c16b9",
            "title": "Generating Visual Spatial Description via Holistic 3D Scene Understanding",
            "abstract": "Visual spatial description (VSD) aims to generate texts that describe the spatial relations of the given objects within images. Existing VSD work merely models the 2D geometrical vision features, thus inevitably falling prey to the problem of skewed spatial understanding of target objects. In this work, we investigate the incorporation of 3D scene features for VSD. With an external 3D scene extractor, we obtain the 3D objects and scene features for input images, based on which we construct a target object-centered 3D spatial scene graph (Go3D-S2G), such that we model the spatial semantics of target objects within the holistic 3D scenes. Besides, we propose a scene subgraph selecting mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the diverse local structure features are navigated to yield spatially-diversified text generation. Experimental results on two VSD datasets demonstrate that our framework outperforms the baselines significantly, especially improving on the cases with complex visual spatial relations. Meanwhile, our method can produce more spatially-diversified generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110263863",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "46959445",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2072613978",
                    "name": "Wei Ji"
                },
                {
                    "authorId": "2170344171",
                    "name": "Jianguo Wei"
                },
                {
                    "authorId": "2117849151",
                    "name": "Meishan Zhang"
                },
                {
                    "authorId": "39767557",
                    "name": "M. Zhang"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        },
        {
            "paperId": "95099a4c1cbe16db4ac7e9f8095bedfc33cf05de",
            "title": "Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic Role Labeling",
            "abstract": "As one of the core video semantic understanding tasks, Video Semantic Role Labeling (VidSRL) aims to detect the salient events from given videos, by recognizing the predict-argument event structures and the interrelationships between events. While recent endeavors have put forth methods for VidSRL, they can be mostly subject to two key drawbacks, including the lack of fine-grained spatial scene perception and the insufficiently modeling of video temporality. Towards this end, this work explores a novel holistic spatio-temporal scene graph (namely HostSG) representation based on the existing dynamic scene graph structures, which well model both the fine-grained spatial semantics and temporal dynamics of videos for VidSRL. Built upon the HostSG, we present a nichetargeting VidSRL framework. A scene-event mapping mechanism is first designed to bridge the gap between the underlying scene structure and the high-level event semantic structure, resulting in an overall hierarchical scene-event (termed ICE) graph structure. We further perform iterative structure refinement to optimize the ICE graph, e.g., filtering noisy branches and newly building informative connections, such that the overall structure representation can best coincide with end task demand. Finally, three subtask predictions of VidSRL are jointly decoded, where the end-to-end paradigm effectively avoids error propagation. On the benchmark dataset, our framework boosts significantly over the current best-performing model. Further analyses are shown for a better understanding of the advances of our methods. Our HostSG representation shows greater potential to facilitate a broader range of other video understanding tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110263863",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "46959445",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "145014675",
                    "name": "Yixin Cao"
                },
                {
                    "authorId": "2132446579",
                    "name": "Bobo Li"
                },
                {
                    "authorId": "2117849151",
                    "name": "Meishan Zhang"
                },
                {
                    "authorId": "2170344171",
                    "name": "Jianguo Wei"
                },
                {
                    "authorId": "39767557",
                    "name": "M. Zhang"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        },
        {
            "paperId": "00b096e3515602adaa3ccd3a87445f2f89838ae7",
            "title": "TeKo: Text-Rich Graph Neural Networks With External Knowledge",
            "abstract": "Graph neural networks (GNNs) have gained great prevalence in tackling various analytical tasks on graph-structured data (i.e., networks). Typical GNNs and their variants adopt a message-passing principle that obtains network representations by the attribute propagates along network topology, which however ignores the rich textual semantics (e.g., local word-sequence) that exist in numerous real-world networks. Existing methods for text-rich networks integrate textual semantics by mainly using internal information such as topics or phrases/words, which often suffer from an inability to comprehensively mine the textual semantics, limiting the reciprocal guidance between network structure and textual semantics. To address these problems, we present a novel text-rich GNN with external knowledge (TeKo), in order to make full use of both structural and textual information within text-rich networks. Specifically, we first present a flexible heterogeneous semantic network that integrates high-quality entities as well as interactions among documents and entities. We then introduce two types of external knowledge, that is, structured triplets and unstructured entity descriptions, to gain a deeper insight into textual semantics. Furthermore, we devise a reciprocal convolutional mechanism for the constructed heterogeneous semantic network, enabling network structure and textual semantics to collaboratively enhance each other and learn high-level network representations. Extensive experiments illustrate that TeKo achieves state-of-the-art performance on a variety of text-rich networks as well as a large-scale e-commerce searching dataset.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "12073135",
                    "name": "Zhizhi Yu"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2170344171",
                    "name": "Jianguo Wei"
                },
                {
                    "authorId": "39789747",
                    "name": "Ziyang Liu"
                },
                {
                    "authorId": "2053234459",
                    "name": "Yue Shang"
                },
                {
                    "authorId": "2122427161",
                    "name": "Yun Xiao"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "3008832",
                    "name": "Lingfei Wu"
                }
            ]
        },
        {
            "paperId": "0b574244f2ecea75a536106789f08d3c3c2590e0",
            "title": "Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation",
            "abstract": "Image-to-text tasks such as open-ended image captioning and controllable image description have received extensive attention for decades. Here we advance this line of work further, presenting Visual Spatial Description (VSD), a new perspective for image-to-text toward spatial semantics. Given an image and two objects inside it, VSD aims to produce one description focusing on the spatial perspective between the two objects. Accordingly, we annotate a dataset manually to facilitate the investigation of the newly-introduced task, and then build several benchmark encoder-decoder models by using VL-BART and VL-T5 as backbones. In addition, we investigate visual spatial relationship classification (VSRC) information into our model by pipeline and end-to-end architectures. Finally, we conduct experiments on our benchmark dataset to evaluate all our models. Results show that our models are awe-inspiring, offering accurate and human-like spatial-oriented text descriptions. Besides, VSRC has great potential for VSD, and the joint end-to-end architecture is the better choice for their integration. We will make the dataset and codes publicly available for research purposes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110263863",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2170344171",
                    "name": "Jianguo Wei"
                },
                {
                    "authorId": "47778813",
                    "name": "Zhichao Lin"
                },
                {
                    "authorId": "1789944",
                    "name": "Yueheng Sun"
                },
                {
                    "authorId": "2117849151",
                    "name": "Meishan Zhang"
                },
                {
                    "authorId": "39767557",
                    "name": "M. Zhang"
                }
            ]
        }
    ]
}