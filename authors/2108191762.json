{
    "authorId": "2108191762",
    "papers": [
        {
            "paperId": "5eee8166ec7f1640cb705c97b0119ab53bcfb6fb",
            "title": "On the Sweet Spot of Contrastive Views for Knowledge-enhanced Recommendation",
            "abstract": "In recommender systems, knowledge graph (KG) can offer critical information that is lacking in the original user-item interaction graph (IG). Recent process has explored this direction and shows that contrastive learning is a promising way to integrate both. However, we observe that existing KG-enhanced recommenders struggle in balancing between the two contrastive views of IG and KG, making them sometimes even less effective than simply applying contrastive learning on IG without using KG. In this paper, we propose a new contrastive learning framework for KG-enhanced recommendation. Specifically, to make full use of the knowledge, we construct two separate contrastive views for KG and IG, and maximize their mutual information; to ease the contrastive learning on the two views, we further fuse KG information into IG in a one-direction manner.Extensive experimental results on three real-world datasets demonstrate the effectiveness and efficiency of our method, compared to the state-of-the-art. Our code is available through the anonymous link:https://figshare.com/articles/conference_contribution/SimKGCL/22783382",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7275375",
                    "name": "Haibo Ye"
                },
                {
                    "authorId": "2108191762",
                    "name": "Xinjie Li"
                },
                {
                    "authorId": "46461580",
                    "name": "Yuan Yao"
                },
                {
                    "authorId": "2058143613",
                    "name": "H. Tong"
                }
            ]
        },
        {
            "paperId": "a96fdfacff96f3a4e944849f856752bc374f7bf1",
            "title": "MEID: Mixture-of-Experts with Internal Distillation for Long-Tailed Video Recognition",
            "abstract": "The long-tailed video recognition problem is especially challenging, as videos tend to be long and untrimmed, and each video may contain multiple classes, causing frame-level class imbalance. The previous method tackles the long-tailed video recognition only through frame-level sampling for class re-balance without distinguishing the frame-level feature representation between head and tail classes. To improve the frame-level feature representation of tail classes, we modulate the frame-level features with an auxiliary distillation loss to reduce the distribution distance between head and tail classes. Moreover, we design a mixture-of-experts framework with two different expert designs, i.e., the first expert with an attention-based classification network handling the original long-tailed distribution, and the second expert dealing with the re-balanced distribution from class-balanced sampling. Notably, in the second expert, we specifically focus on the frames unsolved by the first expert through designing a complementary frame selection module, which inherits the attention weights from the first expert and selects frames with low attention weights, and we also enhance the motion feature representation for these selected frames. To highlight the multi-label challenge in long-tailed video recognition, we create two additional benchmarks based on Charades and CharadesEgo videos with the multi-label property, called CharadesLT and CharadesEgoLT. Extensive experiments are conducted on the existing long-tailed video benchmark VideoLT and the two new benchmarks to verify the effectiveness of our proposed method with state-of-the-art performance. The code and proposed benchmarks are released at https://github.com/VisionLanguageLab/MEID.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108191762",
                    "name": "Xinjie Li"
                },
                {
                    "authorId": "2149190393",
                    "name": "Huijuan Xu"
                }
            ]
        },
        {
            "paperId": "dca7239aceb040b9ed2095fb63d9161d8450c014",
            "title": "Balanced Mixup Loss for Long-Tailed Visual Recognition",
            "abstract": "In the real world, the data collected naturally are often long-tailed, which inevitably leads to class-imbalanced prediction and performance degradation. As a simple and effective data augmentation method, Mixup has been proven to be beneficial for the tail class in recent long-tail learning studies. However, samples selected by Mixup are still imbalanced, which could exacerbate the imbalance problem. Existing work always considers adjusting the input distribution to alleviate this problem, while this may lead to over-fitting in the tail class. In this paper, we detail the theoretical analysis of the data imbalance caused by Mixup, and propose a novel Balanced Mixup (BaMix) loss function from the output perspective. By adding a balance term, we theoretically prove the BaMix loss can overcome the imbalance caused by Mixup. In the experiments, our solution achieves the state-of-the-art performance on CIFAR-LT, ImageNet-LT, and iNaturalist 2018.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7275375",
                    "name": "Haibo Ye"
                },
                {
                    "authorId": null,
                    "name": "Fangyu Zhou"
                },
                {
                    "authorId": "2108191762",
                    "name": "Xinjie Li"
                },
                {
                    "authorId": "9482533",
                    "name": "Qingheng Zhang"
                }
            ]
        },
        {
            "paperId": "01beb517addc5194706dc8d383f005e4f77e5be7",
            "title": "Towards Robust Neural Graph Collaborative Filtering via Structure Denoising and Embedding Perturbation",
            "abstract": "Neural graph collaborative filtering has received great recent attention due to its power of encoding the high-order neighborhood via the backbone graph neural networks. However, their robustness against noisy user-item interactions remains largely unexplored. Existing work on robust collaborative filtering mainly improves the robustness by denoising the graph structure, while recent progress in other fields has shown that directly adding adversarial perturbations in the embedding space can significantly improve the model robustness. In this work, we propose to improve the robustness of neural graph collaborative filtering via both denoising in the structure space and perturbing in the embedding space. Specifically, in the structure space, we measure the reliability of interactions and further use it to affect the message propagation process of the backbone graph neural networks; in the embedding space, we add in-distribution perturbations by mimicking the behavior of adversarial attacks and further combine it with contrastive learning to improve the performance. Extensive experiments have been conducted on four benchmark datasets to evaluate the effectiveness and efficiency of the proposed approach. The results demonstrate that the proposed approach outperforms the recent neural graph collaborative filtering methods especially when there are injected noisy interactions in the training data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7275375",
                    "name": "Haibo Ye"
                },
                {
                    "authorId": "2108191762",
                    "name": "Xinjie Li"
                },
                {
                    "authorId": "46461580",
                    "name": "Yuan Yao"
                },
                {
                    "authorId": "8163721",
                    "name": "Hanghang Tong"
                }
            ]
        },
        {
            "paperId": "398e4178ddd48de41fd1f9928f7762ccbfe17d1d",
            "title": "DANet: Dynamic Attention to Spoof Patterns for Face Anti-Spoofing",
            "abstract": "Face anti-spoofing is a vital part to protect the security of face recognition systems. Many existing face anti-spoofing methods rely on convolutional neural networks (CNNs) and achieve competitive performance. However, due to the power of CNNs, these methods will extract information that is irrelevant to spoof patterns, such as acquisition equipment and environmental characteristics, which makes the network vulnerable to changes of the illumination or camera. In this work, we propose a plug-and-play module called DyAttention, which can improve the robustness against environmental changes. Moreover, we build a network named DANet with DyAttention, which can accurately capture the spoof patterns from coarse to fine. DANet can dynamically capture the texture differences between live and spoof samples in the facial area. Specifically, we use the spatial attention mechanism to generate a mask of the facial area. Then, we extract the intrinsic texture patterns and piecewise enhance them via dynamic activation for clean representation, where the texture patterns are not affected by the environmental and domain factors. Through experiments on three benchmark datasets, our DANet achieves state-of-the-art intra-dataset accuracy on CASIA-MFSD, Replay-Attack, and OULU-NPU. Meanwhile, DANet can enhance the cross-dataset performance between CASIA-MFSD and Replay-Attack, improving the average HTER by 1.3%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118330577",
                    "name": "Chun-Yu Sun"
                },
                {
                    "authorId": "96599575",
                    "name": "Songlu Chen"
                },
                {
                    "authorId": "2108191762",
                    "name": "Xinjie Li"
                },
                {
                    "authorId": "1399870469",
                    "name": "Feng Chen"
                },
                {
                    "authorId": "1682664",
                    "name": "Xu-Cheng Yin"
                }
            ]
        },
        {
            "paperId": "9edc32aeb60b2c38450bc1eb29da829d1bc5f72c",
            "title": "SD-GAN: Semantic Decomposition for Face Image Synthesis with Discrete Attribute",
            "abstract": "Manipulating latent code in generative adversarial networks (GANs) for facial image synthesis mainly focuses on continuous attribute synthesis (e.g., age, pose and emotion), while discrete attribute synthesis (like face mask and eyeglasses) receives less attention. Directly applying existing works to facial discrete attributes may cause inaccurate results. In this work, we propose an innovative framework to tackle challenging facial discrete attribute synthesis via semantic decomposing, dubbed SD-GAN. To be concrete, we explicitly decompose the discrete attribute representation into two components, i.e. the semantic prior basis and offset latent representation. The semantic prior basis shows an initializing direction for manipulating face representation in the latent space. The offset latent presentation obtained by 3D-aware semantic fusion network is proposed to adjust prior basis. In addition, the fusion network integrates 3D embedding for better identity preservation and discrete attribute synthesis. The combination of prior basis and offset latent representation enable our method to synthesize photo-realistic face images with discrete attributes. Notably, we construct a large and valuable dataset MEGN (Face Mask and Eyeglasses images crawled from Google and Naver) for completing the lack of discrete attributes in the existing dataset. Extensive qualitative and quantitative experiments demonstrate the state-of-the-art performance of our method. Our code is available at an anonymous website: https://github.com/MontaEllis/SD-GAN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1491232168",
                    "name": "Kangneng Zhou"
                },
                {
                    "authorId": "2159182559",
                    "name": "Xiaobin Zhu"
                },
                {
                    "authorId": "1380181436",
                    "name": "Daiheng Gao"
                },
                {
                    "authorId": "2176278333",
                    "name": "Kai Lee"
                },
                {
                    "authorId": "2108191762",
                    "name": "Xinjie Li"
                },
                {
                    "authorId": "1682664",
                    "name": "Xu-Cheng Yin"
                }
            ]
        },
        {
            "paperId": "2ef38b974753fe55057de97b3727bf8fd1fdcd2b",
            "title": "Improving Stylized Neural Machine Translation with Iterative Dual Knowledge Transfer",
            "abstract": "Stylized neural machine translation (NMT) aims to translate sentences of one style into sentences of another style, which is essential for the application of machine translation in a real-world scenario. However, a major challenge in this task is the scarcity of high-quality parallel data which is stylized paired. To address this problem, we propose an iterative dual knowledge transfer framework that utilizes informal training data of machine translation and formality style transfer data to create large-scale stylized paired data, for the training of stylized machine translation model. Specifically, we perform bidirectional knowledge transfer between translation model and text style transfer model iteratively through knowledge distillation. Then, we further propose a data-refinement module to process the noisy synthetic parallel data generated during knowledge transfer. Experiment results demonstrate the effectiveness of our method, achieving an improvement over the existing best model by 5 BLEU points on MTFC dataset. Meanwhile, extensive analyses illustrate our method can also improve the accuracy of formality style transfer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145345499",
                    "name": "Xuanxuan Wu"
                },
                {
                    "authorId": "2150168584",
                    "name": "Jian Liu"
                },
                {
                    "authorId": "2108191762",
                    "name": "Xinjie Li"
                },
                {
                    "authorId": "2310092",
                    "name": "Jinan Xu"
                },
                {
                    "authorId": "47559028",
                    "name": "Yufeng Chen"
                },
                {
                    "authorId": "2108257896",
                    "name": "Yujie Zhang"
                },
                {
                    "authorId": "2146086178",
                    "name": "Hui Huang"
                }
            ]
        },
        {
            "paperId": "dd3d72d7ee5bd045fff2f4a9d7b33176f333a645",
            "title": "A Hybrid Framework for Underwater Image Enhancement",
            "abstract": "Underwater captured images often suffer from poor visibility caused by two major degradations: scattering and absorption. In this paper, we propose a hybrid framework for underwater image enhancement, which unifies underwater white balance and variational contrast and saturation enhancement. In our framework, the improved underwater white balance (UWB) algorithm is integrated with histogram stretching, aiming to better compensate the attenuation difference along the propagation path and remove undesired color castings. In addition, a variational contrast and saturation enhancement (VCSE) model is developed based on the enhanced result obtained from UWB. The advantages of VCSE model lie in the improvements of contrast and saturation as well as the elimination of hazy appearance induced by scattering. Moreover, we design a fast Gaussian pyramid-based algorithm to speed up the solving of VCSE model. The improvements achieved by our method include the more effective in color correction, haze removal and detail clarification. Extensive qualitative and quantitative assessments demonstrate that the proposed approach obtains high quality outcomes, which outperforms several state-of-the-art methods. Application tests further verify the effectiveness and broad application prospects of our proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108191762",
                    "name": "Xinjie Li"
                },
                {
                    "authorId": "2895086",
                    "name": "Guojia Hou"
                },
                {
                    "authorId": "48327860",
                    "name": "L. Tan"
                },
                {
                    "authorId": "2143606623",
                    "name": "Wanquan Liu"
                }
            ]
        },
        {
            "paperId": "83d83a36eb6af219f832ffeba081c1d9ea468416",
            "title": "Semantic Bilinear Pooling for Fine-Grained Recognition",
            "abstract": "Naturally, fine-grained recognition, e.g., vehicle identification or bird classification, has specific hierarchical labels, where fine categories are always harder to be classified than coarse categories. However, most of the recent deep learning based methods neglect the semantic structure of fine-grained objects and do not take advantage of the traditional fine-grained recognition techniques (e.g. coarse-to-fine classification). In this paper, we propose a novel framework with a two-branch network (coarse branch and fine branch), i.e., semantic bilinear pooling, for fine-grained recognition with a hierarchical label tree. This framework can adaptively learn the semantic information from the hierarchical levels. Specifically, we design a generalized cross-entropy loss for the training of the proposed framework to fully exploit the semantic priors via considering the relevance between adjacent levels and enlarge the distance between samples of different coarse classes. Furthermore, our method leverages only the fine branch when testing so that it adds no overhead to the testing time. Experimental results show that our proposed method achieves state-of-the-art performance on four public datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108191762",
                    "name": "Xinjie Li"
                },
                {
                    "authorId": "2154925588",
                    "name": "Chun Yang"
                },
                {
                    "authorId": "96599575",
                    "name": "Songlu Chen"
                },
                {
                    "authorId": "1605974915",
                    "name": "Chao Zhu"
                },
                {
                    "authorId": "1682664",
                    "name": "Xu-Cheng Yin"
                }
            ]
        }
    ]
}