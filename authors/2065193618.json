{
    "authorId": "2065193618",
    "papers": [
        {
            "paperId": "3ec0e2c64ecbfd895d0f580ac5b8004398b4b50f",
            "title": "Federated Ensemble Learning: Increasing the Capacity of Label Private Recommendation Systems",
            "abstract": ".",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31255332",
                    "name": "Meisam Hejazinia"
                },
                {
                    "authorId": "68973739",
                    "name": "Dzmitry Huba"
                },
                {
                    "authorId": "2909360",
                    "name": "Ilias Leontiadis"
                },
                {
                    "authorId": "10995410",
                    "name": "Kiwan Maeng"
                },
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                },
                {
                    "authorId": "145557680",
                    "name": "Luca Melis"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                },
                {
                    "authorId": "3490923",
                    "name": "Milad Nasr"
                },
                {
                    "authorId": "2150030991",
                    "name": "Kaikai Wang"
                },
                {
                    "authorId": "2797270",
                    "name": "Carole-Jean Wu"
                }
            ]
        },
        {
            "paperId": "8f5eee0ab94a613cb30722e6573585e44cdd5aee",
            "title": "Green Federated Learning",
            "abstract": "The rapid progress of AI is fueled by increasingly large and computationally intensive machine learning models and datasets. As a consequence, the amount of compute used in training state-of-the-art models is exponentially increasing (doubling every 10 months between 2015 and 2022), resulting in a large carbon footprint. Federated Learning (FL) - a collaborative machine learning technique for training a centralized model using data of decentralized entities - can also be resource-intensive and have a significant carbon footprint, particularly when deployed at scale. Unlike centralized AI that can reliably tap into renewables at strategically placed data centers, cross-device FL may leverage as many as hundreds of millions of globally distributed end-user devices with diverse energy sources. Green AI is a novel and important research area where carbon footprint is regarded as an evaluation criterion for AI, alongside accuracy, convergence speed, and other metrics. In this paper, we propose the concept of Green FL, which involves optimizing FL parameters and making design choices to minimize carbon emissions consistent with competitive performance and training time. The contributions of this work are two-fold. First, we adopt a data-driven approach to quantify the carbon emissions of FL by directly measuring real-world at-scale FL tasks running on millions of phones. Second, we present challenges, guidelines, and lessons learned from studying the trade-off between energy efficiency, performance, and time-to-train in a production FL system. Our findings offer valuable insights into how FL can reduce its carbon footprint, and they provide a foundation for future research in the area of Green AI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2155249906",
                    "name": "Sheng Guo"
                },
                {
                    "authorId": "2052661789",
                    "name": "Ashish Shenoy"
                },
                {
                    "authorId": "2041115682",
                    "name": "Sayan Ghosh"
                },
                {
                    "authorId": "37502184",
                    "name": "Pierre Stock"
                },
                {
                    "authorId": "10995410",
                    "name": "Kiwan Maeng"
                },
                {
                    "authorId": "2212361386",
                    "name": "Schalk-Willem Kruger"
                },
                {
                    "authorId": "2066127975",
                    "name": "Michael G. Rabbat"
                },
                {
                    "authorId": "2797270",
                    "name": "Carole-Jean Wu"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                }
            ]
        },
        {
            "paperId": "42e685d9997e579746b4e2f39988e3ddb59899f3",
            "title": "Reconciling Security and Communication Efficiency in Federated Learning",
            "abstract": "Cross-device Federated Learning is an increasingly popular machine learning setting to train a model by leveraging a large population of client devices with high privacy and security guarantees. However, communication efficiency remains a major bottleneck when scaling federated learning to production environments, particularly due to bandwidth constraints during uplink communication. In this paper, we formalize and address the problem of compressing client-to-server model updates under the Secure Aggregation primitive, a core component of Federated Learning pipelines that allows the server to aggregate the client updates without accessing them individually. In particular, we adapt standard scalar quantization and pruning methods to Secure Aggregation and propose Secure Indexing, a variant of Secure Aggregation that supports quantization for extreme compression. We establish state-of-the-art results on LEAF benchmarks in a secure Federated Learning setup with up to 40$\\times$ compression in uplink communication with no meaningful loss in utility compared to uncompressed baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107060033",
                    "name": "Karthik Prasad"
                },
                {
                    "authorId": "2041115682",
                    "name": "Sayan Ghosh"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                },
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "37502184",
                    "name": "Pierre Stock"
                }
            ]
        },
        {
            "paperId": "43602d85c69fd64c62592fc3a332a5b2eeb2c98b",
            "title": "FEL: High Capacity Learning for Recommendation and Ranking via Federated Ensemble Learning",
            "abstract": "Federated learning (FL) has emerged as an effective approach to address consumer privacy needs. FL has been successfully applied to certain machine learning tasks, such as training smart keyboard models and keyword spotting. Despite FL's initial success, many important deep learning use cases, such as ranking and recommendation tasks, have been limited from on-device learning. One of the key challenges faced by practical FL adoption for DL-based ranking and recommendation is the prohibitive resource requirements that cannot be satisfied by modern mobile systems. We propose Federated Ensemble Learning (FEL) as a solution to tackle the large memory requirement of deep learning ranking and recommendation tasks. FEL enables large-scale ranking and recommendation model training on-device by simultaneously training multiple model versions on disjoint clusters of client devices. FEL integrates the trained sub-models via an over-arch layer into an ensemble model that is hosted on the server. Our experiments demonstrate that FEL leads to 0.43-2.31% model quality improvement over traditional on-device federated learning - a significant improvement for ranking and recommendation system use cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31255332",
                    "name": "Meisam Hejazinia"
                },
                {
                    "authorId": "68973739",
                    "name": "Dzmitry Huba"
                },
                {
                    "authorId": "2909360",
                    "name": "Ilias Leontiadis"
                },
                {
                    "authorId": "10995410",
                    "name": "Kiwan Maeng"
                },
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                },
                {
                    "authorId": "145557680",
                    "name": "Luca Melis"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                },
                {
                    "authorId": "3490923",
                    "name": "Milad Nasr"
                },
                {
                    "authorId": "2150030991",
                    "name": "Kaikai Wang"
                },
                {
                    "authorId": "2797270",
                    "name": "Carole-Jean Wu"
                }
            ]
        },
        {
            "paperId": "4b7215ebc0457a7e171bb3007c1e11623eef6615",
            "title": "Defending against Reconstruction Attacks with R\u00e9nyi Differential Privacy",
            "abstract": "Reconstruction attacks allow an adversary to regenerate data samples of the training set using access to only a trained model. It has been recently shown that simple heuristics can reconstruct data samples from language models, making this threat scenario an important aspect of model release. Differential privacy is a known solution to such attacks, but is often used with a relatively large privacy budget (epsilon>8) which does not translate to meaningful guarantees. In this paper we show that, for a same mechanism, we can derive privacy guarantees for reconstruction attacks that are better than the traditional ones from the literature. In particular, we show that larger privacy budgets do not protect against membership inference, but can still protect extraction of rare secrets. We show experimentally that our guarantees hold against various language models, including GPT-2 finetuned on Wikitext-103.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "37502184",
                    "name": "Pierre Stock"
                },
                {
                    "authorId": "2107059646",
                    "name": "I. Shilov"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                },
                {
                    "authorId": "2256994781",
                    "name": "Alexandre Sablayrolles"
                }
            ]
        },
        {
            "paperId": "4ace0211031f9ef79c70fbeca9e07dc121be90ff",
            "title": "Antipodes of Label Differential Privacy: PATE and ALIBI",
            "abstract": "We consider the privacy-preserving machine learning (ML) setting where the trained model must satisfy differential privacy (DP) with respect to the labels of the training examples. We propose two novel approaches based on, respectively, the Laplace mechanism and the PATE framework, and demonstrate their effectiveness on standard benchmarks. While recent work by Ghazi et al. proposed Label DP schemes based on a randomized response mechanism, we argue that additive Laplace noise coupled with Bayesian inference (ALIBI) is a better fit for typical ML tasks. Moreover, we show how to achieve very strong privacy levels in some regimes, with our adaptation of the PATE framework that builds on recent advances in semi-supervised learning. We complement theoretical analysis of our algorithms' privacy guarantees with empirical evaluation of their memorization properties. Our evaluation suggests that comparing different algorithms according to their provable DP guarantees can be misleading and favor a less private algorithm with a tighter analysis. Code for implementation of algorithms and memorization attacks is available from https://github.com/facebookresearch/label_dp_antipodes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                },
                {
                    "authorId": "2107060033",
                    "name": "Karthik Prasad"
                },
                {
                    "authorId": "2107059646",
                    "name": "I. Shilov"
                },
                {
                    "authorId": "2444919",
                    "name": "Florian Tram\u00e8r"
                }
            ]
        },
        {
            "paperId": "bea1187a1f8a68f1a93f0c2fa10d31f93a30f84e",
            "title": "Opacus: User-Friendly Differential Privacy Library in PyTorch",
            "abstract": "We introduce Opacus, a free, open-source PyTorch library for training deep learning models with differential privacy (hosted at opacus.ai). Opacus is designed for simplicity, flexibility, and speed. It provides a simple and user-friendly API, and enables machine learning practitioners to make a training pipeline private by adding as little as two lines to their code. It supports a wide variety of layers, including multi-head attention, convolution, LSTM, GRU (and generic RNN), and embedding, right out of the box and provides the means for supporting other user-defined layers. Opacus computes batched per-sample gradients, providing higher efficiency compared to the traditional\"micro batch\"approach. In this paper we present Opacus, detail the principles that drove its implementation and unique features, and benchmark it against other frameworks for training models with differential privacy as well as standard PyTorch.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2107059646",
                    "name": "I. Shilov"
                },
                {
                    "authorId": "2319225824",
                    "name": "Alexandre Sablayrolles"
                },
                {
                    "authorId": "1389630028",
                    "name": "Davide Testuggine"
                },
                {
                    "authorId": "2107060033",
                    "name": "Karthik Prasad"
                },
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                },
                {
                    "authorId": "2131399147",
                    "name": "John Nguyen"
                },
                {
                    "authorId": "2129469303",
                    "name": "Sayan Gosh"
                },
                {
                    "authorId": "2528900",
                    "name": "Akash Bharadwaj"
                },
                {
                    "authorId": "2024689450",
                    "name": "Jessica Zhao"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                }
            ]
        },
        {
            "paperId": "e7fab03b45d7a19f25e85fc50edd5890eadbba9b",
            "title": "Practical, Label Private Deep Learning Training based on Secure Multiparty Computation and Differential Privacy",
            "abstract": "Secure Multiparty Computation (MPC) is an invaluable tool for training machine learning models when the training data cannot be directly accessed by the model trainer. Unfortunately, complex algorithms, such as deep learning models, have their computational complexities increased by orders of magnitude when performed using MPC protocols. In this contribution, we study how to ef\ufb01ciently train an important class of machine learning problems by using MPC where features are known by one of the computing parties and only the labels are private. We propose new protocols combining differential privacy (DP) and MPC in order to privately and ef\ufb01ciently train a deep learning model in such scenario. More speci\ufb01cally, we release differentially private information during the MPC computation to dramatically reduce the training time. All released information idoes not compromise the privacy of the labels at the individual level. Our protocols can have running times that are orders of magnitude better than a straightforward use of MPC at a moderate cost in model accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113986962",
                    "name": "Sen Yuan"
                },
                {
                    "authorId": "2117667919",
                    "name": "Milan Shen"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                },
                {
                    "authorId": "143961127",
                    "name": "Anderson C. A. Nascimento"
                }
            ]
        },
        {
            "paperId": "0cf8b0535f3f4683aa6d2b0dc7e7b042b1f2fe87",
            "title": "Privacy-preserving Data Mining in Industry",
            "abstract": "Preserving privacy of users is a key requirement of web-scale data mining applications and systems such as web search, recommender systems, crowdsourced platforms, and analytics applications, and has witnessed a renewed focus in light of recent data breaches and new regulations such as GDPR. In this tutorial, we will first present an overview of privacy breaches over the last two decades and the lessons learned, key regulations and laws, and evolution of privacy techniques leading to differential privacy definition / techniques. Then, we will focus on the application of privacy-preserving data mining techniques in practice, by presenting case studies such as Apple\u2019s differential privacy deployment for iOS / macOS, Google\u2019s RAPPOR, LinkedIn Salary, and Microsoft\u2019s differential privacy deployment for collecting Windows telemetry. We will conclude with open problems and challenges for the data mining / machine learning community, based on our experiences in industry.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                },
                {
                    "authorId": "2623159",
                    "name": "Abhradeep Thakurta"
                }
            ]
        },
        {
            "paperId": "5b90758d0921226ac2e2c44ec218d32dd20d51df",
            "title": "Privacy-preserving Data Mining in Industry",
            "abstract": "Preserving privacy of users is a key requirement of web-scale data mining applications and systems such as web search, recommender systems, crowdsourced platforms, and analytics applications, and has witnessed a renewed focus in light of recent data breaches and new regulations such as GDPR. In this tutorial, we will first present an overview of privacy breaches over the last two decades and the lessons learned, key regulations and laws, and evolution of privacy techniques leading to differential privacy definition / techniques. Then, we will focus on the application of privacy-preserving data mining techniques in practice, by presenting case studies such as Apple's differential privacy deployment for iOS / macOS, Google's RAPPOR, LinkedIn Salary, and Microsoft's differential privacy deployment for collecting Windows telemetry. We will conclude with open problems and challenges for the data mining / machine learning community, based on our experiences in industry.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                },
                {
                    "authorId": "2623159",
                    "name": "Abhradeep Thakurta"
                }
            ]
        }
    ]
}