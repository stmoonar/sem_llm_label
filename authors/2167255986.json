{
    "authorId": "2167255986",
    "papers": [
        {
            "paperId": "02d4096c030d052e1866d52fbc3b83480e1ed9f5",
            "title": "Towards Next-Generation Intelligent Assistants Leveraging LLM Techniques",
            "abstract": "Virtual Intelligent Assistants take user requests in the voice form, perform actions such as setting an alarm, turning on a light, and answering a question, and provide answers or confirmations in the voice form or through other channels such as a screen. Assistants have become prevalent in the past decade, and users have been taking services from assistants like Amazon Alexa, Apple Siri, Google Assistant, and Microsoft Cortana. The emergence of AR/VR devices raised many new challenges for building intelligent assistants. The unique requirements have inspired new research directions such as (a) understanding users' situated multi-modal contexts (e.g. vision, sensor signals) as well as language-oriented conversational contexts, (b) personalizing the assistant services by grounding interactions on growing public and personal knowledge graphs and online search engines, and (c) on- device model inference and training techniques that satisfy strict resource and privacy constraints. In this tutorial, we will provide an in-depth walk-through of techniques in the afore-mentioned areas in the recent literature. We aim to introduce techniques for researchers and practitioners who are building intelligent assistants, and inspire research that will bring us one step closer to realizing the dream of building an all-day accompanying assistant. Additionally, we will highlight the significant role that Large Language Models (LLMs) play in enhancing these strategies, underscoring their potential to reshape the future landscape of intelligent assistance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143918656",
                    "name": "X. Dong"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "15574937",
                    "name": "Y. Xu"
                },
                {
                    "authorId": "40243893",
                    "name": "Kshitiz Malik"
                },
                {
                    "authorId": "2167255986",
                    "name": "Zhou Yu"
                }
            ]
        },
        {
            "paperId": "428d2c491f8dcb942931cc44dab295fd5e1dc9a4",
            "title": "User Adaptive Language Learning Chatbots with a Curriculum",
            "abstract": "Along with the development of systems for natural language understanding and generation, dialog systems have been widely adopted for language learning and practicing. Many current educational dialog systems perform chitchat, where the generated content and vocabulary are not constrained. However, for learners in a school setting, practice through dialog is more effective if it aligns with students' curriculum and focuses on textbook vocabulary. Therefore, we adapt lexically constrained decoding to a dialog system, which urges the dialog system to include curriculum-aligned words and phrases in its generated utterances. We adopt a generative dialog system, BlenderBot3, as our backbone model and evaluate our curriculum-based dialog system with middle school students learning English as their second language. The constrained words and phrases are derived from their textbooks, suggested by their English teachers. The evaluation result demonstrates that the dialog system with curriculum infusion improves students' understanding of target words and increases their interest in practicing English.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143857311",
                    "name": "Kun Qian"
                },
                {
                    "authorId": "2059941813",
                    "name": "Ryan Shea"
                },
                {
                    "authorId": "40058381",
                    "name": "Yu Li"
                },
                {
                    "authorId": "2420534",
                    "name": "Luke K. Fryer"
                },
                {
                    "authorId": "2167255986",
                    "name": "Zhou Yu"
                }
            ]
        },
        {
            "paperId": "50dfc82a2efcf2c50a8118bfcc1a0a74c773e334",
            "title": "IdEALS: Idiomatic Expressions for Advancement of Language Skills",
            "abstract": "Although significant progress has been made in developing methods for Grammatical Error Correction (GEC), addressing word choice improvements has been notably lacking and enhancing sentence expressivity by replacing phrases with advanced expressions is an understudied aspect. In this paper, we focus on this area and present our investigation into the task of incorporating the usage of idiomatic expressions in student writing. To facilitate our study, we curate extensive training sets and expert-annotated testing sets using real-world data and evaluate various approaches and compare their performance against human experts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218040104",
                    "name": "Narutatsu Ri"
                },
                {
                    "authorId": "2218182108",
                    "name": "Bill Sun"
                },
                {
                    "authorId": "102536922",
                    "name": "Sam Davidson"
                },
                {
                    "authorId": "2167255986",
                    "name": "Zhou Yu"
                }
            ]
        },
        {
            "paperId": "57b7fa5100fe227f14fbbd233884baf97e89cf8a",
            "title": "FaceChat: An Emotion-Aware Face-to-face Dialogue Framework",
            "abstract": "While current dialogue systems like ChatGPT have made significant advancements in text-based interactions, they often overlook the potential of other modalities in enhancing the overall user experience. We present FaceChat, a web-based dialogue framework that enables emotionally-sensitive and face-to-face conversations. By seamlessly integrating cutting-edge technologies in natural language processing, computer vision, and speech processing, FaceChat delivers a highly immersive and engaging user experience. FaceChat framework has a wide range of potential applications, including counseling, emotional support, and personalized customer service. The system is designed to be simple and flexible as a platform for future researchers to advance the field of multimodal dialogue systems. The code is publicly available at https://github.com/qywu/FaceChat.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2428276",
                    "name": "Deema Alnuhait"
                },
                {
                    "authorId": "31060482",
                    "name": "Qingyang Wu"
                },
                {
                    "authorId": "2167255986",
                    "name": "Zhou Yu"
                }
            ]
        },
        {
            "paperId": "6f9d3c3a628e8d842de83f4ba2e41dd1b4b404ac",
            "title": "PLACES: Prompting Language Models for Social Conversation Synthesis",
            "abstract": "Collecting high quality conversational data can be very expensive for most applications and infeasible for others due to privacy, ethical, or similar concerns. A promising direction to tackle this problem is to generate synthetic dialogues by prompting large language models. In this work, we use a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting. We perform several thorough evaluations of our synthetic conversations compared to human-collected conversations. This includes various dimensions of conversation quality with human evaluation directly on the synthesized conversations, and interactive human evaluation of chatbots fine-tuned on the synthetically generated dataset. We additionally demonstrate that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi-party tasks. Our synthetic multi-party conversations were rated more favorably across all measured dimensions compared to conversation excerpts sampled from a human-collected multi-party dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66615738",
                    "name": "Maximillian Chen"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "46387857",
                    "name": "Chenyang Tao"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "146177177",
                    "name": "Andrew Rosenbaum"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2167255986",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "b34862afacf36e7011d40c67bb67c5ee9cf7da22",
            "title": "DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI",
            "abstract": "Despite advancements in conversational AI, language models encounter challenges to handle diverse conversational tasks, and existing dialogue dataset collections often lack diversity and comprehensiveness. To tackle these issues, we introduce DialogStudio: the largest and most diverse collection of dialogue datasets, unified under a consistent format while preserving their original information. Our collection encompasses data from open-domain dialogues, task-oriented dialogues, natural language understanding, conversational recommendation, dialogue summarization, and knowledge-grounded dialogues, making it an incredibly rich and diverse resource for dialogue research and model training.To further enhance the utility of DialogStudio, we identify the licenses for each dataset, design external knowledge and domain-aware prompts for selected dialogues to facilitate instruction-aware fine-tuning. To improve transparency and support dataset and task-based research, as well as language model pre-training, all datasets, licenses, codes, and models associated with DialogStudio will be made publicly accessible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108313930",
                    "name": "Jianguo Zhang"
                },
                {
                    "authorId": "143857311",
                    "name": "Kun Qian"
                },
                {
                    "authorId": "2223887365",
                    "name": "Zhiwei Liu"
                },
                {
                    "authorId": "71926704",
                    "name": "Shelby Heinecke"
                },
                {
                    "authorId": "2087884364",
                    "name": "Rui Meng"
                },
                {
                    "authorId": "2221142602",
                    "name": "Ye Liu"
                },
                {
                    "authorId": "2167255986",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "1702137",
                    "name": "S. Savarese"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                }
            ]
        },
        {
            "paperId": "e5c72b92c48d68594b290c84a8904da7c8335554",
            "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
            "abstract": "Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1780690",
                    "name": "Baolin Peng"
                },
                {
                    "authorId": "1947267",
                    "name": "Michel Galley"
                },
                {
                    "authorId": "50462546",
                    "name": "Pengcheng He"
                },
                {
                    "authorId": "47413820",
                    "name": "Hao Cheng"
                },
                {
                    "authorId": "2167703904",
                    "name": "Yujia Xie"
                },
                {
                    "authorId": null,
                    "name": "Yu Hu"
                },
                {
                    "authorId": "2110991956",
                    "name": "Qiuyuan Huang"
                },
                {
                    "authorId": "145417002",
                    "name": "Lars Lid\u00e9n"
                },
                {
                    "authorId": "2167255986",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2109136147",
                    "name": "Weizhu Chen"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ]
        },
        {
            "paperId": "ed2221b2260169acf5fe962cf757e46082f85bbf",
            "title": "Controllable Mixed-Initiative Dialogue Generation through Prompting",
            "abstract": "Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents. However, these supervised generation models are limited by the cost and quality of data annotation.We instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation. We formalize prompt construction for controllable mixed-initiative dialogue. Our findings show improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionForGood and Emotional Support Conversations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66615738",
                    "name": "Maximillian Chen"
                },
                {
                    "authorId": "2193279092",
                    "name": "Xiao Yu"
                },
                {
                    "authorId": "8299781",
                    "name": "Weiyan Shi"
                },
                {
                    "authorId": "2216556621",
                    "name": "Urvi Awasthi"
                },
                {
                    "authorId": "2167255986",
                    "name": "Zhou Yu"
                }
            ]
        },
        {
            "paperId": "3998502814e847e12aa4bf2ee2f911890b388bc5",
            "title": "AutoReply: Detecting Nonsense in Dialogue Introspectively with Discriminative Replies",
            "abstract": "Existing approaches built separate classifiers to detect nonsense in dialogues. In this paper, we show that without external classifiers, dialogue models can detect errors in their own messages introspectively, by calculating the likelihood of replies that are indicative of poor messages. For example, if an agent believes its partner is likely to respond\"I don't understand\"to a candidate message, that message may not make sense, so an alternative message should be chosen. We evaluate our approach on a dataset from the game Diplomacy, which contains long dialogues richly grounded in the game state, on which existing models make many errors. We first show that hand-crafted replies can be effective for the task of detecting nonsense in applications as complex as Diplomacy. We then design AutoReply, an algorithm to search for such discriminative replies automatically, given a small number of annotated dialogue examples. We find that AutoReply-generated replies outperform handcrafted replies and perform on par with carefully fine-tuned large supervised models. Results also show that one single reply without much computation overheads can also detect dialogue nonsense reasonably well.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8299781",
                    "name": "Weiyan Shi"
                },
                {
                    "authorId": "31461304",
                    "name": "Emily Dinan"
                },
                {
                    "authorId": "2106411510",
                    "name": "Adithya Renduchintala"
                },
                {
                    "authorId": "47070750",
                    "name": "Daniel Fried"
                },
                {
                    "authorId": "12782441",
                    "name": "Athul Paul Jacob"
                },
                {
                    "authorId": "2167255986",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "35084211",
                    "name": "M. Lewis"
                }
            ]
        },
        {
            "paperId": "4f83f86428fb7682e132a0f7497a5497f8f1789c",
            "title": "Using Chatbots to Teach Languages",
            "abstract": "This paper reports on progress towards building an online language learning tool to provide learners with conversational experience by using dialog systems as conversation practice partners. Our system can adapt to users' language proficiency on the fly. We also provide automatic grammar error feedback to help users learn from their mistakes. According to our first adopters, our system is entertaining and useful. Furthermore, we will provide the learning technology community a large-scale conversation dataset on language learning and grammar correction. Our next step is to make our system more adaptive to user profile information by using reinforcement learning algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40058381",
                    "name": "Yu Li"
                },
                {
                    "authorId": "2109402256",
                    "name": "Chun-Yen Chen"
                },
                {
                    "authorId": "150978762",
                    "name": "Dian Yu"
                },
                {
                    "authorId": "102536922",
                    "name": "Sam Davidson"
                },
                {
                    "authorId": "2158856983",
                    "name": "Ryan Hou"
                },
                {
                    "authorId": "2148273176",
                    "name": "Xun Yuan"
                },
                {
                    "authorId": "2167225316",
                    "name": "Yinghua Tan"
                },
                {
                    "authorId": "2145991322",
                    "name": "Derek Pham"
                },
                {
                    "authorId": "2167255986",
                    "name": "Zhou Yu"
                }
            ]
        }
    ]
}