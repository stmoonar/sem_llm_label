{
    "authorId": "1382771958",
    "papers": [
        {
            "paperId": "4b5fefaccd9153da9895f69ee3ec7ce6c0b747d0",
            "title": "Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications",
            "abstract": "Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it forward as a practical method to robustly train large-scale NLP models. Our code and datasets are available at https://github.com/Han8931/rsmi_nlp",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1382771958",
                    "name": "Han Cheol Moon"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                },
                {
                    "authorId": "2091437375",
                    "name": "Ruochen Zhao"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2162736149",
                    "name": "Xu Chi"
                }
            ]
        },
        {
            "paperId": "4178159a23f9e07ee05a78aa7ddd921cf094c65a",
            "title": "GradMask: Gradient-Guided Token Masking for Textual Adversarial Example Detection",
            "abstract": "We present GradMask, a simple adversarial example detection scheme for natural language processing (NLP) models. It uses gradient signals to detect adversarially perturbed tokens in an input sequence and occludes such tokens by a masking process. GradMask provides several advantages over existing methods including improved detection performance and an interpretation of its decision with a only moderate computational cost. Its approximated inference cost is no more than a single forward- and back-propagation through the target model without requiring any additional detection module. Extensive evaluation on widely adopted NLP benchmark datasets demonstrates the efficiency and effectiveness of GradMask. Code and models are available at https://github.com/Han8931/grad_mask_detection",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1382771958",
                    "name": "Han Cheol Moon"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                },
                {
                    "authorId": "2162736149",
                    "name": "Xu Chi"
                }
            ]
        },
        {
            "paperId": "2b01e6552cc929e1727fc145db82a252b3294c89",
            "title": "A Unified Neural Coherence Model",
            "abstract": "Recently, neural approaches to coherence modeling have achieved state-of-the-art results in several evaluation tasks. However, we show that most of these models often fail on harder tasks with more realistic application scenarios. In particular, the existing models underperform on tasks that require the model to be sensitive to local contexts such as candidate ranking in conversational dialogue and in machine translation. In this paper, we propose a unified coherence model that incorporates sentence grammar, inter-sentence coherence relations, and global coherence patterns into a common neural framework. With extensive experiments on local and global discrimination tasks, we demonstrate that our proposed model outperforms existing models by a good margin, and establish a new state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1382771958",
                    "name": "Han Cheol Moon"
                },
                {
                    "authorId": "6838342",
                    "name": "Tasnim Mohiuddin"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                },
                {
                    "authorId": "2162736149",
                    "name": "Xu Chi"
                }
            ]
        }
    ]
}