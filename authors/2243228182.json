{
    "authorId": "2243228182",
    "papers": [
        {
            "paperId": "260282639dce1984c1a065aa4feae41cea0fed06",
            "title": "Bilingual Adaptation of Monolingual Foundation Models",
            "abstract": "We present an efficient method for adapting a monolingual Large Language Model (LLM) to another language, addressing challenges of catastrophic forgetting and tokenizer limitations. We focus this study on adapting Llama 2 to Arabic. Our two-stage approach begins with expanding the vocabulary and training only the embeddings matrix, followed by full model continual pre-training on a bilingual corpus. By continually pre-training on a mix of Arabic and English corpora, the model retains its proficiency in English while acquiring capabilities in Arabic. Our approach results in significant improvements in Arabic and slight enhancements in English, demonstrating cost-effective cross-lingual transfer. We perform ablations on embedding initialization techniques, data mix ratios, and learning rates and release a detailed training recipe. To demonstrate generalizability of this approach we also adapted Llama 3 8B to Arabic and Llama 2 13B to Hindi.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2238627171",
                    "name": "Gurpreet Gosal"
                },
                {
                    "authorId": "2312005159",
                    "name": "Yishi Xu"
                },
                {
                    "authorId": "2311889065",
                    "name": "Gokul Ramakrishnan"
                },
                {
                    "authorId": "2311894591",
                    "name": "Rituraj Joshi"
                },
                {
                    "authorId": "2311892291",
                    "name": "Avraham Sheinin"
                },
                {
                    "authorId": "2311991181",
                    "name": "Zhiming Chen"
                },
                {
                    "authorId": "2311891083",
                    "name": "Biswajit Mishra"
                },
                {
                    "authorId": "2243228182",
                    "name": "Natalia Vassilieva"
                },
                {
                    "authorId": "3130228",
                    "name": "Joel Hestness"
                },
                {
                    "authorId": "1880394",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "2311888768",
                    "name": "Sunil Kumar Sahu"
                },
                {
                    "authorId": "2087720002",
                    "name": "Bokang Jia"
                },
                {
                    "authorId": "2311887646",
                    "name": "Onkar Pandit"
                },
                {
                    "authorId": "2235818050",
                    "name": "Satheesh Katipomu"
                },
                {
                    "authorId": "2203791403",
                    "name": "Samta Kamboj"
                },
                {
                    "authorId": "2313594182",
                    "name": "Samujjwal Ghosh"
                },
                {
                    "authorId": "2235794681",
                    "name": "Rahul Pal"
                },
                {
                    "authorId": "2311889115",
                    "name": "Parvez Mullah"
                },
                {
                    "authorId": "2311887505",
                    "name": "Soundar Doraiswamy"
                },
                {
                    "authorId": "2311887344",
                    "name": "Mohamed El Karim Chami"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "2ddef4301dc9f9ef0f36e111e83cf8428716c562",
            "title": "Med42 - Evaluating Fine-Tuning Strategies for Medical LLMs: Full-Parameter vs. Parameter-Efficient Approaches",
            "abstract": "This study presents a comprehensive analysis and comparison of two predominant fine-tuning methodologies - full-parameter fine-tuning and parameter-efficient tuning - within the context of medical Large Language Models (LLMs). We developed and refined a series of LLMs, based on the Llama-2 architecture, specifically designed to enhance medical knowledge retrieval, reasoning, and question-answering capabilities. Our experiments systematically evaluate the effectiveness of these tuning strategies across various well-known medical benchmarks. Notably, our medical LLM Med42 showed an accuracy level of 72% on the US Medical Licensing Examination (USMLE) datasets, setting a new standard in performance for openly available medical LLMs. Through this comparative analysis, we aim to identify the most effective and efficient method for fine-tuning LLMs in the medical domain, thereby contributing significantly to the advancement of AI-driven healthcare applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2297849088",
                    "name": "Cl'ement Christophe"
                },
                {
                    "authorId": "2297848890",
                    "name": "Praveen K Kanithi"
                },
                {
                    "authorId": "102615578",
                    "name": "Prateek Munjal"
                },
                {
                    "authorId": "1840492875",
                    "name": "Tathagata Raha"
                },
                {
                    "authorId": "2297848425",
                    "name": "Nasir Hayat"
                },
                {
                    "authorId": "2297849270",
                    "name": "Ronnie Rajan"
                },
                {
                    "authorId": "2297848914",
                    "name": "Ahmed Al-Mahrooqi"
                },
                {
                    "authorId": "2298005616",
                    "name": "Avani Gupta"
                },
                {
                    "authorId": "2297848569",
                    "name": "Muhammad Umar Salman"
                },
                {
                    "authorId": "2238627171",
                    "name": "Gurpreet Gosal"
                },
                {
                    "authorId": "9645616",
                    "name": "Bhargav Kanakiya"
                },
                {
                    "authorId": "2297853602",
                    "name": "Charles Chen"
                },
                {
                    "authorId": "2243228182",
                    "name": "Natalia Vassilieva"
                },
                {
                    "authorId": "2125606",
                    "name": "B. Amor"
                },
                {
                    "authorId": "2308995611",
                    "name": "M. A. Pimentel"
                },
                {
                    "authorId": "2298728972",
                    "name": "Shadab Khan"
                }
            ]
        },
        {
            "paperId": "39bb5d44735c07b1e1f4341a2d4bc8d5e783f491",
            "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training",
            "abstract": "This paper aims to understand the impacts of various data combinations (e.g., web text, Wikipedia, GitHub, books) on the pretraining of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T token RedPajama dataset contributed by Together. We have termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations on SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our SlimPajama-DC models are available at: https://huggingface.co/MBZUAI-LLM/SlimPajama-DC and the separate SlimPajama-DC datasets are available at: https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2243374493",
                    "name": "Zhiqiang Shen"
                },
                {
                    "authorId": "2242959702",
                    "name": "Tianhua Tao"
                },
                {
                    "authorId": "2243392466",
                    "name": "Liqun Ma"
                },
                {
                    "authorId": "2934259",
                    "name": "W. Neiswanger"
                },
                {
                    "authorId": "100468503",
                    "name": "Zhengzhong Liu"
                },
                {
                    "authorId": "2254303011",
                    "name": "Hongyi Wang"
                },
                {
                    "authorId": "10918587",
                    "name": "Bowen Tan"
                },
                {
                    "authorId": "3130228",
                    "name": "Joel Hestness"
                },
                {
                    "authorId": "2243228182",
                    "name": "Natalia Vassilieva"
                },
                {
                    "authorId": "2243228174",
                    "name": "Daria Soboleva"
                },
                {
                    "authorId": "2243234805",
                    "name": "Eric P. Xing"
                }
            ]
        },
        {
            "paperId": "44b7adbd196e69c8771734aa8c9af5fd69c04370",
            "title": "BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model",
            "abstract": "We introduce the Bittensor Language Model, called\"BTLM-3B-8K\", a new state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and 8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B parameter models. Additionally, BTLM-3B-8K provides excellent long context performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192 context length. We trained the model on a cleaned and deduplicated SlimPajama dataset; aggressively tuned the \\textmu P hyperparameters and schedule; used ALiBi position embeddings; and adopted the SwiGLU nonlinearity. On Hugging Face, the most popular models have 7B parameters, indicating that users prefer the quality-size ratio of 7B models. Compacting the 7B parameter model to one with 3B parameters, with little performance impact, is an important milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision and takes 2.5x less inference compute than 7B models, helping to open up access to a powerful language model on mobile and edge devices. BTLM-3B-8K is available under an Apache 2.0 license on Hugging Face: https://huggingface.co/cerebras/btlm-3b-8k-base.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1564139903",
                    "name": "Nolan Dey"
                },
                {
                    "authorId": "2243228174",
                    "name": "Daria Soboleva"
                },
                {
                    "authorId": "2243335766",
                    "name": "Faisal Al-Khateeb"
                },
                {
                    "authorId": "2243383177",
                    "name": "Bowen Yang"
                },
                {
                    "authorId": "2213728244",
                    "name": "Ribhu Pathria"
                },
                {
                    "authorId": "2213732240",
                    "name": "Hemant Khachane"
                },
                {
                    "authorId": "2243337041",
                    "name": "Shaheer Muhammad"
                },
                {
                    "authorId": "2311991181",
                    "name": "Zhiming Chen"
                },
                {
                    "authorId": "2243336038",
                    "name": "Robert Myers"
                },
                {
                    "authorId": "2243336766",
                    "name": "Jacob Robert Steeves"
                },
                {
                    "authorId": "2243228182",
                    "name": "Natalia Vassilieva"
                },
                {
                    "authorId": "2243336770",
                    "name": "Marvin Tom"
                },
                {
                    "authorId": "3130228",
                    "name": "Joel Hestness"
                }
            ]
        }
    ]
}