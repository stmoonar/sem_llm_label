{
    "authorId": "29962444",
    "papers": [
        {
            "paperId": "a39e78fe2b1e13a8d703404623c1c0a993f6211e",
            "title": "GaitSTR: Gait Recognition with Sequential Two-stream Refinement",
            "abstract": "Gait recognition aims to identify a person based on their walking sequences, serving as a useful biometric modality as it can be observed from long distances without requiring cooperation from the subject. In representing a person's walking sequence, silhouettes and skeletons are the two primary modalities used. Silhouette sequences lack detailed part information when overlapping occurs between different body segments and are affected by carried objects and clothing. Skeletons, comprising joints and bones connecting the joints, provide more accurate part information for different segments; however, they are sensitive to occlusions and low-quality images, causing inconsistencies in frame-wise results within a sequence. In this paper, we explore the use of a two-stream representation of skeletons for gait recognition, alongside silhouettes. By fusing the combined data of silhouettes and skeletons, we refine the two-stream skeletons, joints, and bones through self-correction in graph convolution, along with cross-modal correction with temporal consistency from silhouettes. We demonstrate that with refined skeletons, the performance of the gait recognition model can achieve further improvement on public gait recognition datasets compared with state-of-the-art methods without extra annotations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110569779",
                    "name": "Wanrong Zheng"
                },
                {
                    "authorId": "2115314438",
                    "name": "Haidong Zhu"
                },
                {
                    "authorId": "29962444",
                    "name": "Zhao-Heng Zheng"
                },
                {
                    "authorId": "1395862465",
                    "name": "Ramkant Nevatia"
                }
            ]
        },
        {
            "paperId": "a7e18590473c8aa83d43050ed58a139c1abf6f2e",
            "title": "SEAS: ShapE-Aligned Supervision for Person Re-Identification",
            "abstract": "We introduce SEAS, using ShapE-Aligned Supervision, to enhance appearance-based person re-identification. When recognizing an individual's identity, existing methods primarily rely on appearance, which can be influenced by the background environment due to a lack of body shape awareness. Although some methods attempt to incorporate other modalities, such as gait or body shape, they encode the additional modality separately, resulting in extra computational costs and lacking an inherent connection with appearance. In this paper, we explore the use of implicit 3-D body shape representations as pixel-level guidance to augment the extraction of identity features with body shape knowledge, in addition to appearance. Using body shape as supervision, rather than as input, provides shapeaware enhancements without any increase in computational cost and delivers coherent integration with pixel-wise appearance features. Moreover, for video-based person reidentification, we align pixel-level features across frames with shape awareness to ensure temporal consistency. Our results demonstrate that incorporating body shape as pixel-level supervision reduces rank-1 errors by 1.4% for framebased and by 2.5% for video-based re-identification tasks, respectively, and can also be generalized to other existing appearance-based person re-identification methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115314438",
                    "name": "Haidong Zhu"
                },
                {
                    "authorId": "1988972570",
                    "name": "Pranav Budhwant"
                },
                {
                    "authorId": "29962444",
                    "name": "Zhao-Heng Zheng"
                }
            ]
        },
        {
            "paperId": "0d4bc38b28c74576712c20fd3d636cbab9d0baab",
            "title": "CAT-NeRF: Constancy-Aware Tx2Former for Dynamic Body Modeling",
            "abstract": "This paper addresses the problem of human rendering in the video with temporal appearance constancy. Reconstructing dynamic body shapes with volumetric neural rendering methods, such as NeRF, requires finding the correspondence of the points in the canonical and observation space, which demands understanding human body shape and motion. Some methods use rigid transformation, such as SE(3), which cannot precisely model each frame\u2019s unique motion and muscle movements. Others generate the transformation for each frame with a trainable network, such as neural blend weight field or translation vector field, which does not consider the appearance constancy of general body shape. In this paper, we propose CAT-NeRF for self-awareness of appearance constancy with Tx2Former, a novel way to combine two Transformer layers, to separate appearance constancy and uniqueness. Appearance constancy models the general shape across the video, and uniqueness models the unique patterns for each frame. We further introduce a novel Covariance Loss to limit the correlation between each pair of appearance uniquenesses to ensure the frame-unique pattern is maximally captured in appearance uniqueness. We assess our method on H36M and ZJU-MoCap and show state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115314438",
                    "name": "Haidong Zhu"
                },
                {
                    "authorId": "29962444",
                    "name": "Zhao-Heng Zheng"
                },
                {
                    "authorId": "2110569779",
                    "name": "Wanrong Zheng"
                },
                {
                    "authorId": "1395862465",
                    "name": "Ramkant Nevatia"
                }
            ]
        },
        {
            "paperId": "22d7326ca3d462ae0a4c5d00aa679eba2e4ac51c",
            "title": "Large Language Models are Good Prompt Learners for Low-Shot Image Classification",
            "abstract": "Low-shot image classification, where training images are limited or inaccessible, has benefited from recent progress on pretrained vision-language (VL) models with strong generalizability. e.g. CLIP. Prompt learning methods built with VL models generate text features from the class names that only have confined class-specific information. Large Language Models (LLMs), with their vast en-cyclopedic knowledge, emerge as the complement. Thus, in this paper, we discuss the integration of LLMs to enhance pretrained VL models, specifically on low-shot classification. However, the domain gap between language and vision blocks the direct application of LLMs. Thus, we propose LLaMp, Large Language Models as Prompt learners, that produces adaptive prompts for the CLIP text encoder, establishing it as the connecting bridge. Experiments show that, compared with other state-of-the-art prompt learning methods, LLaMP yields better performance on both zero-shot generalization and few-shot image classification, over a spectrum of 11 datasets. Code will be made available at: https://github.com/zhaohengz/LLaMP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29962444",
                    "name": "Zhao-Heng Zheng"
                },
                {
                    "authorId": "2272189193",
                    "name": "Jingmin Wei"
                },
                {
                    "authorId": "2264540452",
                    "name": "Xuefeng Hu"
                },
                {
                    "authorId": "2115314438",
                    "name": "Haidong Zhu"
                },
                {
                    "authorId": "1395862465",
                    "name": "Ramkant Nevatia"
                }
            ]
        },
        {
            "paperId": "3e23df3d723e35a1eead16c8131cae3feec92343",
            "title": "CAILA: Concept-Aware Intra-Layer Adapters for Compositional Zero-Shot Learning",
            "abstract": "In this paper, we study the problem of Compositional Zero-Shot Learning (CZSL), which is to recognize novel attribute-object combinations with pre-existing concepts. Recent researchers focus on applying large-scale Vision-Language Pre-trained (VLP) models like CLIP with strong generalization ability. However, these methods treat the pre-trained model as a black box and focus on pre- and post-CLIP operations, which do not inherently mine the semantic concept between the layers inside CLIP. We propose to dive deep into the architecture and insert adapters, a parameter-efficient technique proven to be effective among large language models, into each CLIP encoder layer. We further equip adapters with concept awareness so that concept-specific features of \"object\", \"attribute\", and \"composition\" can be extracted. We assess our method on four popular CZSL datasets, MIT-States, C-GQA, UT-Zappos, and VAW-CZSL, which shows state-of-the-art performance compared to existing methods on all of them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29962444",
                    "name": "Zhao-Heng Zheng"
                },
                {
                    "authorId": "2115314438",
                    "name": "Haidong Zhu"
                },
                {
                    "authorId": "1395862465",
                    "name": "Ramkant Nevatia"
                }
            ]
        },
        {
            "paperId": "4eb16a5ff7b2f644d191a88edddf1c951c45d661",
            "title": "ShARc: Shape and Appearance Recognition for Person Identification In-the-wild",
            "abstract": "Identifying individuals in unconstrained video settings is a valuable yet challenging task in biometric analysis due to variations in appearances, environments, degradations, and occlusions. In this paper, we present ShARc, a multimodal approach for video-based person identification in uncontrolled environments that emphasizes 3-D body shape, pose, and appearance. We introduce two encoders: a Pose and Shape Encoder (PSE) and an Aggregated Appearance Encoder (AAE). PSE encodes the body shape via binarized silhouettes, skeleton motions, and 3-D body shape, while AAE provides two levels of temporal appearance feature aggregation: attention-based feature aggregation and averaging aggregation. For attention-based feature aggregation, we employ spatial and temporal attention to focus on key areas for person distinction. For averaging aggregation, we introduce a novel flattening layer after averaging to extract more distinguishable information and reduce overfitting of attention. We utilize centroid feature averaging for gallery registration. We demonstrate significant improvements over existing state-of-the-art methods on public datasets, including CCVID, MEVID, and BRIAR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115314438",
                    "name": "Haidong Zhu"
                },
                {
                    "authorId": "2110569779",
                    "name": "Wanrong Zheng"
                },
                {
                    "authorId": "29962444",
                    "name": "Zhao-Heng Zheng"
                },
                {
                    "authorId": "1395862465",
                    "name": "Ramkant Nevatia"
                }
            ]
        },
        {
            "paperId": "51615bee01c966ba055920e10778d5331d35bccd",
            "title": "MoMo: A shared encoder Model for text, image and multi-Modal representations",
            "abstract": "We propose a self-supervised shared encoder model that achieves strong results on several visual, language and multimodal benchmarks while being data, memory and run-time efficient. We make three key contributions. First, in contrast to most existing works, we use a single transformer with all the encoder layers processing both the text and the image modalities. Second, we propose a stage-wise training strategy where the model is first trained on images, then jointly with unimodal text and image datasets and finally jointly with text and text-image datasets. Third, to preserve information across both the modalities, we propose a training pipeline that learns simultaneously from gradient updates of different modalities at each training update step. The results on downstream text-only, image-only and multimodal tasks show that our model is competitive with several strong models while using fewer parameters and lesser pre-training data. For example, MoMo performs competitively with FLAVA on multimodal (+3.1), image-only (+1.1) and text-only (-0.1) tasks despite having 2/5th the number of parameters and using 1/3rd the image-text training pairs. Finally, we ablate various design choices and further show that increasing model size produces significant performance gains indicating potential for substantial improvements with larger models using our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "147887099",
                    "name": "Rakesh Chada"
                },
                {
                    "authorId": "29962444",
                    "name": "Zhao-Heng Zheng"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                }
            ]
        },
        {
            "paperId": "b74011191cfe84037e4cb1562cdbc2e436859366",
            "title": "AG-ReID 2023: Aerial-Ground Person Re-identification Challenge Results",
            "abstract": "Person re-identification (Re-ID) on aerial-ground platforms has emerged as an intriguing topic within computer vision, presenting a plethora of unique challenges. Highflying altitudes of aerial cameras make persons appear differently in terms of viewpoints, poses, and resolution compared to the images of the same person viewed from ground cameras. Despite its potential, few algorithms have been developed for person re-identification on aerial-ground data, mainly due to the absence of comprehensive datasets. In response, we have collected a large-scale dataset and organized the Aerial-Ground person Re-IDentification Challenge (AG-ReID2023) to foster advancements in the field. The dataset comprises 100,502 images with 1,615 unique identities, including 51,530 training images featuring 807 identities. The test set is divided into two subsets: Aerial to Ground (808 ids, 4,348 query images, 19,259 gallery images) and Ground to Aerial (808 ids, 4,151 query images, 21,214 gallery images). In addition, we manually annotate individuals with their matching IDs across cameras and provide 15 soft attribute labels. The AG-ReID2023 Challenge in conjunction with the 7th IEEE International Joint Conference on Biometrics (IJCB) has garnered interest from numerous institutes, resulting in the submission of five distinct algorithms. We provide an in-depth examination of the evaluation outcomes and present our findings from the contest. For additional details, kindly refer to the official website1.1https://agreid23.github.io.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285432557",
                    "name": "Kien Nguyen"
                },
                {
                    "authorId": "3140440",
                    "name": "C. Fookes"
                },
                {
                    "authorId": "1729760",
                    "name": "S. Sridharan"
                },
                {
                    "authorId": "2285387167",
                    "name": "Feng Liu"
                },
                {
                    "authorId": "2284930824",
                    "name": "Xiaoming Liu"
                },
                {
                    "authorId": "2289137527",
                    "name": "Arun Ross"
                },
                {
                    "authorId": "2284923159",
                    "name": "Dana Michalski"
                },
                {
                    "authorId": "2211653316",
                    "name": "Huy Nguyen"
                },
                {
                    "authorId": "9250292",
                    "name": "Debayan Deb"
                },
                {
                    "authorId": "2074108055",
                    "name": "Mahak Kothari"
                },
                {
                    "authorId": "2284922489",
                    "name": "Manisha Saini"
                },
                {
                    "authorId": "2284915444",
                    "name": "Dawei Du"
                },
                {
                    "authorId": "2284918271",
                    "name": "Scott McCloskey"
                },
                {
                    "authorId": "144226582",
                    "name": "Gabriel Bertocco"
                },
                {
                    "authorId": "2284922420",
                    "name": "Fernanda Andal\u00b4o"
                },
                {
                    "authorId": "2284439078",
                    "name": "Terrance E. Boult"
                },
                {
                    "authorId": "2285175188",
                    "name": "Anderson Rocha"
                },
                {
                    "authorId": "2115314438",
                    "name": "Haidong Zhu"
                },
                {
                    "authorId": "29962444",
                    "name": "Zhao-Heng Zheng"
                },
                {
                    "authorId": "1395862465",
                    "name": "Ramkant Nevatia"
                },
                {
                    "authorId": "2197851114",
                    "name": "Zaigham A. Randhawa"
                },
                {
                    "authorId": "36911226",
                    "name": "Sinan Sabri"
                },
                {
                    "authorId": "2256993173",
                    "name": "Gianfranco Doretto"
                }
            ]
        },
        {
            "paperId": "d4f1fa05945803470281dd09180f64384f2c9f81",
            "title": "GaitRef: Gait Recognition with Refined Sequential Skeletons",
            "abstract": "Identifying humans with their walking sequences, known as gait recognition, is a useful biometric understanding task as it can be observed from a long distance and does not require cooperation from the subject. Two common modalities used for representing the walking sequence of a person are silhouettes and joint skeletons. Silhouette sequences, which record the boundary of the walking person in each frame, may suffer from the variant appearances from carried-on objects and clothes of the person. Framewise joint detections are noisy and introduce some jitters that are not consistent with sequential detections. In this paper, we combine the silhouettes and skeletons and refine the framewise joint predictions for gait recognition. With temporal information from the silhouette sequences. We show that the refined skeletons can improve gait recognition performance without extra annotations. We compare our methods on four public datasets, CASIA-B, OUMVLP, Gait3D and GREW, and show state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115314438",
                    "name": "Haidong Zhu"
                },
                {
                    "authorId": "2110569779",
                    "name": "Wanrong Zheng"
                },
                {
                    "authorId": "29962444",
                    "name": "Zhao-Heng Zheng"
                },
                {
                    "authorId": "1395862465",
                    "name": "Ramkant Nevatia"
                }
            ]
        },
        {
            "paperId": "08cb7c4c8087540407e0386341db982c293c81c3",
            "title": "Temporal Shift and Attention Modules for Graphical Skeleton Action Recognition",
            "abstract": "Skeletons, consisting of joint positions and connections between them, are an important representation for modeling human bodies in image frames. Compared with understanding RGB videos, recognizing actions from the skeletons removes the biases of background and body shapes. Researchers use spatial-temporal graphs to model the skeleton sequences. These methods weigh all frames in the sequence equally even though many of the frames may not be useful for action and prediction and dilute the influence of important frames. Also, the temporal graph focuses on understanding only the low-level feature of the joints for the motion of the skeleton. In this paper, we introduce two modules, temporal shift module and temporal attention module that can be added to graph convolution networks for skeleton action recognition. Temporal attention module focuses on keyframes for making predictions, and temporal shift module helps to exchange the high-level features between different frames along the temporal dimension besides the local patterns. We evaluate the two modules with two existing skeleton action recognition networks, ST-GCN and MS-G3D, on three public datasets and show better results than the original methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115314438",
                    "name": "Haidong Zhu"
                },
                {
                    "authorId": "29962444",
                    "name": "Zhao-Heng Zheng"
                },
                {
                    "authorId": "1395862465",
                    "name": "Ramkant Nevatia"
                }
            ]
        }
    ]
}