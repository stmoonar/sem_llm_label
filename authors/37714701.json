{
    "authorId": "37714701",
    "papers": [
        {
            "paperId": "0123147c5374bc69d896e1206a67c41901126894",
            "title": "An Extensible Multi-Sensor Fusion Framework for 3D Imaging",
            "abstract": "Many autonomous vehicles rely on an array of sensors for safe navigation, where each sensor captures different visual attributes from the surrounding environment. For example, a single conventional camera captures high-resolution images but no 3D information; a LiDAR provides excellent range information but poor spatial resolution; and a prototype single-photon LiDAR (SP-LiDAR) can provide a dense but noisy representation of the 3D scene. Although the outputs of these sensors vary dramatically (e.g., 2D images, point clouds, 3D volumes), they all derive from the same 3D scene. We propose an extensible sensor fusion framework that (1) lifts the sensor output to volumetric representations of the 3D scene, (2) fuses these volumes together, and (3) processes the resulting volume with a deep neural network to generate a depth (or disparity) map. Although our framework can potentially extend to many types of sensors, we focus on fusing combinations of three imaging systems: monocular/stereo cameras, regular LiDARs, and SP-LiDARs. To train our neural network, we generate a synthetic dataset through CARLA that contains the individual measurements. We also conduct various fusion ablation experiments and evaluate the results of different sensor combinations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29899438",
                    "name": "Talha Ahmad Siddiqui"
                },
                {
                    "authorId": "37714701",
                    "name": "Rishi Madhok"
                },
                {
                    "authorId": "81743367",
                    "name": "Matthew O'Toole"
                }
            ]
        },
        {
            "paperId": "e3cf24d8cfdd41700f80ea2827dbe42fda5953f3",
            "title": "Towards Latency-aware DNN Optimization with GPU Runtime Analysis and Tail Effect Elimination",
            "abstract": "Despite the superb performance of State-Of-The-Art (SOTA) DNNs, the increasing computational cost makes them very challenging to meet real-time latency and accuracy requirements. Although DNN runtime latency is dictated by model property (e.g., architecture, operations), hardware property (e.g., utilization, throughput), and more importantly, the effective mapping between these two, many existing approaches focus only on optimizing model property such as FLOPS reduction and overlook the mismatch between DNN model and hardware properties. In this work, we show that the mismatch between the varied DNN computation workloads and GPU capacity can cause the idle GPU tail effect, leading to GPU under-utilization and low throughput. As a result, the FLOPs reduction cannot bring effective latency reduction, which causes sub-optimal accuracy versus latency trade-offs. Motivated by this, we propose a GPU runtime-aware DNN optimization methodology to eliminate such GPU tail effect adaptively on GPU platforms. Our methodology can be applied on top of existing SOTA DNN optimization approaches to achieve better latency and accuracy trade-offs. Experiments show 11%-27% latency reduction and 2.5%-4.0% accuracy improvement over several SOTA DNN pruning and NAS methods, respectively",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143796160",
                    "name": "Fuxun Yu"
                },
                {
                    "authorId": "8800524",
                    "name": "Zirui Xu"
                },
                {
                    "authorId": "2057972235",
                    "name": "Tong Shen"
                },
                {
                    "authorId": "1905484",
                    "name": "Dimitrios Stamoulis"
                },
                {
                    "authorId": "3015446",
                    "name": "Longfei Shangguan"
                },
                {
                    "authorId": "2145384749",
                    "name": "Di Wang"
                },
                {
                    "authorId": "37714701",
                    "name": "Rishi Madhok"
                },
                {
                    "authorId": "3275383",
                    "name": "C. Zhao"
                },
                {
                    "authorId": null,
                    "name": "Xin Li"
                },
                {
                    "authorId": "2474219",
                    "name": "Nikolaos Karianakis"
                },
                {
                    "authorId": "48209560",
                    "name": "Dimitrios Lymberopoulos"
                },
                {
                    "authorId": "2108667778",
                    "name": "Ang Li"
                },
                {
                    "authorId": "47535039",
                    "name": "Chenchen Liu"
                },
                {
                    "authorId": "2135836407",
                    "name": "Yiran Chen"
                },
                {
                    "authorId": "2143734919",
                    "name": "Xiang Chen"
                }
            ]
        },
        {
            "paperId": "794d69e7f2c165a9f4daa3d295c7ce51011884c5",
            "title": "Semantic Understanding for Contextual In-Video Advertising",
            "abstract": "\n \n With the increasing consumer base of online video content, it is important for advertisers to understand the video context when targeting video ads to consumers. To improve the consumer experience and quality of ads, key factors need to be considered such as (i) ad relevance to video content (ii) where and how video ads are placed, and (iii) non-intrusive user experience. We propose a framework to semantically understand the video content for better ad recommendation that ensure these criteria.\n \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37714701",
                    "name": "Rishi Madhok"
                },
                {
                    "authorId": "33906968",
                    "name": "Shashank Mujumdar"
                },
                {
                    "authorId": "2115343577",
                    "name": "Nitin Gupta"
                },
                {
                    "authorId": "145640180",
                    "name": "S. Mehta"
                }
            ]
        },
        {
            "paperId": "a82bfd6530fda11d11d90aee16baac506bc2001d",
            "title": "Content Driven Enrichment of Formal Text using Concept Definitions and Applications",
            "abstract": "Formal text is objective, unambiguous and tends to have complex sentence construction intended to be understood by the target demographic. However, in the absence of domain knowledge it is imperative to define key concepts and their relationship in the text for correct interpretation for general readers. To address this, we propose a text enrichment framework that identifies the key concepts from input text, highlights definitions and fetches the definition from external data sources in case the concept is undefined. Beyond concept definitions, the system enriches the input text with concept applications and a pre-requisite concept graph that showcases the inter-dependency within the extracted concepts. While the problem of learning definition statements is attempted in literature, the task of learning application statements is novel. We manually annotated a dataset for training a deep learning network for identifying application statements in text. We quantitatively compared the results of both application and definition identification models with standard baselines. To validate the utility of the proposed framework for general readers, we report enrichment accuracy and show promising results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144243819",
                    "name": "Abhinav Jain"
                },
                {
                    "authorId": "2115343577",
                    "name": "Nitin Gupta"
                },
                {
                    "authorId": "33906968",
                    "name": "Shashank Mujumdar"
                },
                {
                    "authorId": "145640180",
                    "name": "S. Mehta"
                },
                {
                    "authorId": "37714701",
                    "name": "Rishi Madhok"
                }
            ]
        },
        {
            "paperId": "e572f9f6f68edd8115f55fa38d45449a689839f7",
            "title": "SentiMozart: Music Generation based on Emotions",
            "abstract": "Facial expressions are one of the best and the most intuitive way to determine a persons emotions. They most naturally express how a person is feeling currently. The aim of the proposed framework is to generate music corresponding to the emotion of the person predicted by our model. The proposed framework is divided into two models, the Image Classification Model and the Music Generation Model. The music would be generated by the latter model which is essentially a Doubly Stacked LSTM architecture. This is to be done after classification and identification of the facial expression into one of the seven major sentiment categories: Angry, Disgust, Fear, Happy, Sad, Surprise and Neutral, which would be done by using Convolutional Neural Networks (CNN). Finally, we evaluate the performance of our proposed framework using the emotional Mean Opinion Score (MOS) which is a popular evaluation metric for audio-visual data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37714701",
                    "name": "Rishi Madhok"
                },
                {
                    "authorId": "4054731",
                    "name": "Shivali Goel"
                },
                {
                    "authorId": "50064464",
                    "name": "Shweta Garg"
                }
            ]
        },
        {
            "paperId": "ecbfdcbc09146c87fca594b9dcdf55f9c3504ce3",
            "title": "Video Jigsaw: Unsupervised Learning of Spatiotemporal Context for Video Action Recognition",
            "abstract": "We propose a self-supervised learning method to jointly reason about spatial and temporal context for video recognition. Recent self-supervised approaches have used spatial context [9, 34] as well as temporal coherency [32] but a combination of the two requires extensive preprocessing such as tracking objects through millions of video frames [59] or computing optical flow to determine frame regions with high motion [30]. We propose to combine spatial and temporal context in one self-supervised framework without any heavy preprocessing. We divide multiple video frames into grids of patches and train a network to solve jigsaw puzzles on these patches from multiple frames. So the network is trained to correctly identify the position of a patch within a video frame as well as the position of a patch over time. We also propose a novel permutation strategy that outperforms random permutations while significantly reducing computational and memory constraints. We use our trained network for transfer learning tasks such as video activity recognition and demonstrate the strength of our approach on two benchmark video action recognition datasets without using a single frame from these datasets for unsupervised pretraining of our proposed video jigsaw network.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2308598",
                    "name": "Unaiza Ahsan"
                },
                {
                    "authorId": "37714701",
                    "name": "Rishi Madhok"
                },
                {
                    "authorId": "21472040",
                    "name": "Irfan Essa"
                }
            ]
        }
    ]
}