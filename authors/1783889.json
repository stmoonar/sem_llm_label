{
    "authorId": "1783889",
    "papers": [
        {
            "paperId": "855eccdb127c2987a825275d1afa6f55dcc02cb9",
            "title": "Flow-Edge-Net: Video Saliency Detection Based on Optical Flow and Edge-Weighted Balance Loss",
            "abstract": "Optical flow networks have been widely utilized for video saliency detection (VSD) due to their effective performance in capturing the motion of objects. However, the use of optical flow blurs the edges of salient objects and leads to the problems of poorly defined object boundaries. To address this issue, we propose an optical flow-based edge-weighted loss function, to train a network called Flow-Edge-Net, which can balance the weights of the foreground and background information at the edges of video frames. It has achieved superior performance in detecting salient boundaries. Specifically, we propose two complementary encoding and decoding networks based on the concept of decoupling. That is, the optical flow network focuses on moving objects, while the edge network, based on the encoder-decoder structure, focuses on edge information. As the two networks output features of the same dimension and are from the same input, our proposed self-designed adaptive weighted feature fusion module can compare and integrate the edge information and location information from the two networks through adaptive weighting. The proposed method has been evaluated on five widely used databases. Experiment results demonstrate the superior performance of the proposed Flow-Edge-Net in locating salient objects, with accurate and refined edges. The proposed method achieves superior performance over the state-of-the-art methods in detecting salient objects in videos.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1783889",
                    "name": "Muwei Jian"
                },
                {
                    "authorId": "2127330241",
                    "name": "Xiangwei Lu"
                },
                {
                    "authorId": "2139780382",
                    "name": "Xiaoyang Yu"
                },
                {
                    "authorId": "144904672",
                    "name": "Yakun Ju"
                },
                {
                    "authorId": "2118681612",
                    "name": "Hui Yu"
                },
                {
                    "authorId": "145412853",
                    "name": "K. Lam"
                }
            ]
        },
        {
            "paperId": "b919b9e8320e57955789792ac3f387d67229d80f",
            "title": "Estimating High-Resolution Surface Normals via Low-Resolution Photometric Stereo Images",
            "abstract": "Acquiring high-resolution 3D surface structures is a crucial task in computer vision as it provides more detailed surface textures and clearer structures. Photometric stereo can measure per-pixel surface normals of a 3D object using various shading cues. However, obtaining high-resolution images in a linear response photometric stereo imaging system can be challenging. Additionally, photometric stereo, as a per-pixel reconstruction method, requires higher-resolution surface normal maps to accurately depict complex surface structures, particularly in regions that demand more attention and precise reconstruction. Therefore, measuring high-resolution surface normals via low-resolution photometric stereo images is of great importance. Motivated by these, we propose a Super-resolution Photometric Stereo Network, namely SR-PSN. In order to address the issues of measuring the high-resolution surface normals from low-resolution photometric images, we mainly (1) apply a dual-position threshold normalization pre-processing scheme to effectively handle the spatially-varying reflectance of non-Lambertian surfaces, (2) adopt a local affinity feature module to learn the rich structural representation by explicitly revealing the neighbor relationships, (3) employ a parallel multi-scale feature extractor, which preserves high-resolution representations and deep feature extraction, and (4) propose a shared-weight regressor to handle the multi-scale features, to prevent the model collapsing into learning non-important features related to a certain fixed scale. Extensive ablation experiments validate the effectiveness of our proposed modules. Furthermore, quantitative experiments conducted on public benchmarks demonstrate that SR-PSN outperforms state-of-the-art calibrated photometric stereo methods. Notably, SR-PSN achieves superior results while utilizing photometric stereo images with only half the resolution of other methods. It effectively restores the structure of complex surfaces, producing a high-resolution normal map.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144904672",
                    "name": "Yakun Ju"
                },
                {
                    "authorId": "1783889",
                    "name": "Muwei Jian"
                },
                {
                    "authorId": "2228351437",
                    "name": "Cong Wang"
                },
                {
                    "authorId": "2143528650",
                    "name": "Cong Zhang"
                },
                {
                    "authorId": "1964397",
                    "name": "Junyu Dong"
                },
                {
                    "authorId": "145412853",
                    "name": "K. Lam"
                }
            ]
        },
        {
            "paperId": "d39c6077edf60bcaaa1792c37d8cd1f3325fdf4f",
            "title": "Deep Discrete Wavelet Transform Network for Photometric Stereo",
            "abstract": "Photometric stereo aims to estimate the per-pixel surface normal map of 3D objects via changing the illuminated light directions. Prevalent methods adopt deep neural networks to extract the shading cue features and reconstruct the surface normals. However, previous methods do not consider the frequency of the surface structure, i.e., the complexity of the shape. Simply applying a trained network to all kinds of objects often leads to inter-frequency conflicts and blur in surface normal estimation. This paper presents a discrete wavelet transform-based photometric stereo network (DWTPS-Net) to handle the input photometric stereo images in both the spatial and frequency domains. In DWTPS-Net, we extract shading features from images and also decompose the images using discrete wavelet transform (DWT), which can preserve spatial information naturally, to better extract high-frequency information. We design separate CNN-based feature-extraction modules for the input images and for the different frequency information of the input images via DWT. Ablation studies and experiments on a widely used benchmark dataset show that DWTPS-Net achieves superior performance in surface normal estimation, in terms of mean angular error metric.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144904672",
                    "name": "Yakun Ju"
                },
                {
                    "authorId": "1783889",
                    "name": "Muwei Jian"
                },
                {
                    "authorId": "2143528650",
                    "name": "Cong Zhang"
                },
                {
                    "authorId": "2154496178",
                    "name": "Yeqi Hu"
                },
                {
                    "authorId": "145412853",
                    "name": "K. Lam"
                }
            ]
        },
        {
            "paperId": "2d1ca0f310b17050b74f578844757cf1fdab8346",
            "title": "Stock index time series prediction based on ensemble learning model",
            "abstract": "The accuracy of stock index prediction is of great significance to national economic development. However, because of the nonlinearity and long-term dependence of stock index data, effective prediction of future stock index price becomes a challenge. In order to solve the above problems, this paper proposes a research method of stock index time series prediction based on ensemble learning model. This method first uses an Adaboost.R2 algorithm to iteratively train multiple LSTM models and then integrates these LSTM models based on the parameters obtained by iterative training. Finally, it uses the ensemble model to predict stock index time series data. This paper uses the Shanghai Composite Index, CSI 300 index and Shenzhen Composite index as experimental data sets, and uses the BP model, CNN model and LSTM model as comparative models to conduct an experimental analysis. The experimental results show that the new ensemble learning model proposed in this paper has certain advantages in the research of stock index time series prediction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48120953",
                    "name": "Mei Sun"
                },
                {
                    "authorId": "2109656339",
                    "name": "Jihou Wang"
                },
                {
                    "authorId": "50444146",
                    "name": "Qingtao Li"
                },
                {
                    "authorId": "2218372978",
                    "name": "Jiaqian Zhou"
                },
                {
                    "authorId": "35099667",
                    "name": "C. Cui"
                },
                {
                    "authorId": "1783889",
                    "name": "Muwei Jian"
                }
            ]
        },
        {
            "paperId": "5481149a631313403ca64b88d0a9d9ed88cc6e82",
            "title": "Relation-Aware Facial Expression Recognition",
            "abstract": "Research on facial expression recognition has been moving from the constrained lab scenarios to the in-the-wild situations and has made progress in recent years. However, it is still very challenging to deal with facial expression in the wild due to large poses and occlusion as well as illumination and intensity variations. Generally, existing methods mainly take the whole face as a uniform source of features for facial expression analysis. Actually, physiology and psychology research shows that some crucial regions, such as the eye and mouth, reflect the differences of different facial expressions, which have close relationships with emotion expression. Inspired by this observation, a novel relation-aware facial expression recognition method called relation convolutional neural network (ReCNN) is proposed in this article, which can adaptively capture the relationship between crucial regions and facial expressions leading to the focus on the most discriminative regions for recognition. We have evaluated the proposed ReCNN on two large in-the-wild databases: 1) AffectNet and 2) RAF-DB. Extensive experiments on these databases show that our method has superior recognition accuracy compared with state-of-the-art methods and the relationship between crucial regions and facial expressions is beneficial to improve the performance of facial expression recognition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145526770",
                    "name": "Yifan Xia"
                },
                {
                    "authorId": "2118681985",
                    "name": "Hui Yu"
                },
                {
                    "authorId": "153315870",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "1783889",
                    "name": "Muwei Jian"
                },
                {
                    "authorId": "47939505",
                    "name": "Fei-Yue Wang"
                }
            ]
        },
        {
            "paperId": "5d8b32393a89a2c0999980c0ddc56b6a34d676bb",
            "title": "Face hallucination using multisource references and cross\u2010scale dual residual fusion mechanism",
            "abstract": "There is an increasing interest in enhancing the quality of low\u2010resolution (LR) facial images for various social life applications. Existing methods often use domain\u2010specific prior knowledge, which is effective in improving the face super\u2010resolution model's performance. However, it is challenging to obtain rich and accurate prior information from LR inputs in real\u2010world scenarios, which can limit the robustness and generalization ability of the developed face super\u2010resolution model. In this paper, a multisource reference\u2010based face super\u2010resolution Network, namely MSRNet, is proposed. Without considering the prior knowledge of faces, the network can reconstruct a LR face image with a magnitude factor of 8 under the guidance of multiple reference face images of different identities. By constructing an \u201cappearance\u2010alike\u201d reference data set Face_Ref, the designed MSRNet aims to fully exploit the local and spatially similar high frequency information between the distinct references and the current face. More specifically, to effectively combine the information from multiple references, a cross\u2010scale and cross\u2010space feature fusion mechanism is introduced for external and internal references, and then the enhanced local semantics are finally incorporated into the high\u2010resolution face reconstruction. The robustness of face image super\u2010resolution is increased compared to current correlation approaches, since it not only eliminates the need for face prior knowledge but also avoids performing alignment operations on reference faces with multiple expressions and different poses. Experimental results show that the proposed model is able to produce results for face super\u2010resolution that are satisfying and dependable and outperforms the state\u2010of\u2010the\u2010art methods in terms of visual perceptual quality and quantity evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2151039189",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "1783889",
                    "name": "Muwei Jian"
                },
                {
                    "authorId": "2118681612",
                    "name": "Hui Yu"
                },
                {
                    "authorId": "3265905",
                    "name": "Linshan Wang"
                },
                {
                    "authorId": "2119658606",
                    "name": "Bo Yang"
                }
            ]
        },
        {
            "paperId": "63e2c28e5e326708342d970cba6824dce996f823",
            "title": "AWANet: Attentive-Aware Wide-Kernels Asymmetrical Network with Blended Contour Information for Salient Object Detection",
            "abstract": "Although deep learning-based techniques for salient object detection have considerably improved over recent years, estimated saliency maps still exhibit imprecise predictions owing to the internal complexity and indefinite boundaries of salient objects of varying sizes. Existing methods emphasize the design of an exemplary structure to integrate multi-level features by employing multi-scale features and attention modules to filter salient regions from cluttered scenarios. We propose a saliency detection network based on three novel contributions. First, we use a dense feature extraction unit (DFEU) by introducing large kernels of asymmetric and grouped-wise convolutions with channel reshuffling. The DFEU extracts semantically enriched features with large receptive fields and reduces the gridding problem and parameter sizes for subsequent operations. Second, we suggest a cross-feature integration unit (CFIU) that extracts semantically enriched features from their high resolutions using dense short connections and sub-samples the integrated information into different attentional branches based on the inputs received for each stage of the backbone. The embedded independent attentional branches can observe the importance of the sub-regions for a salient object. With the constraint-wise growth of the sub-attentional branches at various stages, the CFIU can efficiently avoid global and local feature dilution effects by extracting semantically enriched features via dense short-connections from high and low levels. Finally, a contour-aware saliency refinement unit (CSRU) was devised by blending the contour and contextual features in a progressive dense connected fashion to assist the model toward obtaining more accurate saliency maps with precise boundaries in complex and perplexing scenarios. Our proposed model was analyzed with ResNet-50 and VGG-16 and outperforms most contemporary techniques with fewer parameters.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2144579101",
                    "name": "Inam Ullah"
                },
                {
                    "authorId": "1783889",
                    "name": "Muwei Jian"
                },
                {
                    "authorId": "51047515",
                    "name": "Kashif Shaheed"
                },
                {
                    "authorId": "1630822963",
                    "name": "Sumaira Hussain"
                },
                {
                    "authorId": "1998921670",
                    "name": "Yuling Ma"
                },
                {
                    "authorId": "2183183115",
                    "name": "Lixian Xu"
                },
                {
                    "authorId": "2195877798",
                    "name": "Khan Muhammad"
                }
            ]
        },
        {
            "paperId": "764f107b68e88900516fe6da715a6c02cd69b86f",
            "title": "Multiscale Cascaded Attention Network for Saliency Detection Based on ResNet",
            "abstract": "Saliency detection is a key research topic in the field of computer vision. Humans can be accurately and quickly mesmerized by an area of interest in complex and changing scenes through the visual perception area of the brain. Although existing saliency-detection methods can achieve competent performance, they have deficiencies such as unclear margins of salient objects and the interference of background information on the saliency map. In this study, to improve the defects during saliency detection, a multiscale cascaded attention network was designed based on ResNet34. Different from the typical U-shaped encoding\u2013decoding architecture, we devised a contextual feature extraction module to enhance the advanced semantic feature extraction. Specifically, a multiscale cascade block (MCB) and a lightweight channel attention (CA) module were added between the encoding and decoding networks for optimization. To address the blur edge issue, which is neglected by many previous approaches, we adopted the edge thinning module to carry out a deeper edge-thinning process on the output layer image. The experimental results illustrate that this method can achieve competitive saliency-detection performance, and the accuracy and recall rate are improved compared with those of other representative methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1783889",
                    "name": "Muwei Jian"
                },
                {
                    "authorId": "2192270370",
                    "name": "Haodong Jin"
                },
                {
                    "authorId": "2144227555",
                    "name": "Xiangyu Liu"
                },
                {
                    "authorId": "2131179399",
                    "name": "Linsong Zhang"
                }
            ]
        },
        {
            "paperId": "a6dde85998abcea91090f2a4218dfe32bd6f03d8",
            "title": "Face Super-resolution Based on Multi-source References",
            "abstract": "This paper proposes a multi-source references (MSR) based face super-resolution (FSR) model. More specifically, to enhance the low-quality large-scale reconstruction of faces without the involvement of face prior knowledge, we propose a multi-source references based FSR framework exploiting a constructed reference library of nonidentity faces and an information mining module for external and internal references. Experimental results show that the proposed model can provide more satisfactory and reliable face super-resolution results than the-state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2151039177",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "1783889",
                    "name": "Muwei Jian"
                },
                {
                    "authorId": "2183577986",
                    "name": "Paul Smith"
                },
                {
                    "authorId": "2118681612",
                    "name": "Hui Yu"
                }
            ]
        },
        {
            "paperId": "e9d2a7ff01752e5017efb463f9ff5a6f31039198",
            "title": "A Comprehensive Survey on Video Saliency Detection With Auditory Information: The Audio-Visual Consistency Perceptual is the Key!",
            "abstract": "Video saliency detection (VSD) aims at fast locating the most attractive objects/things/patterns in a given video clip. Existing VSD-related works have mainly relied on the visual system but paid less attention to the audio aspect. In contrast, our audio system is the most vital complementary part of our visual system. Also, audio-visual saliency detection (AVSD), one of the most representative research topics for mimicking human perceptual mechanisms, is currently in its infancy, and none of the existing survey papers have touched on it, especially from the perspective of saliency detection. Thus, the ultimate goal of this paper is to provide an extensive review to bridge the gap between audio-visual fusion and saliency detection. In addition, as another highlight of this review, we have provided a deep insight into key factors that could directly determine AVSD deep models\u2019 performances. We claim that the audio-visual consistency degree (AVC) \u2014 a long-overlooked issue, can directly influence the effectiveness of using audio to benefit its visual counterpart when performing saliency detection. Moreover, to make the AVC issue more practical and valuable for future followers, we have newly equipped almost all existing publicly available AVSD datasets with additional frame-wise AVC labels. Based on these upgraded datasets, we have conducted extensive quantitative evaluations to ground our claim on the importance of AVC in the AVSD task. In a word, our ideas and new sets serve as a convenient platform with preliminaries and guidelines, all of which can potentially facilitate future works in further promoting state-of-the-art (SOTA) performance.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "3289090",
                    "name": "Chenglizhao Chen"
                },
                {
                    "authorId": "46519745",
                    "name": "Mengke Song"
                },
                {
                    "authorId": "46895706",
                    "name": "Wenfeng Song"
                },
                {
                    "authorId": "2110765445",
                    "name": "Li Guo"
                },
                {
                    "authorId": "1783889",
                    "name": "Muwei Jian"
                }
            ]
        }
    ]
}