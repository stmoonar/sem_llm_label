{
    "authorId": "2276509067",
    "papers": [
        {
            "paperId": "011d8ff0e4a2a0a13d364117a8e10d93be456455",
            "title": "FaithLM: Towards Faithful Explanations for Large Language Models",
            "abstract": "Large Language Models (LLMs) have become proficient in addressing complex tasks by leveraging their extensive internal knowledge and reasoning capabilities. However, the black-box nature of these models complicates the task of explaining their decision-making processes. While recent advancements demonstrate the potential of leveraging LLMs to self-explain their predictions through natural language (NL) explanations, their explanations may not accurately reflect the LLMs' decision-making process due to a lack of fidelity optimization on the derived explanations. Measuring the fidelity of NL explanations is a challenging issue, as it is difficult to manipulate the input context to mask the semantics of these explanations. To this end, we introduce FaithLM to explain the decision of LLMs with NL explanations. Specifically, FaithLM designs a method for evaluating the fidelity of NL explanations by incorporating the contrary explanations to the query process. Moreover, FaithLM conducts an iterative process to improve the fidelity of derived explanations. Experiment results on three datasets from multiple domains demonstrate that FaithLM can significantly improve the fidelity of derived explanations, which also provides a better alignment with the ground-truth explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27615982",
                    "name": "Yu-Neng Chuang"
                },
                {
                    "authorId": "32780441",
                    "name": "Guanchu Wang"
                },
                {
                    "authorId": "2166879577",
                    "name": "Chia-yuan Chang"
                },
                {
                    "authorId": "2057059798",
                    "name": "Ruixiang Tang"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "3432460",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2145065759",
                    "name": "Xuanting Cai"
                },
                {
                    "authorId": "2276449057",
                    "name": "Xia Hu"
                }
            ]
        },
        {
            "paperId": "03d5db23e5047bedb873da34be7d3ee89e05187a",
            "title": "Large Language Models As Faithful Explainers",
            "abstract": "Large Language Models (LLMs) have recently become proficient in addressing complex tasks by utilizing their rich internal knowledge and reasoning ability. Consequently, this complexity hinders traditional input-focused explanation algorithms for explaining the complex decision-making processes of LLMs. Recent advancements have thus emerged for self-explaining their predictions through a single feed-forward inference in a natu-ral language format. However, natural language explanations are often criticized for lack of faith-fulness since these explanations may not accurately reflect the decision-making behaviors of the LLMs. In this work, we introduce a generative explanation framework, xLLM, to improve the faithfulness of the explanations provided in natural language formats for LLMs. Specifically, we propose an evaluator to quantify the faithfulness of natural language explanation and enhance the faithfulness by an iterative optimization process of xLLM, with the goal of maximizing the faithfulness scores. Experiments conducted on three NLU datasets demonstrate that xLLM can significantly improve the faithfulness of generated explanations, which are in alignment with the behaviors of LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27615982",
                    "name": "Yu-Neng Chuang"
                },
                {
                    "authorId": "32780441",
                    "name": "Guanchu Wang"
                },
                {
                    "authorId": "2166879577",
                    "name": "Chia-yuan Chang"
                },
                {
                    "authorId": "2057059798",
                    "name": "Ruixiang Tang"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "3432460",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2145065759",
                    "name": "Xuanting Cai"
                },
                {
                    "authorId": "2288250991",
                    "name": "Xia Hu"
                }
            ]
        },
        {
            "paperId": "44c5c804442b635a745390d7d17b1ecb5e3ca89a",
            "title": "Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era",
            "abstract": "Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency. This extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the\"black box\"to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1) how XAI can benefit LLMs and AI systems, and (2) how LLMs can contribute to the advancement of XAI. We introduce 10 strategies, introducing the key techniques for each and discussing their associated challenges. We also provide case studies to demonstrate how to obtain and leverage explanations. The code used in this paper can be found at: https://github.com/JacksonWuxs/UsableXAI_LLM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145346360",
                    "name": "Xuansheng Wu"
                },
                {
                    "authorId": "2237987232",
                    "name": "Haiyan Zhao"
                },
                {
                    "authorId": "2261804201",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "2249001715",
                    "name": "Yucheng Shi"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2291322576",
                    "name": "Tianming Liu"
                },
                {
                    "authorId": "2262445470",
                    "name": "Xiaoming Zhai"
                },
                {
                    "authorId": "2291141500",
                    "name": "Wenlin Yao"
                },
                {
                    "authorId": "2261788139",
                    "name": "Jundong Li"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2256183798",
                    "name": "Ninghao Liu"
                }
            ]
        },
        {
            "paperId": "4f60009234e76f9f8969f6cca23b3b07e944e984",
            "title": "Towards Uncovering How Large Language Model Works: An Explainability Perspective",
            "abstract": "Large language models (LLMs) have led to breakthroughs in language tasks, yet the internal mechanisms that enable their remarkable generalization and reasoning abilities remain opaque. This lack of transparency presents challenges such as hallucinations, toxicity, and misalignment with human values, hindering the safe and beneficial deployment of LLMs. This paper aims to uncover the mechanisms underlying LLM functionality through the lens of explainability. First, we review how knowledge is architecturally composed within LLMs and encoded in their internal parameters via mechanistic interpretability techniques. Then, we summarize how knowledge is embedded in LLM representations by leveraging probing techniques and representation engineering. Additionally, we investigate the training dynamics through a mechanistic perspective to explain phenomena such as grokking and memorization. Lastly, we explore how the insights gained from these explanations can enhance LLM performance through model editing, improve efficiency through pruning, and better align with human values.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237987232",
                    "name": "Haiyan Zhao"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2296792230",
                    "name": "Bo Shen"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                }
            ]
        },
        {
            "paperId": "56019756e85646883855e3583523317de465af42",
            "title": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?",
            "abstract": "Large language models (LLMs) have shown remarkable performances across a wide range of tasks. However, the mechanisms by which these models encode tasks of varying complexities remain poorly understood. In this paper, we explore the hypothesis that LLMs process concepts of varying complexities in different layers, introducing the idea of ``Concept Depth'' to suggest that more complex concepts are typically acquired in deeper layers. Specifically, we categorize concepts based on their level of abstraction, defining them in the order of increasing complexity within factual, emotional, and inferential tasks. We conduct extensive probing experiments using layer-wise representations across various LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the three domains of tasks. Our findings reveal that models could efficiently conduct probing for simpler tasks in shallow layers, and more complex tasks typically necessitate deeper layers for accurate understanding. Additionally, we examine how external factors, such as adding noise to the input and quantizing the model weights, might affect layer-wise representations. Our findings suggest that these factors can impede the development of a conceptual understanding of LLMs until deeper layers are explored. We hope that our proposed concept and experimental insights will enhance the understanding of the mechanisms underlying LLMs. Our codes are available at \\url{https://github.com/Luckfort/CD}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220539385",
                    "name": "Mingyu Jin"
                },
                {
                    "authorId": "2220796036",
                    "name": "Qinkai Yu"
                },
                {
                    "authorId": "2295904071",
                    "name": "Jingyuan Huang"
                },
                {
                    "authorId": "2153554138",
                    "name": "Qingcheng Zeng"
                },
                {
                    "authorId": "2292292249",
                    "name": "Zhenting Wang"
                },
                {
                    "authorId": "2007245028",
                    "name": "Wenyue Hua"
                },
                {
                    "authorId": "2237987232",
                    "name": "Haiyan Zhao"
                },
                {
                    "authorId": "2261740874",
                    "name": "Kai Mei"
                },
                {
                    "authorId": "2278984372",
                    "name": "Yanda Meng"
                },
                {
                    "authorId": "2295886392",
                    "name": "Kaize Ding"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2279766837",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "8a9b43946dc10f91ce8c5971a1f247fbacda7a42",
            "title": "Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability",
            "abstract": "As large language models (LLMs) grow more powerful, concerns around potential harms like toxicity, unfairness, and hallucination threaten user trust. Ensuring beneficial alignment of LLMs with human values through model alignment is thus critical yet challenging, requiring a deeper understanding of LLM behaviors and mechanisms. We propose opening the black box of LLMs through a framework of holistic interpretability encompassing complementary bottom-up and top-down perspectives. The bottom-up view, enabled by mechanistic interpretability , focuses on component functionalities and training dynamics. The top-down view utilizes representation engineering to analyze behaviors through hidden representations. In this paper, we review the landscape around mechanistic interpretability and representation engineering, summarizing approaches, discussing limitations and applications, and outlining future challenges in using these techniques to achieve ethical, honest, and reliable reasoning aligned with human values.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237987232",
                    "name": "Haiyan Zhao"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                }
            ]
        },
        {
            "paperId": "96d590faffab47ad70f007541e7196314cc048ac",
            "title": "Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models",
            "abstract": "Large Language Models (LLMs) are employed across various high-stakes domains, where the reliability of their outputs is crucial. One commonly used method to assess the reliability of LLMs' responses is uncertainty estimation, which gauges the likelihood of their answers being correct. While many studies focus on improving the accuracy of uncertainty estimations for LLMs, our research investigates the fragility of uncertainty estimation and explores potential attacks. We demonstrate that an attacker can embed a backdoor in LLMs, which, when activated by a specific trigger in the input, manipulates the model's uncertainty without affecting the final output. Specifically, the proposed backdoor attack method can alter an LLM's output probability distribution, causing the probability distribution to converge towards an attacker-predefined distribution while ensuring that the top-1 prediction remains unchanged. Our experimental results demonstrate that this attack effectively undermines the model's self-evaluation reliability in multiple-choice questions. For instance, we achieved a 100 attack success rate (ASR) across three different triggering strategies in four models. Further, we investigate whether this manipulation generalizes across different prompts and domains. This work highlights a significant threat to the reliability of LLMs and underscores the need for future defenses against such attacks. The code is available at https://github.com/qcznlp/uncertainty_attack.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2312269664",
                    "name": "Qingcheng Zeng"
                },
                {
                    "authorId": "2267333980",
                    "name": "Mingyu Jin"
                },
                {
                    "authorId": "2220796036",
                    "name": "Qinkai Yu"
                },
                {
                    "authorId": "2292292249",
                    "name": "Zhenting Wang"
                },
                {
                    "authorId": "2007245028",
                    "name": "Wenyue Hua"
                },
                {
                    "authorId": "2311646662",
                    "name": "Zihao Zhou"
                },
                {
                    "authorId": "2311626382",
                    "name": "Guangyan Sun"
                },
                {
                    "authorId": "2278984372",
                    "name": "Yanda Meng"
                },
                {
                    "authorId": "2298338546",
                    "name": "Shiqing Ma"
                },
                {
                    "authorId": "2311633436",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2304322026",
                    "name": "Felix Juefei-Xu"
                },
                {
                    "authorId": "2295886392",
                    "name": "Kaize Ding"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2057059798",
                    "name": "Ruixiang Tang"
                },
                {
                    "authorId": "2279766837",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "9a66a6b97601f207ac5e640122fce6ed407f494f",
            "title": "Quantifying Multilingual Performance of Large Language Models Across Languages",
            "abstract": "The development of Large Language Models (LLMs) relies on extensive text corpora, which are often unevenly distributed across languages. This imbalance results in LLMs performing significantly better on high-resource languages like English, German, and French, while their capabilities in low-resource languages remain inadequate. Currently, there is a lack of quantitative methods to evaluate the performance of LLMs in these low-resource languages. To address this gap, we propose the Language Ranker, an intrinsic metric designed to benchmark and rank languages based on LLM performance using internal representations. By comparing the LLM's internal representation of various languages against a baseline derived from English, we can assess the model's multilingual capabilities in a robust and language-agnostic manner. Our analysis reveals that high-resource languages exhibit higher similarity scores with English, demonstrating superior performance, while low-resource languages show lower similarity scores, underscoring the effectiveness of our metric in assessing language-specific capabilities. Besides, the experiments show that there is a strong correlation between the LLM's performance in different languages and the proportion of those languages in its pre-training corpus. These insights underscore the efficacy of the Language Ranker as a tool for evaluating LLM performance across different languages, particularly those with limited resources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275056316",
                    "name": "Zihao Li"
                },
                {
                    "authorId": "2249001715",
                    "name": "Yucheng Shi"
                },
                {
                    "authorId": "2297175601",
                    "name": "Zirui Liu"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "2257675416",
                    "name": "Ninghao Liu"
                },
                {
                    "authorId": "2237804196",
                    "name": "Mengnan Du"
                }
            ]
        },
        {
            "paperId": "0aeb564a0f4b716ed039a21792862add86d188a9",
            "title": "LETA: Learning Transferable Attribution for Generic Vision Explainer",
            "abstract": ".",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32780441",
                    "name": "Guanchu Wang"
                },
                {
                    "authorId": "27615982",
                    "name": "Yu-Neng Chuang"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "3432460",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2166879577",
                    "name": "Chia-yuan Chang"
                },
                {
                    "authorId": "2181946372",
                    "name": "Shaochen Zhong"
                },
                {
                    "authorId": "2305052067",
                    "name": "Zirui Liu"
                },
                {
                    "authorId": "2276485344",
                    "name": "Zhaozhuo Xu"
                },
                {
                    "authorId": "2298221156",
                    "name": "Kaixiong Zhou"
                },
                {
                    "authorId": "2145065759",
                    "name": "Xuanting Cai"
                },
                {
                    "authorId": "2288250991",
                    "name": "Xia Hu"
                }
            ]
        },
        {
            "paperId": "e9c30d521aaded7872c1ddcaeea1ad54c4d36d08",
            "title": "TVE: Learning Meta-attribution for Transferable Vision Explainer",
            "abstract": "Explainable machine learning significantly improves the transparency of deep neural networks. However, existing work is constrained to explaining the behavior of individual model predictions, and lacks the ability to transfer the explanation across various models and tasks. This limitation results in explaining various tasks being time- and resource-consuming. To address this problem, we introduce a Transferable Vision Explainer (TVE) that can effectively explain various vision models in downstream tasks. Specifically, the transferability of TVE is realized through a pre-training process on large-scale datasets towards learning the meta-attribution. This meta-attribution leverages the versatility of generic backbone encoders to comprehensively encode the attribution knowledge for the input instance, which enables TVE to seamlessly transfer to explain various downstream tasks, without the need for training on task-specific data. Empirical studies involve explaining three different architectures of vision models across three diverse downstream datasets. The experimental results indicate TVE is effective in explaining these tasks without the need for additional training on downstream data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32780441",
                    "name": "Guanchu Wang"
                },
                {
                    "authorId": "27615982",
                    "name": "Yu-Neng Chuang"
                },
                {
                    "authorId": "2276509067",
                    "name": "Fan Yang"
                },
                {
                    "authorId": "3432460",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "2166879577",
                    "name": "Chia-yuan Chang"
                },
                {
                    "authorId": "2181946372",
                    "name": "Shaochen Zhong"
                },
                {
                    "authorId": "47781070",
                    "name": "Zirui Liu"
                },
                {
                    "authorId": "2276485344",
                    "name": "Zhaozhuo Xu"
                },
                {
                    "authorId": "3364022",
                    "name": "Kaixiong Zhou"
                },
                {
                    "authorId": "2145065759",
                    "name": "Xuanting Cai"
                },
                {
                    "authorId": "2276449057",
                    "name": "Xia Hu"
                }
            ]
        }
    ]
}