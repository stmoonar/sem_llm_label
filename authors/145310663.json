{
    "authorId": "145310663",
    "papers": [
        {
            "paperId": "78fc4fd81f8bdec24c51d47a1dc35d2948564fff",
            "title": "Reading Broadly to Open Your Mind: Improving Open Relation Extraction With Search Documents Under Self-Supervisions",
            "abstract": "Open relation extraction is the task of extracting open-domain relation facts from natural language sentences. Existing works either utilize distant-supervised annotations to train a supervised classifier over pre-defined relations, or adopt unsupervised methods with additional dependency on external assumptions. However, these works can only obtain information signals from limited existing knowledge bases or datasets. In this work, we propose a self-supervised framework named Web-SelfORE, which exploits self-supervised signals by requiring a large pretrained language model to extensively read real-world relevant documents from the web, and obtain contextualized relational features by mixing contextualized representations of entities from different documents. We perform adaptive clustering on contextualized relational features and bootstrap the self-supervised signals by improving contextualized features in relation classification. We additionally compare the effectiveness of self-supervisions brought by different document sources, and introduce relevance and redundancy evaluation metrics to obtain higher-quality self-supervisions. Experimental results on four public datasets show the effectiveness and robustness of Web-SelfORE on open-domain relation extraction task when comparing with competitive baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109906988",
                    "name": "Xuming Hu"
                },
                {
                    "authorId": "2216513313",
                    "name": "Zhaochen Hong"
                },
                {
                    "authorId": "2418496",
                    "name": "Chenwei Zhang"
                },
                {
                    "authorId": "10017193",
                    "name": "Aiwei Liu"
                },
                {
                    "authorId": "2190693111",
                    "name": "Shiao Meng"
                },
                {
                    "authorId": "2114092431",
                    "name": "Lijie Wen"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                },
                {
                    "authorId": "144019071",
                    "name": "Philip S. Yu"
                }
            ]
        },
        {
            "paperId": "14b4aff027ccf8fde0b19ac60b8e653c621aff30",
            "title": "Practical and Efficient Model Extraction of Sentiment Analysis APIs",
            "abstract": "Despite their stunning performance, developing deep learning models from scratch is a formidable task. Therefore, it popularizes Machine-Learning-as-a-Service (MLaaS), where general users can access the trained models of MLaaS providers via Application Programming Interfaces (APIs) on a pay-per-query basis. Unfortunately, the success of MLaaS is under threat from model extraction attacks, where attackers intend to extract a local model of equivalent functionality to the target MLaaS model. However, existing studies on model extraction of text analytics APIs frequently assume adversaries have strong knowledge about the victim model, like its architecture and parameters, which hardly holds in practice. Besides, since the attacker's and the victim's training data can be considerably discrepant, it is non-trivial to perform efficient model extraction. In this paper, to advance the understanding of such attacks, we propose a framework, PEEP, for practical and efficient model extraction of sentiment analysis APIs with only query access. Specifically, PEEP features a learning-based scheme, which employs out-of-domain public corpora and a novel query strategy to construct proxy training data for model extraction. Besides, PEEP introduces a greedy search algorithm to settle an appropriate architecture for the extracted model. We conducted extensive experiments with two victim models across three datasets and two real-life commercial sentiment analysis APIs. Experimental results corroborate that PEEP can consistently outperform the state-of-the-art baselines in terms of effectiveness and efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110008050",
                    "name": "Weibin Wu"
                },
                {
                    "authorId": "2213104896",
                    "name": "Jianping Zhang"
                },
                {
                    "authorId": "119640702",
                    "name": "Victor Junqiu Wei"
                },
                {
                    "authorId": "2925954",
                    "name": "Xixian Chen"
                },
                {
                    "authorId": "144291579",
                    "name": "Zibin Zheng"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                },
                {
                    "authorId": "145609003",
                    "name": "M. Lyu"
                }
            ]
        },
        {
            "paperId": "3d849b7715e4730ff3a52ec77fb4fe0c45ef474e",
            "title": "1st Workshop on Federated Learning Technologies",
            "abstract": "AI-based systems, especially those based on machine learning technologies, have become central in modern societies. In the meanwhile, users and legislators are becoming aware of privacy issues. Users are increasingly reluctant in sharing their sensitive information, and new laws have been enacted to regulate how private data is handled (e.g., the GDPR). Federated Learning (FL) has been proposed to develop better AI systems without compromising users\u2019 privacy and the legitimate interests of private companies. Although still in its infancy, FL has already shown significant theoretical and practical results making FL one of the hottest topics in the machine learning community. Given the considerable potential in overcoming the challenges of protecting users\u2019 privacy while making the most of available data, we propose a workshop on Federated Learning Technologies (FLT) at TheWebConf 2023. The goal of this workshop is to focus the attention of the TheWebConf research community on addressing the open questions and challenges in this thriving research area. Given the broad range of competencies in the TheWebConf community, the workshop will welcome foundational contributions as well as contributions expanding the scope of these techniques, such as improvements in the interpretability and fairness of the learned models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2658614",
                    "name": "Mirko Polato"
                },
                {
                    "authorId": "2053593694",
                    "name": "Roberto Esposito"
                },
                {
                    "authorId": "1954706",
                    "name": "Walter Riviera"
                },
                {
                    "authorId": "2155432076",
                    "name": "Zenglin Xu"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                }
            ]
        },
        {
            "paperId": "c608fb120ff2de5b2ed25b02731ec092882d7cf8",
            "title": "Rethinking Machine Ethics - Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
            "abstract": "Making moral judgments is an essential step toward developing ethical AI systems. Prevalent approaches are mostly implemented in a bottom-up manner, which uses a large set of annotated data to train models based on crowd-sourced opinions about morality. These approaches have been criticized for overgeneralizing the moral stances of a limited group of annotators and lacking explainability. This work proposes a flexible top-down framework to steer (Large) Language Models (LMs) to perform moral reasoning with well-established moral theories from interdisciplinary research. The theory-guided top-down framework can incorporate various moral theories. Our experiments demonstrate the effectiveness of the proposed framework on datasets derived from moral theories. Furthermore, we show the alignment between different moral theories and existing morality datasets. Our analysis exhibits the potential and flaws in existing resources (models and datasets) in developing explainable moral judgment-making systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30887444",
                    "name": "Jingyan Zhou"
                },
                {
                    "authorId": "2100517979",
                    "name": "Minda Hu"
                },
                {
                    "authorId": "2204171275",
                    "name": "Junan Li"
                },
                {
                    "authorId": "2155167734",
                    "name": "Xiaoying Zhang"
                },
                {
                    "authorId": "1847260",
                    "name": "Xixin Wu"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                },
                {
                    "authorId": "1702243",
                    "name": "Helen M. Meng"
                }
            ]
        },
        {
            "paperId": "011721dbe63b99818dc1c72f52258fb61d2f1c72",
            "title": "An Effective Post-training Embedding Binarization Approach for Fast Online Top-K Passage Matching",
            "abstract": "With the rapid development of Natural Language Understanding for information retrieval, fine-tuned deep language models, e.g., BERT-based, perform remarkably effective in passage searching tasks. To lower the architecture complexity, the recent state-of-the-art model ColBERT employs Contextualized Late Interaction paradigm to independently learn fine-grained query-passage representations. Apart from the architecture simplification, embedding binarization, as another promising branch in model compression, further specializes in the reduction of memory and computation overheads. In this concise paper, we propose an effective post-training embedding binarization approach over ColBERT, achieving both architecture-level and embedding-level optimization for online inference. The empirical results demonstrate the efficaciousness of our proposed approach, empowering it to perform online query-passage matching acceleration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2681738",
                    "name": "Yankai Chen"
                },
                {
                    "authorId": "2108464021",
                    "name": "Yifei Zhang"
                },
                {
                    "authorId": "3339005",
                    "name": "Huifeng Guo"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                }
            ]
        },
        {
            "paperId": "10bde819433f3c25f4a8f6c7b25f6796a2aa1b97",
            "title": "Exploiting Inactive Examples for Natural Language Generation With Data Rejuvenation",
            "abstract": "Recent years have witnessed the success of natural language generation (NLG) accomplished by deep neural networks, which require a large amount of training data for optimization. With the constant increase of data scale, the complex patterns and potential noises make training NLG models difficult. In order to fully utilize large-scale training data, we explore inactive examples in the training data and propose to rejuvenate the inactive examples for improving the performance of NLG models. Specifically, we define inactive examples as those sentence pairs that contribute less to the performance of NLG models, and show that their existence is independent of model variants but mainly determined by the data distribution. We further introduce data rejuvenation to improve the training of NLG models by re-labeling the inactive examples. The rejuvenated examples and active examples are combined to train a final NLG model. We evaluate our approach by experiments on machine translation (MT) and text summarization (TS) tasks, and achieve significant improvements of performance. Extensive analyses reveal that inactive examples are more difficult to learn than active ones and rejuvenation can reduce the learning difficulty, which stabilizes and accelerates the training process of NLG models and results in models with better generalization capability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12386833",
                    "name": "Wenxiang Jiao"
                },
                {
                    "authorId": "48631170",
                    "name": "Xing Wang"
                },
                {
                    "authorId": "3470504",
                    "name": "Shilin He"
                },
                {
                    "authorId": "2909321",
                    "name": "Zhaopeng Tu"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                },
                {
                    "authorId": "145609003",
                    "name": "M. Lyu"
                }
            ]
        },
        {
            "paperId": "16acc0a0f436e990891ce8106216c7e0db50e432",
            "title": "Gradient Imitation Reinforcement Learning for General Low-Resource Information Extraction",
            "abstract": "Information Extraction (IE) aims to extract structured information from heterogeneous sources. IE from natural language texts include sub-tasks such as Named Entity Recognition (NER), Relation Extraction (RE), and Event Extraction (EE). Most IE systems require comprehensive understandings of sentence structure, implied semantics, and domain knowledge to perform well; thus, IE tasks always need adequate external resources and annotations. However, it takes time and effort to obtain more human annotations. Low-Resource Information Extraction (LRIE) strives to use unsupervised data, reducing the required resources and human annotation. In practice, existing systems either utilize self-training schemes to generate pseudo labels that will cause the gradual drift problem, or leverage consistency regularization methods which inevitably possess confirmation bias. To alleviate confirmation bias due to the lack of feedback loops in existing LRIE learning paradigms, we develop a Gradient Imitation Reinforcement Learning (GIRL) method to encourage pseudo-labeled data to imitate the gradient descent direction on labeled data, which can force pseudo-labeled data to achieve better optimization capabilities similar to labeled data. Based on how well the pseudo-labeled data imitates the instructive gradient descent direction obtained from labeled data, we design a reward to quantify the imitation process and bootstrap the optimization capability of pseudo-labeled data through trial and error. In addition to learning paradigms, GIRL is not limited to specific sub-tasks, and we leverage GIRL to solve all IE sub-tasks (named entity recognition, relation extraction, and event extraction) in low-resource settings (semi-supervised IE and few-shot IE).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109906988",
                    "name": "Xuming Hu"
                },
                {
                    "authorId": "2190693111",
                    "name": "Shiao Meng"
                },
                {
                    "authorId": "2418496",
                    "name": "Chenwei Zhang"
                },
                {
                    "authorId": "2046911289",
                    "name": "Xiangli Yang"
                },
                {
                    "authorId": "2114092431",
                    "name": "Lijie Wen"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                },
                {
                    "authorId": "152297693",
                    "name": "Philip S. Yu"
                }
            ]
        },
        {
            "paperId": "1c62647bb8971105c77c1d642991cb1b92a52214",
            "title": "Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training",
            "abstract": "Keyphrase generation is the task of automatically predicting keyphrases given a piece of long text. Despite its recent flourishing, keyphrase generation on non-English languages haven't been vastly investigated. In this paper, we call attention to a new setting named multilingual keyphrase generation and we contribute two new datasets, EcommerceMKP and AcademicMKP, covering six languages. Technically, we propose a retrieval-augmented method for multilingual keyphrase generation to mitigate the data shortage problem in non-English languages. The retrieval-augmented model leverages keyphrase annotations in English datasets to facilitate generating keyphrases in low-resource languages. Given a non-English passage, a cross-lingual dense passage retrieval module finds relevant English passages. Then the associated English keyphrases serve as external knowledge for keyphrase generation in the current language. Moreover, we develop a retriever-generator iterative training algorithm to mine pseudo parallel passage pairs to strengthen the cross-lingual passage retriever. Comprehensive experiments and ablations show that the proposed approach outperforms all baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1921742",
                    "name": "Yifan Gao"
                },
                {
                    "authorId": "3393799",
                    "name": "Qingyu Yin"
                },
                {
                    "authorId": "2146249169",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "1400438808",
                    "name": "Rui Meng"
                },
                {
                    "authorId": "2088210968",
                    "name": "Tong Zhao"
                },
                {
                    "authorId": "2021632793",
                    "name": "Bing Yin"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                },
                {
                    "authorId": "145609003",
                    "name": "M. Lyu"
                }
            ]
        },
        {
            "paperId": "326d8afbea3104bd14d973f4572db34473845659",
            "title": "Hyperbolic Graph Neural Networks: A Review of Methods and Applications",
            "abstract": "Graph neural networks generalize conventional neural networks to graph-structured data and have received widespread attention due to their impressive representation ability. In spite of the remarkable achievements, the performance of Euclidean models in graph-related learning is still bounded and limited by the representation ability of Euclidean geometry, especially for datasets with highly non-Euclidean latent anatomy. Recently, hyperbolic space has gained increasing popularity in processing graph data with tree-like structure and power-law distribution, owing to its exponential growth property. In this survey, we comprehensively revisit the technical details of the current hyperbolic graph neural networks, unifying them into a general framework and summarizing the variants of each component. More importantly, we present various HGNN-related applications. Last, we also identify several challenges, which potentially serve as guidelines for further flourishing the achievements of graph learning in hyperbolic spaces.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111077098",
                    "name": "Menglin Yang"
                },
                {
                    "authorId": "2152175543",
                    "name": "Min Zhou"
                },
                {
                    "authorId": "2116263983",
                    "name": "Zhihao Li"
                },
                {
                    "authorId": "2144131350",
                    "name": "Jiahong Liu"
                },
                {
                    "authorId": "2288532",
                    "name": "Lujia Pan"
                },
                {
                    "authorId": "2054473562",
                    "name": "Hui Xiong"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                }
            ]
        },
        {
            "paperId": "3d2287fffbfe625c7b0c79508c3135d7bd570029",
            "title": "ConcreteGraph: A Data Augmentation Method Leveraging the Properties of Concept Relatedness Estimation",
            "abstract": "The concept relatedness estimation (CRE) task is to determine whether two given concepts are related. Although existing methods for the semantic textual similarity (STS) task can be easily adapted to this task, the CRE task has some unique properties that can be lever-aged to augment the datasets for addressing its data scarcity problem. In this paper, we con-struct a graph named ConcreteGraph ( Conc ept re la t edness e stimation Graph) to take advantage of the CRE properties. For the sampled new concept pairs from the ConcreteGraph, we add an additional step of \ufb01ltering out the new concept pairs with low quality based on simple yet effective quality thresholding. We apply the ConcreteGraph data augmentation on three Transformer-based models to show its ef\ufb01cacy. Detailed ablation study for quality thresholding further shows that even a limited amount of high-quality data is more bene\ufb01cial than a large quantity of un-thresholded data. This paper is the \ufb01rst one to work on the WORD dataset and the pro-posed ConcreteGraph can boost the accuracy of the Transformers by more than 2%. All three Transformers, with the help of ConcreteGraph, can outperform the current state-of-the-art method, Concept Interaction Graph (CIG), on the CNSE and CNSS datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Yueen Ma"
                },
                {
                    "authorId": "2114791973",
                    "name": "Zixing Song"
                },
                {
                    "authorId": "2143611133",
                    "name": "Chi-Chao Chang"
                },
                {
                    "authorId": null,
                    "name": "Yue Yu"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                }
            ]
        }
    ]
}