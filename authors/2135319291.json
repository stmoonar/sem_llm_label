{
    "authorId": "2135319291",
    "papers": [
        {
            "paperId": "18cd294d4dff873a9bc22ae665cb95adecab8786",
            "title": "Compressed Interaction Graph based Framework for Multi-behavior Recommendation",
            "abstract": "Multi-types of user behavior data (e.g., clicking, adding to cart, and purchasing) are recorded in most real-world recommendation scenarios, which can help to learn users\u2019 multi-faceted preferences. However, it is challenging to explore multi-behavior data due to the unbalanced data distribution and sparse target behavior, which lead to the inadequate modeling of high-order relations when treating multi-behavior data \u201cas features\u201d and gradient conflict in multi-task learning when treating multi-behavior data \u201cas labels\u201d. In this paper, we propose CIGF, a Compressed Interaction Graph based Framework, to overcome the above limitations. Specifically, we design a novel Compressed Interaction Graph Convolution Network (CIGCN) to model instance-level high-order relations explicitly. To alleviate the potential gradient conflict when treating multi-behavior data \u201cas labels\u201d, we propose a Multi-Expert with Separate Input (MESI) network with separate input on the top of CIGCN for multi-task learning. Comprehensive experiments on three large-scale real-world datasets demonstrate the superiority of CIGF.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109155646",
                    "name": "Wei Guo"
                },
                {
                    "authorId": "2180167214",
                    "name": "Chang Meng"
                },
                {
                    "authorId": "1601384914",
                    "name": "Enming Yuan"
                },
                {
                    "authorId": "2175453540",
                    "name": "Zhicheng He"
                },
                {
                    "authorId": "3339005",
                    "name": "Huifeng Guo"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "92633145",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2911895",
                    "name": "Yaochen Hu"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2202596977",
                    "name": "Xiufeng Li"
                },
                {
                    "authorId": "144142354",
                    "name": "Rui Zhang"
                }
            ]
        },
        {
            "paperId": "1e9b3d236f15b27856f5551ad23965fac84ff6e5",
            "title": "Hierarchical Projection Enhanced Multi-behavior Recommendation",
            "abstract": "Various types of user behaviors are recorded in most real-world recommendation scenarios. To fully utilize the multi-behavior information, the exploration of multiplex interaction among them is essential. Many multi-task learning based multi-behavior methods are proposed recently to use multiple types of supervision signals and perform information transfer among them. Despite the great successes, these methods fail to design prediction tasks comprehensively, leading to insufficient utilization of multi-behavior correlative information. Besides, these methods are either based on the weighting of expert information extracted from the coupled input or modeling of information transfer between multiple behavior levels through task-specific extractors, which are usually accompanied by negative transfer phenomenon1. To address the above problems, we propose a multi-behavior recommendation framework, called Hierarchical Projection Enhanced Multi-behavior Recommendation (HPMR). The key module, Projection-based Transfer Network (PTN), uses the projection mechanism to \"explicitly\" model the correlations of upstream and downstream behaviors, refines the upstream behavior representations, and fully uses the refined representations to enhance the learning of downstream tasks. Offline experiments on public and industrial datasets and online A/B test further verify the effectiveness of HPMR in modeling the associations from upstream to downstream and alleviating the negative transfer. The source code and datasets are available at https://github.com/MC-CV/HPMR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2180167214",
                    "name": "Chang Meng"
                },
                {
                    "authorId": "2187872267",
                    "name": "Hengyu Zhang"
                },
                {
                    "authorId": "2109155646",
                    "name": "Wei Guo"
                },
                {
                    "authorId": "3339005",
                    "name": "Huifeng Guo"
                },
                {
                    "authorId": null,
                    "name": "Haotian Liu"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2181501520",
                    "name": "Hongkun Zheng"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2164225707",
                    "name": "Xiu Li"
                },
                {
                    "authorId": "144142354",
                    "name": "Rui Zhang"
                }
            ]
        },
        {
            "paperId": "26ebeeb1b9172df34ad21f1000bb6f3c374a222e",
            "title": "Structure Aware Incremental Learning with Personalized Imitation Weights for Recommender Systems",
            "abstract": "Recommender systems now consume large-scale data and play a significant role in improving user experience. Graph Neural Networks (GNNs) have emerged as one of the most effective recommender system models because they model the rich relational information. The ever-growing volume of data can make training GNNs prohibitively expensive. To address this, previous attempts propose to train the GNN models incrementally as new data blocks arrive. \nFeature and structure knowledge distillation techniques have been explored to allow the GNN model to train in a fast incremental fashion while alleviating the catastrophic forgetting problem. \nHowever, preserving the same amount of the historical information for all users is sub-optimal since it fails to take into account the dynamics of each user's change of preferences. \nFor the users whose interests shift substantially, retaining too much of the old knowledge can overly constrain the model, preventing it from quickly adapting to the users\u2019 novel interests. \nIn contrast, for users who have static preferences, model performance can benefit greatly from preserving as much of the user's long-term preferences as possible.\nIn this work, we propose a novel training strategy that adaptively learns personalized imitation weights for each user to balance the contribution from the recent data and the amount of knowledge to be distilled from previous time periods.\nWe demonstrate the effectiveness of learning imitation weights via a comparison on five diverse datasets for three state-of-art structure distillation based recommender systems. The performance shows consistent improvement over competitive incremental learning techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108711613",
                    "name": "Yuening Wang"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "1419478649",
                    "name": "Antonios Valkanas"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "74142381",
                    "name": "Chen Ma"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "58810ee6ea9f9d40816419a2f0ec3ed3c45a8458",
            "title": "Spectral Augmentations for Graph Contrastive Learning",
            "abstract": "Contrastive learning has emerged as a premier method for learning representations with or without supervision. Recent studies have shown its utility in graph representation learning for pre-training. Despite successes, the understanding of how to design effective graph augmentations that can capture structural properties common to many different types of downstream graphs remains incomplete. We propose a set of well-motivated graph transformation operations derived via graph spectral analysis to provide a bank of candidates when constructing augmentations for a graph contrastive objective, enabling contrastive learning to capture useful structural representation from pre-training graph datasets. We first present a spectral graph cropping augmentation that involves filtering nodes by applying thresholds to the eigenvalues of the leading Laplacian eigenvectors. Our second novel augmentation reorders the graph frequency components in a structural Laplacian-derived position graph embedding. Further, we introduce a method that leads to improved views of local subgraphs by performing alignment via global random walk embeddings. Our experimental results indicate consistent improvements in out-of-domain graph data transfer compared to state-of-the-art graph contrastive learning methods, shedding light on how to design a graph learner that is able to learn structural properties common to diverse graph types.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "81407699",
                    "name": "Amur Ghose"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "5fb82c52898e1721489a8e2fd8ff64ad6434ae72",
            "title": "A Survey on User Behavior Modeling in Recommender Systems",
            "abstract": "User Behavior Modeling (UBM) plays a critical role in user interest learning, which has been extensively used in recommender systems. Crucial interactive patterns between users and items have been exploited, which brings compelling improvements in many recommendation tasks. In this paper, we attempt to provide a thorough survey of this research topic. We start by reviewing the research background of UBM. Then, we provide a systematic taxonomy of existing UBM research works, which can be categorized into four different directions including Conventional UBM, Long-Sequence UBM, Multi-Type UBM, and UBM with Side Information. Within each direction, representative models and their strengths and weaknesses are comprehensively discussed. Besides, we elaborate on the industrial practices of UBM methods with the hope of providing insights into the application value of existing UBM solutions. Finally, we summarize the survey and discuss the future prospects of this field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2175453540",
                    "name": "Zhicheng He"
                },
                {
                    "authorId": "2130051800",
                    "name": "Weiwen Liu"
                },
                {
                    "authorId": "2109155646",
                    "name": "Wei Guo"
                },
                {
                    "authorId": "79494403",
                    "name": "Jiarui Qin"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2911895",
                    "name": "Yaochen Hu"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "6f9f52845308d2f57be8032965fe3c20b82ba6b3",
            "title": "Dynamic Embedding Size Search with Minimum Regret for Streaming Recommender System",
            "abstract": "With the continuous increase of users and items, conventional recommender systems trained on static datasets can hardly adapt to changing environments. The high-throughput data requires the model to be updated in a timely manner for capturing the user interest dynamics, which leads to the emergence of streaming recommender systems. Due to the prevalence of deep learning-based recommender systems, the embedding layer is widely adopted to represent the characteristics of users, items, and other features in low-dimensional vectors. However, it has been proved that setting an identical and static embedding size is sub-optimal in terms of recommendation performance and memory cost, especially for streaming recommendations. To tackle this problem, we first rethink the streaming model update process and model the dynamic embedding size search as a bandit problem. Then, we analyze and quantify the factors that influence the optimal embedding sizes from the statistics perspective. Based on this, we propose the Dynamic Embedding Size Search (DESS) method to minimize the embedding size selection regret on both user and item sides in a non-stationary manner. Theoretically, we obtain a sublinear regret upper bound superior to previous methods. Empirical results across two recommendation tasks on four public datasets also demonstrate that our approach can achieve better streaming recommendation performance with lower memory cost and higher time efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "13358736",
                    "name": "Bowei He"
                },
                {
                    "authorId": "1510737168",
                    "name": "Xu He"
                },
                {
                    "authorId": null,
                    "name": "Renrui Zhang"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "74142381",
                    "name": "Chen Ma"
                }
            ]
        },
        {
            "paperId": "9901373674241a909f603ac08fa889dfc86a4d22",
            "title": "Bidirectional Learning for Offline Model-based Biological Sequence Design",
            "abstract": "Offline model-based optimization aims to maximize a black-box objective function with a static dataset of designs and their scores. In this paper, we focus on biological sequence design to maximize some sequence score. A recent approach employs bidirectional learning, combining a forward mapping for exploitation and a backward mapping for constraint, and it relies on the neural tangent kernel (NTK) of an infinitely wide network to build a proxy model. Though effective, the NTK cannot learn features because of its parametrization, and its use prevents the incorporation of powerful pre-trained Language Models (LMs) that can capture the rich biophysical information in millions of biological sequences. We adopt an alternative proxy model, adding a linear head to a pre-trained LM, and propose a linearization scheme. This yields a closed-form loss and also takes into account the biophysical information in the pre-trained LM. In addition, the forward mapping and the backward mapping play different roles and thus deserve different weights during sequence optimization. To achieve this, we train an auxiliary model and leverage its weak supervision signal via a bi-level optimization framework to effectively learn how to balance the two mappings. Further, by extending the framework, we develop the first learning rate adaptation module \\textit{Adaptive}-$\\eta$, which is compatible with all gradient-based algorithms for offline model-based optimization. Experimental results on DNA/protein sequence design tasks verify the effectiveness of our algorithm. Our code is available~\\href{https://anonymous.4open.science/r/BIB-ICLR2023-Submission/README.md}{here.}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109063238",
                    "name": "Can Chen"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2151060489",
                    "name": "Xue Liu"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "a29b6af13601d6cf439c951a99808e2ba209ced6",
            "title": "Towards Automated Negative Sampling in Implicit Recommendation",
            "abstract": "Negative sampling methods are vital in implicit recommendation models as they allow us to obtain negative instances from massive unlabeled data. Most existing approaches focus on sampling hard negative samples in various ways. These studies are orthogonal to the recommendation model and implicit datasets. However, such an idea contradicts the common belief in AutoML that the model and dataset should be matched. Empirical experiments suggest that the best-performing negative sampler depends on the implicit dataset and the specific recommendation model. Hence, we propose a hypothesis that the negative sampler should align with the capacity of the recommendation models as well as the statistics of the datasets to achieve optimal performance. A mismatch between these three would result in sub-optimal outcomes. An intuitive idea to address the mismatch problem is to exhaustively select the best-performing negative sampler given the model and dataset. However, such an approach is computationally expensive and time-consuming, leaving the problem unsolved. In this work, we propose the AutoSample framework that adaptively selects the best-performing negative sampler among candidates. Specifically, we propose a loss-to-instance approximation to transform the negative sampler search task into the learning task over a weighted sum, enabling end-to-end training of the model. We also designed an adaptive search algorithm to extensively and efficiently explore the search space. A specific initialization approach is also obtained to better utilize the obtained model parameters during the search stage, which is similar to curriculum learning and leads to better performance and less computation resource consumption. We evaluate the proposed framework on four benchmarks over three models. Extensive experiments demonstrate the effectiveness and efficiency of our proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1704274486",
                    "name": "Fuyuan Lyu"
                },
                {
                    "authorId": "2911895",
                    "name": "Yaochen Hu"
                },
                {
                    "authorId": "2274302690",
                    "name": "Xing Tang"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2265583881",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2188246843",
                    "name": "Xue Liu"
                }
            ]
        },
        {
            "paperId": "e45401f1c1cca2127bd8d96f8ed773e6208bfe40",
            "title": "Dynamically Expandable Graph Convolution for Streaming Recommendation",
            "abstract": "Personalized recommender systems have been widely studied and deployed to reduce information overload and satisfy users\u2019 diverse needs. However, conventional recommendation models solely conduct a one-time training-test fashion and can hardly adapt to evolving demands, considering user preference shifts and ever-increasing users and items in the real world. To tackle such challenges, the streaming recommendation is proposed and has attracted great attention recently. Among these, continual graph learning is widely regarded as a promising approach for the streaming recommendation by academia and industry. However, existing methods either rely on the historical data replay which is often not practical under increasingly strict data regulations, or can seldom solve the over-stability issue. To overcome these difficulties, we propose a novel Dynamically Expandable Graph Convolution (DEGC) algorithm from a model isolation perspective for the streaming recommendation which is orthogonal to previous methods. Based on the motivation of disentangling outdated short-term preferences from useful long-term preferences, we design a sequence of operations including graph convolution pruning, refining, and expanding to only preserve beneficial long-term preference-related parameters and extract fresh short-term preferences. Moreover, we model the temporal user preference, which is utilized as user embedding initialization, for better capturing the individual-level preference shifts. Extensive experiments on the three most representative GCN-based recommendation models and four industrial datasets demonstrate the effectiveness and robustness of our method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "13358736",
                    "name": "Bowei He"
                },
                {
                    "authorId": "1510737168",
                    "name": "Xu He"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "74142381",
                    "name": "Chen Ma"
                }
            ]
        },
        {
            "paperId": "08ac9fc139aed960fdfed1093cf5e7931c9c98a1",
            "title": "Intent-aware Multi-source Contrastive Alignment for Tag-enhanced Recommendation",
            "abstract": "To offer accurate and diverse recommendation services, recent methods use auxiliary information to foster the learning process of user and item representations. Many state-of-the-art (SOTA) methods fuse different sources of information (user, item, knowledge graph, tags, etc.) into a graph and use Graph Neural Networks (GNNs) to introduce the auxiliary information through the message passing paradigm. In this work, we seek an alternative framework that is light and effective through self-supervised learning across different sources of information, particularly for the commonly accessible item tag information. We use a self-supervision signal to pair users with the auxiliary information (tags) associated with the items they have interacted with before. To achieve the pairing, we create a proxy training task. For a given item, the model predicts which is the correct pairing between the representations obtained from the users that have interacted with this item and the tags assigned to it. This design provides an efficient solution, using the auxiliary information directly to enhance the quality of user and item embeddings. User behavior in recommendation systems is driven by the complex interactions of many factors behind the users\u2019 decision-making processes. To make the pairing process more fine-grained and avoid embedding collapse, we propose a user intent-aware self-supervised pairing process where we split the user embeddings into multiple sub-embedding vectors. Each sub-embedding vector captures a specific user intent via self-supervised alignment with a particular cluster of tags. We integrate our designed framework with various recommendation models, demonstrating its flexibility and compatibility. Through comparison with numerous SOTA methods on seven real-world datasets, we show that our method can achieve better performance while requiring less training time. This indicates the potential of applying our approach on web-scale datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "107747459",
                    "name": "Haolun Wu"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "74142381",
                    "name": "Chen Ma"
                },
                {
                    "authorId": "2109155646",
                    "name": "Wei Guo"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2151058626",
                    "name": "Xue Liu"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        }
    ]
}