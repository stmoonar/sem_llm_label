{
    "authorId": "2256981456",
    "papers": [
        {
            "paperId": "34b8efb22128779c29627a711c12240422f8a658",
            "title": "On the Hardness of Faithful Chain-of-Thought Reasoning in Large Language Models",
            "abstract": "As Large Language Models (LLMs) are increasingly being employed in real-world applications in critical domains such as healthcare, it is important to ensure that the Chain-of-Thought (CoT) reasoning generated by these models faithfully captures their underlying behavior. While LLMs are known to generate CoT reasoning that is appealing to humans, prior studies have shown that these explanations do not accurately reflect the actual behavior of the underlying LLMs. In this work, we explore the promise of three broad approaches commonly employed to steer the behavior of LLMs to enhance the faithfulness of the CoT reasoning generated by LLMs: in-context learning, fine-tuning, and activation editing. Specifically, we introduce novel strategies for in-context learning, fine-tuning, and activation editing aimed at improving the faithfulness of the CoT reasoning. We then carry out extensive empirical analyses with multiple benchmark datasets to explore the promise of these strategies. Our analyses indicate that these strategies offer limited success in improving the faithfulness of the CoT reasoning, with only slight performance enhancements in controlled scenarios. Activation editing demonstrated minimal success, while fine-tuning and in-context learning achieved marginal improvements that failed to generalize across diverse reasoning and truthful question-answering benchmarks. In summary, our work underscores the inherent difficulty in eliciting faithful CoT reasoning from LLMs, suggesting that the current array of approaches may not be sufficient to address this complex challenge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219860381",
                    "name": "Sree Harsha Tanneru"
                },
                {
                    "authorId": "2256981456",
                    "name": "Dan Ley"
                },
                {
                    "authorId": "40228633",
                    "name": "Chirag Agarwal"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "70017c9096cca317942156e3f91f8141da42eef1",
            "title": "In-Context Explainers: Harnessing LLMs for Explaining Black Box Models",
            "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated exceptional capabilities in complex tasks like machine translation, commonsense reasoning, and language understanding. One of the primary reasons for the adaptability of LLMs in such diverse tasks is their in-context learning (ICL) capability, which allows them to perform well on new tasks by simply using a few task samples in the prompt. Despite their effectiveness in enhancing the performance of LLMs on diverse language and tabular tasks, these methods have not been thoroughly explored for their potential to generate post hoc explanations. In this work, we carry out one of the first explorations to analyze the effectiveness of LLMs in explaining other complex predictive models using ICL. To this end, we propose a novel framework, In-Context Explainers, comprising of three novel approaches that exploit the ICL capabilities of LLMs to explain the predictions made by other predictive models. We conduct extensive analysis with these approaches on real-world tabular and text datasets and demonstrate that LLMs are capable of explaining other predictive models similar to state-of-the-art post hoc explainers, opening up promising avenues for future research into LLM-based post hoc explanations of complex predictive models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256987587",
                    "name": "Nicholas Kroeger"
                },
                {
                    "authorId": "2256981456",
                    "name": "Dan Ley"
                },
                {
                    "authorId": "1387484410",
                    "name": "Satyapriya Krishna"
                },
                {
                    "authorId": "40228633",
                    "name": "Chirag Agarwal"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        }
    ]
}