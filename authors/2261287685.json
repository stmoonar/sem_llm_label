{
    "authorId": "2261287685",
    "papers": [
        {
            "paperId": "098be01c95b4c18e2c7e8b4164d29dbb0903e71f",
            "title": "Can a Multichoice Dataset be Repurposed for Extractive Question Answering?",
            "abstract": "The rapid evolution of Natural Language Processing (NLP) has favored major languages such as English, leaving a significant gap for many others due to limited resources. This is especially evident in the context of data annotation, a task whose importance cannot be underestimated, but which is time-consuming and costly. Thus, any dataset for resource-poor languages is precious, in particular when it is task-specific. Here, we explore the feasibility of repurposing existing datasets for a new NLP task: we repurposed the Belebele dataset (Bandarkar et al., 2023), which was designed for multiple-choice question answering (MCQA), to enable extractive QA (EQA) in the style of machine reading comprehension. We present annotation guidelines and a parallel EQA dataset for English and Modern Standard Arabic (MSA). We also present QA evaluation results for several monolingual and cross-lingual QA pairs including English, MSA, and five Arabic dialects. Our aim is to enable others to adapt our approach for the 120+ other language variants in Belebele, many of which are deemed under-resourced. We also conduct a thorough analysis and share our insights from the process, which we hope will contribute to a deeper understanding of the challenges and the opportunities associated with task reformulation in NLP research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298756162",
                    "name": "Teresa Lynn"
                },
                {
                    "authorId": "51935928",
                    "name": "Malik H. Altakrori"
                },
                {
                    "authorId": "148087360",
                    "name": "S. Magdy"
                },
                {
                    "authorId": "2211732585",
                    "name": "Rocktim Jyoti Das"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2056258384",
                    "name": "Mohamed Nasr"
                },
                {
                    "authorId": "2282523149",
                    "name": "Younes Samih"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2111356",
                    "name": "S. Godbole"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                }
            ]
        },
        {
            "paperId": "135f9cf37474485f87cee7b72e544766565014be",
            "title": "A Grounded Preference Model for LLM Alignment",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138053379",
                    "name": "Tahira Naseem"
                },
                {
                    "authorId": "2282677923",
                    "name": "Guangxuan Xu"
                },
                {
                    "authorId": "8769152",
                    "name": "Sarathkrishna Swaminathan"
                },
                {
                    "authorId": "2126416248",
                    "name": "Asaf Yehudai"
                },
                {
                    "authorId": "2261672385",
                    "name": "Subhajit Chaudhury"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "2270425544",
                    "name": "R. Astudillo"
                },
                {
                    "authorId": "2273469018",
                    "name": "Asim Munawar"
                }
            ]
        },
        {
            "paperId": "2355cd6d7415c23e2b97d6835f45e1c00bbd43b0",
            "title": "CHRONOS: A Schema-Based Event Understanding and Prediction System",
            "abstract": "Chronological and Hierarchical Reasoning Over Naturally Occurring Schemas (CHRONOS) is a system that combines language model-based natural language processing with symbolic knowledge representations to analyze and make predictions about newsworthy events. CHRONOS consists of an event-centric information extraction pipeline and a complex event schema instantiation and prediction system. Resulting predictions are detailed with arguments, event types from Wikidata, schema-based justifications, and source document provenance. We evaluate our system by its ability to capture the structure of unseen events described in news articles and make plausible predictions as judged by human annotators.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293665108",
                    "name": "Maria Chang"
                },
                {
                    "authorId": "2297836",
                    "name": "Achille Fokoue"
                },
                {
                    "authorId": "2291963641",
                    "name": "Rosario Uceda-Sosa"
                },
                {
                    "authorId": "112884386",
                    "name": "Parul Awasthy"
                },
                {
                    "authorId": "2265758926",
                    "name": "Ken Barker"
                },
                {
                    "authorId": "1666248581",
                    "name": "Sadhana Kumaravel"
                },
                {
                    "authorId": "1728091",
                    "name": "Oktie Hassanzadeh"
                },
                {
                    "authorId": "2293521529",
                    "name": "Elton Soares"
                },
                {
                    "authorId": "2293430358",
                    "name": "Tian Gao"
                },
                {
                    "authorId": "3237019",
                    "name": "D. Bhattacharjya"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                }
            ]
        },
        {
            "paperId": "557336b26953979acc7031e2390a35ef055a3c5f",
            "title": "Multi-Document Grounded Multi-Turn Synthetic Dialog Generation",
            "abstract": "We introduce a technique for multi-document grounded multi-turn synthetic dialog generation that incorporates three main ideas. First, we control the overall dialog flow using taxonomy-driven user queries that are generated with Chain-of-Thought (CoT) prompting. Second, we support the generation of multi-document grounded dialogs by mimicking real-world use of retrievers to update the grounding documents after every user-turn in the dialog. Third, we apply LLM-as-a-Judge to filter out queries with incorrect answers. Human evaluation of the synthetic dialog data suggests that the data is diverse, coherent, and includes mostly correct answers. Both human and automatic evaluations of answerable queries indicate that models fine-tuned on synthetic dialogs consistently out-perform those fine-tuned on existing human generated training data across four publicly available multi-turn document grounded benchmark test sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2321679661",
                    "name": "Young-Suk Lee"
                },
                {
                    "authorId": "66161659",
                    "name": "Chulaka Gunasekara"
                },
                {
                    "authorId": "2283769527",
                    "name": "Danish Contractor"
                },
                {
                    "authorId": "2270425544",
                    "name": "R. Astudillo"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                }
            ]
        },
        {
            "paperId": "62390c0002c6de5e9252e12e2eec2e78ebc1c3d4",
            "title": "CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems",
            "abstract": "Retrieval Augmented Generation (RAG) has become a popular application for large language models. It is preferable that successful RAG systems provide accurate answers that are supported by being grounded in a passage without any hallucinations. While considerable work is required for building a full RAG pipeline, being able to benchmark performance is also necessary. We present ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG pipeline. ClapNQ includes long answers with grounded gold passages from Natural Questions (NQ) and a corpus to perform either retrieval, generation, or the full RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full passage, and cohesive, with multiple pieces of the passage that are not contiguous. RAG models must adapt to these properties to be successful at ClapNQ. We present baseline experiments and analysis for ClapNQ that highlight areas where there is still significant room for improvement in grounded RAG. CLAPNQ is publicly available at https://github.com/primeqa/clapnq",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144063596",
                    "name": "Sara Rosenthal"
                },
                {
                    "authorId": "2707234",
                    "name": "Avirup Sil"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                }
            ]
        },
        {
            "paperId": "6995f2c2c3b13518c1e3489617ae40a9e7a11b95",
            "title": "Prompts as Auto-Optimized Training Hyperparameters: Training Best-in-Class IR Models from Scratch with 10 Gold Labels",
            "abstract": "We develop a method for training small-scale (under 100M parameter) neural information retrieval models with as few as 10 gold relevance labels. The method depends on generating synthetic queries for documents using a language model (LM), and the key step is that we automatically optimize the LM prompt that is used to generate these queries based on training quality. In experiments with the BIRCO benchmark, we find that models trained with our method outperform RankZephyr and are competitive with RankLLama, both of which are 7B parameter models trained on over 100K labels. These findings point to the power of automatic prompt optimization for synthetic dataset generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307001376",
                    "name": "Jasper Xian"
                },
                {
                    "authorId": "2306998031",
                    "name": "Saron Samuel"
                },
                {
                    "authorId": "2307001087",
                    "name": "Faraz Khoubsirat"
                },
                {
                    "authorId": "2307001538",
                    "name": "Ronak Pradeep"
                },
                {
                    "authorId": "2937809",
                    "name": "Md Arafat Sultan"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                },
                {
                    "authorId": "2707234",
                    "name": "Avirup Sil"
                },
                {
                    "authorId": "2254255092",
                    "name": "Christopher Potts"
                },
                {
                    "authorId": "144112155",
                    "name": "O. Khattab"
                }
            ]
        },
        {
            "paperId": "978aaaeccb4e67041d7c54fc1c3f4520824d8c1a",
            "title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
            "abstract": "It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response. In document-grounded response generation, for example, agent responses are expected to be relevant to a user's query while also being grounded in a given document. In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response. ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time. We apply ProMiSe to open source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its performance on document-grounded question answering datasets, MultiDoc2Dial and QuAC, demonstrating that self-refinement improves response quality. We further show that fine-tuning Llama-2-13B-Chat on the synthetic dialogue data generated by ProMiSe yields significant performance improvements over the zero-shot baseline as well as a supervised fine-tuned model on human annotated data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2289844084",
                    "name": "Keshav Ramji"
                },
                {
                    "authorId": "2145430350",
                    "name": "Young-Suk Lee"
                },
                {
                    "authorId": "2270425544",
                    "name": "R. Astudillo"
                },
                {
                    "authorId": "2284679524",
                    "name": "M. Sultan"
                },
                {
                    "authorId": "2138053379",
                    "name": "Tahira Naseem"
                },
                {
                    "authorId": "2273469018",
                    "name": "Asim Munawar"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                }
            ]
        },
        {
            "paperId": "12aa2b1e9556c20752e37e8b18d0e396c0cea1c5",
            "title": "Ensemble-Instruct: Generating Instruction-Tuning Data with a Heterogeneous Mixture of LMs",
            "abstract": "Using in-context learning (ICL) for data generation, techniques such as Self-Instruct (Wang et al., 2023) or the follow-up Alpaca (Taori et al., 2023) can train strong conversational agents with only a small amount of human supervision. One limitation of these approaches is that they resort to very large language models (around 175B parameters) that are also proprietary and non-public. Here we explore the application of such techniques to language models that are much smaller (around 10B--40B parameters) and have permissive licenses. We find the Self-Instruct approach to be less effective at these sizes and propose new ICL methods that draw on two main ideas: (a) Categorization and simplification of the ICL templates to make prompt learning easier for the LM, and (b) Ensembling over multiple LM outputs to help select high-quality synthetic examples. Our algorithm leverages the 175 Self-Instruct seed tasks and employs separate pipelines for instructions that require an input and instructions that do not. Empirical investigations with different LMs show that: (1) Our proposed method yields higher-quality instruction tuning data than Self-Instruct, (2) It improves performances of both vanilla and instruction-tuned LMs by significant margins, and (3) Smaller instruction-tuned LMs generate more useful outputs than their larger un-tuned counterparts. Our codebase is available at https://github.com/IBM/ensemble-instruct.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145430350",
                    "name": "Young-Suk Lee"
                },
                {
                    "authorId": "2937809",
                    "name": "Md Arafat Sultan"
                },
                {
                    "authorId": "2261287843",
                    "name": "Yousef El-Kurdi"
                },
                {
                    "authorId": "2261287688",
                    "name": "Tahira Naseem Asim Munawar"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                },
                {
                    "authorId": "3394760",
                    "name": "Ram\u00f3n Fern\u00e1ndez Astudillo"
                }
            ]
        }
    ]
}