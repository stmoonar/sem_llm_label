{
    "authorId": "2861442",
    "papers": [
        {
            "paperId": "4d5326408cc5b894fd4e2cc36e3aa5ee4bb2f1c2",
            "title": "Learning Entity Linking Features for Emerging Entities",
            "abstract": "Entity linking (EL) is the process of linking entity mentions appearing in text with their corresponding entities in a knowledge base. EL features of entities (e.g., prior probability, relatedness score, and entity embedding) are usually estimated based on Wikipedia. However, for newly emerging entities (EEs) which have just been discovered in news, they may still not be included in Wikipedia yet. As a consequence, it is unable to obtain required EL features for those EEs from Wikipedia and EL models will always fail to link ambiguous mentions with those EEs correctly as the absence of their EL features. To deal with this problem, in this paper we focus on a new task of learning EL features for emerging entities in a general way. We propose a novel approach called STAMO to learn high-quality EL features for EEs automatically, which needs just a small number of labeled documents for each EE collected from the Web, as it could further leverage the knowledge hidden in the unlabeled data. STAMO is mainly based on self-training, which makes it flexibly integrated with any EL feature or EL model, but also makes it easily suffer from the error reinforcement problem caused by the mislabeled data. Instead of some common self-training strategies that try to throw the mislabeled data away explicitly, we regard self-training as a multiple optimization process with respect to the EL features of EEs, and propose both intra-slot and inter-slot optimizations to alleviate the error reinforcement problem implicitly. We construct two EL datasets involving selected EEs to evaluate the quality of obtained EL features for EEs, and the experimental results show that our approach significantly outperforms other baseline methods of learning EL features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3011549",
                    "name": "Chenwei Ran"
                },
                {
                    "authorId": "144084234",
                    "name": "Wei Shen"
                },
                {
                    "authorId": "1810902946",
                    "name": "Jianbo Gao"
                },
                {
                    "authorId": "2110539208",
                    "name": "Yuhan Li"
                },
                {
                    "authorId": "2447408",
                    "name": "Jianyong Wang"
                },
                {
                    "authorId": "2861442",
                    "name": "Yantao Jia"
                }
            ]
        },
        {
            "paperId": "718343008a6cfca9e86ab6160caba353c52c17cf",
            "title": "KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models",
            "abstract": "Previous works show the great potential of pre-trained language models (PLMs) for storing a large amount of factual knowledge. However, to figure out whether PLMs can be reliable knowledge sources and used as alternative knowledge bases (KBs), we need to further explore some critical features of PLMs. Firstly, knowledge memorization and identification abilities: traditional KBs can store various types of entities and relationships; do PLMs have a high knowledge capacity to store different types of knowledge? Secondly, reasoning ability: a qualified knowledge source should not only provide a collection of facts, but support a symbolic reasoner. Can PLMs derive new knowledge based on the correlations between facts? To evaluate these features of PLMs, we propose a benchmark, named Knowledge Memorization, Identification, and Reasoning test (KMIR). KMIR covers 3 types of knowledge, including general knowledge, domain-specific knowledge, and commonsense, and provides 184,348 well-designed questions. Preliminary experiments with various representative pre-training language models on KMIR reveal many interesting phenomenons: 1) The memorization ability of PLMs depends more on the number of parameters than training schemes. 2) Current PLMs are struggling to robustly remember the facts. 3) Model compression technology retains the amount of knowledge well, but hurts the identification and reasoning abilities. We hope KMIR can facilitate the design of PLMs as better knowledge sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156585517",
                    "name": "Daniel Gao"
                },
                {
                    "authorId": "2861442",
                    "name": "Yantao Jia"
                },
                {
                    "authorId": "2151534953",
                    "name": "Lei Li"
                },
                {
                    "authorId": "41049101",
                    "name": "Chengzhen Fu"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "47067803",
                    "name": "Hao Jiang"
                },
                {
                    "authorId": null,
                    "name": "Xinyu Zhang"
                },
                {
                    "authorId": "2146072021",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2106400572",
                    "name": "Zhao Cao"
                }
            ]
        },
        {
            "paperId": "dbd684b259d6d7383ad4bb354ed53a0d12d38b63",
            "title": "Leveraging Multi-view Inter-passage Interactions for Neural Document Ranking",
            "abstract": "The configuration of 512 window size prevents transformers from being directly applicable to document ranking that requires larger context. Hence, recent works propose to estimate document relevance with fine-grained passage-level relevance signals. A limitation of such models, however, is that scoring each passage independently falls short in modeling inter-passage interactions and leads to unsatisfactory results. In this paper, we propose a Multiview inter-passage Interaction based Ranking model (MIR), to combine intra-passage interactions and inter-passage interactions in a complementary manner. The former captures local semantic relations inside each passage, whereas the latter draws global dependencies between different passages. Moreover, we represent inter-passage relationships via multi-view attention patterns, allowing information propagation at token, sentence, and passage-level. The representations at different levels of granularity, being aware of global context, are then aggregated into a document-level representation for ranking. Experimental results on two benchmarks show that modeling inter-passage interactions brings substantial improvements over existing passage-level methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41049101",
                    "name": "Chengzhen Fu"
                },
                {
                    "authorId": "2118895517",
                    "name": "Enrui Hu"
                },
                {
                    "authorId": "108186460",
                    "name": "Letian Feng"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2861442",
                    "name": "Yantao Jia"
                },
                {
                    "authorId": "2146072021",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2155386012",
                    "name": "Fan Yu"
                },
                {
                    "authorId": "2106400572",
                    "name": "Zhao Cao"
                }
            ]
        },
        {
            "paperId": "e1b8289815cbc3ea2374c5331b2fe7a271e8e8e9",
            "title": "Link Prediction in Knowledge Graphs: A Hierarchy-Constrained Approach",
            "abstract": "Link prediction over a knowledge graph aims to predict the missing head entities <inline-formula><tex-math notation=\"LaTeX\">$h$</tex-math><alternatives><mml:math><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href=\"li-ieq1-2867583.gif\"/></alternatives></inline-formula> or tail entities <inline-formula><tex-math notation=\"LaTeX\">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=\"li-ieq2-2867583.gif\"/></alternatives></inline-formula> and missing relations <inline-formula><tex-math notation=\"LaTeX\">$r$</tex-math><alternatives><mml:math><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href=\"li-ieq3-2867583.gif\"/></alternatives></inline-formula> for a triple <inline-formula><tex-math notation=\"LaTeX\">$(h,r,t)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"li-ieq4-2867583.gif\"/></alternatives></inline-formula>. Recent years have witnessed great advance of knowledge graph embedding based link prediction methods, which represent entities and relations as elements of a continuous vector space. Most methods learn the embedding vectors by optimizing a margin-based loss function, where the margin is used to separate negative and positive triples in the loss function. The loss function utilizes the general structures of knowledge graphs, e.g., the vector of <inline-formula><tex-math notation=\"LaTeX\">$r$</tex-math><alternatives><mml:math><mml:mi>r</mml:mi></mml:math><inline-graphic xlink:href=\"li-ieq5-2867583.gif\"/></alternatives></inline-formula> is the translation of the vector of <inline-formula><tex-math notation=\"LaTeX\">$h$</tex-math><alternatives><mml:math><mml:mi>h</mml:mi></mml:math><inline-graphic xlink:href=\"li-ieq6-2867583.gif\"/></alternatives></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=\"li-ieq7-2867583.gif\"/></alternatives></inline-formula>, and the vector of <inline-formula><tex-math notation=\"LaTeX\">$t$</tex-math><alternatives><mml:math><mml:mi>t</mml:mi></mml:math><inline-graphic xlink:href=\"li-ieq8-2867583.gif\"/></alternatives></inline-formula> should be the nearest neighbor of the vector of <inline-formula><tex-math notation=\"LaTeX\">$h+r$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>h</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"li-ieq9-2867583.gif\"/></alternatives></inline-formula>. However, there are many particular structures, and can be employed to promote the performance of link prediction. One typical structure in knowledge graphs is hierarchical structure, which existing methods have much unexplored. We argue that the hierarchical structures also contain rich inference patterns, and can further enhance the link prediction performance. In this paper, we propose a hierarchy-constrained link prediction method, called hTransM, on the basis of the translation-based knowledge graph embedding methods. It can adaptively determine the optimal margin by detecting the single-step and multi-step hierarchical structures. Moreover, we prove the effectiveness of hTransM theoretically, and experiments over three benchmark datasets and two sub-tasks of link prediction demonstrate the superiority of hTransM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47845008",
                    "name": "Denghui Zhang"
                },
                {
                    "authorId": "2109953207",
                    "name": "Denghui Zhang"
                },
                {
                    "authorId": "2861442",
                    "name": "Yantao Jia"
                },
                {
                    "authorId": "1717004",
                    "name": "Xueqi Cheng"
                }
            ]
        },
        {
            "paperId": "683de10cda9cd3b709983db2da67f1f059fa516a",
            "title": "Towards More Effective and Economic Sparsely-Activated Model",
            "abstract": "The sparsely-activated models have achieved great success in natural language processing through large-scale parameters and relatively low computational cost, and gradually become a feasible technique for training and implementing extremely large models. Due to the limit of communication cost, activating multiple experts is hardly affordable during training and inference. Therefore, previous work usually activate just one expert at a time to alleviate additional communication cost. Such routing mechanism limits the upper bound of model performance. In this paper, we first investigate a phenomenon that increasing the number of activated experts can boost the model performance with higher sparse ratio. To increase the number of activated experts without an increase in computational cost, we propose SAM (Switch and Mixture) routing, an efficient hierarchical routing mechanism that activates multiple experts in a same device (GPU). Our methods shed light on the training of extremely large sparse models and experiments prove that our models can achieve significant performance gain with great efficiency improvement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152631472",
                    "name": "Hao Jiang"
                },
                {
                    "authorId": "50324815",
                    "name": "Ke Zhan"
                },
                {
                    "authorId": "2147138528",
                    "name": "Jianwei Qu"
                },
                {
                    "authorId": "2115859566",
                    "name": "Yongkang Wu"
                },
                {
                    "authorId": "2132200788",
                    "name": "Zhaoye Fei"
                },
                {
                    "authorId": null,
                    "name": "Xinyu Zhang"
                },
                {
                    "authorId": "2146072021",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "1767521",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2112225685",
                    "name": "Zi-Han Guo"
                },
                {
                    "authorId": "2128243415",
                    "name": "Ruofei Lai"
                },
                {
                    "authorId": "2111123834",
                    "name": "Jiawen Wu"
                },
                {
                    "authorId": "2118895517",
                    "name": "Enrui Hu"
                },
                {
                    "authorId": "2129510184",
                    "name": "Yinxia Zhang"
                },
                {
                    "authorId": "2861442",
                    "name": "Yantao Jia"
                },
                {
                    "authorId": "2155386012",
                    "name": "Fan Yu"
                },
                {
                    "authorId": "2106400572",
                    "name": "Zhao Cao"
                }
            ]
        },
        {
            "paperId": "7c6036c75234b0d72b0dc071ee7508f5afae6068",
            "title": "YES SIR!Optimizing Semantic Space of Negatives with Self-Involvement Ranker",
            "abstract": "Pre-trained model such as BERT has been proved to be an effective tool for dealing with Information Retrieval (IR) problems. Due to its inspiring performance, it has been widely used to tackle with real-world IR problems such as document ranking. Recently, researchers have found that selecting\"hard\"rather than\"random\"negative samples would be beneficial for fine-tuning pre-trained models on ranking tasks. However, it remains elusive how to leverage hard negative samples in a principled way. To address the aforementioned issues, we propose a fine-tuning strategy for document ranking, namely Self-Involvement Ranker (SIR), to dynamically select hard negative samples to construct high-quality semantic space for training a high-quality ranking model. Specifically, SIR consists of sequential compressors implemented with pre-trained models. Front compressor selects hard negative samples for rear compressor. Moreover, SIR leverages supervisory signal to adaptively adjust semantic space of negative samples. Finally, supervisory signal in rear compressor is computed based on condition probability and thus can control sample dynamic and further enhance the model performance. SIR is a lightweight and general framework for pre-trained models, which simplifies the ranking process in industry practice. We test our proposed solution on MS MARCO with document ranking setting, and the results show that SIR can significantly improve the ranking performance of various pre-trained models. Moreover, our method became the new SOTA model anonymously on MS MARCO Document ranking leaderboard in May 2021.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2126876087",
                    "name": "Ruizhi Pu"
                },
                {
                    "authorId": null,
                    "name": "Xinyu Zhang"
                },
                {
                    "authorId": "2128243415",
                    "name": "Ruofei Lai"
                },
                {
                    "authorId": "1944323950",
                    "name": "Zikai Guo"
                },
                {
                    "authorId": "2129510184",
                    "name": "Yinxia Zhang"
                },
                {
                    "authorId": "2152631472",
                    "name": "Hao Jiang"
                },
                {
                    "authorId": "2115859566",
                    "name": "Yongkang Wu"
                },
                {
                    "authorId": "2861442",
                    "name": "Yantao Jia"
                },
                {
                    "authorId": "47405877",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2106400572",
                    "name": "Zhao Cao"
                }
            ]
        },
        {
            "paperId": "8c8fe7c49ad0c3fc513c1dbf282ad91c49d1616f",
            "title": "Answer Complex Questions: Path Ranker Is All You Need",
            "abstract": "Currently, the most popular method for open-domain Question Answering (QA) adopts \"Retriever and Reader\" pipeline, where the retriever extracts a list of candidate documents from a large set of documents followed by a ranker to rank the most relevant documents and the reader extracts answer from the candidates. Existing studies take the greedy strategy in the sense that they only use samples for ranking at the current hop, and ignore the global information across the whole documents. In this paper, we propose a purely rank-based framework Thinking Path Re-Ranker (TPRR), which is comprised of Thinking Path Ranker (TPR) for generating document sequences called \"a path\" and External Path Reranker (EPR) for selecting the best path from candidate paths generated by TPR. Specifically, TPR leverages the scores of a dense model and conditional probabilities to score the full paths. Moreover, to further enhance the performance of the dense ranker in the iterative training, we propose a \"thinking\" negatives selection method that the top-K candidates treated as negatives in the current hop are adjusted dynamically through supervised signals. After achieving multiple supporting paths through TPR, the EPR component which integrates several fine-grained training tasks for QA is used to select the best path for answer extraction. We have tested our proposed solution on the multi-hop dataset \"HotpotQA\" with a full wiki set ting, and the results show that TPRR significantly outperforms the existing state-of-the-art models. Moreover, our method has won the first place in the HotpotQA official leaderboard since Feb 1, 2021 under the Fullwiki setting. Code is available at https://gitee.com/mindspore/mindspore/ tree/master/model_zoo/research/nlp/tprr.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Xinyu Zhang"
                },
                {
                    "authorId": "50324815",
                    "name": "Ke Zhan"
                },
                {
                    "authorId": "2118895517",
                    "name": "Enrui Hu"
                },
                {
                    "authorId": "41049101",
                    "name": "Chengzhen Fu"
                },
                {
                    "authorId": "2118896534",
                    "name": "Lan Luo"
                },
                {
                    "authorId": "47067803",
                    "name": "Hao Jiang"
                },
                {
                    "authorId": "2861442",
                    "name": "Yantao Jia"
                },
                {
                    "authorId": "2155386012",
                    "name": "Fan Yu"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2106400572",
                    "name": "Zhao Cao"
                },
                {
                    "authorId": "2146072021",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "c171a8153be1463fce5f28358506241eeb5f8775",
            "title": "Emotion Eliciting Machine: Emotion Eliciting Conversation Generation based on Dual Generator",
            "abstract": "Recent years have witnessed great progress on building emotional chatbots. Tremendous methods have been proposed for chatbots to generate responses with given emotions. However, the emotion changes of the user during the conversation has not been fully explored. In this work, we study the problem of positive emotion elicitation, which aims to generate responses that can elicit positive emotion of the user, in human-machine conversation. We propose a weakly supervised Emotion Eliciting Machine (EEM) to address this problem. Specifically, we first collect weak labels of user emotion status changes in a conversion based on a pre-trained emotion classifier. Then we propose a dual encoder-decoder structure to model the generation of responses in both positive and negative side based on the changes of the user's emotion status in the conversation. An emotion eliciting factor is introduced on top of the dual structure to balance the positive and negative emotional impacts on the generated response during emotion elicitation. The factor also provides a fine-grained controlling manner for emotion elicitation. Experimental results on a large real-world dataset show that EEM outperforms the existing models in generating responses with positive emotion elicitation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47067803",
                    "name": "Hao Jiang"
                },
                {
                    "authorId": "2123659063",
                    "name": "Yutao Zhu"
                },
                {
                    "authorId": null,
                    "name": "Xinyu Zhang"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2407633",
                    "name": "Pan Du"
                },
                {
                    "authorId": "2570924",
                    "name": "Te Pi"
                },
                {
                    "authorId": "2861442",
                    "name": "Yantao Jia"
                }
            ]
        },
        {
            "paperId": "ed4449f065fa225dd7c08929c64e6356ff1264f6",
            "title": "OntoZSL: Ontology-enhanced Zero-shot Learning",
            "abstract": "Zero-shot Learning (ZSL), which aims to predict for those classes that have never appeared in the training data, has arisen hot research interests. The key of implementing ZSL is to leverage the prior knowledge of classes which builds the semantic relationship between classes and enables the transfer of the learned models (e.g., features) from training classes (i.e., seen classes) to unseen classes. However, the priors adopted by the existing methods are relatively limited with incomplete semantics. In this paper, we explore richer and more competitive prior knowledge to model the inter-class relationship for ZSL via ontology-based knowledge representation and semantic embedding. Meanwhile, to address the data imbalance between seen classes and unseen classes, we developed a generative ZSL framework with Generative Adversarial Networks (GANs). Our main findings include: (i) an ontology-enhanced ZSL framework that can be applied to different domains, such as image classification (IMGC) and knowledge graph completion (KGC); (ii) a comprehensive evaluation with multiple zero-shot datasets from different domains, where our method often achieves better performance than the state-of-the-art models. In particular, on four representative ZSL baselines of IMGC, the ontology-based class semantics outperform the previous priors e.g., the word embeddings of classes by an average of 12.4 accuracy points in the standard ZSL across two example datasets (see Figure 4).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152683218",
                    "name": "Yuxia Geng"
                },
                {
                    "authorId": "1731892",
                    "name": "Jiaoyan Chen"
                },
                {
                    "authorId": "2283992213",
                    "name": "Zhuo Chen"
                },
                {
                    "authorId": "9416872",
                    "name": "Jeff Z. Pan"
                },
                {
                    "authorId": "2114132405",
                    "name": "Zhiquan Ye"
                },
                {
                    "authorId": "1617854355",
                    "name": "Zonggang Yuan"
                },
                {
                    "authorId": "2861442",
                    "name": "Yantao Jia"
                },
                {
                    "authorId": "1729778",
                    "name": "Huajun Chen"
                }
            ]
        },
        {
            "paperId": "1c3652ec7c02a8002d6468de448688cf54e23d96",
            "title": "The Devil is the Classifier: Investigating Long Tail Relation Classification with Decoupling Analysis",
            "abstract": "Long-tailed relation classification is a challenging problem as the head classes may dominate the training phase, thereby leading to the deterioration of the tail performance. Existing solutions usually address this issue via class-balancing strategies, e.g., data re-sampling and loss re-weighting, but all these methods adhere to the schema of entangling learning of the representation and classifier. In this study, we conduct an in-depth empirical investigation into the long-tailed problem and found that pre-trained models with instance-balanced sampling already capture the well-learned representations for all classes; moreover, it is possible to achieve better long-tailed classification ability at low cost by only adjusting the classifier. Inspired by this observation, we propose a robust classifier with attentive relation routing, which assigns soft weights by automatically aggregating the relations. Extensive experiments on two datasets demonstrate the effectiveness of our proposed approach. Code and datasets are available in this https URL.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2119316118",
                    "name": "Haiyang Yu"
                },
                {
                    "authorId": "2608639",
                    "name": "Ningyu Zhang"
                },
                {
                    "authorId": "152931849",
                    "name": "Shumin Deng"
                },
                {
                    "authorId": "1617854355",
                    "name": "Zonggang Yuan"
                },
                {
                    "authorId": "2861442",
                    "name": "Yantao Jia"
                },
                {
                    "authorId": "1729778",
                    "name": "Huajun Chen"
                }
            ]
        }
    ]
}