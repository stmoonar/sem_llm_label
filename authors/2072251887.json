{
    "authorId": "2072251887",
    "papers": [
        {
            "paperId": "9317abf3795d35a138e53c815bed97df6ee970a6",
            "title": "Evaluating the Data Model Robustness of Text-to-SQL Systems Based on Real User Queries",
            "abstract": "Text-to-SQL systems (also known as NL-to-SQL systems) have become an increasingly popular solution for bridging the gap between user capabilities and SQL-based data access. These systems translate user requests in natural language to valid SQL statements for a specific database. Recent Text-to-SQL systems have benefited from the rapid improvement of transformer-based language models. However, while Text-to-SQL systems that incorporate such models continuously reach new high scores on -- often synthetic -- benchmark datasets, a systematic exploration of their robustness towards different data models in a real-world, realistic scenario is notably missing. This paper provides the first in-depth evaluation of the data model robustness of Text-to-SQL systems in practice based on a multi-year international project focused on Text-to-SQL interfaces. Our evaluation is based on a real-world deployment of FootballDB, a system that was deployed over a 9 month period in the context of the FIFA World Cup 2022, during which about 6K natural language questions were asked and executed. All of our data is based on real user questions that were asked live to the system. We manually labeled and translated a subset of these questions for three different data models. For each data model, we explore the performance of representative Text-to-SQL systems and language models. We further quantify the impact of training data size, pre-, and post-processing steps as well as language model inference time. Our comprehensive evaluation sheds light on the design choices of real-world Text-to-SQL systems and their impact on moving from research prototypes to real deployments. Last, we provide a new benchmark dataset to the community, which is the first to enable the evaluation of different data models for the same dataset and is substantially more challenging than most previous datasets in terms of query complexity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284665905",
                    "name": "Jonathan F\u00fcrst"
                },
                {
                    "authorId": "2072251887",
                    "name": "Catherine Kosten"
                },
                {
                    "authorId": "2428180",
                    "name": "F. Nooralahzadeh"
                },
                {
                    "authorId": "2284214271",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "2113917675",
                    "name": "Kurt Stockinger"
                }
            ]
        },
        {
            "paperId": "4ac0d89d022a9ca89967cd812800e18618e83b3f",
            "title": "Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems",
            "abstract": "With the recent spike in the number and availability of Large Language Models (LLMs), it has become increasingly important to provide large and realistic benchmarks for evaluating Knowledge Graph Question Answering (KGQA) systems. So far the majority of benchmarks rely on pattern-based SPARQL query generation approaches. The subsequent natural language (NL) question generation is conducted through crowdsourcing or other automated methods, such as rule-based paraphrasing or NL question templates. Although some of these datasets are of considerable size, their pitfall lies in their pattern-based generation approaches, which do not always generalize well to the vague and linguistically diverse questions asked by humans in real-world contexts. In this paper, we introduce Spider4SPARQL -a new SPARQL benchmark dataset featuring 9,693 previously existing manually generated NL questions and 4,721 unique, novel, and complex SPARQL queries of varying complexity. In addition to the NL/SPARQL pairs, we also provide their corresponding 166 knowledge graphs and ontologies, which cover 138 different domains. Our complex benchmark enables novel ways of evaluating the strengths and weaknesses of modern KGQA systems. We evaluate the system with state-of-the-art KGQA systems as well as LLMs, which achieve only up to 45% execution accuracy, demonstrating that Spider4SPARQL is a challenging benchmark for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072251887",
                    "name": "Catherine Kosten"
                },
                {
                    "authorId": "1410039219",
                    "name": "Philippe Cudr\u00e9-Mauroux"
                },
                {
                    "authorId": "2113917675",
                    "name": "Kurt Stockinger"
                }
            ]
        },
        {
            "paperId": "e7a4e7b349b70b1681978cd57a59a6e1684b219f",
            "title": "ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural Language to SQL Systems",
            "abstract": "Natural Language to SQL systems (NL-to-SQL) have recently shown improved accuracy (exceeding 80%) for natural language to SQL query translation due to the emergence of transformer-based language models, and the popularity of the Spider benchmark. However, Spider mainly contains simple databases with few tables, columns, and entries, which do not reflect a realistic setting. Moreover, complex real-world databases with domain-specific content have little to no training data available in the form of NL/SQL-pairs leading to poor performance of existing NL-to-SQL systems.\n \n In this paper, we introduce\n ScienceBenchmark\n , a new complex NL-to-SQL benchmark for three real-world, highly domain-specific databases. For this new benchmark, SQL experts and domain experts created high-quality NL/SQL-pairs for each domain. To garner more data, we extended the small amount of human-generated data with synthetic data generated using GPT-3. We show that our benchmark is highly challenging, as the top performing systems on Spider achieve a very low performance on our benchmark. Thus, the challenge is many-fold: creating NL-to-SQL systems for highly complex domains with a small amount of hand-made training data augmented with synthetic data. To our knowledge,\n ScienceBenchmark\n is the first NL-to-SQL benchmark designed with complex real-world scientific databases, containing challenging training and test data carefully validated by domain experts.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46867002",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "145116511",
                    "name": "Jan Deriu"
                },
                {
                    "authorId": "2055401797",
                    "name": "George Katsogiannis-Meimarakis"
                },
                {
                    "authorId": "2072251887",
                    "name": "Catherine Kosten"
                },
                {
                    "authorId": "1680709",
                    "name": "Georgia Koutrika"
                },
                {
                    "authorId": "2113917675",
                    "name": "Kurt Stockinger"
                }
            ]
        },
        {
            "paperId": "e80aba67e92aec3ef06ed31193832c6f81e9a2ed",
            "title": "INODE",
            "abstract": "A full-fledged data exploration system must combine different access modalities with a powerful concept of guiding the user in the exploration process, by being reactive and anticipative both for data discovery and for data linking. Such systems are a real opportunity for our community to cater to users with different domain and data science expertise. We introduce INODE - an end-to-end data exploration system - that leverages, on the one hand, Machine Learning and, on the other hand, semantics for the purpose of Data Management (DM). Our vision is to develop a classic unified, comprehensive platform that provides extensive access to open datasets, and we demonstrate it in three significant use cases in the fields of Cancer Biomarker Research, Research and Innovation Policy Making, and Astrophysics. INODE offers sustainable services in (a) data modeling and linking, (b) integrated query processing using natural language, (c) guidance, and (d) data exploration through visualization, thus facilitating the user in discovering new insights. We demonstrate that our system is uniquely accessible to a wide range of users from larger scientific communities to the public. Finally, we briefly illustrate how this work paves the way for new research opportunities in DM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1680709",
                    "name": "Georgia Koutrika"
                },
                {
                    "authorId": "2463025",
                    "name": "Fr\u00e9d\u00e9ric B. Bastian"
                },
                {
                    "authorId": "1726022751",
                    "name": "Theofilos Belmpas"
                },
                {
                    "authorId": "3075644",
                    "name": "Martin Braschler"
                },
                {
                    "authorId": "151240818",
                    "name": "Ursin Brunner"
                },
                {
                    "authorId": "2072276517",
                    "name": "D. Calvanese"
                },
                {
                    "authorId": "14469238",
                    "name": "M. Fabricius"
                },
                {
                    "authorId": "1726045725",
                    "name": "Orest Gkini"
                },
                {
                    "authorId": "2072251887",
                    "name": "Catherine Kosten"
                },
                {
                    "authorId": "1796236",
                    "name": "D. Lanti"
                },
                {
                    "authorId": "40469553",
                    "name": "Antonis Litke"
                },
                {
                    "authorId": "1403433652",
                    "name": "Hendrik L\u00fccke-Tieke"
                },
                {
                    "authorId": "1970432",
                    "name": "F. Massucci"
                },
                {
                    "authorId": "1725471",
                    "name": "T. M. Farias"
                },
                {
                    "authorId": "1806182",
                    "name": "A. Mosca"
                },
                {
                    "authorId": "2072251867",
                    "name": "Francesco Multari"
                },
                {
                    "authorId": "145679608",
                    "name": "N. Papadakis"
                },
                {
                    "authorId": "153516669",
                    "name": "D. Papadopoulos"
                },
                {
                    "authorId": "2061639772",
                    "name": "Yogendra Patil"
                },
                {
                    "authorId": "2072259106",
                    "name": "Aur\u00e9lien Personnaz"
                },
                {
                    "authorId": "3235045",
                    "name": "Guillem Rull"
                },
                {
                    "authorId": "2723088",
                    "name": "A. Sima"
                },
                {
                    "authorId": "153437025",
                    "name": "Ellery Smith"
                },
                {
                    "authorId": "1807115",
                    "name": "Dimitrios Skoutas"
                },
                {
                    "authorId": "2075402169",
                    "name": "S. Subramanian"
                },
                {
                    "authorId": "2055961473",
                    "name": "G. Xiao"
                },
                {
                    "authorId": "2113917675",
                    "name": "Kurt Stockinger"
                }
            ]
        }
    ]
}