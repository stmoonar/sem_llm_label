{
    "authorId": "2146416796",
    "papers": [
        {
            "paperId": "b49edd2ff3cfeb23107b10996a4d2798b2c9cb44",
            "title": "Hybrid Text Representation for Explainable Suicide Risk Identification on Social Media",
            "abstract": "Social media data that characterize users can provide mental health signals, including suicide risks. Existing methods for suicide risk identification on social media have demonstrated promising results; however, the limitation of existing methods is that they are unable to capture low- and high-level features with complex structured data on social media and are incapable of explaining the predicted labels. Explainable models are more useful when translated, so we aimed to evaluate a novel method that would produce explainable models. This article presents a hybrid text representation method that integrates word and document-level text representations to explain suicide risk identification on social media. The proposed method is then fed to a transformer-based encoder with ordinal classification to determine suicide risk. Our results show that our method outperforms state-of-the-art baselines with an FScore of 0.79 (an absolute increase of 15%) on a public suicide dataset. Our method shows that an explainable model can perform at a comparable level to the best nonexplainable models but has advantages if translated for use in clinical and public health practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "4631130",
                    "name": "Matloob Khushi"
                },
                {
                    "authorId": "2146416796",
                    "name": "Jinman Kim"
                },
                {
                    "authorId": "34318729",
                    "name": "A. Dunn"
                }
            ]
        },
        {
            "paperId": "5ce0e095e9f3e49eaa9a21647cda58e22ac8f22b",
            "title": "Remote Interactive Surgery Platform (RISP): Proof of Concept for an Augmented-Reality-Based Platform for Surgical Telementoring",
            "abstract": "The \u201cRemote Interactive Surgery Platform\u201d (RISP) is an augmented reality (AR)-based platform for surgical telementoring. It builds upon recent advances of mixed reality head-mounted displays (MR-HMD) and associated immersive visualization technologies to assist the surgeon during an operation. It enables an interactive, real-time collaboration with a remote consultant by sharing the operating surgeon\u2019s field of view through the Microsoft (MS) HoloLens2 (HL2). Development of the RISP started during the Medical Augmented Reality Summer School 2021 and is currently still ongoing. It currently includes features such as three-dimensional annotations, bidirectional voice communication and interactive windows to display radiographs within the sterile field. This manuscript provides an overview of the RISP and preliminary results regarding its annotation accuracy and user experience measured with ten participants.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "26338204",
                    "name": "Y. Kalbas"
                },
                {
                    "authorId": "123132879",
                    "name": "Hoijoon Jung"
                },
                {
                    "authorId": "2209799352",
                    "name": "J. Ricklin"
                },
                {
                    "authorId": "2209799563",
                    "name": "Ge Jin"
                },
                {
                    "authorId": "2209861054",
                    "name": "Mingjian Li"
                },
                {
                    "authorId": "1660521485",
                    "name": "T. Rauer"
                },
                {
                    "authorId": "1999206991",
                    "name": "Shervin Dehghani"
                },
                {
                    "authorId": "145587210",
                    "name": "Nassir Navab"
                },
                {
                    "authorId": "2146416796",
                    "name": "Jinman Kim"
                },
                {
                    "authorId": "2209799243",
                    "name": "Hans-Christoph Pape"
                },
                {
                    "authorId": "2385360",
                    "name": "S. Heining"
                }
            ]
        },
        {
            "paperId": "760ddd989f4a9e2c5281f770595944c661cd8be7",
            "title": "Graph-Based Hierarchical Attention Network for Suicide Risk Detection on Social Media",
            "abstract": "The widespread use of social media for expressing personal thoughts and emotions makes it a valuable resource for identifying individuals at risk of suicide. Existing sequential learning-based methods have shown promising results. However, these methods may fail to capture global features. Due to its inherent ability to learn interconnected data, graph-based methods can address this gap. In this paper, we present a new graph-based hierarchical attention network (GHAN) that uses a graph convolutional neural network with an ordinal loss to improve suicide risk identification on social media. Specifically, GHAN first captures global features by constructing three graphs to capture semantic, syntactic, and sequential contextual information. Then encoded textual features are fed to attentive transformers\u2019 encoder and optimized to factor in the increasing suicide risk levels using an ordinal classification layer hierarchically for suicide risk detection. Experimental results show that the proposed GHAN outperformed state-of-the-art methods on a public Reddit dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "2146416796",
                    "name": "Jinman Kim"
                },
                {
                    "authorId": "4631130",
                    "name": "Matloob Khushi"
                },
                {
                    "authorId": "34318729",
                    "name": "A. Dunn"
                }
            ]
        },
        {
            "paperId": "795f1de72332f2ba6e1d4c6300a586f1232cbe93",
            "title": "K-PathVQA: Knowledge-Aware Multimodal Representation for Pathology Visual Question Answering",
            "abstract": "Pathology imaging is routinely used to detect the underlying effects and causes of diseases or injuries. Pathology visual question answering (PathVQA) aims to enable computers to answer questions about clinical visual findings from pathology images. Prior work on PathVQA has focused on directly analyzing the image content using conventional pretrained encoders without utilizing relevant external information when the image content is inadequate. In this article, we present a knowledge-driven PathVQA (K-PathVQA), which uses a medical knowledge graph (KG) from a complementary external structured knowledge base to infer answers for the PathVQA task. K-PathVQA improves the question representation with external medical knowledge and then aggregates vision, language, and knowledge embeddings to learn a joint knowledge-image-question representation. Our experiments using a publicly available PathVQA dataset showed that our K-PathVQA outperformed the best baseline method with an increase of 4.15% in accuracy for the overall task, an increase of 4.40% in open-ended question type and an absolute increase of 1.03% in closed-ended question types. Ablation testing shows the impact of each of the contributions. Generalizability of the method is demonstrated with a separate medical VQA dataset.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "4631130",
                    "name": "Matloob Khushi"
                },
                {
                    "authorId": "34318729",
                    "name": "A. Dunn"
                },
                {
                    "authorId": "2146416796",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "a41552d69ca566bfc5c7f1b7e0e97fe61cadb7af",
            "title": "Robust Identification of Figurative Language in Personal Health Mentions on Twitter",
            "abstract": "People often discuss their health on social media platforms. Discussion of personal experiences with diseases and symptoms can be useful in public health applications like adverse event surveillance. A major challenge comes from the need to distinguish personal health mentions from other uses of those terms, including figurative use, where words are used to mean something different. Public health applications require the separation of personal health mentions from other uses. Prior approaches incorporate some elements of context but could be improved to capture relationships between the linguistic characteristics of figurative expressions and the representations of the context. In this work, we investigate the role of context representation for identifying personal health mentions on social media and measure the impact of different representation choices on detecting figurative use of a range of disease and symptom words. We present an end-to-end approach that selects representations adaptively for different disease or symptom words. We conduct experiments using a publicly available health-mention dataset, annotated with ten disease or symptom labels. The results demonstrate that our approach outperforms the state of the art (SOTA) in the identification of figurative language use across a range of disease or symptom words, with an F1-score of 0.925 (an increase of 10.7% over the SOTA) and the proportion of correctly identified figurative mentions was 0.923 (an increase of 16.7% over the SOTA). An ablation analysis demonstrates that each of the new modules contributes to this increased performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "2146416796",
                    "name": "Jinman Kim"
                },
                {
                    "authorId": "4631130",
                    "name": "Matloob Khushi"
                },
                {
                    "authorId": "34318729",
                    "name": "A. Dunn"
                }
            ]
        },
        {
            "paperId": "a4d8774210a8af88912159a5cb7c15ac850651b4",
            "title": "RHMD: A Real-World Dataset for Health Mention Classification on Reddit",
            "abstract": "People on social media share their thoughts and experiences using diseases and symptoms words other than to mention their health, which can introduce biases in data-driven public health applications. For the advancement of HMC research, in this study, we present a Reddit health mention dataset (RHMD), a new dataset of multi-domain Reddit data for the HMC. RHMD is composed of 10015 manually annotated Reddit posts that include 15 common disease or symptom terms and are labeled with four labels: personal health mentions (HMs), nonpersonal HMs, figurative HMs, and hyperbolic HMs. Empirical evaluation using recently proposed methods demonstrates the challenge of labeling user-generated text across these four types. Contributions to this work include the public release of a robustly annotated Reddit dataset (RHMD) for HM tasks and a comprehensive performance analysis of baseline methods. We expect the release of the dataset, and the evaluations will help facilitate the development of new methods for detecting HMs in the user-generated text. The dataset is available at https://github.com/usmaann/RHMD-Health-Mention-Dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "4631130",
                    "name": "Matloob Khushi"
                },
                {
                    "authorId": "2146416796",
                    "name": "Jinman Kim"
                },
                {
                    "authorId": "34318729",
                    "name": "A. Dunn"
                }
            ]
        },
        {
            "paperId": "ce33b05bb980d4e518ce1aaaab3eb63072670ae3",
            "title": "A Multimodal Framework for the Identification of Vaccine Critical Memes on Twitter",
            "abstract": "Memes can be a useful way to spread information because they are funny, easy to share, and can spread quickly and reach further than other forms. With increased interest in COVID-19 vaccines, vaccination-related memes have grown in number and reach. Memes analysis can be difficult because they use sarcasm and often require contextual understanding. Previous research has shown promising results but could be improved by capturing global and local representations within memes to model contextual information. Further, the limited public availability of annotated vaccine critical memes datasets limit our ability to design computational methods to help design targeted interventions and boost vaccine uptake. To address these gaps, we present VaxMeme, which consists of 10,244 manually labelled memes. With VaxMeme, we propose a new multimodal framework designed to improve the memes' representation by learning the global and local representations of memes. The improved memes' representations are then fed to an attentive representation learning module to capture contextual information for classification using an optimised loss function. Experimental results show that our framework outperformed state-of-the-art methods with an F1-Score of 84.2%. We further analyse the transferability and generalisability of our framework and show that understanding both modalities is important to identify vaccine critical memes on Twitter. Finally, we discuss how understanding memes can be useful in designing shareable vaccination promotion, myth debunking memes and monitoring their uptake on social media platforms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "2146416796",
                    "name": "Jinman Kim"
                },
                {
                    "authorId": "4631130",
                    "name": "Matloob Khushi"
                },
                {
                    "authorId": "34318729",
                    "name": "A. Dunn"
                }
            ]
        },
        {
            "paperId": "54963e8c80e8e09833cbfe3ce0b5f09953abc8b3",
            "title": "Early Identification of Depression Severity Levels on Reddit Using Ordinal Classification",
            "abstract": "User-generated text on social media is a promising avenue for public health surveillance and has been actively explored for its feasibility in the early identification of depression. Existing methods in the identification of depression have shown promising results; however, these methods were all focused on treating the identification as a binary classification problem. To date, there has been little effort towards identifying users\u2019 depression severity level and disregard the inherent ordinal nature across these fine-grain levels. This paper aims to make early identification of depression severity levels on social media data. To accomplish this, we built a new dataset based on the inherent ordinal nature over depression severity levels using clinical depression standards on Reddit posts. The posts were classified into 4 depression severity levels covering the clinical depression standards on social media. Accordingly, we reformulate the early identification of depression as an ordinal classification task over clinical depression standards such as Beck\u2019s Depression Inventory and the Depressive Disorder Annotation scheme to identify depression severity levels. With these, we propose a hierarchical attention method optimized to factor in the increasing depression severity levels through a soft probability distribution. We experimented using two datasets (a public dataset having more than one post from each user and our built dataset with a single user post) using real-world Reddit posts that have been classified according to questionnaires built by clinical experts and demonstrated that our method outperforms state-of-the-art models. Finally, we conclude by analyzing the minimum number of posts required to identify depression severity level followed by a discussion of empirical and practical considerations of our study.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "34318729",
                    "name": "A. Dunn"
                },
                {
                    "authorId": "2146416796",
                    "name": "Jinman Kim"
                },
                {
                    "authorId": "4631130",
                    "name": "Matloob Khushi"
                }
            ]
        },
        {
            "paperId": "8db60d023e499e2b76758bab1ee09a6eda826b7d",
            "title": "Deep Multimodal Graph-Based Network for Survival Prediction from Highly Multiplexed Images and Patient Variables",
            "abstract": "The spatial architecture of the tumour microenvironment and phenotypic heterogeneity of tumour cells have been shown to be associated with cancer prognosis and clinical outcomes, including survival. Recent advances in highly multiplexed imaging, including imaging mass cytometry (IMC), capture spatially resolved, high-dimensional maps that quantify dozens of disease-relevant biomarkers at single-cell resolution, that contain potential to inform patient-specific prognosis. However, existing automated methods for predicting survival typically do not leverage spatial phenotype information captured at the single-cell level, and current methods tend to focus on a single modality, such as patient variables (PVs). There is no end-to-end method designed to leverage the rich information in whole IMC images and all marker channels, and aggregate this information with PVs in a complementary manner to predict survival with enhanced accuracy. We introduce a deep multimodal graph-based network (DMGN) that integrates entire IMC images and multiple PVs for end-to-end survival prediction of breast cancer. We propose a multimodal graph-based module that considers relationships between spatial phenotype information in all image regions and all PVs, and scales each region\u2013PV pair based on its relevance to survival. We propose another module to automatically generate embeddings specialised for each PV to enhance multimodal aggregation. We show that our modules are consistently effective at improving survival prediction performance using two public datasets, and that DMGN can be applied to an independent validation dataset across the same antigens but different antibody clones. Our DMGN outperformed state-of-the-art methods at survival prediction.",
            "fieldsOfStudy": [
                "Biology",
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "40862833",
                    "name": "Xiaohang Fu"
                },
                {
                    "authorId": "144445312",
                    "name": "E. Patrick"
                },
                {
                    "authorId": "2169127811",
                    "name": "Jean Yee Hwa Yang"
                },
                {
                    "authorId": "2105584624",
                    "name": "D. Feng"
                },
                {
                    "authorId": "2146416796",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "eec5af2128f4c2faaa0bdf701fa5baac7df807a5",
            "title": "Identification of Disease or Symptom terms in Reddit to Improve Health Mention Classification",
            "abstract": "In a user-generated text such as on social media platforms and online forums, people often use disease or symptom terms in ways other than to describe their health. In data-driven public health surveillance, the health mention classification (HMC) task aims to identify posts where users are discussing health conditions rather than using disease and symptom terms for other reasons. Existing computational research typically only studies health mentions in Twitter, with limited coverage of disease or symptom terms, ignore user behavior information, and other ways people use disease or symptom terms. To advance the HMC research, we present a Reddit health mention dataset (RHMD), a new dataset of multi-domain Reddit data for the HMC. RHMD consists of 10,015 manually labeled Reddit posts that mention 15 common disease or symptom terms and are annotated with four labels: namely personal health mentions, non-personal health mentions, figurative health mentions, and hyperbolic health mentions. With RHMD, we propose HMCNET that combines a target keyword (disease or symptom term) identification and user behavior hierarchically to improve HMC. Experimental results demonstrate that the proposed approach outperforms state-of-the-art methods with an F1-Score of 0.75 (an increase of 11% over the state-of-the-art) and shows that our new dataset poses a strong challenge to the existing HMC methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "2146416796",
                    "name": "Jinman Kim"
                },
                {
                    "authorId": "4631130",
                    "name": "Matloob Khushi"
                },
                {
                    "authorId": "34318729",
                    "name": "A. Dunn"
                }
            ]
        }
    ]
}