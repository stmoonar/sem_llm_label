{
    "authorId": "40499153",
    "papers": [
        {
            "paperId": "6a22e0a7c3074bb15649c16dfc2a83ec0096f500",
            "title": "Textual Analysis and Timely Detection of Suspended Social Media Accounts",
            "abstract": "Suspended accounts are high-risk accounts that violate the rules of a social network. These accounts contain spam, offensive and explicit language, among others, and are incredibly variable in terms of textual content. In this work, we perform a detailed linguistic and statistical analysis into the textual information of suspended accounts and show how insights from our study significantly improve a deep-learning-based detection framework. Moreover, we investigate the utility of advanced topic modeling for the automatic creation of word lists that can discriminate suspended from regular accounts. Since early detection of these high-risk accounts is crucial, we evaluate multiple state-of-the-art classification models along the temporal dimension by measuring the minimum amount of textual signal needed to perform reliable predictions. Further, we show that the best performing models are able to detect suspended accounts earlier than the social media platform.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40499153",
                    "name": "Dominic Seyler"
                },
                {
                    "authorId": "144734908",
                    "name": "Shulong Tan"
                },
                {
                    "authorId": "34377382",
                    "name": "Dingcheng Li"
                },
                {
                    "authorId": "2144192495",
                    "name": "Jingyuan Zhang"
                },
                {
                    "authorId": "1739405774",
                    "name": "Ping Li"
                }
            ]
        },
        {
            "paperId": "8cb7b15c104f387cf74652af18147e70c729d45c",
            "title": "DarkJargon.net: A Platform for Understanding Underground Conversation with Latent Meaning",
            "abstract": "An essential part of underground conversation are dark jargon terms. They are benign-looking, but have hidden, sometimes sinister meanings and are used by participants of underground forums for illicit behavior. For example, the dark term \"rat\" is often used in lieu of \"Remote Access Trojan\". We present a novel online platform that caters to the understating of underground conversation with latent meaning. Our system enables researchers, law enforcement agents and \"white-hat\" hackers to gain invaluable insights into underground communication by providing them with a tool to (1) look-up dark jargon terms in a dictionary; (2) explore the usage of dark jargon over time and interpret their meaning; (3) collaborate and contribute their own research findings. Furthermore, we introduce a novel dark jargon interpretation method that leverages masked language modeling of a transformer-based architecture.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40499153",
                    "name": "Dominic Seyler"
                },
                {
                    "authorId": "2157220286",
                    "name": "Wei Liu"
                },
                {
                    "authorId": "2108299318",
                    "name": "Yunan Zhang"
                },
                {
                    "authorId": "50141047",
                    "name": "Xiaofeng Wang"
                },
                {
                    "authorId": "1736467",
                    "name": "ChengXiang Zhai"
                }
            ]
        },
        {
            "paperId": "2b43560f4bc9f04be2b57c5a9f0d2aa4d61f89df",
            "title": "Finding Contextually Consistent Information Units in Legal Text",
            "abstract": "Terms in the laws of a legislature can be highly contextual: especially for corpora of codified laws and regulations where the reader has to be aware of the correct context when the corpus lacks a single level of hierarchy. The goal of this work is to assist professionals when reading legal text within a codified corpus by finding contex-tually consistent information units. To achieve this, we combine NLP and data mining techniques to develop novel methodology that can find these information units in an unsupervised manner. Our method draws on expert experience and is modeled to emulate the \u201ccontextualization process\u201d of experienced readers of legal content. We experimentally evaluate our method by comparing it to multiple expert-annotated datasets and find that our method achieves near perfect performance on four state corpora and high precision on one federal corpus.",
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "authors": [
                {
                    "authorId": "40499153",
                    "name": "Dominic Seyler"
                },
                {
                    "authorId": "2059250712",
                    "name": "P. Bruin"
                },
                {
                    "authorId": "1859549897",
                    "name": "Pavan Bayyapu"
                },
                {
                    "authorId": "143869012",
                    "name": "Chengxiang Zhai"
                }
            ]
        },
        {
            "paperId": "59e61adb9c272093d07c070178f4a4e07fb675c7",
            "title": "A Study of Methods for the Generation of Domain-Aware Word Embeddings",
            "abstract": "Word embeddings are essential components for many text data applications. In most work, \"out-of-the-box\" embeddings trained on general text corpora are used, but they can be less effective when applied to domain-specific settings. Thus, how to create \"domain-aware\" word embeddings is an interesting open research question. In this paper, we study three methods for creating domain-aware word embeddings based on both general and domain-specific text corpora, including concatenation of embedding vectors, weighted fusion of text data, and interpolation of aligned embedding vectors. Even though the investigated strategies are tailored for domain-specific tasks, they are general enough to be applied to any domain and are not specific to a single task. Experimental results show that all three methods can work well, however, the interpolation method consistently works best.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40499153",
                    "name": "Dominic Seyler"
                },
                {
                    "authorId": "143869012",
                    "name": "Chengxiang Zhai"
                }
            ]
        },
        {
            "paperId": "85ed6806f86be81647e9707d563ec3092c21060a",
            "title": "Leveraging Personalized Sentiment Lexicons for Sentiment Analysis",
            "abstract": "We propose a novel personalized approach for the sentiment analysis task. The approach is based on the intuition that the same sentiment words can carry different sentiment weights for different users. For each user, we learn a language model over a sentiment lexicon to capture her writing style. We further correlate this user-specific language model with the user's historical ratings of reviews. Additionally, we discuss how two standard CNN and CNN+LSTM models can be improved by adding these user-based features. Our evaluation on the Yelp dataset shows that the proposed new personalized sentiment analysis features are effective.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40499153",
                    "name": "Dominic Seyler"
                },
                {
                    "authorId": "3363642",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "46851930",
                    "name": "Jinfeng Xiao"
                },
                {
                    "authorId": "2107919851",
                    "name": "Yiren Wang"
                },
                {
                    "authorId": "143869012",
                    "name": "Chengxiang Zhai"
                }
            ]
        },
        {
            "paperId": "1f66bb1c2a5b48eac60481b3e72fb5f651d89e8b",
            "title": "Semantic Text Analysis for Detection of Compromised Accounts on Social Networks",
            "abstract": "Compromised accounts on social networks are regular user accounts that have been taken over by an entity with malicious intent. Since the adversary exploits the already established trust of a compromised account, it is crucial to detect these accounts to limit the damage they can cause. We propose a novel general framework for semantic analysis of text messages coming out from an account to detect compromised accounts. Our framework is built on the observation that normal users will use language that is measurably different from the language that an adversary would use when the account is compromised. We propose to use the difference of language models of users and adversaries to define novel interpretable semantic features for measuring semantic incoherence in a message stream. We study the effectiveness of the proposed semantic features using a Twitter data set. Evaluation results show that the proposed framework is effective for discovering compromised accounts on social networks and a KL-divergence-based language model feature works best.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40499153",
                    "name": "Dominic Seyler"
                },
                {
                    "authorId": "2114026131",
                    "name": "Lunan Li"
                },
                {
                    "authorId": "143869012",
                    "name": "Chengxiang Zhai"
                }
            ]
        },
        {
            "paperId": "3c74d949b280c762199419e9a7ed94a72cee1a4b",
            "title": "Identifying Compromised Accounts on Social Media Using Statistical Text Analysis",
            "abstract": "Compromised social media accounts are legitimate user accounts that have been hijacked by a third (malicious) party and can cause various kinds of damage. Early detection of such compromised accounts is very important in order to control the damage. In this work we propose a novel general framework for discovering compromised accounts by utilizing statistical text analysis. The framework is built on the observation that users will use language that is measurably different from the language that a hacker (or spammer) would use, when the account is compromised. We use the framework to develop specific algorithms based on language modeling and use the similarity of language models of users and spammers as features in a supervised learning setup to identify compromised accounts. Evaluation results on a large Twitter corpus of over 129 million tweets show promising results of the proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40499153",
                    "name": "Dominic Seyler"
                },
                {
                    "authorId": "2114026131",
                    "name": "Lunan Li"
                },
                {
                    "authorId": "1736467",
                    "name": "ChengXiang Zhai"
                }
            ]
        },
        {
            "paperId": "8ee6ea7f38502d3cf43aec17174c44a923868e40",
            "title": "A Study of the Importance of External Knowledge in the Named Entity Recognition Task",
            "abstract": "In this work, we discuss the importance of external knowledge for performing Named Entity Recognition (NER). We present a novel modular framework that divides the knowledge into four categories according to the depth of knowledge they convey. Each category consists of a set of features automatically generated from different information sources, such as a knowledge-base, a list of names, or document-specific semantic annotations. Further, we show the effects on performance when incrementally adding deeper knowledge and discuss effectiveness/efficiency trade-offs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40499153",
                    "name": "Dominic Seyler"
                },
                {
                    "authorId": "26436262",
                    "name": "Tatiana Dembelova"
                },
                {
                    "authorId": "1875906",
                    "name": "Luciano Del Corro"
                },
                {
                    "authorId": "1727527",
                    "name": "Johannes Hoffart"
                },
                {
                    "authorId": "1751591",
                    "name": "G. Weikum"
                }
            ]
        },
        {
            "paperId": "9e226ec1b1b577b109ac272f5f79c87f5097861a",
            "title": "An Information Retrieval Framework for Contextual Suggestion Based on Heterogeneous Information Network Embeddings",
            "abstract": "We present an Information Retrieval framework that leverages Heterogeneous Information Network (HIN) embeddings for contextual suggestion. Our method represents users, documents and other context-related documents as heterogeneous objects in a HIN. Using meta-paths, selected based on domain knowledge, we create graph embeddings from this network, thereby learning a representation of users and objects in the same semantic vector space. This allows inferences of user interest on unseen objects based on distance in the embedding space. These object distances are then incorporated as features in a well-established learning to rank (LTR) framework. We make use of the 2016 TREC Contextual Suggestion (TRECCS) dataset, which contains user profiles in the form of relevance-rated documents, and demonstrate the competitiveness of our approach by comparing our system to the best performing systems of the TRECCS task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40499153",
                    "name": "Dominic Seyler"
                },
                {
                    "authorId": "2667305",
                    "name": "Praveen Chandar"
                },
                {
                    "authorId": "2110852990",
                    "name": "Matthew Davis"
                }
            ]
        },
        {
            "paperId": "07087cff8fff0f5e12880c66aba196a60df2719f",
            "title": "A Study of Feature Construction for Text-based Forecasting of Time Series Variables",
            "abstract": "Time series are ubiquitous in the world since they are used to measure various phenomena (e.g., temperature, spread of a virus, sales, etc.). Forecasting of time series is highly beneficial (and necessary) for optimizing decisions, yet is a very challenging problem; using only the historical values of the time series is often insufficient. In this paper, we study how to construct effective additional features based on related text data for time series forecasting. Besides the commonly used n-gram features, we propose a general strategy for constructing multiple topical features based on the topics discovered by a topic model. We evaluate feature effectiveness using a data set for predicting stock price changes where we constructed additional features from news text articles for stock market prediction. We found that: 1) Text-based features outperform time series-based features, suggesting the great promise of leveraging text data for improving time series forecasting. 2) Topic-based features are not very effective stand-alone, but they can further improve performance when added on top of n-gram features. 3) The best topic-based feature appears to be a long-term aggregation of topics over time with high weights on recent topics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7707551",
                    "name": "Yiren Wang"
                },
                {
                    "authorId": "40499153",
                    "name": "Dominic Seyler"
                },
                {
                    "authorId": "2692077",
                    "name": "Shubhra (Santu) Karmaker"
                },
                {
                    "authorId": "1736467",
                    "name": "ChengXiang Zhai"
                }
            ]
        }
    ]
}