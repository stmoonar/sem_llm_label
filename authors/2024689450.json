{
    "authorId": "2024689450",
    "papers": [
        {
            "paperId": "434e36dc18464cb217d62c81dc8fc6a67d4ec127",
            "title": "Cumulative differences between paired samples",
            "abstract": "The simplest, most common paired samples consist of observations from two populations, with each observed response from one population corresponding to an observed response from the other population at the same value of an ordinal covariate. The pair of observed responses (one from each population) at the same value of the covariate is known as a\"matched pair\"(with the matching based on the value of the covariate). A graph of cumulative differences between the two populations reveals differences in responses as a function of the covariate. Indeed, the slope of the secant line connecting two points on the graph becomes the average difference over the wide interval of values of the covariate between the two points; i.e., slope of the graph is the average difference in responses. (\"Average\"refers to the weighted average if the samples are weighted.) Moreover, a simple statistic known as the Kuiper metric summarizes into a single scalar the overall differences over all values of the covariate. The Kuiper metric is the absolute value of the total difference in responses between the two populations, totaled over the interval of values of the covariate for which the absolute value of the total is greatest. The total should be normalized such that it becomes the (weighted) average over all values of the covariate when the interval over which the total is taken is the entire range of the covariate (i.e., the sum for the total gets divided by the total number of observations, if the samples are unweighted, or divided by the total weight, if the samples are weighted). This cumulative approach is fully nonparametric and uniquely defined (with only one right way to construct the graphs and scalar summary statistics), unlike traditional methods such as reliability diagrams or parametric or semi-parametric regressions, which typically obscure significant differences due to their parameter settings.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2207049",
                    "name": "Isabel M. Kloumann"
                },
                {
                    "authorId": "103405110",
                    "name": "Hannah Korevaar"
                },
                {
                    "authorId": "2217959550",
                    "name": "Chris McConnell"
                },
                {
                    "authorId": "2973721",
                    "name": "M. Tygert"
                },
                {
                    "authorId": "2024689450",
                    "name": "Jessica Zhao"
                }
            ]
        },
        {
            "paperId": "81d75fd1b255874ffb7f76bc784309280230e631",
            "title": "When does the student surpass the teacher? Federated Semi-supervised Learning with Teacher-Student EMA",
            "abstract": "Semi-Supervised Learning (SSL) has received extensive attention in the domain of computer vision, leading to development of promising approaches such as FixMatch. In scenarios where training data is decentralized and resides on client devices, SSL must be integrated with privacy-aware training techniques such as Federated Learning. We consider the problem of federated image classi\ufb01cation and study the performance and privacy challenges with existing federated SSL (FSSL) approaches. Firstly, we note that even state-of-the-art FSSL algorithms can trivially compromise client privacy and other real-world constraints such as client statelessness and communication cost. Secondly, we observe that it is challenging to integrate EMA (Exponential Moving Average) updates into the federated setting, which comes at a trade-off between performance and communication cost. We propose a novel approach FedSwitch , that improves privacy as well as generalization performance through Exponential Moving Average (EMA) updates. FedSwitch utilizes a federated semi-supervised teacher-student EMA framework with two features - local teacher adaptation and adaptive switching between teacher and student for pseudo-label generation . Our proposed approach outperforms the state-of-the-art on federated image classi\ufb01cation, can be adapted to real-world constraints, and achieves good generalization performance with minimal communication cost overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2024689450",
                    "name": "Jessica Zhao"
                },
                {
                    "authorId": "2143032877",
                    "name": "Sayan Ghosh"
                },
                {
                    "authorId": "2528900",
                    "name": "Akash Bharadwaj"
                },
                {
                    "authorId": "2112464124",
                    "name": "Chih-Yao Ma"
                }
            ]
        },
        {
            "paperId": "1ba073ad15ef039e79c845eba68cb6706f402b50",
            "title": "The Design of the User Interfaces for Privacy Enhancements for Android",
            "abstract": "We present the design and design rationale for the user interfaces for Privacy Enhancements for Android (PE for Android). These UIs are built around two core ideas, namely that developers should explicitly declare the purpose of why sensitive data is being used, and these permission-purpose pairs should be split by first party and third party uses. We also present a taxonomy of purposes and ways of how these ideas can be deployed in the existing Android ecosystem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110688724",
                    "name": "Jason I. Hong"
                },
                {
                    "authorId": "1787635",
                    "name": "Yuvraj Agarwal"
                },
                {
                    "authorId": "2623167",
                    "name": "Matt Fredrikson"
                },
                {
                    "authorId": "2084548619",
                    "name": "Mike Czapik"
                },
                {
                    "authorId": "2084552894",
                    "name": "Shawn Hanna"
                },
                {
                    "authorId": "1786587",
                    "name": "S. Sahoo"
                },
                {
                    "authorId": "2084552899",
                    "name": "Judy Chun"
                },
                {
                    "authorId": "2084553001",
                    "name": "Won-Woo Chung"
                },
                {
                    "authorId": "2084552819",
                    "name": "Aniruddh Iyer"
                },
                {
                    "authorId": "2113950737",
                    "name": "Ally Liu"
                },
                {
                    "authorId": "2115340202",
                    "name": "Shen Lu"
                },
                {
                    "authorId": "2084548664",
                    "name": "Rituparna Roychoudhury"
                },
                {
                    "authorId": "2155616645",
                    "name": "Qian Wang"
                },
                {
                    "authorId": "2109456386",
                    "name": "Sha Wang"
                },
                {
                    "authorId": "2116422451",
                    "name": "Siqi Wang"
                },
                {
                    "authorId": "2084549921",
                    "name": "Vida Zhang"
                },
                {
                    "authorId": "2024689450",
                    "name": "Jessica Zhao"
                },
                {
                    "authorId": "2117807897",
                    "name": "Yuan Jiang"
                },
                {
                    "authorId": "2340572",
                    "name": "Haojian Jin"
                },
                {
                    "authorId": "2110027335",
                    "name": "Sam Kim"
                },
                {
                    "authorId": "2084552778",
                    "name": "Evelyn Kuo"
                },
                {
                    "authorId": "3257288",
                    "name": "Tianshi Li"
                },
                {
                    "authorId": "2108531471",
                    "name": "Jinping Liu"
                },
                {
                    "authorId": "2108112642",
                    "name": "Yile Liu"
                },
                {
                    "authorId": "30621009",
                    "name": "Robert Zhang"
                }
            ]
        },
        {
            "paperId": "bea1187a1f8a68f1a93f0c2fa10d31f93a30f84e",
            "title": "Opacus: User-Friendly Differential Privacy Library in PyTorch",
            "abstract": "We introduce Opacus, a free, open-source PyTorch library for training deep learning models with differential privacy (hosted at opacus.ai). Opacus is designed for simplicity, flexibility, and speed. It provides a simple and user-friendly API, and enables machine learning practitioners to make a training pipeline private by adding as little as two lines to their code. It supports a wide variety of layers, including multi-head attention, convolution, LSTM, GRU (and generic RNN), and embedding, right out of the box and provides the means for supporting other user-defined layers. Opacus computes batched per-sample gradients, providing higher efficiency compared to the traditional\"micro batch\"approach. In this paper we present Opacus, detail the principles that drove its implementation and unique features, and benchmark it against other frameworks for training models with differential privacy as well as standard PyTorch.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2107059646",
                    "name": "I. Shilov"
                },
                {
                    "authorId": "2319225824",
                    "name": "Alexandre Sablayrolles"
                },
                {
                    "authorId": "1389630028",
                    "name": "Davide Testuggine"
                },
                {
                    "authorId": "2107060033",
                    "name": "Karthik Prasad"
                },
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                },
                {
                    "authorId": "2131399147",
                    "name": "John Nguyen"
                },
                {
                    "authorId": "2129469303",
                    "name": "Sayan Gosh"
                },
                {
                    "authorId": "2528900",
                    "name": "Akash Bharadwaj"
                },
                {
                    "authorId": "2024689450",
                    "name": "Jessica Zhao"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                }
            ]
        },
        {
            "paperId": "3380d6f2ddfbdfc53444f55c6d34f1ccb344d238",
            "title": "FedCD: Improving Performance in non-IID Federated Learning",
            "abstract": "Federated learning has been widely applied to enable decentralized devices, which each have their own local data, to learn a shared model. However, learning from real-world data can be challenging, as it is rarely identically and independently distributed (IID) across edge devices (a key assumption for current high-performing and low-bandwidth algorithms). We present a novel approach, FedCD, which clones and deletes models to dynamically group devices with similar data. Experiments on the CIFAR-10 dataset show that FedCD achieves higher accuracy and faster convergence compared to a FedAvg baseline on non-IID data while incurring minimal computation, communication, and storage overheads.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1751654639",
                    "name": "Kavya Kopparapu"
                },
                {
                    "authorId": "2062311951",
                    "name": "Eric Lin"
                },
                {
                    "authorId": "2024689450",
                    "name": "Jessica Zhao"
                }
            ]
        }
    ]
}