{
    "authorId": "2939577",
    "papers": [
        {
            "paperId": "01fac76c587a5a28c2646d100bc4bc1ae1048679",
            "title": "CQSumDP: A ChatGPT-Annotated Resource for Query-Focused Abstractive Summarization Based on Debatepedia",
            "abstract": "Debatepedia is a publicly available dataset consisting of arguments and counter-arguments on controversial topics that has been widely used for the single-document query-focused abstractive summarization task in recent years. However, it has been recently found that this dataset is limited by noise and even most queries in this dataset do not have any relevance to the respective document. In this paper, we present a methodology for cleaning the Debatepedia dataset by leveraging the generative power of large language models to make it suitable for query-focused abstractive summarization. More specifically, we harness the language generation capabilities of ChatGPT to regenerate its queries. We evaluate the effectiveness of the proposed ChatGPT annotated version of the Debatepedia dataset using several benchmark summarization models and demonstrate that the newly annotated version of Debatepedia outperforms the original dataset in terms of both query relevance as well as summary generation quality. We will make this annotated and cleaned version of the dataset publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46437970",
                    "name": "Md Tahmid Rahman Laskar"
                },
                {
                    "authorId": "2218664824",
                    "name": "Mizanur Rahman"
                },
                {
                    "authorId": "2216718110",
                    "name": "Israt Jahan"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "1683391",
                    "name": "J. Huang"
                }
            ]
        },
        {
            "paperId": "675e2b3429128a5823275a92637ab52a579dd479",
            "title": "SeeChart: Enabling Accessible Visualizations Through Interactive Natural Language Interface For People with Visual Impairments",
            "abstract": "Web-based data visualizations have become very popular for exploring data and communicating insights. Newspapers, journals, and reports regularly publish visualizations to tell compelling stories with data. Unfortunately, most visualizations are inaccessible to readers with visual impairments. For many charts on the web, there are no accompanying alternative (alt) texts, and even if such texts exist they do not adequately describe important insights from charts. To address the problem, we first interviewed 15 blind users to understand their challenges and requirements for reading data visualizations. Based on the insights from these interviews, we developed SeeChart, an interactive tool that automatically deconstructs charts from web pages and then converts them to accessible visualizations for blind people by enabling them to hear the chart summary as well as to interact through data points using the keyboard. Our evaluation with 14 blind participants suggests the efficacy of SeeChart in understanding key insights from charts and fulfilling their information needs while reducing their required time and cognitive burden.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9725274",
                    "name": "Md. Zubair Ibne Alam"
                },
                {
                    "authorId": "2150051064",
                    "name": "Shehnaz Islam"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                }
            ]
        },
        {
            "paperId": "8ab4863393fceb41d0fa77d632ace4c80a57a154",
            "title": "UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning",
            "abstract": "Charts are very popular for analyzing data, visualizing key insights and answering complex reasoning questions about data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, most of the methods that solve these tasks use pretraining on language or vision-language tasks that do not attempt to explicitly model the structure of the charts (e.g., how data is visually encoded and how chart elements are related to each other). To address this, we first build a large corpus of charts covering a wide variety of topics and visual styles. We then present UniChart, a pretrained model for chart comprehension and reasoning. UniChart encodes the relevant text, data, and visual elements of charts and then uses a chart-grounded text decoder to generate the expected output in natural language. We propose several chart-specific pretraining tasks that include: (i) low-level tasks to extract the visual elements (e.g., bars, lines) and data from charts, and (ii) high-level tasks to acquire chart understanding and reasoning skills. We find that pretraining the model on a large corpus with chart-specific low- and high-level tasks followed by finetuning on three down-streaming tasks results in state-of-the-art performance on three downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "2275056278",
                    "name": "P. Kavehzadeh"
                },
                {
                    "authorId": "2060491855",
                    "name": "Do Xuan Long"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ]
        },
        {
            "paperId": "dd41f3d40f89c93ba866ba482c3c8c617a0b0bad",
            "title": "BenLLM-Eval: A Comprehensive Evaluation into the Potentials and Pitfalls of Large Language Models on Bengali NLP",
            "abstract": "Large Language Models (LLMs) have emerged as one of the most important breakthroughs in natural language processing (NLP) for their impressive skills in language generation and other language-specific tasks. Though LLMs have been evaluated in various tasks, mostly in English, they have not yet undergone thorough evaluation in under-resourced languages such as Bengali (Bangla). To this end, this paper introduces BenLLM-Eval, which consists of a comprehensive evaluation of LLMs to benchmark their performance in the low-resourced Bangla language. In this regard, we select various important and diverse Bangla NLP tasks, such as text summarization, question answering, paraphrasing, natural language inference, text classification, and sentiment analysis for zero-shot evaluation of popular LLMs, namely, ChatGPT, LLaMA-2, and Claude-2. Our experimental results demonstrate that while in some Bangla NLP tasks, zero-shot LLMs could achieve performance on par, or even better than current SOTA fine-tuned models; in most tasks, their performance is quite poor (with the performance of open-source LLMs like LLaMA-2 being significantly bad) in comparison to the current SOTA results. Therefore, it calls for further efforts to develop a better understanding of LLMs in low-resource languages like Bangla.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2245159964",
                    "name": "M. Kabir"
                },
                {
                    "authorId": "2196189043",
                    "name": "Mohammed Saidul Islam"
                },
                {
                    "authorId": "46437970",
                    "name": "Md Tahmid Rahman Laskar"
                },
                {
                    "authorId": "1807355",
                    "name": "Mir Tafseer Nayeem"
                },
                {
                    "authorId": "31773000",
                    "name": "M Saiful Bari"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                }
            ]
        },
        {
            "paperId": "8870dd1ee67aa0e52275c3a7b5d8c26da81fb59c",
            "title": "Chart Question Answering: State of the Art and Future Directions",
            "abstract": "Information visualizations such as bar charts and line charts are very common for analyzing data and discovering critical insights. Often people analyze charts to answer questions that they have in mind. Answering such questions can be challenging as they often require a significant amount of perceptual and cognitive effort. Chart Question Answering (CQA) systems typically take a chart and a natural language question as input and automatically generate the answer to facilitate visual data analysis. Over the last few years, there has been a growing body of literature on the task of CQA. In this survey, we systematically review the current state\u2010of\u2010the\u2010art research focusing on the problem of chart question answering. We provide a taxonomy by identifying several important dimensions of the problem domain including possible inputs and outputs of the task and discuss the advantages and limitations of proposed solutions. We then summarize various evaluation techniques used in the surveyed papers. Finally, we outline the open challenges and future research opportunities related to chart question answering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "2275056278",
                    "name": "P. Kavehzadeh"
                },
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                }
            ]
        },
        {
            "paperId": "b611c501269224702d1a9942c8600a31ec66ab28",
            "title": "ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning",
            "abstract": "Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "2060491855",
                    "name": "Do Xuan Long"
                },
                {
                    "authorId": "101161623",
                    "name": "J. Tan"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                }
            ]
        },
        {
            "paperId": "ca2f63950685a97e5ab6b8e6b2db78a8995e94a2",
            "title": "Chart-to-Text: A Large-Scale Benchmark for Chart Summarization",
            "abstract": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2158815576",
                    "name": "Shankar Kanthara"
                },
                {
                    "authorId": "2158811816",
                    "name": "Rixie Tiffany Ko Leong"
                },
                {
                    "authorId": "144366156",
                    "name": "Xiang Lin"
                },
                {
                    "authorId": "100832735",
                    "name": "Ahmed Masry"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "2708940",
                    "name": "Shafiq R. Joty"
                }
            ]
        },
        {
            "paperId": "7e0c7fdad758482375cb89a110b2f5ad4bee57dd",
            "title": "Domain Adaptation with Pre-trained Transformers for Query-Focused Abstractive Text Summarization",
            "abstract": "Abstract The Query-Focused Text Summarization (QFTS) task aims at building systems that generate the summary of the text document(s) based on the given query. A key challenge in addressing this task is the lack of large labeled data for training the summarization model. In this article, we address this challenge by exploring a series of domain adaptation techniques. Given the recent success of pre-trained transformer models in a wide range of natural language processing tasks, we utilize such models to generate abstractive summaries for the QFTS task for both single-document and multi-document scenarios. For domain adaptation, we apply a variety of techniques using pre-trained transformer-based summarization models including transfer learning, weakly supervised learning, and distant supervision. Extensive experiments on six datasets show that our proposed approach is very effective in generating abstractive summaries for the QFTS task while setting a new state-of-the-art result in several datasets across a set of automatic and human evaluation metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46437970",
                    "name": "Md Tahmid Rahman Laskar"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "1683391",
                    "name": "J. Huang"
                }
            ]
        },
        {
            "paperId": "28307bc149a74cfcae657f782f1c7630b6f4acce",
            "title": "Contextualized Embeddings based Transformer Encoder for Sentence Similarity Modeling in Answer Selection Task",
            "abstract": "Word embeddings that consider context have attracted great attention for various natural language processing tasks in recent years. In this paper, we utilize contextualized word embeddings with the transformer encoder for sentence similarity modeling in the answer selection task. We present two different approaches (feature-based and fine-tuning-based) for answer selection. In the feature-based approach, we utilize two types of contextualized embeddings, namely the Embeddings from Language Models (ELMo) and the Bidirectional Encoder Representations from Transformers (BERT) and integrate each of them with the transformer encoder. We find that integrating these contextual embeddings with the transformer encoder is effective to improve the performance of sentence similarity modeling. In the second approach, we fine-tune two pre-trained transformer encoder models for the answer selection task. Based on our experiments on six datasets, we find that the fine-tuning approach outperforms the feature-based approach on all of them. Among our fine-tuning-based models, the Robustly Optimized BERT Pretraining Approach (RoBERTa) model results in new state-of-the-art performance across five datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46437970",
                    "name": "Md Tahmid Rahman Laskar"
                },
                {
                    "authorId": "6743849",
                    "name": "Xiangji Huang"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                }
            ]
        },
        {
            "paperId": "72ee4df58e385bb004b4c1c14e943ba99f00c371",
            "title": "Designing and Evaluating Multimodal Interactions for Facilitating Visual Analysis With Dashboards",
            "abstract": "Exploring and analyzing data using visualizations is at the heart of many decision-making tasks. Typically, people perform visual data analysis using mouse and touch interactions. While such interactions are often easy to use, they can be inadequate for users to express complex information and may require many steps to complete a task. Recently natural language interaction has emerged as a promising technique for supporting exploration with visualization, as the user can express a complex analytical question more easily. In this paper, we investigate how to synergistically combine language and mouse-based direct manipulations so that the weakness of one modality can be complemented by the other. To this end, we have developed a novel system, named Multimodal Interactions System for Visual Analysis (MIVA), that allows user to provide input using both natural language (e.g., through speech) and direct manipulation (e.g., through mouse or touch) and presents the answer accordingly. To answer the current question in the context of past interactions, the system incorporates previous utterances and direct manipulations made by the user within a finite-state model. The uniqueness of our approach is that unlike most previous approaches which typically support multimodal interactions with a single visualization, MIVA enables multimodal interactions with multiple coordinated visualizations of a dashboard that visually summarizes a dataset. We tested MIVA\u2019s applicability on several dashboards including a COVID-19 dashboard that visualizes coronavirus cases around the globe. We further empirically evaluated our system through a user study with twenty participants. The results of our study revealed that MIVA system enhances the flow of visual analysis by enabling fluid, iterative exploration and refinement of data in a dashboard with multiple-coordinated views.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065311128",
                    "name": "I. Chowdhury"
                },
                {
                    "authorId": "2045775640",
                    "name": "Abdul Moeid"
                },
                {
                    "authorId": "2939577",
                    "name": "Enamul Hoque"
                },
                {
                    "authorId": "3207804",
                    "name": "M. A. Kabir"
                },
                {
                    "authorId": "2110865712",
                    "name": "Md. Sabir Hossain"
                },
                {
                    "authorId": "152993087",
                    "name": "Mohammad Mainul Islam"
                }
            ]
        }
    ]
}