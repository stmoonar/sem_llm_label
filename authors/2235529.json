{
    "authorId": "2235529",
    "papers": [
        {
            "paperId": "0ad61806b055af61b59595d3ce2ab185da279a91",
            "title": "Enable Natural Tactile Interaction for Robot Dog based on Large-format Distributed Flexible Pressure Sensors",
            "abstract": "Touch is an important channel for human-robot interaction, while it is challenging for robots to recognize human touch accurately and make appropriate responses. In this paper, we design and implement a set of large-format distributed flexible pressure sensors on a robot dog to enable natural human-robot tactile interaction. Through a heuristic study, we sorted out 81 tactile gestures commonly used when humans interact with real dogs and 44 dog reactions. A gesture classification algorithm based on ResNet is proposed to recognize these 81 human gestures, and the classification accuracy reaches 98.7%. In addition, an action prediction algorithm based on Transformer is proposed to predict dog actions from human gestures, reaching a 1-gram BLEU score of 0.87. Finally, we compare the tactile interaction with the voice interaction during a freedom human-robot-dog interactive playing study. The results show that tactile interaction plays a more significant role in alleviating user anxiety, stimulating user excitement and improving the acceptability of robot dogs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052114898",
                    "name": "Lishuang Zhan"
                },
                {
                    "authorId": "2211587171",
                    "name": "Yancheng Cao"
                },
                {
                    "authorId": "2211591151",
                    "name": "Qitai Chen"
                },
                {
                    "authorId": "2176147031",
                    "name": "Haole Guo"
                },
                {
                    "authorId": "1996101908",
                    "name": "Jiasi Gao"
                },
                {
                    "authorId": "26889828",
                    "name": "Yiyue Luo"
                },
                {
                    "authorId": "2157301149",
                    "name": "Shihui Guo"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                }
            ]
        },
        {
            "paperId": "1baa0e7f4d84eb571b26dc4f8f31ba9c9df459d2",
            "title": "\"I am the follower, also the boss\": Exploring Different Levels of Autonomy and Machine Forms of Guiding Robots for the Visually Impaired",
            "abstract": "Guiding robots, in the form of canes or cars, have recently been explored to assist blind and low vision (BLV) people. Such robots can provide full or partial autonomy when guiding. However, the pros and cons of different forms and autonomy for guiding robots remain unknown. We sought to fill this gap. We designed autonomy-switchable guiding robotic cane and car. We conducted a controlled lab-study (N=12) and a field study (N=9) on BLV. Results showed that full autonomy received better walking performance and subjective ratings in the controlled study, whereas participants used more partial autonomy in the natural environment as demanding more control. Besides, the car robot has demonstrated abilities to provide a higher sense of safety and navigation efficiency compared with the cane robot. Our findings offered empirical evidence about how the BLV community perceived different machine forms and autonomy, which can inform the design of assistive robots.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39509574",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2187938715",
                    "name": "ZiangL Li"
                },
                {
                    "authorId": "2176147031",
                    "name": "Haole Guo"
                },
                {
                    "authorId": "2188233907",
                    "name": "Luyao Wang"
                },
                {
                    "authorId": "2187935139",
                    "name": "Qihe Chen"
                },
                {
                    "authorId": "2116486701",
                    "name": "Wen-Wen Jiang"
                },
                {
                    "authorId": "2204644498",
                    "name": "Mingming Fan"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                }
            ]
        },
        {
            "paperId": "4afd5803653d36bcb3da62baf9c58d688648da0b",
            "title": "Touch-and-Heal",
            "abstract": "Affective touch plays an important role in human-robot interaction. However, it is challenging for robots to perceive various natural human tactile gestures accurately, and feedback human intentions properly. In this paper, we propose a data-driven affective computing system based on a biomimetic quadruped robot with large-format, high-density flexible pressure sensors, which can mimic the natural tactile interaction between humans and pet dogs. We collect 208-minute videos from 26 participates and construct a dataset of 1212 human gestures-dog actions interaction sequences. The dataset is manually annotated with an 81-tactile-gesture vocabulary and a 44-corresponding-dog-reaction vocabulary, which are constructed through literature, questionnaire, and video observation. Then, we propose a deep learning algorithm pipeline with a gesture classification algorithm based on ResNet and an action prediction algorithm based on Transformer, which achieve the classification accuracy of 99.1% and the 1-gram BLEU score of 0.87 respectively. Finally, we conduct a field study to evaluate the emotion regulation effects through tactile affective interaction, and compare it with voice interaction. The results show that our system with tactile interaction plays a significant role in alleviating user anxiety, stimulating user excitement and improving the acceptability of robotic dogs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2157301149",
                    "name": "Shihui Guo"
                },
                {
                    "authorId": "2052114898",
                    "name": "Lishuang Zhan"
                },
                {
                    "authorId": "2211587171",
                    "name": "Yancheng Cao"
                },
                {
                    "authorId": "2209944245",
                    "name": "Chen Zheng"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                }
            ]
        },
        {
            "paperId": "d8d6542b4758cb80175349d76e0a28b7588cd775",
            "title": "Work with AI and Work for AI: Autonomous Vehicle Safety Drivers\u2019 Lived Experiences",
            "abstract": "The development of Autonomous Vehicle (AV) has created a novel job, the safety driver, recruited from experienced drivers to supervise and operate AV in numerous driving missions. Safety drivers usually work with non-perfect AV in high-risk real-world traffic environments for road testing tasks. However, this group of workers is under-explored in the HCI community. To fill this gap, we conducted semi-structured interviews with 26 safety drivers. Our results present how safety drivers cope with defective algorithms and shape and calibrate their perceptions while working with AV. We found that, as front-line workers, safety drivers are forced to take risks accumulated from the AV industry upstream and are also confronting restricted self-development in working for AV development. We contribute the first empirical evidence of the lived experience of safety drivers, the first passengers in the development of AV, and also the grassroots workers for AV, which can shed light on future human-AI interaction research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188740500",
                    "name": "Mengdi Chu"
                },
                {
                    "authorId": "2162137555",
                    "name": "Keyu Zong"
                },
                {
                    "authorId": "2211097318",
                    "name": "Xin Shu"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                },
                {
                    "authorId": "2110326898",
                    "name": "Zhicong Lu"
                },
                {
                    "authorId": "2211096377",
                    "name": "Kaimin Guo"
                },
                {
                    "authorId": "2200247503",
                    "name": "Xinyi Dai"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                }
            ]
        },
        {
            "paperId": "dd76ab5e69933e77de4d2089f7344c948f79f76d",
            "title": "Annotating Covert Hazardous Driving Scenarios Online: Utilizing Drivers' Electroencephalography (EEG) Signals",
            "abstract": "As autonomous driving systems prevail, it is becoming increasingly critical that the systems learn from databases containing fine-grained driving scenarios. Most databases currently available are human-annotated; they are expensive, time-consuming, and subject to behavioral biases. In this paper, we provide initial evidence supporting a novel technique utilizing drivers' electroencephalography (EEG) signals to implicitly label hazardous driving scenarios while passively viewing recordings of real-road driving, thus sparing the need for manual annotation and avoiding human annotators' behavioral biases during explicit report. We conducted an EEG experiment using real-life and animated recordings of driving scenarios and asked participants to report danger explicitly whenever necessary. Behavioral results showed the participants tended to report danger only when overt hazards (e.g., a vehicle or a pedestrian appearing unexpectedly from behind an occlusion) were in view. By contrast, their EEG signals were enhanced at the sight of both an overt hazard and a covert hazard (e.g., an occlusion signalling possible appearance of a vehicle or a pedestrian from behind). Thus, EEG signals were more sensitive to driving hazards than explicit reports. Further, the Time-Series AI (TSAI, [1]) successfully classified EEG signals corresponding to overt and covert hazards. We discuss future steps necessary to materialize the technique in real life.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2209944245",
                    "name": "Chen Zheng"
                },
                {
                    "authorId": "2209882264",
                    "name": "Muxiao Zi"
                },
                {
                    "authorId": "2116486701",
                    "name": "Wen-Wen Jiang"
                },
                {
                    "authorId": "2188740500",
                    "name": "Mengdi Chu"
                },
                {
                    "authorId": "39509574",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2312163",
                    "name": "Jirui Yuan"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                }
            ]
        },
        {
            "paperId": "f0a571aaf94d4e931215edd54ac19c88ab747005",
            "title": "MR.Brick: Designing A Remote Mixed-reality Educational Game System for Promoting Children\u2019s Social & Collaborative Skills",
            "abstract": "Children are one of the groups most influenced by COVID-19-related social distancing, and a lack of contact with peers can limit their opportunities to develop social and collaborative skills. However, remote socialization and collaboration as an alternative approach is still a great challenge for children. This paper presents MR.Brick, a Mixed Reality (MR) educational game system that helps children adapt to remote collaboration. A controlled experimental study involving 24 children aged six to ten was conducted to compare MR.Brick with the traditional video game by measuring their social and collaborative skills and analyzing their multi-modal playing behaviours. The results showed that MR.Brick was more conducive to children\u2019s remote collaboration experience than the traditional video game. Given the lack of training systems designed for children to collaborate remotely, this study may inspire interaction design and educational research in related fields.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2201818864",
                    "name": "Yudan Wu"
                },
                {
                    "authorId": "2201807734",
                    "name": "Shanhe You"
                },
                {
                    "authorId": "13358715",
                    "name": "Zixuan Guo"
                },
                {
                    "authorId": "2201857954",
                    "name": "Xiangyang Li"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                }
            ]
        },
        {
            "paperId": "05764925e881adbb179d437f3f59c4c227f66131",
            "title": "Can Quadruped Navigation Robots be Used as Guide Dogs?",
            "abstract": "\u2014Bionic robots are generally considered to have strong \ufb02exibility, adaptability, and stability. Their bionic forms are more likely to interact emotionally with people, which means obvious advantages as socially assistive robots. However, it has not been widely concerned and veri\ufb01ed in the blind and low vision community. In this paper, we explored the guiding performance and experience of bionic quadruped robots compared to wheeled robots. We invited visually impaired participants to complete a) the indoor straight & turn task and obstacle avoidance task in a laboratory environment; b) the outdoor real and complex environment. With the transition from indoor to outdoor, we found that the workload of the bionic quadruped robots changed to insigni\ufb01cant. Moreover, obvious temporal demand indoors changed to signi\ufb01cant mental demand outdoors. Also, there was no signi\ufb01cant advantage of quadruped robots in usability, trust, or satisfaction, which was ampli\ufb01ed outdoors. We concluded that walking noise and the gait of quadruped robots would limit the guiding effect to a certain extent, and the empathetic effect of its zoomorphic form for visually impaired people could not be fully re\ufb02ected. This paper provides evidence for the empirical research of bionic quadruped robots in the \ufb01eld of guiding VI people, pointing out their shortcomings in guiding performance and experience, and has good instructive value for the design of bionic guided robots in the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187935139",
                    "name": "Qihe Chen"
                },
                {
                    "authorId": "2188233907",
                    "name": "Luyao Wang"
                },
                {
                    "authorId": "40491415",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2187938715",
                    "name": "ZiangL Li"
                },
                {
                    "authorId": "2187932907",
                    "name": "Tingmin Yan"
                },
                {
                    "authorId": "2145903668",
                    "name": "Fan Wang"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                }
            ]
        },
        {
            "paperId": "43b529f03d32d0e4a2830ac6c3bc066cee733bf8",
            "title": "Understanding Embodied Reference with Touch-Line Transformer",
            "abstract": "We study embodied reference understanding, the task of locating referents using embodied gestural signals and language references. Human studies have revealed that objects referred to or pointed to do not lie on the elbow-wrist line, a common misconception; instead, they lie on the so-called virtual touch line. However, existing human pose representations fail to incorporate the virtual touch line. To tackle this problem, we devise the touch-line transformer: It takes as input tokenized visual and textual features and simultaneously predicts the referent's bounding box and a touch-line vector. Leveraging this touch-line prior, we further devise a geometric consistency loss that encourages the co-linearity between referents and touch lines. Using the touch-line as gestural information improves model performances significantly. Experiments on the YouRefIt dataset show our method achieves a +25.0% accuracy improvement under the 0.75 IoU criterion, closing 63.6% of the gap between model and human performances. Furthermore, we computationally verify prior human studies by showing that computational models more accurately locate referents when using the virtual touch line than when using the elbow-wrist line.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "98177814",
                    "name": "Y. Li"
                },
                {
                    "authorId": "66273773",
                    "name": "Xiaoxue Chen"
                },
                {
                    "authorId": "2176137981",
                    "name": "Hao Zhao"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "6363461",
                    "name": "F. Rossano"
                },
                {
                    "authorId": "2143347222",
                    "name": "Yixin Zhu"
                }
            ]
        },
        {
            "paperId": "c804842f38adc719553d2adc225a53cddcfce632",
            "title": "Evaluation of Pedestrian Safety in a High-Fidelity Simulation Environment Framework",
            "abstract": "Pedestrians' safety is a crucial factor in assessing autonomous driving scenarios. However, pedestrian safety evaluation is rarely considered by existing autonomous driving simulation platforms. This paper proposes a pedestrian safety evaluation method for autonomous driving, in which not only the collision events but also the conflict events together with the characteristics of pedestrians are fully considered. Moreover, to apply the pedestrian safety evaluation system, we construct a high-fidelity simulation framework embedded with pedestrian safety-critical characteristics. We demonstrate our simulation framework and pedestrian safety evaluation with a comparative experiment with two kinds of autonomous driving perception algorithms -- single-vehicle perception and vehicle-to-infrastructure (V2I) cooperative perception. The results show that our framework can evaluate different autonomous driving algorithms with detailed and quantitative pedestrian safety indexes. To this end, the proposed simulation method and framework can be used to access different autonomous driving algorithms and evaluate pedestrians' safety performance in future autonomous driving simulations, which can inspire more pedestrian-friendly autonomous driving algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188774806",
                    "name": "Longrui Chen"
                },
                {
                    "authorId": "39509574",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2116486701",
                    "name": "Wen-Wen Jiang"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                },
                {
                    "authorId": "2115734074",
                    "name": "Jiahao Shen"
                },
                {
                    "authorId": "2188740500",
                    "name": "Mengdi Chu"
                },
                {
                    "authorId": "2185491599",
                    "name": "Chuxuan Li"
                },
                {
                    "authorId": "2213448590",
                    "name": "Yifeng Pan"
                },
                {
                    "authorId": "2118898046",
                    "name": "Yifeng Shi"
                },
                {
                    "authorId": "2188739588",
                    "name": "Nairui Luo"
                },
                {
                    "authorId": "2118141841",
                    "name": "Xuming Gao"
                },
                {
                    "authorId": "2312163",
                    "name": "Jirui Yuan"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2188826021",
                    "name": "Yaqin Zhang"
                }
            ]
        },
        {
            "paperId": "e5c009ba023c50217abd2793a7a010e064bb30b9",
            "title": "Learning with Yourself: a Tangible Twin Robot System to Promote STEM Education",
            "abstract": "This paper presents a customized programmable robotic system, TanTwin (Tangible Twin), designed to promote STEM education for K-12 children. Firstly, TanTwin is implemented based on a wheel-robot with standard LEGO bricks. With several deep neural networks, a child can convert a captured portrait of himself/herself into standard LEGO bricks, therefore he/she can build a tangible twin robot of him-selflherself automatically. Besides, to adapt to the customized appearance, the corresponding visual element and content of the robotic system were also changed by a rule-based adaption algorithm. To demonstrate the effectiveness of TanTwin and to investigate whether tangible twin robots could contribute to children's learning, we conducted a controlled experimental study to compare learning with a TanTwin and with a standard robot system through measuring students' cognitive learning outcomes. The pre-/post- knowledge test results indicated that learning with a tangible twin robot leads to significantly better learning outcomes. Given the results, we validate our system and customization technology can promote STEM education.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1996101908",
                    "name": "Jiasi Gao"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2176147031",
                    "name": "Haole Guo"
                },
                {
                    "authorId": "2198399255",
                    "name": "Tong Qi"
                }
            ]
        }
    ]
}