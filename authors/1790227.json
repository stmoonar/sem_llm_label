{
    "authorId": "1790227",
    "papers": [
        {
            "paperId": "06135f875daa7131115995afcd0ad15c6c615a86",
            "title": "UPPAM: A Unified Pre-training Architecture for Political Actor Modeling based on Language",
            "abstract": "Modeling political actors is at the core of quantitative political science. Existing works have incorporated contextual information to better learn the representation of political actors for specific tasks through graph models. However, they are limited to the structure and objective of training settings and can not be generalized to all politicians and other tasks. In this paper, we propose a Unified Pre-training Architecture for Political Actor Modeling based on language (UPPAM). In UPPAM, we aggregate statements to represent political actors and learn the mapping from languages to representation, instead of learning the representation of particular persons. We further design structure-aware contrastive learning and behavior-driven contrastive learning tasks, to inject multidimensional information in the political context into the mapping. In this framework, we can profile political actors from different aspects and solve various downstream tasks. Experimental results demonstrate the effectiveness and capability of generalization of our method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2134027736",
                    "name": "Xinyi Mou"
                },
                {
                    "authorId": "2118602528",
                    "name": "Zhongyu Wei"
                },
                {
                    "authorId": "49346854",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                }
            ]
        },
        {
            "paperId": "192b5d572a3505c47c690b8d8c5dab3e884acfd7",
            "title": "Enhancing Contrastive Learning with Noise-Guided Attack: Towards Continual Relation Extraction in the Wild",
            "abstract": "The principle of continual relation extraction~(CRE) involves adapting to emerging novel relations while preserving od knowledge. While current endeavors in CRE succeed in preserving old knowledge, they tend to fail when exposed to contaminated data streams. We assume this is attributed to their reliance on an artificial hypothesis that the data stream has no annotation errors, which hinders real-world applications for CRE. Considering the ubiquity of noisy labels in real-world datasets, in this paper, we formalize a more practical learning scenario, termed as \\textit{noisy-CRE}. Building upon this challenging setting, we develop a noise-resistant contrastive framework named as \\textbf{N}oise-guided \\textbf{a}ttack in \\textbf{C}ontrative \\textbf{L}earning~(NaCL) to learn incremental corrupted relations. Compared to direct noise discarding or inaccessible noise relabeling, we present modifying the feature space to match the given noisy labels via attacking can better enrich contrastive representations. Extensive empirical validations highlight that NaCL can achieve consistent performance improvements with increasing noise rates, outperforming state-of-the-art baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116518450",
                    "name": "Ting Wu"
                },
                {
                    "authorId": "2213942971",
                    "name": "Jingyi Liu"
                },
                {
                    "authorId": "2058585152",
                    "name": "Rui Zheng"
                },
                {
                    "authorId": "47835189",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2067331064",
                    "name": "Tao Gui"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                }
            ]
        },
        {
            "paperId": "1e26b42669b060a3850e4766dea0db6e3c85cdec",
            "title": "Towards Understanding the Capability of Large Language Models on Code Clone Detection: A Survey",
            "abstract": "Code cloning, the duplication of code fragments, is common in software development. While some reuse aids productivity, excessive cloning hurts maintainability and introduces bugs. Hence, automatic code clone detection is vital. Meanwhile, large language models (LLMs) possess diverse code-related knowledge, making them versatile for various software engineering challenges. However, LLMs' performance in code clone detection is unclear and needs more study for accurate assessment. In this paper, we provide the first comprehensive evaluation of LLMs for clone detection, covering different clone types, languages, and prompts. We find advanced LLMs excel in detecting complex semantic clones, surpassing existing methods. Adding intermediate reasoning steps via chain-of-thought prompts noticeably enhances performance. Additionally, representing code as vector embeddings, especially with text encoders, effectively aids clone detection.Lastly, the ability of LLMs to detect code clones differs among various programming languages. Our study suggests that LLMs have potential for clone detection due to their language capabilities, offering insights for developing robust LLM-based methods to enhance software engineering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2042683163",
                    "name": "Shihan Dou"
                },
                {
                    "authorId": "1486063362",
                    "name": "Junjie Shan"
                },
                {
                    "authorId": "2223116448",
                    "name": "Haoxiang Jia"
                },
                {
                    "authorId": "2217948574",
                    "name": "Wenhao Deng"
                },
                {
                    "authorId": "2190751523",
                    "name": "Zhiheng Xi"
                },
                {
                    "authorId": "2241408708",
                    "name": "Wei He"
                },
                {
                    "authorId": "2109036133",
                    "name": "Yueming Wu"
                },
                {
                    "authorId": "2067331064",
                    "name": "Tao Gui"
                },
                {
                    "authorId": "46399556",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                }
            ]
        },
        {
            "paperId": "1f040c3a8d49f8e54169a0e07013692c7d58de4b",
            "title": "How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks",
            "abstract": "The GPT-3.5 models have demonstrated impressive performance in various Natural Language Processing (NLP) tasks, showcasing their strong understanding and reasoning capabilities. However, their robustness and abilities to handle various complexities of the open world have yet to be explored, which is especially crucial in assessing the stability of models and is a key aspect of trustworthy AI. In this study, we perform a comprehensive experimental analysis of GPT-3.5, exploring its robustness using 21 datasets (about 116K test samples) with 66 text transformations from TextFlint that cover 9 popular Natural Language Understanding (NLU) tasks. Our findings indicate that while GPT-3.5 outperforms existing fine-tuned models on some tasks, it still encounters significant robustness degradation, such as its average performance dropping by up to 35.74\\% and 43.59\\% in natural language inference and sentiment analysis tasks, respectively. We also show that GPT-3.5 faces some specific robustness challenges, including robustness instability, prompt sensitivity, and number sensitivity. These insights are valuable for understanding its limitations and guiding future research in addressing these challenges to enhance GPT-3.5's overall performance and generalization abilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2168731359",
                    "name": "Xuanting Chen"
                },
                {
                    "authorId": "2143616977",
                    "name": "Junjie Ye"
                },
                {
                    "authorId": "15190018",
                    "name": "Can Zu"
                },
                {
                    "authorId": "2072805812",
                    "name": "Nuo Xu"
                },
                {
                    "authorId": "2058585152",
                    "name": "Rui Zheng"
                },
                {
                    "authorId": "24859244",
                    "name": "Minlong Peng"
                },
                {
                    "authorId": "2119617269",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2067331064",
                    "name": "Tao Gui"
                },
                {
                    "authorId": "47835189",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                }
            ]
        },
        {
            "paperId": "25abc4e828aafe321cf250010323ec29b03d1f6b",
            "title": "Detecting Adversarial Samples through Sharpness of Loss Landscape",
            "abstract": "Deep neural networks (DNNs) have been proven to be sensitive towards perturbations on input samples, and previous works high-light that adversarial samples are even more vulnerable than normal ones. In this work, this phenomenon is illustrated from the perspective of sharpness via visualizing the input loss landscape of models. We \ufb01rst show that adversarial samples locate in steep and narrow local minima of the loss landscape ( high sharpness ) while normal samples, which differs distinctly from adversarial ones, reside in the loss surface that is more \ufb02atter ( low sharpness ). Based on this, we propose a simple and effective sharpness-based detector to distinct adversarial samples by maximizing the loss increment within the region where the inference sample is located. Considering that the notion of sharpness of a loss landscape is relative, we further propose an adaptive optimization strategy in an attempt to fairly compare the relative sharpness among different samples. Experimental results show that our approach can outperform previous detection methods by large margins (average +6.6 F1 score) for four advanced attack strategies considered in this paper across three text classi\ufb01cation tasks. Our codes are publicly available at https://github",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2058585152",
                    "name": "Rui Zheng"
                },
                {
                    "authorId": "2042683163",
                    "name": "Shihan Dou"
                },
                {
                    "authorId": "2212175381",
                    "name": "Yuhao Zhou"
                },
                {
                    "authorId": "2109185819",
                    "name": "Qin Liu"
                },
                {
                    "authorId": "2067331064",
                    "name": "Tao Gui"
                },
                {
                    "authorId": "47835189",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2118602528",
                    "name": "Zhongyu Wei"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                },
                {
                    "authorId": "2213627466",
                    "name": "Menghan Zhang"
                }
            ]
        },
        {
            "paperId": "2ebf8821ac370023734e9e7d6981c97a7f8c3bc4",
            "title": "KNSE: A Knowledge-aware Natural Language Inference Framework for Dialogue Symptom Status Recognition",
            "abstract": "Symptom diagnosis in medical conversations aims to correctly extract both symptom entities and their status from the doctor-patient dialogue. In this paper, we propose a novel framework called KNSE for symptom status recognition (SSR), where the SSR is formulated as a natural language inference (NLI) task. For each mentioned symptom in a dialogue window, we first generate knowledge about the symptom and hypothesis about status of the symptom, to form a (premise, knowledge, hypothesis) triplet. The BERT model is then used to encode the triplet, which is further processed by modules including utterance aggregation, self-attention, cross-attention, and GRU to predict the symptom status. Benefiting from the NLI formalization, the proposed framework can encode more informative prior knowledge to better localize and track symptom status, which can effectively improve the performance of symptom status recognition. Preliminary experiments on Chinese medical dialogue datasets show that KNSE outperforms previous competitive baselines and has advantages in cross-disease and cross-symptom scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47482777",
                    "name": "W. Chen"
                },
                {
                    "authorId": "2216430085",
                    "name": "Shiqi Wei"
                },
                {
                    "authorId": "2118602528",
                    "name": "Zhongyu Wei"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                }
            ]
        },
        {
            "paperId": "30fb9b771a59fec39c0ed06382cc5faee819b023",
            "title": "CausalAPM: Generalizable Literal Disentanglement for NLU Debiasing",
            "abstract": "Dataset bias, i.e., the over-reliance on dataset-specific literal heuristics, is getting increasing attention for its detrimental effect on the generalization ability of NLU models. Existing works focus on eliminating dataset bias by down-weighting problematic data in the training process, which induce the omission of valid feature information while mitigating bias. In this work, We analyze the causes of dataset bias from the perspective of causal inference and propose CausalAPM, a generalizable literal disentangling framework to ameliorate the bias problem from feature granularity. The proposed approach projects literal and semantic information into independent feature subspaces, and constrains the involvement of literal information in subsequent predictions. Extensive experiments on three NLP benchmarks (MNLI, FEVER, and QQP) demonstrate that our proposed framework significantly improves the OOD generalization performance while maintaining ID performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2181306462",
                    "name": "Songyang Gao"
                },
                {
                    "authorId": "2042683163",
                    "name": "Shihan Dou"
                },
                {
                    "authorId": "1486063362",
                    "name": "Junjie Shan"
                },
                {
                    "authorId": "49346854",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                }
            ]
        },
        {
            "paperId": "44dec1bde5b036f81eab7fa79c80ac2eb260c7bc",
            "title": "Connectivity Patterns are Task Embeddings",
            "abstract": "Task embeddings are task-specific vectors designed to construct a semantic space of tasks, which can be used to predict the most transferable source task for a given target task via the similarity between task embeddings. However, existing methods use optimized parameters and representations as task embeddings, resulting in substantial computational complexity and storage requirements. In this work, we draw inspiration from the operating mechanism of deep neural networks (DNNs) and biological brains, where neuronal activations are sparse and task-specific, and we use the connectivity patterns of neurons as a unique identifier associated with the task. The proposed method learns to assign importance masks for sub-structures of DNNs, and accordingly indicate the task-specific connectivity patterns. In addition to the storage advantages brought by the binary masking mechanism and structured sparsity, the early-bird nature of the sparse optimization process can deliver an efficient computation advantage. Experiments show that our method consistently outperforms other baselines in predicting inter-task transferability across data regimes and transfer settings, while keeping high efficiency in computation and storage.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2190751523",
                    "name": "Zhiheng Xi"
                },
                {
                    "authorId": "2058585152",
                    "name": "Rui Zheng"
                },
                {
                    "authorId": "2213635839",
                    "name": "Yuan Zhang"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                },
                {
                    "authorId": "2118602528",
                    "name": "Zhongyu Wei"
                },
                {
                    "authorId": "24859244",
                    "name": "Minlong Peng"
                },
                {
                    "authorId": "2219726324",
                    "name": "Mingming Sun"
                },
                {
                    "authorId": "47835189",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2067331064",
                    "name": "Tao Gui"
                }
            ]
        },
        {
            "paperId": "47d69cf2a9d8acaefab20b70d70a6df9e8011a0a",
            "title": "Efficient Link Prediction via GNN Layers Induced by Negative Sampling",
            "abstract": "Graph neural networks (GNNs) for link prediction can loosely be divided into two broad categories. First, \\emph{node-wise} architectures pre-compute individual embeddings for each node that are later combined by a simple decoder to make predictions. While extremely efficient at inference time (since node embeddings are only computed once and repeatedly reused), model expressiveness is limited such that isomorphic nodes contributing to candidate edges may not be distinguishable, compromising accuracy. In contrast, \\emph{edge-wise} methods rely on the formation of edge-specific subgraph embeddings to enrich the representation of pair-wise relationships, disambiguating isomorphic nodes to improve accuracy, but with the cost of increased model complexity. To better navigate this trade-off, we propose a novel GNN architecture whereby the \\emph{forward pass} explicitly depends on \\emph{both} positive (as is typical) and negative (unique to our approach) edges to inform more flexible, yet still cheap node-wise embeddings. This is achieved by recasting the embeddings themselves as minimizers of a forward-pass-specific energy function (distinct from the actual training loss) that favors separation of positive and negative samples. As demonstrated by extensive empirical evaluations, the resulting architecture retains the inference speed of node-wise models, while producing competitive accuracy with edge-wise alternatives.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2258757827",
                    "name": "Yuxin Wang"
                },
                {
                    "authorId": "2197411811",
                    "name": "Xiannian Hu"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                },
                {
                    "authorId": "2256661980",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2256635267",
                    "name": "David Wipf"
                }
            ]
        },
        {
            "paperId": "4c5b4a8e31d3119c1e3b5753693ff283c9717218",
            "title": "DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation",
            "abstract": "We propose DISC-MedLLM, a comprehensive solution that leverages Large Language Models (LLMs) to provide accurate and truthful medical response in end-to-end conversational healthcare services. To construct high-quality Supervised Fine-Tuning (SFT) datasets, we employ three strategies: utilizing medical knowledge-graphs, reconstructing real-world dialogues, and incorporating human-guided preference rephrasing. These datasets are instrumental in training DISC-MedLLM, surpassing existing medical LLMs in both single-turn and multi-turn consultation scenarios. Extensive experimental results demonstrate the effectiveness of the proposed model in bridging the gap between general language models and real-world medical consultation. Additionally, we release the constructed dataset and model weights to further contribute to research and development. Further details and resources can be found at https://github.com/FudanDISC/DISC-MedLLM",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2234360519",
                    "name": "Zhijie Bao"
                },
                {
                    "authorId": "2256716476",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2234363077",
                    "name": "Shengze Xiao"
                },
                {
                    "authorId": "2234351785",
                    "name": "Kuang Ren"
                },
                {
                    "authorId": "2234503209",
                    "name": "Jiaao Wu"
                },
                {
                    "authorId": "2046752978",
                    "name": "Cheng Zhong"
                },
                {
                    "authorId": "2122807664",
                    "name": "J. Peng"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                },
                {
                    "authorId": "2118602528",
                    "name": "Zhongyu Wei"
                }
            ]
        }
    ]
}