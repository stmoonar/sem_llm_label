{
    "authorId": "1390533012",
    "papers": [
        {
            "paperId": "2dc55f83ec773698b6bdc8328df1df7a3e252750",
            "title": "Federated Domain Adaptation for Named Entity Recognition via Distilling with Heterogeneous Tag Sets",
            "abstract": "Federated learning involves collaborative training with private data from multiple platforms, while not violating data privacy. We study the problem of federated domain adaptation for Named Entity Recognition (NER), where we seek to transfer knowledge across different platforms with data of multiple domains. In addition, we consider a practical and challenging scenario, where NER datasets of different platforms of federated learning are annotated with heterogeneous tag sets, i.e. , different sets of entity types. The goal is to train a global model with federated learning, such that it can predict with a complete tag set, i.e. , with all the occurring entity types for data across all platforms. To cope with the heterogeneous tag sets in a multi-domain setting, we propose a distillation approach along with a mechanism of instance weighting to facilitate knowledge transfer across platforms. Besides, we release two re-annotated clinic NER datasets, for testing the proposed method in the clinic domain. Our method shows superior empirical performance for clinic NER with federated learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39077217",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "2146666303",
                    "name": "Junda Wu"
                },
                {
                    "authorId": "7574699",
                    "name": "Handong Zhao"
                },
                {
                    "authorId": "2187905618",
                    "name": "Sungchul Kim"
                },
                {
                    "authorId": "1390533012",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2112997744",
                    "name": "Subrata Mitra"
                },
                {
                    "authorId": "145153424",
                    "name": "Ricardo Henao"
                }
            ]
        },
        {
            "paperId": "3ce32d0142fa8d0c645c2f531afa4fe695570f9f",
            "title": "Understanding Demonstration-based Learning from a Causal Perspective",
            "abstract": "Demonstration-based learning has shown impressive performance in exploiting pretrained language models under few-shot learning settings. It is interesting to see that demonstrations, even those composed of random tokens, can still improve performance. In this paper, we build a Structural Causal Model (SCM) to understand demonstration-based learning from causal perspectives and interpret random demonstrations as interventions on the demonstration variable within the causal model. We investigate the causal effects and find that the concurrence of specific words in the demonstration will induce bias, while randomly sampled tokens in the demonstration do not. Based on this finding, we further propose simple ways to construct random demonstrations, which even outperform hand-crafted, meaningful demonstrations on public sequence labeling benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1390533012",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                }
            ]
        },
        {
            "paperId": "510f41c71a8751832cc97d79a0df673a0c2b0539",
            "title": "Few-Shot Composition Learning for Image Retrieval with Prompt Tuning",
            "abstract": "We study the problem of composition learning for image retrieval, for which we learn to retrieve target images with search queries in the form of a composition of a reference image and a modification text that describes desired modifications of the image. Existing models of composition learning for image retrieval are generally built with large-scale datasets, demanding extensive training samples, i.e., query-target pairs, as supervision, which restricts their application for the scenario of few-shot learning with only few query-target pairs available. Recently, prompt tuning with frozen pretrained language models has shown remarkable performance when the amount of training data is limited. Inspired by this, we propose a prompt tuning mechanism with the pretrained CLIP model for the task of few-shot composition learning for image retrieval. Specifically, we regard the representation of the reference image as a trainable visual prompt, prefixed to the embedding of the text sequence. One challenge is to efficiently train visual prompt with few-shot samples. To deal with this issue, we further propose a self-upervised auxiliary task via ensuring that the reference image can retrieve itself when no modification information is given from the text, which facilitates training for the visual prompt, while not requiring additional annotations for query-target pairs. Experiments on multiple benchmarks show that our proposed model can yield superior performance when trained with only few query-target pairs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146666303",
                    "name": "Junda Wu"
                },
                {
                    "authorId": "39077217",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "7574699",
                    "name": "Handong Zhao"
                },
                {
                    "authorId": "1390533012",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2312486",
                    "name": "Chaochao Lu"
                },
                {
                    "authorId": "1491648207",
                    "name": "Shuai Li"
                },
                {
                    "authorId": "145153424",
                    "name": "Ricardo Henao"
                }
            ]
        },
        {
            "paperId": "629f44f5fb78ec390ef66633dc627f1d04f3eb85",
            "title": "Knowledge Graph Prompting for Multi-Document Question Answering",
            "abstract": "The `pre-train, prompt, predict' paradigm of large language models (LLMs) has achieved remarkable success in open-domain question answering (OD-QA). However, few works explore this paradigm in multi-document question answering (MD-QA), a task demanding a thorough understanding of the logical associations among the contents and structures of documents. To fill this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to formulate the right context in prompting LLMs for MD-QA, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph (KG) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or document structural relations. For graph traversal, we design an LLM-based graph traversal agent that navigates across nodes and gathers supporting passages assisting LLMs in MD-QA. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the graph traversal agent acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the potential of leveraging graphs in enhancing the prompt design and retrieval augmented generation for LLMs. Our code: https://github.com/YuWVandy/KG-LLM-MDQA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153607948",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                },
                {
                    "authorId": "2066337266",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "2233085914",
                    "name": "Alexa F. Siu"
                },
                {
                    "authorId": "1390533012",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "12524628",
                    "name": "Tyler Derr"
                }
            ]
        },
        {
            "paperId": "6561ece074467a15e62e19bc4fd66a8e74de5eec",
            "title": "Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach",
            "abstract": "Recent text-to-image generation models have demonstrated impressive capability of generating text-aligned images with high fidelity. However, generating images of novel concept provided by the user input image is still a challenging task. To address this problem, researchers have been exploring various methods for customizing pre-trained text-to-image generation models. Currently, most existing methods for customizing pre-trained text-to-image generation models involve the use of regularization techniques to prevent over-fitting. While regularization will ease the challenge of customization and leads to successful content creation with respect to text guidance, it may restrict the model capability, resulting in the loss of detailed information and inferior performance. In this work, we propose a novel framework for customized text-to-image generation without the use of regularization. Specifically, our proposed framework consists of an encoder network and a novel sampling method which can tackle the over-fitting problem without the use of regularization. With the proposed framework, we are able to customize a large-scale text-to-image generation model within half a minute on single GPU, with only one image provided by the user. We demonstrate in experiments that our proposed framework outperforms existing methods, and preserves more fine-grained details.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1652060942",
                    "name": "Yufan Zhou"
                },
                {
                    "authorId": "1390533012",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2190051546",
                    "name": "Tongfei Sun"
                },
                {
                    "authorId": "2128691876",
                    "name": "Jinhui Xu"
                }
            ]
        },
        {
            "paperId": "6703296c8c3b8d18cec0a14dfa7999e5d93e4f46",
            "title": "Learning Navigational Visual Representations with Semantic Map Supervision",
            "abstract": "Being able to perceive the semantics and the spatial structure of the environment is essential for visual navigation of a household robot. However, most existing works only employ visual backbones pre-trained either with independent images for classification or with self-supervised learning methods to adapt to the indoor navigation domain, neglecting the spatial relationships that are essential to the learning of navigation. Inspired by the behavior that humans naturally build semantically and spatially meaningful cognitive maps in their brains during navigation, in this paper, we propose a novel navigational-specific visual representation learning method by contrasting the agent\u2019s egocentric views and semantic maps (Ego2-Map). We apply the visual transformer as the backbone encoder and train the model with data collected from the large-scale HabitatMatterport3D environments. Ego2-Map learning transfers the compact and rich information from a map, such as objects, structure and transition, to the agent\u2019s egocentric representations for navigation. Experiments show that agents using our learned representations on object-goal navigation outperform recent visual pre-training methods. Moreover, our representations significantly improve vision-and-language navigation in continuous environments for both high-level and low-level action spaces, achieving new state-of-the-art results of 47% SR and 41% SPL on the test server.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1612421029",
                    "name": "Yicong Hong"
                },
                {
                    "authorId": "144586052",
                    "name": "Yang Zhou"
                },
                {
                    "authorId": "1390533012",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "145262461",
                    "name": "Trung Bui"
                },
                {
                    "authorId": "47873182",
                    "name": "Stephen Gould"
                },
                {
                    "authorId": "47300698",
                    "name": "Hao Tan"
                }
            ]
        },
        {
            "paperId": "99a062c83e4ec3574f6a93c7b551f3ef5e3635fa",
            "title": "Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer in Prompt Tuning",
            "abstract": "In real-world scenarios, labeled samples for dialogue summarization are usually limited (i.e., few-shot) due to high annotation costs for high-quality dialogue summaries. To efficiently learn from few-shot samples, previous works have utilized massive annotated data from other downstream tasks and then performed prompt transfer in prompt tuning so as to enable cross-task knowledge transfer. However, existing general-purpose prompt transfer techniques lack consideration for dialogue-specific information. In this paper, we focus on improving the prompt transfer from dialogue state tracking to dialogue summarization and propose Skeleton-Assisted Prompt Transfer (SAPT), which leverages skeleton generation as extra supervision that functions as a medium connecting the distinct source and target task and resulting in the model\u2019s better consumption of dialogue state information. To automatically extract dialogue skeletons as supervised training data for skeleton generation, we design a novel approach with perturbation-based probes requiring neither annotation effort nor domain knowledge. Training the model on such skeletons can also help preserve model capability during prompt transfer. Our method significantly outperforms existing baselines. In-depth analyses demonstrate the effectiveness of our method in facilitating cross-task knowledge transfer in few-shot dialogue summarization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51145483",
                    "name": "Kaige Xie"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "2108873673",
                    "name": "Haoliang Wang"
                },
                {
                    "authorId": "2146666303",
                    "name": "Junda Wu"
                },
                {
                    "authorId": "7574699",
                    "name": "Handong Zhao"
                },
                {
                    "authorId": "1390533012",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "39015414",
                    "name": "K. Mahadik"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                },
                {
                    "authorId": "2065904932",
                    "name": "Mark O. Riedl"
                }
            ]
        },
        {
            "paperId": "a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3",
            "title": "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding",
            "abstract": "Instruction tuning unlocks the superior capability of Large Language Models (LLM) to interact with humans. Furthermore, recent instruction-following datasets include images as visual inputs, collecting responses for image-based instructions. However, visual instruction-tuned models cannot comprehend textual details within images well. This work enhances the current visual instruction tuning pipeline with text-rich images (e.g., movie posters, book covers, etc.). Specifically, we first use publicly available OCR tools to collect results on 422K text-rich images from the LAION dataset. Moreover, we prompt text-only GPT-4 with recognized texts and image captions to generate 16K conversations, each containing question-answer pairs for text-rich images. By combining our collected data with previous multi-modal instruction-following data, our model, LLaVAR, substantially improves the LLaVA model's capability on text-based VQA datasets (up to 20% accuracy improvement) while achieving an accuracy of 91.42% on ScienceQA. The GPT-4-based instruction-following evaluation also demonstrates the improvement of our model on both natural images and text-rich images. Through qualitative analysis, LLaVAR shows promising interaction (e.g., reasoning, writing, and elaboration) skills with humans based on the latest real-world online content that combines text and images. We make our code/data/models publicly available at https://llavar.github.io/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2121290295",
                    "name": "Yanzhe Zhang"
                },
                {
                    "authorId": "1390533012",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "1652060942",
                    "name": "Yufan Zhou"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                },
                {
                    "authorId": "2143919864",
                    "name": "Diyi Yang"
                },
                {
                    "authorId": "2190051546",
                    "name": "Tongfei Sun"
                }
            ]
        },
        {
            "paperId": "acf1dfa02eefcab63124869f152fd36d2aad172a",
            "title": "InfoPrompt: Information-Theoretic Soft Prompt Tuning for Natural Language Understanding",
            "abstract": "Soft prompt tuning achieves superior performances across a wide range of few-shot tasks. However, the performances of prompt tuning can be highly sensitive to the initialization of the prompts. We also empirically observe that conventional prompt tuning methods cannot encode and learn sufficient task-relevant information from prompt tokens. In this work, we develop an information-theoretic framework that formulates soft prompt tuning as maximizing mutual information between prompts and other model parameters (or encoded representations). This novel view helps us to develop a more efficient, accurate and robust soft prompt tuning method InfoPrompt. With this framework, we develop two novel mutual information based loss functions, to (i) discover proper prompt initialization for the downstream tasks and learn sufficient task-relevant information from prompt tokens and (ii) encourage the output representation from the pretrained language model to be more aware of the task-relevant information captured in the learnt prompt. Extensive experiments validate that InfoPrompt can significantly accelerate the convergence of the prompt tuning and outperform traditional prompt tuning methods. Finally, we provide a formal theoretical result for showing to show that gradient descent type algorithm can be used to train our mutual information loss.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2146666303",
                    "name": "Junda Wu"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "39077217",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "2214956470",
                    "name": "Zhao Song"
                },
                {
                    "authorId": "1390533012",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "7574699",
                    "name": "Handong Zhao"
                },
                {
                    "authorId": "2312486",
                    "name": "Chaochao Lu"
                },
                {
                    "authorId": "1491648207",
                    "name": "Shuai Li"
                },
                {
                    "authorId": "145153424",
                    "name": "Ricardo Henao"
                }
            ]
        },
        {
            "paperId": "4248120adf60d715cfb9ae2e95b4ef32f6b1678e",
            "title": "Learning Adaptive Axis Attentions in Fine-tuning: Beyond Fixed Sparse Attention Patterns",
            "abstract": "We present a comprehensive study of sparse attention patterns in Transformer models. We first question the need for pre-training with sparse attention and present experiments showing that an efficient fine-tuning only approach yields a slightly worse but still competitive model. Then we compare the widely used local attention pattern and the less-well-studied global attention pattern, demonstrating that global patterns have several unique advantages. We also demonstrate that a flexible approach to attention, with different patterns across different layers of the model, is beneficial for some tasks. Drawing on this insight, we propose a novel Adaptive Axis Attention method, which learns\u2014during fine-tuning\u2014different attention patterns for each Transformer layer depending on the downstream task. Rather than choosing a fixed attention pattern, the adaptive axis attention method identifies important tokens\u2014for each task and model layer\u2014and focuses attention on those. It does not require pre-training to accommodate the sparse patterns and demonstrates competitive and sometimes better performance against fixed sparse attention patterns that require resource-intensive pre-training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240689",
                    "name": "Zihan Wang"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "1859486",
                    "name": "Jason Kuen"
                },
                {
                    "authorId": "7574699",
                    "name": "Handong Zhao"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "1390533012",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                },
                {
                    "authorId": "1500530510",
                    "name": "Tong Sun"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        }
    ]
}