{
    "authorId": "2082545536",
    "papers": [
        {
            "paperId": "0d5005e50c7136a549dc35e38d389b3ef14f8b78",
            "title": "ShadowNet for Data-Centric Quantum System Learning",
            "abstract": "Understanding the dynamics of large quantum systems is hindered by the curse of dimensionality. Statistical learning offers new possibilities in this regime by neural-network protocols and classical shadows, while both methods have limitations: the former is plagued by the predictive uncertainty and the latter lacks the generalization ability. Here we propose a data-centric learning paradigm combining the strength of these two approaches to facilitate diverse quantum system learning (QSL) tasks. Particularly, our paradigm utilizes classical shadows along with other easily obtainable information of quantum systems to create the training dataset, which is then learnt by neural networks to unveil the underlying mapping rule of the explored QSL problem. Capitalizing on the generalization power of neural networks, this paradigm can be trained offline and excel at predicting previously unseen systems at the inference stage, even with few state copies. Besides, it inherits the characteristic of classical shadows, enabling memory-efficient storage and faithful prediction. These features underscore the immense potential of the proposed data-centric approach in discovering novel and large-scale quantum systems. For concreteness, we present the instantiation of our paradigm in quantum state tomography and direct fidelity estimation tasks and conduct numerical analysis up to 60 qubits. Our work showcases the profound prospects of data-centric artificial intelligence to advance QSL in a faithful and generalizable manner.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2265526113",
                    "name": "Yuxuan Du"
                },
                {
                    "authorId": "2130306785",
                    "name": "Yibo Yang"
                },
                {
                    "authorId": "121698214",
                    "name": "Tongliang Liu"
                },
                {
                    "authorId": "33383055",
                    "name": "Zhouchen Lin"
                },
                {
                    "authorId": "2931652",
                    "name": "Bernard Ghanem"
                },
                {
                    "authorId": "2082545536",
                    "name": "Dacheng Tao"
                }
            ]
        },
        {
            "paperId": "1e194b3c8bbe9bb0f1cd52966351b0dacee5a695",
            "title": "Multimodal deep representation learning for quantum cross-platform verification",
            "abstract": "Cross-platform verification, a critical undertaking in the realm of early-stage quantum computing, endeavors to characterize the similarity of two imperfect quantum devices executing identical algorithms, utilizing minimal measurements. While the random measurement approach has been instrumental in this context, the quasi-exponential computational demand with increasing qubit count hurdles its feasibility in large-qubit scenarios. To bridge this knowledge gap, here we introduce an innovative multimodal learning approach, recognizing that the formalism of data in this task embodies two distinct modalities: measurement outcomes and classical description of compiled circuits on explored quantum devices, both enriched with unique information. Building upon this insight, we devise a multimodal neural network to independently extract knowledge from these modalities, followed by a fusion operation to create a comprehensive data representation. The learned representation can effectively characterize the similarity between the explored quantum devices when executing new quantum algorithms not present in the training data. We evaluate our proposal on platforms featuring diverse noise models, encompassing system sizes up to 50 qubits. The achieved results demonstrate a three-orders-of-magnitude improvement in prediction accuracy compared to the random measurements and offer compelling evidence of the complementary roles played by each modality in cross-platform verification. These findings pave the way for harnessing the power of multimodal learning to overcome challenges in wider quantum system learning tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2087074206",
                    "name": "Yan Qian"
                },
                {
                    "authorId": "2265526113",
                    "name": "Yuxuan Du"
                },
                {
                    "authorId": "2265617104",
                    "name": "Zhenliang He"
                },
                {
                    "authorId": "2265528440",
                    "name": "Min-hsiu Hsieh"
                },
                {
                    "authorId": "2082545536",
                    "name": "Dacheng Tao"
                }
            ]
        },
        {
            "paperId": "2c2a0534d7da4f7a5d6e009ae73fe352f4ebfe22",
            "title": "Extending the Design Space of Graph Neural Networks by Rethinking Folklore Weisfeiler-Lehman",
            "abstract": "Message passing neural networks (MPNNs) have emerged as the most popular framework of graph neural networks (GNNs) in recent years. However, their expressive power is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Some works are inspired by $k$-WL/FWL (Folklore WL) and design the corresponding neural versions. Despite the high expressive power, there are serious limitations in this line of research. In particular, (1) $k$-WL/FWL requires at least $O(n^k)$ space complexity, which is impractical for large graphs even when $k=3$; (2) The design space of $k$-WL/FWL is rigid, with the only adjustable hyper-parameter being $k$. To tackle the first limitation, we propose an extension, $(k,t)$-FWL. We theoretically prove that even if we fix the space complexity to $O(n^k)$ (for any $k\\geq 2$) in $(k,t)$-FWL, we can construct an expressiveness hierarchy up to solving the graph isomorphism problem. To tackle the second problem, we propose $k$-FWL+, which considers any equivariant set as neighbors instead of all nodes, thereby greatly expanding the design space of $k$-FWL. Combining these two modifications results in a flexible and powerful framework $(k,t)$-FWL+. We demonstrate $(k,t)$-FWL+ can implement most existing models with matching expressiveness. We then introduce an instance of $(k,t)$-FWL+ called Neighborhood$^2$-FWL (N$^2$-FWL), which is practically and theoretically sound. We prove that N$^2$-FWL is no less powerful than 3-WL, and can encode many substructures while only requiring $O(n^2)$ space. Finally, we design its neural version named N$^2$-GNN and evaluate its performance on various tasks. N$^2$-GNN achieves record-breaking results on ZINC-Subset (0.059), outperforming previous SOTA results by 10.6%. Moreover, N$^2$-GNN achieves new SOTA results on the BREC dataset (71.8%) among all existing high-expressive GNN methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "48441034",
                    "name": "Jiarui Feng"
                },
                {
                    "authorId": "2164063663",
                    "name": "Lecheng Kong"
                },
                {
                    "authorId": "2264134998",
                    "name": "Hao Liu"
                },
                {
                    "authorId": "2082545536",
                    "name": "Dacheng Tao"
                },
                {
                    "authorId": "2146340100",
                    "name": "Fuhai Li"
                },
                {
                    "authorId": "3098251",
                    "name": "Muhan Zhang"
                },
                {
                    "authorId": "2116664181",
                    "name": "Yixin Chen"
                }
            ]
        },
        {
            "paperId": "430a5e44fc2fc16e2376ef7197101816688f2ca7",
            "title": "CktGNN: Circuit Graph Neural Network for Electronic Design Automation",
            "abstract": "The electronic design automation of analog circuits has been a longstanding challenge in the integrated circuit field due to the huge design space and complex design trade-offs among circuit specifications. In the past decades, intensive research efforts have mostly been paid to automate the transistor sizing with a given circuit topology. By recognizing the graph nature of circuits, this paper presents a Circuit Graph Neural Network (CktGNN) that simultaneously automates the circuit topology generation and device sizing based on the encoder-dependent optimization subroutines. Particularly, CktGNN encodes circuit graphs using a two-level GNN framework (of nested GNN) where circuits are represented as combinations of subgraphs in a known subgraph basis. In this way, it significantly improves design efficiency by reducing the number of subgraphs to perform message passing. Nonetheless, another critical roadblock to advancing learning-assisted circuit design automation is a lack of public benchmarks to perform canonical assessment and reproducible research. To tackle the challenge, we introduce Open Circuit Benchmark (OCB), an open-sourced dataset that contains $10$K distinct operational amplifiers with carefully-extracted circuit specifications. OCB is also equipped with communicative circuit generation and evaluation capabilities such that it can help to generalize CktGNN to design various analog circuits by producing corresponding datasets. Experiments on OCB show the extraordinary advantages of CktGNN through representation-based optimization frameworks over other recent powerful GNN baselines and human experts' manual designs. Our work paves the way toward a learning-based open-sourced design automation for analog circuits. Our source code is available at \\url{https://github.com/zehao-dong/CktGNN}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2004163392",
                    "name": "Zehao Dong"
                },
                {
                    "authorId": "47416277",
                    "name": "Weidong Cao"
                },
                {
                    "authorId": "3098251",
                    "name": "Muhan Zhang"
                },
                {
                    "authorId": "2082545536",
                    "name": "Dacheng Tao"
                },
                {
                    "authorId": "2116664181",
                    "name": "Yixin Chen"
                },
                {
                    "authorId": "1416524986",
                    "name": "Xuan Zhang"
                }
            ]
        },
        {
            "paperId": "4d4630ab76dafbdde809f132c41adf0c191e9ab3",
            "title": "The Visual Footsteps Planning System for Exoskeleton Robots Under Complex Terrain",
            "abstract": "The lower limb power-assist exoskeletons are expected to help paraplegic people to walk again in daily life. However, most of these exoskeletons deal with walking in the scene that has been seen or has an external vision sensor, rather than in the unknown environment. It is a great challenge to understand the wear\u2019s intention and plan the footstep sequence in an unknown scene. Moreover, the traditional visual footstep planning is dominated by the robot, which can lead to an awkward trajectory plan. Therefore, we construct a visual footstep planning system and propose an onboard vision planning algorithm based on the Bezier curve to address the previous two problems. Specially, our human\u2013computer interaction system understands the environment and the wearer\u2019s behavior intention by integrating Hololens and Realsense. Then, we apply the Bezier curve to plan footsteps for the first time in the field of the exoskeleton and define two parameters of the Bezier curve, which are more suitable for our exoskeleton system and could increase the planning speed. Finally, we add the tracking feature cost in the cost function, which could better fit the planned footprints to the planned path and make the gait smoother. Extended experimental results show that the average planning time is 67.46% less than that of the traditional search algorithm. Moreover, the effectiveness of our system is also verified on the visual interaction platform.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48661888",
                    "name": "Xin Wu"
                },
                {
                    "authorId": "46276250",
                    "name": "Jinke Li"
                },
                {
                    "authorId": "2109528221",
                    "name": "Liu Liu"
                },
                {
                    "authorId": "2082545536",
                    "name": "Dacheng Tao"
                }
            ]
        },
        {
            "paperId": "9931fdbf11fea0540bc0ae83e3a8cfcf7e6822d4",
            "title": "Understanding Deep Neural Networks via Linear Separability of Hidden Layers",
            "abstract": "In this paper, we measure the linear separability of hidden layer outputs to study the characteristics of deep neural networks. In particular, we first propose Minkowski difference based linear separability measures (MD-LSMs) to evaluate the linear separability degree of two points sets. Then, we demonstrate that there is a synchronicity between the linear separability degree of hidden layer outputs and the network training performance, i.e., if the updated weights can enhance the linear separability degree of hidden layer outputs, the updated network will achieve a better training performance, and vice versa. Moreover, we study the effect of activation function and network size (including width and depth) on the linear separability of hidden layers. Finally, we conduct the numerical experiments to validate our findings on some popular deep networks including multilayer perceptron (MLP), convolutional neural network (CNN), deep belief network (DBN), ResNet, VGGNet, AlexNet, vision transformer (ViT) and GoogLeNet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152735784",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "2109080997",
                    "name": "Xinyuan Chen"
                },
                {
                    "authorId": "2129492913",
                    "name": "Wensheng Li"
                },
                {
                    "authorId": "2154560769",
                    "name": "Lixue Liu"
                },
                {
                    "authorId": "2118255316",
                    "name": "Wei Wu"
                },
                {
                    "authorId": "2082545536",
                    "name": "Dacheng Tao"
                }
            ]
        },
        {
            "paperId": "0408d545342f0228b7a2fb62345df48163b0634a",
            "title": "Do We Really Need a Learnable Classifier at the End of Deep Neural Network?",
            "abstract": "Modern deep neural networks for classi\ufb01cation usually jointly learn a backbone for representation and a linear classi\ufb01er to output the logit of each class. A recent study has shown a phenomenon called neural collapse that the within-class means of features and the classi\ufb01er vectors converge to the vertices of a simplex equiangular tight frame (ETF) at the terminal phase of training on a balanced dataset. Since the ETF geometric structure maximally separates the pair-wise angles of all classes in the classi\ufb01er, it is natural to raise the question, why do we spend an effort to learn a classi\ufb01er when we know its optimal geometric structure? In this paper, we study the potential of learning a neural network for classi\ufb01cation with the classi\ufb01er randomly initialized as an ETF and \ufb01xed during training. Our analytical work based on the layer-peeled model indicates that the feature learning with a \ufb01xed ETF classi\ufb01er naturally leads to the neural collapse state even when the dataset is imbalanced among classes. We further show that in this case the cross entropy (CE) loss is not necessary and can be replaced by a simple squared loss that shares the same global optimality but enjoys a better convergence property. Our experimental results show that our method is able to bring signi\ufb01cant improvements with faster convergence on multiple imbalanced datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2130306785",
                    "name": "Yibo Yang"
                },
                {
                    "authorId": "1978753448",
                    "name": "Liangru Xie"
                },
                {
                    "authorId": "51248119",
                    "name": "Shixiang Chen"
                },
                {
                    "authorId": "2292145984",
                    "name": "Xiangtai Li"
                },
                {
                    "authorId": "33383055",
                    "name": "Zhouchen Lin"
                },
                {
                    "authorId": "2082545536",
                    "name": "Dacheng Tao"
                }
            ]
        },
        {
            "paperId": "046a4b8cfdc9fc69dbbdfe029c3bfc4157158d24",
            "title": "Identifiability and Asymptotics in Learning Homogeneous Linear ODE Systems from Discrete Observations",
            "abstract": "Ordinary Differential Equations (ODEs) have recently gained a lot of attention in machine learning. However, the theoretical aspects, e.g., identifiability and asymptotic properties of statistical estimation are still obscure. This paper derives a sufficient condition for the identifiability of homogeneous linear ODE systems from a sequence of equally-spaced error-free observations sampled from a single trajectory. When observations are disturbed by measurement noise, we prove that under mild conditions, the parameter estimator based on the Nonlinear Least Squares (NLS) method is consistent and asymptotic normal with $n^{-1/2}$ convergence rate. Based on the asymptotic normality property, we construct confidence sets for the unknown system parameters and propose a new method to infer the causal structure of the ODE system, i.e., inferring whether there is a causal link between system variables. Furthermore, we extend the results to degraded observations, including aggregated and time-scaled ones. To the best of our knowledge, our work is the first systematic study of the identifiability and asymptotic properties in learning linear ODE systems. We also construct simulations with various system dimensions to illustrate the established theoretical results.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2146024583",
                    "name": "Yuanyuan Wang"
                },
                {
                    "authorId": "2152491802",
                    "name": "Wei Huang"
                },
                {
                    "authorId": "29393235",
                    "name": "Mingming Gong"
                },
                {
                    "authorId": "51053977",
                    "name": "Xinpeng Geng"
                },
                {
                    "authorId": "121698214",
                    "name": "Tongliang Liu"
                },
                {
                    "authorId": "2119017697",
                    "name": "Kun Zhang"
                },
                {
                    "authorId": "2082545536",
                    "name": "Dacheng Tao"
                }
            ]
        },
        {
            "paperId": "065f9775101218665b4baaa2833c91480ec25ccd",
            "title": "TimeAutoAD: Autonomous Anomaly Detection With Self-Supervised Contrastive Loss for Multivariate Time Series",
            "abstract": "Multivariate time series (MTS) data are becoming increasingly ubiquitous in networked systems, e.g., IoT systems and 5G networks. Anomaly detection in MTS refers to identifying time series which exhibit different behaviors from normal status. Building such a system, however, is challenging due to a few reasons: i) labels for anomaly cases are usually unavailable or very rare; ii) most existing approaches rely on manual model-design and hyperparameter tuning, which may cost a huge amount of labor effort. To this end, we propose an autonomous anomaly detection technique for multivariate time series data (TimeAutoAD) based on a novel self-supervised contrastive loss. Specifically, we first present an automatic anomaly detection pipeline to optimize the model configuration and hyperparameters automatically. Next, we introduce three different strategies to augment the training data for generating pseudo negative time series and employ a self-supervised contrastive loss to distinguish the original time series and the generated time series. In this way, the representation learning capability of TimeAutoAD can be greatly enhanced and the anomaly detection performance can thus be improved. Extensive empirical studies on real-world datasets demonstrate that the proposed TimeAutoAD not only outperforms state-of-the-art anomaly detection approaches but also exhibits robustness when training data are contaminated.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055181125",
                    "name": "Yang Jiao"
                },
                {
                    "authorId": "2118049422",
                    "name": "Kai Yang"
                },
                {
                    "authorId": "2451800",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "2082545536",
                    "name": "Dacheng Tao"
                }
            ]
        },
        {
            "paperId": "066032061437b2cc54db20aa3e918f1e5ba90618",
            "title": "Power of Quantum Generative Learning",
            "abstract": "The intrinsic probabilistic nature of quantum mechanics invokes endeavors of designing quantum generative learning models (QGLMs). Despite the empirical achievements, the foundations and the potential advantages of QGLMs remain largely obscure. To narrow this knowledge gap, here we explore the generalization property of QGLMs, the capability to extend the model from learned to unknown data. We consider two prototypical QGLMs, quantum circuit Born machines and quantum generative adversarial networks, and explicitly give their generalization bounds. The result identifies superiorities of QGLMs over classical methods when quantum devices can directly access the target distribution and quantum kernels are employed. We further employ these generalization bounds to exhibit potential advantages in quantum state preparation and Hamiltonian learning. Numerical results of QGLMs in loading Gaussian distribution and estimating ground states of parameterized Hamiltonians accord with the theoretical analysis. Our work opens the avenue for quantitatively understanding the power of quantum generative learning models.",
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265526113",
                    "name": "Yuxuan Du"
                },
                {
                    "authorId": "2148465690",
                    "name": "Zhuozhuo Tu"
                },
                {
                    "authorId": "102624450",
                    "name": "Bujiao Wu"
                },
                {
                    "authorId": "2115844704",
                    "name": "Xiao Yuan"
                },
                {
                    "authorId": "2082545536",
                    "name": "Dacheng Tao"
                }
            ]
        }
    ]
}