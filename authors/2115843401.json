{
    "authorId": "2115843401",
    "papers": [
        {
            "paperId": "4c36bae234509abb7974b047fadd58548044bcba",
            "title": "Performance Evaluation of Epileptic Seizure Prediction Using Time, Frequency, and Time\u2013Frequency Domain Measures",
            "abstract": "The prediction of epileptic seizures is crucial to aid patients in gaining early warning and taking effective intervention. Several features have been explored to predict the onset via electroencephalography signals, which are typically non-stationary, dynamic, and varying from person-to-person. In the former literature, features applied in the classification have shared similar contributions to all patients. Therefore, in this paper, we analyze the impact of the specific combination of feature and channel from time, frequency, and time\u2013frequency domains on prediction performance of disparate patients. Based on the minimal-redundancy-maximal-relevance criterion, the proposed framework uses a sequential forward selection approach to individually find the optimal features and channels. Trained models could discriminate the pre-ictal and inter-ictal electroencephalography with a sensitivity of 90.2% and a false prediction rate of 0.096/h. We also present the comparison between the classification accuracy obtained by the optimal features, several features summarized from optimal features, and the complete set of features from three domains. The results indicate that various patient interpretations have a certain specificity in the selection of feature-channel. Furthermore, the detailed list of optimal features and summarized features are proffered for reference to those who research the corresponding database.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2100406683",
                    "name": "Debiao Ma"
                },
                {
                    "authorId": "2115843401",
                    "name": "Junteng Zheng"
                },
                {
                    "authorId": "47994729",
                    "name": "Lizhi Peng"
                }
            ]
        },
        {
            "paperId": "7102417cddf26d581591780aaa2f2066cb282ad9",
            "title": "Solid Texture Synthesis using Generative Adversarial Networks",
            "abstract": "Solid texture synthesis (STS), as an effective way to extend 2D exemplar to a 3D solid volume, exhibits advantages in numerous application domains. However, existing methods generally synthesize solid texture with speci\ufb01c features, which may result in the failure of capturing diversi\ufb01ed textural information. In this paper, we propose a novel generative adversarial nets-based approach (STS-GAN) to hierarchically learn solid texture with a feature-free nature. Our multi-scale discriminators evaluate the similarity between patch from exemplar and slice from the generated volume, promoting the generator to synthesize realistic solid textures. Experimental results demonstrate that the proposed method can generate high-quality solid textures with similar visual characteristics to the exemplar.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145734886",
                    "name": "Xin Zhao"
                },
                {
                    "authorId": "48169697",
                    "name": "Lin Wang"
                },
                {
                    "authorId": "15563174",
                    "name": "Jifeng Guo"
                },
                {
                    "authorId": "2119658606",
                    "name": "Bo Yang"
                },
                {
                    "authorId": "2115843401",
                    "name": "Junteng Zheng"
                },
                {
                    "authorId": "2146329569",
                    "name": "Fanqi Li"
                }
            ]
        },
        {
            "paperId": "89ce04c63050703c57983abe866543a0020015d9",
            "title": "STS-GAN: Can We Synthesize Solid Texture with High Fidelity from Arbitrary 2D Exemplar?",
            "abstract": "Solid texture synthesis (STS), an effective way to extend a 2D exemplar to a 3D solid volume, exhibits advantages in computational photography. However, existing methods generally fail to accurately learn arbitrary textures, which may result in the failure to synthesize solid textures with high fidelity. In this paper, we propose a novel generative adversarial nets-based framework (STS-GAN) to extend the given 2D exemplar to arbitrary 3D solid textures. In STS-GAN, multi-scale 2D texture discriminators evaluate the similarity between the given 2D exemplar and slices from the generated 3D texture, promoting the 3D texture generator synthesizing realistic solid textures. Finally, experiments demonstrate that the proposed method can generate high-fidelity solid textures with similar visual characteristics to the 2D exemplar.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2145734886",
                    "name": "Xin Zhao"
                },
                {
                    "authorId": "15563174",
                    "name": "Jifeng Guo"
                },
                {
                    "authorId": "3265905",
                    "name": "Linshan Wang"
                },
                {
                    "authorId": "2146329569",
                    "name": "Fanqi Li"
                },
                {
                    "authorId": "2115843401",
                    "name": "Junteng Zheng"
                },
                {
                    "authorId": "2119658606",
                    "name": "Bo Yang"
                }
            ]
        },
        {
            "paperId": "58b4d3c8c6ad666a4e798674ef7a54511029b684",
            "title": "Evolving Generalized Modulatory Learning: Unifying Neuromodulation and Synaptic Plasticity",
            "abstract": "Neuromodulation and neuroplasticity work together to help organisms learn to cope in their environment. Experiments demonstrate that simple forms of neuromodulation aid in the learning process. In those studies, neuromodulation was used as a multiplier to scale the learning rate. However, more complex interactions have not been investigated. Our contributions are twofold: 1) we evolve a subnetwork that produces a modulatory signal that 2) is incorporated into the synaptic plasticity rule in nonlinear ways. In our experiments, we compute synaptic updates using a neural network and include the modulatory signal as one of the inputs. This allows evolution to combine the synaptic activity with modulation in highly nonlinear ways to arrive at weight updates. We show that organisms that evolve with this added complexity outperform simpler multiplicative neuromodulation, suggesting that gains might be won by investigating a broader class of interactions between neuromodulation and synaptic plasticity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144736558",
                    "name": "Lin Wang"
                },
                {
                    "authorId": "2115843401",
                    "name": "Junteng Zheng"
                },
                {
                    "authorId": "144461620",
                    "name": "Jeff Orchard"
                }
            ]
        }
    ]
}