{
    "authorId": "145867172",
    "papers": [
        {
            "paperId": "02dd9ca2145393ac1865dd7c026970a460fafb49",
            "title": "Personal Data for Personal Use: Vision or Reality?",
            "abstract": "The vision of collecting all of one's personal information into one searchable database has been around at least since Vannevar Bush's 1945 paper on the Memex System [2]. In the late 1990's, Gordon Bell and his colleagues at Microsoft Research built MyLifeBits [1, 6], which was the first serious attempt to build such a database. Since then, there has been continued interest in our community to build personal information management systems [3-5, 7, 8, 10]. Recently, the Solid Project proposes a more radical approach to personal information, arguing that all of one's data should reside in their own data pod, and applications should be redesigned to fetch data from the pod [9].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2155883057",
                    "name": "Bo Li"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "1699730",
                    "name": "A. Tung"
                },
                {
                    "authorId": "1751591",
                    "name": "G. Weikum"
                },
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "2131764025",
                    "name": "Wang-Chiew Tan"
                }
            ]
        },
        {
            "paperId": "7c3af255d08c33581ebe4bb267212ca25fe51565",
            "title": "Publication Culture and Review Processes in the Data Management Community: An Open Discussion",
            "abstract": "The Data Management community has explored many options in recent years to improve our publication culture and review processes, ranging from innovative journal-conference hybrids that decouple publication from presentation, incorporating journal-style reviewing for conference-style papers, requesting code reproducibility and code/data availability, multiple submission deadlines in a year, new categories of papers, informal shepherding processes, guidelines for diversity and inclusion, automated COI check, and so on. This panel seeks to examine our many experiments, comparing them with other CS disciplines, and help determine (i) have our experiments worked? (ii) what has their impact been? and (iii) can we do better?",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1730344",
                    "name": "S. Bhowmick"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "2e2a31b0bb5c2ba3c7f4cf429e2dc81d4327af9d",
            "title": "FoundationDB: A Distributed Unbundled Transactional Key Value Store",
            "abstract": "FoundationDB is an open source transactional key value store created more than ten years ago. It is one of the first systems to combine the flexibility and scalability of NoSQL architectures with the power of ACID transactions (a.k.a. NewSQL). FoundationDB adopts an unbundled architecture that decouples an in-memory transaction management system, a distributed storage system, and a built-in distributed configuration system. Each sub-system can be independently provisioned and configured to achieve the desired scalability, high-availability and fault tolerance properties. FoundationDB uniquely integrates a deterministic simulation framework, used to test every new feature of the system under a myriad of possible faults. This rigorous testing makes FoundationDB extremely stable and allows developers to introduce and release new features in a rapid cadence. FoundationDB offers a minimal and carefully chosen feature set, which has enabled a range of disparate systems (from semi-relational databases, document and object stores, to graph databases and more) to be built as layers on top. FoundationDB is the underpinning of cloud infrastructure at Apple, Snowflake and other companies, due to its consistency, robustness and availability for storing user data, system metadata and configuration, and other critical information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145787641",
                    "name": "Jingyu Zhou"
                },
                {
                    "authorId": "2110682254",
                    "name": "Meng Xu"
                },
                {
                    "authorId": "3009338",
                    "name": "A. Shraer"
                },
                {
                    "authorId": "6791374",
                    "name": "B. Namasivayam"
                },
                {
                    "authorId": "2109474916",
                    "name": "Alex Miller"
                },
                {
                    "authorId": "2113612879",
                    "name": "Evan Tschannen"
                },
                {
                    "authorId": "2113612860",
                    "name": "Steve Atherton"
                },
                {
                    "authorId": "2113622666",
                    "name": "Andrew J. Beamon"
                },
                {
                    "authorId": "52133816",
                    "name": "Rusty Sears"
                },
                {
                    "authorId": "2053972681",
                    "name": "J. Leach"
                },
                {
                    "authorId": "46838301",
                    "name": "D. Rosenthal"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "39597366",
                    "name": "Willie B. Wilson"
                },
                {
                    "authorId": "2072510503",
                    "name": "Ben Collins"
                },
                {
                    "authorId": "143650662",
                    "name": "David Scherer"
                },
                {
                    "authorId": "1405315419",
                    "name": "Alec Grieser"
                },
                {
                    "authorId": "2116518888",
                    "name": "Young Liu"
                },
                {
                    "authorId": "2113616300",
                    "name": "Alvin Moore"
                },
                {
                    "authorId": "2113611926",
                    "name": "Bhaskar Muppana"
                },
                {
                    "authorId": "10687959",
                    "name": "Xi-sheng Su"
                },
                {
                    "authorId": "2113618403",
                    "name": "Vishesh Yadav"
                }
            ]
        },
        {
            "paperId": "0c8a52c3eb1e1b031c69bf18879a1040f1639f02",
            "title": "Big Data Integration for Product Specifications",
            "abstract": "The product domain contains valuable data for many important applications. Given the large and increasing number of sources that provide data about product speci\ufb01cations and the velocity as well as the variety with which such data are available, this domain represents a challenging scenario for developing and evaluating big data integration solutions. In this paper, we present the results of our efforts towards big data integration for product speci\ufb01cations. We present a pipeline that decomposes the problem into different tasks from source and data discovery, to extraction, data linkage, schema alignment and data fusion. Although we present the pipeline as a sequence of tasks, different con\ufb01gurations can be de\ufb01ned depending on the application goals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056435855",
                    "name": "Luciano Barbosa"
                },
                {
                    "authorId": "1791339",
                    "name": "Valter Crescenzi"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1796590",
                    "name": "P. Merialdo"
                },
                {
                    "authorId": "50880083",
                    "name": "Federico Piai"
                },
                {
                    "authorId": "1756080",
                    "name": "Disheng Qiu"
                },
                {
                    "authorId": "2115435801",
                    "name": "Yanyan Shen"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "235b91e9d562707481c2d6db1b0f94313b519eb7",
            "title": "Lessons Learned and Research Agenda for Big Data Integration of Product Specifications",
            "abstract": "The product domain represents a challenging scenario for developing and evaluating big data integration solutions: the number of sources providing product specifications is very large, and ever increasing over time. The volume of available data is impressive, and these data keep changing very frequently. In this paper, we present ongoing efforts, challenges and our research agenda to address big data integration for product specifications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056435855",
                    "name": "Luciano Barbosa"
                },
                {
                    "authorId": "1791339",
                    "name": "Valter Crescenzi"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1796590",
                    "name": "P. Merialdo"
                },
                {
                    "authorId": "50880083",
                    "name": "Federico Piai"
                },
                {
                    "authorId": "1756080",
                    "name": "Disheng Qiu"
                },
                {
                    "authorId": "2115435801",
                    "name": "Yanyan Shen"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "410d06ff1dc1aa602681ff947c781522df4ae02f",
            "title": "Data Integration and Machine Learning: A Natural Synergy",
            "abstract": "There is now more data to analyze than ever before. As data volume and variety have increased, so have the ties between machine learning and data integration become stronger. For machine learning to be effective, one must utilize data from the greatest possible variety of sources; and this is why data integration plays a key role. At the same time machine learning is driving automation in data integration, resulting in overall reduction of integration costs and improved accuracy. This tutorial focuses on three aspects of the synergistic relationship between data integration and machine learning: (1) we survey how state-of-the-art data integration solutions rely on machine learning-based approaches for accurate results and effective human-in-the-loop pipelines, (2) we review how end-to-end machine learning applications rely on data integration to identify accurate, clean, and relevant data for their analytics exercises, and (3) we discuss open research challenges and opportunities that span across data integration and machine learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "145071799",
                    "name": "Theodoros Rekatsinas"
                }
            ]
        },
        {
            "paperId": "e3e3eaa0834ae16afc78641650bab979d7561821",
            "title": "Data Quality: The Role of Empiricism",
            "abstract": "We outline a call to action for promoting empiricism in data quality research. The action points result from an analysis of the landscape of data quality research. The landscape exhibits two dimensions of empiricism in data quality research relating to type of metrics and scope of method. Our study indicates the presence of a data continuum ranging from real to synthetic data, which has implications for how data quality methods are evaluated. The dimensions of empiricism and their inter-relationships provide a means of positioning data quality research, and help expose limitations, gaps and opportunities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2092368",
                    "name": "S. Sadiq"
                },
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                },
                {
                    "authorId": "1743316",
                    "name": "Ihab F. Ilyas"
                },
                {
                    "authorId": "2288285",
                    "name": "S. Link"
                },
                {
                    "authorId": "2110417078",
                    "name": "Ren\u00e9e J. Miller"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "48667278",
                    "name": "Xiaofang Zhou"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "1f5ae98af7f36bc468376a0b108109ae8251ef12",
            "title": "SourceSight: Enabling Effective Source Selection",
            "abstract": "Recently there has been a rapid increase in the number of data sources and data services, such as cloud-based data markets and data portals, that facilitate the collection, publishing and trading of data. Data sources typically exhibit large heterogeneity in the type and quality of data they provide. Unfortunately, when the number of data sources is large, it is difficult for users to reason about the actual usefulness of sources for their applications and the trade-offs between the benefits and costs of acquiring and integrating sources. In this demonstration we present \\textsc{SourceSight}, a system that allows users to interactively explore a large number of heterogeneous data sources, and discover valuable sets of sources for diverse integration tasks. \\textsc{SourceSight}~uses a novel multi-level source quality index that enables effective source selection at different granularity levels, and introduces a collection of new techniques to discover and evaluate relevant sources for integration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145071799",
                    "name": "Theodoros Rekatsinas"
                },
                {
                    "authorId": "144520191",
                    "name": "A. Deshpande"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1746034",
                    "name": "L. Getoor"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "005a13ebd72cb06249da7b72195d29d970352d61",
            "title": "Error Diagnosis and Data Profiling with Data X-Ray",
            "abstract": "The problem of identifying and repairing data errors has been an area of persistent focus in data management research. However, while traditional data cleaning techniques can be effective at identifying several data discrepancies, they disregard the fact that many errors are systematic, inherent to the process that produces the data, and thus will keep occurring unless the root cause is identified and corrected. \n \nIn this demonstration, we will present a large-scale diagnostic framework called DataXRay. Like a medical X-ray that aids the diagnosis of medical conditions by revealing problems underneath the surface, DataXRay reveals hidden connections and common properties among data errors. Thus, in contrast to traditional cleaning methods, which treat the symptoms, our system investigates the underlying conditions that cause the errors. \n \nThe core of DataXRay combines an intuitive and principled cost model derived by Bayesian analysis, and an efficient, highly-parallelizable diagnostic algorithm that discovers common properties among erroneous data elements in a top-down fashion. Our system has a simple interface that allows users to load different datasets, to interactively adjust key diagnostic parameters, to explore the derived diagnoses, and to compare with solutions produced by alternative algorithms. Through this demonstration, participants will understand (1) the characteristics of good diagnoses, (2) how and why errors occur in real-world datasets, and (3) the distinctions with other related problems and approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47120191",
                    "name": "Xiaolan Wang"
                },
                {
                    "authorId": "49308909",
                    "name": "Mary Feng"
                },
                {
                    "authorId": "2144334327",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2283085",
                    "name": "A. Meliou"
                }
            ]
        },
        {
            "paperId": "117da44f01ef45ef8223bec8f9c2346b131321f4",
            "title": "Knowledge-Based Trust: Estimating the Trustworthiness of Web Sources",
            "abstract": "\n The quality of web sources has been traditionally evaluated using\n exogenous\n signals such as the hyperlink structure of the graph. We propose a new approach that relies on\n endogenous\n signals, namely, the correctness of factual information provided by the source. A source that has few false facts is considered to be trustworthy.\n \n The facts are automatically extracted from each source by information extraction methods commonly used to construct knowledge bases. We propose a way to distinguish errors made in the extraction process from factual errors in the web source per se, by using joint inference in a novel multi-layer probabilistic model.\n \n We call the trustworthiness score we computed\n Knowledge-Based Trust (KBT)\n . On synthetic data, we show that our method can reliably compute the true trustworthiness levels of the sources. We then apply it to a database of 2.8B facts extracted from the web, and thereby estimate the trustworthiness of 119M webpages. Manual evaluation of a subset of the results confirms the effectiveness of the method.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1718798",
                    "name": "E. Gabrilovich"
                },
                {
                    "authorId": "1702318",
                    "name": "K. Murphy"
                },
                {
                    "authorId": "2095406372",
                    "name": "Van Dang"
                },
                {
                    "authorId": "40428294",
                    "name": "Wilko Horn"
                },
                {
                    "authorId": "47757859",
                    "name": "C. Lugaresi"
                },
                {
                    "authorId": "2109375570",
                    "name": "Shaohua Sun"
                },
                {
                    "authorId": null,
                    "name": "Wei Zhang"
                }
            ]
        }
    ]
}