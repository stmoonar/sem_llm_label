{
    "authorId": "48678411",
    "papers": [
        {
            "paperId": "52fcda1bfd02b92d946ef22f4dd001c61f7da039",
            "title": "A New Paradigm for Counterfactual Reasoning in Fairness and Recourse",
            "abstract": "Counterfactuals underpin numerous techniques for auditing and understanding artificial intelligence (AI) systems. The traditional paradigm for counterfactual reasoning in this literature is the interventional counterfactual, where hypothetical interventions are imagined and simulated. For this reason, the starting point for causal reasoning about legal protections and demographic data in AI is an imagined intervention on a legally-protected characteristic, such as ethnicity, race, gender, disability, age, etc. We ask, for example, what would have happened had your race been different? An inherent limitation of this paradigm is that some demographic interventions \u2014 like interventions on race \u2014 may not be well-defined or translate into the formalisms of interventional counterfactuals. In this work, we explore a new paradigm based instead on the backtracking counterfactual, where rather than imagine hypothetical interventions on legally-protected characteristics, we imagine alternate initial conditions while holding these characteristics fixed. We ask instead, what would explain a counterfactual outcome for you as you actually are or could be? This alternate framework allows us to address many of the same social concerns, but to do so while asking fundamentally different questions that do not rely on demographic interventions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "53c93e07d4ddc8c4d2e6b7714e827f31177060e1",
            "title": "Model-Agnostic Auditing: A Lost Cause?",
            "abstract": "Tools for interpretable machine learning (IML) or explainable artificial intelligence (xAI) can be used to audit algorithms for fairness or other desiderata. In a black-box setting without access to the algorithm\u2019s internal structure an auditor may be limited to methods that are model-agnostic. These methods have severe limitations with important consequences for outcomes such as fairness. Among model-agnostic IML methods, visualizations such as the partial dependence plot (PDP) or individual conditional expectation (ICE) plots are popular and useful for displaying qualitative relationships. Although we focus on fairness auditing with PDP/ICE plots, the consequences we highlight generalize to other auditing or IML/xAI applications. This paper questions the validity of auditing in high-stakes settings with contested values or conflicting interests if the audit methods are model-agnostic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2210982171",
                    "name": "Sakina Hansen"
                },
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                }
            ]
        },
        {
            "paperId": "6ae352e19261ec177dd8aed3e7fc432708402757",
            "title": "Causal Dependence Plots",
            "abstract": "Explaining artificial intelligence or machine learning models is increasingly important. To use such data-driven systems wisely we must understand how they interact with the world, including how they depend causally on data inputs. In this work we develop Causal Dependence Plots (CDPs) to visualize how one variable--an outcome--depends on changes in another variable--a predictor--$\\textit{along with any consequent causal changes in other predictor variables}$. Crucially, CDPs differ from standard methods based on holding other predictors constant or assuming they are independent. CDPs make use of an auxiliary causal model because causal conclusions require causal assumptions. With simulations and real data experiments, we show CDPs can be combined in a modular way with methods for causal learning or sensitivity analysis. Since people often think causally about input-output dependence, CDPs can be powerful tools in the xAI or interpretable machine learning toolkit and contribute to applications like scientific machine learning and algorithmic fairness.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "2210982171",
                    "name": "Sakina Hansen"
                }
            ]
        },
        {
            "paperId": "c4cea044af3ad99024290345e0287383e5708606",
            "title": "Causal Dependence Plots for Interpretable Machine Learning",
            "abstract": "Explaining arti\ufb01cial intelligence or machine learning models is an increasingly important problem. For humans to stay in the loop and control such systems, we must be able to understand how they interact with the world. This work proposes us-ing known or assumed causal structure in the input variables to produce simple and practical explanations of supervised learning models. Our explanations\u2014which we name Causal Dependence Plots or CDP\u2014visualize how the model output depends on changes in a given predictor along with any consequent causal changes in other predictors . Since this causal dependence captures how humans often think about input-output dependence, CDPs can be powerful tools in the explainable AI or interpretable ML toolkit and contribute to applications including scienti\ufb01c machine learning and algorithmic fairness. CDP can also be used for model-agnostic or black-box explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "2210982171",
                    "name": "Sakina Hansen"
                }
            ]
        },
        {
            "paperId": "7657eb913f77e3d95f2ebcd5f8e55d77f856064a",
            "title": "Counterfactuals for the Future",
            "abstract": "Counterfactuals are often described as 'retrospective,' focusing on hypothetical alternatives to a realized past. This description relates to an often implicit assumption about the structure and stability of exogenous variables in the system being modeled --- an assumption that is reasonable in many settings where counterfactuals are used. In this work, we consider cases where we might reasonably make a different assumption about exogenous variables; namely, that the exogenous noise terms of each unit do exhibit some unit-specific structure and/or stability. This leads us to a different use of counterfactuals --- a forward-looking rather than retrospective counterfactual. We introduce \"counterfactual treatment choice,\" a type of treatment choice problem that motivates using forward-looking counterfactuals. We then explore how mismatches between interventional versus forward-looking counterfactual approaches to treatment choice, consistent with different assumptions about exogenous noise, can lead to counterintuitive results.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "13b48e8acdfa98aedc83cb23832145e9b5352bbe",
            "title": "Disaggregated Interventions to Reduce Inequality",
            "abstract": "A significant body of research in the data sciences considers unfair discrimination against social categories such as race or gender that could occur or be amplified as a result of algorithmic decisions. Simultaneously, real-world disparities continue to exist, even before algorithmic decisions are made. In this work, we draw on insights from the social sciences brought into the realm of causal modeling and constrained optimization, and develop a novel algorithmic framework for tackling pre-existing real-world disparities. The purpose of our framework, which we call the \u201cimpact remediation framework,\u201d is to measure real-world disparities and discover the optimal intervention policies that could help improve equity or access to opportunity for those who are underserved with respect to an outcome of interest. We develop a disaggregated approach to tackling pre-existing disparities that relaxes the typical set of assumptions required for the use of social categories in structural causal models. Our approach flexibly incorporates counterfactuals and is compatible with various ontological assumptions about the nature of social categories. We demonstrate impact remediation with a hypothetical case study and compare our disaggregated approach to an existing state-of-the-art approach, comparing its structure and resulting policy recommendations. In contrast to most work on optimal policy learning, we explore disparity reduction itself as an objective, explicitly focusing the power of algorithms on reducing inequality.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "508887cd69bbd67a800bb019c3dede6b938171af",
            "title": "Causal Intersectionality and Fair Ranking",
            "abstract": "In this paper we propose a causal modeling approach to intersectional fairness, and a flexible, task-specific method for computing intersectionally fair rankings. Rankings are used in many contexts, ranging from Web search to college admissions",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119301078",
                    "name": "Ke Yang"
                },
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "85517e6e34857cd923b6875090f7be9b26d17473",
            "title": "Impact Remediation: Optimal Interventions to Reduce Inequality",
            "abstract": "A significant body of research in the data sciences considers un-fair discrimination against social categories such as race or gender that could occur or be amplified as a result of algorithmic decisions. Simultaneously, real-world disparities continue to exist, even before algorithmic decisions are made. In this work, we draw on insights from the social sciences and humanistic studies brought into the realm of causal modeling and constrained optimization, and develop a novel algorithmic framework for tackling pre-existing real-world disparities. The purpose of our framework, which we call the \u201cimpact remediation framework,\u201d is to measure real-world disparities and discover the optimal intervention policies that could help improve equity or access to opportunity for those who are underserved with respect to an outcome of interest. We develop a disaggregated approach to tackling pre-existing disparities that relaxes the typical set of assumptions required for the use of social categories in structural causal models. Our approach flexibly incorporates counterfactuals and is compatible with various ontological assumptions about the nature of social categories. We demonstrate impact remediation with a real-world case study and compare our disaggregated approach to an existing state-of-the-art approach, comparing its structure and resulting policy recommendations. In contrast to most work on optimal policy learning, we explore disparity reduction itself as an objective, explicitly focusing the power of algorithms on reducing inequality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138875066",
                    "name": "Lucius E.J. Bynum"
                },
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "810f396f8f2f3721e3744fcf5ce9d2bfb4f684cb",
            "title": "Causal intersectionality for fair ranking",
            "abstract": "In this paper we propose a causal modeling approach to intersectional fairness, and a flexible, task-specific method for computing intersectionally fair rankings. Rankings are used in many contexts, ranging from Web search results to college admissions, but causal inference for fair rankings has received limited attention. Additionally, the growing literature on causal fairness has directed little attention to intersectionality. By bringing these issues together in a formal causal framework we make the application of intersectionality in fair machine learning explicit, connected to important real world effects and domain knowledge, and transparent about technical limitations. We experimentally evaluate our approach on real and synthetic datasets, exploring its behaviour under different structural assumptions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2119301078",
                    "name": "Ke Yang"
                },
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "06e04fd496cd805bca69eea2c1977f90afeeef83",
            "title": "Causal Interventions for Fairness",
            "abstract": "Most approaches in algorithmic fairness constrain machine learning methods so the resulting predictions satisfy one of several intuitive notions of fairness. While this may help private companies comply with non-discrimination laws or avoid negative publicity, we believe it is often too little, too late. By the time the training data is collected, individuals in disadvantaged groups have already suffered from discrimination and lost opportunities due to factors out of their control. In the present work we focus instead on interventions such as a new public policy, and in particular, how to maximize their positive effects while improving the fairness of the overall system. We use causal methods to model the effects of interventions, allowing for potential interference--each individual's outcome may depend on who else receives the intervention. We demonstrate this with an example of allocating a budget of teaching resources using a dataset of schools in New York City.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1940272",
                    "name": "Matt J. Kusner"
                },
                {
                    "authorId": "2052380526",
                    "name": "Chris Russell"
                },
                {
                    "authorId": "48678411",
                    "name": "Joshua R. Loftus"
                },
                {
                    "authorId": "2187716",
                    "name": "Ricardo Silva"
                }
            ]
        }
    ]
}