{
    "authorId": "151505506",
    "papers": [
        {
            "paperId": "2907e8e322af55b7d06d57786d456c034fc89e05",
            "title": "GunStance: Stance Detection for Gun Control and Gun Regulation",
            "abstract": "The debate surrounding gun control and gun regulation in the United States has intensified in the wake of numerous mass shooting events. As perspectives on this matter vary, it becomes increasingly important to comprehend individuals\u2019 positions. Stance detection, the task of determining an author\u2019s position towards a proposition or target, has gained attention for its potential use in understanding public perceptions towards controversial topics and identifying the best strategies to address public concerns. In this paper, we present G UN S TANCE , a dataset of tweets pertaining to shooting events, focusing specifically on the controversial topics of \u201cbanning guns\u201d and \u201cregulating guns.\u201d The tweets in the dataset are sourced from discussions on Twitter following various shooting incidents in the United States. Amazon Mechanical Turk was used to manually annotate a subset of the tweets relevant to the targets of interest (\u201cbanning guns\u201d and \u201cregulating guns\u201d) into three classes: In-Favor , Against , and Neutral . The remaining unlabeled tweets are included in the dataset to facilitate studies on semi-supervised learning (SSL) approaches that can help address the scarcity of the labeled data in stance detection tasks. Furthermore, we propose a hybrid approach that combines curriculum-based SSL and Large Language Models (LLM), and show that the proposed approach outperforms supervised, semi-supervised, and LLM-based zero-shot models in most experiments on our assembled dataset. The dataset and code are available on GitHub. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1906359346",
                    "name": "Nikesh Gyawali"
                },
                {
                    "authorId": "2187455696",
                    "name": "Iustin Sirbu"
                },
                {
                    "authorId": "2008183567",
                    "name": "Tiberiu Sosea"
                },
                {
                    "authorId": "31026404",
                    "name": "Sarthak Khanal"
                },
                {
                    "authorId": "151505506",
                    "name": "Doina Caragea"
                },
                {
                    "authorId": "2796756",
                    "name": "Traian Rebedea"
                },
                {
                    "authorId": "2316579314",
                    "name": "Cornelia Caragea"
                }
            ]
        },
        {
            "paperId": "77b20188da592c7c2b913039e2bfcbb842e71ce5",
            "title": "Contrastive Learning for Multimodal Classification of Crisis related Tweets",
            "abstract": "Multimodal tasks require learning a joint representation of the constituent modalities of data. Contrastive learning learns a joint representation by using a contrastive loss. For example, CLIP takes as input image-caption pairs and is trained to maximize the similarity between an image and its corresponding caption in actual image-caption pairs, while minimizing the similarity for arbitrary image-caption pairs. This approach operates on the premise that the caption depicts the image's content. However, this assumption does not always hold true for tweets that contain both text and images. Previous studies have indicated that the connection between the image and the text in a tweet is more intricate and complex. We study the effectiveness of pre-trained multimodal contrastive learning models, specifically, CLIP, and ALIGN, on the task of classifying multimodal crisis related tweets. Our experiments using two publicly available datasets, CrisisMMD and DMD, show that despite the intricate relationships in tweets, pre-trained contrastive learning models fine-tuned with task-specific data produce better results than prior approaches used for the multimodal classification of crisis related tweets. Additionally, the experiments show that the contrastive learning models are effective in low-data few-shot and cross-domain settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1379987324",
                    "name": "Bishwas Mandal"
                },
                {
                    "authorId": "31026404",
                    "name": "Sarthak Khanal"
                },
                {
                    "authorId": "151505506",
                    "name": "Doina Caragea"
                }
            ]
        },
        {
            "paperId": "448ec5104e7b59227fc10326d9828125b46cad2b",
            "title": "CrisisMatch: Semi-Supervised Few-Shot Learning for Fine-Grained Disaster Tweet Classification",
            "abstract": "The shared real-time information about natural disasters on social media platforms like Twitter and Facebook plays a critical role in informing volunteers, emergency managers, and response organizations. However, supervised learning models for monitoring disaster events require large amounts of annotated data, making them unrealistic for real-time use in disaster events. To address this challenge, we present a fine-grained disaster tweet classification model under the semi-supervised, few-shot learning setting where only a small number of annotated data is required. Our model, CrisisMatch, effectively classifies tweets into fine-grained classes of interest using few labeled data and large amounts of unlabeled data, mimicking the early stage of a disaster. Through integrating effective semi-supervised learning ideas and incorporating TextMixUp, CrisisMatch achieves performance improvement on two disaster datasets of 11.2\\% on average. Further analyses are also provided for the influence of the number of labeled data and out-of-domain results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261285492",
                    "name": "Henry Peng Zou"
                },
                {
                    "authorId": "2261321156",
                    "name": "Yue Zhou"
                },
                {
                    "authorId": "2261283079",
                    "name": "Cornelia Caragea"
                },
                {
                    "authorId": "151505506",
                    "name": "Doina Caragea"
                }
            ]
        },
        {
            "paperId": "74ce0b2edebf19dbea67bf9c5c9e75c724003a67",
            "title": "Disaster Image Classification Using Pre-trained Transformer and Contrastive Learning Models",
            "abstract": "Natural disasters can have devastating consequences for communities, causing loss of life and significant economic damage. To mitigate these impacts, it is crucial to quickly and accurately identify situational awareness and actionable information useful for disaster relief and response organizations. In this paper, we study the use of advanced transformer and contrastive learning models for disaster image classification in a humanitarian context, with focus on state-of-the-art pre-trained vision transformers such as ViT, CSWin and a state-of-the-art pre-trained contrastive learning model, CLIP. We evaluate the performance of these models across various disaster scenarios, including in-domain and cross-domain settings, as well as few-shot learning and zero-shot learning settings. Our results show that the CLIP model outperforms the two transformer models (ViT and CSWin) and also ConvNeXts, a competitive CNN-based model resembling transformers, in all the settings. By improving the performance of disaster image classification, our work can contribute to the goal of reducing the number of deaths and economic losses caused by disasters, as well as helping to decrease the number of people affected by these events.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4653527",
                    "name": "Soudabeh Taghian Dinani"
                },
                {
                    "authorId": "151505506",
                    "name": "Doina Caragea"
                }
            ]
        },
        {
            "paperId": "97aa296b7f43827a5a1812f7af9a7da02296acb3",
            "title": "A Comparison Study for Disaster Tweet Classification Using Deep Learning Models",
            "abstract": "Abstract",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4653527",
                    "name": "Soudabeh Taghian Dinani"
                },
                {
                    "authorId": "151505506",
                    "name": "Doina Caragea"
                }
            ]
        },
        {
            "paperId": "d6348e8128628fcf6298aa6be8f093fdfb4ac659",
            "title": "Predicting Pedestrian Involvement in Fatal Crashes Using a TabNet Deep Learning Model",
            "abstract": "To make road transportation systems safe for pedestrians, understanding the contributing features in vehicle-pedestrian fatal crashes is critical. With a better prediction model, it is possible to design effective countermeasures and reduce fatal crashes involving pedestrians. This paper aims to develop a deep learning-based model to predict fatal crashes that involve pedestrians in the United States using the Fatality Analysis Reporting System (FARS) database from the National Highway Traffic Safety Administration (NHTSA). TabNet architecture has been used to train a model from historical data. At the same time, other traditional classifiers such as support vector machines, random forests, and decision trees have been utilized to develop baseline results. An ensemble model of the five best models from the single model analysis was also developed. Metrics such as Precision, Recall, F1 score, and the area under the ROC curve (auROC) were calculated for each model. Since the problem requires correct prediction of all possible fatal cases, Recall is considered the most crucial evaluation metric. Not surprisingly, the ensemble model was found to have the highest recall value among all models. However, the TabNet model was found to have the highest recall score among single models, indicating that this model is the most suitable for the fatal vehicle-pedestrian crash prediction task out of all models analyzed. Another advantage of the TabNet model is that it can be interpreted, which helps understand the variables that contribute the most to the prediction. It was seen that factors such as roadway geometry, light conditions, impaired drivers, and land use had the highest contributions to the predictions made by the model. This fatal crash prediction model was found to have the potential to aid all relevant stakeholders in decision-making processes to make roadways safer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284371651",
                    "name": "Omar Al-Ani"
                },
                {
                    "authorId": "2284371978",
                    "name": "S. M. Haroon"
                },
                {
                    "authorId": "151505506",
                    "name": "Doina Caragea"
                },
                {
                    "authorId": "2284370268",
                    "name": "HM Abdul Aziz"
                },
                {
                    "authorId": "2284371645",
                    "name": "Eric J. Fitzsimmons"
                }
            ]
        },
        {
            "paperId": "fbf52bb0fe6ac2c74d89f3c6debcc3872186f141",
            "title": "Leveraging Existing Literature on the Web and Deep Neural Models to Build a Knowledge Graph Focused on Water Quality and Health Risks",
            "abstract": "A knowledge graph focusing on water quality in relation to health risks posed by water activities (such as diving or swimming) is not currently available. To address this limitation, we first use existing resources to construct a knowledge graph relevant to water quality and health risks using KNowledge Acquisition and Representation Methodology (KNARM). Subsequently, we explore knowledge graph completion approaches for maintaining and updating the graph. Specifically, we manually identify a set of domain-specific UMLS concepts and use them to extract a graph of approximately 75,000 semantic triples from the Semantic MEDLINE database (which contains head-relation-tail triples extracted from PubMed). Using the resulting knowledge graph, we experiment with the KG-BERT approach for graph completion by employing pre-trained BERT/RoBERTa models and also models fine-tuned on a collection of water quality and health risks abstracts retrieved from the Web of Science. Experimental results show that KG-BERT with BERT/RoBERTa models fine-tuned on a domain-specific corpus improves the performance of KG-BERT with pre-trained models. Furthermore, KG-BERT gives better results than several translational distance or semantic matching baseline models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2215549551",
                    "name": "Nikita Gautam"
                },
                {
                    "authorId": "2077244590",
                    "name": "David Shumway"
                },
                {
                    "authorId": "2215547288",
                    "name": "Megan Kowalcyk"
                },
                {
                    "authorId": "31026404",
                    "name": "Sarthak Khanal"
                },
                {
                    "authorId": "151505506",
                    "name": "Doina Caragea"
                },
                {
                    "authorId": "1690656",
                    "name": "Cornelia Caragea"
                },
                {
                    "authorId": "153612944",
                    "name": "H. Mcginty"
                },
                {
                    "authorId": "5524986",
                    "name": "S. Dorevitch"
                }
            ]
        },
        {
            "paperId": "825f72d39029dc1782b7869ba37091fc8a5c53f5",
            "title": "Using Deep Learning to Improve Detection and Decoding Of Barcodes",
            "abstract": "We propose an end-to-end pipeline for transforming raw images containing barcodes into sharp barcode images that can be accurately decoded. Our pipeline leverages recent deep learning approaches and consists of a rotation-decoupled detector (RDD) for oriented barcode detection and a deblurring model. The deblurring model uses a generative adversarial network (specifically, DeblurGAN-v2) trained on pairs of noisy and sharp images generated using ground truth numeric codes. Evaluation of the proposed pipeline using real barcode images enhanced by the DeblurGAN-v2 model shows a 14% improvement of the decoding rate as compared to the decoding rate obtained on the original images.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135742380",
                    "name": "Chaoxin Wang"
                },
                {
                    "authorId": "2074007082",
                    "name": "Nicolais Guevara"
                },
                {
                    "authorId": "151505506",
                    "name": "Doina Caragea"
                }
            ]
        },
        {
            "paperId": "863c939d396245c943fd039e25a5933e6e26cfb1",
            "title": "Multimodal Semi-supervised Learning for Disaster Tweet Classification",
            "abstract": "During natural disasters, people often use social media platforms, such as Twitter, to post information about casualties and damage produced by disasters. This information can help relief authorities gain situational awareness in nearly real time, and enable them to quickly distribute resources where most needed. However, annotating data for this purpose can be burdensome, subjective and expensive. In this paper, we investigate how to leverage the copious amounts of unlabeled data generated on social media by disaster eyewitnesses and affected individuals during disaster events. To this end, we propose a semi-supervised learning approach to improve the performance of neural models on several multimodal disaster tweet classification tasks. Our approach shows significant improvements, obtaining up to 7.7% improvements in F-1 in low-data regimes and 1.9% when using the entire training data. We make our code and data publicly available at https://github.com/iustinsirbu13/multimodal-ssl-for-disaster-tweet-classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187455696",
                    "name": "Iustin Sirbu"
                },
                {
                    "authorId": "2008183567",
                    "name": "Tiberiu Sosea"
                },
                {
                    "authorId": "2140493460",
                    "name": "Cornelia Caragea"
                },
                {
                    "authorId": "151505506",
                    "name": "Doina Caragea"
                },
                {
                    "authorId": "2796756",
                    "name": "Traian Rebedea"
                }
            ]
        },
        {
            "paperId": "4621a0c6dd287291bed4593aa4537b21c16c0f1c",
            "title": "Disaster Image Classification Using Capsule Networks",
            "abstract": "When a disaster happens, affected individuals may use social media platforms, such as Twitter or Facebook, to ask for help or post information about the disaster. From a disaster response point of view, it is important to filter posts, in particular, text and images that provide situational awareness information, in a timely manner. For image classification, capsule networks have shown superiority over convolutional neural networks (CNN). Given their success in other application domains, in this study, we used capsule networks to classify disaster images as Informative or Non-informative. Using publicly available images collected from several disasters, we compared capsule network models with ResNet-18 models, for both in-domain and cross-domain settings. The results showed that the capsule network models had better performance for all the disaster datasets considered in the in-domain experiments, and also for most of the cross-domain pairs of disasters used in the study.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4653527",
                    "name": "Soudabeh Taghian Dinani"
                },
                {
                    "authorId": "151505506",
                    "name": "Doina Caragea"
                }
            ]
        }
    ]
}