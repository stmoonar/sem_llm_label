{
    "authorId": "145052856",
    "papers": [
        {
            "paperId": "1b70fe4dc8d39ebc08b9472e6e90a1f21eddb6a3",
            "title": "Open Assistant Toolkit - version 2",
            "abstract": "We present the second version of the Open Assistant Toolkit (OAT-v2), an open-source task-oriented conversational system for composing generative neural models. OAT-v2 is a scalable and flexible assistant platform supporting multiple domains and modalities of user interaction. It splits processing a user utterance into modular system components, including submodules such as action code generation, multimodal content retrieval, and knowledge-augmented response generation. Developed over multiple years of the Alexa TaskBot challenge, OAT-v2 is a proven system that enables scalable and robust experimentation in experimental and real-world deployment. OAT-v2 provides open models and software for research and commercial applications to enable the future of multimodal virtual assistants across diverse applications and types of rich interaction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2164704207",
                    "name": "Sophie Fischer"
                },
                {
                    "authorId": "48890086",
                    "name": "Federico Rossetto"
                },
                {
                    "authorId": "1796270950",
                    "name": "Carlos Gemmell"
                },
                {
                    "authorId": "2289842043",
                    "name": "Andrew Ramsay"
                },
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "2289842340",
                    "name": "Philip Zubel"
                },
                {
                    "authorId": "2283847249",
                    "name": "Niklas Tecklenburg"
                },
                {
                    "authorId": "2261782190",
                    "name": "Jeffrey Dalton"
                }
            ]
        },
        {
            "paperId": "20025b2ef0cececcbc646a66093468da0c43769b",
            "title": "GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language Models for Adaptable Conversational Task Assistants",
            "abstract": "We tackle the challenge of building real-world multimodal assistants for complex real-world tasks. We describe the practicalities and challenges of developing and deploying GRILLBot, a leading (first and second prize winning in 2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture that leverages Large Language Models (LLMs) and specialised models tuned for specific subtasks requiring very low latency. OAT allows us to define when, how and which LLMs should be used in a structured and deployable manner. For knowledge-grounded question answering and live task adaptations, we show that LLM reasoning abilities over task context and world knowledge outweigh latency concerns. For dialogue state management, we implement a code generation approach and show that specialised smaller models have 84% effectiveness with 100x lower latency. Overall, we provide insights and discuss tradeoffs for deploying both traditional models and LLMs to users in complex real-world multimodal environments in the Alexa TaskBot challenge. These experiences will continue to evolve as LLMs become more capable and efficient -- fundamentally reshaping OAT and future assistant architectures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2164704207",
                    "name": "Sophie Fischer"
                },
                {
                    "authorId": "1796270950",
                    "name": "Carlos Gemmell"
                },
                {
                    "authorId": "2283847249",
                    "name": "Niklas Tecklenburg"
                },
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "48890086",
                    "name": "Federico Rossetto"
                },
                {
                    "authorId": "2261782190",
                    "name": "Jeffrey Dalton"
                }
            ]
        },
        {
            "paperId": "fc51fd66f49c43dfe602ccf59784a4d02a8ecc50",
            "title": "DREQ: Document Re-Ranking Using Entity-based Query Understanding",
            "abstract": "While entity-oriented neural IR models have advanced significantly, they often overlook a key nuance: the varying degrees of influence individual entities within a document have on its overall relevance. Addressing this gap, we present DREQ, an entity-oriented dense document re-ranking model. Uniquely, we emphasize the query-relevant entities within a document's representation while simultaneously attenuating the less relevant ones, thus obtaining a query-specific entity-centric document representation. We then combine this entity-centric document representation with the text-centric representation of the document to obtain a\"hybrid\"representation of the document. We learn a relevance score for the document using this hybrid representation. Using four large-scale benchmarks, we show that DREQ outperforms state-of-the-art neural and non-neural re-ranking methods, highlighting the effectiveness of our entity-oriented representation approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113355478",
                    "name": "Shubham Chatterjee"
                },
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "2277449598",
                    "name": "Jeffery Dalton"
                }
            ]
        },
        {
            "paperId": "0b8eaf52001bafa01dda642a0358ce3355318bc9",
            "title": "GRM: Generative Relevance Modeling Using Relevance-Aware Sample Estimation for Document Retrieval",
            "abstract": "Recent studies show that Generative Relevance Feedback (GRF), using text generated by Large Language Models (LLMs), can enhance the effectiveness of query expansion. However, LLMs can generate irrelevant information that harms retrieval effectiveness. To address this, we propose Generative Relevance Modeling (GRM) that uses Relevance-Aware Sample Estimation (RASE) for more accurate weighting of expansion terms. Specifically, we identify similar real documents for each generated document and use a neural re-ranker to estimate their relevance. Experiments on three standard document ranking benchmarks show that GRM improves MAP by 6-9% and R@1k by 2-4%, surpassing previous methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "3305422",
                    "name": "Ivan Sekulic"
                },
                {
                    "authorId": "2113355478",
                    "name": "Shubham Chatterjee"
                },
                {
                    "authorId": "49694325",
                    "name": "Jeffrey Stephen Dalton"
                },
                {
                    "authorId": "145876066",
                    "name": "F. Crestani"
                }
            ]
        },
        {
            "paperId": "2c32a5fcb393df9cc7a978cf35dbe880b0f45622",
            "title": "Generative Relevance Feedback with Large Language Models",
            "abstract": "Current query expansion models use pseudo-relevance feedback to improve first-pass retrieval effectiveness; however, this fails when the initial results are not relevant. Instead of building a language model from retrieved results, we propose Generative Relevance Feedback (GRF) that builds probabilistic feedback models from long-form text generated from Large Language Models. We study the effective methods for generating text by varying the zero-shot generation subtasks: queries, entities, facts, news articles, documents, and essays. We evaluate GRF on document retrieval benchmarks covering a diverse set of queries and document collections, and the results show that GRF methods significantly outperform previous PRF methods. Specifically, we improve MAP between 5-19% and NDCG@10 17-24% compared to RM3 expansion, and achieve state-of-the-art recall across all datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "2113355478",
                    "name": "Shubham Chatterjee"
                },
                {
                    "authorId": "49694325",
                    "name": "Jeffrey Stephen Dalton"
                }
            ]
        },
        {
            "paperId": "4c0c159c48fb8516cae8a2a42788e3de58ccff3c",
            "title": "Adaptive Latent Entity Expansion for Document Retrieval",
            "abstract": "Despite considerable progress in neural relevance ranking techniques, search engines still struggle to process complex queries effectively - both in terms of precision and recall. Sparse and dense Pseudo-Relevance Feedback (PRF) approaches have the potential to overcome limitations in recall, but are only effective with high precision in the top ranks. In this work, we tackle the problem of search over complex queries using three complementary techniques. First, we demonstrate that applying a strong neural re-ranker before sparse or dense PRF can improve the retrieval effectiveness by 5-8%. This improvement in PRF effectiveness can be attributed directly to improving the precision of the feedback set. Second, we propose an enhanced expansion model, Latent Entity Expansion (LEE), which applies fine-grained word and entity-based relevance modelling incorporating localized features. Specifically, we find that by including both words and entities for expansion achieve a further 2-8% improvement in NDCG. Our analysis also demonstrated that LEE is largely robust to its parameters across datasets and performs well on entity-centric queries. And third, we include an 'adaptive' component in the retrieval process, which iteratively refines the re-ranking pool during scoring using the expansion model and avoids re-ranking additional documents. We find that this combination of techniques achieves the best NDCG, MAP and R@1000 results on the TREC Robust 2004 and CODEC document datasets, demonstrating a significant advancement in expansion effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "2113355478",
                    "name": "Shubham Chatterjee"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "49694325",
                    "name": "Jeffrey Stephen Dalton"
                }
            ]
        },
        {
            "paperId": "f5448f9e6c3d916cb52ad6b9f9eee0ad379914f7",
            "title": "Generative and Pseudo-Relevant Feedback for Sparse, Dense and Learned Sparse Retrieval",
            "abstract": "Pseudo-relevance feedback (PRF) is a classical approach to address lexical mismatch by enriching the query using first-pass retrieval. Moreover, recent work on generative-relevance feedback (GRF) shows that query expansion models using text generated from large language models can improve sparse retrieval without depending on first-pass retrieval effectiveness. This work extends GRF to dense and learned sparse retrieval paradigms with experiments over six standard document ranking benchmarks. We find that GRF improves over comparable PRF techniques by around 10% on both precision and recall-oriented measures. Nonetheless, query analysis shows that GRF and PRF have contrasting benefits, with GRF providing external context not present in first-pass retrieval, whereas PRF grounds the query to the information contained within the target corpus. Thus, we propose combining generative and pseudo-relevance feedback ranking signals to achieve the benefits of both feedback classes, which significantly increases recall over PRF methods on 95% of experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "2113355478",
                    "name": "Shubham Chatterjee"
                },
                {
                    "authorId": "49694325",
                    "name": "Jeffrey Stephen Dalton"
                }
            ]
        },
        {
            "paperId": "0ccc62837912325636e7eb993c2f0626aae2dc26",
            "title": "Query-Specific Knowledge Graphs for Complex Finance Topics",
            "abstract": "Across the financial domain, researchers answer complex questions by extensively \u201csearching\u201d for relevant information to generate long-form reports. This workshop paper discusses automating the construction of query-specific document and entity knowledge graphs (KGs) for complex research topics. We focus on the CODEC dataset, where domain experts (1) create challenging questions, (2) construct long natural language narratives, and (3) iteratively search and assess the relevance of documents and entities. For the construction of query-specific KGs, we show that state-of-the-art ranking systems have headroom for improvement, with specific failings due to a lack of context or explicit knowledge representation. We demonstrate that entity and document relevance are positively correlated, and that entity-based query feedback improves document ranking effectiveness. Furthermore, we construct query-specific KGs using retrieval and evaluate using CODEC\u2019s \u201cground-truth graphs\u201d, showing the precision and recall trade-offs. Lastly, we point to future work, including adaptive KG retrieval algorithms and GNN-based weighting methods, while highlighting key challenges such as high-quality data, information extraction recall, and the size and sparsity of complex topic graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "49694325",
                    "name": "Jeffrey Stephen Dalton"
                }
            ]
        },
        {
            "paperId": "21f7c8135396ee19e78fe6c9002bddf3ac6c056d",
            "title": "CODEC: Complex Document and Entity Collection",
            "abstract": "CODEC is a document and entity ranking benchmark that focuses on complex research topics. We target essay-style information needs of social science researchers, i.e. \"How has the UK's Open Banking Regulation benefited Challenger Banks\". CODEC includes 42 topics developed by researchers and a new focused web corpus with semantic annotations including entity links. This resource includes expert judgments on 17,509 documents and entities (416.9 per topic) from diverse automatic and interactive manual runs. The manual runs include 387 query reformulations, providing data for query performance prediction and automatic rewriting evaluation. CODEC includes analysis of state-of-the-art systems, including dense retrieval and neural re-ranking. The results show the topics are challenging with headroom for document and entity ranking improvement. Query expansion with entity information shows significant gains on document ranking, demonstrating the resource's value for evaluating and improving entity-oriented search. We also show that the manual query reformulations significantly improve document ranking and entity ranking performance. Overall, CODEC provides challenging research topics to support the development and evaluation of entity-centric search methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "2105439683",
                    "name": "Paul Owoicho"
                },
                {
                    "authorId": "1796270950",
                    "name": "Carlos Gemmell"
                },
                {
                    "authorId": "2164704207",
                    "name": "Sophie Fischer"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "145269114",
                    "name": "Jeffrey Dalton"
                }
            ]
        },
        {
            "paperId": "36342da8ef61828be9c756f225e13c21616e41d3",
            "title": "VILT: Video Instructions Linking for Complex Tasks",
            "abstract": "This work addresses challenges in developing conversational assistants that support rich multimodal video interactions to accomplish real-world tasks interactively. We introduce the task of automatically linking instructional videos to task steps as \"Video Instructions Linking for Complex Tasks\" (VILT). Specifically, we focus on the domain of cooking and empowering users to cook meals interactively with a video-enabled Alexa skill. We create a reusable benchmark with 61 queries from recipe tasks and curate a collection of 2,133 instructional \"How-To\" cooking videos. Studying VILT with state-of-the-art retrieval methods, we find that dense retrieval with ANCE is the most effective, achieving an NDCG@3 of 0.566 and P@1 of 0.644. We also conduct a user study that measures the effect of incorporating videos in a real-world task setting, where 10 participants perform several cooking tasks with varying multimodal experimental conditions using a state-of-the-art Alexa TaskBot system. The users interacting with manually linked videos said they learned something new 64% of the time, which is a 9% increase compared to the automatically linked videos (55%), indicating that linked video relevance is important for task learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2164704207",
                    "name": "Sophie Fischer"
                },
                {
                    "authorId": "1796270950",
                    "name": "Carlos Gemmell"
                },
                {
                    "authorId": "145052856",
                    "name": "Iain Mackie"
                },
                {
                    "authorId": "145269114",
                    "name": "Jeffrey Dalton"
                }
            ]
        }
    ]
}