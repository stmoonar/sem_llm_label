{
    "authorId": "1694274",
    "papers": [
        {
            "paperId": "cba74e0f34cab9b27b9075d6b933caf59267b7ba",
            "title": "Identification of Regulatory Requirements Relevant to Business Processes: A Comparative Study on Generative AI, Embedding-based Ranking, Crowd and Expert-driven Methods",
            "abstract": "Organizations face the challenge of ensuring compliance with an increasing amount of requirements from various regulatory documents. Which requirements are relevant depends on aspects such as the geographic location of the organization, its domain, size, and business processes. Considering these contextual factors, as a first step, relevant documents (e.g., laws, regulations, directives, policies) are identified, followed by a more detailed analysis of which parts of the identified documents are relevant for which step of a given business process. Nowadays the identification of regulatory requirements relevant to business processes is mostly done manually by domain and legal experts, posing a tremendous effort on them, especially for a large number of regulatory documents which might frequently change. Hence, this work examines how legal and domain experts can be assisted in the assessment of relevant requirements. For this, we compare an embedding-based NLP ranking method, a generative AI method using GPT-4, and a crowdsourced method with the purely manual method of creating relevancy labels by experts. The proposed methods are evaluated based on two case studies: an Australian insurance case created with domain experts and a global banking use case, adapted from SAP Signavio's workflow example of an international guideline. A gold standard is created for both BPMN2.0 processes and matched to real-world textual requirements from multiple regulatory documents. The evaluation and discussion provide insights into strengths and weaknesses of each method regarding applicability, automation, transparency, and reproducibility and provide guidelines on which method combinations will maximize benefits for given characteristics such as process usage, impact, and dynamics of an application scenario.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219779350",
                    "name": "Catherine Sai"
                },
                {
                    "authorId": "2092368",
                    "name": "S. Sadiq"
                },
                {
                    "authorId": "2112660847",
                    "name": "Lei Han"
                },
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                },
                {
                    "authorId": "2237649528",
                    "name": "Stefanie Rinderle-Ma"
                }
            ]
        },
        {
            "paperId": "0540d0f570ba79c3518a2c33e4224132c415a0a2",
            "title": "DataOps-4G: On Supporting Generalists in Data Quality Discovery",
            "abstract": "Data preparation has become a necessary but labor and resource-intensive step to perform data analytics. To date, such activities still require considerable manual effort from experts. In this paper, we focus on a specific data preparation activity, namely data quality discovery. We explore different settings in which data workers undertake data quality discovery tasks and the implications of those settings for the efficiency and effectiveness of data workers. To this end, we propose DataOps-4G, a data quality discovery platform for generalists that allows users to interact with data without the need to write code. We wrap up pre-defined code snippets that implement useful functionalities to explore data quality and bundle the code into so-called DataOps. Then, we conduct a lab-based user study to evaluate our DataOps-4G platform from two perspectives: (i) effectiveness, the accuracy of the outcomes achieved by participants; and (ii) efficiency, their effort and strategies in task completion. Our experimental results uncover how effectiveness and efficiency can be affected by their task completion patterns and strategies. This opens up the possibility of popularizing data quality discovery processes by employing non-experts (e.g., from crowdsourcing platforms) and consequently allowing experts to focus on more complex activities (e.g., building machine learning models).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155808180",
                    "name": "Shaochen Yu"
                },
                {
                    "authorId": "51226297",
                    "name": "Tianwa Chen"
                },
                {
                    "authorId": "2112660847",
                    "name": "Lei Han"
                },
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                },
                {
                    "authorId": "2092368",
                    "name": "S. Sadiq"
                }
            ]
        },
        {
            "paperId": "068f5fbc8c3f754fb23421a3b3240e1ab308c5cb",
            "title": "Human-AI Cooperation to Tackle Misinformation and Polarization",
            "abstract": "I M A G E B Y H U R C A A DOMINANT NARRATIVE of the past decade is that algorithms contribute to a misinformed and segregated society. Perhaps paradoxically, algorithms are often sought as solutions to such problems. We describe a significant emerging trend away from this techno-solutionist approach that seeks to create and understand a new paradigm: a productive interplay between algorithms and people. Two relevant test cases are being explored in our region: The first addresses a new framework to tackle misinformation by assisting fact-checkers with computational methods, and the second seeks new models to understand how search engines deliver personalized search results when little or no algorithmic personalization exists. In late 2020 and early 2021, the Australian Communication and Media Authority conducted a study to analyze the state of misinformation in Australia. The findings, reported to the Australian Government in June 2021, showed that four out of five Australian adults had been exposed to misinformation about COVID-19. They also found that online misinformation, such as the propagation of anti-vaccine narratives within the Australian community, had a direct negative impact on the trust that people place in democratic institutions and public health agencies. These narratives often originate overseas but quickly spread through local communities. The fact-checking organizations that have traditionally verified statements made by public figures or politicians in public and mainstream media now must also monitor and debunk dramatically faster-spreading claims on social media platforms. Narratives containing misinformation are having a direct and negative impact on how people consume information: They may influence the content we engage with and the search terms we enter.10 Given that an informed citizenry is a cornerstone of democracy, public decision making is at risk. The significance of the problem was also recognized in the International Cyber and Critical Technology Engagement Strategy released by the Australian Government, which identifies digital misinformation as a clear risk to the security and safety of Australia, the Indo-Pacific region, and beyond. Countries across East Asia and Oceania introduced legislation that specifically targets so-called \u2018fake news\u2019 and they created voluntary codes of practice developed in partnership with the technology industry. Despite such efforts, as of December 2022, out of the 122 currently verified signatories of the Poynter\u2019s International FactChecking Network (IFCN), only eight Human-AI Cooperation to Tackle Misinformation and Polarization DOI:10.1145/3588431",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1630446247",
                    "name": "Damiano Spina"
                },
                {
                    "authorId": "144721996",
                    "name": "M. Sanderson"
                },
                {
                    "authorId": "16054378",
                    "name": "Daniel Angus"
                },
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                },
                {
                    "authorId": "4797155",
                    "name": "Dana Mckay"
                },
                {
                    "authorId": "2796370",
                    "name": "L. L. Saling"
                },
                {
                    "authorId": "34286525",
                    "name": "Ryen W. White"
                }
            ]
        },
        {
            "paperId": "10bf7808a599c4c94402eb18404d946fb6309a8e",
            "title": "Human-in-the-loop Regular Expression Extraction for Single Column Format Inconsistency",
            "abstract": "Format inconsistency is one of the most frequently appearing data quality issues encountered during data cleaning. Existing automated approaches commonly lack applicability and generalisability, while approaches with human inputs typically require specialized skills such as writing regular expressions. This paper proposes a novel hybrid human-machine system, namely \u201cData-Scanner-4C\u201d, which leverages crowdsourcing to address syntactic format inconsistencies in a single column effectively. We first ask crowd workers to create examples from single-column data through \u201cdata selection\u201d and \u201cresult validation\u201d tasks. Then, we propose and use a novel rule-based learning algorithm to infer the regular expressions that propagate formats from created examples to the entire column. Our system integrates crowdsourcing and algorithmic format extraction techniques in a single workflow. Having human experts write regular expressions is no longer required, thereby reducing both the time as well as the opportunity for error. We conducted experiments through both synthetic and real-world datasets, and our results show how the proposed approach is applicable and effective across data types and formats.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155808180",
                    "name": "Shaochen Yu"
                },
                {
                    "authorId": "2112660847",
                    "name": "Lei Han"
                },
                {
                    "authorId": "1780384",
                    "name": "M. Indulska"
                },
                {
                    "authorId": "2092368",
                    "name": "S. Sadiq"
                },
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                }
            ]
        },
        {
            "paperId": "1af16f432f124c4b5c5f67b0d6d639bfb539131c",
            "title": "Firearms on Twitter: A Novel Object Detection Pipeline",
            "abstract": "Social media is an important source of real-time imagery concerning world events. One subset of social media posts which may be of particular interest are those featuring firearms. These posts can give insight into weapon movements, troop activity and civilian safety. Object detection tools offer important opportunities for insight into these images. Unfortunately, these images can be visually complex, poorly lit and generally challenging for object detection models. We present an analysis of existing gun detection datasets, and find that these datasets to not effectively address the challenge of gun detection on real-life images. Following this, we present a novel object detection pipeline. We train our pipeline on a number of datasets including one created for this investigation made up of Twitter images of the Russo-Ukrainian War. We compare the performance of our model as trained on the different datasets to baseline numbers provided by original authors as well as a YOLO v5 benchmark. We find that our model outperforms the state-of-the-art benchmarks on contextually rich, real-life-derived imagery of firearms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2221419258",
                    "name": "Ryan Harvey"
                },
                {
                    "authorId": "2875254",
                    "name": "R. Lebret"
                },
                {
                    "authorId": "2186551984",
                    "name": "St\u00e9phane Massonnet"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                },
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                }
            ]
        },
        {
            "paperId": "5e34f0ab2724e626e2e7cbad4aacb15a5bcdf27d",
            "title": "Report on the Dagstuhl Seminar on Frontiers of Information Access Experimentation for Research and Education",
            "abstract": "This report documents the program and the outcomes of Dagstuhl Seminar 23031 \"Frontiers of Information Access Experimentation for Research and Education\", which brought together 38 participants from 12 countries. The seminar addressed technology-enhanced information access (information retrieval, recommender systems, natural language processing) and specifically focused on developing more responsible experimental practices leading to more valid results, both for research as well as for scientific education. The seminar featured a series of long and short talks delivered by participants, who helped in setting a common ground and in letting emerge topics of interest to be explored as the main output of the seminar. This led to the definition of five groups which investigated challenges, opportunities, and next steps in the following areas: reality check, i.e. conducting real-world studies, human-machine-collaborative relevance judgment frameworks, overcoming methodological challenges in information retrieval and recommender systems through awareness and education, results-blind reviewing, and guidance for authors. Date: 15--20 January 2023. Website: https://www.dagstuhl.de/23031.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052765096",
                    "name": "Christine Bauer"
                },
                {
                    "authorId": "1750995",
                    "name": "Ben Carterette"
                },
                {
                    "authorId": "2137867216",
                    "name": "Joeran Beel"
                },
                {
                    "authorId": "1491609376",
                    "name": "Timo Breuer"
                },
                {
                    "authorId": "1751287",
                    "name": "C. Clarke"
                },
                {
                    "authorId": "2815511",
                    "name": "Anita Crescenzi"
                },
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                },
                {
                    "authorId": "2051747173",
                    "name": "G. Nunzio"
                },
                {
                    "authorId": "145798497",
                    "name": "Laura Dietz"
                },
                {
                    "authorId": "80808662",
                    "name": "G. Faggioli"
                },
                {
                    "authorId": "3001795",
                    "name": "B. Ferwerda"
                },
                {
                    "authorId": "1490763899",
                    "name": "Maik Fr\u00f6be"
                },
                {
                    "authorId": "145072133",
                    "name": "Matthias Hagen"
                },
                {
                    "authorId": "1699657",
                    "name": "A. Hanbury"
                },
                {
                    "authorId": "2731925",
                    "name": "C. Hauff"
                },
                {
                    "authorId": "1705282",
                    "name": "D. Jannach"
                },
                {
                    "authorId": "2167781708",
                    "name": "Noriko Kando"
                },
                {
                    "authorId": "1713134",
                    "name": "E. Kanoulas"
                },
                {
                    "authorId": "2477993",
                    "name": "Bart P. Knijnenburg"
                },
                {
                    "authorId": "2993548",
                    "name": "Udo Kruschwitz"
                },
                {
                    "authorId": "1954475",
                    "name": "Maria Maistro"
                },
                {
                    "authorId": "119665711",
                    "name": "L. Michiels"
                },
                {
                    "authorId": "73425445",
                    "name": "A. Papenmeier"
                },
                {
                    "authorId": "3046200",
                    "name": "Martin Potthast"
                },
                {
                    "authorId": "143752702",
                    "name": "Paolo Rosso"
                },
                {
                    "authorId": "40404161",
                    "name": "A. Said"
                },
                {
                    "authorId": "34588911",
                    "name": "Philipp Schaer"
                },
                {
                    "authorId": "145566115",
                    "name": "C. Seifert"
                },
                {
                    "authorId": "1630446247",
                    "name": "Damiano Spina"
                },
                {
                    "authorId": "1405867539",
                    "name": "Benno Stein"
                },
                {
                    "authorId": "1803171",
                    "name": "N. Tintarev"
                },
                {
                    "authorId": "2060623050",
                    "name": "J. Urbano"
                },
                {
                    "authorId": "2626599",
                    "name": "Henning Wachsmuth"
                },
                {
                    "authorId": "1918235",
                    "name": "M. Willemsen"
                },
                {
                    "authorId": "151068958",
                    "name": "Justin W. Zobel"
                }
            ]
        },
        {
            "paperId": "8c81d4468ed754a215e85bc01a937e68b4ff2d11",
            "title": "Data Bias Management",
            "abstract": "Envisioning a unique approach toward bias and fairness research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                },
                {
                    "authorId": "3445334",
                    "name": "Kevin Roitero"
                },
                {
                    "authorId": "1726978",
                    "name": "Stefano Mizzaro"
                }
            ]
        },
        {
            "paperId": "944713b47f133847bf810b6af3e472b22c18cf45",
            "title": "The Community Notes Observatory: Can Crowdsourced Fact-Checking be Trusted in Practice?",
            "abstract": "Fact-checking is an important tool in fighting online misinformation. However, it requires expert human resources, and thus does not scale well on social media because of the flow of new content. Crowdsourcing has been proposed to tackle this challenge, as it can scale with a smaller cost, but it has always been studied in controlled environments. In this demo, we present the Community Notes Observatory, an online system to evaluate the first large-scale effort of crowdsourced fact-checking deployed in practice. We let demo attendees search and analyze tweets that are fact-checked by Community Notes users and compare the crowd\u2019s activity against professional fact-checkers. The attendees will explore evidence of i) differences in how the crowd and experts select content to be checked, ii) how the crowd and the experts retrieve different resources to fact-check, and iii) the edge the crowd shows in fact-checking scalability and efficiency as compared to expert checkers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2215620251",
                    "name": "Luca Righes"
                },
                {
                    "authorId": "2073358417",
                    "name": "Mohammed Saeed"
                },
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                },
                {
                    "authorId": "1802817",
                    "name": "Paolo Papotti"
                }
            ]
        },
        {
            "paperId": "a07ff1bb709fde521cd4bf106c3bb4fa059092f6",
            "title": "How Many Crowd Workers Do I Need? On Statistical Power when Crowdsourcing Relevance Judgments",
            "abstract": "To scale the size of Information Retrieval collections, crowdsourcing has become a common way to collect relevance judgments at scale. Crowdsourcing experiments usually employ 100\u201310,000 workers, but such a number is often decided in a heuristic way. The downside is that the resulting dataset does not have any guarantee of meeting predefined statistical requirements as, for example, have enough statistical power to be able to distinguish in a statistically significant way between the relevance of two documents. We propose a methodology adapted from literature on sound topic set size design, based on t-test and ANOVA, which aims at guaranteeing the resulting dataset to meet a predefined set of statistical requirements. We validate our approach on several public datasets. Our results show that we can reliably estimate the recommended number of workers needed to achieve statistical power, and that such estimation is dependent on the topic, while the effect of the relevance scale is limited. Furthermore, we found that such estimation is dependent on worker features such as agreement. Finally, we describe a set of practical estimation strategies that can be used to estimate the worker set size, and we also provide results on the estimation of document set sizes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3445334",
                    "name": "Kevin Roitero"
                },
                {
                    "authorId": "2065380079",
                    "name": "David La Barbera"
                },
                {
                    "authorId": "51006308",
                    "name": "Michael Soprano"
                },
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                },
                {
                    "authorId": "1726978",
                    "name": "Stefano Mizzaro"
                },
                {
                    "authorId": "2187429049",
                    "name": "Tetsuya Sakai"
                }
            ]
        },
        {
            "paperId": "bb198f11f7eb1d65150022971fa505dee4b43933",
            "title": "On the Impact of Data Quality on Image Classification Fairness",
            "abstract": "With the proliferation of algorithmic decision-making, increased scrutiny has been placed on these systems. This paper explores the relationship between the quality of the training data and the overall fairness of the models trained with such data in the context of supervised classification. We measure key fairness metrics across a range of algorithms over multiple image classification datasets that have a varying level of noise in both the labels and the training data itself. We describe noise in the labels as inaccuracies in the labelling of the data in the training set and noise in the data as distortions in the data, also in the training set. By adding noise to the original datasets, we can explore the relationship between the quality of the training data and the fairness of the output of the models trained on that data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2215869514",
                    "name": "Aki Barry"
                },
                {
                    "authorId": "2112660847",
                    "name": "Lei Han"
                },
                {
                    "authorId": "1694274",
                    "name": "Gianluca Demartini"
                }
            ]
        }
    ]
}