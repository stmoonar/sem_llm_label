{
    "authorId": "49330176",
    "papers": [
        {
            "paperId": "1c2bba04feadf41f215509f9e2c193f031e01571",
            "title": "Characterizing Performance Limits in Payment Channel Networks",
            "abstract": "With their instant transaction confirmation and high scalability, payment channel networks (PCNs), running off-chain and in parallel with blockchain systems, have recently attracted a substantial amount of research attention. It has been shown that there exists a significant gap between the theoretically optimal performance and the performance achievable given the stringent privacy requirements in practice. However, it remains unclear what the fundamental performance limits and key factors involved are, which turns out to be a challenging problem due to the unique characteristics in PCNs. In this paper, we, for the first time, develop a mathematical model capturing the PCN performance, and examine the impact from a number of factors including channel capacity and transactions. We are articularly interested in obtaining the gap between the theoretically optimal performance and the performance achievable in practice, which characterizes the design space in PCNs for scheduling transactions. Specifically, we derive how different transactions and channel capacities affect the PCN performance and the performance gap. Our analytical characterization of PCNs offers an in-depth understanding on their fundamental trade-off, and provides important insights on the design of PCNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48273384",
                    "name": "Yuechen Tao"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "91269142",
                    "name": "Baochun Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "360425be48fd54983553cb9fcb7934b77d1a9c8e",
            "title": "Publishing Graphs Under Node Differential Privacy",
            "abstract": "Differential privacy (DP) has become the de facto standard of privacy protection. For graphs, there are two widely used definitions of differential privacy, namely, edge differential privacy (edge-DP) and node differential privacy (node-DP), and node-DP is preferred when the minimal unit of interest is a node. To preserve node-DP, one can develop different methods to answer each specific graph query, or develop a graph publishing method to answer all graph queries. However, no existing works worked on such graph publishing methods. In this work, we propose two methods for publishing graphs under node-DP. One is the node-level perturbation algorithm which modifies the input graph by randomly inserting and removing nodes. The other one is the edge-level perturbation algorithm which randomly removing edges and inserting nodes. Both methods can achieve a flexible privacy guarantee by adjusting the running parameters. We conduct extensive experiments on both real-world and synthetic graphs to show the effectiveness and efficiency of proposed algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054880853",
                    "name": "Xun Jian"
                },
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "533194b8718136eb40b2fe897d84fcd8fbc92510",
            "title": "Centralized Routing for Bike-Sharing Systems",
            "abstract": "Bike-sharing systems, where people rent bikes typically for last-mile commuting, have gained great popularity in recent years due to the rapid development of mobile networks. Station-based bike-sharing systems have been widely studied in both academia and industry, where problems like bike rental demand prediction and bike redistribution have been discussed. In contrast, not much attention has been paid to the routing algorithms for shared-bike riders. A routing solution consists of two stations, suggesting where to rent and return a bike. Existing routing works generally target a single rider. However, during rush hours, there often exist routing requests from multiple riders simultaneously, which has not been carefully investigated before. In this paper, we study the routing problem for multiple shared-bike riders with hardness analyses and approximation algorithms. The challenge lies in how to allocate the limited resources (bikes/docks at the stations) among the competing riders. We show that this problem is NP-hard, and thus propose two heuristics. We also propose an optimization technique on routing plan generations, to improve the efficiency of the algorithms. Extensive experiments have been carried out to verify the performance of the proposed algorithms. It turns out that the greedy-based routing algorithm, which has an approximation factor of <inline-formula><tex-math notation=\"LaTeX\">$\\frac{1}{3}$</tex-math><alternatives><mml:math><mml:mfrac><mml:mn>1</mml:mn><mml:mn>3</mml:mn></mml:mfrac></mml:math><inline-graphic xlink:href=\"zheng-ieq1-3073983.gif\"/></alternatives></inline-formula>, is both effective and efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49537866",
                    "name": "Libin Zheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1773086",
                    "name": "C. Shahabi"
                }
            ]
        },
        {
            "paperId": "94147b0526077512b2e0abd5dbff93b2252623e3",
            "title": "Complex Event Summarization Using Multi-Social Attribute Correlation",
            "abstract": "Complex social event summarization is a problem which has been shown having great utility for real-world applications, including crisis management, rumor control and government policy tracking. In recent years there has been significant research effort spent on effectively extracting meaningful textual descriptions of an event. However, in many critical situations, social events are complex and context-sensitive, which demands the online summarization of social events in an integrated manner. In this paper, we propose the first online complex social event summarization approach, namely SOMA, which summarizes the complex social events over multiple attributes including media content and contexts simultaneously. Specifically, we first propose a deep learning model that comprehensively summarizes events in regards to the text description and locations that they appear in, by utilizing their hidden connections in posts. We then propose a summary generator over time, text and location to achieve a maximal coverage of the summary over the original social event and minimal redundancy of the summary. Furthermore, we propose a location estimation method to address the location sparsity issue of complex events by mining the correlation between text and location. The evaluation over four real-event datasets and three benchmark datasets shows that our proposed approach outperforms the existing solutions for event summarizaiton in terms of effectiveness and efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145309212",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "2090427139",
                    "name": "Jeffrey Chan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144302930",
                    "name": "T. Sellis"
                },
                {
                    "authorId": "2192801889",
                    "name": "Yanchun Zhang"
                }
            ]
        },
        {
            "paperId": "180c9652c80ea6415e616ea46d0153186d3590c5",
            "title": "Privacy-preserving Cooperative Online Matching over Spatial Crowdsourcing Platforms",
            "abstract": "With the continuous development of spatial crowdsourcing platform, online task assignment problem has been widely studied as a typical problem in spatial crowdsourcing. Most of the existing studies are based on a single-platform task assignment to maximize the platform's revenue. Recently, cross online task assignment has been proposed, aiming at increasing the mutual benefit through cooperations. However, existing methods fail to consider the data privacy protection in the process of cooperation and cause the leakage of sensitive data such as the location of a request and the historical data of cooperative platforms. In this paper, we propose Privacy-preserving Cooperative Online Matching (PCOM), which protects the privacy of the users and workers on their respective platforms. We design a PCOM framework and provide theoretical proof that the framework satisfies the differential privacy property. We then propose two PCOM algorithms based on two different privacy-preserving strategies. Extensive experiments on real and synthetic datasets confirm the effectiveness and efficiency of our algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143688367",
                    "name": "Yi Yang"
                },
                {
                    "authorId": "2329311",
                    "name": "Yurong Cheng"
                },
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2199045",
                    "name": "Yongjiao Sun"
                }
            ]
        },
        {
            "paperId": "27162389d8dc6594f3922cd746f9e8880cd35eee",
            "title": "Faster and Better Solution to Embed Lp Metrics by Tree Metrics",
            "abstract": "Hierarchically Separated Tree (HST) is the most popular solution to embed a metric space into a tree metric. By using HSTs, many optimization problems, which are hard on defined metrics, become easier to get good approximation bounds with respect to the effectiveness, e.g., task assignment, trip planning, and facility location planning. Existing work focuses on constructing HSTs for arbitrary metric spaces, which makes a general-purpose algorithm take at least O(n2)-time to get tight distortion guarantees O(log(n)). Here, distortion is a prevalent measurement of HSTs' effectiveness and usability. However, we observe that (1) in many applications that HSTs are applied, only Lp metrics are used (e.g., Euclidean space), (2) the state-of-the-art solution is still time-consuming to construct HSTs for large-scale data, and (3) distortions of existing algorithms are only satisfactory for high-dimensional data. Thus, in this paper, we are motivated to study the Embedding Lp metrics through Tree metrics (ELT) problem. We aim to design a faster algorithm than O(n2) time to construct HSTs with not only O(log(n)) distortion guarantees but also good and robust empirical results. Specifically, we first present a divide-and-conquer based general framework and prove that it has a distortion guarantee of O(log(n)). To achieve a better time complexity than O(n2), we next design two optimization techniques: reducing to nearest neighbor search (by indexing) and sampling. Finally, extensive experiments demonstrate that our algorithm DCsam outperforms the state-of-the-art algorithms by a large margin in terms of both distortion and running time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47211830",
                    "name": "Yuxiang Zeng"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "2fa2b1c2712656733cdb273965632d15d2b3442d",
            "title": "Efficient Approximate Range Aggregation over Large-scale Spatial Data Federation (Extended Abstract)",
            "abstract": "Data federations notably increase the amount of data available for data-intensive applications such as smart mobility planning and public health emergency responses. Yet they also challenge the conventional implementation of range aggregation queries because the raw data cannot be shared within the federation and the data partition at each data silo is fixed during query processing. In this work, we propose the first-of-its-kind approximate algorithms for efficient range aggregation over spatial data federation. We devise novel single-silo sampling algorithms that process queries in parallel and design a level sampling based algorithm which reduces the time complexity of local queries at each data silo to $O(\\log\\frac{1}{\\epsilon})$, where \u220a is the approximation ratio of the accuracy guarantee. Extensive experiments on real-world dataset validate the efficiency and effectiveness of the solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "114438759",
                    "name": "Yexuan Shi"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "47211830",
                    "name": "Yuxiang Zeng"
                },
                {
                    "authorId": "2735715",
                    "name": "Zimu Zhou"
                },
                {
                    "authorId": "1696332",
                    "name": "Bolin Ding"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "525b317e55f0eedc1eaf88fb79716469b03c0c51",
            "title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry",
            "abstract": "Interdisciplinary experimental science (e.g., medicinal chemistry) refers to the disciplines that integrate knowledge from different scientific backgrounds and involve experiments in the research process. Deciding \u201cin what direction to proceed\u201d is critical for the success of the research in such disciplines, since the time, money, and resource costs of the subsequent research steps depend largely on this decision. However, such a direction identification task is challenging in that researchers need to integrate information from large-scale, heterogeneous materials from all associated disciplines and summarize the related publications of which the core contributions are often showcased in diverse formats. The task also requires researchers to estimate the feasibility and potential in future experiments in the selected directions. In this work, we selected medicinal chemistry as a case and presented an interactive visual tool, MedChemLens, to assist medicinal chemists in choosing their intended directions of research. This task is also known as drug target (i.e., disease-linked proteins) selection. Given a candidate target name, MedChemLens automatically extracts the molecular features of drug compounds from chemical papers and clinical trial records, organizes them based on the drug structures, and interactively visualizes factors concerning subsequent experiments. We evaluated MedChemLens through a within-subjects study (N=16). Compared with the control condition (i.e., unrestricted online search without using our tool), participants who only used MedChemLens reported faster search, better-informed selections, higher confidence in their selections, and lower cognitive load.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1790935109",
                    "name": "Chuhan Shi"
                },
                {
                    "authorId": "2186286938",
                    "name": "Fei Nie"
                },
                {
                    "authorId": "11979743",
                    "name": "Yicheng Hu"
                },
                {
                    "authorId": "26339093",
                    "name": "Yige Xu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2149143741",
                    "name": "Xiaojuan Ma"
                },
                {
                    "authorId": "2163710313",
                    "name": "Qiong Luo"
                }
            ]
        },
        {
            "paperId": "5e2acd94c45ec36fc8849d2cca5b56a2695b1fed",
            "title": "Crowdsourced Fact Validation for Knowledge Bases",
            "abstract": "In spite of its wide usage in various applications, existing construction methods for Knowledge Base (KB) are still on their way to obtaining 100% correct facts. Thus, employing crowd workers to validate a KB has been proposed to improve its reliability. Most of the existing works focus on devising games with proper incentives to engage workers in validating more facts, but rarely consider matching facts with proper workers. Facts have diverse domains (topics), which naturally require workers of different expertise. In addition, they also generally have different utilities, i.e., some are more heavily used than others. Thus, distinguishing the facts in terms of utility to give them different validation priorities is meaningful, especially when the budget is limited. To this end, we study the crowdsourced fact validation problem which considers worker domains and fact utilities, and find that with some reductions, it can be solved by the existing minimum cost network flow method. However, directly employing that method requires a huge time cost. We thereby propose an optimized network flow method which reduces the network complexity to save the time cost by properly grouping the facts. Furthermore, we propose an incremental validation method, which utilizes the previous results for validating an evolving KB. We finally conduct extensive experiments to demonstrate the effectiveness of the proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49537866",
                    "name": "Libin Zheng"
                },
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2155521332",
                    "name": "Jianxing Yu"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                },
                {
                    "authorId": "2152938197",
                    "name": "Jian Yin"
                }
            ]
        },
        {
            "paperId": "60693f19d985bfc768ce4910ac98f3ccd016a865",
            "title": "Mixing Transactions with Arbitrary Values on Blockchains",
            "abstract": "Due to the transparency of blockchain, adversaries can observe the details of a transaction, and then utilize the amount as a unique quasi-identifier to make deanonymization. Nowadays, to obscure the linkages between receivers and senders within a transaction on the blockchain, mixing services are widely applied in many real applications to enhance cryptocurrencies' anonymity. The basic idea of mixing services is to hide an output within several other outputs in a transaction such that adversaries cannot distinguish them by their amounts since they are purposely selected to have the same amount. For a set of original outputs with different amounts, mixing services need to decompose them into a set of decomposed outputs, where any decomposed output has some other decomposed outputs with the same amount. Since the transaction fee is related to the number of outputs, we are motivated to decompose original outputs into a minimal set of decomposed outputs, which is challenging to guarantee the privacy-preserving effect at the same time. In this paper, we formally define the anonymity-aware output decomposition (AA-OD) problem, which aims to find a c-decomposition with a minimum number of decomposed outputs for a given original output set. A c-decomposition guarantees that for any original output $o$, there are at most $c$ of all decomposed outputs with an amount of $x$ coming from $o$. We prove that the AA-OD problem is NP-hard. Thus, we propose an approximation algorithm, namely Boggart11Boggart is a magical creature in J. K. Rowling's Harry Potter series who can shift his shape and no one knows what it looks like., to solve the AA-OD problem with a (2/c + 3)-approximation bound on the number of decomposed outputs. We verify the efficiency and effectiveness of our approach through comprehensive experiments on both real and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1567491249",
                    "name": "Wangze Ni"
                },
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "7a05a5351cc30d7500c0291481899e1a358a72a8",
            "title": "HAP: An Efficient Hamming Space Index Based on Augmented Pigeonhole Principle",
            "abstract": "The emerging deep learning techniques prefer mapping complex data objects (e.g., images, documents) to compact binary vectors (i.e., hash codes) for efficient similarity search. In this paper, we study the problem of indexing large-scale binary databases to support fast Hamming distance-based similarity queries. Existing Hamming space indices usually divide long binary vectors into short disjoint pieces and apply the Pigeonhole Principle to prune unnecessary candidates. In our work, we relax the disjoint partition constraint by allowing dimension redundancy, which yields a tighter pruning bound named Augmented Pigeonhole Principle (APP). Intuitively, APP enables more optimization opportunities by capturing the correlation between database and query workloads. Based on APP, we propose HAP, an efficient Hamming space index framework to support both Hamming range queries and k-NN queries. To guide index construction and run-time query optimization, we introduce a novel DL-base query cardinality estimator named SimCardNet. To further reduce the index space cost, we propose a learned index compression scheme by combining the piece-wise linear approximation (PLA) and Elias-Fano encoding. In addition, we also study the problem of optimizing the execution time of a batch of queries using our index framework. The experimental results on large-scale binary databases reveal that our indexing scheme outperforms the state-of-the-art baselines in terms of both space and time efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112215527",
                    "name": "Qiyu Liu"
                },
                {
                    "authorId": "2923152",
                    "name": "Yanyan Shen"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "9d526d1eca2ec06b4f412b4db5e822bd7fb99982",
            "title": "Black-box Adversarial Attack and Defense on Graph Neural Networks",
            "abstract": "Graph neural networks (GNNs) have achieved great success on various graph tasks. However, recent studies have re-vealed that GNNs are vulnerable to adversarial attacks, including topology modifications and feature perturbations. Regardless of the fruitful progress, existing attackers require node labels and GNN parameters to optimize a bi-level problem, or cannot cover both topology modifications and feature perturbations, which are not practical, efficient, or effective. In this paper, we propose a black-box attacker PEEGA, which is restricted to access node features and graph topology for practicability. Specifically, we propose to measure the negative impact of various adversarial attacks from the perspective of node representations, thereby we formulate a single-level problem that can be efficiently solved. Furthermore, we observe that existing attackers tend to blur the context of nodes through adding edges between nodes with different labels. As a result, GNNs are unable to recognize nodes. Based on this observation, we propose a GNN defender GNAT, which incorporates three augmented graphs, i.e., a topology graph, a feature graph, and an ego graph, to make the context of nodes more distinguishable. Extensive experiments on three real-world datasets demonstrate the effectiveness and efficiency of our proposed attacker, despite the fact that we do not access node labels and GNN parameters. Moreover, the effectiveness and efficiency of our proposed defender are also validated by substantial experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145537904",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "51129379",
                    "name": "Shimin Di"
                },
                {
                    "authorId": "2145274105",
                    "name": "Zijian Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144115026",
                    "name": "Jiannong Cao"
                }
            ]
        },
        {
            "paperId": "9ee5ff538e30a32621e5437301846ec4badaf5d9",
            "title": "Effective Similarity Search on Heterogeneous Networks: A Meta-Path Free Approach",
            "abstract": "Heterogeneous information networks (HINs) are usually used to model information systems with multi-type objects and relations. In contrast, graphs that have a single type of nodes and edges, are often called homogeneous graphs. Measuring similarities among objects is an important task in data mining applications, such as web search, link prediction, and clustering. Currently, several similarity measures are defined for HINs. Most of these measures are based on meta-paths, which show sequences of node classes and edge types along the paths between two nodes. However, meta-paths, which are often designed by domain experts, are hard to enumerate and choose w.r.t. the quality of similarity scores. This makes using existing similarity measures in real applications difficult. To address this problem, we extend SimRank, a well-known similarity measure on homogeneous graphs, to HINs, by introducing the concept of the decay graph. The newly proposed similarity measure is called HowSim, which has the property of being meta-path free, and capturing the structural and semantic similarity simultaneously. The generality and effectiveness of HowSim, and the efficiency of our proposed algorithms for computing HowSim scores, are demonstrated by extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "2108196278",
                    "name": "Zhe Wang"
                },
                {
                    "authorId": null,
                    "name": "Ziyuan Zhao"
                },
                {
                    "authorId": "2145274105",
                    "name": "Zijian Li"
                },
                {
                    "authorId": "2054880853",
                    "name": "Xun Jian"
                },
                {
                    "authorId": "2057309752",
                    "name": "Hao Xin"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1720752866",
                    "name": "Jianchun Song"
                },
                {
                    "authorId": "2109345548",
                    "name": "Zhenhong Chen"
                },
                {
                    "authorId": "2116693875",
                    "name": "Meng-shi Zhao"
                }
            ]
        },
        {
            "paperId": "a0a4d1fe264e7cba5731868babaa78bb3e93bbec",
            "title": "Event Popularity Prediction Using Influential Hashtags from Social Media (Extended Abstract)",
            "abstract": "Event popularity prediction over social media is crucial for estimating information propagation scope, decision making, and emergency prevention. It has been widely inves-tigated by existing approaches focusing on predicting single attribute occurrences which are not comprehensive enough for representing complex social event propagation. Motivated by this, we propose a novel hashtag-influence-based event popularity prediction by mining the impact of an influential hashtag set on the event propagation. We have conducted extensive experiments to prove the effectiveness and efficiency of the proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145309212",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "2090427139",
                    "name": "Jeffrey Chan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144302930",
                    "name": "T. Sellis"
                },
                {
                    "authorId": "49889222",
                    "name": "Yanchun Zhang"
                }
            ]
        },
        {
            "paperId": "b0dec66d8970da461b4bfb5ceb064d26a259b8a3",
            "title": "Lasagne: A Multi-Layer Graph Convolutional Network Framework via Node-aware Deep Architecture (Extended Abstract)",
            "abstract": "In this paper, we propose Lasagne, a novel multi-layer graph convolutional network (GCN) framework to over-come the over-smoothing problem and realize the full poten-tials of deep GCNs. We analyze how node localities affect the information propagation in GCN, propose an adaptive novel node aggregation mechanism and further demystify from a mutual information view. Evaluation results on both real-world benchmark data sets and large-scale industrial production data sets show Lasagne significantly outperforms the state-of- the-art methods without considering the node locality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720763480",
                    "name": "Xupeng Miao"
                },
                {
                    "authorId": "2108232566",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2237813",
                    "name": "Yingxia Shao"
                },
                {
                    "authorId": "144585959",
                    "name": "B. Cui"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1776014",
                    "name": "Ce Zhang"
                },
                {
                    "authorId": "3452475",
                    "name": "Jiawei Jiang"
                }
            ]
        },
        {
            "paperId": "b4d94712d040d02fdd33ae3fb9a56014ae377a1e",
            "title": "Strict and Flexible Rule-Based Graph Repairing",
            "abstract": "Real-life graph datasets extracted from the Web are inevitably full of incompleteness, conflicts, and redundancies, so graph data cleaning shows its necessity. Although rules like data dependencies have been widely studied in relational data repairing, very few works exist to repair graph data. In this article, we introduce a repairing semantics for graphs, called <italic>Graph-Repairing Rules</italic> (<inline-formula><tex-math notation=\"LaTeX\">${\\sf GRR}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"sans-serif\">GRR</mml:mi></mml:math><inline-graphic xlink:href=\"cheng-ieq1-3019817.gif\"/></alternatives></inline-formula>s). This semantics can capture the incompleteness, conflicts, and redundancies in graphs and indicate how to correct these errors. However, this graph repairing semantics can only repair the graphs strictly isomorphic to the rule patterns, which decreases the utility of the rules. To overcome this shortcoming, we further propose a flexible rule-based graph repairing semantics (called <inline-formula><tex-math notation=\"LaTeX\">$\\delta$</tex-math><alternatives><mml:math><mml:mi>\u03b4</mml:mi></mml:math><inline-graphic xlink:href=\"cheng-ieq2-3019817.gif\"/></alternatives></inline-formula><italic>-GRR</italic>). We study three fundamental problems associated with both <inline-formula><tex-math notation=\"LaTeX\">${\\sf GRR}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"sans-serif\">GRR</mml:mi></mml:math><inline-graphic xlink:href=\"cheng-ieq3-3019817.gif\"/></alternatives></inline-formula>s and <inline-formula><tex-math notation=\"LaTeX\">$\\delta$</tex-math><alternatives><mml:math><mml:mi>\u03b4</mml:mi></mml:math><inline-graphic xlink:href=\"cheng-ieq4-3019817.gif\"/></alternatives></inline-formula><italic>-GRR</italic>s, consistency, implication, and termination, which show whether a given set of rules make sense. Repairing the graph data using <inline-formula><tex-math notation=\"LaTeX\">${\\sf GRR}$</tex-math><alternatives><mml:math><mml:mi mathvariant=\"sans-serif\">GRR</mml:mi></mml:math><inline-graphic xlink:href=\"cheng-ieq5-3019817.gif\"/></alternatives></inline-formula>s or <inline-formula><tex-math notation=\"LaTeX\">$\\delta$</tex-math><alternatives><mml:math><mml:mi>\u03b4</mml:mi></mml:math><inline-graphic xlink:href=\"cheng-ieq6-3019817.gif\"/></alternatives></inline-formula><italic>-GRR</italic>s involves a problem of finding isomorphic subgraphs of the graph data, which is NP-complete. To efficiently circumvent the complex calculation of subgraph isomorphism, we design a decomposition-and-join strategy to solve this problem. Extensive experiments on real datasets show that our two graph repairing semantics and corresponding repairing algorithms can effectively and efficiently repair real-life graph data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2329311",
                    "name": "Yurong Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "152358042",
                    "name": "Boyang Li"
                },
                {
                    "authorId": "3068896",
                    "name": "Fusheng Jin"
                }
            ]
        },
        {
            "paperId": "e0129a6b11940ca6ab1396fdd9a57fd9dd219a12",
            "title": "Unified Route Planning for Shared Mobility: An Insertion-based Framework",
            "abstract": "There has been a dramatic growth of shared mobility applications such as ride-sharing, food delivery, and crowdsourced parcel delivery. Shared mobility refers to transportation services that are shared among users, where a central issue is route planning. Given a set of workers and requests, route planning finds for each worker a route, i.e., a sequence of locations to pick up and drop off passengers/parcels that arrive from time to time, with different optimization objectives. Previous studies lack practicability due to their conflicted objectives and inefficiency in inserting a new request into a route, a basic operation called insertion. In addition, previous route planning solutions fail to exploit the appearance patterns of future requests hidden in historical data for optimization. In this paper, we present a unified formulation of route planning called URPSM. It has a well-defined parameterized objective function which eliminates the contradicted objectives in previous studies and enables flexible multi-objective route planning for shared mobility. We propose two insertion-based frameworks to solve the URPSM problem. The first is built upon the plain-insertion widely used in prior studies, which processes online requests only, whereas the second relies on a new insertion operator called prophet-insertion that handles both online and predicted requests. Novel dynamic programming algorithms are designed to accelerate both insertions to only linear time. Theoretical analysis shows that no online algorithm can have a constant competitive ratio for the URPSM problem under the competitive analysis model, yet our prophet-insertion-based framework can achieve a constant optimality ratio under the instance-optimality model. Extensive experimental results on real datasets show that our insertion-based solutions outperform the state-of-the-art algorithms in both effectiveness and efficiency by a large margin (e.g., up to 30 \\( \\times \\) more effective in the objective and up to 20 \\( \\times \\) faster).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "47211830",
                    "name": "Yuxiang Zeng"
                },
                {
                    "authorId": "2735715",
                    "name": "Zimu Zhou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2117101465",
                    "name": "Ke Xu"
                }
            ]
        },
        {
            "paperId": "ed82fe8bf0e3b694a931f823b75d0a261429aa93",
            "title": "Event Popularity Prediction Using Influential Hashtags From Social Media",
            "abstract": "Event popularity prediction over social media is crucial for estimating information propagation scope, decision making, and emergency prevention. However, existing approaches only focus on predicting the occurrences of single attribute such as a message, a hashtag or an image, which are not comprehensive enough for representing complex social event propagation. In this paper, we predict the event popularity, where an event is described as a set of messages containing multiple hashtags. We propose a novel hashtag-influence-based event popularity prediction by mining the impact of an influential hashtag set on the event propagation. Specifically, we first propose a hashtag-influence-based cascade model to select the influential hashtags over an event hashtag graph built by the pairwise hashtag similarity and the topic distribution of event-related hashtags. A novel measurement is proposed to identify the hashtag influence of an event over its content and social impacts. A hashtag correlation-based algorithm is proposed to optimize the seed selection in a greedy manner. Then, we propose an event-fitting boosting model to predict the event popularity by embedding the feature importance over events into the XGBOOST model. Moreover, we propose an event-structure-based method, which incrementally updates the prediction model over social streams. We have conducted extensive experiments to prove the effectiveness and efficiency of the proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145309212",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "2090427139",
                    "name": "Jeffrey Chan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144302930",
                    "name": "T. Sellis"
                },
                {
                    "authorId": "2145048318",
                    "name": "Yanchun Zhang"
                }
            ]
        },
        {
            "paperId": "ef82127fed8b6e3f0246539bf8fafc84cf22f582",
            "title": "Noisy Interactive Graph Search",
            "abstract": "The interactive graph search (IGS) problem aims to locate an initially unknown target node leveraging human intelligence. In IGS, we can gradually find the target node by sequentially asking humans some reachability queries like \"is the target node reachable from a given node x?\". However, human workers may make mistakes when answering these queries. Motivated by this concern, in this paper, we study a noisy version of the IGS problem. Our objective in this problem is to minimize the query complexity while ensuring accuracy. We propose a method to select the query node such that we can push the search process as much as possible and an online method to infer which node is the target after collecting a new answer. By rigorous theoretical analysis, we show that the query complexity of our approach is near-optimal up to a constant factor. The extensive experiments on two real datasets also demonstrate the superiorities of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060868466",
                    "name": "Qianhao Cong"
                },
                {
                    "authorId": "144066821",
                    "name": "Jing Tang"
                },
                {
                    "authorId": "2112027880",
                    "name": "Kai Han"
                },
                {
                    "authorId": "2108616746",
                    "name": "Yuming Huang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1718686",
                    "name": "Yeow Meng Chee"
                }
            ]
        },
        {
            "paperId": "f68b18405328cc471e29a2880665a403002b2477",
            "title": "Maximizing the Utility in Location-Based Mobile Advertising",
            "abstract": "With the rapid development of mobile technology, nowadays, people spend a large amount of time on mobile devices. The locations and contexts of users are easily accessed by mobile advertising brokers, and the brokers can send customers related location-based advertisements. In this paper, we consider an important location-based advertising problem, namely maximum utility advertisement assignment (MUAA) problem, with the estimation of the interests of customers and the contexts of the vendors, we want to maximize the overall utility of ads by determining the ads sent to each customer subject to the constraints of the capacities of customers, the distance ranges and the budgets of vendors. We prove that the MUAA problem is NP-hard and intractable. Thus, we propose one offline approach, namely the <inline-formula><tex-math notation=\"LaTeX\">${\\sf reconciliation\\ approach}$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant=\"sans-serif\">reconciliation</mml:mi><mml:mspace width=\"4pt\"/><mml:mi mathvariant=\"sans-serif\">approach</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"cheng-ieq1-2986198.gif\"/></alternatives></inline-formula>, which has an approximation ratio of <inline-formula><tex-math notation=\"LaTeX\">$(1-\\epsilon)\\cdot \\theta$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>\u03b5</mml:mi><mml:mo>)</mml:mo><mml:mo>\u00b7</mml:mo><mml:mi>\u03b8</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"cheng-ieq2-2986198.gif\"/></alternatives></inline-formula>. In addition, we also address the online scenario, in which customers arrive in a streaming fashion, with one novel online algorithm, namely the <inline-formula><tex-math notation=\"LaTeX\">${\\sf online\\ adaptive\\ factor-aware\\ approach}$</tex-math><alternatives><mml:math><mml:mrow><mml:mi mathvariant=\"sans-serif\">online</mml:mi><mml:mspace width=\"4pt\"/><mml:mi mathvariant=\"sans-serif\">adaptive</mml:mi><mml:mspace width=\"4pt\"/><mml:mi mathvariant=\"sans-serif\">factor</mml:mi><mml:mo>-</mml:mo><mml:mi mathvariant=\"sans-serif\">aware</mml:mi><mml:mspace width=\"4pt\"/><mml:mi mathvariant=\"sans-serif\">approach</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"cheng-ieq3-2986198.gif\"/></alternatives></inline-formula>, which has a competitive ratio (compared to the optimal solution of the offline scenario) of <inline-formula><tex-math notation=\"LaTeX\">$\\frac{\\ln (g)+1}{\\theta }$</tex-math><alternatives><mml:math><mml:mfrac><mml:mrow><mml:mo form=\"prefix\">ln</mml:mo><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>\u03b8</mml:mi></mml:mfrac></mml:math><inline-graphic xlink:href=\"cheng-ieq4-2986198.gif\"/></alternatives></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$g>e$</tex-math><alternatives><mml:math><mml:mrow><mml:mi>g</mml:mi><mml:mo>></mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=\"cheng-ieq5-2986198.gif\"/></alternatives></inline-formula>, where <inline-formula><tex-math notation=\"LaTeX\">$e$</tex-math><alternatives><mml:math><mml:mi>e</mml:mi></mml:math><inline-graphic xlink:href=\"cheng-ieq6-2986198.gif\"/></alternatives></inline-formula> is the base of the natural logarithm. Through extensive experiments, we demonstrate the efficiency and effectiveness of our proposed approaches over both real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "47130133",
                    "name": "Siyuan Liu"
                }
            ]
        },
        {
            "paperId": "006e91b9bc2317af6c791a1ecd444ed64a3f79c9",
            "title": "Crowdrebate: An Effective Platform to Get more Rebate for Customers",
            "abstract": "To encourage users to buy more products, many online stores offer coupons. When a customer finds that the price of the products she/he wants to order is below the threshold of a coupon, she/he might want to place the order together with others to meet this threshold and enjoy more instant rebates. However, to conduct these orders and deliver products to receivers, users may need to pay extra delivery costs. When an order comprises several receivers\u2019 requests, the products in the order should first be delivered from stores to an assigned warehouse, packed into different packages, and delivered to the different receivers. It may be costly than directly delivering products from stores to receivers. For the benefits of buyers, we propose a platform, the Crowdrebate platform, which collects requests from users, groups requests into a set of orders to get more rebates, and relays products to different receivers in an order. The platform will make a profit by getting a proportion from the benefit of the receiver (defined as the rebate minus the extra cost) of orders as its revenue. In this paper, we define the Crowdrebate problem, which aims to maximize the benefit of receivers. We prove the NP-hardness of the Crowdrebate problem. Therefore, we propose a heuristic solution to address the problem. Moreover, we evaluate the effectiveness and efficiency of our algorithm via comprehensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1567491249",
                    "name": "Wangze Ni"
                },
                {
                    "authorId": "2118769565",
                    "name": "Nian Chen"
                },
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                }
            ]
        },
        {
            "paperId": "04bca6cd389c2ec08227e364dabf043c111d9bd4",
            "title": "Privacy-Preserving Batch-based Task Assignment in Spatial Crowdsourcing with Untrusted Server",
            "abstract": "In this paper, we study the privacy-preserving task assignment problem in spatial crowdsourcing, where the locations of both workers and tasks, prior to their release to the server, are perturbed with Geo-Indistinguishability (a differential privacy notion for location-based systems). Different from the previously studied online setting, where each task is assigned immediately upon arrival, we target the batch-based setting, where the server maximizes the number of successfully assigned tasks after a batch of tasks arrive. To achieve this goal, we propose the k-Switch solution, which first divides the workers into small groups based on the perturbed distance between workers/tasks, and then utilizes Homomorphic Encryption (HE) based secure computation to enhance the task assignment. Furthermore, we expedite HE-based computation by limiting the size of the small groups under k. Extensive experiments demonstrate that, in terms of the number of successfully assigned tasks, the k-Switch solution improves batch-based baselines by 5.9X and the existing online solution by 1.74X, with no privacy leak.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "91796388",
                    "name": "Maocheng Li"
                },
                {
                    "authorId": "2110175222",
                    "name": "Jiachuan Wang"
                },
                {
                    "authorId": "49537866",
                    "name": "Libin Zheng"
                },
                {
                    "authorId": "2118499631",
                    "name": "Han Wu"
                },
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                }
            ]
        },
        {
            "paperId": "06f0a9c49438f826a72c6a20ae110af1bd6ae1c8",
            "title": "Event-Participant and Incremental Planning over Event-Based Social Networks",
            "abstract": "In recent years, online Event Based Social Network (EBSN) platforms have become increasingly popular. One typical task of EBSN platforms is to help users make suitable and personalized plans for participating in different interesting social events. Existing techniques either ignore the minimum-participant requirement constraint for each event, which is crucially needed for some events to be held successfully, or assume that events would not change once announced. In this paper, we address the above inadequacies of existing EBSN techniques. We formally define the Global Event Planning with Constraints (GEPC) problem, and its incremental variant. Since these problems are NP-hard, and provide approximate solutions. Finally, we verify the effectiveness and efficiency of our proposed algorithms through extensive experiments over real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2329311",
                    "name": "Yurong Cheng"
                },
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1398517004",
                    "name": "C. Giraud-Carrier"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "152358042",
                    "name": "Boyang Li"
                }
            ]
        },
        {
            "paperId": "12de79e60271e2bd8aed4efc98d4a0b67fe62ade",
            "title": "An Adaptive Graph Pre-training Framework for Localized Collaborative Filtering",
            "abstract": "Graph neural networks (GNNs) have been widely applied in the recommendation tasks and have achieved very appealing performance. However, most GNN-based recommendation methods suffer from the problem of data sparsity in practice. Meanwhile, pre-training techniques have achieved great success in mitigating data sparsity in various domains such as natural language processing (NLP) and computer vision (CV). Thus, graph pre-training has the great potential to alleviate data sparsity in GNN-based recommendations. However, pre-training GNNs for recommendations faces unique challenges. For example, user-item interaction graphs in different recommendation tasks have distinct sets of users and items, and they often present different properties. Therefore, the successful mechanisms commonly used in NLP and CV to transfer knowledge from pre-training tasks to downstream tasks such as sharing learned embeddings or feature extractors are not directly applicable to existing GNN-based recommendations models. To tackle these challenges, we delicately design an adaptive graph pre-training framework for localized collaborative filtering (ADAPT). It does not require transferring user/item embeddings, and is able to capture both the common knowledge across different graphs and the uniqueness for each graph simultaneously. Extensive experimental results have demonstrated the effectiveness and superiority of ADAPT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108941389",
                    "name": "Yiqi Wang"
                },
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "2145976175",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2112108587",
                    "name": "Mingzheng Li"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "2110972816",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "152297693",
                    "name": "Philip S. Yu"
                }
            ]
        },
        {
            "paperId": "1dc2edea1b328c846bd48c59541f7b7c49b9d59e",
            "title": "Cache-based GNN System for Dynamic Graphs",
            "abstract": "Graph Neural Networks (GNNs) have achieved great success in downstream applications due to their ability to learn node representations. However, in many applications, graphs are not static. They often evolve with changes, such as the adjustment of node attributes or graph structures. These changes require node representations to be updated accordingly. It is non-trivial to apply current GNNs to update node representations in a scalable manner. Recent research proposes two types of solutions. The first solution, sampling neighbors for the influenced nodes, requires expensive processing for each node. The second solution, reducing the repeated computations by merging the shared neighbors, cannot speed up the updating process if the influenced nodes do not share neighbors. Most importantly, the above solutions ignore the hidden representations obtained in the previous times that can be reused to accelerate the representation updating. In this paper, we propose a general cache-based GNN system to accelerate the representation updating. Specifically, we cache a set of hidden representations obtained in the previous times, and then reuse them in the next time. To identify valuable hidden representations, we first estimate the number of hidden representations and their combinations that can be reused. Secondly, we formulate the k-assembler problem that selects k representations to maximize the saved time for the next updating process. Experiments on three real-world graphs show that the cache-based GNN system can significantly speed up the representation updating for various GNNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145537904",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "40c25a73f59ed5c2a4a56c34f4a899729f6667c2",
            "title": "Two-Sided Online Micro-Task Assignment in Spatial Crowdsourcing",
            "abstract": "With the rapid development of smartphones, spatial crowdsourcing platforms are getting popular. A foundational research of spatial crowdsourcing is to allocate micro-tasks to suitable crowd workers. Many existing studies focus on the offline scenario, where all the spatiotemporal information of micro-tasks and crowd workers is given. In this paper, we focus on the online scenario and identify a more practical micro-task allocation problem, called the <italic><underline>G</underline>lobal <underline>O</underline>nline <underline>M</underline>icro-task <underline>A</underline>llocation in spatial crowdsourcing</italic> (GOMA) problem. We first extend the state-of-the-art algorithm for the online maximum weighted bipartite matching problem to the GOMA problem as the baseline algorithm. Although the baseline algorithm provides a theoretical guarantee for the worst case, its average performance in practice is not good enough since the worst case happens with a very low probability in the real world. Thus, we consider the average performance of online algorithms, <italic>a.k.a.</italic> random order model. We propose a two-phase-based framework, based on which we present the TGOA algorithm with a <inline-formula><tex-math notation=\"LaTeX\">$\\frac{1}{4}$</tex-math><alternatives><mml:math><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:math><inline-graphic xlink:href=\"chen-ieq1-2948863.gif\"/></alternatives></inline-formula>-competitive ratio under the random order model. To improve its efficiency, we further design the TGOA-Greedy and TGOA-OP algorithm following this framework, which runs faster than the TGOA algorithm with a competitive ratio of <inline-formula><tex-math notation=\"LaTeX\">$\\frac{1}{8}$</tex-math><alternatives><mml:math><mml:mfrac><mml:mn>1</mml:mn><mml:mn>8</mml:mn></mml:mfrac></mml:math><inline-graphic xlink:href=\"chen-ieq2-2948863.gif\"/></alternatives></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$\\frac{1}{4}$</tex-math><alternatives><mml:math><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:math><inline-graphic xlink:href=\"chen-ieq3-2948863.gif\"/></alternatives></inline-formula>, respectively. We also revisit the average performance of Greedy, which has long been considered as the worst due to its unbounded competitive ratio in the worst case. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive experiments on synthetic and real datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "47211830",
                    "name": "Yuxiang Zeng"
                },
                {
                    "authorId": "1696332",
                    "name": "Bolin Ding"
                },
                {
                    "authorId": "2108532674",
                    "name": "Libin Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "46a27924165ef9341cd3301491b0a20419b91896",
            "title": "DIV: Resolving the Dynamic Issues of Zero-knowledge Set Membership Proof in the Blockchain",
            "abstract": "Zero-knowledge set membership (ZKSM) proof is widely used in blockchain to enable private membership attestation. However, existing mechanisms do not fully consider dynamic issues in the blockchain scenario. Particularly, frequent addition/removal of set elements, not only brings the significant cost to keep public parameters up to date to provers and verifiers but also affects mechanism efficiency (e.g., generation time of the proof and verification, etc.). In this paper, we propose DIV to shard elements on blockchain into independent subsets with the same cardinality to reduce the effect of dynamic issues. However, due to the diverse proof frequency, an improper element-set assignment can result in frequently used elements being easily inferred and corrupted. Thus, we formalize the assignment problem under both element addition and removal cases as two optimization problems and prove their NP-hardness. For each problem, we consider two cases if each element proof frequency is known in advance by the set maintainer or not, and propose solutions with theoretical guarantees. We implement DIV on both Merkle tree and RSA-based ZKSM mechanisms to evaluate its efficiency and effectiveness and apply DIV on a ZKSMbased application named zkSync to demonstrate its applicability. Results show that DIV can achieve O(1) time/space cost on ZKSM under dynamic situations while protecting the information about frequently used elements. It also notably reduces the system latency of zkSync.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "80541392",
                    "name": "Zihuan Xu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "5a8c6890e524b708dc262d3f456c985e8a46d7d1",
            "title": "Efficient Relation-aware Scoring Function Search for Knowledge Graph Embedding",
            "abstract": "The scoring function, which measures the plausibility of triplets in knowledge graphs (KGs), is the key to ensure the excellent performance of KG embedding, and its design is also an important problem in the literature. Automated machine learning (AutoML) techniques have recently been introduced into KG to design task-aware scoring functions, which achieve the state-of-the-art performance in KG embedding. However, the effectiveness of searched scoring functions is still not as good as desired. In this paper, observing that existing scoring functions can exhibit distinct performance on different semantic patterns, we are motivated to explore such semantics by searching relationa-ware scoring functions. But the relation-aware search requires a much larger search space than the previous one. Hence, we propose to encode the space as a supernet and propose an efficient alternative minimization algorithm to search through the supernet in a one-shot manner. Finally, experimental results on benchmark datasets demonstrate that the proposed method can efficiently search relation-aware scoring functions, and achieve better embedding performance than state-of-the-art methods.1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51129379",
                    "name": "Shimin Di"
                },
                {
                    "authorId": "3259992",
                    "name": "Quanming Yao"
                },
                {
                    "authorId": "48379419",
                    "name": "Yongqi Zhang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "5c53544466ade7fa353e01b5d97d87f7722295f2",
            "title": "Searching to Sparsify Tensor Decomposition for N-ary Relational Data",
            "abstract": "Tensor, an extension of the vector and matrix to the multi-dimensional case, is a natural way to describe the N-ary relational data. Recently, tensor decomposition methods have been introduced into N-ary relational data and become state-of-the-art on embedding learning. However, the performance of existing tensor decomposition methods is not as good as desired. First, they suffer from the data-sparsity issue since they can only learn from the N-ary relational data with a specific arity, i.e., parts of common N-ary relational data. Besides, they are neither effective nor efficient enough to be trained due to the over-parameterization problem. In this paper, we propose a novel method, i.e., S2S, for effectively and efficiently learning from the N-ary relational data. Specifically, we propose a new tensor decomposition framework, which allows embedding sharing to learn from facts with mixed arity. Since the core tensors may still suffer from the over-parameterization, we propose to reduce parameters by sparsifying the core tensors while retaining their expressive power using neural architecture search (NAS) techniques, which can search for data-dependent architectures. As a result, the proposed S2S not only guarantees to be expressive but also efficiently learns from mixed arity. Finally, empirical results have demonstrated that S2S is efficient to train and achieves state-of-the-art performance. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51129379",
                    "name": "Shimin Di"
                },
                {
                    "authorId": "3259992",
                    "name": "Quanming Yao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "71d149d448da88dc80f9fabf48db60ccaeb1bda4",
            "title": "HST+: An Efficient Index for Embedding Arbitrary Metric Spaces",
            "abstract": "Metric embeddings have been widely used in approximate algorithms to guarantee the effectiveness of geometric problems. Among the metric embedding techniques, Hierarchically Separated Tree (HST) is one of the most prevalent data structures, which maps the points of the original metric space into a tree-based metric space. A few selected applications of the HST include clustering, task assignment, trip planning, privacy preservation, information routing in wireless sensor networks, etc. Despite the popularity in ensuring the effectiveness, the HST-based solutions can be inefficient in large-scale datasets, since the state-of-the-art construction method has high time and space complexity (O(n3) and O(n2) in the worst-case). Moreover, existing studies overlook the insertion of new points in real applications (deletions can be trivially supported), which can cause the reconstruction of the whole HST. To address these limitations, we focus on designing an efficient index for embedding arbitrary metric spaces by tree metric spaces. Specifically, for construction, we design a dynamic programming-based method, which significantly reduces the time and space complexity to O(n2) and O(n) respectively. For insertion of new points, we propose a new data structure, called Hierarchically Separated Forest (HSF), i.e., a collection of HSTs. An HSF can efficiently support insertion of new points with a tight theoretical guarantee (O(log n)). Finally, extensive experiments demonstrate the superior performance of our proposed algorithms with respect to the effectiveness and the running time. For instance, compared with the state-of-the-art algorithms, our construction method is up to 29.8\u00d7 faster and our insertion method is up to 491\u00d7 faster.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47211830",
                    "name": "Yuxiang Zeng"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "7b799d8f75331c713c8c1497b080c19e05f192d4",
            "title": "Cohesive Group Nearest Neighbor Queries on Road-Social Networks under Multi-Criteria",
            "abstract": "The group nearest neighbor (GNN) search on a road network <inline-formula><tex-math notation=\"LaTeX\">$G_r$</tex-math><alternatives><mml:math><mml:msub><mml:mi>G</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"guo-ieq1-2974943.gif\"/></alternatives></inline-formula>, i.e., finding the spatial objects as activity assembly points with the smallest sum of distances to query users on <inline-formula><tex-math notation=\"LaTeX\">$G_r$</tex-math><alternatives><mml:math><mml:msub><mml:mi>G</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math><inline-graphic xlink:href=\"guo-ieq2-2974943.gif\"/></alternatives></inline-formula>, has been extensively studied; however, previous works neglected the fact that social relationships among query users, which ensure the maximally favorable atmosphere in the activity, can play an important role in GNN queries. Meanwhile, the ratings of spatial objects can also be used as recommended guidelines. Many real-world applications, such as location-based social networking services, require such queries. In this paper, we study two new problems: (1) a GNN search on a road network that incorporates cohesive social relationships (CGNN) and (2) a CGNN query under multi-criteria (MCGNN). Specifically, both the query users of highest closeness and the corresponding top-<inline-formula><tex-math notation=\"LaTeX\">$j$</tex-math><alternatives><mml:math><mml:mi>j</mml:mi></mml:math><inline-graphic xlink:href=\"guo-ieq3-2974943.gif\"/></alternatives></inline-formula> objects are retrieved. To address critical challenges on the effectiveness of results and the efficiency of computation over large road-social networks: (1) for CGNN, we propose a filtering-and-verification framework. During filtering, we prune substantial unpromising users and objects using social and geospatial constraints. During verification, we obtain the object candidates, among which the top <inline-formula><tex-math notation=\"LaTeX\">$j$</tex-math><alternatives><mml:math><mml:mi>j</mml:mi></mml:math><inline-graphic xlink:href=\"guo-ieq4-2974943.gif\"/></alternatives></inline-formula> are selected, with respect to the qualified users; (2) for MCGNN, we propose threshold-based selection and expansion strategies, where different strict boundaries are proposed to ensure that correct top-<inline-formula><tex-math notation=\"LaTeX\">$j$</tex-math><alternatives><mml:math><mml:mi>j</mml:mi></mml:math><inline-graphic xlink:href=\"guo-ieq5-2974943.gif\"/></alternatives></inline-formula> objects are found early. Moreover, we further optimize search strategies to improve query performance. Finally, experimental results on real social and road networks significantly demonstrate the efficiency and efficacy of our solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2279918",
                    "name": "Fangda Guo"
                },
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "2117418478",
                    "name": "Zimeng Wang"
                }
            ]
        },
        {
            "paperId": "7dd9b574012f60f87ca11017d88bd10a337e0258",
            "title": "ALG: Fast and Accurate Active Learning Framework for Graph Convolutional Networks",
            "abstract": "Graph Convolutional Networks (GCNs) have become state-of-the-art methods in many supervised and semi-supervised graph representation learning scenarios. In order to achieve satisfactory performance, GCNs require a sufficient amount of labeled data. However, in real-world scenarios, labeled data is often expensive to obtain. Therefore, we propose ALG, a novel Active Learning framework for GCNs, which employs domain-specific intelligence to achieve much higher performance and efficiency compared to the generic AL frameworks. First, by decoupling GCN models, ALG serves as an effective and efficient AL framework for measuring and combining node representativeness and informativeness. Second, by exploiting the characteristic of the reception field in GCNs, ALG considers both the importance and correlation of nodes by proposing a new node selection metric that maximizes the effective reception field (ERF). We prove that this ERF maximization problem is NP-hard and provide an efficient algorithm accompanied with a provable approximation guarantee. The empirical studies on four public datasets demonstrate that ALG can significantly improve both the performance and efficiency of active learning for GCNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108232566",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2117688209",
                    "name": "Yu Shen"
                },
                {
                    "authorId": "1864836046",
                    "name": "Yang Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2109540175",
                    "name": "Zhi Yang"
                },
                {
                    "authorId": "2068228300",
                    "name": "Bin Cui"
                }
            ]
        },
        {
            "paperId": "825950f0fbe25e419bbdad8fa8f68733e13a5ffd",
            "title": "Block Access Pattern Discovery via Compressed Full Tensor Transformer",
            "abstract": "The discovery and prediction of block access patterns in hybrid storage systems is of crucial importance for effective tier management. Existing methods are usually based on heuristics and unable to handle complex patterns. This work newly introduces transformer to block access pattern prediction. We remark that block accesses in the tier management systems are aggregated temporally and spatially as multivariate time series of block access frequency, so the runtime requirements are relaxed, making complex models applicable for the deployment. Moreover, enormous and rarely accessed blocks in storage systems and the structure of traditional transformer models would result in millions of redundant parameters and make them impractical to be deployed. We incorporate Tensor-Train Decomposition (TTD) with transformer and propose the Compressed Full Tenor Transformer (CFTT), in which all linear layers in the vanilla transformer are replaced with tensor-train layers. Weights of input and output layers are shared to further reduce parameters and reuse knowledge implicitly. CFTT can significantly reduce the model size and computation cost, which is critical to save storage space and inference time. Extensive experiments are conducted on synthetic and real-world datasets. The results demonstrate that transformers achieve state-of-the-art performance stably in terms of top-k hit rates. Moreover, the proposed CFTT compresses transformers 16\u00d7 to 461\u00d7 and speeds up inference 5\u00d7 without sacrificing performance on the whole, which facilitates its applications in tier management in hybrid storage systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155447824",
                    "name": "Xing Li"
                },
                {
                    "authorId": "3292742",
                    "name": "Qiquan Shi"
                },
                {
                    "authorId": "71247781",
                    "name": "G. Hu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1491682190",
                    "name": "Hui-Ying Mao"
                },
                {
                    "authorId": "79327135",
                    "name": "Yiyuan Yang"
                },
                {
                    "authorId": "1688812",
                    "name": "Mingxuan Yuan"
                },
                {
                    "authorId": "152280479",
                    "name": "Jia Zeng"
                },
                {
                    "authorId": "2152159672",
                    "name": "Zhuo Cheng"
                }
            ]
        },
        {
            "paperId": "93e99505e4cb8be8fb3073bda7940658054dd50c",
            "title": "Efficient Similarity Search for Sets over Graphs",
            "abstract": "Measuring similarities among different nodes is important in graph analysis tasks, such as link prediction, and recommendation. Among different similarity measures, SimRank is one of the most popular and promising ones, and has received a lot of research attention. While most current studies focus on single-pair, single-source/top-k, and all-pairs SimRank computation, few of them have studied finding similar pairs given a set of node pairs, which has attractive applications in personalized search and recommendation tasks. In this paper, we present Carmo, an efficient algorithm for retrieving the top-k similarities from an arbitrary set of pairs. In addition, we introduce two types of indexes to boost the efficiency of Carmo: one is hub-based, the other is tree-based. We show the effectiveness and efficiency of our proposed methods by extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "104994614",
                    "name": "Zonghao Feng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2145274105",
                    "name": "Zijian Li"
                },
                {
                    "authorId": "2054880853",
                    "name": "Xun Jian"
                },
                {
                    "authorId": "153889459",
                    "name": "Qiong Luo"
                }
            ]
        },
        {
            "paperId": "a1962a7fc55e84d5de5ebbbc3355199f33058bd9",
            "title": "Semi-Supervised Variational User Identity Linkage via Noise-Aware Self-Learning",
            "abstract": "User identity linkage, which aims to link identities of a natural person across different social platforms, has attracted increasing research interest recently. Existing approaches usually first embed the identities as deterministic vectors in a shared latent space, and then learn a classifier based on the available annotations. However, the formation and characteristics of real-world social platforms are full of uncertainties, which makes these deterministic embedding based methods sub-optimal. Besides, semi-supervised models utilize the unlabeled data to help capture the intrinsic data distribution. However, the existing semi-supervised linkage methods heavily rely on the heuristically defined similarity measurements to incorporate the innate closeness between labeled and unlabeled samples. Such manually designed assumptions may not be consistent with the actual linkage signals and further introduce the noises. To address the mentioned limitations, in this paper we propose a novel Noise-aware Semi-supervised Variational User Identity Linkage (NSVUIL) model. Specifically, we first propose a novel supervised linkage module to incorporate the available annotations. Each social identity is represented by a Gaussian distribution in the Wasserstein space to simultaneously preserve the fine-grained social profiles and model the uncertainty of identities. Then, a noise-aware self-learning module is designed to faithfully augment the few available annotations, which is capable of filtering noises from the pseudo-labels generated by the supervised module. The filtered reliable candidates are added into the labeled set to provide enhanced training guidance for the next training iteration. Empirically, we evaluate the NSVUIL model over multiple real-world datasets, and the experimental results demonstrate its superiority.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2869810",
                    "name": "Chaozhuo Li"
                },
                {
                    "authorId": "3210262",
                    "name": "Senzhang Wang"
                },
                {
                    "authorId": "2111063150",
                    "name": "Jie Xu"
                },
                {
                    "authorId": "2145976175",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2256768786",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2110972323",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "152297693",
                    "name": "Philip S. Yu"
                }
            ]
        },
        {
            "paperId": "a76baabfc94174a310a060185694a1351f97d6ad",
            "title": "When the Recursive Diversity Anonymity Meets the Ring Signature",
            "abstract": "In privacy-preserving blockchain systems, to protect a sender's identity of a transaction in privacy-preserving blockchain systems, ring signature (RS) schemes have been widely implemented, which allow users to obscure consumed tokens via including \"mixin'' (i.e., chaff tokens). However, recent works point out that existing RS schemes are vulnerable to the \"chain-reaction'' analysis, where adversaries eliminate mixins of RSs by utilizing the fact that each token can only be consumed in a RS. By \"chain-reaction'' analysis, adversaries can find some definite token-RS pair sets (DTRSs) to confirm the sender's identity of a RS. Besides, the existing RS schemes do not consider the diversity of mixins when generating a RS. Moreover, since the transaction fee is proportional to the number of mixins, a use is motivated to use a RS with the minimum number of mixins. In this paper, we formally define the diversity-aware mixins selection (DA-MS) problem, which aims to generate a RS with the minimum number of mixins satisfying the constraints of its diversity and the anonymity of other RSs. We prove the DA-MS problem is $\\#P$ and propose a breadth-first search algorithm to get the optimal solution. Furthermore, to efficiently solve the DA-MS problem, we propose two practical configurations and two approximation algorithms with theoretic guarantees. Through comprehensive experiments on real data sets as well as synthetic data sets, we illustrate the effectiveness and the efficiency of our solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1567491249",
                    "name": "Wangze Ni"
                },
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                }
            ]
        },
        {
            "paperId": "d710e541daddd7a105e66f4030dbfbfd98c11c64",
            "title": "TENET: Joint Entity and Relation Linking with Coherence Relaxation",
            "abstract": "The joint entity and relation linking task aims to connect the noun phrases (resp., relational phrases) extracted from natural language documents to the entities (resp., predicates) in general knowledge bases (KBs). This task benefits numerous downstream systems, such as question answering and KB population. Previous works on entity and relation linking rely on the global coherence assumption, i.e., entities and predicates within the same document are highly correlated with each other. However, this assumption is not always valid in many real-world scenarios. Due to KB incompleteness or data sparsity, sparse coherence among the entities and predicates within the same document is common. Moreover, there may exist isolated entities or predicates that are not related to any other linked concepts. In this paper, we propose TENET, a joint entity and relation linking technique, which relaxes the coherence assumption in an unsupervised manner. Specifically, we formulate the joint entity and relation linking task as a minimum-cost rooted tree cover problem on the knowledge coherence graph constructed based on the document. We then propose effective approximation algorithms with pruning strategies to solve this problem and derive the linking results. Extensive experiments on real-world datasets demonstrate the superior effectiveness and efficiency of our method against the state-of-the-art techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2893899",
                    "name": "Xueling Lin"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2152736066",
                    "name": "Chaorui Zhang"
                }
            ]
        },
        {
            "paperId": "dce2ec2c9e0f54570640bda9a77aa36c509b0553",
            "title": "Learning-Aided Heuristics Design for Storage System",
            "abstract": "Computer systems such as storage systems normally require transparent white-box algorithms that are interpretable for human experts. In this work, we propose a learning-aided heuristic design method, which automatically generates human-readable strategies from Deep Reinforcement Learning (DRL) agents. This method benefits from the power of deep learning but avoids the shortcoming of its black-box property. Besides the white-box advantage, experiments in our storage production's resource allocation scenario also show that this solution outperforms the system's default settings and the elaborately handcrafted strategy by human experts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1712223981",
                    "name": "Yingtian Tang"
                },
                {
                    "authorId": "71206713",
                    "name": "H. Lu"
                },
                {
                    "authorId": "3916869",
                    "name": "Xijun Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1688812",
                    "name": "Mingxuan Yuan"
                },
                {
                    "authorId": "152280479",
                    "name": "Jia Zeng"
                }
            ]
        },
        {
            "paperId": "dd880b8327e143dc7a686335a20933d4b79cde80",
            "title": "Butterfly Counting on Uncertain Bipartite Networks",
            "abstract": "\n When considering uncertain bipartite networks, the number of instances of the popular graphlet structure the butterfly may be used as an important metric to quickly gauge information about the network. This Uncertain Butterfly Count has practical usages in a variety of areas such as biomedical/biological fields, E-Commerce and road networks. In this paper we formally define the uncertain butterfly structure (in which the existential probability of the butterfly is greater than or equal to some user-defined threshold\n t\n ) as well as the Uncertain Butterfly Counting Problem (to determine the number of unique instances of this structure on any uncertain bipartite network). We then examine exact solutions by proposing a non-trivial baseline\n (UBFC)\n as well as an improved solution\n (IUBFC)\n which reduces the time complexity and employs heuristics to further reduce the runtime in practice. In addition to exact solutions, we propose two approximate solutions via sampling,\n UBS\n and\n PES\n , which can be used to quickly estimate the Uncertain Butterfly Count, a powerful tool when the exact count is unnecessary. Using a range of networks with different edge existential probability distributions, we validate the efficiency and effectiveness of our solutions.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48543245",
                    "name": "Alexander Zhou"
                },
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "eb48908397cba3e20ac01e668c9a82aaac216b6c",
            "title": "High-Precision Multicamera-Assisted Camera-IMU Calibration: Theory and Method",
            "abstract": "This article presents a high-precision single-camera inertial measurement unit (IMU) extrinsic calibration method by tightly fusing the visual information from other cameras. Specifically, multiple additional cameras are added to the monocular camera-IMU system for assisting calibration as we theoretically prove that more cameras used in calibration can lead to smaller lower bound on the covariance of the estimated extrinsic parameters, which then results in better calibration accuracy. Moreover, we provide two degenerative motion conditions in the resulting multicamera visual-inertial system, which impair the calibration accuracy and should be avoided in real application whenever possible. More importantly, we present the requirement of minimum motion for a reliable extrinsic calibration to provide the practical guideline. Finally, the full validation on both simulation and real-world data is demonstrated. By evaluating the Cram\u00e9r\u2013Rao lower bound on the covariance, the proposed camera-IMU calibration method is shown to be statistically efficient for accurate calibration with errors less than 0.01 m in translation and 0.5\u00b0 in rotation, which is consistent with the theoretical analysis in this article.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2032435059",
                    "name": "Bo Fu"
                },
                {
                    "authorId": "2047905198",
                    "name": "Fuzhang Han"
                },
                {
                    "authorId": "2118461398",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "77239951",
                    "name": "Yanmei Jiao"
                },
                {
                    "authorId": "31627893",
                    "name": "X. Ding"
                },
                {
                    "authorId": "32659574",
                    "name": "Qimeng Tan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "9378543",
                    "name": "Minhang Wang"
                },
                {
                    "authorId": "5738033",
                    "name": "R. Xiong"
                }
            ]
        },
        {
            "paperId": "f82cc4e1ae54af46776312d69083c8ba415dc141",
            "title": "CaSIE: Canonicalize and Informative Selection of the OpenIE system",
            "abstract": "Knowledge extraction has become a hot topic recently with the increasing number of applications needed for large-scale knowledge bases (KBs), such as semantic search and QA systems. The goal of knowledge extraction is to extract relations and their arguments from natural language text. Recent research proposes two kinds of solutions. The first one, called Closed IE, tries to construct KB through predefined features or rules with respect to a specific domain. It requires specifying the interested predicates in advance, which restricts its application to the domains where prior knowledge about the interested predicates must be given. The second one, called Open IE, tries to extract facts by using the parsing structure from the unstructured text. However, they cannot avoid extracting redundant facts. Such extractions can hardly be directly used to populate the existing KB. Moreover, many correct extractions are not relevant to the document, which limits the applications to understand the essential information that the document conveys. In this paper, we propose an end-to-end system which takes a target incomplete KB and documents as input. It first performs joint entity and relation linking to the existing KB based on both contexts of document and background KB information. Then it summarizes the extracted facts by considering the relevance to the document and the diversity between them. Extensive experiments over real datasets demonstrate the effectiveness and efficiency of the proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057309752",
                    "name": "Hao Xin"
                },
                {
                    "authorId": "2893899",
                    "name": "Xueling Lin"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "041dfea127cf3807e81d6a51722bd79abcb892b4",
            "title": "DISK: A Distributed Framework for Single-Source SimRank with Accuracy Guarantee",
            "abstract": "Measuring similarities among different nodes is important in graph analysis. SimRank is one of the most popular similarity measures. Given a graph G ( V , E ) and a source node u , a single-source Sim-Rank query returns the similarities between u and each node v \u2208 V . This type of query is often used in link prediction, personalized recommendation and spam detection. While dealing with a large graph is beyond the ability of a single machine due to its limited memory and computational power, it is necessary to process single-source SimRank queries in a distributed environment, where the graph is partitioned and distributed across multiple machines. However, most current solutions are based on shared-memory model, where the whole graph is loaded into a shared memory and all processors can access the graph randomly. It is difficult to deploy such algorithms on shared-nothing model. In this paper, we present DISK, a distributed framework for processing single-source Sim-Rank queries. DISK follows the linearized formulation of SimRank, and consists of offline and online phases. In the offline phase, a tree-based method is used to estimate the diagonal correction matrix of SimRank accurately, and in the online phase, single-source similarities are computed iteratively. Under this framework, we propose different optimization techniques to boost the indexing and queries. DISK guarantees both accuracy and parallel scalability, which distinguishes itself from existing solutions. Its accuracy, efficiency, parallel scalability and scalability are also verified by extensive experimental studies. The experiments show that DISK scales up to graphs of billions of nodes and edges, and answers",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "144104152",
                    "name": "Ruiqi Xu"
                },
                {
                    "authorId": "104994614",
                    "name": "Zonghao Feng"
                },
                {
                    "authorId": "36916715",
                    "name": "Yulin Che"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "153889459",
                    "name": "Qiong Luo"
                },
                {
                    "authorId": "2106694683",
                    "name": "Rui Mao"
                }
            ]
        },
        {
            "paperId": "0f0acede413a7ad589bb220f3cadacbbcfa6a1e0",
            "title": "Fair task assignment in spatial crowdsourcing",
            "abstract": "With the pervasiveness of mobile devices, wireless broadband and sharing economy, spatial crowdsourcing is becoming part of our daily life. Existing studies on spatial crowdsourcing usually focus on enhancing the platform interests and customer experiences. In this work, however, we study the fair assignment of tasks to workers in spatial crowdsourcing. That is, we aim to assign tasks, considered as a resource in short supply, to individual spatial workers in a fair manner. In this paper, we first formally define an online bi-objective matching problem, namely the Fair and Effective Task Assignment (FETA) problem, with its special cases/variants of it to capture most typical spatial crowdsourcing scenarios. We propose corresponding solutions for each variant of FETA. Particularly, we show that the dynamic sequential variant, which is a generalization of an existing fairness scheduling problem, can be solved with an O(n) fairness cost bound (n is the total number of workers), and give an O(n/m) fairness cost bound for the m-sized general batch case (m is the minimum batch size). Finally, we evaluate the effectiveness and efficiency of our algorithm on both synthetic and real data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111605555",
                    "name": "Zhao Chen"
                },
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                },
                {
                    "authorId": "1773086",
                    "name": "C. Shahabi"
                }
            ]
        },
        {
            "paperId": "215c7f23c69b745909d6c58ace6d36a141b8c656",
            "title": "Optimizing DNN computation graph using graph substitutions",
            "abstract": "Deep learning has achieved great success in various real-world applications. As deep neural networks (DNNs) are getting larger, the inference and training cost of DNNs increases significantly. Since one round of inference or one iteration in the training phase of a DNN is typically modeled as a computation graph, existing works propose to optimize computation graphs by performing a sequence of functionally equivalent graph substitutions, leading to higher inference and training efficiency. In this work, we formally define the Optimizing Computation Graph using Graph Substitutions (OCGGS) problem, and prove it to be NP-hard and Poly-APX-complete. We develop two exact and efficient methods to the OCGGS problem. The pruning-based algorithm eliminates the examination of redundant graph substitution sequences, and the dynamic programming with pruning algorithm makes use of the explored graph substitutions. To further speed up the search process, we propose a sampling heuristic which is effective to optimize complex computation graphs with polynomial time and space complexity. Extensive experiments on various DNN architectures and sizes are conducted to verify the effectiveness and efficiency of our proposed solutions compared with existing techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115379722",
                    "name": "Jin Fang"
                },
                {
                    "authorId": "2923152",
                    "name": "Yanyan Shen"
                },
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "248b28325517687390572ba65ef54761ecc69402",
            "title": "KBPearl",
            "abstract": "Nowadays, most openly available knowledge bases (KBs) are incomplete, since they are not synchronized with the emerging facts happening in the real world. Therefore, knowledge base population (KBP) from external data sources, which extracts knowledge from unstructured text to populate KBs, becomes a vital task. Recent research proposes two types of solutions that partially address this problem, but the performance of these solutions is limited. The first solution, dynamic KB construction from unstructured text, requires specifications of which predicates are of interest to the KB, which needs preliminary setups and is not suitable for an in-time population scenario. The second solution, Open Information Extraction (Open IE) from unstructured text, has limitations in producing facts that can be directly linked to the target KB without redundancy and ambiguity. In this paper, we present an end-to-end system, KBPearl, for KBP, which takes an incomplete KB and a large corpus of text as input, to (1) organize the noisy extraction from Open IE into canonicalized facts; and (2) populate the KB by joint entity and relation linking, utilizing the context knowledge of the facts and the side information inferred from the source text. We demonstrate the effectiveness and efficiency of KBPearl against the state-of-the-art techniques, through extensive experiments on real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2893899",
                    "name": "Xueling Lin"
                },
                {
                    "authorId": "2145537904",
                    "name": "Haoyang Li"
                },
                {
                    "authorId": "2057309752",
                    "name": "Hao Xin"
                },
                {
                    "authorId": "2145274105",
                    "name": "Zijian Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "2d664a33cfdff7de78167710eac995aaf34c9003",
            "title": "Reliable Data Distillation on Graph Convolutional Network",
            "abstract": "Graph Convolutional Network (GCN) is a widely used method for learning from graph-based data. However, it fails to use the unlabeled data to its full potential, thereby hindering its ability. Given some pseudo labels of the unlabeled data, the GCN can benefit from this extra supervision. Based on Knowledge Distillation and Ensemble Learning, lots of methods use a teacher-student architecture to make better use of the unlabeled data and then make a better prediction. However, these methods introduce unnecessary training costs and a high bias of student model if the teacher's predictions are unreliable. Besides, the final ensemble gains are limited due to limited diversity in the combined models. Therefore, we propose Reliable Data Distillation, a reliable data driven semi-supervised GCN training method. By defining the node reliability and edge reliability in a graph, we can make better use of high quality data and improve the graph representation learning. Furthermore, considering the data reliability and data importance, we propose a new ensemble learning method for GCN and a novel Self-Boosting SSL Framework to combine the above optimizations. Finally, our extensive evaluation of Reliable Data Distillation on real-world datasets shows that our approach outperforms the state-of-the-art methods on semi-supervised node classification tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108232566",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "1720763480",
                    "name": "Xupeng Miao"
                },
                {
                    "authorId": "2237813",
                    "name": "Yingxia Shao"
                },
                {
                    "authorId": "3452475",
                    "name": "Jiawei Jiang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "51186954",
                    "name": "Olivier Ruas"
                },
                {
                    "authorId": "144585959",
                    "name": "B. Cui"
                }
            ]
        },
        {
            "paperId": "31f8d96e8aafd796ab899a17666f8fd0b4117172",
            "title": "Real-Time Cross Online Matching in Spatial Crowdsourcing",
            "abstract": "With the development of mobile communication techniques, spatial crowdsourcing has become popular recently. A typical topic of spatial crowdsourcing is task assignment, which assigns crowd workers to users\u2019 requests in real time and maximizes the total revenue. However, it is common that the available crowd workers over a platform are too far away to serve the requests, so some user requests may be rejected or responded at high money cost after long waiting. Fortunately, the neighbors of a platform usually have available resources for the same services. Collaboratively conducting the task allocation among different platforms can greatly improve the quality of services, but have not been investigated yet. In this paper, we propose a Cross Online Matching (COM), which enables a platform to \"borrow\" unoccupied crowd workers from other platforms for completing the user requests. We propose two algorithms, deterministic cross online matching (DemCOM) and randomized cross online matching (RamCom) for COM. DemCOM focuses on the largest obtained revenue in a greedy manner, while RamCom considers the trade-off between the obtained revenue and the probability of request being accepted by the borrowed workers. Extensive experimental results verify the effectiveness and efficiency of our algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2329311",
                    "name": "Yurong Cheng"
                },
                {
                    "authorId": "2132848303",
                    "name": "Boyang Li"
                },
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "38ac7223aae326ee624d68d6899a364cdf5f9fbd",
            "title": "Block Hankel Tensor ARIMA for Multiple Short Time Series Forecasting",
            "abstract": "This work proposes a novel approach for multiple time series forecasting. At first, multi-way delay embedding transform (MDT) is employed to represent time series as low-rank block Hankel tensors (BHT). Then, the higher-order tensors are projected to compressed core tensors by applying Tucker decomposition. At the same time, the generalized tensor Autoregressive Integrated Moving Average (ARIMA) is explicitly used on consecutive core tensors to predict future samples. In this manner, the proposed approach tactically incorporates the unique advantages of MDT tensorization (to exploit mutual correlations) and tensor ARIMA coupled with low-rank Tucker decomposition into a unified framework. This framework exploits the low-rank structure of block Hankel tensors in the embedded space and captures the intrinsic correlations among multiple TS, which thus can improve the forecasting results, especially for multiple short time series. Experiments conducted on three public datasets and two industrial datasets verify that the proposed BHT-ARIMA effectively improves forecasting accuracy and reduces computational cost compared with the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3292742",
                    "name": "Qiquan Shi"
                },
                {
                    "authorId": "144724946",
                    "name": "Jiaming Yin"
                },
                {
                    "authorId": "48979830",
                    "name": "Jiajun Cai"
                },
                {
                    "authorId": "145683892",
                    "name": "A. Cichocki"
                },
                {
                    "authorId": "47641638",
                    "name": "Tatsuya Yokota"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1688812",
                    "name": "Mingxuan Yuan"
                },
                {
                    "authorId": "152280479",
                    "name": "Jia Zeng"
                }
            ]
        },
        {
            "paperId": "3b4c791fb2c17dbdff32361f9b9e9187116267e3",
            "title": "RIDE: A System for Generalized Region of Interest Discovery and Exploration",
            "abstract": "As an important operator for spatial data analytics, Region of Interest (ROI) query is of great importance in many location-based services such as event detection, location recommendation and smart transportation. To address the challenge of conducting ROI queries on the increasingly complex spatial data, we present RIDE, an efficient and effective system for generalized ROI Discovery and Exploration. Different from existing studies and systems, RIDE supports a large spectrum of region score functions and query geometries, enabling customized ROI queries for different application scenarios. This demonstration proposal introduces the basic concept of ROI queries and key components of the RIDE system, including data storage and indexing, ROI query processing and optimization and user interface.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112215527",
                    "name": "Qiyu Liu"
                },
                {
                    "authorId": "49537866",
                    "name": "Libin Zheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "435bc228e947c22a3a68ab9aa029042c3bed907e",
            "title": "Finding large diverse communities on networks",
            "abstract": "In this work we examine the problem of finding large, diverse communities on graphs where the users are separated into distinct groups. More specifically, this work considers diversity to be the inclusion of users from multiple groups as opposed to homogeneous communities in which the majority of users are from one group. We design and propose the k*-Partite Clique (and the edge-maximum k*-Partite Clique Problem) which modifies the k-Partite Clique structure as a means to capture these large, diverse communities in a way that does not currently exist. We then design a non-trivial baseline enumeration algorithm, which is further improved via heuristics to significantly reduce the running time whilst avoiding excessive memory requirements. Moreover, we propose a core as well as a truss structure for the k-Partite environment aimed at finding the edge-maximum k*-Partite Clique structure on the network. Comprehensive experiments on real-world datasets verify both the effectiveness of the k*-Partite Clique at finding diverse communities as well as the efficiency of the proposed heuristics to our algorithms compared to reasonable baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48543245",
                    "name": "Alexander Zhou"
                },
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "5b89724c66909ddedd9fac1f639b27dae243868f",
            "title": "Dynamic Connection-Based Social Group Recommendation",
            "abstract": "Group recommendation has become highly demanded when users communicate in the forms of group activities in online sharing communities. These group activities include student group study, family TV program watching, friends travel decision, etc. Existing group recommendation techniques mainly focus on the small user groups. However, online sharing communities have enabled group activities among thousands of users. Accordingly, recommendation over large groups has become urgent. In this paper, we propose a new framework to accomplish this goal by exploring the group interests and the connections between group users. We first divide a big group into different interest subgroups, each of which contains users closely connected with each other and sharing the similar interests. Then, for each interest subgroup, our framework exploits the connections between group users to collect a comparably compact potential candidate set of media-user pairs, on which the collaborative filtering is performed to generate an interest subgroup-based recommendation list. After that, a novel aggregation function is proposed to integrate the recommended media lists of all interest subgroups as the final group recommendation results. Extensive experiments have been conducted on two real social media datasets to demonstrate the effectiveness and efficiency of our proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064191826",
                    "name": "D. Qin"
                },
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2074183",
                    "name": "Guangyan Huang"
                },
                {
                    "authorId": "49889222",
                    "name": "Yanchun Zhang"
                }
            ]
        },
        {
            "paperId": "5c535d5a4d272bec5411a959fa179063e7d18be9",
            "title": "LERI: Local Exploration for Rare-Category Identification",
            "abstract": "To identify the data examples of rare categories that form small compact clusters in large data sets, existing approaches mostly require enough labeled data examples as a training set to learn a classifier, assuming that the rare-category clusters are spherical or nearly spherical. Nonetheless, a large enough training set is usually difficult to obtain in practice, and rare categories in many real-world applications often form small compact clusters with arbitrary shapes. In this paper, we investigate how to identify all data examples of a rare category with an arbitrary shape based on only one seed (i.e., a labeled rare-category data example). Instead of finding a compact and spherical local region around the seed, we locally explore the data set from the seed by continuously searching and visiting the <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"gao-ieq1-2911941.gif\"/></alternatives></inline-formula>-nearest neighbors of each newly visited data example. The local exploration connects the data examples in the objective rare category by the relationship of <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"gao-ieq2-2911941.gif\"/></alternatives></inline-formula>-nearest neighbors, and meanwhile, suspected external data examples are filtered out if they are not close enough to any visited data example. Experimental results on both synthetic and real-world data sets are conducted, and the results verify the effectiveness and efficiency of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51211749",
                    "name": "H. Huang"
                },
                {
                    "authorId": "145670685",
                    "name": "Qian Yan"
                },
                {
                    "authorId": "144209288",
                    "name": "W. Lu"
                },
                {
                    "authorId": "2535945",
                    "name": "Huaizhong Lin"
                },
                {
                    "authorId": "1409828392",
                    "name": "Yunjun Gao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "6598e5c3e6fb135972f04f089140e2bd6b276437",
            "title": "Demand-aware route planning for shared mobility services",
            "abstract": "The dramatic development of shared mobility in food delivery, ridesharing, and crowdsourced parcel delivery has drawn great concerns. Specifically, shared mobility refers to transferring or delivering more than one passenger/package together when their traveling routes have common sub-routes or can be shared. A core problem for shared mobility is to plan a route for each driver to fulfill the requests arriving dynamically with given objectives. Previous studies greedily and incrementally insert each newly coming request to the most suitable worker with a minimum travel cost increase, which only considers the current situation and thus not optimal. In this paper, we propose a demand-aware route planning (DARP) for shared mobility services. Based on prediction, DARP tends to make optimal route planning with more information about requests in the future. We prove that the DARP problem is NP-hard, and further show that there is no polynomial-time deterministic algorithm with a constant competitive ratio for the DARP problem unless P=NP. Hence, we devise an approximation algorithm to realize the insertion operation for our goal. With the insertion algorithm, we devise a prediction based solution for the DARP problem. Extensive experiment results on real datasets validate the effectiveness and efficiency of our technique.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110175222",
                    "name": "Jiachuan Wang"
                },
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "49537866",
                    "name": "Libin Zheng"
                },
                {
                    "authorId": "2117344775",
                    "name": "Chao Feng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                },
                {
                    "authorId": null,
                    "name": "Zheng Wang"
                }
            ]
        },
        {
            "paperId": "67bbcfc590a66dd8c0254cb7611f134c6cf5bfee",
            "title": "Simultaneous Arrival Matching for New Spatial Crowdsourcing Platforms",
            "abstract": "In recent years, 3D spatial crowdsourcing platforms become popular, in which users and workers travel together to their assigned workplaces for services, such as InterestingSport and Nanguache. A typical problem over 3D spatial crowdsourcing platforms is to match users with suitable workers and workplaces. Existing studies all ignored that the workers and users assigned to the same workplace should arrive almost at the same time, which is very practical in the real world. Thus, in this paper, we propose a new Simultaneous Arrival Matching (SAM), which enables workers and users to arrive at their assigned workplace within a given tolerant time. We find that the new considered arriving time constraint breaks the monotonic additivity of the result set. Thus, it brings a large challenge in designing effective and efficient algorithms for the SAM. We design Sliding Window algorithm and Threshold Scanning algorithm to solve the SAM. We conduct the experiments on real and synthetic datasets, experimental results show the effectiveness and efficiency of our algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152358042",
                    "name": "Boyang Li"
                },
                {
                    "authorId": "2329311",
                    "name": "Yurong Cheng"
                },
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "6e8689469aaf72a816b957469d556e6d8ba2b2dc",
            "title": "Effective and Efficient Relational Community Detection and Search in Large Dynamic Heterogeneous Information Networks",
            "abstract": "Community search in heterogeneous information networks (HINs) has attracted much attention in graph analysis. Given a vertex, the goal is to find a densely-connected sub-graph that contains the vertex. In practice, the user may need to restrict the number of connections between vertices, but none of the existing methods can handle such queries. In this paper, we propose the relational constraint that allows the user to specify fine-grained connection requirements between vertices. Base on this, we define the relational community as well as the problems of detecting and searching relational communities, respectively. For the detection problem, we propose an efficient solution that has near-linear time complexity. For the searching problem, although it is shown to be NP-hard and even hard-to-approximate, we devise two efficient approximate solutions. We further design the round index to accelerate the searching algorithm and show that it can handle dynamic graphs by its nature. Extensive experiments on both synthetic and real-world graphs are conducted to evaluate both the effectiveness and efficiency of our proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054880853",
                    "name": "Xun Jian"
                },
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "70f6dee1496677872ade27a194251201f943005d",
            "title": "Enriching Data Imputation under Similarity Rule Constraints",
            "abstract": "Incomplete information often occurs along with many database applications, e.g., in data integration, data cleaning, or data exchange. The idea of data imputation is often to fill the missing data with the values of its neighbors who share the same/similar information. Such neighbors could either be identified certainly by editing rules or extensively by similarity relationships. Owing to data sparsity, the number of neighbors identified by editing rules w.r.t. value equality is rather limited, especially in the presence of data values with variances. To enrich the imputation candidates, a natural idea is to extensively consider the neighbors with similarity relationship. However, the candidates suggested by these (heterogenous) similarity neighbors may conflict with each other. In this paper, we propose to utilize the similarity rules with tolerance to small variations (instead of the aforesaid editing rules with strict equality constraints) to rule out the invalid candidates provided by similarity neighbors. To enrich the data imputation, i.e., imputing the missing values more, we study the problem of maximizing the missing data imputation. Our major contributions include (1) the np-hardness analysis on solving as well as approximating the problem, (2) exact algorithms for tackling the problem, and (3) efficient approximation with performance guarantees. Experiments on real and synthetic data sets demonstrate the superiority of our proposal in filling accuracy. We also demonstrate that the record matching application is indeed improved, after applying the proposed imputation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "2143830649",
                    "name": "Yu Sun"
                },
                {
                    "authorId": "3358457",
                    "name": "Aoqian Zhang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2016222572",
                    "name": "Jianmin Wang"
                }
            ]
        },
        {
            "paperId": "72bebb5127204f4d800c1100e8c554406bcf9c05",
            "title": "Stable learned bloom filters for data streams",
            "abstract": "Bloom filter and its variants are elegant space-efficient probabilistic data structures for approximate set membership queries. It has been recently shown that the space cost of Bloom filters can be significantly reduced via a combination with pre-trained machine learning models, named Learned Bloom filters (LBF). LBF eases the space requirement of a Bloom filter by undertaking part of the queries using a classifier. However, current LBF structures generally target a static member set. Their performances would inevitably decay when there is a member update on the set, while this update requirement is not uncommon for real-world data streaming applications such as duplicate item detection, malicious URL checking, and web caching. To adapt LBF to data streams, we propose the Stable Learned Bloom Filters (SLBF) which addresses the performance decay issue on intensive insertion workloads by combining classifier with updatable backup filters. Specifically, we propose two SLBF structures, Single SLBF (s-SLBF) and Grouping SLBF (g-SLBF). The theoretical analysis on these two structures shows that the expected false positive rate (FPR) of SLBF is asymptotically a constant over the insertion of new members. Extensive experiments on real-world datasets show that SLBF introduces a similar level of false negative rate (FNR) but yields a better FPR/storage trade-off compared with the state-of-the-art (non-learned) Bloom filters optimized on data streams.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112215527",
                    "name": "Qiyu Liu"
                },
                {
                    "authorId": "49537866",
                    "name": "Libin Zheng"
                },
                {
                    "authorId": "2923152",
                    "name": "Yanyan Shen"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "73fd904716c58cab9520ee6b268e274dc7642766",
            "title": "UniNet: Scalable Network Representation Learning with Metropolis-Hastings Sampling",
            "abstract": "Network representation learning (NRL) has been successfully adopted in various data mining and machine learning applications. Random walk based NRL is one popular paradigm, which uses a set of random walks to capture the network structural information, and then employs word2vec models to learn the low-dimensional representations. However, until now there is lack of a framework, which unifies existing random walk based NRL models and efficiently learns from large networks. The main obstacle comes from the diverse random walk models and the inefficient sampling method for the random walk generation. In this paper, we first introduce a new and efficient edge sampler based on Metropolis-Hastings sampling technique, and theoretically show the convergence property of the edge sampler to arbitrary discrete probability distributions. Then we propose a random walk model abstraction, in which users can easily define different transition probability by specifying dynamic edge weights and random walk states. The abstraction is efficiently supported by our edge sampler, since our sampler can draw samples from unnormalized probability distribution in constant time complexity. Finally, with the new edge sampler and random walk model abstraction, we carefully implement a scalable NRL framework called UniNet. We conduct extensive experiments with five random walk based NRL models over eleven real-world datasets, and the results verify the efficiency of UniNet over billion-edge networks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2115587254",
                    "name": "Xingyu Yao"
                },
                {
                    "authorId": "2237813",
                    "name": "Yingxia Shao"
                },
                {
                    "authorId": "144585959",
                    "name": "B. Cui"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "77d4654663358871673d28d838d44124e902c3e8",
            "title": "Differentially Private Online Task Assignment in Spatial Crowdsourcing: A Tree-based Approach",
            "abstract": "With spatial crowdsourcing applications such as Uber and Waze deeply penetrated into everyday life, there is a growing concern to protect user privacy in spatial crowdsourcing. Particularly, locations of workers and tasks should be properly processed via certain privacy mechanism before reporting to the untrusted spatial crowdsourcing server for task assignment. Privacy mechanisms typically permute the location information, which tends to make task assignment ineffective. Prior studies only provide guarantees on privacy protection without assuring the effectiveness of task assignment. In this paper, we investigate privacy protection for online task assignment with the objective of minimizing the total distance, an important task assignment formulation in spatial crowdsourcing. We design a novel privacy mechanism based on Hierarchically Well-Separated Trees (HSTs). We prove that the mechanism is \u03b5-Geo-Indistinguishable and show that there is a task assignment algorithm with a competitive ratio of $O\\left( {\\frac{1}{{{\\varepsilon ^4}}}\\log N{{\\log }^2}k} \\right)$, where is the privacy budget, N is the number of predefined points on the HST, and k is the matching size. Extensive experiments on synthetic and real datasets show that online task assignment under our privacy mechanism is notably more effective in terms of total distance than under prior differentially private mechanisms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720763685",
                    "name": "Qian Tao"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "2735715",
                    "name": "Zimu Zhou"
                },
                {
                    "authorId": "114438759",
                    "name": "Yexuan Shi"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145389711",
                    "name": "Ke Xu"
                }
            ]
        },
        {
            "paperId": "7bdc9fd27e3c4b686e069efccc34757667821fd6",
            "title": "SPARQL Rewriting: Towards Desired Results",
            "abstract": "Recent years witnessed the emergence of various applications on knowledge graphs, which are often represented as RDF graphs. However, due to the lack of data schema and the complexity of SPARQL language, there is usually a gap between the user's real desire and the actual meaning of a SPARQL query, especially when the query itself is complicated. In this paper, we try to narrow this gap by modifying a given query with a set of modifiers, so that its result approaches a user-provided example set. Specifically, we model this problem as two individual sub-problems, query-restricting, and query-relaxing, both of which are shown to be NP-hard. We further prove that unless P=NP, query-restricting has no polynomial-time approximation scheme (PTAS), and query-relaxing has no polynomial-time constant-factor approximation algorithm. Despite their hardness, we propose a (1-1/\u03b5)-approximation method for query-restricting and 2 heuristics for query-relaxing. Extensive experiments have been conducted on real-world knowledge graphs to evaluate the effectiveness and efficiency of our proposed solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054880853",
                    "name": "Xun Jian"
                },
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "79642851",
                    "name": "Xiayu Lei"
                },
                {
                    "authorId": "49537866",
                    "name": "Libin Zheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "86385f9b3c72b935d30e94822ef9828e426fb53a",
            "title": "Task Allocation in Dependency-aware Spatial Crowdsourcing",
            "abstract": "Ubiquitous smart devices and high-quality wireless networks enable people to participate in spatial crowdsourcing tasks easily, which require workers to physically move to specific locations to conduct their assigned tasks. Spatial crowdsourcing has attracted much attention from both academia and industry. In this paper, we consider a spatial crowdsourcing scenario, where the tasks may have some dependencies among them. Specifically, one task can only be dispatched when its dependent tasks have already been assigned. In fact, task dependencies are quite common in many real-life applications, such as house repairing and holding sports games. We formally define the dependency-aware spatial crowdsourcing (DA-SC), which focuses on finding an optimal worker-and-task assignment under the constraints of dependencies, skills of workers, moving distances and deadlines to maximize the successfully assigned tasks. We prove that the DA-SC problem is NP-hard and thus intractable. Therefore, we propose two approximation algorithms, including a greedy approach and a game-theoretic approach, which can guarantee the approximate bounds of the results in each batch process. Through extensive experiments on both real and synthetic data sets, we demonstrate the efficiency and effectiveness of our DA-SC approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1567491249",
                    "name": "Wangze Ni"
                },
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                }
            ]
        },
        {
            "paperId": "95a501bfe4b09323e6e178edd64dc24a6935c23f",
            "title": "Interstellar: Searching Recurrent Architecture for Knowledge Graph Embedding",
            "abstract": "Knowledge graph (KG) embedding is well-known in learning representations of KGs. Many models have been proposed to learn the interactions between entities and relations of the triplets. However, long-term information among multiple triplets is also important to KG. In this work, based on the relational paths, which are composed of a sequence of triplets, we define the Interstellar as a recurrent neural architecture search problem for the short-term and long-term information along the paths. First, we analyze the difficulty of using a unified model to work as the Interstellar. Then, we propose to search for recurrent architecture as the Interstellar for different KG tasks. A case study on synthetic data illustrates the importance of the defined search problem. Experiments on real datasets demonstrate the effectiveness of the searched models and the efficiency of the proposed hybrid-search algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48379419",
                    "name": "Yongqi Zhang"
                },
                {
                    "authorId": "3259992",
                    "name": "Quanming Yao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "9c76ef06cc5ee72ec4f075fc25db38ddc0190193",
            "title": "Memory-Aware Framework for Efficient Second-Order Random Walk on Large Graphs",
            "abstract": "Second-order random walk is an important technique for graph analysis. Many applications use it to capture higher-order patterns in the graph, thus improving the model accuracy. However, the memory explosion problem of this technique hinders it from analyzing large graphs. When processing a billion-edge graph like Twitter, existing solutions (e.g., alias method) of the second-order random walk may take up 1796TB memory. Such high memory overhead comes from the memory-unaware strategies for node sampling across the graph. In this paper, to clearly study the efficiency of various node sampling methods in the context of second-order random walk, we design a cost model, and then propose a new node sampling method following the acceptance-rejection paradigm to achieve a better balance between memory and time cost. Further, to guarantee the efficiency of the second-order random walk within arbitrary memory budgets, we propose a memory-aware framework on the basis of the cost model. The framework applies a cost-based optimizer to assign desirable node sampling method for each node in the graph within a memory budget while minimizing the time cost. Finally, we provide general programming interfaces for users to benefit from the memory-aware framework easily. The empirical studies demonstrate that our memory-aware framework is robust with respect to memory and is able to achieve considerable efficiency by reducing 90% of the memory cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237813",
                    "name": "Yingxia Shao"
                },
                {
                    "authorId": "2110443183",
                    "name": "Shiyu Huang"
                },
                {
                    "authorId": "1720763480",
                    "name": "Xupeng Miao"
                },
                {
                    "authorId": "144585959",
                    "name": "B. Cui"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "a3076cd3b4fd4795b46bc44816fe3adb9ec0abe1",
            "title": "Efficient Graph Query Processing over Geo-Distributed Datacenters",
            "abstract": "Graph queries have emerged as one of the fundamental techniques to support modern search services, such as PageRank web search, social networking search and knowledge graph search. As such graphs are maintained globally and very huge (e.g., billions of nodes), we need to efficiently process graph queries across multiple geographically distributed datacenters, running geo-distributed graph queries. Existing graph computing frameworks may not work well for geographically distributed datacenters, because they implement a Bulk Synchronous Parallel model that requires excessive inter-datacenter transfers, thereby introducing extremely large latency for query processing. In this paper, we propose GeoGraph --a universal framework to support efficient geo-distributed graph query processing based on clustering datacenters and meta-graph, while reducing the inter-datacenter communication. Our new framework can be applied to many types of graph algorithms without any modification. The framework is developed on the top of Apache Giraph. The experiments were conducted by applying four important graph queries, i.e., shortest path, graph keyword search, subgraph isomorphism and PageRank. The evaluation results show that our proposed framework can achieve up to 82% faster convergence, 42% lower WAN bandwidth usage, and 45% less total monetary cost for the four graph queries, with input graphs stored across ten geo-distributed datacenters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "13280872",
                    "name": "Delong Ma"
                },
                {
                    "authorId": "1788122",
                    "name": "Z. Wen"
                },
                {
                    "authorId": "150350246",
                    "name": "Yuliang Ma"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "a317723bb953a5ab658cdbe7cd40b0861008b6d7",
            "title": "TransN: Heterogeneous Network Representation Learning by Translating Node Embeddings",
            "abstract": "Learning network embeddings has attracted growing attention in recent years. However, most of the existing methods focus on homogeneous networks, which cannot capture the important type information in heterogeneous networks. To address this problem, in this paper, we propose TransN, a novel multi-view network embedding framework for heterogeneous networks. Compared with the existing methods, TransN is an unsupervised framework which does not require node labels or user-specified meta-paths as inputs. In addition, TransN is capable of handling more general types of heterogeneous networks than the previous works. Specifically, in our framework TransN, we propose a novel algorithm to capture the proximity information inside each single view. Moreover, to transfer the learned information across views, we propose an algorithm to translate the node embeddings between different views based on the dual-learning mechanism, which can both capture the complex relations between node embeddings in different views, and preserve the proximity information inside each view during the translation. We conduct extensive experiments on real-world heterogeneous networks, whose results demonstrate that the node embeddings generated by TransN outperform those of competitors in various network mining tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145274105",
                    "name": "Zijian Li"
                },
                {
                    "authorId": "50010722",
                    "name": "Wenhao Zheng"
                },
                {
                    "authorId": "2893899",
                    "name": "Xueling Lin"
                },
                {
                    "authorId": null,
                    "name": "Ziyuan Zhao"
                },
                {
                    "authorId": "2108196278",
                    "name": "Zhe Wang"
                },
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "2054880853",
                    "name": "Xun Jian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1720801864",
                    "name": "Qiang Yan"
                },
                {
                    "authorId": "1720873015",
                    "name": "Tiezheng Mao"
                }
            ]
        },
        {
            "paperId": "aef213ec596af4a84df292aeff9f0fc013673b90",
            "title": "The simpler the better",
            "abstract": "Ridesharing services have gained global popularity as a convenient, economic, and sustainable transportation mode in recent years. One fundamental challenge in these services is planning the shared-routes (i.e., sequences of origins and destinations) among the passengers for the vehicles, such that the platform's total revenue is maximized. Though many methods can solve this problem, their effectiveness is still far from optimal on either empirical study (e.g., over 31% lower total revenue than our approach) or theoretical study (e.g., arbitrarily bad or impractical theoretical guarantee). In this paper, we study the shared-route planning queries in ridesharing services and focus on designing efficient algorithms with good approximation guarantees. Particularly, our idea is to iteratively search the most profitable route among the unassigned requests for each vehicle, which is simpler than the existing methods. Unexpectedly, we prove this simple method has an approximation ratio of 0.5 to the optimal result. Moreover, we also design an index called additive tree to improve the efficiency and apply randomization to improve the approximation guarantee. Finally, experimental results on two real datasets demonstrate that our additive-tree-based approach outperforms the state-of-the-art algorithms by obtaining up to 31.4%--127.4% higher total revenue.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47211830",
                    "name": "Yuxiang Zeng"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "51299185",
                    "name": "Yuguang Song"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "bcb791c0fbdaac376b4178f92392d600b01e08fb",
            "title": "HowSim: A General and Effective Similarity Measure on Heterogeneous Information Networks",
            "abstract": "Heterogeneous information networks (HINs) are usually used to model information systems with multi-type objects and relations. Measuring the similarity among objects is an important task in data mining applications. Currently, several similarity measures are defined for HIN. Most of these measures are based on meta-paths, which show sequences of node classes and edge types along the paths between two nodes. However, meta-paths, which are often designed by domain experts, are hard to enumerate and choose w.r.t. the quality of the similarity scores. This makes the existing similarity measures difficult to use in real applications. To address this problem, we extend SimRank, a well-known similarity measure for homogeneous graphs, to HINs, by introducing the concept of decay graph. The newly proposed relevance measure is called HowSim, which has the property of being meta-path free, and capturing the structural and semantic similarity simultaneously. The generality and effectiveness of HowSim, are demonstrated by extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "2108196278",
                    "name": "Zhe Wang"
                },
                {
                    "authorId": null,
                    "name": "Ziyuan Zhao"
                },
                {
                    "authorId": "2145274105",
                    "name": "Zijian Li"
                },
                {
                    "authorId": "2054880853",
                    "name": "Xun Jian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1720752866",
                    "name": "Jianchun Song"
                }
            ]
        },
        {
            "paperId": "c0109756c534777cee53c5d3391712fcc230d013",
            "title": "CoinMagic: A Differential Privacy Framework for Ring Signature Schemes",
            "abstract": "By allowing users to obscure their transactions via including \"mixins\" (chaff coins), ring signature schemes have been widely used to protect a sender's identity of a transaction in privacy-preserving blockchain systems, like Monero and Bytecoin. However, recent works point out that the existing ring signature scheme is vulnerable to the \"chain-reaction\" analysis (i.e., the spent coin in a given ring signature can be deduced through elimination). Especially, when the diversity of mixins is low, the spent coin will have a high risk to be detected. To overcome the weakness, the ring signature should be consisted of a set of mixins with high diversity and produce observations having \"similar\" distributions for any two coins. In this paper, we propose a notion, namely $\\epsilon$-coin-indistinguishability ($\\epsilon$-CI), to formally define the \"similar\" distribution guaranteed through a differential privacy scheme. Then, we formally define the CI-aware mixins selection problem with disjoint-superset constraint (CIA-MS-DS), which aims to find a mixin set that has maximal diversity and satisfies the constraints of $\\epsilon$-CI and the budget. In CIA-MS-DS, each ring signature is either disjoint with or the superset of its preceding ring signatures. We prove that CIA-MS-DS is NP-hard and thus intractable. To solve the CIA-MS-DS problem, we propose two approximation algorithms, namely the Progressive Algorithm and the Game Theoretic Algorithm, with theoretic guarantees. Through extensive experiments on both real data sets and synthetic data sets, we demonstrate the efficiency and the effectiveness of our approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1567491249",
                    "name": "Wangze Ni"
                },
                {
                    "authorId": "2112254088",
                    "name": "Han Wu"
                },
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2111717939",
                    "name": "Xin Lai"
                },
                {
                    "authorId": "2115477492",
                    "name": "Xiao Zhang"
                }
            ]
        },
        {
            "paperId": "d1e2329e13eb6acddce3f919f83e9116cd080e18",
            "title": "ELite: Cost-effective Approximation of Exploration-based Graph Analysis",
            "abstract": "Vertex-centric block synchronous processing systems, exemplified by Pregel and Giraph, have received extensive attention for graph processing. These systems allow programmers to think only about operations that take place at one vertex and provide the underlying computation framework that involves multiple iterations (supersteps) with communication between neighboring vertices between supersteps. As graphs grow in size to billions of vertices and trillions of edges, processing them in this model face challenges: (1) The poor latency of supersteps dominated by the tasks performed on high degree vertices or densely connected components; and (2) The overwhelming network communication among vertices that can be proved of high redundancy. For many applications, approximate results are acceptable, and if these can be computed rapidly, they may be preferable. Many of the existing approximate solutions suffer from algorithm-specific designs that are not generic or lacking theoretical guarantees on the results' quality. In this paper we tackle this problem using a generic approach that can be incorporated into the graph processing platform. The approach we advocate involves communicating vertex states to a subset of the neighbors at each superstep; this is called selective edge lookup. We show how this approach can be incorporated into two primitive graph operators: BFS and DFS, which can be the basis of many graph analysis workloads. Extensive experiments over real-world and synthetic graphs validate the effectiveness and efficiency of the selective edge lookup approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108166998",
                    "name": "Xiaofei Zhang"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "d5d1da387254a09e9fbd11ffa341d4104a6709fe",
            "title": "Globally optimal consensus maximization for robust visual inertial localization in point and line map",
            "abstract": "Map based visual inertial localization is a crucial step to reduce the drift in state estimation of mobile robots. The underlying problem for localization is to estimate the pose from a set of 3D-2D feature correspondences, of which the main challenge is the presence of outliers, especially in changing environment. In this paper, we propose a robust solution based on efficient global optimization of the consensus maximization problem, which is insensitive to high percentage of outliers. We first introduce translation invariant measurements (TIMs) for both points and lines to decouple the consensus maximization problem into rotation and translation subproblems, allowing for a two-stage solver with reduced search space. Then we show that (i) the rotation can be estimated by minimizing TIMs using only 1-dimensional branch-and-bound (BnB), (ii) the translation can be estimated by running 1-dimensional search for each of the three axes with prioritized progressive voting. Compared with the popular randomized solver, our solver achieves deterministic global convergence without requiring an initial value. Furthermore, ours is exponentially faster compared with existing BnB based methods. Finally, our experiments on both simulation and real-world datasets demonstrate that the proposed method gives accurate pose estimation even in the presence of 90% outliers (only 2 inliers).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "77239951",
                    "name": "Yanmei Jiao"
                },
                {
                    "authorId": "2118461398",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "2032435059",
                    "name": "Bo Fu"
                },
                {
                    "authorId": "32659574",
                    "name": "Qimeng Tan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1707031",
                    "name": "Shoudong Huang"
                },
                {
                    "authorId": "5738033",
                    "name": "R. Xiong"
                }
            ]
        },
        {
            "paperId": "d9d7b26199f5354d6f417f15ba53ed3b9f64a22f",
            "title": "Multi-Campaign Oriented Spatial Crowdsourcing",
            "abstract": "Recently, spatial crowdsourcing has been drawing increasing attention with its great potential in collecting geographical knowledge. The system throughput (number of assigned tasks) and workers\u2019 travel distance are two of many important factors in spatial crowdsourcing, and the improvement to one of them usually means the sacrifice of the other. However, most existing works resolve the trade-off between these two factors by simply targeting tasks within a bounding circle of each worker. In this paper, we compromise between the throughput and the distance by formulating these two factors as score terms in the objective function. This flexible formulation has the advantages of abandoning distant tasks and minimizing workers\u2019 travel distance for reachable tasks. Aside from that, we study the multi-campaign scenario of spatial crowdsourcing, which is not uncommon in practical applications while not yet discussed in existing works. The worker diversity of the campaigns is considered to be another goal and formulated as another score term in the objective function. Subsequently, the problem of multi-campaign oriented spatial crowdsourcing is to maximize the objective function comprised by the aforementioned score terms. We prove that the problem is NP-hard, thus, we propose several approximation solutions. Extensive experiments have been conducted to confirm the effectiveness and the efficiency of the devised solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49537866",
                    "name": "Libin Zheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "f3084e0d1e6fa1ff467d6c70a072f2e0bdc85ebe",
            "title": "Efficient, Simple and Automated Negative Sampling for Knowledge Graph Embedding",
            "abstract": "Negative sampling, which samples negative triplets from non-observed ones in knowledge graph (KG), is an essential step in KG embedding. Recently, generative adversarial network (GAN), has been introduced in negative sampling. By sampling negative triplets with large gradients, these methods avoid the problem of vanishing gradient and thus obtain better performance. However, they make the original model more complex and harder to train. In this paper, motivated by the observation that negative triplets with large gradients are important but rare, we propose to directly keep track of them with the cache. In this way, our method acts as a \"distilled\" version of previous GAN-based methods, which does not waste training time on additional parameters to fit the full distribution of negative triplets. However, how to sample from and update the cache are two critical questions. We propose to solve these issues by automated machine learning techniques. The automated version also covers GAN-based methods as special cases. Theoretical explanation of NSCaching is also provided, justifying the superior over fixed sampling scheme. Besides, we further extend NSCaching with skip-gram model for graph embedding. Finally, extensive experiments show that our method can gain significant improvements on various KG embedding models and the skip-gram model, and outperforms the state-of-the-art negative sampling methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48379419",
                    "name": "Yongqi Zhang"
                },
                {
                    "authorId": "3259992",
                    "name": "Quanming Yao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "f5d8f76b07518867e38e4666f38a0b93f7d1fc83",
            "title": "Fast and Accurate SimRank Computation via Forward Local Push and its Parallelization",
            "abstract": "Measuring similarity among data objects is important in data analysis and mining. SimRank is a popular link-based similarity measurement among nodes in a graph. To compute the all-pairs SimRank matrix accurately, iterative methods are usually used. For static graphs, current iterative solutions are not efficient enough, both in time and space, due to the unnecessary cost and storage by the nature of iterative updating. For dynamic graphs, all current incremental solutions for updating the SimRank matrix are based on an approximated SimRank definition, and thus have no accuracy guarantee. In this paper, we propose a novel local push based algorithm for computing and tracking all-pairs SimRank. Furthermore, we develop an iterative parallel two-step framework for local push to take advantage of modern hardwares with multicore CPUs. We show that our algorithms outperform the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "36916715",
                    "name": "Yulin Che"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "153889459",
                    "name": "Qiong Luo"
                }
            ]
        },
        {
            "paperId": "0269af9f872da49206545ed3ac463c5cb393a907",
            "title": "Social-Aware Optimal Electric Vehicle Charger Deployment on Road Network",
            "abstract": "With the increasing awareness towards protecting environment, people are paying more attention to the electric vehicles (EVs). Accompanying the rapid growing number of EVs, challenges raise at the same time about how to place EV chargers (EVC), within a city, to satisfy multiple types of charging demand. To provide a better EVC station deployment plan to benefit the whole society, we propose a problem called Social-Aware Optimal Electric Vehicle Charger Deployment (SOCD) on road network. The SOCD problem is hard and different from existing work in three aspects, 1) we assume that the charging demand should be satisfied not only in urban areas but also in relatively rural areas; 2) our work is the first one that considers an EVC station should have multiple types of charging plugs, which is more reasonable in real world; 3) different from the regional deployment solutions in previous literature, our SOCD directly works on a real road network and EVC stations are placed at appropriate POIs laying on the road network. We show that the SOCD problem is NP-hard. To deal with the hardness, we design two heuristic algorithms whose efficiency and effectiveness can be experimentally demonstrated. Furthermore, we investigate the incremental case, that is, given an existing EVC station deployment plan and extra more budget, we need to decide where and how many to place more chargers. Finally, we conduct extensive experiments on real road network of Shanghai to demonstrate both effectiveness and efficiency of our algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112215527",
                    "name": "Qiyu Liu"
                },
                {
                    "authorId": "47211830",
                    "name": "Yuxiang Zeng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2331559",
                    "name": "Xiuwen Zheng"
                }
            ]
        },
        {
            "paperId": "0b7ce2b64b547021e5924d89d88c0e5989c24944",
            "title": "Building Hierarchical Spatial Histograms for Exploratory Analysis in Array DBMS",
            "abstract": "SUMMARY As big data attracts attention in a variety of \ufb01elds, research on data exploration for analyzing large-scale scienti\ufb01c data has gained popularity. To support exploratory analysis of scienti\ufb01c data, e \ufb00 ective summarization and visualization of the target data as well as seamless cooperation with modern data management systems are in demand. In this paper, we focus on the exploration-based analysis of scienti\ufb01c array data, and de\ufb01ne a spatial V-Optimal histogram to summarize it based on the notion of histograms in the database research area. We propose histogram construction approaches based on a general hierarchical partitioning as well as a more speci\ufb01c one, the l-grid partitioning , for e \ufb00 ective and e \ufb03 cient data visualization in scienti\ufb01c data analysis. In addition, we implement the proposed algorithms on the state-of-the-art array DBMS, which is appropriate to process and manage scienti\ufb01c data. Experiments are conducted using massive evacuation simulation data in tsunami disasters, real taxi data as well as synthetic data, to verify the e \ufb00 ectiveness and e \ufb03 ciency of our methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Jing Zhao"
                },
                {
                    "authorId": "1689430",
                    "name": "Y. Ishikawa"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "50365559",
                    "name": "Chuan Xiao"
                },
                {
                    "authorId": "3265352",
                    "name": "Kento Sugiura"
                }
            ]
        },
        {
            "paperId": "2417d5ddb62e47b4fb04ac310334586eefb31205",
            "title": "Cohesive Group Nearest Neighbor Queries Over Road-Social Networks",
            "abstract": "The group nearest neighbor (GNN) search on a road network Gr, i.e., finding the spatial objects as activity assembly points with the smallest sum of distances to query users on Gr, has been extensively studied; however, previous works have neglected the fact that social relationships among query users, which ensure the maximally favorable atmosphere in the activity, can play an important role in GNN queries. Many real-world applications, such as location-based social networking services, require such queries. In this paper, we study a new problem: a GNN search on a road network that incorporates cohesive social relationships (CGNN). Specifically, both the query users of highest closeness and the corresponding top-j objects are retrieved. One critical challenge is to speed up the computation of CGNN queries over large social and road networks. To address this challenge, we propose a filtering-and-verification framework for efficient query processing. During filtering, we prune substantial unpromising users and objects using social and geographically spatial constraints. During verification, we obtain the object candidates, among which the top j are selected, with respect to the qualified users. Moreover, we further optimize search strategies to improve query performance. Finally, experimental results on real social and road networks significantly demonstrate the efficiency and efficacy of our solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2279918",
                    "name": "Fangda Guo"
                },
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "2117418478",
                    "name": "Zimeng Wang"
                }
            ]
        },
        {
            "paperId": "25b950024b0d094915fcd25f0ecf6bae2f41706c",
            "title": "Neural Recurrent Structure Search for Knowledge Graph Embedding",
            "abstract": "Knowledge graph (KG) embedding is a fundamental problem in mining relational patterns. It aims to encode the entities and relations in KG into low dimensional vector space that can be used for subsequent algorithms. Lots of KG embedding models have been proposed to learn the interactions between entities and relations, which contain meaningful semantic information. However, structural information, which encodes local topology among entities, is also important to KG. In this work, we propose S2E to distill structural information and combine it with semantic information for different KGs as a neural architecture search (NAS) problem. First, we analyze the difficulty of using a unified model to solve the distillation problem. Based on it, we define the path distiller to recurrently combine structural and semantic information along relational paths, which are sampled to preserve both local topologies and semantics. Then, inspired by the recent success of NAS, we design a recurrent network-based search space for specific KG tasks and propose a natural gradient (NG) based search algorithm to update architectures. Experimental results demonstrate that the searched models by our proposed S2E outperform human-designed ones, and the NG based search algorithm is efficient compared with other NAS methods. Besides, our work is the first NAS method for RNN that can search architectures with better performance than human-designed models.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "48379419",
                    "name": "Yongqi Zhang"
                },
                {
                    "authorId": "3259992",
                    "name": "Quanming Yao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "2dd616aa126eecdef20af317bfcde4d370f43db7",
            "title": "Interaction Management in Crowdsourcing",
            "abstract": "Crowdsourcing is anticipated as a promising paradigm for Future of Work (FoW), where groups of humans are engaged for problem-solving, services and innovation which are usually dif\ufb01cult for machines. During the entire work\ufb02ow of crowdsourcing, intensive interactions take place between workers and the crowdsourcing platform, as well as among groups of workers. For sustainable crowdsourcing, the design and management of these interactions should not regard human workers as machines, but rather as individuals and social beings. In this article, we highlight the critical interactions in typical crowdsourcing ecosystems, summarize past efforts on human-centric interaction management in crowdsourcing, and discuss emerging interaction management research towards cross-platform crowdsourcing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107987065",
                    "name": "Yansheng Wang"
                },
                {
                    "authorId": "3406090",
                    "name": "Tianshu Song"
                },
                {
                    "authorId": "1720763685",
                    "name": "Qian Tao"
                },
                {
                    "authorId": "47211830",
                    "name": "Yuxiang Zeng"
                },
                {
                    "authorId": "2735715",
                    "name": "Zimu Zhou"
                },
                {
                    "authorId": "2110289813",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "33e5f12a371d4b085d507a6c1c036e7742f13fdf",
            "title": "Maximizing the Utility in Location-Based Mobile Advertising",
            "abstract": "Nowadays, the locations and contexts of users are easily accessed by mobile advertising brokers, and the brokers can send customers related location-based advertisement. In this paper, we consider a location-based advertising problem, namely maximum utility advertisement assignment (MUAA) problem, with the estimation of the interests of customers and the contexts of the vendors, we want to maximize the overall utility of ads by determining the ads sent to each customer subject to the constraints of the capacities of customers, the distance ranges and the budgets of vendors. We prove that the MUAA problem is NP-hard and intractable. Thus, we propose one offline approach, namely the reconciliation approach, which has an approximation ratio of (1-\u03b5)\u22c5\u03b8, where \u03b8 = min(a_1\\n^c_1, a_2 n^c_2,..., a_m n^c_m), and n^c_i is the larger value between the number of valid vendors and the capacity a_i of customer u_i. Experiments on real data sets confirm the efficiency and effectiveness of our proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "47130133",
                    "name": "Siyuan Liu"
                }
            ]
        },
        {
            "paperId": "34f0a58643b560419529783f7e92dabd8b10e994",
            "title": "SLADE: A Smart Large-Scale Task Decomposer in Crowdsourcing",
            "abstract": "A crowdsourcing task in real-world applications often consists of thousands of atomic tasks. A common practice to distribute a large-scale crowdsourcing task is to pack atomic tasks into task bins and send to crowd workers in batches. It is challenging to decompose a large-scale crowdsourcing task into task bins to ensure reliability at a minimal total cost. In this paper, we propose the Smart Large-scAle task DEcomposer (SLADE) problem, which aims to decompose a large-scale crowdsourcing task to achieve the desired reliability at a minimal cost. We prove its NP-hardness and study two variants of the problem. For the homogeneous SLADE problem, we propose a greedy algorithm and an approximation framework using an optimal priority queue (OPQ) structure with provable approximation ratio. For the heterogeneous SLADE problem, we extend this framework and prove its approximation guarantee. Extensive experiments validate the effectiveness and efficiency of the solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2735715",
                    "name": "Zimu Zhou"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "144128216",
                    "name": "Lidan Shou"
                },
                {
                    "authorId": "2722757",
                    "name": "Weifeng Lv"
                }
            ]
        },
        {
            "paperId": "58b338181f75417c8e3b234126f6a92faab2fd6f",
            "title": "Last-Mile Delivery Made Practical: An Efficient Route Planning Framework with Theoretical Guarantees",
            "abstract": "\n Last-mile delivery (LMD) refers to the movement of goods from transportation origins to the final destinations. It has widespread applications such as urban logistics, e-commerce, etc. One fundamental problem in last-mile delivery is route planning, which schedules multiple couriers' routes,\n i.e.\n , sequences of origins and destinations of the requests under certain optimization objectives. Prior studies usually designed heuristic solutions to two strongly NP-hard optimization objectives: minimizing the makespan (\n i.e.\n , maximum travel time) of couriers and total latency (\n i.e.\n , waiting time) of requesters. There is no algorithm with theoretical guarantees for either optimization objective in practical cases. In this paper, we propose a theoretically guaranteed solution framework for both objectives. It achieves both approximation ratios of 6\u03c1, where \u03c1 is the approximation ratio of a core operation, called\n k\n LMD, which plans for one courier a route consisting of\n k\n requests. Leveraging a spatial index called hierarchically separated tree, we further design an efficient approximation algorithm for\n k\n LMD with \u03c1 =\n O\n (log\n n\n ), where\n n\n is the number of requests. Experimental results show that our approach outperforms state-of-the-art methods by averagely 48.4%-96.0% and 49.7%-96.1% for both objectives. Especially in large-scale real datasets, our algorithm has 29.3x-108.9x shorter makespan and 20.2x-175.1x lower total latency than the state-of-the-art algorithms.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47211830",
                    "name": "Yuxiang Zeng"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "5eb31015f8b446cefe0bb1d86e274b1a6d333fc7",
            "title": "Probabilistic Maximum Range-Sum Queries on Spatial Database",
            "abstract": "Maximum Range-Sum (MaxRS) query is an important operator in spatial database for retrieving regions of interest (ROIs). Given a rectangular query size a \u00d7 b and a set of spatial objects associated with positive weights, MaxRS retrieves rectangular regions Q of size a \u00d7 b, such that the sum of object weights covered by Q (i.e., range-sum) is maximized. Due to the inaccuracy of the location acquisition, the collected locations of spatial objects are inherently uncertain and imprecise, which can be modeled by uncertain objects. In this paper, we propose a Probabilistic Maximum Range-Sum (PMaxRS) query over uncertain spatial objects, which obtains a set \u03b3* of rectangles such that the probability that each region Q \u03b5 \u03b3* has the maximum range-sum exceeds a user-specified threshold Pt. We show that determining whether a given region Q is #P-complete. To tackle the hardness, we introduce the PMaxRS_Framework based on pruning and refinement strategies. In the pruning step, we propose a candidate generation technique to reduce the search space. In the refinement step, we design an efficient sampling-based approximation algorithm to verify the remaining candidate regions. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of our algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112215527",
                    "name": "Qiyu Liu"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "60fefe8e7116b2820e89a22df7cf2dc1a964233b",
            "title": "GCache: Neighborhood-Guided Graph Caching in a Distributed Environment",
            "abstract": "Distributed graph systems are becoming extremely popular due to their flexibility, scalability, and robustness in big graph processing. In order to improve the performance of the distributed graph systems, caching is a very effective technique to achieve fast response and reduce the communication cost. Existing works include online and offline caching algorithms. Online caching algorithms (such as least recently used (LRU) and most recently used (MRU)) are lightweight and flexible, however, neglect the topological properties of big graphs. Offline caching algorithms (such as node pre-ordered) consider the graph topology, but are very expensive and heavy. In this paper, we propose a novel caching mechanism, GraphCache (GCache), for big distributed graphs. GCache consists of an offline phase and an online phase, which inherits the advantages of online and offline caching algorithms. Specifically, the offline phase provides a caching model based on the bipartite graph clustering and give efficient algorithms to solve it. The online phase caches and schedules the graph clusters output from the offline phase, based on the LRU and MRU strategies. GCache can be seamlessly integrated into the state-of-the-art graph processing systems, e.g., Giraph. Finally, our experimental results demonstrate the feasibility of our proposed caching techniques in speeding up graph algorithms over distributed big graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                },
                {
                    "authorId": "2120271633",
                    "name": "Yishu Wang"
                },
                {
                    "authorId": "150350246",
                    "name": "Yuliang Ma"
                }
            ]
        },
        {
            "paperId": "6996c5bb049b06914701a8e7d81d13f65bf6b72e",
            "title": "Online Adaptive Approximate Stream Processing With Customized Error Control",
            "abstract": "In approximate processing on stream data, most works focus on how to approximate online arrival data. However, the efficiency of approximation needs to consider multiple aspects. Generally, customers submit their requests with specific quality requirements (e.g., maximum error). This raises a critical problem that online quality control is required to meet the desired quality of service. Since the continuous arriving data may not be entirely stored and needs to be processed immediately, it brings the difficulty of acquiring knowledge online which significantly affects the quality of results. To address these problems, we present an online adaptive approximate processing framework with a delicate combination of data learning, sampling, and quality control. We first design an online data learning strategy for stream data. With the real-time learning results, we propose a dynamic sampling strategy that switches to different sampling methods based on the change of the load. Finally, we present a double-check error control strategy to monitor and correct large errors. Each operation module is correlated through online learning and feedback. The experiments with both synthetic and real-world datasets show that the proposed approximate framework is not only applicable to different data distributions but also provides a customized error control.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2094025",
                    "name": "Xiaohui Wei"
                },
                {
                    "authorId": "2143861114",
                    "name": "Yuanyuan Liu"
                },
                {
                    "authorId": "34740942",
                    "name": "Xingwang Wang"
                },
                {
                    "authorId": "145369990",
                    "name": "Shang Gao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "6fc12f57cebed730734ac0c73520d33376c852c0",
            "title": "Canonicalization of Open Knowledge Bases with Side Information from the Source Text",
            "abstract": "Nowadays Open Information Extraction (Open IE) approaches, which extract  triples from unstructured text, contribute to the construction of large Open Knowledge Bases (Open KBs). However, one crucial problem is that the noun phrases and relation phrases in the extracted triples are not well canonicalized, which leads to a large number of redundant and ambiguous facts. For example, both  and  may be extracted and stored in Open KBs. Recent research proposes to solve this problem by clustering over manually-defined feature spaces based on the similarity of the noun phrases and relation phrases. However, the performance of such techniques is limited, since only the information contained in the triples is utilized to measure their similarity. In this paper, we propose to perform canonicalization over Open IE triples by incorporating the side information from the original data sources, including the candidate entities of the noun phrases detected in the source text, the types of the candidate entities and the domain knowledge of the source text. We model the canonicalization problem of noun phrases and relation phrases jointly based on such side information, and demonstrate the effectiveness of our approach through extensive experiments on two real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2893899",
                    "name": "Xueling Lin"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "767717e2907d2b025a7a29b931b62510dcbff96e",
            "title": "2-Entity RANSAC for robust visual localization in changing environment",
            "abstract": "Visual localization has attracted considerable attention due to its low-cost and stable sensor, which is desired in many applications, such as autonomous driving, inspection robots and unmanned aerial vehicles. However, current visual localization methods still struggle with environmental changes across weathers and seasons, as there is significant appearance variation between the map and the query image. The crucial challenge in this situation is that the percentage of outliers, i.e. incorrect feature matches, is high. In this paper, we derive minimal closed form solutions for 3D-2D localization with the aid of inertial measurements, using only 2 point matches or 1 point match and 1 line match. These solutions are further utilized in the proposed 2-entity RANSAC, which is more robust to outliers as both line and point features can be used simultaneously and the number of matches required for pose calculation is reduced. Furthermore, we introduce three feature sampling strategies with different advantages, enabling an automatic selection mechanism. With the mechanism, our 2-entity RANSAC can be adaptive to the environments with different distribution of feature types in different segments. Finally, we evaluate the method on both synthetic and real-world datasets, validating its performance and effectiveness in inter-session scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "77239951",
                    "name": "Yanmei Jiao"
                },
                {
                    "authorId": "2118461398",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "2032435059",
                    "name": "Bo Fu"
                },
                {
                    "authorId": "31627893",
                    "name": "X. Ding"
                },
                {
                    "authorId": "32659574",
                    "name": "Qimeng Tan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "5738033",
                    "name": "R. Xiong"
                }
            ]
        },
        {
            "paperId": "76becc6128662262e3962e6799cede7bba2256c5",
            "title": "Three-Dimensional Stable Matching Problem for Spatial Crowdsourcing Platforms",
            "abstract": "The popularity of mobile Internet techniques and Online-To-Offline(O2O) business models has led to the emergence of various spatial crowdsourcing (SC) platforms in our daily life. A core issue of SC platforms is to assign tasks to suitable crowd workers. Existing approaches usually focus on the matching of two types of objects,tasks and workers, and let workers to travel to the location of usersto provide services, which is a 2D matching problem. However, recent services provided by some new platforms, such as person-alized haircut service1and station ride-sharing, need users andworkers travel together to a third workplace to complete the service, which is indeed a 3D matching problem. Approaches in the existingstudies either cannot solve such 3D matching problem, or lack aassignment plan satisfying both users' and workers' preference inreal applications. Thus, in this paper, we propose a 3-Dimensional Stable Spatial Matching(3D-SSM) for the 3D matching problem innew SC services. We prove that the 3D-SSM problem is NP-hard, and propose two baseline algorithms and two efficient approximatealgorithms with bounded approximate ratios to solve it. Finally, weconduct extensive experiment studies which verify the efficiencyand effectiveness of the proposed algorithms on real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2132848303",
                    "name": "Boyang Li"
                },
                {
                    "authorId": "2329311",
                    "name": "Yurong Cheng"
                },
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "77afd2685a30c968c2c32165ddb5f2982e23d67f",
            "title": "Adaptive Distributed RDF Graph Fragmentation and Allocation based on Query Workload",
            "abstract": "As massive volumes of Resource Description Framework (RDF) data are growing, designing a distributed RDF database system to manage them is necessary. In designing this system, it is very common to partition the RDF data into some parts, called fragments, which are then distributed. Thus, the distribution design comprises two steps: fragmentation and allocation. In this study, we explore the workload for fragmentation and allocation, which aims to reduce the communication cost during SPARQL query processing. Specifically, we adaptively maintain some frequent access patterns (FAPs) to reflect the characteristics of the workload while ensuring the data integrity and approximation ratio. Based on these frequent access patterns, we propose three fragmentation strategies, namely vertical, horizontal, and mixed fragmentation, to divide RDF graphs while meeting different types of query processing objectives. After fragmentation, we discuss how to allocate these fragments to various sites while balancing the fragments. Finally, we discuss how to process queries based on the results of fragmentation and allocation. Experiments over large RDF datasets confirm the superior performance of our proposed solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144189270",
                    "name": "Peng Peng"
                },
                {
                    "authorId": "144554647",
                    "name": "Lei Zou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144060462",
                    "name": "Dongyan Zhao"
                }
            ]
        },
        {
            "paperId": "7dfe376934208ff702980d080835a985c91a143e",
            "title": "G*-Tree: An Efficient Spatial Index on Road Networks",
            "abstract": "In this paper, we propose an efficient hierarchical index, G*-tree, to optimize spatial queries on road networks. Most existing graph indexes can only support one kind of query, and thus we need to build multiple indexes on a road network to handle various kinds of spatial queries, which is inefficient and unscalable for real-world applications. To address the problem, a recent study proposes G-tree to support multiple types of spatial queries on road networks within one framework. However, the assembly-based method on G-tree is not efficient enough to handle spatial queries when vertices, which are close in a road network, are distant in G-tree. To address the inefficiency problem of G-tree, in this paper, we propose a novel index structure on road networks, namely G*-tree, whose key idea is to build shortcuts between selected leaf nodes. Based on G*-tree, we propose three shortcut-based algorithms to answer distance queries, k-nearest neighbor queries and range queries, respectively, which are more efficient than the existing assembly-based algorithms on G-tree. Moreover, we propose a shortcut selection algorithm to optimize the performance of spatial queries on G*-tree. We conduct extensive experiments to compare our G*-tree and the state-of-the-art indexing methods on various large-scale road networks, where the results demonstrate that our G*-tree has better efficiency and scalability than the competitors to handle spatial queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145274105",
                    "name": "Zijian Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                }
            ]
        },
        {
            "paperId": "8062bc94efd1da75d192f5e312f28e4bd70c7ec7",
            "title": "Relation Extraction via Domain-aware Transfer Learning",
            "abstract": "Relation extraction in knowledge base construction has been researched for the last decades due to its applicability to many problems. Most classical works, such as supervised information extraction and distant supervision, focus on how to construct the knowledge base (KB) by utilizing the large number of labels or certain related KBs. However, in many real-world scenarios, the existing methods may not perform well when a new knowledge base is required but only scarce labels or few related KBs available. In this paper, we propose a novel approach called, Relation Extraction via Domain-aware Transfer Learning (ReTrans), to extract relation mentions from a given text corpus by exploring the experience from a large amount of existing KBs which may not be closely related to the target relation. We first propose to initialize the representation of relation mentions from the massive text corpus and update those representations according to existing KBs. Based on the representations of relation mentions, we investigate the contribution of each KB to the target task and propose to select useful KBs for boosting the effectiveness of the proposed approach. Based on selected KBs, we develop a novel domain-aware transfer learning framework to transfer knowledge from source domains to the target domain, aiming to infer the true relation mentions in the unstructured text corpus. Most importantly, we give the stability and generalization bound of ReTrans. Experimental results on the real world datasets well demonstrate that the effectiveness of our approach, which outperforms all the state-of-the-art baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51129379",
                    "name": "Shimin Di"
                },
                {
                    "authorId": "2923152",
                    "name": "Yanyan Shen"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "826ecdce35e8db40e10ddcb58327b4c231caa46d",
            "title": "Online Social Media Recommendation Over Streams",
            "abstract": "As one of the most popular services over online platforms, social recommendation has attracted increasing research efforts recently. Among all the recommendation tasks, an important one is item recommendation over high speed social media streams. Existing stream recommendation techniques are not effective for handling social users with diverse interests. Meanwhile, approaches for recommending items to a particular user are not efficient when applied to a huge number of users over high speed streams. In this paper, we propose a novel framework for the social recommendation over streams. Specifically, we first propose a novel Bi-Layer Hidden Markov Model (BiHMM) that adaptively captures the users' behaviors and their interactions with influential official accounts to predict their long-term and short-term interests. Then, we design a new probabilistic entity matching scheme for identifying the relevance score of a streaming item to a user. Moreover, we propose a novel index scheme called CPPse-index for improving the efficiency of our solution. Extensive tests are conducted to prove the superiority of our approach in terms of the recommendation quality and time cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "2064191826",
                    "name": "D. Qin"
                },
                {
                    "authorId": "40505029",
                    "name": "Xiaolu Lu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "49889222",
                    "name": "Yanchun Zhang"
                }
            ]
        },
        {
            "paperId": "8aae6657a57ccc11a2426ede4f5b6199311bb20c",
            "title": "A Queueing-Theoretic Framework for Vehicle Dispatching in Dynamic Car-Hailing",
            "abstract": "With the rapid development of smart mobile devices, the car-hailing platforms (e.g., Uber or Lyft) have attracted much attention from both the academia and the industry. In this paper, we consider an important dynamic car-hailing problem, namely maximum revenue vehicle dispatching (MRVD), in which rider requests dynamically arrive and drivers need to serve as many riders as possible such that the entire revenue of the platform is maximized. We prove that the MRVD problem is NP-hard and intractable. To handle the MRVD problem, we propose a queueing-based vehicle dispatching framework, which first uses existing machine learning algorithms to predict the future vehicle demand of each region, then estimates the idle time periods of drivers through a queueing model for each region. With the information of the predicted vehicle demands and estimated idle time periods of drivers, we propose one batch-based vehicle dispatching algorithm to efficiently assign suitable drivers to riders such that the expected entire revenue of the platform is maximized during each batch processing. Through experiments over real data sets, we demonstrate the efficiency and effectiveness of our proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "2117344775",
                    "name": "Chao Feng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "50219447",
                    "name": "Zheng Wang"
                }
            ]
        },
        {
            "paperId": "a10521e8446b48228adcddfabd8c8e99608d6295",
            "title": "Auction-Based Order Dispatch and Pricing in Ridesharing",
            "abstract": "Ridesharing plays a more and more important role in modern transport. In this paper, we propose solutions for the bonus-offering scenario of ridesharing platforms (service providers). When vehicles are in shortage, requesters are allowed to offer bonus so that their orders can get prioritized in the dispatch process. To enable self-motivated bonus bidding of requesters, we devise an auction mechanism, where requesters are supposed to submit their bids truthfully and the platform conducts order dispatch and pricing. Our goal is to maximize the overall utility of the auction, while ensuring desirable auction properties such as truthfulness and individual rationality. To realize that, we propose a greedy and a ranking approach for order dispatch and their corresponding pricing strategies. Extensive experiments on real data suggest that the ranking approach is both effective and efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49537866",
                    "name": "Libin Zheng"
                },
                {
                    "authorId": "2112624793",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "afc0e147d0f5ba83b12282138d44068588ccc7d2",
            "title": "DLTA: A Framework for Dynamic Crowdsourcing Classification Tasks",
            "abstract": "The increasing popularity of crowdsourcing markets enables the application of crowdsourcing classification tasks. How to conduct quality control in such an application to achieve accurate classification results from noisy workers is an important and challenging task, and has drawn broad research interests. However, most existing works do not exploit the label acquisition phase, which results in their disability of making a proper budget allocation. Moreover, some works impractically make the assumption of managing workers, which is not supported by common crowdsourcing platforms such as AMT or CrowdFlower. To overcome these drawbacks, in this paper, we devise a Dynamic Label Acquisition and Answer Aggregation (DLTA) framework for crowdsourcing classification tasks. The framework proceeds in a sequence of rounds, adaptively conducting label inference and label acquisition. In each round, it analyzes the collected answers of previous rounds to perform proper budget allocation, and then issues the resultant query to the crowd. To support DLTA, we propose a generative model for the collection of labels, and correspondingly strategies for label inference and budget allocation. Experimental results show that compared with existing methods, DLTA obtains competitive accuracy in the binary case. Besides, its extended version, which plugs in the state-of-the-art inference technique, achieves the highest accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49537866",
                    "name": "Libin Zheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "b1ee6151c1b17eecb802e307694f2b610b3f2cc0",
            "title": "Weight-Constrained Route Planning Over Time-Dependent Graphs",
            "abstract": "Weight-constrained route planning (WRP) over static graphs has been extensively studied due to its wide application to transportation networks. However, real transportation networks often evolve over time and are thus modeled as time-dependent graphs. In this paper, we study the WRP problem over a large time-dependent graph by incorporating continuous time and weight functions into it. Most existing works regarding route planning over time-dependent graphs are based on the first-in-first-out (FIFO) property. Unfortunately, the FIFO property does not hold for our problem. To solve the problem, we propose two novel route planning algorithms, namely, a baseline algorithm and an advanced algorithm. Specifically, the advanced algorithm is even more efficient than the baseline algorithm, as the advanced algorithm incorporates a fast traversal scheme and tight bounds of time functions to terminate the traversal as early as possible. We confirm the effectiveness and efficiency of our algorithms by extensive experiments on real datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "150350246",
                    "name": "Yuliang Ma"
                },
                {
                    "authorId": "2120271633",
                    "name": "Yishu Wang"
                }
            ]
        },
        {
            "paperId": "b594b21557395c6a8fa8356249373f8e318c2df2",
            "title": "AutoSF: Searching Scoring Functions for Knowledge Graph Embedding",
            "abstract": "Scoring functions (SFs), which measure the plausibility of triplets in knowledge graph (KG), have become the crux of KG embedding. Lots of SFs, which target at capturing different kinds of relations in KGs, have been designed by humans in recent years. However, as relations can exhibit complex patterns that are hard to infer before training, none of them can consistently perform better than others on existing benchmark data sets. In this paper, inspired by the recent success of automated machine learning (AutoML), we propose to automatically design SFs (AutoSF) for distinct KGs by the AutoML techniques. However, it is non-trivial to explore domain- specific information here to make AutoSF efficient and effective. We firstly identify a unified representation over popularly used SFs, which helps to set up a search space for AutoSF. Then, we propose a greedy algorithm to search in such a space efficiently. The algorithm is further sped up by a filter and a predictor, which can avoid repeatedly training SFs with same expressive ability and help removing bad candidates during the search before model training. Finally, we perform extensive experiments on benchmark data sets. Results on link prediction and triplets classification show that the searched SFs by AutoSF, are KG dependent, new to the literature, and outperform the state-of- the-art SFs designed by humans. 1",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "48379419",
                    "name": "Yongqi Zhang"
                },
                {
                    "authorId": "3259992",
                    "name": "Quanming Yao"
                },
                {
                    "authorId": "1752769",
                    "name": "Wenyuan Dai"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "bf6f3498d887da7b9dea07a45ac500638a6ab624",
            "title": "Minimizing Maximum Delay of Task Assignment in Spatial Crowdsourcing",
            "abstract": "Spatial crowdsourcing services, such as Uber and Grabhub, become popular recently. Task assignment plays an important role in offering high-quality services. However, most of the existing solutions for task assignment only focus on the entire performance of the platform and do not optimize the maximum assignment delay. As a result, they cannot handle some real world scenarios which require minimizing the maximum delay in task assignment. In this paper, we study the minimizing maximum delay spatial crowdsourcing (MMD-SC) problem and propose solutions aiming at achieving a worst case controlled task assignment. The MMD-SC problem assumes that both workers and requesters come dynamically and considers not only the workers' travel costs but also the buffering time of tasks, thus it is very challenging due to two-sided online setting. To address these challenges, in this work, we propose a space embedding based online random algorithm with a competitive ratio of O(log n) and two efficient heuristic algorithms, namely the threshold based greedy approach and the batch-based approach. In addition, we demonstrate the effectiveness and efficiency of our methods via extensive experiments on both synthetic and real datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111605555",
                    "name": "Zhao Chen"
                },
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "47211830",
                    "name": "Yuxiang Zeng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "de2e66374ffb449267191e55d6c5482a471eafa7",
            "title": "I-CARS: An Interactive Context-Aware Recommender System",
            "abstract": "Context-aware recommendation has attracted significant attentions over online sites due to its smart context adaption in improving recommendation quality. However, the user's instant contexts do not follow his/her regular user behaviour patterns, thus have not been well captured for advanced personalization of recommendation generation. In this work, we propose an Interactive Context-Aware Recommender System (I-CARS), which allows users to interact and present their needs, so the system can personalize and refine user preferences. I-CARS iteratively asks a question to a user to trigger feedback in term of her recent contexts and incorporates the response to recommend items most likely satisfying his/her instant interests. Specifically, we first propose a Personalized Weighted Context-Aware Matrix Factorization (PW-CAMF) that enables the personalization of important contexts for each user. Then we propose two question selection strategies that exploit user preferences through feedback. We have conducted comprehensive experiments over two real datasets. The experimental results prove the effectiveness of our I-CARS system compare to existing competitors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66674918",
                    "name": "Rosni Lumbantoruan"
                },
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "7800596",
                    "name": "Yongli Ren"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "e6df6629416619cdf741502423aaf29fcff4f3d5",
            "title": "Cooperation-Aware Task Assignment in Spatial Crowdsourcing",
            "abstract": "With the popularity of smart devices and the development of high-speed wireless networks, the spatial crowdsourcing has attracted much attention from both academia and industry (e.g., Uber and TaskRabbit). Specifically, a spatial crowdsourcing platform assigns workers to location-based tasks according to their current positions, then the workers need to physically move to the specified locations to conduct the assigned tasks. In this paper, we consider an important spatial crowdsourcing problem, namely cooperation-aware spatial crowdsourcing (CA-SC), where spatial tasks (e.g., collecting the Wi-Fi signal strength in one building) are time-constrained and require more than one worker to complete thus the cooperation among assigned workers is essential to the result. Our CA-SC problem is to assign workers to spatial tasks such that the overall cooperation quality is maximized. We prove that the CA-SC problem is NP-hard by reducing from the k-set packing problem, thus intractable. To tackle the CA-SC problem, we propose task-priority greedy (TPG) approach and game theoretic (GT) approach with two optimization methods to quickly solve the CA-SC problem and achieve high total cooperation quality scores. Through extensive experiments, we demonstrate the efficiency and effectiveness of our proposed approaches over both real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2778556",
                    "name": "Jieping Ye"
                }
            ]
        },
        {
            "paperId": "f57085e5d83707447bb0dfc8137e0acf8f9b9579",
            "title": "Fluid: A Blockchain based Framework for Crowdsourcing",
            "abstract": "Recently, crowdsourcing has emerged as a new computing paradigm to solve problems that need human intrinsic, such as image annotation. However, there are two limitations in existing crowdsourcing platforms, i.e. non-transparent incentive mechanism and isolated profiles of workers, which harms the interests of both requesters and workers. Meanwhile, Blockchain technology introduces a solution to build a transparent, immutable data model in the Byzantine environment. Moreover, Blockchain systems (e.g. Ethereum) can also support the Tuning-complete script called smart contracts. Thus, we are motivated to use the feature of the transparent data model and smart contract in Blockchain to address the two limitations. Based on the proposed solutions, we have designed a Blockchain based framework which supports foundations of general crowdsourcing platforms. In addition, our framework also has following novel features: (1) it provides the transparent incentive mechanisms; (2) it supports a trusted worker's profile sharing in a cross-platform mode.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109601286",
                    "name": "Siyuan Han"
                },
                {
                    "authorId": "80541392",
                    "name": "Zihuan Xu"
                },
                {
                    "authorId": "47211830",
                    "name": "Yuxiang Zeng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "fabc4f72a2b96e383e23a16da9b4a075e12c81d3",
            "title": "Procrastination-Aware Scheduling: A Bipartite Graph Perspective",
            "abstract": "Procrastination is a prevalent form of self-control failure. As it often concerns with the individual's ability to meet the deadline, an efficient time management is crucial for overcoming it. Though a considerable amount of work in behavioral economics provides useful insights, there is not a computational way to guide us how to obtain an appropriate schedule for all the things to be done, especially when the relationship of the deadlines is intrinsic. In this paper, we first propose the Procrastination-aware Scheduling Problem (PSP) to model an appropriate schedule. A bipartite graph formulation is then developed to further illustrate the concepts. We find the PSP is NP-hard in the strong sense and design an approximation algorithm. In addition, we note the significance of the PSP under the online scenario (called OnlinePSP). Finally, we verify the effectiveness and efficiency of the proposed algorithms through extensive experiments on real datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108532674",
                    "name": "Libin Wang"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "1767378",
                    "name": "Chunming Hu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2110530936",
                    "name": "Yiming Li"
                }
            ]
        },
        {
            "paperId": "063a429549772e676c4627a90835648a2e447872",
            "title": "Latency-Oriented Task Completion via Spatial Crowdsourcing",
            "abstract": "Spatial crowdsourcing brings in a new approach for social media and location-based services (LBS) to collect location specific information via mobile users. For example, when a user checks in at a shop on Facebook, he will immediately receive and is asked to complete a set of tasks such as \"what is the opening hour of the shop\". It is non-trivial to complete a set of tasks timely and accurately via spatial crowdsourcing. Since workers in spatial crowdsourcing are often transient and limited in number, these social media platforms need to properly allocate workers within the set of tasks such that all tasks are completed (i) with high quality and (ii) with a minimal latency (estimated by the arriving index of the last recruited worker). Solutions to quality and latency control in traditional crowdsourcing are inapplicable in this problem because they either assume sufficient workers or ignore the spatiotemporal factors. In this work, we define the Latency-oriented Task Completion (LTC) problem, which trades off quality and latency (number of workers) of task completion in spatial crowdsourcing. We prove that the LTC problem is NP-hard. We first devise a minimum-cost-flow based algorithm with a constant approximation ratio for the LTC problem in the offline scenario, where all information is known a prior. Then we study the more practical online scenario of the LTC problem, where workers appear dynamically and the platform needs to arrange tasks for each worker immediately based on partial information. We design two greedy-based algorithms with competitive ratio guarantees to solve the LTC problem in the online scenario. Finally, we validate the effectiveness and efficiency of the proposed solutions through extensive evaluations on both synthetic and real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47211830",
                    "name": "Yuxiang Zeng"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2735715",
                    "name": "Zimu Zhou"
                }
            ]
        },
        {
            "paperId": "1cc0e8657b2c84605b03464af1998304e274f175",
            "title": "Realtime Traffic Speed Estimation with Sparse Crowdsourced Data",
            "abstract": "Realtime traffic speed estimation is an important issue in urban computation. Existing approaches usually focus on exploiting the periodicity properties of the traffic speed and utilize crowdsourcing techniques to facilitate real-time estimation. The quality of such estimation is limited in real world: 1) the accuracy of existing estimation over-relies on the probed data; 2) the accidental traffic variance is ignored; 3) existing strategies incur exhaustive usage of human workers to get fine-grained estimation results. Thus, a more intelligent RTSE approach is desired. In this paper, we propose the framework of CrowdRTSE (Crowdsourcing-based Real-time Traffic Speed Estimation), which adopts a hybrid offline-online process to collaboratively exploit the historical and real-time data to produce high-quality RTSE. To accomplish such a framework, we devise effective algorithms to judiciously select the best group of human workers with a constant approximation ratio, and effectively propagate the crowdsourced data with high efficiency. Comprehensive evaluations have been conducted on both synthetic and real world datasets. The experimental results verify the effectiveness and efficiency of our proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145976421",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                }
            ]
        },
        {
            "paperId": "21723c9c6cd360b9c5f3410e64ea8cfb49233c8a",
            "title": "Revolution of Self-Organizing Network for 5G MmWave Small Cell Management: From Reactive to Proactive",
            "abstract": "The mmWave frequency band has been widely accepted as an ideal carrier for the next-generation ultra-dense small cell communications to achieve the ambitious objective of 1000-fold increase in network capacity, but such a novel architecture also invokes tremendous anxiety for operators regarding the planning, configuration, maintenance, optimization, and troubleshooting. SON, introduced about 10 years ago, aiming to affiliate network management through addressing complexity and decreasing cost in an automatic way, nowadays has been extensively considered the only viable way to enable small cell deployments; however, as a technology traditionally designed and standardized for LTE-based systems, the conventional paradigm will no longer be suitable for mmWavebased cellular networks, which contain some peculiar characteristics, such as directivity of signal transmission and sensitivity to blockage and mobility, differing from their low-frequency counterparts and thereby bringing numerous unprecedented challenges. To smoothly migrate toward the 5G era, the legacy SON campaign eagerly demands a thorough revolution to think over the new features and cope with the new problems by defining new functionalities and proposing new algorithms. It is important, but not easy. In this article, we share some preliminary insights about the mmWave-oriented proactive SON paradigm, hopefully spurring further research in this area and accelerating its practical application in 5G.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150635915",
                    "name": "Jun Xu"
                },
                {
                    "authorId": "2225242",
                    "name": "Junmei Yao"
                },
                {
                    "authorId": "50815748",
                    "name": "Lu Wang"
                },
                {
                    "authorId": "8584850",
                    "name": "Kaishun Wu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145612192",
                    "name": "W. Lou"
                }
            ]
        },
        {
            "paperId": "3cb0d6fcc12f1b16092fc49e8a8a69ff283aa93b",
            "title": "Quality-Assured Synchronized Task Assignment in Crowdsourcing",
            "abstract": "With the rapid development of crowdsourcing platforms that aggregate the intelligence of Internet workers, crowdsourcing has been widely utilized to address problems that require human cognitive abilities. Considering great dynamics of worker arrival and departure, it is of vital importance to design a task assignment scheme to adaptively select the most beneficial tasks for the available workers. In this paper, in order to make the most efficient utilization of the worker labor and balance the accuracy of answers and the overall latency, we a) develop a parameter estimation model that assists in estimating worker expertise, question easiness, and answer confidence; b) propose a quality-assured synchronized task assignment scheme that executes in batches and maximizes the number of potentially completed questions (MCQ) within each batch. We prove that MCQ problem is NP-hard and present two greedy approximation solutions to address the problem. The effectiveness and efficiency of the approximation solutions are further evaluated through extensive experiments on synthetic and real datasets. The experimental results show that the accuracy and the overall latency of the MCQ approaches outperform the existing online task assignment algorithms in the synchronized task assignment scenario.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35734465",
                    "name": "Jiayang Tu"
                },
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "45b786377a37baa1c0d8485171c2730eb74f8b4a",
            "title": "Subjective Knowledge Base Construction Powered By Crowdsourcing and Knowledge Base",
            "abstract": "Knowledge base construction (KBC) has become a hot and in-time topic recently with the increasing application need of large-scale knowledge bases (KBs), such as semantic search, QA systems, the Google Knowledge Graph and IBM Watson QA System. Existing KBs mainly focus on encoding the factual facts of the world, e.g., city area and company product, which are regarded as the objective knowledge, whereas the subjective knowledge, which is frequently mentioned in Web queries, has been neglected. The subjective knowledge has no documented ground truth, instead, the truth relies on people's dominant opinion, which can be solicited from online crowd workers. In our work, we propose a KBC framework for subjective knowledge base construction taking advantage of the knowledge from the crowd and existing KBs. We develop a two-staged framework for subjective KB construction which consists of core subjective KB construction and subjective KB enrichment. Firstly, we try to build a core subjective KB mined from existing KBs, where every instance has rich objective properties. Then, we populate the core subjective KB with instances extracted from existing KBs, in which the crowd is leverage to annotate the subjective property of the instances. In order to optimize the crowd annotation process, we formulate the problem of subjective KB enrichment procedure as a cost-aware instance annotation problem and propose two instance annotation algorithms, i.e., adaptive instance annotation and batch-mode instance annotation algorithms. We develop a two-stage system for subjective KB construction which consists of core subjective KB construction and subjective knowledge enrichment. We evaluate our framework on real knowledge bases and a real crowdsourcing platform, the experimental results show that we can derive high quality subjective knowledge facts from existing KBs and crowdsourcing techniques through our proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057309752",
                    "name": "Hao Xin"
                },
                {
                    "authorId": "2087884364",
                    "name": "Rui Meng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "4c85eca01e6f13f4f3b36293cd41ab75bd0cd289",
            "title": "Jupiter: A Blockchain Platform for Mobile Devices",
            "abstract": "With the success of Bitcoin, the technique behind it, Blockchain, is catching massive attention recently. Blockchain is a collection of several techniques like cryptology, P2P and distributed consensus protocol. The main idea of Blockchain is that nodes in the network keep the same distributed ledger. Because of this immutable ledger, a trusted bridge is built among parties without fully trust. Blockchain can be used in variety of areas, especially in financial fields, like supply chain management, cross-border payment and global bank settlement. Meanwhile, we can observe that mobile network is growing rapidly and nibbling the PC market. However, the current public Blockchain applications like Bitcoin or Ethereum require the nodes to store the whole ledger which exceeds the capacity of the mobile devices. Thus, we need to develop a blockchain platform to support mobile devices. In this demo, we introduce Jupiter, a mobile-based Blockchain platform which provides a novel concept called consensus unit (CU) to alleviate the storage problem of mobile. We present the system architecture and demonstrate several CU scenarios via Jupiter.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109601286",
                    "name": "Siyuan Han"
                },
                {
                    "authorId": "80541392",
                    "name": "Zihuan Xu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "52b9180e6ec779acd772b8cd678245296ced28b1",
            "title": "Reducing Uncertainty of Schema Matching via Crowdsourcing with Accuracy Rates",
            "abstract": "Schema matching is a central challenge for data integration systems. Inspired by the popularity and the success of crowdsourcing platforms, we explore the use of crowdsourcing to reduce the uncertainty of schema matching. Since crowdsourcing platforms are most effective for simple questions, we assume that each Correspondence Correctness Question (CCQ) asks the crowd to decide whether a given correspondence should exist in the correct matching. Furthermore, members of a crowd may sometimes return incorrect answers with different probabilities. Accuracy rates of individual crowd workers can be attributes of CCQs as well as evaluations of individual workers. We prove that uncertainty reduction equals to entropy of answers minus entropy of crowds and show how to obtain lower and upper bounds for it. We propose frameworks and efficient algorithms to dynamically manage the CCQs to maximize the uncertainty reduction within a limited budget of questions. We develop two novel approaches, namely \u201cSingle CCQ\u201d and \u201cMultiple CCQ\u201d, which adaptively select, publish, and manage questions. We verify the value of our solutions with simulation and real implementation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50445897",
                    "name": "C. Zhang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "2153210268",
                    "name": "Mengchen Zhang"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                }
            ]
        },
        {
            "paperId": "70912aecc5e4edd517263b7092eaf8d73e16f116",
            "title": "Dynamic Pricing in Spatial Crowdsourcing: A Matching-Based Approach",
            "abstract": "In spatial crowdsourcing, requesters submit their task-related locations and increase the demand of a local area. The platform prices these tasks and assigns spatial workers to serve if the prices are accepted by requesters. There exist mature pricing strategies which specialize in tackling the imbalance between supply and demand in a local market. However, in global optimization, the platform should consider the mobility of workers; that is, any single worker can be the potential supply for several areas, while it can only be the true supply of one area when assigned by the platform. The hardness lies in the uncertainty of the true supply of each area, hence the existing pricing strategies do not work. In the paper, we formally define this Global Dynamic Pricing(GDP) problem in spatial crowdsourcing. And since the objective is concerned with how the platform matches the supply to areas, we let the matching algorithm guide us how to price. We propose a MAtching-based Pricing Strategy (MAPS) with guaranteed bound. Extensive experiments conducted on the synthetic and real datasets demonstrate the effectiveness of MAPS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "2108532674",
                    "name": "Libin Wang"
                },
                {
                    "authorId": "2735715",
                    "name": "Zimu Zhou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2525530",
                    "name": "Bowen Du"
                },
                {
                    "authorId": "144030870",
                    "name": "Jieping Ye"
                }
            ]
        },
        {
            "paperId": "7ac1e9d3f56ec67f521018f44ddb2a0c39f0f877",
            "title": "Time-Aware Boolean Spatial Keyword Queries (Extended Abstract)",
            "abstract": "This paper explores the Time-Aware Boolean Spatial Keyword Query (TABSKQ) that finds the geo-tagged objects satisfying user's spatial, textual, and temporal constraints. Towards this, we propose an efficient index structure so-called the TA-tree and its corresponding algorithms, which can efficiently prune the search space using both spatio-temporal and textual information. Extensive experiments with real datasets offer insight into the performance of our proposed index and algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46965289",
                    "name": "Gang Chen"
                },
                {
                    "authorId": "49530802",
                    "name": "Jingwen Zhao"
                },
                {
                    "authorId": "1409828392",
                    "name": "Yunjun Gao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144313209",
                    "name": "R. Chen"
                }
            ]
        },
        {
            "paperId": "990ba5c799a413f7c083e4139c877cc9fe780a7e",
            "title": "Fast Parallel Path Concatenation for Graph Extraction",
            "abstract": "In this paper, we study the problem of extracting a homogeneous graph from a heterogeneous graph. The key challenges of the extraction problem are how to efficiently enumerate paths matched by the provided line pattern and aggregate values for each pair of vertices from the matched paths. To address above two challenges, we propose a parallel a graph extraction framework (PGE), where we use vertex-centric model to enumerate paths and compute aggregate functions in parallel. The framework compiles the line pattern into a path concatenation plan and generates the final weighted edges in a divide-and-conquer manner. The new solution outperforms the state-of-the-art ones through the comprehensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237813",
                    "name": "Yingxia Shao"
                },
                {
                    "authorId": "145558284",
                    "name": "Kai Lei"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145622169",
                    "name": "Zi Huang"
                },
                {
                    "authorId": "144585959",
                    "name": "B. Cui"
                },
                {
                    "authorId": "2145285454",
                    "name": "Zhongyi Liu"
                },
                {
                    "authorId": "8230405",
                    "name": "Yunhai Tong"
                },
                {
                    "authorId": "2110639333",
                    "name": "Jin Xu"
                }
            ]
        },
        {
            "paperId": "9d15aad71b403f34480c39b1767f5f2b000eedd2",
            "title": "An Experimental Evaluation of Task Assignment in Spatial Crowdsourcing",
            "abstract": "Recently, with the rapid development of mobile devices and the crowdsourcing platforms, the spatial crowdsourcing has attracted much attention from the database community. Specifically, spatial crowdsourcing refers to sending a location-based request to workers according to their positions, and workers need to physically move to specified locations to conduct tasks. Many works have studied task assignment problems in spatial crowdsourcing, however, their problem settings are different from each other. Thus, it is hard to compare the performances of existing algorithms on task assignment in spatial crowdsourcing. In this paper, we present a comprehensive experimental comparison of most existing algorithms on task assignment in spatial crowdsourcing. Specifically, we first give general definitions about spatial workers and spatial tasks based on definitions in the existing works such that the existing algorithms can be applied on the same synthetic and real data sets. Then, we provide a uniform implementation for all the tested algorithms of task assignment problems in spatial crowdsourcing (open sourced). Finally, based on the results on both synthetic and real data sets, we discuss the strengths and weaknesses of tested algorithms, which can guide future research on the same area and practical implementations of spatial crowdsourcing systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "2054880853",
                    "name": "Xun Jian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "9e024fdd7841f8faec85ca11c7d5e24ad7551aa6",
            "title": "Multi-Campaign Oriented Spatial Crowdsourcing",
            "abstract": "The system throughput and workers' travel distance are two important factors in spatial crowdsourcing, and improving one of them usually means sacrificing the other. However, existing works either fail to consider the trade-off between these two factors or resolve their conflicts by simply targeting tasks within a bounding circle for each worker. In this paper, we compromise between the throughput and the distance by formulating these two factors as score terms in the objective function. Apart from that, we study the multi-campaign scenario in our problem, which is not uncommon in practical applications while not yet discussed in existing works. The worker diversity of the campaigns is formulated as another score term in the objective function. The problem of multi-campaign oriented spatial crowdsourcing is to maximize the aforementioned score function. We prove the problem is NP-hard and provide several approximation solutions. Extensive experiments have been conducted to validate the devised solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49537866",
                    "name": "Libin Zheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "b70540fe0ccad461d808f1fd8382db8cf917adc2",
            "title": "Domain-Aware Multi-Truth Discovery from Conflicting Sources",
            "abstract": "In the Big Data era, truth discovery has served as a promising technique to solve con\ufb02icts in the facts provided by numerous data sources. The most signi\ufb01cant challenge for this task is to estimate source reliability and select the answers supported by high quality sources. However, existing works assume that one data source has the same reliability on any kinds of entity, ignoring the possibility that a source may vary in reliability on different domains. To capture the in\ufb02uence of various levels of expertise in different domains, we integrate domain expertise knowledge to achieve a more precise estimation of source reliability. We propose to infer the domain expertise of a data source based on its data richness in different domains. We also study the mutual in\ufb02uence between domains, which will affect the inference of domain expertise. Through leveraging the unique features of the multi-truth problem that sources may provide partially correct values of a data item, we assign more reasonable con\ufb01dence scores to value sets. We propose an integrated Bayesian approach to incorporate the domain expertise of data sources and con\ufb01dence scores of value sets, aiming to \ufb01nd multiple possible truths without any supervision. Experimental re-sults on two real-world datasets demonstrate the feasibility, ef\ufb01-ciency and effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2893899",
                    "name": "Xueling Lin"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "baa787136af6a165473d50aa5a035691210cf856",
            "title": "Efficient SimRank Tracking in Dynamic Graphs",
            "abstract": "SimRank is a popular link-based similarity measurement among nodes in a graph. To compute the all-pairs SimRank matrix accurately, iterative methods are usually used. For static graphs, current iterative solutions are not efficient enough, both in time and space, due to unnecessary cost and storage by the nature of iterative updating. For dynamic graphs, all current incremental solutions for updating the Sim-Rank matrix are based on an approximated SimRank definition, and thus have no accuracy guarantee. In this paper, we propose a novel local push based algorithm for computing all-pairs SimRank. We show that our algorithms outperform the state-of-the-art static and dynamic all-pairs SimRank algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118461742",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "beade097ff41c62a8d8d29065be0e1339be39f30",
            "title": "NSCaching: Simple and Efficient Negative Sampling for Knowledge Graph Embedding",
            "abstract": "Knowledge graph (KG) embedding is a fundamental problem in data mining research with many real-world applications. It aims to encode the entities and relations in the graph into low dimensional vector space, which can be used for subsequent algorithms. Negative sampling, which samples negative triplets from non-observed ones in the training data, is an important step in KG embedding. Recently, generative adversarial network (GAN), has been introduced in negative sampling. By sampling negative triplets with large scores, these methods avoid the problem of vanishing gradient and thus obtain better performance. However, using GAN makes the original model more complex and harder to train, where reinforcement learning must be used. In this paper, motivated by the observation that negative triplets with large scores are important but rare, we propose to directly keep track of them with cache. However, how to sample from and update the cache are two important questions. We carefully design the solutions, which are not only efficient but also achieve good balance between exploration and exploitation. In this way, our method acts as a \"distilled\" version of previous GAN-based methods, which does not waste training time on additional parameters to fit the full distribution of negative triplets. The extensive experiments show that our method can gain significant improvement on various KG embedding models, and outperform the state-of-the-arts negative sampling methods based on GAN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48379419",
                    "name": "Yongqi Zhang"
                },
                {
                    "authorId": "3259992",
                    "name": "Quanming Yao"
                },
                {
                    "authorId": "2237813",
                    "name": "Yingxia Shao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "c5967549d4f8d3aefe958074d4e316c548972c00",
            "title": "CUB, a Consensus Unit-Based Storage Scheme for Blockchain System",
            "abstract": "Recently, Blockchain becomes a hot research topic due to the success of Blockchain in many applications, such as cryptocurrency, smart contract, digital assets, distributed cloud storage and so on. The power of Blockchain is that it can achieve the consensus of an ordered set of transactions among nodes which do not trust each other, even with the existence of malicious nodes. However, compared to traditional databases, the current Blockchain technology still cannot handle a massive number of transactions, which is caused by many factors, such as the consensus protocol, structure of the blocks and storage challenge. Among them, the high storage requirement is a key factor that prevents the wide usage of Blockchain on various devices such as mobile phones or low-end PCs. In this paper, to address the storage challenge, we introduce a novel concept called Consensus Unit (CU), which organizes different nodes into one unit and lets them to store at least one copy of Blockchain data in the system together. Based on this idea, we further define the Blocks Assignment Optimization (BAO) problem which determines the optimal assignment of blocks such that the storage space is fully used and the query cost is minimized. We prove that the BAO problem is NP-hard. Thus, we propose three efficient heuristic algorithms to solve the static assignment problem. Furthermore, we present solutions to address the dynamic scenarios when new blocks arrive and nodes join or depart from the CU. To verify the effectiveness of CU, we have conducted extensive experiments on synthetic data and BLOCKBENCH [1]. The results have confirmed the superiority of CU in saving the storage and maintaining the system throughput.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "80541392",
                    "name": "Zihuan Xu"
                },
                {
                    "authorId": "2109601286",
                    "name": "Siyuan Han"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "cb93268ff41588dd47e50d2ca7deaf3a10bd2ef3",
            "title": "On Efficiently Detecting Overlapping Communities over Distributed Dynamic Graphs",
            "abstract": "Modern networks are of huge sizes as well as high dynamics, which challenges the efficiency of community detection algorithms. In this paper, we study the problem of overlapping community detection on distributed and dynamic graphs. Given a distributed, undirected and unweighted graph, the goal is to detect overlapping communities incrementally as the graph is dynamically changing. We propose an efficient algorithm, called randomized Speaker-Listener Label Propagation Algorithm (rSLPA), based on the Speaker-Listener Label Propagation Algorithm (SLPA) by relaxing the probability distribution of label propagation. Besides detecting high-quality communities, rSLPA can incrementally update the detected communities after a batch of edge insertion and deletion operations. To the best of our knowledge, rSLPA is the first algorithm that can incrementally capture the same communities as those obtained by applying the detection algorithm from the scratch on the updated graph. Extensive experiments are conducted on both synthetic and real-world datasets, and the results show that our algorithm can achieve high accuracy and efficiency at the same time.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2054880853",
                    "name": "Xun Jian"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "da7595f1e5b9316cf42f26afde3b140d8f74fe49",
            "title": "SLADE: A Smart Large-Scale Task Decomposer in Crowdsourcing",
            "abstract": "Crowdsourcing has been shown to be effective in a wide range of applications, and is seeing increasing use. A large-scale crowdsourcing task often consists of thousands or millions of atomic tasks, each of which is usually a simple task such as binary choice or simple voting. To distribute a large-scale crowdsourcing task to limited crowd workers, a common practice is to pack a set of atomic tasks into a task bin and send to a crowd worker in a batch. It is challenging to decompose a large-scale crowdsourcing task and execute batches of atomic tasks, which ensures reliable answers at a minimal total cost. Large batches lead to unreliable answers of atomic tasks, while small batches incur unnecessary cost. In this paper, we investigate a general crowdsourcing task decomposition problem, called the Smart Large-scAle task DE composer (SLADE) problem, which aims to decompose a large-scale crowdsourcing task to achieve the desired reliability at a minimal cost. We prove the NP-hardness of the SLADE problem and propose solutions in both  homogeneous and heterogeneous scenarios. For the homogeneous SLADE problem, where all the atomic tasks share the same reliability requirement, we propose a greedy heuristic algorithm and an efficient and effective approximation framework using an optimal priority queue (OPQ) structure with provable approximation ratio. For the heterogeneous SLADE problem, where the atomic tasks can have different reliability requirements, we extend the OPQ-based framework leveraging a partition strategy, and also prove its approximation guarantee. Finally, we verify the effectiveness and efficiency of the proposed solutions through extensive experiments on representative crowdsourcing platforms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2735715",
                    "name": "Zimu Zhou"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "144128216",
                    "name": "Lidan Shou"
                },
                {
                    "authorId": "2722757",
                    "name": "Weifeng Lv"
                }
            ]
        },
        {
            "paperId": "e0953db8a235c1f033b6d1408ef734128856bc48",
            "title": "Order Dispatch in Price-aware Ridesharing",
            "abstract": "With the prevalence of car-hailing applications, ridesharing becomes more and more popular because of its great potential in monetary saving and environmental protection. Order dispatch is the key problem in ridesharing, which has a strong impact on riders' experience and platform's performance. Existing order dispatch research works fail to consider the price of the orders, which can be an important reference because it directly relates to the platform's profit. Our work takes the order price into concern, and formulates a constrained optimization problem, which takes platform's profit as the optimization objective and performs controls on riders' detour distance and waiting time. We prove the problem is NP-hard, thus, we propose approximation methods. We further develop a simulation framework based on real ridesharing order and vehicle data. We conduct experiments with this simulation framework to evaluate the effectiveness and efficiency of the proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49537866",
                    "name": "Libin Zheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144030870",
                    "name": "Jieping Ye"
                }
            ]
        },
        {
            "paperId": "e3564cc621b4612af9e6fcb3164dee7d3f68d795",
            "title": "Knowledge Base Enhancement via Data Facts and Crowdsourcing",
            "abstract": "Recently, knowledge base systems such as Freebase, YAGO, etc. have been designed and widely applied while most of the knowledge bases are far from being of a high quality. According to the recent researches, the low quality is mainly caused by the loss and low accuracy of the RDF triples, which are the main components of knowledge base systems. In this paper, we propose approaches to enhance the RDF triples in knowledge bases, which is significant for providing good information retrieval service. Specifically, we utilize data facts stored in database systems to obtain possible updates for knowledge bases. Furthermore, inspired by the popular and successful applications of crowdsourcing platforms, we explore the use of crowdsourcing to verify the updates. We propose KD graph to model the possible updates and design a comprehensive framework for knowledge base enhancement problem. Since crowdsourcing employs human power and requires expenditure, we propose an optimal and dynamic method to select candidates for crowdsourcing within a limited budget so that the benefit of enhancing the knowledge base can be maximized. To reduce the time cost, we adopt split techniques and design Simple Split(SS) and Dynamic Split(DS) algorithms. We verify the effectiveness of our solutions by conducting crowdsourcing simulation experiments and experiments on a crowdsourcing platform namely gMission.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047907712",
                    "name": "Lin-Bo Jiang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2111605555",
                    "name": "Zhao Chen"
                }
            ]
        },
        {
            "paperId": "e6f36e5db4de408fbbf29c39c94c16d51382b19a",
            "title": "Context-aware Academic Collaborator Recommendation",
            "abstract": "Collaborator Recommendation is a useful application in exploiting big academic data. However, existing works leave out the contextual restriction (i.e., research topics) of people's academic collaboration, thus cannot recommend suitable collaborators for the required research topics. In this work, we propose Context-aware Collaborator Recommendation (CACR), which aims to recommend high-potential new collaborators for people's context-restricted requests. To this end, we design a novel recommendation framework, which consists of two fundamental components: the Collaborative Entity Embedding network (CEE) and the Hierarchical Factorization Model (HFM). In particular, CEE jointly represents researchers and research topics as compact vectors based on their co-occurrence relationships, whereby capturing researchers' context-aware collaboration tendencies and topics' underlying semantics. Meanwhile, HFM extracts researchers' activenesses and conservativenesses, which reflect their intensities of making academic collaborations and tendencies of working with non-collaborated fellows. The extracted activenesses and conservativenesses work collaboratively with the context-aware collaboration tendencies, such that high-quality recommendation can be produced. Extensive experimental studies are conducted with large-scale academic data, whose results verify the effectiveness of our proposed approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145976421",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "144076239",
                    "name": "Xing Xie"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "e70ab345eaa08e0c51b85098ae1d51245705be82",
            "title": "Rule-Based Graph Repairing: Semantic and Efficient Repairing Methods",
            "abstract": "Real-life graph datasets extracted from Web are inevitably full of incompleteness, conflicts, and redundancies, so graph data cleaning shows its necessity. One of the main issues is to automatically repair the graph with some repairing rules. Although rules like data dependencies have been widely studied in relational data repairing, very few works exist to repair the graph data. In this paper, we introduce an automatic repairing semantic for graphs, called Graph-Repairing Rules (GRRs). This semantic can capture the incompleteness, conflicts, and redundancies in the graphs and indicate how to correct these errors. We study three fundamental problems associated with GRRs, implication, consistency and termination, which show whether a given set of GRRs make sense. Repairing the graph data using GRRs involves a problem of finding isomorphic subgraphs of the graph data for each GRR, which is NP-complete. To efficiently circumvent the complex calculation of subgraph isomorphism, we design a decomposition-and-join strategy to solve this problem. Extensive experiments on real datasets show that our GRR semantic and corresponding repairing algorithms can effectively and efficiently repair real-life graph data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2329311",
                    "name": "Yurong Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                }
            ]
        },
        {
            "paperId": "edc2cdc0e7749e5c8a12e9222427297128fadf1b",
            "title": "On Uncertain Graphs",
            "abstract": "Abstract Large-scale, highly interconnected networks, which are often modeled as graphs, pervade both our society and the natural world around us. Uncertainty, on the other hand, is inherent in the...",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108514592",
                    "name": "Arijit Khan"
                },
                {
                    "authorId": "144568261",
                    "name": "Yuan Ye"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "f5d39ee0d20d6101af58e5f50ce657758b9e9158",
            "title": "A Comparative Study of Consistent Snapshot Algorithms for Main-Memory Database Systems",
            "abstract": "In-memory databases (IMDBs) are gaining increasing popularity in big data applications, where clients commit updates intensively. Specifically, it is necessary for IMDBs to have efficient snapshot performance to support certain special applications (e.g., consistent checkpoint, HTAP). Formally, the in-memory consistent snapshot problem refers to taking an in-memory consistent time-in-point snapshot with the constraints that 1) clients can read the latest data items and 2) any data item in the snapshot should not be overwritten. Various snapshot algorithms have been proposed in academia to trade off throughput and latency, but industrial IMDBs such as Redis adhere to the simple fork algorithm. To understand this phenomenon, we conduct comprehensive performance evaluations on mainstream snapshot algorithms. Surprisingly, we observe that the simple fork algorithm indeed outperforms the state-of-the-arts in update-intensive workload scenarios. On this basis, we identify the drawbacks of existing research and propose two lightweight improvements. Extensive evaluations on synthetic data and Redis show that our lightweight improvements yield better performance than fork, the current industrial standard, and the representative snapshot algorithms from academia. Finally, we have opensourced the implementation of all the above snapshot algorithms so that practitioners are able to benchmark the performance of each algorithm and select proper methods for different application scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2154884023",
                    "name": "Liang Li"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "2108244645",
                    "name": "Gang Wu"
                },
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                }
            ]
        },
        {
            "paperId": "fa192e467393f83294f1c24f4d20c92fad4455b2",
            "title": "Narrowband Internet of Things: Evolutions, Technologies, and Open Issues",
            "abstract": "We are on the threshold of the explosive growth in the global Internet-of-Things (IoT) market. Comparing with the legacy human-centric applications, machine type communication scenarios exhibit totally different characteristics, such as low throughput, delay insensitivity, occasional transmission, and deep coverage. Meanwhile, it also requires the terminal devices to be cheap enough and sustain long battery life. These demands hasten the prosperity of low power wide area (LPWA) technologies. Narrowband IoT (NB-IoT) is the newest Long Term Evolution (LTE) specification ratified by the third generation partner project as one of the LPWA solutions to achieve the objectives of super coverage, low power, low cost, and massive connection. Working in the licensed frequency band, it is designed to reuse and coexist with the existing LTE cellular networks, which endows it with outstanding advantages in decreasing network installation cost and minimizing product time-to-market. In this backdrop, it has been extensively regarded as one of the most promising technologies toward the IoT landscape. However, as a new LTE standard, there are still a lot of challenges that need to overcome. This paper surveys its evolutions, technologies, and issues, spanning from performance analysis, design optimization, combination with other leading technologies, to implementation and application. The goal is to deliver a holistic understanding for the emerging wireless communication system, in helping to spur further research in accelerating the broad use of NB-IoT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150635915",
                    "name": "Jun Xu"
                },
                {
                    "authorId": "2225242",
                    "name": "Junmei Yao"
                },
                {
                    "authorId": "50815748",
                    "name": "Lu Wang"
                },
                {
                    "authorId": "144039153",
                    "name": "Zhong Ming"
                },
                {
                    "authorId": "8584850",
                    "name": "Kaishun Wu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "ff6168235c18f4bec0ac49bc81d0d74135d13b62",
            "title": "A Unified Approach to Route Planning for Shared Mobility",
            "abstract": "\n There has been a dramatic growth of shared mobility applications such as ride-sharing, food delivery and crowdsourced parcel delivery. Shared mobility refers to transportation services that are shared among users, where a central issue is\n route planning\n . Given a set of workers and requests, route planning finds for each worker a route,\n i.e.\n , a sequence of locations to pick up and drop off passengers/parcels that arrive from time to time, with different optimization objectives. Previous studies lack practicability due to their conflicted objectives and inefficiency in inserting a new request into a route, a basic operation called\n insertion\n . In this paper, we present a unified formulation of route planning called URPSM. It has a well-defined parameterized objective function which eliminates the contradicted objectives in previous studies and enables flexible multi-objective route planning for shared mobility. We prove the problem is NP-hard and there is no polynomial-time algorithm with constant competitive ratio for the URPSM problem and its variants. In response, we devise an effective and efficient solution to address the URPSM problem approximately. We design a novel dynamic programming (DP) algorithm to accelerate the insertion operation from cubic or quadric time in previous work to only linear time. On basis of the DP algorithm, we propose a greedy based solution to the URPSM problem. Experimental results on real datasets show that our solution outperforms the state-of-the-arts by 1.2 to 12.8 times in effectiveness, and also runs 2.6 to 20.7 times faster.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "47211830",
                    "name": "Yuxiang Zeng"
                },
                {
                    "authorId": "2735715",
                    "name": "Zimu Zhou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144030870",
                    "name": "Jieping Ye"
                },
                {
                    "authorId": "145389711",
                    "name": "Ke Xu"
                }
            ]
        },
        {
            "paperId": "0acaebd7bb5428506024fe95a91c1a79c5e8d8a0",
            "title": "Maximizing Acceptance in Rejection-Aware Spatial Crowdsourcing",
            "abstract": "With the rapid development of mobile networks and the widespread usage of mobile devices, spatial crowdsourcing, which refers to assigning location-based tasks to moving workers, has drawn increasing attention. One of the important issues in spatial crowdsourcing is task assignment, which allocates tasks to appropriate workers. However, existing works generally assume that no rejection would happen after the task assignment is completed by the server. Ignorance of such an operation can lead to low system throughput. Thus, in this paper, we take workers\u2019 rejection into consideration and try to maximize workers\u2019 acceptance in order to improve the system throughput. Specifically, we first formally define the problem of maximizing workers\u2019 acceptance in rejection-aware spatial crowdsourcing. Unfortunately, the problem is NP-hard. We propose two exact solutions to obtain the optimal assignment, but they are not efficient enough and not scalable for large inputs. Then, we present four approximation approaches for improving the efficiency. Finally, we show the effectiveness of the proposed pruning strategy for the exact solutions and the superiority of the proposed Greedy algorithm over other approximation methods through extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49537866",
                    "name": "Libin Zheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "3162f956caffd1751dfdda1ddef546b588f6393b",
            "title": "Feedback-Aware Social Event-Participant Arrangement",
            "abstract": "Online event-based social networks (EBSNs) and studies on global event-participant arrangement strategies for EBSNs are becoming popular recently. Existing works measure satisfaction of an arrangement by a linear combination of few factors, weights of which are predefined and fixed, and do not allow users to provide feedbacks on whether accepting the arrangement or not. Besides, most of them only consider offline scenarios, where full information of users is known in advance. However, on real-world EBSN platforms, users can dynamically log in the platform and register for events on a first come, first served basis. In other words, online scenarios of event-participant arrangement strategies should be considered. In this work, we study a new event-participant arrangement strategy for online scenarios, the Feedback-Aware Social Event-participant Arrangement (FASEA) problem, where satisfaction scores of an arrangement are learned adaptively and users can choose to accept or reject the arranged events. Particularly, we model the problem as a contextual combinatorial bandit setting and use efficient and effective algorithms to solve the problem. The effectiveness and efficiency of the solutions are evaluated with extensive experimental studies and our findings indicate that the state-of-the-art Thompson Sampling that is reported to work well under basic multi-armed bandit does not perform well under FASEA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1872111",
                    "name": "Jieying She"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "3406090",
                    "name": "Tianshu Song"
                }
            ]
        },
        {
            "paperId": "36064454e7ea990333a4635ddc96f9a4340760d7",
            "title": "Complex Event-Participant Planning and Its Incremental Variant",
            "abstract": "In recent years, online Event Based Social Network (EBSN) platforms have become increasingly popular. One typical task of EBSN platforms is to help users make suitable and personalized plans for participating in different interesting social events. Existing techniques either ignore the minimumparticipant requirement constraint for each event, which is crucially needed for some events to be held successfully, or assume that events would not change once announced. In this paper, we address the above inadequacies of existing EBSN techniques. We formally define the Global Event Planning with Constraints (GEPC) problem, and its incremental variant. We prove that both are NP-hard, and provide approximate solutions. Finally, we verify the effectiveness and efficiency of our proposed algorithms through extensive experiments over real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2329311",
                    "name": "Yurong Cheng"
                },
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1398517004",
                    "name": "C. Giraud-Carrier"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                }
            ]
        },
        {
            "paperId": "41f42dd7d784fca159338ce263501acfd0b38f4b",
            "title": "Discovering Conditional Matching Rules",
            "abstract": "Matching dependencies (MDs) have recently been proposed to make data dependencies tolerant to various information representations, and found useful in data quality applications such as record matching. Instead of the strict equality function used in traditional dependency syntax (e.g., functional dependencies), MDs specify constraints based on similarity and identification. However, in practice, MDs may still be too strict and applicable only in a subset of tuples in a relation. Thereby, we study the conditional matching dependencies (CMDs), which bind matching dependencies only in a certain part of a table, i.e., MDs conditionally applicable in a subset of tuples. Compared to MDs, CMDs have more expressive power that enables them to satisfy wider application needs. In this article, we study several important theoretical and practical issues of CMDs, including irreducible CMDs with respect to the implication, discovery of CMDs from data, reliable CMDs agreed most by a relation, approximate CMDs almost satisfied in a relation, and finally applications of CMDs in record matching and missing value repairing. Through an extensive experimental evaluation in real data sets, we demonstrate the efficiency of proposed CMDs discovery algorithms and effectiveness of CMDs in real applications.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108927961",
                    "name": "Yihan Wang"
                },
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                },
                {
                    "authorId": "144488755",
                    "name": "Hong Cheng"
                }
            ]
        },
        {
            "paperId": "4e345d9f4a2fd04829ba80927b94cdce6c068d6d",
            "title": "Efficient SimRank-Based Similarity Join",
            "abstract": "Graphs have been widely used to model complex data in many real-world applications. Answering vertex join queries over large graphs is meaningful and interesting, which can benefit friend recommendation in social networks and link prediction, and so on. In this article, we adopt \u201cSimRank\u201d [13] to evaluate the similarity between two vertices in a large graph because of its generality. Note that \u201cSimank\u201d is purely structure dependent, and it does not rely on the domain knowledge. Specifically, we define a SimRank-based join (SRJ) query to find all vertex pairs satisfying the threshold from two sets of vertices U and V. To reduce the search space, we propose a shortest-path-distance-based upper bound for SimRank scores to prune unpromising vertex pairs. In the verification, we propose a novel index, called h-go cover+, to efficiently compute the SimRank score of any single vertex pair. Given a graph G, we only materialize the SimRank scores of a small proportion of vertex pairs (i.e., the h-go cover + vertex pairs), based on which the SimRank score of any vertex pair can be computed easily. To find the h-go cover + vertex pairs, we propose an efficient method without building the vertex-pair graph. Hence, large graphs can be dealt with easily. Extensive experiments over both real and synthetic datasets confirm the efficiency of our solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1807168",
                    "name": "Weiguo Zheng"
                },
                {
                    "authorId": "144554647",
                    "name": "Lei Zou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144060462",
                    "name": "Dongyan Zhao"
                }
            ]
        },
        {
            "paperId": "4fa811c9623e493ae9345e20ba30979d1046cc8e",
            "title": "Fast Parallel Path Concatenation for Graph Extraction",
            "abstract": "Heterogeneous graph is a popular data model to represent the real-world relations with abundant semantics. To analyze heterogeneous graphs, an important step is extracting homogeneous graphs from the heterogeneous graphs, called homogeneous graph extraction. In an extracted homogeneous graph, the relation is defined by a line pattern on the heterogeneous graph and the new attribute values of the relation are calculated by user-defined aggregate functions. The key challenges of the extraction problem are how to efficiently enumerate paths matched by the line pattern and aggregate values for each pair of vertices from the matched paths. To address above two challenges, we propose a parallel graph extraction framework, where we use vertex-centric model to enumerate paths and compute aggregate functions in parallel. The framework compiles the line pattern into a path concatenation plan, which determines the order of concatenating paths and generates the final paths in a divide-and-conquer manner. We introduce a cost model to estimate the cost of a plan and discuss three plan selection strategies, among which the best plan can enumerate paths in <inline-formula><tex-math notation=\"LaTeX\">$\\mathcal {O}(log(l))$</tex-math><alternatives> <inline-graphic xlink:href=\"huang-ieq1-2716939.gif\"/></alternatives></inline-formula> iterations, where <inline-formula> <tex-math notation=\"LaTeX\">$l$</tex-math><alternatives><inline-graphic xlink:href=\"huang-ieq2-2716939.gif\"/> </alternatives></inline-formula> is the length of a pattern. Furthermore, to improve the performance of evaluating aggregate functions, we classify the aggregate functions into three categories, i.e., distributive aggregation, algebraic aggregation, and holistic aggregation. Since the distributive and algebraic aggregations can be computed from the partial paths, we speed up the aggregation by computing partial aggregate values during the path enumeration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237813",
                    "name": "Yingxia Shao"
                },
                {
                    "authorId": "145558284",
                    "name": "Kai Lei"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145622169",
                    "name": "Zi Huang"
                },
                {
                    "authorId": "144585959",
                    "name": "B. Cui"
                },
                {
                    "authorId": "2145285454",
                    "name": "Zhongyi Liu"
                },
                {
                    "authorId": "8230405",
                    "name": "Yunhai Tong"
                },
                {
                    "authorId": "2110639333",
                    "name": "Jin Xu"
                }
            ]
        },
        {
            "paperId": "5796a0462ec41d1be829a73c591de605770913c8",
            "title": "Keyword Search over Distributed Graphs with Compressed Signature",
            "abstract": "Graph keyword search has drawn many research interests, since graph models can generally represent both structured and unstructured databases and keyword searches can extract valuable information for users without the knowledge of the underlying schema and query language. In practice, data graphs can be extremely large, e.g., a Web-scale graph containing billions of vertices. The state-of-the-art approaches employ centralized algorithms to process graph keyword searches, and thus they are infeasible for such large graphs, due to the limited computational power and storage space of a centralized server. To address this problem, we investigate keyword search for Web-scale graphs deployed in a distributed environment. We first give a naive search algorithm to answer the query efficiently. However, the naive search algorithm uses a flooding search strategy that incurs large time and network overhead. To remedy this shortcoming, we then propose a signature-based search algorithm. Specifically, we design a vertex signature that encodes the shortest-path distance from a vertex to any given keyword in the graph. As a result, we can find query answers by exploring fewer paths, so that the time and communication costs are low. Moreover, we reorganize the graph data in the cluster after its initial random partitioning so that the signature-based techniques are more effective. Finally, our experimental results demonstrate the feasibility of our proposed approach in performing keyword searches over Web-scale graph data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "2199045",
                    "name": "Yongjiao Sun"
                }
            ]
        },
        {
            "paperId": "6210bcdf7ea156d73efb820e439e33de81caaceb",
            "title": "Time-Aware Boolean Spatial Keyword Queries",
            "abstract": "With advances in geo-positioning technologies and mobile internet, location-based services have attracted much attention, and spatial keyword queries are catching on fast. However, as far as we aware, no prior work considers the temporal information of geo-tagged objects. Temporal information is important in the spatial keyword query because many objects are not always valid. For example, visitors may plan their trips according to the opening time of attractions. In this paper, we identify and solve a novel problem, i.e., the time-aware Boolean spatial keyword query (TABSKQ), which returns the <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives> <inline-graphic xlink:href=\"gao-ieq1-2742956.gif\"/></alternatives></inline-formula> objects that satisfy users\u2019 spatio-temporal description and textual constraint. We first present pruning strategies and algorithm based on the CIR <inline-formula><tex-math notation=\"LaTeX\">$^{+}$</tex-math><alternatives> <inline-graphic xlink:href=\"gao-ieq2-2742956.gif\"/></alternatives></inline-formula>-tree (i.e., the CIR-tree with temporal information). Then, we propose an efficient index structure, called the TA-tree, and its corresponding algorithms, which can prune the search space using both spatio-temporal and textual information. Furthermore, we study an interesting TABSKQ variant, i.e., Joint TABSKQ (JTABSKQ), which aims to process a set of TABSKQs jointly, and extend our techniques to tackle it. Extensive experiments with real datasets offer insight into the performance of our proposed indices and algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46965289",
                    "name": "Gang Chen"
                },
                {
                    "authorId": "49530802",
                    "name": "Jingwen Zhao"
                },
                {
                    "authorId": "1409828392",
                    "name": "Yunjun Gao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144313209",
                    "name": "R. Chen"
                }
            ]
        },
        {
            "paperId": "64fd61db1d4736be8f0becc3ecf3c53e620ef810",
            "title": "Deep Cross-Modal Correlation Learning for Audio and Lyrics in Music Retrieval",
            "abstract": "Deep cross-modal learning has successfully demonstrated excellent performance in cross-modal multimedia retrieval, with the aim of learning joint representations between different data modalities. Unfortunately, little research focuses on cross-modal correlation learning where temporal structures of different data modalities, such as audio and lyrics, should be taken into account. Stemming from the characteristic of temporal structures of music in nature, we are motivated to learn the deep sequential correlation between audio and lyrics. In this work, we propose a deep cross-modal correlation learning architecture involving two-branch deep neural networks for audio modality and text modality (lyrics). Data in different modalities are converted to the same canonical space where intermodal canonical correlation analysis is utilized as an objective function to calculate the similarity of temporal structures. This is the first study that uses deep architectures for learning the temporal correlation between audio and lyrics. A pretrained Doc2Vec model followed by fully connected layers is used to represent lyrics. Two significant contributions are made in the audio branch, as follows: (i) We propose an end-to-end network to learn cross-modal correlation between audio and lyrics, where feature extraction and correlation learning are simultaneously performed and joint representation is learned by considering temporal structures. (ii) And, as for feature extraction, we further represent an audio signal by a short sequence of local summaries (VGG16 features) and apply a recurrent neural network to compute a compact feature that better learns the temporal structures of music audio. Experimental results, using audio to retrieve lyrics or using lyrics to retrieve audio, verify the effectiveness of the proposed deep correlation learning architectures in cross-modal music retrieval.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2119041014",
                    "name": "Yi Yu"
                },
                {
                    "authorId": "1764527",
                    "name": "Suhua Tang"
                },
                {
                    "authorId": "2064510530",
                    "name": "F. Raposo"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "6e578d6e9531dbf0d948081fe109df9b254ad4c4",
            "title": "The Simpler The Better: A Unified Approach to Predicting Original Taxi Demands based on Large-Scale Online Platforms",
            "abstract": "Taxi-calling apps are gaining increasing popularity for their efficiency in dispatching idle taxis to passengers in need. To precisely balance the supply and the demand of taxis, online taxicab platforms need to predict the Unit Original Taxi Demand (UOTD), which refers to the number of taxi-calling requirements submitted per unit time (e.g., every hour) and per unit region (e.g., each POI). Predicting UOTD is non-trivial for large-scale industrial online taxicab platforms because both accuracy and flexibility are essential. Complex non-linear models such as GBRT and deep learning are generally accurate, yet require labor-intensive model redesign after scenario changes (e.g., extra constraints due to new regulations). To accurately predict UOTD while remaining flexible to scenario changes, we propose LinUOTD, a unified linear regression model with more than 200 million dimensions of features. The simple model structure eliminates the need of repeated model redesign, while the high-dimensional features contribute to accurate UOTD prediction. We further design a series of optimization techniques for efficient model training and updating. Evaluations on two large-scale datasets from an industrial online taxicab platform verify that LinUOTD outperforms popular non-linear models in accuracy. We envision our experiences to adopt simple linear models with high-dimensional features in UOTD prediction as a pilot study and can shed insights upon other industrial large-scale spatio-temporal prediction problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "2756531",
                    "name": "Yuqiang Chen"
                },
                {
                    "authorId": "2735715",
                    "name": "Zimu Zhou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2146043456",
                    "name": "Jie Wang"
                },
                {
                    "authorId": "152290618",
                    "name": "Qiang Yang"
                },
                {
                    "authorId": "144030870",
                    "name": "Jieping Ye"
                },
                {
                    "authorId": "2722757",
                    "name": "Weifeng Lv"
                }
            ]
        },
        {
            "paperId": "8f2e0440ab4d1d9ebad2b7d4103b6609a93a92b9",
            "title": "Utility-Aware Ridesharing on Road Networks",
            "abstract": "Ridesharing enables drivers to share any empty seats in their vehicles with riders to improve the efficiency of transportation for the benefit of both drivers and riders. Different from existing studies in ridesharing that focus on minimizing the travel costs of vehicles, we consider that the satisfaction of riders (the utility values) is more important nowadays. Thus, we formulate the problem of utility-aware ridesharing on road networks (URR) with the goal of providing the optimal rider schedules for vehicles to maximize the overall utility, subject to spatial-temporal and capacity constraints. To assign a new rider to a given vehicle, we propose an efficient algorithm with a minimum increase in travel cost without reordering the existing schedule of the vehicle. We prove that the URR problem is NP-hard by reducing it from the 0-1 Knapsack problem and it is unlikely to be approximated within any constant factor in polynomial time through a reduction from the DENS k-SUBGRAPH problem. Therefore, we propose three efficient approximate algorithms, including a bilateral arrangement algorithm, an efficient greedy algorithm and a grouping-based scheduling algorithm, to assign riders to suitable vehicles with a high overall utility. Through extensive experiments, we demonstrate the efficiency and effectiveness of our URR approaches on both real and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112624793",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "2057309752",
                    "name": "Hao Xin"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "92eb95cfca38c97e4a7cac2813191a48a4c0d127",
            "title": "Spatial Crowdsourcing: Challenges, Techniques, and Applications",
            "abstract": "Crowdsourcing is a new computing paradigm where humans are actively enrolled to participate in the procedure of computing, especially for tasks that are intrinsically easier for humans than for computers. The popularity of mobile computing and sharing economy has extended conventional web-based crowdsourcing to spatial crowdsourcing (SC), where spatial data such as location, mobility and the associated contextual information, plays a central role. In fact, spatial crowdsourcing has stimulated a series of recent industrial successes including Citizen Sensing (Waze), P2P ride-sharing (Uber) and Real-time Online-To-Offline (O2O) services (Instacart and Postmates). \n \nIn this tutorial, we review the paradigm shift from web-based crowdsourcing to spatial crowdsourcing. We dive deep into the challenges and techniques brought by the unique spatio-temporal characteristics of spatial crowdsourcing. Particularly, we survey new designs in task assignment, quality control, incentive mechanism design and privacy protection on spatial crowdsourcing platforms, as well as the new trend to incorporate crowdsourcing to enhance existing spatial data processing techniques. We also discuss case studies of representative spatial crowdsourcing systems and raise open questions and current challenges for the audience to easily comprehend the tutorial and to advance this important research area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1773086",
                    "name": "C. Shahabi"
                }
            ]
        },
        {
            "paperId": "a42a028b662e43e0b7790c8f0addc856557153c2",
            "title": "Trichromatic Online Matching in Real-Time Spatial Crowdsourcing",
            "abstract": "The prevalence of mobile Internet techniques and Online-To-Offline (O2O) business models has led the emergence of various spatial crowdsourcing (SC) platforms in our daily life. A core issue of SC is to assign real-time tasks to suitable crowd workers. Existing approaches usually focus on the matching of two types of objects, tasks and workers, or assume the static offline scenarios, where the spatio-temporal information of all the tasks and workers is known in advance. Recently, some new emerging O2O applications incur new challenges: SC platforms need to assign three types of objects, tasks, workers and workplaces, and support dynamic real-time online scenarios, where the existing solutions cannot handle. In this paper, based on the aforementioned challenges, we formally define a novel dynamic online task assignment problem, called the trichromatic online matching in real-time spatial crowdsourcing (TOM) problem, which is proven to be NP-hard. Thus, we first devise an efficient greedy online algorithm. However, the greedy algorithm can be trapped into local optimal solutions easily. We then present a threshold-based randomized algorithm that not only guarantees a tighter competitive ratio but also includes an adaptive optimization technique, which can quickly learn the optimal threshold for the randomized algorithm. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive experiments on real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3406090",
                    "name": "Tianshu Song"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "2108532674",
                    "name": "Libin Wang"
                },
                {
                    "authorId": "1872111",
                    "name": "Jieying She"
                },
                {
                    "authorId": "144340823",
                    "name": "Bin Yao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145389711",
                    "name": "Ke Xu"
                }
            ]
        },
        {
            "paperId": "a48dca9eda1507887da9d2cccb98df0b560d7c34",
            "title": "CrowdFusion: A Crowdsourced Approach on Data Fusion Refinement",
            "abstract": "Data fusion has played an important role in data mining because high quality data is required in a lot of applications. As on-line data may be out-of-date and errors in the data may propagate with copying and referring between sources, it is hard to achieve satisfying results with merely applying existing data fusion methods to fuse Web data. In this paper, we make use of the crowd to achieve high quality data fusion result. We design a framework selecting a set of tasks to ask crowds in order to improve the confidence of data. Since data are correlated and crowds may provide incorrect answers, how to select a proper set of tasks to ask the crowd is a very challenging problem. In this paper, we design an approximation solution to address this challenge since we prove that the problem is at NP-hard. To further improve the efficiency, we design a pruning strategy and a preprocessing method, which effectively improve the performance of the proposed approximation solution. We verify the solutions with extensive experiments on a real crowdsourcing platform.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8727612",
                    "name": "Yunfan Chen"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "50445897",
                    "name": "C. Zhang"
                }
            ]
        },
        {
            "paperId": "b1d8d7bc3da2079a46b39c4e2eb6f8417e9995cb",
            "title": "Subjective Knowledge Acquisition and Enrichment Powered By Crowdsourcing",
            "abstract": "Knowledge bases (KBs) have attracted increasing attention due to its great success in various areas, such as Web and mobile search.Existing KBs are restricted to objective factual knowledge, such as city population or fruit shape, whereas,subjective knowledge, such as big city, which is commonly mentioned in Web and mobile queries, has been neglected. Subjective knowledge differs from objective knowledge in that it has no documented or observed ground truth. Instead, the truth relies on people's dominant opinion. Thus, we can use the crowdsourcing technique to get opinion from the crowd. In our work, we propose a system, called crowdsourced subjective knowledge acquisition (CoSKA),for subjective knowledge acquisition powered by crowdsourcing and existing KBs. The acquired knowledge can be used to enrich existing KBs in the subjective dimension which bridges the gap between existing objective knowledge and subjective queries.The main challenge of CoSKA is the conflict between large scale knowledge facts and limited crowdsourcing resource. To address this challenge, in this work, we define knowledge inference rules and then select the seed knowledge judiciously for crowdsourcing to maximize the inference power under the resource constraint. Our experimental results on real knowledge base and crowdsourcing platform verify the effectiveness of CoSKA system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087884364",
                    "name": "Rui Meng"
                },
                {
                    "authorId": "2057309752",
                    "name": "Hao Xin"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1809614",
                    "name": "Yangqiu Song"
                }
            ]
        },
        {
            "paperId": "c4797053a23531e16fed0535cdf7a33bb5edfa99",
            "title": "Knowledge Base Semantic Integration Using Crowdsourcing",
            "abstract": "The semantic web has enabled the creation of a growing number of knowledge bases (KBs), which are designed independently using different techniques. Integration of KBs has attracted much attention as different KBs usually contain overlapping and complementary information. Automatic techniques for KB integration have been improved but far from perfect. Therefore, in this paper, we study the problem of knowledge base semantic integration using crowd intelligence. There are both classes and instances in a KB, in our work, we propose a novel hybrid framework for KB semantic integration considering the semantic heterogeneity of KB class structures. We first perform semantic integration of the class structures via crowdsourcing, then apply the blocking-based instance matching approach according to the integrated class structure. For class structure (taxonomy) semantic integration, the crowd is leveraged to help identifying the semantic relationships between classes to handle the semantic heterogeneity problem. Under the conditions of both large scale KBs and limited monetary budget for crowdsourcing, we formalize the class structure (taxonomy) semantic integration problem as a Local Tree Based Query Selection  (LTQS) problem. We show that the LTQS problem is NP-hard and propose two greedy-based algorithms, i.e., static query selection and adaptive query selection. Furthermore, the KBs are usually of large scales and have millions of instances, direct pairwise-based instance matching is inefficient. Therefore, we adopt the blocking-based strategy for instance matching, taking advantage of the class structure (taxonomy) integration result. The experiments on real large scale KBs verify the effectiveness and efficiency of the proposed approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087884364",
                    "name": "Rui Meng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "50445897",
                    "name": "C. Zhang"
                }
            ]
        },
        {
            "paperId": "dd191fe78d3d5fec6213d3ef6d00dee9159347f5",
            "title": "Flexible Online Task Assignment in Real-Time Spatial Data",
            "abstract": "The popularity of Online To Offline (O2O) service platforms has spurred the need for online task assignment in real-time spatial data, where streams of spatially distributed tasks and workers are matched in real time such that the total number of assigned pairs is maximized. Existing online task assignment models assume that each worker is either assigned a task immediately or waits for a subsequent task at a fixed location once she/he appears on the platform. Yet in practice a worker may actively move around rather than passively wait in place if no task is assigned. In this paper, we define a new problem F lexible T wo-sided O nline task A ssignment (FTOA). FTOA aims to guide idle workers based on the prediction of tasks and workers so as to increase the total number of assigned worker-task pairs. To address the FTOA problem, we face two challenges: (i) How to generate guidance for idle workers based on the prediction of the spatiotemporal distribution of tasks and workers? (ii) How to leverage the guidance of workers' movements to optimize the online task assignment? To this end, we propose a novel two-step framework, which integrates offline prediction and online task assignment. Specifically, we estimate the distributions of tasks and workers per time slot and per unit area, and design an online task assignment algorithm, P rediction-oriented O nline task A ssignment in R eal-time spatial data (POLAR-OP). It yields a 0.47-competitive ratio, which is nearly twice better than that of the state-of-the-art. POLAR-OP also reduces the time complexity to process each newly-arrived task/worker to O(1). We validate the effectiveness and efficiency of our methods via extensive experiments on both synthetic datasets and real-world datasets from a large-scale taxi-calling platform.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "2108532674",
                    "name": "Libin Wang"
                },
                {
                    "authorId": "2735715",
                    "name": "Zimu Zhou"
                },
                {
                    "authorId": "1696332",
                    "name": "Bolin Ding"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144030870",
                    "name": "Jieping Ye"
                },
                {
                    "authorId": "145389711",
                    "name": "Ke Xu"
                }
            ]
        },
        {
            "paperId": "e5ac13c2377043124ac4242c9f0f0fd81aacf877",
            "title": "Worker Recommendation for Crowdsourced Q&A Services: A Triple-Factor Aware Approach",
            "abstract": "W orker R ecommendation (WR) is one of the most important functions for crowdsourced Q&A services. Specifically, given a set of tasks to be solved, WR recommends each task with a certain group of workers, whom are expected to give timely answers with high qualities. To address the WR problem, recent studies have introduced a number of recommendation approaches, which take advantage of workers' expertises or preferences towards different types of tasks. However, without a thorough consideration of workers' characters, such approaches will lead to either inadequate task fulfillment or inferior answer quality. \n \nIn this work, we propose the Triple-factor Aware Worker Recommendation framework, which collectively considers workers' expertises, preferences and activenesses to maximize the overall production of high quality answers. We construct the Latent Hierarchical Factorization Model, which is able to infer the tasks' underlying categories and workers' latent characters from the historical data; and we propose a novel parameter inference method, which only requires the processing of positive instances, giving rise to significantly higher time efficiency and better inference quality. What's more, the sampling-based recommendation algorithm is developed, such that the near optimal worker recommendation can be generated for a presented batch of tasks with considerably reduced time consumption. Comprehensive experiments have been carried out using both real and synthetic datasets, whose results verify the effectiveness and efficiency of our proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145976421",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "fe91bf5831ab0179c602827a381042fd210a20a2",
            "title": "An Efficient Probabilistic Approach for Graph Similarity Search",
            "abstract": "Graph similarity search is a common and fundamental operation in graph databases. One of the most popular graph similarity measures is the Graph Edit Distance (GED) mainly because of its broad applicability and high interpretability. Despite its prevalence, exact GED computation is proved to be NP-hard, which could result in unsatisfactory computational efficiency on large graphs. However, exactly accurate search results are usually unnecessary for real-world applications especially when the responsiveness is far more important than the accuracy. Thus, in this paper, we propose a novel probabilistic approach to efficiently estimate GED, which is further leveraged for the graph similarity search. Specifically, we first take branches as elementary structures in graphs, and introduce a novel graph similarity measure by comparing branches between graphs, i.e., Graph Branch Distance (GBD), which can be efficiently calculated in polynomial time. Then, we formulate the relationship between GED and GBD by considering branch variations as the result ascribed to graph edit operations, and model this process by probabilistic approaches. By applying our model, the GED between any two graphs can be efficiently estimated by their GBD, and these estimations are finally utilized in the graph similarity search. Extensive experiments show that our approach has better accuracy, efficiency and scalability than other comparable methods in the graph similarity search over real and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145274105",
                    "name": "Zijian Li"
                },
                {
                    "authorId": "2054880853",
                    "name": "Xun Jian"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "ff6bb01ffeec684becf780ce9bd34d41c8f2897b",
            "title": "Response to \u201cDifferential Dependencies Revisited\u201d",
            "abstract": "A recent article [Vincent et al. 2015] concerns the correctness of several results in reasoning about differential dependencies (dds), originally reported in Song and Chen [2011]. The major concern by Vincent et al. [2015] roots from assuming a type of infeasible differential functions in the given dds for consistency and implication analysis, which are not allowed in Song and Chen [2011]. A differential function is said to be infeasible if there is no tuple pair with values that can satisfy the specified distance constraints. For example, [price(<2, > 4)] requires the difference of two price values to be < 2 and > 4 at the same time, which is clearly impossible. Although dds involving infeasible differential functions may be syntactically interesting, they are semantically meaningless and would neither be specified by domain experts nor discovered from data. For these reasons, infeasible differential functions are not considered [Song and Chen 2011] and the results in Song and Chen [2011] are correct, in contrast to what is claimed in Vincent et al. [2015].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "0e3dd0cf9a8d693e5bc766b0f352814727d84e0a",
            "title": "Task Assignment on Spatial Crowdsourcing (Technical Report)",
            "abstract": "Recently, with the rapid development of mobile devices and the crowdsourcing platforms, the spatial crowdsourcing has attracted much attention from the database community. Specifically, spatial crowdsourcing refers to sending a location-based request to workers according to their positions, and workers need to physically move to specified locations to conduct tasks. Many works have studied task assignment problems in spatial crowdsourcing, however, their problem definitions are quite different from each other. As a result, there is no work to compare the performances of existing algorithms on task assignment in spatial crowdsourcing. In this paper, we present a comprehensive experimental comparison of most existing algorithms on task assignment in spatial crowdsourcing. Specifically, we first give some general definitions about spatial workers and spatial tasks based on definitions in the existing works studying task assignment problems in spatial crowdsourcing such that the existing algorithms can be applied on same synthetic and real data sets. Then, we provide a uniform implementation for all the algorithms of task assignment problems in spatial crowdsourcing. Finally, based on the results on both synthetic and real data sets, we conclude the strengths and weaknesses of tested algorithms, which can guide further researches on the same area and practical implementations of spatial crowdsourcing systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "2054880853",
                    "name": "Xun Jian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "0e58abcfdc905e1eea1fe78780b544226dd5983f",
            "title": "Task Assignment on Spatial Crowdsourcing [Experiments and Analyses] (Technical Report).",
            "abstract": "Recently, with the rapid development of mobile devices and the crowdsourcing platforms, the spatial crowdsourcing has attracted much attention from the database community. Specifically, spatial crowdsourcing refers to sending a location-based request to workers according to their positions, and workers need to physically move to specified locations to conduct tasks. Many works have studied task assignment problems in spatial crowdsourcing, however, their problem settings are different from each other. Thus, it is hard to compare the performances of existing algorithms on task assignment in spatial crowdsourcing. In this paper, we present a comprehensive experimental comparison of most existing algorithms on task assignment in spatial crowdsourcing. Specifically, we first give general definitions about spatial workers and spatial tasks based on definitions in the existing works such that the existing algorithms can be applied on the same synthetic and real data sets. Then, we provide an uniform implementation for all the tested algorithms of task assignment problems in spatial crowdsourcing (open sourced). Finally, based on the results on both synthetic and real data sets, we discuss the strengths and weaknesses of tested algorithms, which can guide future research on the same area and practical implementations of spatial crowdsourcing systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "2054880853",
                    "name": "Xun Jian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "187152e1aa46e8bbece8f6b8b1fb4f20f9927e5b",
            "title": "Quality-Aware Subgraph Matching Over Inconsistent Probabilistic Graph Databases",
            "abstract": "Resource Description Framework (RDF) has been widely used in the Semantic Web to describe resources and their relationships. The RDF graph is one of the most commonly used representations for RDF data. However, in many real applications such as the data extraction/integration, RDF graphs integrated from different data sources may often contain uncertain and inconsistent information (e.g., uncertain labels or that violate facts/rules), due to the unreliability of data sources. In this paper, we formalize the RDF data by inconsistent probabilistic RDF graphs, which contain both inconsistencies and uncertainty. With such a probabilistic graph model, we focus on an important problem, quality-aware subgraph matching over inconsistent probabilistic RDF graphs (QA-gMatch), which retrieves subgraphs from inconsistent probabilistic RDF graphs that are isomorphic to a given query graph and with high quality scores (considering both consistency and uncertainty). In order to efficiently answer QA-gMatch queries, we provide two effective pruning methods, namely adaptive label pruning and quality score pruning, which can greatly filter out false alarms of subgraphs. We also design an effective index to facilitate our proposed pruning methods, and propose an efficient approach for processing QA-gMatch queries. Finally, we demonstrate the efficiency and effectiveness of our proposed approaches through extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                }
            ]
        },
        {
            "paperId": "32498aa8813d3860b65ebbff789fd747e256d56e",
            "title": "DistR: A Distributed Method for the Reachability Query over Large Uncertain Graphs",
            "abstract": "Among uncertain graph queries, reachability, i.e., the probability that one vertex is reachable from another, is likely the most fundamental one. Although this problem has been studied within the field of network reliability, solutions are implemented on a single computer and can only handle small graphs. However, as the size of graph applications continually increases, the corresponding graph data can no longer fit within a single computer's memory and must therefore be distributed across several machines. Furthermore, the computation of probabilistic reachability queries is #P-complete making it very expensive even on small graphs. In this paper, we develop an efficient distributed strategy, called DistR, to solve the problem of reachability query over large uncertain graphs. Specifically, we perform the task in two steps: distributed graph reduction and distributed consolidation. In the distributed graph reduction step, we find all of the maximal subgraphs of the original graph, whose reachability probabilities can be calculated in polynomial time, compute them and reduce the graph accordingly. After this step, only a small graph remains. In the distributed consolidation step, we transform the problem into a relational join process and provide an approximate answer to the #P-complete reachability query. Extensive experimental studies show that our distributed approach is efficient in terms of both computational and communication costs, and has high accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2329311",
                    "name": "Yurong Cheng"
                },
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "1398517004",
                    "name": "C. Giraud-Carrier"
                },
                {
                    "authorId": "2199045",
                    "name": "Yongjiao Sun"
                }
            ]
        },
        {
            "paperId": "6b7427d6669754aed97974d813940b386f628226",
            "title": "Online Minimum Matching in Real-Time Spatial Data: Experiments and Analysis",
            "abstract": "Recently, with the development of mobile Internet and smartphones, the o nline m inimum b ipartite m atching in real time spatial data (OMBM) problem becomes popular. Specifically, given a set of service providers with specific locations and a set of users who dynamically appear one by one, the OMBM problem is to find a maximum-cardinality matching with minimum total distance following that once a user appears, s/he must be immediately matched to an unmatched service provider, which cannot be revoked, before subsequent users arrive. To address this problem, existing studies mainly focus on analyzing the worst-case competitive ratios of the proposed online algorithms, but study on the performance of the algorithms in practice is absent. In this paper, we present a comprehensive experimental comparison of the representative algorithms of the OMBM problem. Particularly, we observe a surprising result that the simple and efficient greedy algorithm, which has been considered as the worst due to its exponential worst-case competitive ratio, is significantly more effective than other algorithms. We investigate the results and further show that the competitive ratio of the worst case of the greedy algorithm is actually just a constant, 3.195, in the average-case analysis. We try to clarify a 25-year misunderstanding towards the greedy algorithm and justify that the greedy algorithm is not bad at all. Finally, we provide a uniform implementation for all the algorithms of the OMBM problem and clarify their strengths and weaknesses, which can guide practitioners to select appropriate algorithms for various scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "1872111",
                    "name": "Jieying She"
                },
                {
                    "authorId": "1696332",
                    "name": "Bolin Ding"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2592610",
                    "name": "Tianyu Wo"
                },
                {
                    "authorId": "145389711",
                    "name": "Ke Xu"
                }
            ]
        },
        {
            "paperId": "8a724df220e2e94af55c8e0b111bf642bbab8584",
            "title": "A Survey of General-Purpose Crowdsourcing Techniques",
            "abstract": "Since Jeff Howe introduced the term Crowdsourcing in 2006, this human-powered problem-solving paradigm has gained a lot of attention and has been a hot research topic in the field of computer science. Even though a lot of work has been conducted on this topic, so far we do not have a comprehensive survey on most relevant work done in the crowdsourcing field. In this paper, we aim to offer an overall picture of the current state of the art techniques in general-purpose crowdsourcing. According to their focus, we divide this work into three parts, which are: incentive design, task assignment, and quality control. For each part, we start with different problems faced in that area followed by a brief description of existing work and a discussion of pros and cons. In addition, we also present a real scenario on how the different techniques are used in implementing a location-based crowdsourcing platform, gMission. Finally, we highlight the limitations of the current general-purpose crowdsourcing techniques and present some open problems in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1412560973",
                    "name": "Anand Inasu Chittilappilly"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "8cc2af6a1be881cb303ee7daa2a529b9313e20cb",
            "title": "Fast Composite Counting in RFID Systems",
            "abstract": "Counting the number of tags is a fundamental issue and has a wide range of applications in RFID systems. Most existing protocols, however, only apply to the scenario where a single reader counts the number of tags covered by its radio, or at most the union of tags covered by multiple readers. They are unable to achieve more complex counting objectives, i.e., counting the number of tags in a composite set expression such as (S1 \u222a S2) - (S3 \u2229 S4). This type of counting has realistic significance as it provides more diversity than existing counting scenario, and can be applied in various applications. We formally introduce the RFID composite counting problem, which aims at counting the tags in an arbitrary set expression and obtain its strong lower bounds on the communication cost. We then propose a generic Composite Counting Framework (CCF) that provides estimates for any set expression with desired accuracy. The communication cost of CCF is proved to be within a small factor from the optimal. We build a prototype system for CCF using USRP software defined radio and Intel WISP computational tags. Also, extensive simulations are conducted to evaluate the performance of CCF. The experimental results show that CCF is generic, accurate and time-efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055685016",
                    "name": "Wei Gong"
                },
                {
                    "authorId": "2452544",
                    "name": "Haoxiang Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "46578147",
                    "name": "Kebin Liu"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                }
            ]
        },
        {
            "paperId": "97cfc710079d8906712eab73e3716afbcf4f6c59",
            "title": "Conflict-Aware Event-Participant Arrangement and Its Variant for Online Setting",
            "abstract": "With the rapid development of Web 2.0 and Online To Offline (O2O) marketing model, various <italic>online <underline> e</underline>vent-<underline>b</underline>ased <underline>s</underline>ocial <underline>n</underline>etwork<underline>s </underline></italic> (EBSNs) are getting popular. An important task of EBSNs is to facilitate the most satisfactory event-participant arrangement for both sides, i.e., events enroll more participants and participants are arranged with personally interesting events. Existing approaches usually focus on the arrangement of each single event to a set of potential users, or ignore the conflicts between different events, which leads to infeasible or redundant arrangements. In this paper, to address the shortcomings of existing approaches, we first identify a more general and useful event-participant arrangement problem, called <italic><underline>G</underline>lobal <underline>E</underline> vent-participant <underline>A</underline>rrangement with <underline>C</underline>onflict and <underline>C</underline> apacity</italic> (<inline-formula><tex-math notation=\"LaTeX\">$GEACC$</tex-math><alternatives> <inline-graphic xlink:type=\"simple\" xlink:href=\"chen-ieq1-2565468.gif\"/></alternatives></inline-formula>) problem, focusing on the conflicts of different events and making event-participant arrangements in a global view. We find that the GEACC problem is NP-hard due to the conflicts among events. Thus, we design two approximation algorithms with provable approximation ratios and an exact algorithm with pruning technique to address this problem. In addition, we propose an online setting of GEACC, called OnlineGEACC, which is also practical in real-world scenarios. We further design an online algorithm with provable performance guarantee. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive experiments on real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1872111",
                    "name": "Jieying She"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "3151540",
                    "name": "Caleb Chen Cao"
                }
            ]
        },
        {
            "paperId": "abb6b473d50cb36f098904a17d248517d4e6a67c",
            "title": "Spatial Crowdsourcing: Challenges and Opportunities",
            "abstract": ".",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1773086",
                    "name": "C. Shahabi"
                }
            ]
        },
        {
            "paperId": "b6de61c6b0912357e1ef06ad81dfbdb9e399c034",
            "title": "RSkNN: kNN Search on Road Networks by Incorporating Social Influence",
            "abstract": "Although <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives> <inline-graphic xlink:type=\"simple\" xlink:href=\"chen-ieq1-2518692.gif\"/></alternatives></inline-formula>NN search on a road network <inline-formula><tex-math notation=\"LaTeX\">$G_r$</tex-math><alternatives> <inline-graphic xlink:type=\"simple\" xlink:href=\"chen-ieq2-2518692.gif\"/></alternatives></inline-formula>, i.e., finding <inline-formula> <tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"chen-ieq3-2518692.gif\"/> </alternatives></inline-formula> nearest objects to a query user <inline-formula><tex-math notation=\"LaTeX\"> $q$</tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"chen-ieq4-2518692.gif\"/></alternatives> </inline-formula> on <inline-formula><tex-math notation=\"LaTeX\">$G_r$</tex-math><alternatives> <inline-graphic xlink:type=\"simple\" xlink:href=\"chen-ieq5-2518692.gif\"/></alternatives></inline-formula>, has been extensively studied, existing works neglected the fact that the <inline-formula><tex-math notation=\"LaTeX\">$q$</tex-math> <alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"chen-ieq6-2518692.gif\"/></alternatives></inline-formula>'s social information can play an important role in this <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math> <alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"chen-ieq7-2518692.gif\"/></alternatives></inline-formula>NN query. Many real-world applications, such as location-based social networking services, require such a query. In this paper, we study a new problem: <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives> <inline-graphic xlink:type=\"simple\" xlink:href=\"chen-ieq8-2518692.gif\"/></alternatives></inline-formula>NN search on road networks by incorporating social influence (RS<italic>k</italic>NN). Specifically, the state-of-the-art <italic>Independent Cascade</italic> (IC) model in social network is applied to define social influence. One critical challenge of the problem is to speed up the computation of the social influence over large road and social networks. To address this challenge, we propose three efficient index-based search algorithms, i.e., road network-based (RN-based), social network-based (SN-based), and hybrid indexing algorithms. In the RN-based algorithm, we employ a filtering-and-verification framework for tackling the hard problem of computing social influence. In the SN-based algorithm, we embed social cuts into the index, so that we speed up the query. In the hybrid algorithm, we propose an index, summarizing the road and social networks, based on which we can obtain query answers efficiently. Finally, we use real road and social network data to empirically verify the efficiency and efficacy of our solutions.",
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2199045",
                    "name": "Yongjiao Sun"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                }
            ]
        },
        {
            "paperId": "bc07091baa7c7b75b7516a807bee126b5820c074",
            "title": "FROG: A Fast and Reliable Crowdsourcing Framework",
            "abstract": "For decades, the crowdsourcing has gained much attention from both academia and industry, which outsources a number of tasks to human workers. Typically, existing crowdsourcing platforms include CrowdFlower, Amazon Mechanical Turk (AMT), and so on, in which workers can autonomously select tasks to do. However, due to the unreliability of workers or the difficulties of tasks, workers may sometimes finish doing tasks either with incorrect/incomplete answers or with significant time delays. Existing studies considered improving the task accuracy through voting or learning methods, they usually did not fully take into account reducing the latency of the task completion. This is especially critical, when a task requester posts a group of tasks (e.g., sentiment analysis), and one can only obtain answers of all tasks after the last task is accomplished. As a consequence, the time delay of even one task in this group could delay the next step of the task requester\u2019s work from minutes to days, which is quite undesirable for the task requester. Inspired by the importance of the task accuracy and latency, in this paper, we will propose a novel crowdsourcing framework, namely Fast and Reliable crOwdsourcinG framework (FROG), which intelligently assigns tasks to workers, such that the latencies of tasks are reduced and the expected accuracies of tasks are met. Specifically, our FROG framework consists of two important components, task scheduler and notification modules. For the task scheduler module, we formalize a FROG task scheduling (FROG-TS) problem, in which the server actively assigns workers to tasks to achieve high task reliability and low task latency. We prove that the FROG-TS problem is NP-hard. Thus, we design two heuristic approaches, request-based and batch-based scheduling. For the notification module, we define an efficient worker notifying (EWN) problem, which only sends task invitations to those workers with high probabilities of accepting the tasks. To tackle the EWN problem, we propose a smooth kernel density estimation approach to estimate the probability that a worker accepts the task invitation. Through extensive experiments, we demonstrate the effectiveness and efficiency of our proposed FROG platform on both real and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "2054880853",
                    "name": "Xun Jian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "c537e76e57726b841d036bf3466f10bffe68b082",
            "title": "Tuning Crowdsourced Human Computation",
            "abstract": "As crowdsourcing has been dramatically investigated and utilized to address problems in the real world, it is essential and important to think about performance optimization. Analogous to computer systems with CPUs, treating each worker as a HPU (Human Processing Unit [1]) and studying the performance optimization on top of HPUs are interesting perspectives to resolve crowdsourcing issues. However, as we characterize HPUs in detail for this purpose, we find that there are significant differences between CPUs and HPUs, leading to the need of completely new optimization algorithms. In this paper, we study the specific optimization problem of obtaining results the fastest for a crowdsourced job with a fixed total budget. In crowdsourcing, jobs are usually broken down into sets of small tasks, which are assigned to workers one at a time. We consider three scenarios of increasing complexity: Identical Round Homogeneous Tasks, Multiplex Round Homogeneous Tasks, and Multiple Round Heterogeneous Tasks. For each scenario, we analyze the stochastic behavior of the HPU clock rate as a function of the remuneration offered. After that, we develop an optimum Budget Allocation Strategy to minimize the latency of the job completion. We validate our results through extensive simulations and experiments on Amazon Mechanical Turk.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36005450",
                    "name": "Chen Cao"
                },
                {
                    "authorId": "35734465",
                    "name": "Jiayang Tu"
                },
                {
                    "authorId": "2145976421",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                }
            ]
        },
        {
            "paperId": "d3102b671914e801643fe42da13aa461db7935ce",
            "title": "Mutual benefit aware task assignment in a bipartite labor market",
            "abstract": "As one of the three major steps (question design, task assignment, answer aggregation) in crowdsourcing, task assignment directly affects the quality of the crowdsourcing result. A good assignment will not only improve the answers' quality, but also boost the workers' willingness to participate. Although a lot of works have been made to produce better assignment, most of them neglected one of its most important properties: the bipartition, which exists widely in real world scenarios. Such ignorance greatly limits their application under general settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145976421",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "df940ae0a76535c62eae105fc02ad5bf503a3fa5",
            "title": "Online mobile Micro-Task Allocation in spatial crowdsourcing",
            "abstract": "With the rapid development of smartphones, spatial crowdsourcing platforms are getting popular. A foundational research of spatial crowdsourcing is to allocate micro-tasks to suitable crowd workers. Most existing studies focus on offline scenarios, where all the spatiotemporal information of micro-tasks and crowd workers is given. However, they are impractical since micro-tasks and crowd workers in real applications appear dynamically and their spatiotemporal information cannot be known in advance. In this paper, to address the shortcomings of existing offline approaches, we first identify a more practical micro-task allocation problem, called the Global Online Micro-task Allocation in spatial crowdsourcing (GOMA) problem. We first extend the state-of-art algorithm for the online maximum weighted bipartite matching problem to the GOMA problem as the baseline algorithm. Although the baseline algorithm provides theoretical guarantee for the worst case, its average performance in practice is not good enough since the worst case happens with a very low probability in real world. Thus, we consider the average performance of online algorithms, a.k.a online random order model.We propose a two-phase-based framework, based on which we present the TGOA algorithm with 1 over 4 -competitive ratio under the online random order model. To improve its efficiency, we further design the TGOA-Greedy algorithm following the framework, which runs faster than the TGOA algorithm but has lower competitive ratio of 1 over 8. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive experiments on real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "1872111",
                    "name": "Jieying She"
                },
                {
                    "authorId": "1696332",
                    "name": "Bolin Ding"
                },
                {
                    "authorId": "2108532674",
                    "name": "Libin Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "e771511994fccd295b1b98bbc48b445b6ea1b1ad",
            "title": "A Survey on Accessing Dataspaces",
            "abstract": "Dataspaces provide a co-existence approach for heterogeneous data. Relationships among these heterogeneous data are often incrementally identified, such as object associations or attribute synonyms. With the different degree of relationships recognized, various query answers may be obtained. In this paper, we review the major techniques for processing and optimizing queries in dataspaces, according to their different abilities of handling relationships, including 1) simple search query without considering relationships, 2) association query over object associations, 3) heterogeneity query with attribute correspondences, and 4) similarity query for similar objects. Techniques such as indexing, query rewriting, expansion, and semantic query optimization are discussed for these query types. Finally, we highlight possible directions in accessing dataspaces.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108927961",
                    "name": "Yihan Wang"
                },
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "048f2ea490f21d286071b5c55e5739a00b0e798d",
            "title": "Utility-Aware Social Event-Participant Planning",
            "abstract": "Online event-based social network (EBSN) platforms are becoming popular these days. An important task of managing EBSNs is to arrange proper social events to interested users. Existing approaches usually assume that each user only attends one event or ignore location information. The overall utility of such strategy is limited in real world: 1) each user may attend multiple events; 2) attending multiple events will incur spatio-temporal conflicts and travel expenses. Thus, a more intelligent EBSN platform that provides personalized event planning for each participant is desired. In this paper, we first formally define the problem of Utility-aware Social Event-participant Planning (USEP), which is proven to be NP-hard. To solve the USEP problem, we first devise a greedy-based heuristic algorithm, which performs fast under certain circumstances but has no approximation guarantee. We then present a two-step approximation framework, which not only guarantees a 1/2-approximation ratio but also includes a series of optimization techniques to improve its space/time efficiency. Finally, we verify the efficiency and effectiveness of the proposed methods through extensive experiments on real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1872111",
                    "name": "Jieying She"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "07d6e9ac55863c2f0321988795ecb161be649319",
            "title": "Boosting Mobile Apps under Imbalanced Sensing Data",
            "abstract": "Mobile sensing apps have proliferated rapidly over the recent years. Most of them rely on inference components heavily for detecting interesting activities or contexts. Existing work implements inference components using traditional models designed for balanced data sets, where the sizes of interesting (positive) and non-interesting (negative) data are comparable. Practically, however, the positive and negative sensing data are highly imbalanced. For example, a single daily activity such as bicycling or driving usually occupies a small portion of time, resulting in rare positive instances. Under this circumstance, the trained models based on imbalanced data tend to mislabel positive ones as negative. In this paper, we propose a new inference framework SLIM based on several machine learning techniques in order to accommodate the imbalanced nature of sensing data. Especially, guided under-sampling is employed to obtain balanced labelled subsets, followed by a similarity-based sampling that draws massive unlabelled data to enhance training. To the best of our knowledge, SLIM is the first model that considers data imbalance in mobile sensing. We prototype two sensing apps and the experimental results show that SLIM achieves higher recall (activity recognition rate) while maintaining the precision compared with five classical models. In terms of the overall recall and precision, SLIM is around 12 percent better than the compared solutions on average.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2468508",
                    "name": "Xinglin Zhang"
                },
                {
                    "authorId": "40176711",
                    "name": "Zheng Yang"
                },
                {
                    "authorId": "3015446",
                    "name": "Longfei Shangguan"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "1deaf722a252eb2c060108db164bd2e0a2977b3f",
            "title": "Efficient Parallel Processing of Distance Join Queries Over Distributed Graphs",
            "abstract": "Distance join queries have recently been recognized as a particularly useful operation over graph data, since they capture graph similarity in a meaningful way. Consequently, they have been studied extensively in recent years [1], [2]. However, current methods are designed for centralized systems, and rely on the graph embedding for effective pruning and indexing. As graph sizes become very large and graph data must be deployed in the distributed environment, these techniques become impractical. In this work, we propose a solution for efficient parallel processing of distance join queries over distributed large graphs. There have been emerging efforts devoted to managing large graphs in distributed and parallel systems. Programming models like Pregel [3] and iterative computing framework like HaLoop [4] have been proposed to handle queries over distributed graphs. However, they are designed in the perspective of functionality instead of the query efficiency. In this work, we define an optimization problem: combining the iterative join and the graph exploration method to minimize the evaluation time of distance join queries. Without sacrificing a system's scalability, our technique exploits a light-weight vertex centric encoding schema built on a distance-aware partition of the entire graph. Extensive experiments over both real and synthetic large graphs show that, by employing an adaptive query plan generation and scheduling method, we can effectively reduce the redundant message passing and I/O costs. Compared to simply using iterative join or graph exploration method, our solution achieves as many as one order of magnitude of time saving for the query evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108166998",
                    "name": "Xiaofei Zhang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "143894230",
                    "name": "Min Wang"
                }
            ]
        },
        {
            "paperId": "1e2047df5f53000f59da8619fa565d1a4cafb70a",
            "title": "Hear the Whole Story: Towards the Diversity of Opinion in Crowdsourcing Markets",
            "abstract": "\n The recent surge in popularity of crowdsourcing has brought with it a new opportunity for engaging human intelligence in the process of data analysis. Crowdsourcing provides a fundamental mechanism for enabling online workers to participate in tasks that are either too difficult to be solved solely by a computer or too expensive to employ experts to perform. In the field of social science, four elements are required to form a wise crowd - Diversity of Opinion, Independence, Decentralization and Aggregation. However, while the other three elements are already studied and implemented in current crowdsourcing platforms, the 'Diversity of Opinion' has not been functionally enabled. In this paper, we address the algorithmic optimizations towards the\n diversity of opinion\n of crowdsourcing marketplaces.\n \n From a computational perspective, in order to build a wise crowd, we need to quantitatively modeling the diversity, and take it into consideration for constructing the crowd. In a crowdsourcing marketplace, we usually encounter two basic paradigms for worker selection: building a crowd to wait for tasks to come and selecting workers for a given task. Therefore, we propose our Similarity-driven Model (S-Model) and Task-driven Model (T-Model) for both of the paradigms. Under both of the models, we propose efficient and effective algorithms to enlist a budgeted number of workers, which have the optimal diversity. We have verified our solutions with extensive experiments on both synthetic datasets and real data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112662972",
                    "name": "Ting Wu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "143966172",
                    "name": "P. Hui"
                },
                {
                    "authorId": "50445897",
                    "name": "C. Zhang"
                },
                {
                    "authorId": "2108770911",
                    "name": "Weikai Li"
                }
            ]
        },
        {
            "paperId": "1e90b34da4627115d2d6e9a9ff950a8044ad5c26",
            "title": "SMC: A Practical Schema for Privacy-Preserved Data Sharing over Distributed Data Streams",
            "abstract": "Data collection is required to be safe and efficient considering both data privacy and system performance. In this paper, we study a new problem: distributed data sharing with privacy-preserving requirements. Given a data demander requesting data from multiple distributed data providers, the objective is to enable the data demander to access the distributed data without knowing the privacy of any individual provider. The problem is challenged by two questions: how to transmit the data safely and accurately; and how to efficiently handle data streams? As the first study, we propose a practical method, Shadow Coding, to preserve the privacy in data transmission and ensure the recovery in data collection, which achieves privacy preserving computation in a data-recoverable, efficient, and scalable way. We also provide practical techniques to make Shadow Coding efficient and safe in data streams. Extensive experimental study on a large-scale real-life dataset offers insight into the performance of our schema. The proposed schema is also implemented as a pilot system in a city to collect distributed mobile phone data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47130133",
                    "name": "Siyuan Liu"
                },
                {
                    "authorId": "143980270",
                    "name": "Qiang Qu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1726587",
                    "name": "L. Ni"
                }
            ]
        },
        {
            "paperId": "28054dba2ffc6dbe59ae3325ce7fade36d1c6f6b",
            "title": "Bonding Vertex Sets Over Distributed Graph: A Betweenness Aware Approach",
            "abstract": "Given two sets of vertices in a graph, it is often of a great interest to find out how these vertices are connected, especially to identify the vertices of high prominence defined on the topological structure. In this work, we formally define a Vertex Set Bonding query (shorted as VSB), which returns a minimum set of vertices with the maximum importance w.r.t total betweenness and shortest path reachability in connecting two sets of input vertices. We find that such a kind of query is representative and could be widely applied in many real world scenarios, e.g., logistic planning, social community bonding and etc. Challenges are that many of such applications are constructed on graphs that are too large to fit in single server, and the VSB query evaluation turns to be NP-hard. To cope with the scalability issue and return the near optimal result in almost real time, we propose a generic solution framework on a shared nothing distributed environment. With the development of two novel techniques, guided graph exploration and betweenness ranking on exploration, we are able to efficiently evaluate queries for error bounded results with bounded space cost. We demonstrate the effectiveness of our solution with extensive experiments over both real and synthetic large graphs on the Google's Cloud platform. Comparing to the exploration only baseline method, our method achieves several times of speedup.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108166998",
                    "name": "Xiaofei Zhang"
                },
                {
                    "authorId": "144488755",
                    "name": "Hong Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "2f2bccaba323b0efe46dc737d9109b0af8bfca57",
            "title": "The Reachability Query over Distributed Uncertain Graphs",
            "abstract": "Reachability, one of the most fundamental queries over uncertain graphs, which asks the probability that two given query vertices are reachable over an uncertain graph. Although this problem has been widely studied, the existing works are all processed in a single server. However, as graph data becomes larger, it usually cannot be stored in a single server. Moreover, processing probabilistic reachability queries is #P-complete, so the calculation is very expensive even on small graphs. Thus, in this paper, our purpose is to develop efficient distributed strategies to firstly pick out all the maximal subgraphs whose reachability probabilities can be calculated in polynomial time efficiently. After this step, only a small graph remains, and we provide an approximate method. Extensive experimental studies show that our distributed algorithms are efficient and have a low communication cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2329311",
                    "name": "Yurong Cheng"
                },
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                }
            ]
        },
        {
            "paperId": "3d359fca5a3b632d890f1640d78defc09a561d8d",
            "title": "Cleaning uncertain data with a noisy crowd",
            "abstract": "Uncertain data has been emerged as an important problem in database systems due to the imprecise nature of many applications. To handle the uncertainty, probabilistic databases can be used to store uncertain data, and querying facilities are provided to yield answers with confidence. However, the uncertainty may propagate, hence the returned results from a query or mining process may not be useful. In this paper, we leverage the power of crowdsourcing for cleaning uncertain data. Specifically, we will design a set of Human Intelligence Tasks (HIT)s to ask a crowd to improve the quality of uncertain data. Each HIT is associated with a cost, thus, we need to design solutions to maximize the data quality with minimal number of HITs. There are two obstacles for this non-trivial optimization - first, the crowd has a probability to return incorrect answers; second, the HITs decomposed from uncertain data are often correlated. These two obstacles lead to very high computational cost for selecting the optimal set of HITs. Thus, in this paper, we have addressed these challenges by designing an effective approximation algorithm and an efficient heuristic solution. To further improve the efficiency, we derive tight lower and upper bounds, which are used for effective filtering and estimation. We have verified the solutions with extensive experiments on both a simulated crowd and a real crowdsourcing platform.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50445897",
                    "name": "C. Zhang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "2145976421",
                    "name": "Zheng Liu"
                }
            ]
        },
        {
            "paperId": "40e0588779c473cf56a09d2b5bb0af00a8cdb8f0",
            "title": "Traffic prediction in a bike-sharing system",
            "abstract": "Bike-sharing systems are widely deployed in many major cities, providing a convenient transportation mode for citizens' commutes. As the rents/returns of bikes at different stations in different periods are unbalanced, the bikes in a system need to be rebalanced frequently. Real-time monitoring cannot tackle this problem well as it takes too much time to reallocate the bikes after an imbalance has occurred. In this paper, we propose a hierarchical prediction model to predict the number of bikes that will be rent from/returned to each station cluster in a future period so that reallocation can be executed in advance. We first propose a bipartite clustering algorithm to cluster bike stations into groups, formulating a two-level hierarchy of stations. The total number of bikes that will be rent in a city is predicted by a Gradient Boosting Regression Tree (GBRT). Then a multi-similarity-based inference model is proposed to predict the rent proportion across clusters and the inter-cluster transition, based on which the number of bikes rent from/ returned to each cluster can be easily inferred. We evaluate our model on two bike-sharing systems in New York City (NYC) and Washington D.C. (D.C.) respectively, confirming our model's advantage beyond baseline approaches (0.03 reduction of error rate), especially for anomalous periods (0.18/0.23 reduction of error rate).",
            "fieldsOfStudy": [
                "Computer Science",
                "Geography"
            ],
            "authors": [
                {
                    "authorId": "47002671",
                    "name": "Yexin Li"
                },
                {
                    "authorId": "2149514474",
                    "name": "Yu Zheng"
                },
                {
                    "authorId": "3364661",
                    "name": "Huichu Zhang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "445e4442ce33a04d5c4d45eb191f431e0bc2444c",
            "title": "CrowdLink: An Error-Tolerant Model for Linking Complex Records",
            "abstract": "Record linkage (RL) refers to the task of finding records in a data set that refer to the same entity across different data sources (e.g., data files, books, websites, databases), which is a long-standing challenge in database management. Algorithmic approaches have been proposed to improve RL quality, but remain far from perfect. Crowdsourcing offers a more accurate but expensive (and slow) way to bring human insight into the process. In this paper, we propose a new probabilistic model, namely CrowdLink, to tackle the above limitations. In particular, our model gracefully handles the crowd error and the correlation among different pairs, as well as enables us to decompose the records into small pieces (i.e. attributes) so that crowdsourcing workers can easily verify. Further, we develop efficient and effective algorithms to select the most valuable questions, in order to reduce the monetary cost of crowdsourcing. We conducted extensive experiments on both synthetic and real-world datasets. The experimental results verified the effectiveness and the applicability of our model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50445897",
                    "name": "C. Zhang"
                },
                {
                    "authorId": "2087884364",
                    "name": "Rui Meng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "143663410",
                    "name": "Feida Zhu"
                }
            ]
        },
        {
            "paperId": "5494bbc270625a441db4525b34bdf8fe6b193309",
            "title": "Query Workload-based RDF Graph Fragmentation and Allocation",
            "abstract": "As the volume of the RDF data becomes increasingly large, it is essential for us to design a distributed database system to manage it. For distributed RDF data design, it is quite common to partition the RDF data into some parts, called fragments, which are then distributed. Thus, the distribution design consists of two steps: fragmentation and allocation. In this paper, we propose a method to explore the intrinsic similarities among the structures of queries in a workload for fragmentation and allocation, which aims to reduce the number of crossing matches and the communication cost during SPARQL query processing. Specifically, we mine and select some frequent access patterns to reflect the characteristics of the workload. Here, although we prove that selecting the optimal set of frequent access patterns is NP-hard, we propose a heuristic algorithm which guarantees both the data integrity and the approximation ratio. Based on the selected frequent access patterns, we propose two fragmentation strategies, vertical and horizontal fragmentation strategies, to divide RDF graphs while meeting different kinds of query processing objectives. Vertical fragmentation is for better throughput and horizontal fragmentation is for better performance. After fragmentation, we discuss how to allocate these fragments to various sites. Finally, we discuss how to process a query based on the results of fragmentation and allocation. Extensive experiments confirm the superior performance of our proposed solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144189270",
                    "name": "Peng Peng"
                },
                {
                    "authorId": "144554647",
                    "name": "Lei Zou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144060462",
                    "name": "Dongyan Zhao"
                }
            ]
        },
        {
            "paperId": "687c203c9d66f0b870fa4a48d02171c72824368d",
            "title": "Heterogeneous Environment Aware Streaming Graph Partitioning",
            "abstract": "With the increasing availability of graph data and widely adopted cloud computing paradigm, graph partitioning has become an efficient pre-processing technique to balance the computing workload and cope with the large scale of input data. Since the cost of partitioning the entire graph is strictly prohibitive, there are some recent tentative works towards streaming graph partitioning which run faster, are easily parallelized, and can be incrementally updated. Most of the existing works on streaming partitioning assume that worker nodes within a cluster are homogeneous in nature. Unfortunately, this assumption does not always hold. Experiments show that these homogeneous algorithms suffer a significant performance degradation when running at heterogeneous environment. In this paper, we propose a novel adaptive streaming graph partitioning approach to cope with heterogeneous environment. We first formally model the heterogeneous computing environment with the consideration of the unbalance of computing ability (e.g., the CPU frequency) and communication ability (e.g., the network bandwidth) for each node. Based on this model, we propose a new graph partitioning objective function that aims to minimize the total execution time of the graph-processing job. We then explore some simple yet effective streaming algorithms for this objective function that can achieve balanced and efficient partitioning result. Extensive experiments are conducted on a moderate sized computing cluster with real-world web and social network graphs. The results demonstrate that the proposed approach achieves significant improvement compared with the state-of-the-art solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145857599",
                    "name": "N. Xu"
                },
                {
                    "authorId": "144585959",
                    "name": "B. Cui"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145622169",
                    "name": "Zi Huang"
                },
                {
                    "authorId": "2237813",
                    "name": "Yingxia Shao"
                }
            ]
        },
        {
            "paperId": "69a80667199860c28ac218e6105466de9407d223",
            "title": "Data-driven crowdsourcing: Management, mining, and applications",
            "abstract": "In this 3-hour tutorial, we present the landscape of recent developments in data management and mining research, and survey a selected set of state-of-the-art works that significantly extended existing database reserach in order to incorporate and exploit the novel notion of \u201ccrowdsourcing\u201d in a creative fashion. In particular, three speakers take turns to present the topics of human-powered database operations, crowdsourced data mining, and the application of crowdsourcing in social media, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                }
            ]
        },
        {
            "paperId": "7b159172e65a3745b97d1e79aa1761e2ceec59d4",
            "title": "LINQ: A Framework for Location-Aware Indexing and Query Processing",
            "abstract": "This paper studies the generic location-aware rank query (GLRQ) over a set of location-aware objects. A GLRQ is composed of a spatial location, a set of keywords, a query predicate, and a ranking function formulated on location, text and other attributes. The result consists of k objects satisfying the predicate ranked according to the ranking function. An example is a query searching for the restaurants that 1) are nearby, 2) offer \u201cAmerican\u201d food, and 3) have high ratings (rating > 4.0). Such queries can not be processed efficiently using existing techniques. In this work, we propose a novel framework called LINQ for efficient processing of GLRQs. To handle the predicate and the attribute-based scoring, we devise a new index structure called synopses tree, which contains the synopses of different subsets of the dataset. The synopses tree enables pruning of search space according to the satisfiability of the predicate. To process the query constraints over the location and keywords, the framework integrates the synopses tree with the spatio-textual index such as IR-tree. The framework therefore is capable of processing the GLRQs efficiently and holistically. We conduct extensive experiments to demonstrate that our solution provides excellent query performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27469537",
                    "name": "X. Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1761110",
                    "name": "Changxuan Wan"
                }
            ]
        },
        {
            "paperId": "801b6a05ebaf53ecc560f8f0a97b8dbb62d9aee8",
            "title": "CrowdTC: Crowdsourced Taxonomy Construction",
            "abstract": "Recently, taxonomy has attracted much attention. Both automatic construction solutions and human-based computation approaches have been proposed. The automatic methods suffer from the problem of either low precision or low recall and human computation, on the other hand, is not suitable for large scale tasks. Motivated by the shortcomings of both approaches, we present a hybrid framework, which combines the power of machine-based approaches and human computation (the crowd) to construct a more complete and accurate taxonomy. Specifically, our framework consists of two steps: we first construct a complete but noisy taxonomy automatically, then crowd is introduced to adjust the entity positions in the constructed taxonomy. However, the adjustment is challenging as the budget (money) for asking the crowd is often limited. In our work, we formulate the problem of finding the optimal adjustment as an entity selection optimization (ESO) problem, which is proved to be NP-hard. We then propose an exact algorithm and a more efficient approximation algorithm with an approximation ratio of 1/2(1-1/e). We conduct extensive experiments on real datasets, the results show that our hybrid approach largely improves the recall of the taxonomy with little impairment for precision.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087884364",
                    "name": "Rui Meng"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "3151540",
                    "name": "Caleb Chen Cao"
                }
            ]
        },
        {
            "paperId": "85a0cafb7a53de76b3fef13919eeff1907054ae7",
            "title": "An Efficient Similarity Search Framework for SimRank over Large Dynamic Graphs",
            "abstract": "SimRank is an important measure of vertex-pair similarity according to the structure of graphs. The similarity search based on SimRank is an important operation for identifying similar vertices in a graph and has been employed in many data analysis applications. Nowadays, graphs in the real world become much larger and more dynamic. The existing solutions for similarity search are expensive in terms of time and space cost. None of them can efficiently support similarity search over large dynamic graphs. In this paper, we propose a novel two-stage random-walk sampling framework (TSF) for SimRank-based similarity search (e.g., top-k search). In the preprocessing stage, TSF samples a set of one-way graphs to index raw random walks in a novel manner within O(NRg) time and space, where N is the number of vertices and Rg is the number of one-way graphs. The one-way graph can be efficiently updated in accordance with the graph modification, thus TSF is well suited to dynamic graphs. During the query stage, TSF can search similar vertices fast by naturally pruning unqualified vertices based on the connectivity of one-way graphs. Furthermore, with additional Rq samples, TSF can estimate the SimRank score with probability [EQUATION] if the error of approximation is bounded by 1 -- e. Finally, to guarantee the scalability of TSF, the one-way graphs can also be compactly stored on the disk when the memory is limited. Extensive experiments have demonstrated that TSF can handle dynamic billion-edge graphs with high performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237813",
                    "name": "Yingxia Shao"
                },
                {
                    "authorId": "144585959",
                    "name": "B. Cui"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2112314045",
                    "name": "Mingming Liu"
                },
                {
                    "authorId": "144076239",
                    "name": "Xing Xie"
                }
            ]
        },
        {
            "paperId": "880a06701e2b65802b577b676614255826171cbe",
            "title": "On Uncertain Graphs Modeling and Queries",
            "abstract": "Large-scale, highly-interconnected networks pervade both our society and the natural world around us. Uncertainty, on the other hand, is inherent in the underlying data due to a variety of reasons, such as noisy measurements, lack of precise information needs, inference and prediction models, or explicit manipulation, e.g., for privacy purposes. Therefore, uncertain, or probabilistic, graphs are increasingly used to represent noisy linked data in many emerging application scenarios, and they have recently become a hot topic in the database research community. While many classical graph algorithms such as reachability and shortest path queries become #P-complete, and hence, more expensive in uncertain graphs; various complex queries are also emerging over uncertain networks, such as pattern matching, information diffusion, and influence maximization queries. In this tutorial, we discuss the sources of uncertain graphs and their applications, uncertainty modeling, as well as the complexities and algorithmic advances on uncertain graphs processing in the context of both classical and emerging graph queries. We emphasize the current challenges and highlight some future research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108514592",
                    "name": "Arijit Khan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "968ffe128296d24d4f377446a4c43cc14f64a4f4",
            "title": "Discover the Misinformation Broadcasting in On-Line Social Networks",
            "abstract": "In recent years, more and more people join social networks to share information with others. At the same time, the information sharing/spreading becomes far more frequent and convenient due to the wide usage of mobile devices. As a result, the messages created are very arbitrary, which may contain a lot of misinformation. Proper actions must be taken to avoid the spreading of misinformation or rumors before it causes serious damages. Therefore, any misinformation should be discovered in time when it does not spread to a large group of people. All previous works studied either how the information is spread in the social network or how to inhibit the further pervasion of an observed misinformation. However, no works considered how to discover the broadcasting of misinformation in time. A possible solution is to set observers across the network to discover the suspects of misinformation. In this paper, we design a novel mechanism to select a set of observers in a social network with the minimum cost, where these observers guarantee any misinformation can be discovered with a high probability before it reaches a bounded number of users. Extensive experiment on real data sets verifies the effectiveness of our solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2016279",
                    "name": "Peiguang Lin"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1688812",
                    "name": "Mingxuan Yuan"
                },
                {
                    "authorId": "2983816",
                    "name": "P. Nie"
                }
            ]
        },
        {
            "paperId": "9808874acdd84f9ab3fb4967925928fb7452c2bf",
            "title": "Enriching Data Imputation with Extensive Similarity Neighbors",
            "abstract": "Incomplete information often occur along with many database applications, e.g., in data integration, data cleaning or data exchange. The idea of data imputation is to fill the missing data with the values of its neighbors who share the same information. Such neighbors could either be identified certainly by editing rules or statistically by relational dependency networks. Unfortunately, owing to data sparsity, the number of neighbors (identified w.r.t. value equality) is rather limited, especially in the presence of data values with variances. In this paper, we argue to extensively enrich similarity neighbors by similarity rules with tolerance to small variations. More fillings can thus be acquired that the aforesaid equality neighbors fail to reveal. To fill the missing values more, we study the problem of maximizing the missing data imputation. Our major contributions include (1) the np-hardness analysis on solving and approximating the problem, (2) exact algorithms for tackling the problem, and (3) efficient approximation with performance guarantees. Experiments on real and synthetic data sets demonstrate that the filling accuracy can be improved.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "3358457",
                    "name": "Aoqian Zhang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2016222572",
                    "name": "Jianmin Wang"
                }
            ]
        },
        {
            "paperId": "9d99350e7fe062118c3ab7ee249374f74ccf3b4e",
            "title": "Task Assignment on Multi-Skill Oriented Spatial Crowdsourcing",
            "abstract": "With the rapid development of mobile devices and crowdsourcing platforms, the spatial crowdsourcing has attracted much attention from the database community. Specifically, the spatial crowdsourcing refers to sending location-based requests to workers, based on their current positions. In this paper, we consider a spatial crowdsourcing scenario, in which each worker has a set of qualified skills, whereas each spatial task (e.g., repairing a house, decorating a room, and performing entertainment shows for a ceremony) is time-constrained, under the budget constraint, and required a set of skills. Under this scenario, we will study an important problem, namely <italic>multi-skill spatial crowdsourcing</italic> (MS-SC), which finds an optimal worker-and-task assignment strategy, such that skills between workers and tasks match with each other, and workers\u2019 benefits are maximized under the budget constraint. We prove that the MS-SC problem is NP-hard and intractable. Therefore, we propose three effective heuristic approaches, including greedy, <inline-formula><tex-math notation=\"LaTeX\">$g$</tex-math><alternatives> <inline-graphic xlink:type=\"simple\" xlink:href=\"chen-ieq1-2550041.gif\"/></alternatives></inline-formula>-divide-and-conquer and cost-model-based adaptive algorithms to get worker-and-task assignments. Through extensive experiments, we demonstrate the efficiency and effectiveness of our MS-SC processing approaches on both real and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1783892",
                    "name": "Jinsong Han"
                },
                {
                    "authorId": "103567432",
                    "name": "Jizhong Zhao"
                }
            ]
        },
        {
            "paperId": "a1ebc4ba0496997aad5d16ea3016eb0e9f74622a",
            "title": "L-Graph: A General Graph Analytic System on Continuous Computation",
            "abstract": "Massive graph analytics have become an important aspect of multiple diverse applications. With the growing scale of real world graphs, efficient execution of entire graph analytics has become a challenging problem. Recently a number of distributed graph processing systems (Pregel, PowerGraph, Trinity) and centralized systems (GraphChi and XStream) have been designed. Compared with high expense of distributed systems deployed on a cluster of commodity machines, the centralized systems on cheap PCs are very attractive propositions with low expense and comparable performance. By careful analysis, we find that (i) the graph computation abstraction in the centralized systems inherently adopted a batch model similar to the distributed systems. The batch model could lead to suboptimal performance. (ii) The execution model in the centralized systems advocates sequential operations on Solid State Disk (SSD) which are still slower than memory-based operations. In order to tackle the above efficiency issues in centralized systems, we first propose a novel continuous graph computation abstraction. This model continuously processes edges and updates computation results. It allows much faster convergence than the batch model. Second, we propose to maintain vertex states in memory and advocates memory-based operations for much faster I/O operations than sequential operations on SSD. Finally, we design an adaptive memory layout to minimize overall I/O cost. We develop a proof of concept prototype L-Graph and implement four example graph analytic applications atop L-Graph. Preliminary evaluation on real and synthetic graphs have verified that the proposed continuous model greatly performs the widely used batch model and L-Graph can achiever much higher efficiency than the state of arts GraphChi.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144084147",
                    "name": "Weixiong Rao"
                },
                {
                    "authorId": "2004672",
                    "name": "Eiko Yoneki"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "a7a4eda2d166e4b615050054da76162529e10d95",
            "title": "Quality-Aware Online Task Assignment in Mobile Crowdsourcing",
            "abstract": "Mobile crowd sourcing (MCS) has grown to be a powerful computation paradigm to harness human power to solve real-world problems. Many commercial MCS platforms have arisen, enabling various novel applications. As crowd workers can be unreliable, a critical issue of these platforms is quality control. Many task assignment approaches have been proposed to increase the quality of crowd sourced tasks by matching workers and tasks in a bipartite graph. However, they fail to apply to MCS platforms where tasks are bound with locations. This paper considers the quality-aware online task assignment problem with location-based tasks. The goal is to optimize tasks' overall quality by assigning appropriate sets of tasks to workers in an online manner. To solve this problem, we propose a probabilistic quality measurement model and a hitchhiking model to characterize workers' behavior. Then we design a polynomial-time online assignment algorithm and prove that the proposed algorithm approximates the offline optimal solution with a competitive ratio of 10/7. Through extensive simulations, we demonstrate the efficiency and effectiveness of our solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145460246",
                    "name": "Xin Miao"
                },
                {
                    "authorId": "46578147",
                    "name": "Kebin Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                }
            ]
        },
        {
            "paperId": "b9891da00ada2831b89d1b7e1ebb685d4edea9f8",
            "title": "A graph-based RDF triple store",
            "abstract": "In this demonstration, we present the gStore RDF triple store. gStore is based on graph encoding and subgraph match, distinct from many other systems. More importantly, it can handle, in a uniform manner, different data types (strings and numerical data) and SPARQL queries with wildcards, aggregate, range and top-k operators over dynamic RDF datasets. We will demonstrate the main features of our system, show how to search Wikipedia documents using gStore and how to build users' own application using gStore through C++/Java API.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2179750",
                    "name": "Xuchuan Shen"
                },
                {
                    "authorId": "144554647",
                    "name": "Lei Zou"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2749392",
                    "name": "Youhuan Li"
                },
                {
                    "authorId": "2109751167",
                    "name": "Shuo Han"
                },
                {
                    "authorId": "144060462",
                    "name": "Dongyan Zhao"
                }
            ]
        },
        {
            "paperId": "c74002a28c7d4881aa243e40788e757de0e41e3a",
            "title": "MTAF: An Adaptive Design for Keyword-Based Content Dissemination on DHT Networks",
            "abstract": "Beyond offering the widely used keyword search function, many peer-to-peer systems nowadays support the subscription function. For example, Vuze allows users to create subscription filters based on the keyword search. Given the subscription, episodic or related content will be delivered to the users whenever new episodes are available. Unfortunately, these applications suffer from the downsides, for example, high network traffic in the nodes maintaining popular terms. In this paper, we propose the MTAF mechanism to overcome the issues. The key of MTAF is to carefully select a subset of terms without incurring false negatives and to forward the content item toward the home nodes of such selected terms for low content forwarding cost. Experimental results based on real datasets indicate that the proposed solutions are efficient compared to existing approaches. In particular, the similarity-based replication of filters is shown to mitigate the effect of hot spots that arise due to the fact that some document terms are substantially more popular than the others.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144084147",
                    "name": "Weixiong Rao"
                },
                {
                    "authorId": "1803191",
                    "name": "R. Vitenberg"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1759241",
                    "name": "Sasu Tarkoma"
                }
            ]
        },
        {
            "paperId": "c9bd1dd864ff113a474569c2dba27d75984fe46b",
            "title": "Online Video Recommendation in Sharing Community",
            "abstract": "The creation of sharing communities has resulted in the astonishing increasing of digital videos, and their wide applications in the domains such as entertainment, online news broadcasting etc. The improvement of these applications relies on effective solutions for social user access to video data. This fact has driven the recent research interest in social recommendation in shared communities. Although certain effort has been put into video recommendation in shared communities, the contextual information on social users has not been well exploited for effective recommendation. In this paper, we propose an approach based on the content and social information of videos for the recommendation in sharing communities. Specifically, we first exploit a robust video cuboid signature together with the Earth Mover's Distance to capture the content relevance of videos. Then, we propose to identify the social relevance of clips using the set of users belonging to a video. We fuse the content relevance and social relevance to identify the relevant videos for recommendation. Following that, we propose a novel scheme called sub-community-based approximation together with a hash-based optimization for improving the efficiency of our solution. Finally, we propose an algorithm for efficiently maintaining the social updates in dynamic shared communities. The extensive experiments are conducted to prove the high effectiveness and efficiency of our proposed video recommendation approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "49889222",
                    "name": "Yanchun Zhang"
                },
                {
                    "authorId": "2148761004",
                    "name": "Longbing Cao"
                },
                {
                    "authorId": "2074183",
                    "name": "Guangyan Huang"
                },
                {
                    "authorId": "2109117515",
                    "name": "Chen Wang"
                }
            ]
        },
        {
            "paperId": "d02d924b95bba907bd99810abef86b5c71d6ca76",
            "title": "Differential Privacy in Telco Big Data Platform",
            "abstract": "Differential privacy (DP) has been widely explored in academia recently but less so in industry possibly due to its strong privacy guarantee. This paper makes the first attempt to implement three basic DP architectures in the deployed telecommunication (telco) big data platform for data mining applications. We find that all DP architectures have less than 5% loss of prediction accuracy when the weak privacy guarantee is adopted (e.g., privacy budget parameter e \u2265 3). However, when the strong privacy guarantee is assumed (e.g., privacy budget parameter e \u2264 0:1), all DP architectures lead to 15% ~ 30% accuracy loss, which implies that real-word industrial data mining systems cannot work well under such a strong privacy guarantee recommended by previous research works. Among the three basic DP architectures, the Hybridized DM (Data Mining) and DB (Database) architecture performs the best because of its complicated privacy protection design for the specific data mining algorithm. Through extensive experiments on big data, we also observe that the accuracy loss increases by increasing the variety of features, but decreases by increasing the volume of training data. Therefore, to make DP practically usable in large-scale industrial systems, our observations suggest that we may explore three possible research directions in future: (1) Relaxing the privacy guarantee (e.g., increasing privacy budget e) and studying its effectiveness on specific industrial applications; (2) Designing specific privacy scheme for specific data mining algorithms; and (3) Using large volume of data but with low variety for training the classification models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109907636",
                    "name": "X. Hu"
                },
                {
                    "authorId": "1688812",
                    "name": "Mingxuan Yuan"
                },
                {
                    "authorId": "8248062",
                    "name": "Jianguo Yao"
                },
                {
                    "authorId": "2111233999",
                    "name": "Yu Deng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "152290618",
                    "name": "Qiang Yang"
                },
                {
                    "authorId": "145676478",
                    "name": "Haibing Guan"
                },
                {
                    "authorId": "152280479",
                    "name": "Jia Zeng"
                }
            ]
        },
        {
            "paperId": "d298fd130bb118e69a30ce0d359b20b8451cb3e1",
            "title": "Conflict-aware event-participant arrangement",
            "abstract": "With the rapid development of Web 2.0 and Online To Offline (O2O) marketing model, various online event-based social networks (EBSNs), such as Meetup and Whova, are getting popular. An important task of EBSNs is to facilitate the most satisfactory event-participant arrangement for both sides, i.e. events enroll more participants and participants are arranged with personally interesting events. Existing approaches usually focus on the arrangement of each single event to a set of potential users, and ignore the conflicts between different events, which leads to infeasible or redundant arrangements. In this paper, to address the shortcomings of existing approaches, we first identify a more general and useful event-participant arrangement problem, called Global Event-participant Arrangement with Conflict and Capacity (GEACC) problem, focusing on the conflicts of different events and making event-participant arrangements in a global view. Though it is useful, unfortunately, we find that the GEACC problem is NP-hard due to the conflict constraints among events. Thus, we design two approximation algorithms with provable approximation ratios and an exact algorithm with pruning technique to address this problem. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive experiments on real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1872111",
                    "name": "Jieying She"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "3151540",
                    "name": "Caleb Chen Cao"
                }
            ]
        },
        {
            "paperId": "d9905592827665b02cf34b8599dbb2a83de8c9c0",
            "title": "Prediction-Based Task Assignment in Spatial Crowdsourcing",
            "abstract": "With the rapid advancement of mobile devices and crowdsourcing platforms, spatial crowdsourcing has attracted much attention from various research communities. A spatial crowdsourcing system periodically matches a number of locationbased workers with nearby spatial tasks (e.g., taking photos or videos at some specific locations). Previous studies on spatial crowdsourcing focus on task assignment strategies that maximize an assignment score based solely on the available information about workers/tasks at the time of assignment. These strategies can only achieve local optimality by neglecting the workers/tasks that may join the system in a future time. In contrast, in this paper, we aim to improve the global assignment, by considering both present and future (via predictions) workers/tasks. In particular, we formalize a new optimization problem, namely maximum quality task assignment (MQA). The optimization objective of MQA is to maximize a global assignment quality score, under a traveling budget constraint. To tackle this problem, we design an effective grid-based prediction method to estimate the spatial distributions of workers/tasks in the future, and then utilize the predictions to assign workers to tasks at any given time instance. We prove that the MQA problem is NPhard, and thus intractable. Therefore, we propose efficient heuristics to tackle the MQA problem, including MQA greedy and MQA divide-and-conquer approaches, which can efficiently assign workers to spatial tasks with high quality scores and low budget consumptions. Through extensive experiments, we demonstrate the efficiency and effectiveness of our approaches on both real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1773086",
                    "name": "C. Shahabi"
                }
            ]
        },
        {
            "paperId": "ebce8b0c30f367bb54a38e25df34ea459b41582a",
            "title": "Keyword Search Over Probabilistic RDF Graphs",
            "abstract": "In many real applications, RDF (Resource Description Framework) has been widely used as a W3C standard to describe data in the Semantic Web. In practice, RDF data may often suffer from the unreliability of their data sources, and exhibit errors or inconsistencies. In this paper, we model such unreliable RDF data by probabilistic RDF graphs, and study an important problem, keyword search query over probabilistic RDF graphs (namely, the pg-KWS query). To retrieve meaningful keyword search answers, we design the score rankings for subgraph answers specific for RDF data. Furthermore, we propose effective pruning methods (via offline pre-computed score bounds and probabilistic threshold) to quickly filter out false alarms. We construct an index over the pre-computed data for RDF, and present an efficient query answering approach through the index. Extensive experiments have been conducted to verify the effectiveness and efficiency of our proposed approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145622169",
                    "name": "Zi Huang"
                }
            ]
        },
        {
            "paperId": "ff784e951325b54f911c3c82bde5b3d6810459c3",
            "title": "Hermes: Dynamic Partitioning for Distributed Social Network Graph Databases",
            "abstract": "Social networks are large graphs that require multiple graph database servers to store and manage them. Each database server hosts a graph partition with the objectives of balancing server loads, reducing remote traversals (edge-cuts), and adapting the partitioning to changes in the structure of the graph in the face of changing workloads. To achieve these objectives, a dynamic repartitioning algorithm is required to modify an existing partitioning to maintain good quality partitions while not imposing a significant overhead to the system. In this paper, we introduce a lightweight repartitioner, which dynamically modifies a partitioning using a small amount of resources. In contrast to the existing repartitioning algorithms, our lightweight repartitioner is e cient, making it suitable for use in a real system. We integrated our lightweight repartitioner into Hermes, which we designed as an extension of the open source Neo4j graph database system, to support workloads over partitioned graph data distributed over multiple servers. Using real-world social network data, we show that Hermes leverages the lightweight repartitioner to maintain high quality partitions and provides a 2 to 3 times performance improvement over the de-facto standard random hash-based partitioning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21024202",
                    "name": "D. Nicoar\u0103"
                },
                {
                    "authorId": "34399700",
                    "name": "Shahin Kamali"
                },
                {
                    "authorId": "2176381",
                    "name": "K. Daudjee"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "04272a11aa0a8c8a9fe1d47b4e64f7578211fdde",
            "title": "CrowdCleaner: Data cleaning for multi-version data on the web via crowdsourcing",
            "abstract": "Multi-version data is often one of the most concerned information on the Web since this type of data is usually updated frequently. Even though there exist some Web information integration systems that try to maintain the latest update version, the maintained multi-version data usually includes inaccurate and invalid information due to the data integration or update delay errors. In this demo, we present CrowdCleaner, a smart data cleaning system for cleaning multi-version data on the Web, which utilizes crowdsourcing-based approaches for detecting and repairing errors that usually cannot be solved by traditional data integration and cleaning techniques. In particular, CrowdCleaner blends active and passive crowdsourcing methods together for rectifying errors for multi-version data. We demonstrate the following four facilities provided by CrowdCleaner: (1) an error-monitor to find out which items (e.g., submission date, price of real estate, etc.) are wrong versions according to the reports from the crowds, which belongs to a passive crowdsourcing strategy; (2) a task-manager to allocate the tasks to human workers intelligently; (3) a smart-decision-maker to identify which answer from the crowds is correct with active crowdsourcing methods; and (4) a whom-to-ask-finder to discover which users (or human workers) should be the most credible according to their answer records.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "3151540",
                    "name": "Caleb Chen Cao"
                },
                {
                    "authorId": "50445897",
                    "name": "C. Zhang"
                },
                {
                    "authorId": "2111156066",
                    "name": "Yatao Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "07debeead4720ae9767ba477e01b04ca161159d8",
            "title": "TOMOHA: TOpic model-based HAshtag recommendation on twitter",
            "abstract": "On Twitter, hashtags are used to summarize topics of the tweet content and to help to categorize and search tweets. However, hashtags are created in a free style and thus heterogeneous, increasing difficulty of their usage. We propose TOMOHA, a supervised TOpic MOdel-based solution for HAshtag recommendation on Twitter. We treat hashtags as labels of topics, and develop a supervised topic model to discover relationship among words, hashtags and topics of tweets. We also novelly add user following relationship into the model. We infer the probability that a hashtag will be contained in a new tweet, and recommend k most probable ones. We propose parallel computing and pruning techniques to speed up model training and recommendation process. Experiments show that our method can properly and efficiently recommend hashtags.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1872111",
                    "name": "Jieying She"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "08308b1ac13874345e62cc8e6cc2c69c079577fd",
            "title": "On Concise Set of Relative Candidate Keys",
            "abstract": "Matching keys, specifying what attributes to compare and how to compare them for identifying the same real-world entities, are found to be useful in applications like record matching, blocking and windowing [7]. Owing to the complex redundant semantics among matching keys, capturing a proper set of matching keys is highly non-trivial. Analogous to minimal/candidate keys w.r.t. functional dependencies, relative candidate keys (RCKs [7], with a minimal number of compared attributes, see a more formal definition in Section 2) can clear up redundant semantics w.r.t. \"what attributes to compare\". However, we note that redundancy issues may still exist among rcks on the same attributes about \"how to compare them\". In this paper, we propose to find a concise set of matching keys, which has less redundancy and can still meet the requirements on coverage and validity. Specifically, we study approximation algorithms to efficiently discover a near optimal set. To ensure the quality of matching keys, the returned results are guaranteed to be RCKs (minimal on compared attributes), and most importantly, minimal w.r.t. distance restrictions (i.e., redundancy free w.r.t. \"how to compare the attributes\"). The experimental evaluation demonstrates that our concise RCK set is more effective than the existing rck choosing method. Moreover, the proposed pruning methods show up to 2 orders of magnitude improvement w.r.t. time costs on concise RCK set discovery.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144488755",
                    "name": "Hong Cheng"
                }
            ]
        },
        {
            "paperId": "1e2e376e979a456dfac10ff985550694e36af7ff",
            "title": "Efficient Probabilistic Supergraph Search Over Large Uncertain Graphs",
            "abstract": "In recent years, with the emergence of a number of new real applications, such as protein-protein interaction (PPI) networks, visual pattern recognition, and intelligent traffic systems, managing huge volumes of uncertain graphs has attracted much attention in the database community. Currently, most existing fundamental queries over graphs only support deterministic (or certain) graphs, although real graph data are often noisy, inaccurate, and incomplete. In this paper, we study a new type of uncertain graph query, probabilistic supergraph containment query over large uncertain graphs. Specifically, given an uncertain graph database UGD which contains a set of uncertain graphs, a deterministic query graph q, and a probabilistic threshold \u03b4, a probabilistic supergraph containment query is to find the set of uncertain graphs from UGD, denoted as UGDq, such that UGDq={ugi\u2208 UGD|Pr(ugi\u2286 q)\u2265\u03b4} where Pr(ugi\u2286q) means the likelihood that ugi is a subgraph of q. We prove that the computation of Pr(ugi\u2286q) is #P-hard and design an efficient filtering-and-verification framework to avoid the expensive computation. In particular, we propose an effective filtering strategy and a novel probabilistic inverted index, called PS-Index, to enhance pruning power in the filtering phase. Furthermore, the candidate graphs which pass the filtering phase are tested in the verification phase via an efficient unequal probability sampling-based approximation algorithm. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "2108166998",
                    "name": "Xiaofei Zhang"
                },
                {
                    "authorId": "3151540",
                    "name": "Caleb Chen Cao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "279a1e8714e8eb2b169cb4ef9a070df875fec802",
            "title": "LogGP: A Log-based Dynamic Graph Partitioning Method",
            "abstract": "With the increasing availability and scale of graph data from Web 2.0, graph partitioning becomes one of efficient preprocessing techniques to balance the computing workload. Since the cost of partitioning the entire graph is strictly prohibitive, there are some recent tentative works towards streaming graph partitioning which can run faster, be easily paralleled, and be incrementally updated. Unfortunately, the experiments show that the running time of each partitioning is still unbalanced due to the variation of workload access pattens during the supersteps. In addition, the one-pass streaming partitioning result is not always satisfactory for the algorithms' local view of the graph. \n \nIn this paper, we present LogGP, a log-based graph partitioning system that records, analyzes and reuses the historical statistical information to refine the partitioning result. LogGP can be used as a middle-ware and deployed to many state-of-the-art paralleled graph processing systems easily. LogGP utilizes the historical partitioning results to generate a hyper-graph and uses a novel hyper-graph streaming partitioning approach to generate a better initial streaming graph partitioning result. During the execution, the system uses running logs to optimize graph partitioning which prevents performance degradation. Moreover, LogGP can dynamically repartition the massive graphs in accordance with the structural changes. Extensive experiments conducted on a moderate size of computing cluster with real-world graph datasets demonstrate the superiority of our approach against the state-of-the-art solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145857599",
                    "name": "N. Xu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144585959",
                    "name": "B. Cui"
                }
            ]
        },
        {
            "paperId": "29325bdb13ab6c85469aa998babce18468064ca2",
            "title": "CrowdMatcher: crowd-assisted schema matching",
            "abstract": "Schema matching is a central challenge for data integration systems. Due to the inherent uncertainty arose from the inability of schema in fully capturing the semantics of the represented data, automatic tools are often uncertain about suggested matching results. However, human is good at understanding data represented in various forms and crowdsourcing platforms are making the human annotation process more affordable. Thus in this demo, we will show how to utilize the crowd to find the right matching. In order to do that, we need to make the tasks posted on the crowdsouricng platforms extremely simple, to be performed by non-expert people, and reduce the number of tasks as less as possible to save the cost. We demonstrate CrowdMatcher, a hybrid machine-crowd system for schema matching. The machine-generated matchings are verified by correspondence correctness queries (CCQs), which is to ask the crowd to determine whether a given correspondence is correct or not. CrowdMatcher includes several original features: it integrates different matchings generated from classical schema matching tools; in order to minimize the cost of crowdsourcing, it automatically selects the most informative set of CCQs from the possible matchings; it is able to manage inaccurate answers provided by the workers; the crowdsourced answers are used to improve matching results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50445897",
                    "name": "C. Zhang"
                },
                {
                    "authorId": "2117928058",
                    "name": "Ziyuan Zhao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "3151540",
                    "name": "Caleb Chen Cao"
                }
            ]
        },
        {
            "paperId": "2b939a996e86d6f1dbd289ac42f75cada9d384e3",
            "title": "gMission: A General Spatial Crowdsourcing Platform",
            "abstract": "As one of the successful forms of using Wisdom of Crowd, crowdsourcing, has been widely used for many human intrinsic tasks, such as image labeling, natural language understanding, market predication and opinion mining. Meanwhile, with advances in pervasive technology, mobile devices, such as mobile phones and tablets, have become extremely popular. These mobile devices can work as sensors to collect multimedia data(audios, images and videos) and location information. This power makes it possible to implement the new crowdsourcing mode: spatial crowdsourcing. In spatial crowdsourcing, a requester can ask for resources related a specific location, the mobile users who would like to take the task will travel to that place and get the data. Due to the rapid growth of mobile device uses, spatial crowdsourcing is likely to become more popular than general crowdsourcing, such as Amazon Turk and Crowdflower. However, to implement such a platform, effective and efficient solutions for worker incentives, task assignment, result aggregation and data quality control must be developed. \n \nIn this demo, we will introduce gMission, a general spatial crowdsourcing platform, which features with a collection of novel techniques, including geographic sensing, worker detection, and task recommendation. We introduce the sketch of system architecture and illustrate scenarios via several case analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111605555",
                    "name": "Zhao Chen"
                },
                {
                    "authorId": "2054309467",
                    "name": "Rui Fu"
                },
                {
                    "authorId": "2117928058",
                    "name": "Ziyuan Zhao"
                },
                {
                    "authorId": "2145976421",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "20148726",
                    "name": "Leihao Xia"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "3151540",
                    "name": "Caleb Chen Cao"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "50445897",
                    "name": "C. Zhang"
                }
            ]
        },
        {
            "paperId": "3162f0db8ceaaedeed0eb104fd67ffe373163bf0",
            "title": "C-DMr: Crowd-powered Decision Maker for real world Knapsack Problems",
            "abstract": "Knapsack problems range over a large sphere of real world challenges [?]. For example, every year a professor has to decide her new \u201csquad\u201d of students/staff from possibly hundreds of candidates, while having a restricted budget of funding in consideration. Moreover, in many cases, she has to resort to her colleagues and senior students to make comparisons among the candidates. The difficulties of such tasks are mainly three-fold: 1) the knowledge about the candidates are distributed among a crowd; 2) the underlying factors are human-intrinsic and hard to be formatted; 3) the size of candidates exceeds the capacity of human for a one-shot decision. Other examples in this category include gear set preparation for a venture trip, syllabus design for a popular course and inventory design for goods shelf, where the two difficulties are commonly observed. Consequently, a person may be heavily entangled to work out a final decision, which may even be inaccurate. Driven by this demand, in this demo, we present C-DMr - a Crowd-powered Decision Maker that incorporates the wisdom of the informed crowds to solve such real world Knapsack Problems. The core module of this web-based system is a set of algorithms along with a novel interactive interface. The interface incrementally presents comparison jobs and motivates the crowd to participate with a rewarding mechanism, and the set of algorithms solves the Knapsack Problem given only pairwise preferences among candidates. We demonstrate the novelty and usefulness of C-DMr by forming a aforementioned \u201csquad\u201d for a recruiting professor. Specifically four functionalities are shown: 1) a Candidates Entrance that collects the information about all candidates; 2) a Jury Trial that facilitates informed crowds to contribute preferences; 3) an Knapsack Analyzer that measures the on-going \u201csquad\u201d; and 4) a Consultant that recommends a final set of candidates to the professor.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "20148726",
                    "name": "Leihao Xia"
                },
                {
                    "authorId": "3151540",
                    "name": "Caleb Chen Cao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2111605555",
                    "name": "Zhao Chen"
                }
            ]
        },
        {
            "paperId": "3a263773a77381bfa1a8876a01809985976142b2",
            "title": "Free Market of Crowdsourcing: Incentive Mechanism Design for Mobile Sensing",
            "abstract": "Off-the-shelf smartphones have boosted large scale participatory sensing applications as they are equipped with various functional sensors, possess powerful computation and communication capabilities, and proliferate at a breathtaking pace. Yet the low participation level of smartphone users due to various resource consumptions, such as time and power, remains a hurdle that prevents the enjoyment brought by sensing applications. Recently, some researchers have done pioneer works in motivating users to contribute their resources by designing incentive mechanisms, which are able to provide certain rewards for participation. However, none of these works considered smartphone users' nature of opportunistically occurring in the area of interest. Specifically, for a general smartphone sensing application, the platform would distribute tasks to each user on her arrival and has to make an immediate decision according to the user's reply. To accommodate this general setting, we design three online incentive mechanisms, named TBA, TOIM and TOIMAD, based on online reverse auction. TBA is designed to pursue platform utility maximization, while TOIM and TOIM-AD achieve the crucial property of truthfulness. All mechanisms possess the desired properties of computational efficiency, individual rationality, and profitability. Besides, they are highly competitive compared to the optimal offline solution. The extensive simulation results reveal the impact of the key parameters and show good approximation to the state-of-the-art offline mechanism.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2468508",
                    "name": "Xinglin Zhang"
                },
                {
                    "authorId": "40176711",
                    "name": "Zheng Yang"
                },
                {
                    "authorId": "2735715",
                    "name": "Zimu Zhou"
                },
                {
                    "authorId": "1694217",
                    "name": "Haibin Cai"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "46218395",
                    "name": "Xiangyang Li"
                }
            ]
        },
        {
            "paperId": "3e9388fa1f504c5a13ec20402f475fd0dd56dc7e",
            "title": "Towards effective and efficient mining of arbitrary shaped clusters",
            "abstract": "Mining arbitrary shaped clusters in large data sets is an open challenge in data mining. Various approaches to this problem have been proposed with high time complexity. To save computational cost, some algorithms try to shrink a data set size to a smaller amount of representative data examples. However, their user-defined shrinking ratios may significantly affect the clustering performance. In this paper, we present CLASP an effective and efficient algorithm for mining arbitrary shaped clusters. It automatically shrinks the size of a data set while effectively preserving the shape information of clusters in the data set with representative data examples. Then, it adjusts the positions of these representative data examples to enhance their intrinsic relationship and make the cluster structures more clear and distinct for clustering. Finally, it performs agglomerative clustering to identify the cluster structures with the help of a mutual k-nearest neighbors-based similarity metric called Pk. Extensive experiments on both synthetic and real data sets are conducted, and the results verify the effectiveness and efficiency of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51211749",
                    "name": "H. Huang"
                },
                {
                    "authorId": "1409828392",
                    "name": "Yunjun Gao"
                },
                {
                    "authorId": "1887215",
                    "name": "K. Chiew"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "8159757",
                    "name": "Qinming He"
                }
            ]
        },
        {
            "paperId": "536d867754889d96265972eb2722ef05c58d2a4c",
            "title": "From labor to trader: opinion elicitation via online crowds as a market",
            "abstract": "We often care about people's degrees of belief about certain events: e.g. causality between an action and the outcomes, odds distribution among the outcome of a horse race and so on. It is well recognized that the best form to elicit opinion from human is probability distribution instead of simple voting, because the form of distribution retains the delicate information that an opinion expresses. In the past, opinion elicitation has relied on experts, who are expensive and not always available. More recently, crowdsourcing has gained prominence as an inexpensive way to get a great deal of human input. However, traditional crowdsourcing has primarily focused on issuing very simple (e.g. binary decision) tasks to the crowd. In this paper, we study how to use crowds for Opinion Elicitation. There are three major challenges to eliciting opinion information in the form of probability distributions: how to measure the quality of distribution; how to aggregate the distributions; and, how to strategically implement such a system. To address these challenges, we design and implement COPE Crowd-powered OPinion Elicitation market. COPE models crowdsourced work as a trading market, where the \"workers\" behave like \"traders\" to maximize their profit by presenting their opinion. Among the innovative features in this system, we design COPE updating to combine the multiple elicited distributions following a Bayesian scheme. Also to provide more flexibility while running COPE, we propose a series of efficient algorithms and a slope based strategy to manage the ending condition of COPE. We then demonstrate the implementation of COPE and report experimental results running on real commercial platform to demonstrate the practical value of this system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3151540",
                    "name": "Caleb Chen Cao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                }
            ]
        },
        {
            "paperId": "76e33210f37e39f2f96107c4a5f60a524523e8c9",
            "title": "Proceedings of the 2014 SIGMOD PhD symposium",
            "abstract": "It is our great pleasure to welcome you to the 2014 ACM SIGMOD/PODS Ph.D. Symposium. This year's symposium continues its tradition, which is intended to bring together Ph.D. students working on topics related to the SIGMOD and PODS conference. The workshop will offer Ph.D. students the opportunity to present, discuss, and receive feedback on their research in a constructive and international atmosphere. The workshop will be accompanied by prominent professors, researchers and practitioners in the fields of database technology. These accompanying professors will participate actively and contribute to the discussions. The call for papers attracted 13 submissions from Asia, Canada, Europe, and the United States. The program committee accepted 10 proposals that cover a variety of topics, including stream and large scale data processing, query language and query processing. In addition, the program includes a keynote speech given by Dr. Divesh Srivastava, head of the Database Research Department at AT&T Labs-Research. We hope that these proceedings will serve as a valuable reference for Ph.D. students.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35106192",
                    "name": "P. Barcel\u00f3"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "786a1ef5a7aebc0b50767802a0ef73c20838b6c0",
            "title": "Trip Planner Over Probabilistic Time-Dependent Road Networks",
            "abstract": "Recently, the management of transportation systems has become increasingly important in many real applications such as location-based services, supply chain management, traffic control, and so on. These applications usually involve queries over spatial road networks with dynamically changing and complicated traffic conditions. In this paper, we model such a network by a probabilistic time-dependent graph (PT-Graph), whose edges are associated with uncertain delay functions. We propose a useful query in the PT-Graph, namely a trip planner query (TPQ), which retrieves trip plans that traverse a set of query points in PT-Graph, having the minimum traveling time with high confidence. To tackle the efficiency issue, we present the pruning methods time interval pruning and probabilistic pruning to effectively rule out false alarms of trip plans. Furthermore, we design a pre-computation technique based on the cost model and construct an index structure over the pre-computed data to enable the pruning via the index. We integrate our proposed pruning methods into an efficient query procedure to answer TPQs. Through extensive experiments, we demonstrate the efficiency and effectiveness of our TPQ query answering approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "7b7bac025c0a725f60a3d27f89eda72507d72a6c",
            "title": "Anomaly Detection from Incomplete Data",
            "abstract": "Anomaly detection (a.k.a., outlier or burst detection) is a well-motivated problem and a major data mining and knowledge discovery task. In this article, we study the problem of population anomaly detection, one of the key issues related to event monitoring and population management within a city. Through studying detected population anomalies, we can trace and analyze these anomalies, which could help to model city traffic design and event impact analysis and prediction. Although a significant and interesting issue, it is very hard to detect population anomalies and retrieve anomaly trajectories, especially given that it is difficult to get actual and sufficient population data. To address the difficulties of a lack of real population data, we take advantage of mobile phone networks, which offer enormous spatial and temporal communication data on persons. More importantly, we claim that we can utilize these mobile phone data to infer and approximate population data. Thus, we can study the population anomaly detection problem by taking advantages of unique features hidden in mobile phone data. In this article, we present a system to conduct Population Anomaly Detection (PAD). First, we propose an effective clustering method, correlation-based clustering, to cluster the incomplete location information from mobile phone data (i.e., from mobile call volume distribution to population density distribution). Then, we design an adaptive parameter-free detection method, R-scan, to capture the distributed dynamic anomalies. Finally, we devise an efficient algorithm, BT-miner, to retrieve anomaly trajectories. The experimental results from real-life mobile phone data confirm the effectiveness and efficiency of the proposed algorithms. Finally, the proposed methods are realized as a pilot system in a city in China.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47130133",
                    "name": "Siyuan Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1726587",
                    "name": "L. Ni"
                }
            ]
        },
        {
            "paperId": "816b7505efe7493f426dde76d1e82fdd35536822",
            "title": "Efficient Determination of Distance Thresholds for Differential Dependencies",
            "abstract": "The importance of introducing distance constraints to data dependencies, such as differential dependencies (DDs), has recently been recognized. The differential dependencies are tolerant to small variations, which enable them to apply to wide data quality checking applications, such as detecting data violations. However, the determination of distance thresholds for the differential dependencies is non-trivial. It often relies on a truth data instance which embeds the distance constraints. To find useful distance threshold patterns from data, there are several guidelines of statistical measures to specify, e.g., support, confidence and dependent quality. Unfortunately, given a data instance, users might not have any knowledge about the data distribution, thus it is very challenging to set the right parameters. In this paper, we study the determination of distance thresholds for differential dependencies, in a parameter-free style. Specifically, we compute an expected utility based on the statistical measures from the data. According to our analysis as well as experimental verification, distance threshold patterns with higher expected utility could offer better use in real applications, such as violation detection. We then develop efficient algorithms to determine the distance thresholds having the maximum expected utility. Finally, our extensive experimental evaluation demonstrates the effectiveness and efficiency of the proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144488755",
                    "name": "Hong Cheng"
                }
            ]
        },
        {
            "paperId": "8cc70a48d8b28563d0e1d817140504b81cc5d151",
            "title": "Reliable Diversity-Based Spatial Crowdsourcing by Moving Workers",
            "abstract": "With the rapid development of mobile devices and the crowdsourcing platforms, the spatial crowdsourcing has attracted much attention from the database community, specifically, spatial crowdsourcing refers to sending a location-based request to workers according to their positions. In this paper, we consider an important spatial crowdsourcing problem, namely reliable diversity-based spatial crowdsourcing (RDB-SC), in which spatial tasks (such as taking videos/photos of a landmark or firework shows, and checking whether or not parking spaces are available) are time-constrained, and workers are moving towards some directions. Our RDB-SC problem is to assign workers to spatial tasks such that the completion reliability and the spatial/temporal diversities of spatial tasks are maximized. We prove that the RDB-SC problem is NP-hard and intractable. Thus, we propose three effective approximation approaches, including greedy, sampling, and divide-and-conquer algorithms. In order to improve the efficiency, we also design an effective cost-model-based index, which can dynamically maintain moving workers and spatial tasks with low cost, and efficiently facilitate the retrieval of RDB-SC answers. Through extensive experiments, we demonstrate the efficiency and effectiveness of our proposed approaches over both real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144632798",
                    "name": "Peng Cheng"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "2111605555",
                    "name": "Zhao Chen"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1783892",
                    "name": "Jinsong Han"
                },
                {
                    "authorId": "103567432",
                    "name": "Jizhong Zhao"
                }
            ]
        },
        {
            "paperId": "95ee65d8775324be6340ce8baf9b0a8749886c5c",
            "title": "Pattern Match Query in a Large Uncertain Graph",
            "abstract": "Many studies have been conducted on seeking an efficient solution for pattern matching over graphs. This interest is largely due to large number of applications in many fields, which require efficient solutions for pattern matching, including protein complex prediction, social network analysis and structural pattern recognition. However, in many real applications, the graph data are often noisy, incomplete, and inaccurate. In other words, there exist many uncertain graphs. Therefore, in this paper, we study pattern matching in a large uncertain graph. Specifically, we want to retrieve all qualified matches of a query pattern in the uncertain graph. Though pattern matching over an uncertain graph is NP-hard, we employ a filtering-and verification framework to speed up the search. In the filtering phase, we propose a probabilistic matching tree, PM-tree, based on match cuts obtained by a cut selection process. Based on PM-tree, we devise a collective pruning strategy to prune a large number of unqualified matches. During the verification phase, we develop an efficient sampling algorithm to validate the remaining candidates. Extensive experimental results demonstrate the effectiveness and efficiency of the proposed algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "a998109ffd68dc94fc0f8967222e2b0a62be3554",
            "title": "MaC: A Probabilistic Framework for Query Answering with Machine-Crowd Collaboration",
            "abstract": "The popularity of crowdsourcing has recently brought about brand new opportunities for engaging human intelligence in the process of data analysis. Most existing works on crowdsourcing have developed sophisticated methods to utilize the crowd as a new kind of processor, a.k.a. Human Processor Units (HPU). In this paper, we propose a framework, called MaC, to combine the powers of both CPUs and HPUs. In order to build MaC, we need to tackle the following two challenges: (1) HIT Selection: Selecting the \"right\" HITs (Human Intelligent Tasks) can help reducing the uncertainty significantly and the results can converge quickly. Thus, we propose an entropy-based model to evaluate the informativeness of HITs. Furthermore, we find that selecting HITs has factorial complexity and the optimization function is non-linear, thus, we propose an efficient approximation algorithm with a bounded error. (2) Uncertainty Management: Crowdsourced answers can be inaccurate. To address this issue, we provide effective solutions in three common scenarios of crowdsourcing: (a) the answer and the confidence of each worker are available; (b) the confidence of each worker and the voting score for each HIT are available; (c) only the answer of each worker is available. To verify the effectiveness of the MaC framework, we built a hybrid Machine-Crowd system and tested it on three real-world applications - data fusion, information extraction and pattern recognition. The experimental results verified the effectiveness and the applicability of our framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50445897",
                    "name": "C. Zhang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                }
            ]
        },
        {
            "paperId": "b7e44eb637356570af6e980657d461d0d3266bfa",
            "title": "Where To: Crowd-Aided Path Selection",
            "abstract": "With the widespread use of geo-positioning services (GPS), GPS-based navigation systems have become ever more of an integral part of our daily lives. GPS-based navigation systems usually suggest multiple paths for any given pair of source and target, leaving users perplexed when trying to select the best one among them, namely the problem of best path selection. Too many suggested paths may jeopardize the usability of the recommendation data, and decrease user satisfaction. Although existing studies have already partially relieved this problem through integrating historical traffic logs or updating traffic conditions periodically, their solutions neglect the potential contribution of human experience. \n \nIn this paper, we resort to crowdsourcing to ease the pain of the best path selection. The first step of appropriately using the crowd is to ask proper questions. For the best path selection problem, simple questions (e.g. binary voting) over compete paths cannot be directly applied to road networks due to their being too complex for crowd workers. Thus, this paper makes the first contribution by designing two types of questions, namely Routing Query (RQ) and Binary Routing Query (BRQ), to ask the crowd to decide which direction to take at each road intersection. Furthermore, we propose a series of efficient algorithms to dynamically manage the questions in order to reduce the selection hardness within a limited budget. Finally, we compare the proposed methods against two baselines, and the effectiveness and efficiency of our proposals are verified by the results from simulations and experiments on a real-world crowdsourcing platform.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50445897",
                    "name": "C. Zhang"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "be1afabd7729c19693a2cffe283809922b7fefe4",
            "title": "Efficient cohesive subgraphs detection in parallel",
            "abstract": "A cohesive subgraph is a primary vehicle for massive graph analysis, and a newly introduced cohesive subgraph, k-truss, which is motivated by a natural observation of social cohesion, has attracted more and more attention. However, the existing parallel solutions to identify the k-truss are inefficient for very large graphs, as they still suffer from huge communication cost and large number of iterations during the computation. In this paper, we propose a novel parallel and efficient truss detection algorithm, called PeTa. The PeTa produces a triangle complete subgraph (TC-subgraph) for every computing node. Based on the TC-subgraphs, PeTa can detect the local k-truss in parallel within a few iterations. We theoretically prove, within this new paradigm, the communication cost of PeTa is bounded by three times of the number of triangles, the total computation complexity of PeTa is the same order as the best known serial algorithm and the number of iterations for a given partition scheme is minimized as well. Furthermore, we present a subgraph-oriented model to efficiently express PeTa in parallel graph computing systems. The results of comprehensive experiments demonstrate, compared with the existing solutions, PeTa saves 2X to 19X in communication cost, reduces 80% to 95% number of iterations and improves the overall performance by 80% across various real-world graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237813",
                    "name": "Yingxia Shao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144585959",
                    "name": "B. Cui"
                }
            ]
        },
        {
            "paperId": "c1deca1cfc3c86cdbaae1326bfd8a0533c17a36c",
            "title": "Wonder: Efficient Tag Identification for Large-Scale RFID Systems",
            "abstract": "Efficient tag identification is fundamentally required in large-scale RFID systems. Tag signal collision degrades identification efficiency as tag IDs involved in collision cannot be decoded. The situation becomes even worse in large-scale RFID systems when tag cardinality booms. Existing anti-collision protocols focus on either reducing collision probability or adopting spread spectrum techniques. Unfortunately, the former approach cannot resolve collision radically and the latter one occupies extra bandwidth resources. To address these issues, we propose to resolve tag collision using orthogonal Walsh code, in which tags map their IDs to a group of Walsh codes and transmit them sequentially. The reader can retrieve tag IDs by inverse mapping even under collision circumstances. We further design a new efficient tag identification protocol, Wonder, which reduces identification time without spreading the bandwidth. We conduct extensive simulations to examine its effectiveness and the results show that our protocol significantly improves identification efficiency over previous anti-collision protocols.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2452544",
                    "name": "Haoxiang Liu"
                },
                {
                    "authorId": "46578147",
                    "name": "Kebin Liu"
                },
                {
                    "authorId": "2055685016",
                    "name": "Wei Gong"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "c21875fd7451a162e27906e12f2fc3da1ab5b89a",
            "title": "Processing SPARQL Queries Over Linked Data-A Distributed Graph-based Approach",
            "abstract": "We propose techniques for processing SPARQL queries over linked data. We follow a graph-based approach where answering a query Q is equivalent to finding its matches over a distributed RDF data graph G. We adopt a \u201cpartial evaluation and assembly\u201d framework. Partial evaluation results of query Q over each repository\u2014called local partial match\u2014are found. In the assembly stage, we pro- pose a centralized and a distributed assembly strategy. We analyze our algorithms both theoretically and the experimentally. Extensive experiments over both real and benchmark RDF repositories with billion triples demonstrate the high performance and scalability of our methods compared with that of the existing solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144189270",
                    "name": "Peng Peng"
                },
                {
                    "authorId": "144554647",
                    "name": "Lei Zou"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144060462",
                    "name": "Dongyan Zhao"
                }
            ]
        },
        {
            "paperId": "c5a108bcc104252b94c8bf9e37ee847ebe0e82cf",
            "title": "TCS: efficient topic discovery over crowd-oriented service data",
            "abstract": "In recent years, with the widespread usage of Web 2.0 techniques, crowdsourcing plays an important role in offering human intelligence in various service websites, such as Yahoo! Answer and Quora. With the increasing amount of crowd-oriented service data, an important task is to analyze latest hot topics and track topic evolution over time. However, the existing techniques in text mining cannot effectively work due to the unique structure of crowd-oriented service data, task-response pairs, which consists of the task and its corresponding responses. In particular, existing approaches become ineffective with the ever-increasing crowd-oriented service data that accumulate along the time. In this paper, we first study the problem of discovering topics over crowd-oriented service data. Then we propose a new probabilistic topic model, the Topic Crowd Service Model (TCS model), to effectively discover latent topics from massive crowd-oriented service data. In particular, in order to train TCS efficiently, we design a novel parameter inference algorithm, the Bucket Parameter Estimation (BPE), which utilizes belief propagation and a new sketching technique, called Pairwise Sketch (pSketch). Finally, we conduct extensive experiments to verify the effectiveness and efficiency of the TCS model and the BPE algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "3151540",
                    "name": "Caleb Chen Cao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "d1f0283d5eec32ec04b823fa6345cd354cb1e31e",
            "title": "Parallel subgraph listing in a large-scale graph",
            "abstract": "Subgraph listing is a fundamental operation to many graph and network analyses. The problem itself is computationally expensive and is well-studied in centralized processing algorithms. However, the centralized solutions cannot scale well to large graphs. Recently, several parallel approaches are introduced to handle the large graphs. Unfortunately, these parallel approaches still rely on the expensive join operations, thus cannot achieve high performance. In this paper, we design a novel parallel subgraph listing framework, named PSgL. The PSgL iteratively enumerates subgraph instances and solves the subgraph listing in a divide-and-conquer fashion. The framework completely relies on the graph traversal, and avoids the explicit join operation. Moreover, in order to improve its performance, we propose several solutions to balance the workload and reduce the size of intermediate results. Specially, we prove the problem of partial subgraph instance distribution for workload balance is NP-hard, and carefully design a set of heuristic strategies. To further reduce the enormous intermediate results, we introduce three independent mechanisms, which are automorphism breaking of the pattern graph, initial pattern vertex selection based on a cost model, and a pruning method based on a light-weight index. We have implemented the prototype of PSgL, and run comprehensive experiments of various graph listing operations on diverse large graphs. The experiments clearly demonstrate that PSgL is robust and can achieve performance gain over the state-of-the-art solutions up to 90%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237813",
                    "name": "Yingxia Shao"
                },
                {
                    "authorId": "144585959",
                    "name": "B. Cui"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2115503235",
                    "name": "Lin Ma"
                },
                {
                    "authorId": "49781824",
                    "name": "Junjie Yao"
                },
                {
                    "authorId": "145857599",
                    "name": "N. Xu"
                }
            ]
        },
        {
            "paperId": "d5b3c9a5ff7363eb1dd8724a8e881d4e522a4284",
            "title": "Repairing Vertex Labels under Neighborhood Constraints",
            "abstract": "A broad class of data, ranging from similarity networks, workflow networks to protein networks, can be modeled as graphs with data values as vertex labels. The vertex labels (data values) are often dirty for various reasons such as typos or erroneous reporting of results in scientific experiments. Neighborhood constraints, specifying label pairs that are allowed to appear on adjacent vertexes in the graph, are employed to detect and repair erroneous vertex labels. In this paper, we study the problem of repairing vertex labels to make graphs satisfy neighborhood constraints. Unfortunately, the relabeling problem is proved to be NP hard, which motivates us to devise approximation methods for repairing, and identify interesting special cases (star and clique constraints) that can be efficiently solved. We propose several approximate repairing algorithms including greedy heuristics, contraction method and a hybrid approach. The performances of algorithms are also analyzed for the special case. Our extensive experimental evaluation, on both synthetic and real data, demonstrates the effectiveness of eliminating frauds in several types of application networks. Remarkably, the hybrid method performs well in practice, i.e., guarantees termination, while achieving high effectiveness at the same time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "144488755",
                    "name": "Hong Cheng"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "da2085bba385b66a220528b0e8eda38b7fe73386",
            "title": "How to Conduct Distributed IncompletePattern Matching",
            "abstract": "In this paper, we first propose a very interesting and practical problem, pattern matching in a distributed mobile environment. Pattern matching is a well-known problem and extensive research has been conducted for performing effective and efficient search. However, previous proposed approaches assume that data are centrally stored, which is not the case in a mobile environment (e.g., mobile phone networks), where one person's pattern could be separately stored in a number of different stations, and such a local pattern is incomplete compared with the global pattern. A simple solution to pattern matching over a mobile environment is to collect all the data distributed in base stations to a data center and conduct pattern matching at the data center afterwards. Clearly, such a simple solution will raise huge amount of communication traffic, which could cause the communication bottleneck brought by the limited wireless bandwidth to be even worse. Therefore, a communication efficient and search effective solution is necessary. In our work, we present a novel solution which is based on our well-designed weighted bloom filter (WBF), called, Distributed Incomplete pattern matching ( DI-matching), to find target patterns over a distributed mobile environment. Specifically, to save communication cost and ensure pattern matching in distributed incomplete patterns, we use WBF to encode a query pattern and disseminate the encoded data to each base station. Each base station conducts a local pattern search according to the received WBF. Only qualified IDs and corresponding weights in each base station are sent to the data center for aggregation and verification. Through non-trivial theoretical analysis and extensive empirical experiments on a real city-scale mobile networks data set, we demonstrate the effectiveness and efficiency of our proposed solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47130133",
                    "name": "Siyuan Liu"
                },
                {
                    "authorId": "2055048199",
                    "name": "Lei Kang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1726587",
                    "name": "L. Ni"
                }
            ]
        },
        {
            "paperId": "ebbc9b0a9d539cabd60fc1be2edfec02bcc7c109",
            "title": "Generic Composite Counting in RFID Systems",
            "abstract": "Counting the number of RFID tags is a fundamental issue and has a wide range of applications in RFID systems. Most existing protocols, however, only apply to the scenario where a single reader counts the number of tags covered by its radio, or at most the union of tags covered by multiple readers. They are unable to achieve more complex counting objectives, i.e., counting the number of tags in a composite set expression such as (S_1 big cup S_2) - (S_3 big cap S_4). This type of counting has realistic significance since it provides more diversity than existing counting scenario, and can be applied in various applications. In this paper, we formally introduce the RFID composite counting problem, which aims at counting the tags in arbitrary set expression. We obtain strong lower bounds on the communication cost of composite counting. We then propose a generic Composite Counting Framework (CCF) that provides estimates for any set expression with desired accuracy. The communication cost of CCF is proved to be within a small factor from the optimal. We build a prototype system for CCF using USRP software defined radio and Intel WISP computational tags. Also, extensive simulations are conducted to evaluate the performance of CCF. The experimental results show that CCF is generic, accurate and time-efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2452544",
                    "name": "Haoxiang Liu"
                },
                {
                    "authorId": "2055685016",
                    "name": "Wei Gong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "50101686",
                    "name": "Wenbo He"
                },
                {
                    "authorId": "46578147",
                    "name": "Kebin Liu"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                }
            ]
        },
        {
            "paperId": "0441d9a99831d50626c4145df3f08e4fe033f2f2",
            "title": "Causality and responsibility: probabilistic queries revisited in uncertain databases",
            "abstract": "Recently, due to ubiquitous data uncertainty in many real-life applications, it has become increasingly important to study efficient and effective processing of various probabilistic queries over uncertain data, which usually retrieve uncertain objects that satisfy query predicates with high probabilities. However, one annoying, yet challenging, problem is that, some probabilistic queries are very sensitive to low-quality objects in uncertain databases, and the returned query answers might miss some important results (due to low data quality). To identify both accurate query answers and those potentially low-quality objects, in this paper, we investigate the causes of query answers/non-answers from a novel angle of causality and responsibility (CR), and propose a new interpretation of probabilistic queries. Particularly, we focus on the problem of CR-based probabilistic nearest neighbor (CR-PNN) query, and design a general framework for answering CR-based queries (including CR-PNN), which can return both query answers with high confidences and low-quality objects that may potentially affect query results (for data cleaning purposes). To efficiently process CR-PNN queries, we propose effective pruning strategies to quickly filter out false alarms, and design efficient algorithms to obtain CR-PNN answers. Extensive experiments have been conducted to verify the efficiency and effectiveness of our proposed approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "1b7aa565a15bb56bd09f5ec0345951e81f3d07d2",
            "title": "ASVTDECTOR: A practical near duplicate video retrieval system",
            "abstract": "In this paper, we present a system, named ASVT-DECTOR, to retrieve the near duplicate videos with large variations based on an 3D structure tensor model, named ASVT series, over the local descriptors of video segments. Different from the traditional global feature-based video detection systems that incur severe information loss, ASVT model is built over the local descriptor set of each video segment, keeping the robustness of local descriptors. Meanwhile, unlike the traditional local feature-based methods that suffer from the high cost of pair-wise descriptor comparison, ASVT model describes a video segment as an 3D structure tensor that is actually a 3\u00d73 matrix, obtaining high retrieval efficiency. In this demonstration, we show that, given a clip, our ASVTDETECTOR system can effectively find the near-duplicates with large variations from a large collection in real time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "35414e31c6766ba917b5d1460b6c75128130c326",
            "title": "Toward Efficient Filter Privacy-Aware Content-Based Pub/Sub Systems",
            "abstract": "In recent years, the content-based publish/subscribe [12], [22] has become a popular paradigm to decouple information producers and consumers with the help of brokers. Unfortunately, when users register their personal interests to the brokers, the privacy pertaining to filters defined by honest subscribers could be easily exposed by untrusted brokers, and this situation is further aggravated by the collusion attack between untrusted brokers and compromised subscribers. To protect the filter privacy, we introduce an anonymizer engine to separate the roles of brokers into two parts, and adapt the k-anonymity and `-diversity models to the contentbased pub/sub. When the anonymization model is applied to protect the filter privacy, there is an inherent tradeoff between the anonymization level and the publication redundancy. By leveraging partial-order-based generalization of filters to track filters satisfying k-anonymity and \u2113-diversity, we design algorithms to minimize the publication redundancy. Our experiments show the proposed scheme, when compared with studied counterparts, has smaller forwarding cost while achieving comparable attack resilience.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144084147",
                    "name": "Weixiong Rao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1759241",
                    "name": "Sasu Tarkoma"
                }
            ]
        },
        {
            "paperId": "425cc98dc13580d9bc37441d3760b05378736af6",
            "title": "Protecting Sensitive Labels in Social Network Data Anonymization",
            "abstract": "Privacy is one of the major concerns when publishing or sharing social network data for social science research and business analysis. Recently, researchers have developed privacy models similar to k-anonymity to prevent node reidentification through structure information. However, even when these privacy models are enforced, an attacker may still be able to infer one's private information if a group of nodes largely share the same sensitive labels (i.e., attributes). In other words, the label-node relationship is not well protected by pure structure anonymization methods. Furthermore, existing approaches, which rely on edge editing or node clustering, may significantly alter key graph properties. In this paper, we define a k-degree-l-diversity anonymity model that considers the protection of structural information as well as sensitive labels of individuals. We further propose a novel anonymization methodology based on adding noise nodes. We develop a new algorithm by adding noise nodes into the original graph with the consideration of introducing the least distortion to graph properties. Most importantly, we provide a rigorous analysis of the theoretical bounds on the number of noise nodes added and their impacts on an important graph property. We conduct extensive experiments to evaluate the effectiveness of the proposed technique.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688812",
                    "name": "Mingxuan Yuan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144019071",
                    "name": "Philip S. Yu"
                },
                {
                    "authorId": "144478228",
                    "name": "Ting Yu"
                }
            ]
        },
        {
            "paperId": "61677a5e593fc54493990b32d909807036c5a667",
            "title": "Bitlist: New Full-text Index for Low Space Cost and Efficient Keyword Search",
            "abstract": "Nowadays Web search engines are experiencing significant performance challenges caused by a huge amount of Web pages and increasingly larger number of Web users. The key issue for addressing these challenges is to design a compact structure which can index Web documents with low space and meanwhile process keyword search very fast. Unfortunately, the current solutions typically separate the space optimization from the search improvement. As a result, such solutions either save space yet with search inefficiency, or allow fast keyword search but with huge space requirement. In this paper, to address the challenges, we propose a novel structure bitlist with both low space requirement and supporting fast keyword search. Specifically, based on a simple and yet very efficient encoding scheme, bitlist uses a single number to encode a set of integer document IDs for low space, and adopts fast bitwise operations for very efficient boolean-based keyword search. Our extensive experimental results on real and synthetic data sets verify that bitlist outperforms the recent proposed solution, inverted list compression [23, 22] by spending 36.71% less space and 61.91% faster processing time, and achieves comparable running time as [8] but with significantly lower space.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144084147",
                    "name": "Weixiong Rao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "143966172",
                    "name": "P. Hui"
                },
                {
                    "authorId": "1759241",
                    "name": "Sasu Tarkoma"
                }
            ]
        },
        {
            "paperId": "7e8af0ece40de50da273bb7b1b81ed684a94fa9e",
            "title": "Efficient Keyword Search on Uncertain Graph Data",
            "abstract": "As a popular search mechanism, keyword search has been applied to retrieve useful data in documents, texts, graphs, and even relational databases. However, so far, there is no work on keyword search over uncertain graph data even though the uncertain graphs have been widely used in many real applications, such as modeling road networks, influential detection in social networks, and data analysis on PPI networks. Therefore, in this paper, we study the problem of top-k keyword search over uncertain graph data. Following the similar answer definition for keyword search over deterministic graphs, we consider a subtree in the uncertain graph as an answer to a keyword query if 1) it contains all the keywords; 2) it has a high score (defined by users or applications) based on keyword matching; and 3) it has low uncertainty. Keyword search over deterministic graphs is already a hard problem as stated in [1], [2], [3]. Due to the existence of uncertainty, keyword search over uncertain graphs is much harder. Therefore, to improve the search efficiency, we employ a filtering-and-verification strategy based on a probabilistic keyword index, PKIndex. For each keyword, we offline compute path-based top-k probabilities, and attach these values to PKIndex in an optimal, compressed way. In the filtering phase, we perform existence, path-based and tree-based probabilistic pruning phases, which filter out most false subtrees. In the verification, we propose a sampling algorithm to verify the candidates. Extensive experimental results demonstrate the effectiveness of the proposed algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2109590665",
                    "name": "Haixun Wang"
                }
            ]
        },
        {
            "paperId": "844f6472ef8a4b4869f11907f559a1b4c164d1b2",
            "title": "Probabilistic Nearest Neighbor Queries on Uncertain Moving Object Trajectories",
            "abstract": "Nearest neighbor (NN) queries in trajectory databases have received significant attention in the past, due to their applications in spatio-temporal data analysis. More recent work has considered the realistic case where the trajectories are uncertain; however, only simple uncertainty models have been proposed, which do not allow for accurate probabilistic search. In this paper, we fill this gap by addressing probabilistic nearest neighbor queries in databases with uncertain trajectories modeled by stochastic processes, specifically the Markov chain model. We study three nearest neighbor query semantics that take as input a query state or trajectory q and a time interval, and theoretically evaluate their runtime complexity. Furthermore we propose a sampling approach which uses Bayesian inference to guarantee that sampled trajectories conform to the observation data stored in the database. This sampling approach can be used in Monte-Carlo based approximation solutions. We include an extensive experimental study to support our theoretical results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2351013",
                    "name": "Johannes Niedermayer"
                },
                {
                    "authorId": "3221975",
                    "name": "Andreas Z\u00fcfle"
                },
                {
                    "authorId": "2277396",
                    "name": "Tobias Emrich"
                },
                {
                    "authorId": "1723035",
                    "name": "M. Renz"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1688561",
                    "name": "H. Kriegel"
                }
            ]
        },
        {
            "paperId": "9ba137798b2f9714b35180e4e350bbb6bf0b21d3",
            "title": "EAGRE: Towards scalable I/O efficient SPARQL query evaluation on the cloud",
            "abstract": "To benefit from the Cloud platform's unlimited resources, managing and evaluating huge volume of RDF data in a scalable manner has attracted intensive research efforts recently. Progresses have been made on evaluating SPARQL queries with either high-level declarative programming languages, like Pig [1], or a sequence of sophisticated designed MapReduce jobs, both of which tend to answer the query with multiple join operations. However, due to the simplicity of Cloud storage and the coarse organization of RDF data in existing solutions, multiple join operations easily bring significant I/O and network traffic which can severely degrade the system performance. In this work, we first propose EAGRE, an Entity-Aware Graph compREssion technique to form a new representation of RDF data on Cloud platforms, based on which we propose an I/O efficient strategy to evaluate SPARQL queries as quickly as possible, especially queries with specified solution sequence modifiers, e.g., PROJECTION, ORDER BY, etc. We implement a prototype system and conduct extensive experiments over both real and synthetic datasets on an in-house cluster. The experimental results show that our solution can achieve over an order of magnitude of time saving for the SPARQL query evaluation compared to the state-of-art MapReduce-based solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108166998",
                    "name": "Xiaofei Zhang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "143894230",
                    "name": "Min Wang"
                }
            ]
        },
        {
            "paperId": "a2fa400bcd21d75fa1b73a6c2771fcc7e0396300",
            "title": "Efficient SimRank-based Similarity Join Over Large Graphs",
            "abstract": "\n Graphs have been widely used to model complex data in many real-world applications. Answering vertex join queries over large graphs is meaningful and interesting, which can benefit friend recommendation in social networks and link prediction, etc. In this paper, we adopt \"SimRank\" to evaluate the similarity of two vertices in a large graph because of its generality. Note that \"SimRank\" is purely structure dependent and it does not rely on the domain knowledge. Specifically, we define a SimRank-based join (SRJ) query to find all the vertex pairs satisfying the threshold in a data graph\n G\n . In order to reduce the search space, we propose an estimated shortest-path distance based upper bound for SimRank scores to prune unpromising vertex pairs. In the verification, we propose a novel index, called h-go cover, to efficiently compute the SimRank score of a single vertex pair. Given a graph\n G\n , we only materialize the SimRank scores of a small proportion of vertex pairs (called h-go covers), based on which, the SimRank score of any vertex pair can be computed easily. In order to handle large graphs, we extend our technique to the partition-based framework. Thorough theoretical analysis and extensive experiments over both real and synthetic datasets confirm the efficiency and effectiveness of our solution.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1807168",
                    "name": "Weiguo Zheng"
                },
                {
                    "authorId": "144554647",
                    "name": "Lei Zou"
                },
                {
                    "authorId": "1717629",
                    "name": "Yansong Feng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144060462",
                    "name": "Dongyan Zhao"
                }
            ]
        },
        {
            "paperId": "a82f5fb23c88c5d61c390826a8959c800826ddc3",
            "title": "Optimizing Multi-Top-k Queries over Uncertain Data Streams",
            "abstract": "Query processing over uncertain data streams, in particular top-\u03ba query processing, has become increasingly important due to its wide application in many fields such as sensor network monitoring and internet traffic control. In many real applications, multiple top-\u03ba queries are registered in the system. Sharing the results of these queries is a key factor in saving the computation cost and providing real-time response. However, due to the complex semantics of uncertain top-\u03ba query processing, it is nontrivial to implement sharing among different top-\u03ba queries and few works have addressed the sharing issue. In this paper, we formulate various types of sharing among multiple top-\u03ba queries over uncertain data streams based on the frequency upper bound of each top-\u03ba query. We present an optimal dynamic programming solution as well as a more efficient (in terms of time and space complexity) greedy algorithm to compute the execution plan of executing queries for saving the computation cost between them. Experiments have demonstrated that the greedy algorithm can find the optimal solution in most cases, and it can almost achieve the same performance (in terms of latency and throughput) as the dynamic programming approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118212364",
                    "name": "Tao Chen"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                },
                {
                    "authorId": "1730284",
                    "name": "Nong Xiao"
                }
            ]
        },
        {
            "paperId": "af1e1bee41d004a6c1fa608a9fe2a884f48c6e5f",
            "title": "Reducing Uncertainty of Schema Matching via Crowdsourcing",
            "abstract": "Schema matching is a central challenge for data integration systems. Automated tools are often uncertain about schema matchings they suggest, and this uncertainty is inherent since it arises from the inability of the schema to fully capture the semantics of the represented data. Human common sense can often help. Inspired by the popularity and the success of easily accessible crowdsourcing platforms, we explore the use of crowdsourcing to reduce the uncertainty of schema matching. \n \nSince it is typical to ask simple questions on crowdsourcing platforms, we assume that each question, namely Correspondence Correctness Question (CCQ), is to ask the crowd to decide whether a given correspondence should exist in the correct matching. We propose frameworks and efficient algorithms to dynamically manage the CCQs, in order to maximize the uncertainty reduction within a limited budget of questions. We develop two novel approaches, namely \"Single CCQ\" and \"Multiple CCQ\", which adaptively select, publish and manage the questions. We verified the value of our solutions with simulation and real implementation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50445897",
                    "name": "C. Zhang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "3151540",
                    "name": "Caleb Chen Cao"
                }
            ]
        },
        {
            "paperId": "c426938cf6ab2a9443c7fc8bfb123ec3ad62b84b",
            "title": "GeoTruCrowd: trustworthy query answering with spatial crowdsourcing",
            "abstract": "With the abundance and ubiquity of mobile devices, a new class of applications, called spatial crowdsourcing, is emerging, which enables spatial tasks (i.e., tasks related to a location) assigned to and performed by human workers. However, one of the major challenges with spatial crowdsourcing is how to verify the validity of the results provided by workers, when the workers are not trusted equally. To tackle this problem, we assume every worker has a reputation score, which states the probability that the worker performs a task correctly. Moreover, we define a confidence level for every spatial task, which states that the answer to the given spatial task is only accepted if its confidence is higher than a certain threshold. Thus, the problem we are trying to solve is to maximize the number of spatial tasks that are assigned to a set of workers while satisfying the confidence levels of those tasks. Note that a unique aspect of our problem is that the optimal assignment of tasks heavily depends on the geographical locations of workers and tasks. This means that every spatial task should be assigned to enough number of workers such that their aggregate reputation satisfies the confidence of the task. Consequently, an exhaustive approach needs to compute the aggregate reputation score (using a typical decision fusion aggregation mechanism, such as voting) for all possible subsets of the workers, which renders the problem complex (we show it is NP-hard). Subsequently, we propose a number of heuristics and utilizing real-world and synthetic data in extensive sets of experiments we show that we can achieve close to optimal performance with the cost of a greedy approach, by exploiting our problem's unique characteristics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1773256",
                    "name": "Leyla Kazemi"
                },
                {
                    "authorId": "1773086",
                    "name": "C. Shahabi"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "d0c2d1f0f9f12c676ba20a25f956b96cee05c302",
            "title": "Interval reverse nearest neighbor queries on uncertain data with Markov correlations",
            "abstract": "Nowadays, many applications return to the user a set of results that take the query as their nearest neighbor, which are commonly expressed through reverse nearest neighbor (RNN) queries. When considering moving objects, users would like to find objects that appear in the RNN result set for a period of time in some real-world applications such as collaboration recommendation and anti-tracking. In this work, we formally define the problem of interval reverse nearest neighbor (IRNN) queries over moving objects, which return the objects that maintain nearest neighboring relations to the moving query objects for the longest time in the given interval. Location uncertainty of moving data objects and moving query objects is inherent in various domains, and we investigate objects that exhibit Markov correlations, that is, each object's location is only correlated with its own location at previous timestamp while being independent of other objects. There exists the efficiency challenge for answering IRNN queries on uncertain moving objects with Markov correlations since we have to retrieve not only all the possible locations of each object at current time but also its historically possible locations. To speed up the query processing, we present a general framework for answering IRNN queries on uncertain moving objects with Markov correlations in two phases. In the first phase, we apply space pruning and probability pruning techniques, which reduce the search space significantly. In the second phase, we verify whether each unpruned object is an IRNN of the query object. During this phase, we propose an approach termed Probability Decomposition Verification (PDV) algorithm which avoid computing the probability of any object being an RNN of the query object exactly and thus improve the efficiency of verification. The performance of the proposed algorithm is demonstrated by extensive experiments on synthetic and real datasets, and the experimental results show that our algorithm is more efficient than the Monte-Carlo based approximate algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2895879",
                    "name": "Chuanfei Xu"
                },
                {
                    "authorId": "144955064",
                    "name": "Yu Gu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1747015",
                    "name": "Jianzhong Qiao"
                },
                {
                    "authorId": "145024112",
                    "name": "Ge Yu"
                }
            ]
        },
        {
            "paperId": "d1ea04f22f5fa7f1afc6dd59e9e4af4b06954b4b",
            "title": "Proceedings of the 2013 SIGMOD/PODS Ph.D. Symposium, New York, NY, USA, June 23, 2013",
            "abstract": "It is our great pleasure to welcome you to the 2013 Sigmod/PODS Ph.D. Symposium, which is co-located with SIGMOD/PODS 2013 and takes place on Sunday June 23rd, 2013 in New York City, USA. This is a forum where PhD students have an opportunity of presenting their research ideas, receiving feedback from and interacting with senior members of the community. The focus of the Symposium is on mentorship and providing constructive feedback to the students. Many members of the Program Committee attend the Symposium, providing ample opportunity for discussions and feedback. Each student whose submission is accepted for presentation is paired with a member of the PC for mentorship and one-on-one discussions. \n \nThis year we have two tracks for submission. The General Track is for students who are in early stages of their studies; the submissions specify the problem they plan to focus on, the thesis that they plan to investigate, and the outline of the methodology that they plan to follow. The Towards Graduation Track is for students who expect to graduate by the end of 2013; the submissions specify the contributions they have made and what remains to be accomplished. We received 26 papers in response to the call for papers: 14 for the General Track and 12 for the Towards Graduation Track. Each of these was reviewed by three PC members with a focus on selecting submissions that were ready and could benefit from the feedbacks. In the end, we selected 12 submissions, 6 for each track, for discussions at the Symposium. The proceedings include short descriptions of these PhD theses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                }
            ]
        },
        {
            "paperId": "d8152278e81fbe66d20b4a1ac88e7b15b7af4a2e",
            "title": "Finding time period-based most frequent path in big trajectory data",
            "abstract": "The rise of GPS-equipped mobile devices has led to the emergence of big trajectory data. In this paper, we study a new path finding query which finds the most frequent path (MFP) during user-specified time periods in large-scale historical trajectory data. We refer to this query as time period-based MFP (TPMFP). Specifically, given a time period T, a source v_s and a destination v_d, TPMFP searches the MFP from v_s to v_d during T. Though there exist several proposals on defining MFP, they only consider a fixed time period. Most importantly, we find that none of them can well reflect people's common sense notion which can be described by three key properties, namely suffix-optimal (i.e., any suffix of an MFP is also an MFP), length-insensitive (i.e., MFP should not favor shorter or longer paths), and bottleneck-free (i.e., MFP should not contain infrequent edges). The TPMFP with the above properties will reveal not only common routing preferences of the past travelers, but also take the time effectiveness into consideration. Therefore, our first task is to give a TPMFP definition that satisfies the above three properties. Then, given the comprehensive TPMFP definition, our next task is to find TPMFP over huge amount of trajectory data efficiently. Particularly, we propose efficient search algorithms together with novel indexes to speed up the processing of TPMFP. To demonstrate both the effectiveness and the efficiency of our approach, we conduct extensive experiments using a real dataset containing over 11 million trajectories.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1775142",
                    "name": "Wuman Luo"
                },
                {
                    "authorId": "1728553",
                    "name": "Haoyu Tan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1726587",
                    "name": "L. Ni"
                }
            ]
        },
        {
            "paperId": "02015f96e0c112460cc37833cf43feb45f83c73e",
            "title": "MOVE: A Large Scale Keyword-Based Content Filtering and Dissemination System",
            "abstract": "The Web 2.0 era is characterized by the emergence of a very large amount of live content. A real time and fine grained content filtering approach can precisely keep users up-to-date the information that they are interested. The key of the approach is to offer a scalable match algorithm. One might treat the content match as a special kind of content search, and resort to the classic algorithm [5]. However, due to blind flooding, [5] cannot be simply adapted for scalable content match. To increase the throughput of scalable match, we propose an adaptive approach to allocate (i.e, replicate and partition) filters. The allocation is based on our observation on real datasets: most users prefer to use short queries, consisting of around 2-3 terms per query, and web content typically contains tens and even thousands of terms per article. Thus, by reducing the number of processed documents, we can reduce the latency of matching large articles with filters, and have chance to achieve higher throughput. We implement our approach on an open source project, Apache Cassandra. The experiment with real datasets shows that our approach can achieve around folds of better throughput than two counterpart state-of-the-arts solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144084147",
                    "name": "Weixiong Rao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "143966172",
                    "name": "P. Hui"
                },
                {
                    "authorId": "1759241",
                    "name": "Sasu Tarkoma"
                }
            ]
        },
        {
            "paperId": "04a022ee9775eb7abb72f933c65d88583a65c56f",
            "title": "Discovering Threshold-based Frequent Closed Itemsets over Probabilistic Data",
            "abstract": "In recent years, many new applications, such as sensor network monitoring and moving object search, show a growing amount of importance of uncertain data management and mining. In this paper, we study the problem of discovering threshold-based frequent closed item sets over probabilistic data. Frequent item set mining over probabilistic database has attracted much attention recently. However, existing solutions may lead an exponential number of results due to the downward closure property over probabilistic data. Moreover, it is hard to directly extend the successful experiences from mining exact data to a probabilistic environment due to the inherent uncertainty of data. Thus, in order to obtain a reasonable result set with small size, we study discovering frequent closed item sets over probabilistic data. We prove that even a sub-problem of this problem, computing the frequent closed probability of an item set, is #P-Hard. Therefore, we develop an efficient mining algorithm based on depth-first search strategy to obtain all probabilistic frequent closed item sets. To reduce the search space and avoid redundant computation, we further design several probabilistic pruning and bounding techniques. Finally, we verify the effectiveness and efficiency of the proposed methods through extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1696332",
                    "name": "Bolin Ding"
                }
            ]
        },
        {
            "paperId": "1a4be0bf4d975e22aca130bcb6a6a0ea9b1b1d69",
            "title": "Mining Frequent Itemsets over Uncertain Databases",
            "abstract": "In recent years, due to the wide applications of uncertain data, mining frequent itemsets over uncertain databases has attracted much attention. In uncertain databases, the support of an itemset is a random variable instead of a fixed occurrence counting of this itemset. Thus, unlike the corresponding problem in deterministic databases where the frequent itemset has a unique definition, the frequent itemset under uncertain environments has two different definitions so far. The first definition, referred as the expected support-based frequent itemset, employs the expectation of the support of an itemset to measure whether this itemset is frequent. The second definition, referred as the probabilistic frequent itemset, uses the probability of the support of an itemset to measure its frequency. Thus, existing work on mining frequent itemsets over uncertain databases is divided into two different groups and no study is conducted to comprehensively compare the two different definitions. In addition, since no uniform experimental platform exists, current solutions for the same definition even generate inconsistent results. In this paper, we firstly aim to clarify the relationship between the two different definitions. Through extensive experiments, we verify that the two definitions have a tight connection and can be unified together when the size of data is large enough. Secondly, we provide baseline implementations of eight existing representative algorithms and test their performances with uniform measures fairly. Finally, according to the fair tests over many different benchmark data sets, we clarify several existing inconsistent conclusions and discuss some new findings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2329311",
                    "name": "Yurong Cheng"
                },
                {
                    "authorId": "144019071",
                    "name": "Philip S. Yu"
                }
            ]
        },
        {
            "paperId": "381cbf1da5fd064db852897b23a68b848cb10663",
            "title": "Subspace Similarity Search under {\\rm L}_p-Norm",
            "abstract": "Similarity search has been widely used in many applications such as information retrieval, image data analysis, and time-series matching. Previous work on similarity search usually consider the search problem in the full space. In this paper, however, we tackle a problem, subspace similarity search, which finds all data objects that match with a query object in the subspace instead of the original full space. In particular, the query object can specify arbitrary subspace with arbitrary number of dimensions. Due to the exponential number of possible subspaces specified by users, we introduce an efficient and effective pruning technique, which assigns scores to data objects with respect to pivots and prunes candidates via scores. We propose an effective multipivot-based method to preprocess data objects by selecting appropriate pivots, where the entire procedure is guided by a formal cost model, such that the pruning power is maximized. Then, scores of each data object are organized in sorted lists to facilitate an efficient subspace similarity search. Furthermore, many real-world application data such as image databases, time-series data, and sensory data often contain noises, which can be modeled as uncertain objects. Different from certain data, efficient query processing on uncertain data is more challenging due to its intensive computation of probability confidences. Thus, it is also crucial to answer subspace queries efficiently and effectively over uncertain objects. Specifically, we define a novel query, namely probabilistic subspace range query (PSRQ) in the uncertain database, which finds objects within a distance from a query object in any subspace with high probability. To address this query, we extend our proposed pruning techniques for precise data to that of answering PSRQ in arbitrary subspaces. Extensive experiments demonstrated the performance of our proposed approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "3d87c9221f30159e65cf4ce083bd993e9817692b",
            "title": "A General Framework for Publishing Privacy Protected and Utility Preserved Graph",
            "abstract": "The privacy protection of graph data has become more and more important in recent years. Many works have been proposed to publish a privacy preserving graph. All these works prefer publishing a graph, which guarantees the protection of certain privacy with the smallest change to the original graph. However, there is no guarantee on how the utilities are preserved in the published graph. In this paper, we propose a general fine-grained adjusting framework to publish a privacy protected and utility preserved graph. With this framework, the data publisher can get a trade-off between the privacy and utility according to his customized preferences. We used the protection of a weighted graph as an example to demonstrate the implementation of this framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688812",
                    "name": "Mingxuan Yuan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144084147",
                    "name": "Weixiong Rao"
                },
                {
                    "authorId": "145642974",
                    "name": "Hong Mei"
                }
            ]
        },
        {
            "paperId": "4a1b3dfe199b438c5cf760e598e1bb9a8ca5c78e",
            "title": "Parameter-Free Determination of Distance Thresholds for Metric Distance Constraints",
            "abstract": "The importance of introducing distance constraints to data dependencies, such as differential dependencies (DDs) [28], has recently been recognized. The metric distance constraints are tolerant to small variations, which enable them apply to wide data quality checking applications, such as detecting data violations. However, the determination of distance thresholds for the metric distance constraints is non-trivial. It often relies on a truth data instance which embeds the distance constraints. To find useful distance threshold patterns from data, there are several guidelines of statistical measures to specify, e.g., support, confidence and dependent quality. Unfortunately, given a data instance, users might not have any knowledge about the data distribution, thus it is very challenging to set the right parameters. In this paper, we study the determination of distance thresholds for metric distance constraints, in a parameter-free style. Specifically, we compute an expected utility based on the statistical measures from the data. According to our analysis as well as experimental verification, distance threshold patterns with higher expected utility could offer better usage in real applications, such as violation detection. We then develop efficient algorithms to determine the distance thresholds having the maximum expected utility. Finally, our extensive experimental evaluation demonstrates the effectiveness and efficiency of the proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144488755",
                    "name": "Hong Cheng"
                }
            ]
        },
        {
            "paperId": "6443d3110c865118b18fd00914a95fd5557e2427",
            "title": "Privacy preserving graph data publication",
            "abstract": "Nowadays, more and more people join social networks, such as Facebook, Linkedin, and Livespace, to share information and to monitor or participate in different activities. This gives people a great opportunity to obtain useful information from these social network data. Meanwhile, the information stored in these social networks are under high risk of attack by various malicious users, in other words, people's privacy could be easily breached via some domain knowledge. Thus, for a service provider, such as Facebook and Linkedin, how to publish a privacy preserving graph becomes an important problem. It is essential to protect users' privacy and at the same time provide \"useful\" data. Targeting the privacy preserving graph publication problem, in this thesis, we propose graph publishing models, which cover different aspects of graph publication issues \n1) We propose a novel privacy preserving graph construction technique based on adding noise nodes. This new graph construction algorithm provides privacy protection for the individuals in the graph and their attributes as well as maintaining good graph utility. \n2) We further propose a personalized protection framework for social networks, which publishes graphs with the consideration of users' personalized privacy settings. \n3) After observing the lack of related work on the weighted graph model, which is general for online social network application, we propose graph protection models which protect individuals when the weights on the relationships are considered. \n4) To solve the privacy leakage when the algorithm which generates the published graph is known by the attacker, we propose a new protecting model which protects both the sensitive labels and sensitive links in a graph. \n5) We propose a general fine-grained adjusting framework to publish a privacy protected and utility preserved graph. Using this framework, the data publisher gets a trade-off between privacy and utility according to the data publisher's customized preference. The protected privacy and preserved utilities can be quantified. We use protecting the privacy of a weighted graph as an example to demonstrate the implementation of this framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1688812",
                    "name": "Mingxuan Yuan"
                }
            ]
        },
        {
            "paperId": "6be09855716d7eef7d21f104795508de88af1630",
            "title": "Efficient Subgraph Similarity Search on Large Probabilistic Graph Databases",
            "abstract": "Many studies have been conducted on seeking the efficient solution for subgraph similarity search over certain (deterministic) graphs due to its wide application in many fields, including bioinformatics, social network analysis, and Resource Description Framework (RDF) data management. All these works assume that the underlying data are certain. However, in reality, graphs are often noisy and uncertain due to various factors, such as errors in data extraction, inconsistencies in data integration, and privacy preserving purposes. Therefore, in this paper, we study subgraph similarity search on large probabilistic graph databases. Different from previous works assuming that edges in an uncertain graph are independent of each other, we study the uncertain graphs where edges' occurrences are correlated. We formally prove that subgraph similarity search over probabilistic graphs is #P-complete, thus, we employ a filter-and-verify framework to speed up the search. In the filtering phase, we develop tight lower and upper bounds of subgraph similarity probability based on a probabilistic matrix index, PMI. PMI is composed of discriminative subgraph features associated with tight lower and upper bounds of subgraph isomorphism probability. Based on PMI, we can sort out a large number of probabilistic graphs and maximize the pruning capability. During the verification phase, we develop an efficient sampling algorithm to validate the remaining candidates. The efficiency of our proposed solutions has been verified through extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2109590665",
                    "name": "Haixun Wang"
                }
            ]
        },
        {
            "paperId": "72e5b02e5b083575d505395108bb58753819b69f",
            "title": "Structure Tensor Series-Based Large Scale Near-Duplicate Video Retrieval",
            "abstract": "With the huge amount of video data and its exponential growth in recent years, many new challenges, like storage, search and navigation, have arisen. Among these challenges, near-duplicate video retrieval aims to find clips that are identical or nearly identical in content to a query clip. This has attracted much attention due to its wide applications including copyright detection, commercial monitoring and news video tracking. In this paper, we propose a practical solution based on 3-D structure tensor model for this problem. We first propose a novel video representation, adaptive structure video tensor series, together with a robust similarity measure, to improve the retrieval effectiveness. Then, we design a dimensionality reduction technique for tensor series to improve the search efficiency. Finally, we prove the effectiveness and efficiency of the proposed method by extensive experiments on hundreds of hours of real video data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2155797559",
                    "name": "Xiaofang Zhou"
                }
            ]
        },
        {
            "paperId": "91abc4f0298fd0b07de325ae1785d685bebb9948",
            "title": "Efficient Multi-way Theta-Join Processing Using MapReduce",
            "abstract": "Multi-way Theta-join queries are powerful in describing complex relations and therefore widely employed in real practices. However, existing solutions from traditional distributed and parallel databases for multi-way Theta-join queries cannot be easily extended to fit a shared-nothing distributed computing paradigm, which is proven to be able to support OLAP applications over immense data volumes. In this work, we study the problem of efficient processing of multi-way Theta-join queries using MapReduce from a cost-effective perspective. Although there have been some works using the (key, value) pair-based programming model to support join operations, efficient processing of multi-way Theta-join queries has never been fully explored. The substantial challenge lies in, given a number of processing units (that can run Map or Reduce tasks), mapping a multi-way Theta-join query to a number of MapReduce jobs and having them executed in a well scheduled sequence, such that the total processing time span is minimized. Our solution mainly includes two parts: 1) cost metrics for both single MapReduce job and a number of MapReduce jobs executed in a certain order; 2) the efficient execution of a chain-typed Theta-join with only one MapReduce job. Comparing with the query evaluation strategy proposed in [23] and the widely adopted Pig Latin and Hive SQL solutions, our method achieves significant improvement of the join processing efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108166998",
                    "name": "Xiaofei Zhang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "143894230",
                    "name": "Min Wang"
                }
            ]
        },
        {
            "paperId": "921d71d2d487ccc1dd064d80f4cda3ae255df9e1",
            "title": "Query Processing over Uncertain Databases",
            "abstract": "Due to measurement errors, transmission lost, or injected noise for privacy protection, uncertainty exists in the data of many real applications. However, query processing techniques for deterministic data cannot be directly applied to uncertain data because they do not have mechanisms to handle data uncertainty. Therefore, efficient and effective manipulation of uncertain data is a practical yet challenging research topic. In this book, we start from the data models for imprecise and uncertain data, move on to defining different semantics for queries on uncertain data, and finally discuss the advanced query processing techniques for various probabilistic queries in uncertain databases. The book serves as a comprehensive guideline for query processing over uncertain databases. Table of Contents: Introduction / Uncertain Data Models / Spatial Query Semantics over Uncertain Data Models / Spatial Query Processing over Uncertain Databases / Conclusion",
            "fieldsOfStudy": [
                "Computer Science",
                "Geography"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                }
            ]
        },
        {
            "paperId": "93941cc7f022348f08b0636360b76c3e519d90e5",
            "title": "Distributed Incomplete Pattern Matching via a Novel Weighted Bloom Filter",
            "abstract": "In this paper, we first propose a very interesting and practical problem, pattern matching in a distributed mobile environment. Pattern matching is a well-known problem and extensive research has been conducted for performing effective and efficient search. However, previous proposed approaches assume that data are centrally stored, which is not the case in a mobile environment (e.g., mobile phone networks), where one person's pattern could be separately stored in a number of different stations, and such a local pattern is incomplete compared with the global pattern. A simple solution to pattern matching over a mobile environment is to collect all the data distributed in base stations to a data center and conduct pattern matching at the data center afterwards. Clearly, such a simple solution will raise huge amount of communication traffic, which could cause the communication bottleneck brought by the limited wireless bandwidth to be even worse. Therefore, a communication efficient and search effective solution is necessary. In our work, we present a novel solution which is based on our well-designed Weighted Bloom Filter (WBF), called, Distributed Incomplete pattern matching (DI-matching), to find target patterns over a distributed mobile environment. Specifically, to save communication cost and ensure pattern matching in distributed incomplete patterns, we use WBF to encode a query pattern and disseminate the encoded data to each base station. Each base station conducts a local pattern search according to the received WBF. Only qualified IDs and corresponding weights in each base station are sent to the data center for aggregation and verification. Through extensive empirical experiments on a real city-scale mobile networks data set, we demonstrate the effectiveness and efficiency of our proposed solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47130133",
                    "name": "Siyuan Liu"
                },
                {
                    "authorId": "2055048199",
                    "name": "Lei Kang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1726587",
                    "name": "L. Ni"
                }
            ]
        },
        {
            "paperId": "b3af008337f881ee28930059c12f946681f5c7d3",
            "title": "Discovering areas of interest with geo-tagged images and check-ins",
            "abstract": "Geo-tagged image is an ideal source for the discovery of popular travel places. However, the aspects of popular venues for daily-life purposes like dining and shopping are often missing in the mined locations from geo-tagged images. Fortunately check-in websites provide us a unique opportunity of analyzing people's preferences in their daily lives to complement the knowledge mined from geo-tagged images. This paper presents a novel approach for the discovery of Areas of Interest (AoI). By analyzing both geo-tagged images and check-ins, the approach exploits travelers' flavors as well as the preferences of daily-life activities of local residents to find AoI in a city. The proposed approach consists of two major steps. Firstly, we devise a density-based clustering method to discover AoI, mainly based on the image densities but also reinforced by the secondary densities from the images' neighboring venues. Then we propose a novel joint authority analysis framework to rank AoI. The framework simultaneously considers both the location-location transitions, and the user-location relations. An interactive presentation interface for visualizing AoI is also presented. The approach is tested with very large datasets for Shanghai city. They consist of 49,460 geo-tagged images from Panoramio.com, and 1,361,547 check-ins from the check-in website Qieke.com. By evaluating the ranking accuracy and quality of AoI, we demonstrate great improvements of our method over compared methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49721729",
                    "name": "Jiajun Liu"
                },
                {
                    "authorId": "145622169",
                    "name": "Zi Huang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1724393",
                    "name": "Heng Tao Shen"
                },
                {
                    "authorId": "4013198",
                    "name": "Zhixian Yan"
                }
            ]
        },
        {
            "paperId": "b407f96f3eaeffa9a6ceff24153e54ea81baa145",
            "title": "Optimizing Bloom Filter Settings in Peer-to-Peer Multikeyword Searching",
            "abstract": "Peer-to-Peer multikeyword searching requires distributed intersection/union operations across wide area networks, raising a large amount of traffic cost. Existing schemes commonly utilize Bloom Filters (BFs) encoding to effectively reduce the traffic cost during the intersection/union operations. In this paper, we address the problem of optimizing the settings of a BF. We show, through mathematical proof, that the optimal setting of BF in terms of traffic cost is determined by the statistical information of the involved inverted lists, not the minimized false positive rate as claimed by previous studies. Through numerical analysis, we demonstrate how to obtain optimal settings. To better evaluate the performance of this design, we conduct comprehensive simulations on TREC WT10G test collection and query logs of a major commercial web search engine. Results show that our design significantly reduces the search traffic and latency of the existing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1713345",
                    "name": "Hanhua Chen"
                },
                {
                    "authorId": "145914256",
                    "name": "Hai Jin"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                },
                {
                    "authorId": "1726587",
                    "name": "L. Ni"
                }
            ]
        },
        {
            "paperId": "d681b8cd87ae587a86869e884863c9b5c654b167",
            "title": "Energy-Efficient Reverse Skyline Query Processing over Wireless Sensor Networks",
            "abstract": "Reverse skyline query plays an important role in many sensing applications, such as environmental monitoring, habitat monitoring, and battlefield monitoring. Due to the limited power supplies of wireless sensor nodes, the existing centralized approaches, which do not consider energy efficiency, cannot be directly applied to the distributed sensor environment. In this paper, we investigate how to process reverse skyline queries energy efficiently in wireless sensor networks. Initially, we theoretically analyzed the properties of reverse skyline query and proposed a skyband-based approach to tackle the problem of reverse skyline query answering over wireless sensor networks. Then, an energy-efficient approach is proposed to minimize the communication cost among sensor nodes of evaluating range reverse skyline query. Moreover, optimization mechanisms to improve the performance of multiple reverse skylines are also discussed. Extensive experiments on both real-world data and synthetic data have demonstrated the efficiency and effectiveness of our proposed approaches with various experimental settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "1759388",
                    "name": "Junchang Xin"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                }
            ]
        },
        {
            "paperId": "e5b32bdfbb9a75e716aaab7532d3d1154011ac32",
            "title": "UFIMT: an uncertain frequent itemset mining toolbox",
            "abstract": "In recent years, mining frequent itemsets over uncertain data has attracted much attention in the data mining community. Unlike the corresponding problem in deterministic data, the frequent itemset under uncertain data has two different definitions: the expected support-based frequent itemset and the probabilistic frequent itemset. Most existing works only focus on one of the definitions and no comprehensive study is conducted to compare the two different definitions. Moreover, due to lacking the uniform implementation platform, existing solutions for the same definition even generate inconsistent results. In this demo, we present a demonstration called as UFIMT (underline Uncertain Frequent Itemset Mining Toolbox) which not only discovers frequent itemsets over uncertain data but also compares the performance of different algorithms and demonstrates the relationship between different definitions. In this demo, we firstly present important techniques and implementation skills of the mining problem, secondly, we show the system architecture of UFIMT, thirdly, we report an empirical analysis on extensive both real and synthetic benchmark data sets, which are used to compare different algorithms and to show the close relationship between two different frequent itemset definitions, and finally we discuss some existing challenges and new findings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144019071",
                    "name": "Philip S. Yu"
                }
            ]
        },
        {
            "paperId": "f1321126361a13c0bfb9b4ce45bb1d29b4305b37",
            "title": "Whom to Ask? Jury Selection for Decision Making Tasks on Micro-blog Services",
            "abstract": "It is universal to see people obtain knowledge on micro-blog services by asking others decision making questions. In this paper, we study the Jury Selection Problem(JSP) by utilizing crowdsourcing for decision making tasks on micro-blog services. Specifically, the problem is to enroll a subset of crowd under a limited budget, whose aggregated wisdom via Majority Voting scheme has the lowest probability of drawing a wrong answer(Jury Error Rate-JER). \n \nDue to various individual error-rates of the crowd, the calculation of JER is non-trivial. Firstly, we explicitly state that JER is the probability when the number of wrong jurors is larger than half of the size of a jury. To avoid the exponentially increasing calculation of JER, we propose two efficient algorithms and an effective bounding technique. Furthermore, we study the Jury Selection Problem on two crowdsourcing models, one is for altruistic users(AltrM) and the other is for incentive-requiring users(PayM) who require extra payment when enrolled into a task. For the AltrM model, we prove the monotonicity of JER on individual error rate and propose an efficient exact algorithm for JSP. For the PayM model, we prove the NP-hardness of JSP on PayM and propose an efficient greedy-based heuristic algorithm. Finally, we conduct a series of experiments to investigate the traits of JSP, and validate the efficiency and effectiveness of our proposed algorithms on both synthetic and real micro-blog data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3151540",
                    "name": "Caleb Chen Cao"
                },
                {
                    "authorId": "1872111",
                    "name": "Jieying She"
                },
                {
                    "authorId": "8230559",
                    "name": "Yongxin Tong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "0443448b390dae224b9f9aa717d893032f4085fc",
            "title": "gStore: Answering SPARQL Queries via Subgraph Matching",
            "abstract": "Due to the increasing use of RDF data, efficient processing of SPARQL queries over RDF datasets has become an important issue. However, existing solutions suffer from two limitations: 1) they cannot answer SPARQL queries with wildcards in a scalable manner; and 2) they cannot handle frequent updates in RDF repositories efficiently. Thus, most of them have to reprocess the dataset from scratch. In this paper, we propose a graph-based approach to store and query RDF data. Rather than mapping RDF triples into a relational database as most existing methods do, we store RDF data as a large graph. A SPARQL query is then converted into a corresponding subgraph matching query. In order to speed up query processing, we develop a novel index, together with some effective pruning rules and efficient search algorithms. Our method can answer exact SPARQL queries and queries with wildcards in a uniform manner. We also propose an effective maintenance algorithm to handle online updates over RDF repositories. Extensive experiments confirm the efficiency and effectiveness of our solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144554647",
                    "name": "Lei Zou"
                },
                {
                    "authorId": "2489153",
                    "name": "Jinghui Mo"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                },
                {
                    "authorId": "144060462",
                    "name": "Dongyan Zhao"
                }
            ]
        },
        {
            "paperId": "1c1ba0cd208522ca601c1984518a5039d14f0a61",
            "title": "KLEAP: an efficient cleaning method to remove cross-reads in RFID streams",
            "abstract": "Recently, the RFID technology has been widely used in many kinds of applications. However, because of the interference from environmental factors and limitations of the radio frequency technology, the data streams collected by the RFID readers are usually contain a lot of cross-reads. To address this issue, we propose a KerneL dEnsity-bAsed Probability cleaning method (KLEAP) to remove cross-reads within a sliding window. The method estimates the density of each tag using a kernel-based function. The reader corresponding to the micro-cluster with the largest density will be regarded as the position that the tagged object should locate in current window, and the readings derived from other readers will be treated as the cross-reads. Experiments verify the effectiveness and efficiency of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2381461",
                    "name": "Guoqiong Liao"
                },
                {
                    "authorId": "2153124005",
                    "name": "Jing Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1761110",
                    "name": "Changxuan Wan"
                }
            ]
        },
        {
            "paperId": "24637d72bbb8faafa7932fabd64f06bba470fec2",
            "title": "Structure tensor series-based matching for near-duplicate video retrieval",
            "abstract": "Near duplicate video retrieval has attracted much attention due to its wide spectrum of applications including copyright detection, commercial monitoring and news video tracking. In recent years, there has been significant research effort on efficiently identifying near duplicates from large video collections. However, existing approaches for large video databases suffer from low accuracy due to the serious information loss. In this paper, we propose a practical solution based on 3D structure tensor model for this problem. We first propose a novel video representation scheme, adaptive structure video tensor series (ASVT series), together with a robust similarity measure, to improve the retrieval effectiveness. Then, we prove the effectiveness of the proposed method by extensive experiments on hundreds hours real video data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2155797559",
                    "name": "Xiaofang Zhou"
                }
            ]
        },
        {
            "paperId": "43a068652277ca9213202d984a69965e7fec5dd9",
            "title": "Answering label-constraint reachability in large graphs",
            "abstract": "In this paper, we study a variant of reachability queries, called label-constraint reachability (LCR) queries, specifically,given a label set S and two vertices u1 and u2 in a large directed graph G, we verify whether there exists a path from u1 to u2 under label constraint S. Like traditional reachability queries, LCR queries are very useful, such as pathway finding in biological networks, inferring over RDF (resource description f ramework) graphs, relationship finding in social networks. However, LCR queries are much more complicated than their traditional counterpart.Several techniques are proposed in this paper to minimize the search space in computing path-label transitive closure. Furthermore, we demonstrate the superiority of our method by extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151485141",
                    "name": "Kun Xu"
                },
                {
                    "authorId": "144554647",
                    "name": "Lei Zou"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "3011950",
                    "name": "Yanghua Xiao"
                },
                {
                    "authorId": "144060462",
                    "name": "Dongyan Zhao"
                }
            ]
        },
        {
            "paperId": "46ff32d4cac43bb4a8331421aee22c3a098285ad",
            "title": "Returning Clustered Results for Keyword Search on XML Documents",
            "abstract": "Keyword search is an effective paradigm for information discovery and has been introduced recently to query XML documents. In this paper, we address the problem of returning clustered results for keyword search on XML documents. We first propose a novel semantics for answers to an XML keyword query. The core of the semantics is the conceptually related relationship between keyword matches, which is based on the conceptual relationship between nodes in XML trees. Then, we propose a new clustering methodology for XML search results, which clusters results according to the way they match the given query. Two approaches to implement the methodology are discussed. The first approach is a conventional one which does clustering after search results are retrieved; the second one clusters search results actively, which has characteristics of clustering on the fly. The generated clusters are then organized into a cluster hierarchy with different granularities to enable users locate the results of interest easily and precisely. Experimental results demonstrate the meaningfulness of the proposed semantics as well as the efficiency of the proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27469537",
                    "name": "X. Liu"
                },
                {
                    "authorId": "1761110",
                    "name": "Changxuan Wan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "5e9f9941629705e0266592deac0b97bc0884ff4a",
            "title": "The World in a Nutshell: Concise Range Queries",
            "abstract": "With the advance of wireless communication technology, it is quite common for people to view maps or get related services from the handheld devices, such as mobile phones and PDAs. Range queries, as one of the most commonly used tools, are often posed by the users to retrieve needful information from a spatial database. However, due to the limits of communication bandwidth and hardware power of handheld devices, displaying all the results of a range query on a handheld device is neither communication-efficient nor informative to the users. This is simply because that there are often too many results returned from a range query. In view of this problem, we present a novel idea that a concise representation of a specified size for the range query results, while incurring minimal information loss, shall be computed and returned to the user. Such a concise range query not only reduces communication costs, but also offers better usability to the users, providing an opportunity for interactive exploration. The usefulness of the concise range queries is confirmed by comparing it with other possible alternatives, such as sampling and clustering. Unfortunately, we prove that finding the optimal representation with minimum information loss is an NP-hard problem. Therefore, we propose several effective and nontrivial algorithms to find a good approximate result. Extensive experiments on real-world data have demonstrated the effectiveness and efficiency of the proposed techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144744111",
                    "name": "K. Yi"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "3245752",
                    "name": "Feifei Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "62361c7a9a624380b711d8f065b0d7954e605b89",
            "title": "Materialization and Decomposition of Dataspaces for Efficient Search",
            "abstract": "Dataspaces consist of large-scale heterogeneous data. The query interface of accessing tuples should be provided as a fundamental facility by practical dataspace systems. Previously, an efficient index has been proposed for queries with keyword neighborhood over dataspaces. In this paper, we study the materialization and decomposition of dataspaces, in order to improve the query efficiency. First, we study the views of items, which are materialized in order to be reused by queries. When a set of views are materialized, it leads to select some of them as the optimal plan with the minimum query cost. Efficient algorithms are developed for query planning and view generation. Second, we study the partitions of tuples for answering top-k queries. Given a query, we can evaluate the score bounds of the tuples in partitions and prune those partitions with bounds lower than the scores of top-k answers. We also provide theoretical analysis of query cost and prove that the query efficiency cannot be improved by increasing the number of partitions. Finally, we conduct an extensive experimental evaluation to illustrate the superior performance of proposed techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1688812",
                    "name": "Mingxuan Yuan"
                }
            ]
        },
        {
            "paperId": "7a54f03b5c3eb7e94339c11f681b2e6900c8d06c",
            "title": "Managing uncertain spatio-temporal data",
            "abstract": "Many spatial query problems defined on uncertain data are computationally expensive, in particular, if in addition to spatial attributes, a time component is added. Although there exists a wide range of applications dealing with uncertain spatio-temporal data, there is no solution for efficient management of such data available yet. This paper is the first work to propose general models for spatio-temporal uncertain data that have the potential to allow efficient processing on a wide range of queries. The main challenge here is to unfold this potential by developing new algorithms based on these models. In addition, we give examples of interesting spatio-temporal queries on uncertain data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3049466",
                    "name": "T. Bernecker"
                },
                {
                    "authorId": "2277396",
                    "name": "Tobias Emrich"
                },
                {
                    "authorId": "1688561",
                    "name": "H. Kriegel"
                },
                {
                    "authorId": "3221975",
                    "name": "Andreas Z\u00fcfle"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "1718168",
                    "name": "N. Mamoulis"
                }
            ]
        },
        {
            "paperId": "7d6fc6aa6d1294e4e969177c018a749df91a71fa",
            "title": "Similarity Join Processing on Uncertain Data Streams",
            "abstract": "Similarity join processing in the streaming environment has many practical applications such as sensor networks, object tracking and monitoring, and so on. Previous works usually assume that stream processing is conducted over precise data. In this paper, we study an important problem of similarity join processing on stream data that inherently contain uncertainty (or called uncertain data streams), where the incoming data at each time stamp are uncertain and imprecise. Specifically, we formalize this problem as join on uncertain data streams (USJ), which can guarantee the accuracy of USJ answers over uncertain data. To tackle the challenges with respect to efficiency and effectiveness such as limited memory and small response time, we propose effective pruning methods on both object and sample levels to filter out false alarms. We integrate the proposed pruning methods into an efficient query procedure that can incrementally maintain the USJ answers. Most importantly, we further design a novel strategy, namely, adaptive superset prejoin (ASP), to maintain a superset of USJ candidate pairs. ASP is in light of our proposed formal cost model such that the average USJ processing cost is minimized. We have conducted extensive experiments to demonstrate the efficiency and effectiveness of our proposed approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "b495c815e5838edf5ad510ba63684abe89937675",
            "title": "Cost-efficient repair in inconsistent probabilistic databases",
            "abstract": "Due to the ubiquitous data uncertainty in many emerging real applications, efficient management of probabilistic databases has become an increasingly important yet challenging problem. In particular, one fundamental task of data management is to identify those unreliable data in the probabilistic database that violate integrity constraints (e.g., functional dependencies), and then quickly resolve data inconsistencies. In this paper, we formulate and tackle an important problem of repairing inconsistent probabilistic databases efficiently by value modification. Specifically, we propose a repair semantic, namely possible-world-oriented repair (PW-repair), which partitions possible worlds into several disjoint groups, and repairs these groups individually with minimum repair costs. Due to the intractable result that finding such a PW-repair strategy is NP-complete, we carefully design a heuristic-based greedy approach for PW-repair, which can efficiently obtain an effective repair of the inconsistent probabilistic database. Through extensive experiments, we show that our approach can achieve the efficiency and effectiveness of the repair on inconsistent probabilistic data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "3011577",
                    "name": "Yincheng Lin"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "b5b4d3035660ef444d7499dcba3ef1587027dbc5",
            "title": "Differential dependencies: Reasoning and discovery",
            "abstract": "The importance of difference semantics (e.g., \u201csimilar\u201d or \u201cdissimilar\u201d) has been recently recognized for declaring dependencies among various types of data, such as numerical values or text values. We propose a novel form of Differential Dependencies (dds), which specifies constraints on difference, called differential functions, instead of identification functions in traditional dependency notations like functional dependencies. Informally, a differential dependency states that if two tuples have distances on attributes X agreeing with a certain differential function, then their distances on attributes Y should also agree with the corresponding differential function on Y. For example, [date(\u2264 7)]\u2192[price(< 100)] states that the price difference of any two days within a week length should be no greater than 100 dollars. Such differential dependencies are useful in various applications, for example, violation detection, data partition, query optimization, record linkage, etc.\n In this article, we first address several theoretical issues of differential dependencies, including formal definitions of dds and differential keys, subsumption order relation of differential functions, implication of dds, closure of a differential function, a sound and complete inference system, and minimal cover for dds. Then, we investigate a practical problem, that is, how to discover dds and differential keys from a given dataset. Due to the intrinsic hardness, we develop several pruning methods to improve the discovery efficiency in practice. Finally, through an extensive experimental evaluation on real datasets, we demonstrate the discovery performance and the effectiveness of dds in several real applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "bed7d9c60cc8e848bb7689e629981888642214fa",
            "title": "Answering Frequent Probabilistic Inference Queries in Databases",
            "abstract": "Existing solutions for probabilistic inference queries mainly focus on answering a single inference query, but seldom address the issues of efficiently returning results for a sequence of frequent queries, which is more popular and practical in many real applications. In this paper, we mainly study the computation caching and sharing among a sequence of inference queries in databases. The clique tree propagation (CTP) algorithm is first introduced in databases for probabilistic inference queries. We use the materialized views to cache the intermediate results of the previous inference queries, which might be shared with the following queries, and consequently reduce the time cost. Moreover, we take the query workload into account to identify the frequently queried variables. To optimize probabilistic inference queries with CTP, we cache these frequent query variables into the materialized views to maximize the reuse. Due to the existence of different query plans, we present heuristics to estimate costs and select the optimal query plan. Finally, we present the experimental evaluation in relational databases to illustrate the validity and superiority of our approaches in answering frequent probabilistic inference queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                }
            ]
        },
        {
            "paperId": "c8e778bc5b39026d8f69143d2961585d01133438",
            "title": "CIM: categorical influence maximization",
            "abstract": "Influence maximization is an interesting and well-motivated problem in social networks study. The traditional influence maximization problem is defined as finding the most \"influential\" vertices without considering the vertex attribute. Though it is useful, in practice, there exist different attributes for vertices, e.g., mobile phone social networks. So, it is more important and useful to capture the vertices having the maximum influence in different search categories, which is exactly the problem that we study in this work. Thus, we name this new problem as Categorical Influence Maximization (CIM). Compare with identifying maximum influence vertices in a single category social network, CIM is much harder because we have to deal with large scale complex data. In this work, based on the observations from real mobile phone social network data, we propose a Probability Distribution based Search method (PDS) to tackle the CIM problem. Specifically, the PDS method consists of three steps. First, we propose a probability distribution based parameter free method (PD-max) to identify the maximum influential vertex set for the specified category by studying the categorical influential distribution within a time interval. Second, among these detected influential vertices, we design a probability distribution based minimizing method (PD-minmax) to find the minimum number of vertices in each category having the maximum influences. We test our solutions with real data sets, which were collected for one year in a city in China. The extensive experiment results show that our methods outperform the existing ones.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47130133",
                    "name": "Siyuan Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1726587",
                    "name": "L. Ni"
                },
                {
                    "authorId": "1692580",
                    "name": "Jianping Fan"
                }
            ]
        },
        {
            "paperId": "d56064a50c53ff19204fe8eb674852e4315fa91e",
            "title": "Efficient subgraph search over large uncertain graphs",
            "abstract": "Retrieving graphs containing a query graph from a large graph database is a key task in many graph-based applications, including chemical compounds discovery, protein complex prediction, and structural pattern recognition. However, graph data handled by these applications is often noisy, incomplete, and inaccurate because of the way the data is produced. In this paper, we study subgraph queries over uncertain graphs. Specifically, we consider the problem of answering threshold-based probabilistic queries over a large uncertain graph database with the possible world semantics. We prove that problem is #P-complete, therefore, we adopt a filtering-and-verification strategy to speed up the search. In the filtering phase, we use a probabilistic inverted index, PIndex, based on subgraph features obtained by an optimal feature selection process. During the verification phase, we develop exact and bound algorithms to validate the remaining candidates. Extensive experimental results demonstrate the effectiveness of the proposed algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14886336",
                    "name": "Ye Yuan"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "2109590665",
                    "name": "Haixun Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "e49269783b1561bf936a3767747c3b2cf059533a",
            "title": "Efficient query answering in probabilistic RDF graphs",
            "abstract": "In this paper, we tackle the problem of efficiently answering queries on probabilistic RDF data graphs. Specifically, we model RDF data by probabilistic graphs, and an RDF query is equivalent to a search over subgraphs of probabilistic graphs that have high probabilities to match with a given query graph. To efficiently processqueries on probabilistic RDF graphs, we propose effective pruning mechanisms, structural and probabilistic pruning. For the structural pruning, we carefully design synopses for vertex/edge labels by considering their distributions and other structural information, in order to improve the pruning power. For the probabilistic pruning, we derive a cost model to guide the pre-computation of probability upper bounds such that the query cost is expected to be low. We construct an index structure that integrates synopses/statistics for structural and robabilistic pruning, and propose an efficient approach to answer queries on probabilistic RDF graph data. The efficiency of our solutions has been verified through extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "ea6099792574c1ada994dff6f1eb0eb89d6f087e",
            "title": "On data dependencies in dataspaces",
            "abstract": "To study data dependencies over heterogeneous data in dataspaces, we define a general dependency form, namely comparable dependencies (CDs), which specifies constraints on comparable attributes. It covers the semantics of a broad class of dependencies in databases, including functional dependencies (FDs), metric functional dependencies (MFDs), and matching dependencies (MDs). As we illustrated, comparable dependencies are useful in real practice of dataspaces, e.g., semantic query optimization. Due to the heterogeneous data in dataspaces, the first question, known as the validation problem, is to determine whether a dependency (almost) holds in a data instance. Unfortunately, as we proved, the validation problem with certain error or confidence guarantee is generally hard. In fact, the confidence validation problem is also NP-hard to approximate to within any constant factor. Nevertheless, we develop several approaches for efficient approximation computation, including greedy and randomized approaches with an approximation bound on the maximum number of violations that an object may introduce. Finally, through an extensive experimental evaluation on real data, we verify the superiority of our methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144019071",
                    "name": "Philip S. Yu"
                }
            ]
        },
        {
            "paperId": "f21620b68dae8a705a55533ee27d022e4935d7de",
            "title": "Fault tolerance study for durable storage on the cloud",
            "abstract": "Durability indicates a system's ability to preserve committed data in the long term. It is one of the key requirements for storage systems. Various fault tolerance strategies have been proposed for traditional distributed databases, peer-to-peer (P2P) systems, and grid systems. These strategies were all developed for specific platforms and application types and have been tailored to the characteristics of the underlying system architectures and application requirements. Cloud systems differ from these previous frameworks in that they are designed to support large numbers of customer-oriented applications, each with different quality of service (QoS) requirements and resource consumption characteristics. Moreover, most cloud architectures are deployed over large-scale geographically distributed infrastructures, raising challenges on data durability as well as system efficiency and scalability. In this paper, we investigated different approaches to the design of durable data cloud platforms. In particular, we consider prevailing cloud platforms and cloud-based applications and examine the impacts of different redundancy and repair strategies to enhance the durability of the data in the cloud. We verify the performance of various approaches to data durability in the Cloud through extensive simulations.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2108166998",
                    "name": "Xiaofei Zhang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "18576cdf59178db60a34832c1c95912a29b4d97c",
            "title": "Data Vitalization: A New Paradigm for Large-Scale Dataset Analysis",
            "abstract": "Nowadays, datasets grow enormously both in size and complexity. One of the key issues confronted by large-scale dataset analysis is how to adapt systems to new, unprecedented query loads. Existing systems nail down the data organization scheme once and for all at the beginning of the system design, thus inevitably will see the performance goes down when user requirements change. In this paper, we propose a new paradigm, Data Vitalization, for large-scale dataset analysis. Our goal is to enable high flexibility such that the system is adaptive to complex analytical applications. Specifically, data are organized into a group of vitalized cells, each of which is a collection of data coupled with computing power. As user requirements change over time, cells evolve spontaneously to meet the potential new query loads. Besides basic functionality of Data Vitalization, we also explore an envisioned architecture of Data Vitalization including possible approaches for query processing, data evolution, as well as its tight-coupled mechanism for data storage and computing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2061494908",
                    "name": "Zhang Xiong"
                },
                {
                    "authorId": "1775142",
                    "name": "Wuman Luo"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1726587",
                    "name": "L. Ni"
                }
            ]
        },
        {
            "paperId": "1ae6930e26707eae81f54bcce26712fa437004d6",
            "title": "Adaptive Subspace Symbolization for Content-Based Video Detection",
            "abstract": "Efficiently and effectively identifying similar videos is an important and nontrivial problem in content-based video retrieval. This paper proposes a subspace symbolization approach, namely SUDS, for content-based retrieval on very large video databases. The novelty of SUDS is that it explores the data distribution in subspaces to build a visual dictionary with which the videos are processed by deriving the string matching techniques with two-step data simplification. Specifically, we first propose an adaptive approach, called VLP, to extract a series of dominant subspaces of variable lengths from the whole visual feature space without the constraint of dimension consecutiveness. A stable visual dictionary is built by clustering the video keyframes over each dominant subspace. A compact video representation model is developed by transforming each keyframe into a word that is a series of symbols in the dominant subspaces, and further each video into a series of words. Then, we present an innovative similarity measure called CVE, which adopts a complementary information compensation scheme based on the visual features and sequence context of videos. Finally, an efficient two-layered index strategy with a number of query optimizations is proposed to facilitate video retrieval. The experimental results demonstrate the high effectiveness and efficiency of SUDS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "48667278",
                    "name": "Xiaofang Zhou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2923064",
                    "name": "Y. Shu"
                },
                {
                    "authorId": "1705708",
                    "name": "A. Bouguettaya"
                },
                {
                    "authorId": "2110316098",
                    "name": "John A. Taylor"
                }
            ]
        },
        {
            "paperId": "4c5311c10b894769e529df22fc6aee02160184ef",
            "title": "Personalized Privacy Protection in Social Networks",
            "abstract": "Due to the popularity of social networks, many proposals have been proposed to protect the privacy of the networks. All these works assume that the attacks use the same background knowledge. However, in practice, different users have different privacy protect requirements. Thus, assuming the attacks with the same background knowledge does not meet the personalized privacy requirements, meanwhile, it looses the chance to achieve better utility by taking advantage of differences of users' privacy requirements. In this paper, we introduce a framework which provides privacy preserving services based on the user's personal privacy requests. Specifically, we define three levels of protection requirements based on the gradually increasing attacker's background knowledge and combine the label generalization protection and the structure protection techniques (i.e. adding noise edge or nodes) together to satisfy different users' protection requirements. We verify the effectiveness of the framework through extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688812",
                    "name": "Mingxuan Yuan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144019071",
                    "name": "Philip S. Yu"
                }
            ]
        },
        {
            "paperId": "4e96d26530cb3002e7b738cb9e09293e715bbbe6",
            "title": "A Generic Framework for Handling Uncertain Data with Local Correlations",
            "abstract": "Data uncertainty is ubiquitous in many real-world applications such as sensor/RFID data analysis. In this paper, we investigate uncertain data that exhibit local correlations, that is, each uncertain object is only locally correlated with a small subset of data, while being independent of others. We propose a generic framework for dealing with this kind of uncertain and locally correlated data, in which we investigate a classical spatial query, nearest neighbor query, on uncertain data with local correlations (namely LC-PNN). Most importantly, to enable fast LC-PNN query processing, we propose a novel filtering technique via offline pre-computations to reduce the query search space. We demonstrate through extensive experiments the efficiency and effectiveness of our approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "54d30a359552aebc66b46f8150708cc6236b2bbc",
            "title": "Monitoring near duplicates over video streams",
            "abstract": "Since near duplicates are ubiquitous over different data sources, increasing research efforts have been put to near duplicate detection recently. Among all the near duplicate detection tasks, an important one is continuous near duplicate monitoring over video streams. Existing video monitoring techniques are not effective for handling the variations that commonly exist among near duplicates. Moreover, approaches proposed for the near duplicate detection in archived video databases are inefficient when applied to high speed video streams. In this work, we propose a framework for effectively online monitoring near duplicates over video streams. Specifically, we first propose a novel representation, a video cuboid signature, to describe a video segment. To capture the local spatio-temporal information of video subclips, we employ the Earth Mover's Distance (EMD) to measure the similarity between two signatures. Both the signature construction and the sequence similarity measure are incrementally processed by exploiting the inherent property of signature series. Then, we propose a novel scheme called locality sensitive multi-leveled approximation (LSMA) that optimizes the near duplicate video similarity matching over streams based on the locality sensitive hashing under EMD metric. The extensive experiments demonstrate the high performance of our approach in terms of the detection accuracy and time cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "7b11dfda2d9ec12cef87e6975576615097b742e9",
            "title": "Data dependencies in the presence of difference",
            "abstract": "The importance of difference semantics (e.g., \u201csimilar\u201d or \u201cdissimilar\u201d) is recently recognized for declaring dependencies among various types of data, such as numerical values or text values. We propose a novel form of differential dependencies (DDs), which specifies constraints on difference, instead of equality function in traditional dependency notations like functional dependencies. Informally, a differential dependency states that if two tuples have distances on attributes X agreeing with a certain differential function, then their distances on attributes Y should also agree with the corresponding differential function on Y. For example, [date(\u2264 7)]\u2192[price(< 100)] states that the flight price difference of any two days in a week length should be no greater than 100$. Such differential dependencies are useful in various applications, e.g., violation detection, data partition, query optimization, record linkage, etc. \nIn this thesis, we first report our preliminary work on several theoretical issues of differential dependencies, including formal definitions of DDs and differential keys, subsumption order relation of differential functions, implication of DDs, closure of a differential function, a sound and complete inference system, and minimal cover for DDs. Then, we investigate a practical problem, i.e., how to discover DD s and differential keys from a given sample data. Due to the intrinsic hardness, we develop several pruning methods to improve the discovery efficiency in practice. Moreover, we address differential dependencies that \u201calmost\u201d hold in a given data instance, namely approximate differential dependencies . Several approaches are studied to evaluate how a DD approximately holds in a data instance. Through the extensive experimental evaluation on real data sets, we demonstrate the performance of discovery algorithms, and the effectiveness of DDs in several real applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                }
            ]
        },
        {
            "paperId": "838587555854d27925db7af27bd49686d3e3eda4",
            "title": "Two Novel Semantics of Top-k Queries Processing in Uncertain Database",
            "abstract": "Top-k query is a powerful technique in uncertain databases because of the existence of exponential possible worlds, and it is necessary to combine score and confidence of tuples to derive top k answers. Different semantics, the combination methods of score and confidence, lead to different results. U-kRanks and Global Top-k are two semantics of Top-k queries in uncertain database, which consider every alternative in x-tuple as single one and return the tuple which has the highest probability appearing at top k or a given rank. However, no matter which alternative (tuple) of an x-tuple appears in a possible world, it undoubtedly believes that this x-tuple appears in the same possible world accordingly. Thus, instead of ranking every individual tuple, we define two novel Top-k queries semantics in uncertain database, Uncertain x-kRanks queries (U-x-kRanks) and Global x-Top-k queries (G-x-Top-k), which return k entities according to the score and the confidence of alternatives in x-tuple, respectively. In order to reduce the search space, we present an efficient algorithm to process U-x-kRanks queries and G-x-Top-k queries. Comprehensive experiments on different data sets demonstrate the effectiveness of the proposed solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48929153",
                    "name": "Dexi Liu"
                },
                {
                    "authorId": "1761110",
                    "name": "Changxuan Wan"
                },
                {
                    "authorId": "145826495",
                    "name": "N. Xiong"
                },
                {
                    "authorId": "1690341",
                    "name": "L. Yang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "83ea8822954585dd7a3c7f154fd04d38213bf705",
            "title": "Continuous Subgraph Pattern Search over Certain and Uncertain Graph Streams",
            "abstract": "Search over graph databases has attracted much attention recently due to its usefulness in many fields, such as the analysis of chemical compounds, intrusion detection in network traffic data, and pattern matching over users' visiting logs. However, most of the existing works focus on search over static graph databases, while in many real applications, graphs are changing over time. In this paper, we investigate a new problem on continuous subgraph pattern search under the situation where multiple target graphs are constantly changing in a stream style, namely, the subgraph pattern search over graph streams. Obviously, the proposed problem is a continuous join between query patterns and graph streams where the join predicate is the existence of subgraph isomorphism. Due to the NP-completeness of subgraph isomorphism checking, to achieve the real-time monitoring of the existence of certain subgraph patterns, we would like to avoid using subgraph isomorphism verification to find the exact query-stream subgraph isomorphic pairs but to offer an approximate answer that could report all probable pairs without missing any actual answer pairs. Therefore, we propose a lightweight yet effective feature structure called Node-Neighbor Tree to filter out false candidate query-stream pairs. To reduce the computational cost, we propose a novel idea, projecting the feature structures into a numerical vector space and conducting dominant relationship checking in the projected space. We design two methods to efficiently verify dominant relationships, and thus, answer the subgraph search over graph streams efficiently. In addition to answering queries over certain graph streams, we propose a novel problem, detecting the appearance of subgraph patterns over uncertain graph streams with high probability (i.e., larger than the probability threshold specified by users). To address this problem, we not only extend the proposed solutions for certain graphs streams, but also propose a new pruning technique by utilizing the probability threshold. We substantiate our methods with extensive experiments on both certain and uncertain graph streams.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "72462556",
                    "name": "Changliang Wang"
                }
            ]
        },
        {
            "paperId": "88b43ea9898f3d338be204d87fd72b7a7b704930",
            "title": "Enabling routing control in a DHT",
            "abstract": "DHTs are scalable, self-organizing, and adaptive to underlying topology changes, thus being a promising infrastructure for realizing autonomic communications in distributed systems. To provide the above advantages, however, DHTs sacrifice flexibility, that is, all messages are routed by using a common algorithm in a DHT on the assumption that all participant nodes are homogeneous. In practice, nodes in large-scale systems might be heterogeneous with respect to their capabilities, reputations, affiliations of administrative domains, and so on, which consequently makes it preferable to distinguish the heterogeneity of participant nodes and enable flexible control of routing destinations and paths. To achieve this, in this paper we propose a novel approach that supports organizing nodes into groups and enables routing control in a DHT. The effectiveness of our proposals is demonstrated through theoretical analysis and extensive simulations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1908353",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "7828826",
                    "name": "Xicheng Lu"
                },
                {
                    "authorId": "144032853",
                    "name": "Dongsheng Li"
                }
            ]
        },
        {
            "paperId": "b7b2bcff743999d06d81463245314cb4c2fd223f",
            "title": "Optimal Resource Placement in Structured Peer-to-Peer Networks",
            "abstract": "Utilizing the skewed popularity distribution in P2P systems, common in Gnutella and KazaA like P2P applications, we propose an optimal resource (replica or link) placement strategy, which can optimally trade off the performance gain and paid cost. The proposed resource placement strategy, with better results than existing works, can be generally applied in randomized P2P systems (Symphony) and deterministic P2P systems (e.g., Chord, Pastry, Tapestry, etc.). We apply the proposed resource placement strategy, respectively, to two novel applications: PCache (a P2P-based caching scheme) and PRing (a P2P ring structure). The simulation results as well as a real deployment on Planetlab demonstrate the effectiveness of the proposed resource placement strategy in reducing the average search cost of the whole system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144084147",
                    "name": "Weixiong Rao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1699363",
                    "name": "A. Fu"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                }
            ]
        },
        {
            "paperId": "c0f896562ca45916205aa7ee67285e89fa8efc3d",
            "title": "Set similarity join on probabilistic data",
            "abstract": "Set similarity join has played an important role in many real-world applications such as data cleaning, near duplication detection, data integration, and so on. In these applications, set data often contain noises and are thus uncertain and imprecise. In this paper, we model such probabilistic set data on two uncertainty levels, that is, set and element levels. Based on them, we investigate the problem of probabilistic set similarity join (PS2J) over two probabilistic set databases, under the possible worlds semantics. To efficiently process the PS2J operator, we first reduce our problem by condensing the possible worlds, and then propose effective pruning techniques, including Jaccard distance pruning, probability upper bound pruning, and aggregate pruning, which can filter out false alarms of probabilistic set pairs, with the help of indexes and our designed synopses. We demonstrate through extensive experiments the PS2J processing performance on both real and synthetic data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "df1ae3e306f2d06fe5e85ca916bdc69aa60777da",
            "title": "Consistent query answers in inconsistent probabilistic databases",
            "abstract": "Efficient and effective manipulation of probabilistic data has become increasingly important recently due to many real applications that involve the data uncertainty. This is especially crucial when probabilistic data collected from different sources disagree with each other and incur inconsistencies. In order to accommodate such inconsistencies and enable consistent query answering (CQA), in this paper, we propose the all-possible-repair semantics in the context of inconsistent probabilistic databases, which formalize the repairs on the database as repair worlds via a graph representation. In turn, the CQA problem can be converted into one in the so-called repaired possible worlds (w.r.t. both repair worlds and possible worlds). We investigate a series of consistent queries in inconsistent probabilistic databases, including consistent range queries, join, and top-k queries, which, however, need to deal with an exponential number of the repaired possible worlds at high cost. To tackle the efficiency problem of CQA, in this paper, we propose efficient approaches for retrieving consistent query answers, including effective pruning methods to filter out false positives. Extensive experiments have been conducted to demonstrate the efficiency and effectiveness of our approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                }
            ]
        },
        {
            "paperId": "ee0931711019110fb38cae2665256363974c24ce",
            "title": "Ranked Query Processing in Uncertain Databases",
            "abstract": "Recently, many new applications, such as sensor data monitoring and mobile device tracking, raise up the issue of uncertain data management. Compared to \"certain\u00bf data, the data in the uncertain database are not exact points, which, instead, often reside within a region. In this paper, we study the ranked queries over uncertain data. In fact, ranked queries have been studied extensively in traditional database literature due to their popularity in many applications, such as decision making, recommendation raising, and data mining tasks. Many proposals have been made in order to improve the efficiency in answering ranked queries. However, the existing approaches are all based on the assumption that the underlying data are exact (or certain). Due to the intrinsic differences between uncertain and certain data, these methods are designed only for ranked queries in certain databases and cannot be applied to uncertain case directly. Motivated by this, we propose novel solutions to speed up the probabilistic ranked query (PRank) with monotonic preference functions over the uncertain database. Specifically, we introduce two effective pruning methods, spatial and probabilistic pruning, to help reduce the PRank search space. A special case of PRank with linear preference functions is also studied. Then, we seamlessly integrate these pruning heuristics into the PRank query procedure. Furthermore, we propose and tackle the PRank query processing over the join of two distinct uncertain databases. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed approaches in answering PRank queries, in terms of both wall clock time and the number of candidates to be refined.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "f81817ff66923bea473b54c74e2197658a12878f",
            "title": "Automatically weighting tags in XML collection",
            "abstract": "In XML retrieval, nodes with different tags play different roles in XML documents and then tags should be reflected in the relevance ranking. An automatic method is proposed in this paper to infer the weights of tags. We first investigate 15 features about tags, and then select five of them based on the correlations between these features and manual tag weights. Using these features, a tag weight assignment model, ATG, is designed. We evaluate the performance of ATG on two real data sets, IEEECS and Wikipedia from two different perspectives. One is to evaluate the quality of the model by measuring the correlation between weights generated by our model and those given by experts. The other is to test the effectiveness of the model in improving retrieval performance. Experimental results show that the tag weights generated by ATG are highly correlated with the manually assigned weights and the ATG model improves retrieval effectiveness significantly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48929153",
                    "name": "Dexi Liu"
                },
                {
                    "authorId": "1761110",
                    "name": "Changxuan Wan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "27469537",
                    "name": "X. Liu"
                }
            ]
        },
        {
            "paperId": "2b26cbc8ed090d22f05f8d06a0448053152af7e9",
            "title": "Efficient anomaly monitoring over moving object trajectory streams",
            "abstract": "Lately there exist increasing demands for online abnormality monitoring over trajectory streams, which are obtained from moving object tracking devices. This problem is challenging due to the requirement of high speed data processing within limited space cost. In this paper, we present a novel framework for monitoring anomalies over continuous trajectory streams. First, we illustrate the importance of distance-based anomaly monitoring over moving object trajectories. Then, we utilize the local continuity characteristics of trajectories to build local clusters upon trajectory streams and monitor anomalies via efficient pruning strategies. Finally, we propose a piecewise metric index structure to reschedule the joining order of local clusters to further reduce the time cost. Our extensive experiments demonstrate the effectiveness and efficiency of our methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1925686",
                    "name": "Yingyi Bu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1699363",
                    "name": "A. Fu"
                },
                {
                    "authorId": "48929092",
                    "name": "Dawei Liu"
                }
            ]
        },
        {
            "paperId": "36dcb81d874d655bce12f2576410f95c11ba178d",
            "title": "Multiscale Representations for Fast Pattern Matching in Stream Time Series",
            "abstract": "Similarity-based time-series retrieval has been a subject of long-term study due to its wide usage in many applications, such as financial data analysis, weather data forecasting, and multimedia data retrieval. Its original task was to find those time series similar to a pattern (query) time-series data, where both the pattern and data time series are static. Recently, with an increasing demand on stream data management, similarity-based stream time-series retrieval has raised new research issues due to its unique requirements during the stream processing, such as one-pass search and fast response. In this paper, we address the problem of matching both static and dynamic patterns over stream time-series data. We will develop a novel multiscale representation, called multiscale segment mean, for stream time-series data, which can be incrementally computed and thus perfectly adapted to the stream characteristics. Most importantly, we propose a novel multistep filtering mechanism, step by step, over the multiscale representation. Analysis indicates that the mechanism can greatly prune the search space and thus offer fast response. Furthermore, batch processing optimization and the dynamic case where patterns are also from stream time series are discussed. Extensive experiments show the multiscale representation together with the multistep filtering scheme can efficiently filter out false candidates and detect patterns, compared to the multiscale wavelet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                },
                {
                    "authorId": "2146363759",
                    "name": "Jinsong Han"
                },
                {
                    "authorId": "2146393154",
                    "name": "Jian Ma"
                }
            ]
        },
        {
            "paperId": "3b7e4d1767da71035e790e2eed7c53005daf9cca",
            "title": "Top-k dominating queries in uncertain databases",
            "abstract": "Due to the existence of uncertain data in a wide spectrum of real applications, uncertain query processing has become increasingly important, which dramatically differs from handling certain data in a traditional database. In this paper, we formulate and tackle an important query, namely probabilistic top-k dominating (PTD) query, in the uncertain database. In particular, a PTD query retrieves k uncertain objects that are expected to dynamically dominate the largest number of uncertain objects. We propose an effective pruning approach to reduce the PTD search space, and present an efficient query procedure to answer PTD queries. Furthermore, approximate PTD query processing and the case where the PTD query is issued from an uncertain query object are also discussed. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed PTD query processing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "581a1f5560936f7a134b37a91ceabd67f0c3b6f6",
            "title": "Virtual Surrounding Face Geocasting in Wireless Ad Hoc and Sensor Networks",
            "abstract": "Geocasting in wireless sensor and ad hoc networks means delivering a message from a source node to all the nodes in a given geographical region. The objectives of a geocasting protocol are two-fold: guaranteed message delivery and low transmission cost. Most of the existing protocols do not guarantee message delivery, and those that do, incur high transmission costs. In this study, we propose the concept of Virtual Surrounding Face (VSF), and design a VSF-based geocasting protocol (VSFG). We also design a SKIP method and a local dominating set (DS) based restricted flooding technique to further reduce the cost of VSFG. Through mathematical analysis and comprehensive simulations, we show that VSFG, together with SKIP and local DS based restricted flooding, guarantees message delivery and has a much lower transmission cost than the previous approaches. The reduction of cost can be up to 65 % compared with the most efficient existing approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056408901",
                    "name": "Jie Lian"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                },
                {
                    "authorId": "145431598",
                    "name": "S. Naik"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "7596e5078cf725ab32afd59d5773d22dcf1086f6",
            "title": "K-Automorphism: A General Framework For Privacy Preserving Network Publication",
            "abstract": "The growing popularity of social networks has generated interesting data management and data mining problems. An important concern in the release of these data for study is their privacy, since social networks usually contain personal information. Simply removing all identifiable personal information (such as names and social security number) before releasing the data is insufficient. It is easy for an attacker to identify the target by performing different structural queries. In this paper we propose k-automorphism to protect against multiple structural attacks and develop an algorithm (called KM) that ensures k-automorphism. We also discuss an extension of KM to handle \"dynamic\" releases of the data. Extensive experiments show that the algorithm performs well in terms of protection it provides.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144554647",
                    "name": "Lei Zou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "824718ad50c5e3d705405a87b51a6e56e7dce29b",
            "title": "DistanceJoin: Pattern Match Query In a Large Graph Database",
            "abstract": "The growing popularity of graph databases has generated interesting data management problems, such as subgraph search, shortest-path query, reachability verification, and pattern match. Among these, a pattern match query is more flexible compared to a subgraph search and more informative compared to a shortest-path or reachability query. In this paper, we address pattern match problems over a large data graph G. Specifically, given a pattern graph (i.e., query Q), we want to find all matches (in G) that have the similar connections as those in Q. In order to reduce the search space significantly, we first transform the vertices into points in a vector space via graph embedding techniques, coverting a pattern match query into a distance-based multi-way join problem over the converted vector space. We also propose several pruning strategies and a join order selection method to process join processing efficiently. Extensive experiments on both real and synthetic datasets show that our method outperforms existing ones by orders of magnitude.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144554647",
                    "name": "Lei Zou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "90c8e2ec92a14243a7e0c0c8e5a41cd48d68b39a",
            "title": "Max Regional Aggregate over Sensor Networks",
            "abstract": "Nowadays, wireless sensor networks are widely used in many environmental monitoring applications. However, due to the limitation of the current hardware technology, sensors are often battery powered and it is very difficult to change batteries. Therefore, for applications over wireless sensor networks, it is a critical issue to save the energy of sensors. Many attempts have been made to answer various types of queries energy-efficiently, such as max, top-k, and skylines. However, all of them return the readings of individual sensors that satisfy the query constraint. In practice, query results based on individual sensor readings are unreliable because sensor readings are often noisy. Thus, in this paper, we present a new type of query, max aggregate query over a region (i.e. max regional aggregate), which aims to find a fixed-size region whose regional aggregate is the maximum among all the possible regions with the same size. Compared to traditional max queries, max regional aggregate is more reliable in detecting events. Designing an energy-efficient approach to answer max regional aggregate is non-trivial because a huge number of regions need to be investigated. Thus, in this paper, we propose a novel two-level sampling approach, with region and sensor sampling, to collect the sensor readings intelligently and compute the approximate max regional aggregate based on the collected results. Specifically, region sampling is used to select regions and sensor sampling isused to choose the sensors within a selected region. Our extensive simulation results demonstrate that the proposed two-level sampling approach can answer the max regional aggregate energy-efficiently with a desirable accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2766544",
                    "name": "Y. Zhuang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "a404526124746151371cd4bf0cf79cc6e321423c",
            "title": "Multi-Version Music Search Using Acoustic Feature Union and Exact Soft Mapping",
            "abstract": "Research on audio-based music retrieval has primarily concentrated on refining audio features to improve search quality. However, much less work has been done on improving the time efficiency of music audio searches. Representing music audio documents in an indexable format provides a mechanism for achieving efficiency. To address this issue, in this work Exact Locality Sensitive Mapping (ELSM) is suggested to join the concatenated feature sets and soft hash values. On this basis we propose audio-based music indexing techniques, ELSM and Soft Locality Sensitive Hash (SoftLSH) using an optimized Feature Union (FU) set of extracted audio features. Two contributions are made here. First, the principle of similarity-invariance is applied in summarizing audio feature sequences and utilized in training semantic audio representations based on regression. Second, soft hash values are pre-calculated to help locate the searching range more accurately and improve collision probability among features similar to each other. Our algorithms are implemented in a demonstration system to show how to retrieve and evaluate multi-version audio documents. Experimental evaluation over a real \"multi-version\" audio dataset confirms the practicality of ELSM and SoftLSH with FU and proves that our algorithms are effective for both multi-version detection (online query, one-query vs. multi-object) and same content detection (batch queries, multi-queries vs. one-object).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119041014",
                    "name": "Yi Yu"
                },
                {
                    "authorId": "144796189",
                    "name": "K. Joe"
                },
                {
                    "authorId": "1693176",
                    "name": "Vincent Oria"
                },
                {
                    "authorId": "1803673",
                    "name": "F. M\u00f6rchen"
                },
                {
                    "authorId": "145656248",
                    "name": "J. S. Downie"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "a791ddc4ed65d10cb12efd1e393467fa646f168d",
            "title": "General Cost Models for Evaluating Dimensionality Reduction in High-Dimensional Spaces",
            "abstract": "Similarity search usually encounters a serious problem in the high-dimensional space, known as the \"curse of dimensionality\". In order to speed up the retrieval efficiency, most previous approaches reduce the dimensionality of the entire data set to a fixed lower value before building indexes (referred to as global dimensionality reduction (GDR)). More recent works focus on locally reducing the dimensionality of data to different values (called the local dimensionality reduction (LDR)). In addition, random projection is proposed as an approximate dimensionality reduction (ADR) technique to answer the approximate similarity search instead of the exact one. However, so far little work has formally evaluated the effectiveness and efficiency of GDR, LDR, and ADR for the range query. Motivated by this, in this paper, we propose general cost models for evaluating the query performance over the reduced data sets by GDR, LDR, and ADR, in light of which we introduce a novel (A)LDR method, partitioning based on randomized search (PRANS). It can achieve high retrieval efficiency with the guarantee of optimality given by the formal models. Finally, a B+-tree index is constructed over the reduced partitions for fast similarity search. Extensive experiments validate the correctness of our cost models on both real and synthetic data sets and demonstrate the efficiency and effectiveness of the proposed PRANS method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "ba0f7a557dfcfbd5da288a006d88e97047a6a07f",
            "title": "Continuous Subgraph Pattern Search over Graph Streams",
            "abstract": "Search over graph databases has attracted much attention recently due to its usefulness in many fields, such as the analysis of chemical compounds, intrusion detection in network traffic data, and pattern matching over users' visiting logs. However, most of the existing work focuses on search over static graph databases while in many real applications graphs are changing over time. In this paper we investigate a new problem on continuous subgraph pattern search under the situation where multiple target graphs are constantly changing in a stream style, namely the subgraph pattern search over graph streams. Obviously the proposed problem is a continuous join between query patterns and graph streams where the join predicate is the existence of subgraph isomorphism. Due to the NP-completeness of subgraph isomorphism checking, to achieve the real time monitoring of the existence of certain subgraph patterns, we would like to avoid using subgraph isomorphism verification to find the exact query-stream subgraph isomorphic pairs but to offer an approximate answer that could report all probable pairs without missing any of the actual answer pairs. In this paper we propose a light-weight yet effective feature structure called Node-Neighbor Tree to filter false candidate query-stream pairs. To reduce the computational cost, we further project the feature structures into a numerical vector space and conduct dominant relationship checking in the projected space. We propose two methods to efficiently check dominant relationships and substantiate our methods with extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "72462556",
                    "name": "Changliang Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "bbbcfefe14cc17cc62d03541f7eced31c41ca741",
            "title": "Local summarization and multi-level LSH for retrieving multi-variant audio tracks",
            "abstract": "In this paper we study the problem of detecting and grouping multi-variant audio tracks in large audio datasets. To address this issue, a fast and reliable retrieval method is necessary. But reliability requires elaborate representations of audio content, which challenges fast retrieval by similarity from a large audio database. To find a better tradeoff between retrieval quality and efficiency, we put forward an approach relying on local summarization and multi-level Locality-Sensitive Hashing (LSH). More precisely, each audio track is divided into multiple Continuously Correlated Periods (CCP) of variable length according to spectral similarity. The description for each CCP is calculated based on its Weighted Mean Chroma (WMC). A track is thus represented as a sequence of WMCs. Then, an adapted two-level LSH is employed for efficiently delineating a narrow relevant search region. The \"coarse\" hashing level restricts search to items having a non-negligible similarity to the query. The subsequent, \"refined\" level only returns items showing a much higher similarity. Experimental evaluations performed on a real multi-variant audio dataset confirm that our approach supports fast and reliable retrieval of audio track variants.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119041014",
                    "name": "Yi Yu"
                },
                {
                    "authorId": "1719698",
                    "name": "M. Crucianu"
                },
                {
                    "authorId": "1693176",
                    "name": "Vincent Oria"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "bf78503cd84e5f920a7db55c88e67bda2a0f05cb",
            "title": "Effective XML content and structure retrieval with relevance ranking",
            "abstract": "XML documents can be retrieved by means of not only content-only (CO) queries, but also content-and-structure (CAS) queries. Though promising better retrieval precision, CAS queries introduce several new challenges. To address these challenges, we propose a novel approach for XML CAS retrieval. The distinctive feature of the approach is that it adopts a content-oriented point of view. Specifically, the approach first decomposes a CAS query into several fragments, then retrieves results for each query fragment in a content-centric way, and finally scores each answer node. The approach is adaptive to versatile homogeneous and heterogeneous data environments. To assess the relevance of retrieval results to a query fragment, we present a scoring strategy that measures relevance from both content and structure perspectives. In addition, an effective approach is proposed to infer answer nodes based on the CAS query and document structure. An efficient algorithm is also presented for CAS retrieval. Finally, we demonstrate the effectiveness of the proposed methods through comprehensive experimental studies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27469537",
                    "name": "X. Liu"
                },
                {
                    "authorId": "1761110",
                    "name": "Changxuan Wan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "c5b29c5d4ea2ad02e78ace91f31fd9b0adf5f667",
            "title": "Efficient Similarity Join over Multiple Stream Time Series",
            "abstract": "Similarity join (SJ) in time-series databases has a wide spectrum of applications such as data cleaning and mining. Specifically, an SJ query retrieves all pairs of (sub)sequences from two time-series databases that epsiv-match with each other, where epsiv is the matching threshold. Previous work on this problem usually considers static time-series databases, where queries are performed either on disk-based multidimensional indexes built on static data or by nested loop join (NLJ) without indexes. SJ over multiple stream time series, which continuously outputs pairs of similar subsequences from stream time series, strongly requires low memory consumption, low processing cost, and query procedures that are themselves adaptive to time-varying stream data. These requirements invalidate the existing approaches in static databases. In this paper, we propose an efficient and effective approach to perform SJ among multiple stream time series incrementally. In particular, we present a novel method, Adaptive Radius-based Search (ARES), which can answer the similarity search without false dismissals and is seamlessly integrated into SJ processing. Most importantly, we provide a formal cost model for ARES, based on which ARES can be adaptive to data characteristics, achieving the minimum number of refined candidate pairs, and thus, suitable for stream processing. Furthermore, in light of the cost model, we utilize space-efficient synopses that are constructed for stream time series to further reduce the candidate set. Extensive experiments demonstrate the efficiency and effectiveness of our proposed approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "d8316453ba5e63570177dd6e16e756552dba2f4a",
            "title": "A Concise Representation of Range Queries",
            "abstract": "With the advance of wireless communication technology, it is quite common for people to view maps or get related services from the handheld devices, such as mobile phones and PDAs. Range queries, as one of the most commonly used tools, are often posed by the users to retrieve needful information from a spatial database. However, due to the limits of communication bandwidth and hardware power of handheld devices, displaying all the results of a range query on a handheld device is neither communication efficient nor informative to the users. This is simply because that there are often too many results returned from a range query. In view of this problem, we present a novel idea that a concise representation of a specified size for the range query results, while incurring minimal information loss, shall be computed and returned to the user. Such a concise range query not only reduces communication costs, but also offers better usability to the users, providing an opportunity for interactive exploration. The usefulness of the concise range queries is confirmed by comparing it with other possible alternatives, such as sampling and clustering. Then we propose algorithms to find a good concise representation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144744111",
                    "name": "K. Yi"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "3245752",
                    "name": "Feifei Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "da0be79ed07cac19061e00a6b7d3a0e5708ea8bb",
            "title": "Efficient join processing on uncertain data streams",
            "abstract": "Join processing in the streaming environment has many practical applications such as data cleaning and outlier detection. Due to the inherent uncertainty in the real-world data, it has become an increasingly important problem to consider the join processing on uncertain data streams, where the incoming data at each timestamp are uncertain and imprecise. Different from the static databases, processing uncertain data streams has its own requirements such as the limited memory, small response time, and so on. To tackle the challenges with respect to efficiency and effectiveness, in this paper, we formalize the problem of join on uncertain data streams (USJ), which can guarantee the accuracy of USJ answers over uncertain data, and propose effective pruning methods to filter out false alarms. We integrate the pruning methods into an efficient query procedure for incrementally maintaining USJ answers. Extensive experiments have been conducted to demonstrate the efficiency and effectiveness of our approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "e11834698fa1f8c4baaecbd9ffe051040e485fd2",
            "title": "Efficient Processing of Metric Skyline Queries",
            "abstract": "Skyline query is of great importance in many applications, such as multi-criteria decision making and business planning. In particular, a skyline point is a data object in the database whose attribute vector is not dominated by that of any other objects. Previous methods to retrieve skyline points usually assume static data objects in the database (i.e. their attribute vectors are fixed), whereas several recent work focus on skyline queries with dynamic attributes. In this paper, we propose a novel variant of skyline queries, namely metric skyline, whose dynamic attributes are defined in the metric space (i.e. not limited to the Euclidean space). We illustrate an efficient and effective pruning mechanism to answer metric skyline queries through a metric index. Most importantly, we formalize the query performance of the metric skyline query in terms of the pruning power, by a cost model, in light of which we construct an optimized metric index aiming to maximize the pruning power of metric skyline queries. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed pruning techniques as well as the constructed index in answering metric skyline queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                }
            ]
        },
        {
            "paperId": "e2d5c87fe259c3fce3d872159f7f5a7e77775dbd",
            "title": "Evaluating probability threshold k-nearest-neighbor queries over uncertain data",
            "abstract": "In emerging applications such as location-based services, sensor monitoring and biological management systems, the values of the database items are naturally imprecise. For these uncertain databases, an important query is the Probabilistic k-Nearest-Neighbor Query (k-PNN), which computes the probabilities of sets of k objects for being the closest to a given query point. The evaluation of this query can be both computationally- and I/O-expensive, since there is an exponentially large number of k object-sets, and numerical integration is required. Often a user may not be concerned about the exact probability values. For example, he may only need answers that have sufficiently high confidence. We thus propose the Probabilistic Threshold k-Nearest-Neighbor Query (T-k-PNN), which returns sets of k objects that satisfy the query with probabilities higher than some threshold T. Three steps are proposed to handle this query efficiently. In the first stage, objects that cannot constitute an answer are filtered with the aid of a spatial index. The second step, called probabilistic candidate selection, significantly prunes a number of candidate sets to be examined. The remaining sets are sent for verification, which derives the lower and upper bounds of answer probabilities, so that a candidate set can be quickly decided on whether it should be included in the answer. We also examine spatially-efficient data structures that support these methods. Our solution can be applied to uncertain data with arbitrary probability density functions. We have also performed extensive experiments to examine the effectiveness of our methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2108304551",
                    "name": "Jinchuan Chen"
                },
                {
                    "authorId": "1678927",
                    "name": "Xike Xie"
                }
            ]
        },
        {
            "paperId": "f1373c9f83fd8e27c5fca2e82db4e5d850116e61",
            "title": "Discovering matching dependencies",
            "abstract": "Matching dependencies (MDs) are recently proposed for various data quality applications such as detecting the violation of integrity constraints and duplicate object identification. In this paper, we study the problem of discovering matching dependencies for a given database instance. First, we formally define the measures, support and confidence, for evaluating the utility of MDs in the given database instance. Then, we study the discovery of MDs with certain utility requirements of support and confidence. Exact algorithms are developed, together with pruning strategies to improve the time performance. Finally, our experimental evaluation demonstrates the efficiency of the proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "f43b66f229b6c9602dc601336151dbff125eba53",
            "title": "An Efficient Near-Duplicate Video Shot Detection Method Using Shot-Based Interest Points",
            "abstract": "We propose a shot-based interest point selection approach for effective and efficient near-duplicate search over a large collection of video shots. The basic idea is to eliminate the local descriptors with lower frequencies among the selected video frames from a shot to ensure that the shot representation is compact and discriminative. Specifically, we propose an adaptive frame selection strategy called furthest point voronoi (FPV) to produce the shot frame set according to the shot content and frame distribution. We describe a novel strategy named reference extraction (RE) to extract the shot interest descriptors from a keyframe with the support of the selected frame set. We demonstrate the effectiveness and efficiency of the proposed approaches with extensive experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1832787",
                    "name": "Xiangmin Zhou"
                },
                {
                    "authorId": "2155797559",
                    "name": "Xiaofang Zhou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1705708",
                    "name": "A. Bouguettaya"
                },
                {
                    "authorId": "2068583263",
                    "name": "Nong Xiao"
                },
                {
                    "authorId": "2110316098",
                    "name": "John A. Taylor"
                }
            ]
        },
        {
            "paperId": "ff462c1a94cc8e3e725a570e9b8a422277d43743",
            "title": "Proceedings of the First International Workshop on Keyword Search on Structured Data",
            "abstract": "Information search is an indispensable component of our lives. Web search engines are widely used for searching textual documents, images, and video. However, there are also vast collections of structured and semi-structured data both on the Web and in enterprises, such as relational databases, XML data, etc. Traditionally, to access these resources, a user must learn structured or semi-structured query languages, and must be able to access data schemas, which are most likely heterogeneous, complex, and fastevolving. To relieve web and scientific users from the learning curve and enable them to easily access structured and semi-structured data, there is a growing research interest to support keyword search on these data sources. \n \nThe first International Workshop on Keyword Search on Structured Data (KEYS 2009) is held in Providence, Rhode Island, USA on 28th June, 2009, in conjunction with SIGMOD 2009 conference, and aims to encourage researchers from both academia and industry communities to discuss the opportunities and challenges in keyword search on (semi-)structured data, and to present the key issues and novel techniques in this area. In response to the call for papers, KEYS 2009 has attracted 25 submissions. The submissions are highly diversified, coming from Canada, China, Germany, Italy, Japan, Greece, Singapore, Sweden, Thailand, and USA, resulting in an international final program. All submissions were peer reviewed by three program committee members. The program committee selected 6 full research papers and 4 demo and poster papers for inclusion in the proceeding. The accepted papers covered a wide range of research topics and novel applications on keyword search on structured data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                },
                {
                    "authorId": "2118050587",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "074c6f16811ffb95212eccc48dfe3eee817ddd3b",
            "title": "Signature-File-Based Approach for Query Answering Over Wireless Sensor Networks",
            "abstract": "Wireless sensor networks (WSNs) are widely used in many application fields. Because sensor nodes are generally battery powered, to prolong network lifetime, energy conservation becomes a major concern in answering queries over sensor networks. In addition, a robust and fault-tolerant data-collection method is highly desirable against a lossy network with low-quality wireless communication links and unreliable sensor nodes. We propose a signature-file-based approach to approximately answer queries over WSNs. By combining the duplicate-insensitive structure of signature files and the redundant multipath routing approach, we create a robust in-network aggregation scheme, which can answer both aggregative and range queries with high accuracy while significantly reducing the cost of message transmissions. Simulations have been conducted to evaluate the performance of this approach under various network conditions. Compared with previous solutions, our signature-file-based approach achieves the highest accuracy under reasonable energy cost.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2112155230",
                    "name": "Mo Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "103567432",
                    "name": "Jizhong Zhao"
                },
                {
                    "authorId": "1737486",
                    "name": "Qian Zhang"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                }
            ]
        },
        {
            "paperId": "159d316a33f6a25d783f74b2c9e0259b1e9a4605",
            "title": "Nonthreshold-Based Event Detection for 3D Environment Monitoring in Sensor Networks",
            "abstract": "Event detection is a crucial task for wireless sensor network applications, especially environment monitoring. Existing approaches for event detection are mainly based on some predefined threshold values, and thus are often inaccurate and incapable of capturing complex events. For example, in coal mine monitoring scenarios, gas leakage or water osmosis can hardly be described by the overrun of specified attribute thresholds, but some complex pattern in the full-scale view of the environmental data. To address this issue, we propose a non-threshold based approach for the real 3D sensor monitoring environment. We employ energy-efficient methods to collect a time series of data maps from the sensor network and detect complex events through matching the gathered data to spatio-temporal data patterns. Finally, we conduct trace driven simulations to prove the efficacy and efficiency of this approach on detecting events of complex phenomena from real-life records.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112155230",
                    "name": "Mo Li"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "2dae10cb912c408a34e9374fc5f456a431f75fa7",
            "title": "Spotting Significant Changing Subgraphs in Evolving Graphs",
            "abstract": "Graphs are popularly used to model structural relationships between objects. In many application domains such as social networks, sensor networks and telecommunication, graphs evolve over time. In this paper, we study a new problem of discovering the subgraphs that exhibit significant changes in evolving graphs. This problem is challenging since it is hard to define changing regions that are closely related to the actual changes (i.e., additions/deletions of edges/nodes) in graphs. We formalize the problem, and design an efficient algorithm that is able to identify the changing subgraphs incrementally. Our experimental results on real datasets show that our solution is very efficient and the resultant subgraphs are of high quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145976567",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                },
                {
                    "authorId": "2636068",
                    "name": "Yiping Ke"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "352ceaa34a3ea84cd175095795c4193ffde3444c",
            "title": "Efficient Similarity Search in Nonmetric Spaces with Local Constant Embedding",
            "abstract": "Similarity-based search has been a key factor for many applications such as multimedia retrieval, data mining, Web search and retrieval, and so on. There are two important issues related to the similarity search, namely, the design of a distance function to measure the similarity and improving the search efficiency. Many distance functions have been proposed, which attempt to closely mimic human recognition. Unfortunately, some of these well-designed distance functions do not follow the triangle inequality and are therefore nonmetric. As a consequence, efficient retrieval by using these nonmetric distance functions becomes more challenging, since most existing index structures assume that the indexed distance functions are metric. In this paper, we address this challenging problem by proposing an efficient method, that is, local constant embedding (LCE), which divides the data set into disjoint groups so that the triangle inequality holds within each group by constant shifting. Furthermore, we design a pivot selection approach for the converted metric distance and create an index structure to speed up the retrieval efficiency. Moreover, we also propose a novel method to answer approximate similarity search in the nonmetric space with a guaranteed query accuracy. Extensive experiments show that our method works well on various nonmetric distance functions and improves the retrieval efficiency by an order of magnitude compared to the linear scan and existing retrieval approaches with no false dismissals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                }
            ]
        },
        {
            "paperId": "43d617dae735db1f38b776f71c6d69592bb58fa0",
            "title": "SenCast: Scalable multicast in wireless sensor networks",
            "abstract": "Multicast is essential for wireless sensor network (WSN) applications. Existing multicast protocols in WSNs are often designed in a P2P pattern, assuming small number of destination nodes and frequent changes on network topologies. In order to truly adopt multicast in WSNs, we propose a base-station model- based multicast, SenCast, to meet the general requirements of applications. SenCast is scalable and energy-efficient for large group communications in WSNs. Theoretical analysis shows that SenCast is able to approximate the Minimum Nonleaf Nodes (MNN) problem to a ratio of ln\\R\\ (R is the set of all destinations), best known lowest bound. We evaluate our design through comprehensive simulations. Experimental results demonstrate that SenCast outperforms previous multicast protocols including the most recent work uCast.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144478280",
                    "name": "Shaoliang Peng"
                },
                {
                    "authorId": "50342128",
                    "name": "Shanshan Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1730284",
                    "name": "Nong Xiao"
                },
                {
                    "authorId": "2653582",
                    "name": "Yuxing Peng"
                }
            ]
        },
        {
            "paperId": "4639a98fbbbb89649236268d825ad6816544d280",
            "title": "Efficient multi-keyword search over p2p web",
            "abstract": "Current search mechanisms of DHT-based P2P systems can well handle a single keyword search problem. Other than single keyword search, multi-keyword search is quite popular and useful in many real applications. Simply using the solution for single keyword search will require distributed intersection/union operations in wide area networks, leading to unacceptable traffic cost. As it is well known that Bloom Filter (BF) is effective in reducing traffic, we would like to use BF encoding to handle multi-keyword search.\n Applying BF is not difficult, but how to get optimal results is not trivial. In this study we show, through mathematical proof, that the optimal setting of BF in terms of traffic cost is determined by the global statistical information of keywords, not the minimized false positive rate as claimed by previous methods. Through extensive experiments, we demonstrate how to obtain optimal settings. We further argue that the intersection order between sets is important for multi-keyword search. Thus, we design optimal order strategies based on BF for both \"and\" and \"or\" queries. To better evaluate the performance of this design, we conduct extensive simulations on TREC WT10G test collection and the query log of a commercial search engine. Results show that our design significantly reduces the search traffic of existing approach by 73%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1713345",
                    "name": "Hanhua Chen"
                },
                {
                    "authorId": "145914256",
                    "name": "Hai Jin"
                },
                {
                    "authorId": "7899075",
                    "name": "Jiliang Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                },
                {
                    "authorId": "1726587",
                    "name": "L. Ni"
                }
            ]
        },
        {
            "paperId": "52f3e69cf4da5010dba0321ac1fc3dc4b549c6fc",
            "title": "Reverse skyline search in uncertain databases",
            "abstract": "Reverse skyline queries over uncertain databases have many important applications such as sensor data monitoring and business planning. Due to the wide existence of uncertainty in many real-world data, answering reverse skyline queries accurately and efficiently over uncertain data has become increasingly important. In this article, we formalize the probabilistic reverse skyline query over uncertain data, in both monochromatic and bichromatic cases, and propose effective pruning methods, namely spatial pruning and probabilistic pruning, to reduce the search space of the reverse skyline query processing. Moreover, efficient query procedures have been presented seamlessly integrating the proposed pruning methods. Furthermore, a novel query type, namely Probabilistic Reverse Furthest Skyline (PRFS) query, is proposed and tackled under \u201cthe larger, the better\u201d dominance semantics of skyline. Variants of probabilistic reverse skyline have been proposed and tackled, including those that return objects with top-k highest probabilities and that retrieve top-k reverse skylines. Extensive experiments demonstrated the efficiency and effectiveness of our approaches with various experimental settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "53799b9b42c8fe3a5126687ab013689db58bd929",
            "title": "A novel spectral coding in a large graph database",
            "abstract": "Retrieving related graphs containing a query graph from a large graph database is a key issue in many graph-based applications, such as drug discovery and structural pattern recognition. Because sub-graph isomorphism is a NP-complete problem [4], we have to employ a filter-and-verification framework to speed up the search efficiency, that is, using an effective and efficient pruning strategy to filter out the false positives (graphs that are not possible in the results) as many as possible first, then validating the remaining candidates by subgraph isomorphism checking. In this paper, we propose a novel filtering method, a spectral encoding method, i.e. GCoding. Specifically, we assign a signature to each vertex based on its local structures. Then, we generate a spectral graph code by combining all vertex signatures in a graph. Based on spectral graph codes, we derive a necessary condition for sub-graph isomorphism. Then we propose two pruning rules for sub-graph search problem, and prove that they satisfy the no-false-negative requirement (no dismissal in answers). Since graph codes are in numerical space, we take this advantage and conduct efficient filtering over graph codes. Extensive experiments show that GCoding outperforms existing counterpart methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144554647",
                    "name": "Lei Zou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                },
                {
                    "authorId": "1691412",
                    "name": "Yansheng Lu"
                }
            ]
        },
        {
            "paperId": "581a3fbc40a3d3bb2006feec9dffc75c1bc52d16",
            "title": "Efficient Similarity Search over Future Stream Time Series",
            "abstract": "With the advance of hardware and communication technologies, stream time series is gaining ever-increasing attention due to its importance in many applications such as financial data processing, network monitoring, Web click-stream analysis, sensor data mining, and anomaly detection. For all of these applications, an efficient and effective similarity search over stream data is essential. Because of the unique characteristics of the stream, for example, data are frequently updated and real-time response is required, the previous approaches proposed for searching through archived data may not work in the stream scenarios. Especially, in the cases where data often arrive periodically for various reasons (for example, the communication congestion or batch processing), queries on such incomplete time series or even future time series may result in inaccuracy using traditional approaches. Therefore, in this paper, we propose three approaches, polynomial, Discrete Fourier Transform (DFT), and probabilistic, to predict the unknown values that have not arrived at the system and answer similarity queries based on the predicted data. We also apply efficient indexes, that is, a multidimensional hash index and a B+-tree, to facilitate the prediction and similarity search on future time series, respectively. Extensive experiments demonstrate the efficiency and effectiveness of our methods for prediction and answering queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "5932872ab8ff591592892ada0876c18f61263fdb",
            "title": "Searching musical audio datasets by a batch of multi-variant tracks",
            "abstract": "Multi-variant music tracks are those audio tracks of a particular song which are sung and recorded by different people (i.e., cover songs). As music social clubs grow on the Internet, more and more people like to upload music recordings onto such music social sites to share their own home-produced albums and participate in Internet singing contests. Therefore it is very important to explore a computer-assisted evaluation tool to detect these audio-based multi-variant tracks. In this paper we investigate such a task: the original track of a song is embedded in datasets, with a batch of multi-variant audio tracks of this song as input, our retrieval system returns an ordered list by similarity and indicates the position of relevant audio track. To help process multi-variant audio tracks, we suggest a semantic indexing framework and propose the Federated Features (FF) scheme to generate the semantic summarization of audio feature sequences. The conjunction of federated features with three typical similarity searching schemes, K-Nearest Neighbor (KNN), Locality Sensitive Hashing (LSH), and Exact Euclidian LSH (E2LSH), is evaluated. From these findings, a computer-assisted evaluation tool for searching multi-variant audio tracks was developed to search over large musical audio datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119041014",
                    "name": "Yi Yu"
                },
                {
                    "authorId": "145656248",
                    "name": "J. S. Downie"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1693176",
                    "name": "Vincent Oria"
                },
                {
                    "authorId": "144796189",
                    "name": "K. Joe"
                }
            ]
        },
        {
            "paperId": "7ab4682208a5e7549f5d55756ff3a191e551d0b5",
            "title": "COSIN: content-based retrieval system for cover songs",
            "abstract": "We develop a content-based audio COver Song IdeNtification (COSIN) system to detect/group cover songs. The COSIN takes music audio content as input and performs similarity searching to locate variants of the input (i.e., cover versions). Identified cover songs are returned in the rank order according to their similarity to the input. The COSIN also incorporates a set of tools to evaluate retrieval performance so researchers can explore different retrieval schemes and parameters (e.g. recall, precision). The COSIN utilizes a suite of techniques to detect cover songs including: Pitch + Dynamic Programming (DP), Chroma + DP, and Semantic Feature Summarization (SFS) + Hash-Based Approximate Matching (HBAM). Demonstration system shows that COSIN is a very potential music content retrieval tool. Running some music retrieval schemes on COSIN platform, recent experiments with SFS + LSH Variants demonstrate a nicely balanced efficiency (search speed) v. performance (search accuracy) tradeoff.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119041014",
                    "name": "Yi Yu"
                },
                {
                    "authorId": "145656248",
                    "name": "J. S. Downie"
                },
                {
                    "authorId": "1803673",
                    "name": "F. M\u00f6rchen"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144796189",
                    "name": "K. Joe"
                },
                {
                    "authorId": "1693176",
                    "name": "Vincent Oria"
                }
            ]
        },
        {
            "paperId": "81ae32ba30650fd6085113fbe6851763a9689880",
            "title": "Privacy preserving serial data publishing by role composition",
            "abstract": "Previous works about privacy preserving serial data publishing on dynamic databases have relied on unrealistic assumptions of the nature of dynamic databases. In many applications, some sensitive values changes freely while others never change. For example, in medical applications, the disease attribute changes with time when patients recover from one disease and develop another disease. However, patients do not recover from some diseases such as HIV. We call such diseases permanent sensitive values. To the best of our knowledge, none of the existing solutions handle these realistic issues. We propose a novel anonymization approach called HD-composition to solve the above problems. Extensive experiments with real data confirm our theoretical results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1925686",
                    "name": "Yingyi Bu"
                },
                {
                    "authorId": "1699363",
                    "name": "A. Fu"
                },
                {
                    "authorId": "1723745",
                    "name": "R. C. Wong"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1786185",
                    "name": "Jiuyong Li"
                }
            ]
        },
        {
            "paperId": "85c92732e75dabb361ec4d7d06b5956c8719e167",
            "title": "Pattern Matching over Cloaked Time Series",
            "abstract": "In many privacy preserving applications such as Location-Based Services (LBS), medical data analysis, and data sequence matching, users often deliberately disturb the original data in order to avoid the release of their private information. Although these disturbed cloaked data cannot reveal the privacy information of individual users, they can still help perform some data mining tasks such as data classification. In this paper, we study one important and fundamental query predicate, that is, to find the cloaked time series that are similar to a query pattern. In this paper, we formalize such similarity search problem over the cloaked time series, and propose a novel approach to index the cloaked series, which can facilitate the similarity query.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                }
            ]
        },
        {
            "paperId": "91397fb14c2872885e4f5863e8b501650fc44065",
            "title": "Similarity Search in Arbitrary Subspaces Under Lp-Norm",
            "abstract": "Similarity search has been widely used in many applications such as information retrieval, image data analysis, and time-series matching. Specifically, a similarity query retrieves all data objects in a data set that are similar to a given query object. Previous work on similarity search usually consider the search problem in the full space. In this paper, however, we propose a novel problem, subspace similarity search, which finds all data objects that match with a query object in the subspace instead of the original full space. In particular, the query object can specify arbitrary subspace with arbitrary number of dimensions. Since traditional approaches for similarity search cannot be applied to solve the proposed problem, we introduce an efficient and effective pruning technique, which assigns scores to data objects with respect to pivots and prunes candidates via scores. We propose an effective multipivot-based method to pre-process data objects by selecting appropriate pivots, where the entire procedure is guided by a formal cost model, such that the pruning power is maximized. Finally, scores of each data object are organized in sorted list to facilitate an efficient subspace similarity search. Extensive experiments have verified the correctness of our cost model and demonstrated the efficiency and effectiveness of our proposed approach for the subspace similarity search.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "944120ab35b71e81a5cc158c795af0c48cf251c5",
            "title": "Monochromatic and bichromatic reverse skyline search over uncertain databases",
            "abstract": "Reverse skyline queries over uncertain databases have many important applications such as sensor data monitoring and business planning. Due to the existence of uncertainty in many real-world data, answering reverse skyline queries accurately and efficiently over uncertain data has become increasingly important. In this paper, we model the probabilistic reverse skyline query on uncertain data, in both monochromatic and bichromatic cases, and propose effective pruning methods to reduce the search space of query processing. Moreover, efficient query procedures have been presented seamlessly integrating the proposed pruning methods. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed approach with various experimental settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "97669a145aa4011e1cabd1a2ca269256774c240e",
            "title": "Dynamic skyline queries in metric spaces",
            "abstract": "Skyline query is of great importance in many applications, such as multi-criteria decision making and business planning. In particular, a skyline point is a data object in the database whose attribute vector is not dominated by that of any other objects. Previous methods to retrieve skyline points usually assume static data objects in the database (i.e. their attribute vectors are fixed), whereas several recent work focus on skyline queries with dynamic attributes. In this paper, we propose a novel variant of skyline queries, namely metric skyline, whose dynamic attributes are defined in the metric space (i.e. not limited to the Euclidean space). We illustrate an efficient and effective pruning mechanism to answer metric skyline queries through a metric index. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed pruning techniques over the metric index in answering metric skyline queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                }
            ]
        },
        {
            "paperId": "9a1b4d7504bab8282affab8e241e951c7009090f",
            "title": "Maximal Subspace Coregulated Gene Clustering",
            "abstract": "Clustering is a popular technique for analyzing microarray data sets, with n genes and m experimental conditions. As explored by biologists, there is a real need to identify coregulated gene clusters, which include both positive and negative regulated gene clusters. The existing pattern-based and tendency-based clustering approaches cannot directly be applied to find such coregulated gene clusters, because they are designed for finding positive regulated gene clusters. In this paper, in order to cluster coregulated genes, we propose a coding scheme that allows us to cluster two genes into the same cluster if they have the same code, where two genes that have the same code can be either positive or negative regulated. Based on the coding scheme, we propose a new algorithm for finding maximal subspace coregulated gene clusters with new pruning techniques. A maximal subspace coregulated gene cluster clusters a set of genes on a condition sequence such that the cluster is not included in any other subspace coregulated gene clusters. We conduct extensive experimental studies. Our approach can effectively and efficiently find maximal subspace coregulated gene clusters. In addition, our approach outperforms the existing approaches for finding positive regulated gene clusters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2124211783",
                    "name": "Yuhai Zhao"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": null,
                    "name": "Bin Wang"
                },
                {
                    "authorId": "145024112",
                    "name": "Ge Yu"
                }
            ]
        },
        {
            "paperId": "a0378320ca434a40c392df7a964d020afd047869",
            "title": "Pareto-Based Dominant Graph: An Efficient Indexing Structure to Answer Top-K Queries",
            "abstract": "Given a record set D and a query score function F, a top-k query returns k records from D, whose values of function F on their attributes are the highest. In this paper, we investigate the intrinsic connection between top-k queries and dominant relationships between records, and based on which, we propose an efficient layer-based indexing structure, Pareto-Based Dominant Graph (DG), to improve the query efficiency. Specifically, DG is built offline to express the dominant relationship between records and top-k query is implemented as a graph traversal problem, i.e., Traveler algorithm. We prove theoretically that the size of search space (that is the number of retrieved records from the record set to answer top-k query) in our algorithm is directly related to the cardinality of skyline points in the record set (see Theorem 3). Considering I/O cost, we propose cluster-based storage schema to reduce I/O cost in Traveler algorithm. We also propose the cost estimation methods in this paper. Based on cost analysis, we propose an optimization technique, pseudorecord, to further improve the search efficiency. In order to handle the top-k query in the high-dimension record set, we also propose N-Way Traveler algorithm. In order to handle DG maintenance efficiently, we propose \u201cInsertion\u201d and \u201cDeletion\u201d algorithms for DG. Finally, extensive experiments demonstrate that our proposed methods have significant improvement over its counterparts, including both classical and state art of top-k algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144554647",
                    "name": "Lei Zou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "afa85a6ab1d9e2752412fe3beea56afed5de1147",
            "title": "Using Exact Locality Sensitive Mapping to Group and Detect Audio-Based Cover Songs",
            "abstract": "Cover song detection is becoming a very hot research topic when plentiful personal music recordings or performance are released on the Internet. A nice cover song recognizer helps us group and detect cover songs to improve the searching experience. The traditional detection is to match two musical audio sequences by exhaustive pairwise comparisons. Different from the existing work, our aim is to generate a group of concatenated feature sets based on regression modeling and arrange them by indexing-based approximate techniques to avoid complicated audio sequence comparisons. We mainly focus on using exact locality sensitive mapping (ELSM) to join the concatenated feature sets and soft hash values. Similarity-invariance among audio sequence comparison is applied to define an optimal combination of several audio features. Soft hash values are pre-calculated to help locate searching range more accurately. Furthermore, we implement our algorithms in analyzing the real audio cover songs and grouping and detecting a batch of relevant cover songs embedded in large audio datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119041014",
                    "name": "Yi Yu"
                },
                {
                    "authorId": "145656248",
                    "name": "J. S. Downie"
                },
                {
                    "authorId": "1803673",
                    "name": "F. M\u00f6rchen"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144796189",
                    "name": "K. Joe"
                }
            ]
        },
        {
            "paperId": "cb1f27baba28ff5e064890470b25a11202ea70e6",
            "title": "Speed up semantic search in p2p networks",
            "abstract": "Peer-to-peer architectures become popular in modern massively distributed systems, which are often in very large scale and contain a huge volume of heterogeneous data. To facilitate the information retrieval process in P2P networks, we consider semantic search approach, where syntax-based queries are shipped to peers based on semantic correlations. Motivated by an interesting experience in Web information retrieval, we propose a novel ontology-based scheme to measure similarity of peer interests accurately and consistently in a decentralized way, and group peers under a scalable hierarchical overlay network. Given queries, our approach either floods them within local peer groups or guides them towards remote groups based on the similarity of interests. Our work overcomes the limitations of the existing P2P hybrid-search approaches by avoiding costly data popularity measurement. Performance evaluation and comparison against baseline algorithms show that our approach provides a better solution for information retrieval in large-scale P2P networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2160673558",
                    "name": "Qiang Wang"
                },
                {
                    "authorId": "145765726",
                    "name": "Rui Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2056408901",
                    "name": "Jie Lian"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "d9b8d1914db1793faa01e950d0dc715c14b286ec",
            "title": "Flexible Routing in Grouped DHTs",
            "abstract": "In most DHTs proposed so far, all nodes are assumed to be homogeneous, and all messages are routed using a common algorithm. In practice, however, nodes in large-scale systems might be heterogeneous with respect to their capabilities, reputations, affiliations of administrative domains, and so on, which consequently makes it preferable to distinguish the heterogeneity of participant nodes. To achieve this, in this paper we present grouped tapestry (GTap), a novel tapestry-based DHT that supports organizing nodes into groups and allows flexible DHT routing. The effectiveness of our proposals is demonstrated through theoretical analysis and extensive simulations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1908353",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "144032853",
                    "name": "Dongsheng Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "7828826",
                    "name": "Xicheng Lu"
                }
            ]
        },
        {
            "paperId": "e30cfce4d2664b02b6997733e138b95c8c724e20",
            "title": "Detection of Shape Anomalies: A Probabilistic Approach Using Hidden Markov Models",
            "abstract": "We study the problem of detecting the shape anomalies in this paper. Our shape anomaly detection algorithm is performed on the one-dimensional representation (time series) of shapes, whose similarity is modeled by a generalized segmental hidden Markov model (HMM) under a scaling, translation and rotation invariant manner. Experimental results show that our proposed approach can find shape anomalies in a large collection of shapes effectively and efficiently.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145976567",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145200192",
                    "name": "Di Wu"
                }
            ]
        },
        {
            "paperId": "ee25e4d1afb8f78926a69e17d6f5203aee69a9ab",
            "title": "Continuous Answering Holistic Queries over Sensor Networks",
            "abstract": "Sensor networks are widely used in various domains like the intelligent transportation systems. Users issue queries to sensors and collect sensing data. Due to the low quality sensing devices or random link failures, sensor data are often noisy. In order to increase the reliability of the query results, continuous queries are often employed. In this work we focus on continuous holistic queries like Median. Existing approaches are mainly designed for non-holistic queries like Average. However, it is not trivial to answer holistic ones due to their non-decomposable property. We first propose two schemes based on the data correlation between different rounds, with one for getting the exact answers and the other one for deriving the approximate results. We then combine the two proposed schemes into a hybrid approach, which is adaptive to the data changing speed. We evaluate this design through extensive simulations. The results show that our approach significantly reduces the traffic cost compared with previous works while maintaining the same accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46578147",
                    "name": "Kebin Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                },
                {
                    "authorId": "2055685016",
                    "name": "Wei Gong"
                },
                {
                    "authorId": "144297190",
                    "name": "A. Nayak"
                }
            ]
        },
        {
            "paperId": "f07ed030a2ce2e0b5d0346636bb249541acc81a3",
            "title": "Probabilistic Group Nearest Neighbor Queries in Uncertain Databases",
            "abstract": "The importance of query processing over uncertain data has recently arisen due to its wide usage in many real-world applications. In the context of uncertain databases, previous works have studied many query types such as nearest neighbor query, range query, top-k query, skyline query, and similarity join. In this paper, we focus on another important query, namely, probabilistic group nearest neighbor (PGNN) query, in the uncertain database, which also has many applications. Specifically, given a set, Q, of query points, a PGNN query retrieves data objects that minimize the aggregate distance (e.g., sum, min, and max) to query set Q. Due to the inherent uncertainty of data objects, previous techniques to answer group nearest neighbor (GNN) query cannot be directly applied to our PGNN problem. Motivated by this, we propose effective pruning methods, namely, spatial pruning and probabilistic pruning, to reduce the PGNN search space, which can be seamlessly integrated into our PGNN query procedure. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed approach, in terms of the wall clock time and the speed-up ratio against linear scan.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "f861ad9efe9b6af44896dcd96c41030ed34b8c0e",
            "title": "Probabilistic ranked queries in uncertain databases",
            "abstract": "Recently, many new applications, such as sensor data monitoring and mobile device tracking, raise up the issue of uncertain data management. Compared to \"certain\" data, the data in the uncertain database are not exact points, which, instead, often locate within a region. In this paper, we study the ranked queries over uncertain data. In fact, ranked queries have been studied extensively in traditional database literature due to their popularity in many applications, such as decision making, recommendation raising, and data mining tasks. Many proposals have been made in order to improve the efficiency in answering ranked queries. However, the existing approaches are all based on the assumption that the underlying data are exact (or certain). Due to the intrinsic differences between uncertain and certain data, these methods are designed only for ranked queries in certain databases and cannot be applied to uncertain case directly. Motivated by this, we propose novel solutions to speed up the probabilistic ranked query (PRank) over the uncertain database. Specifically, we introduce two effective pruning methods, spatial and probabilistic, to help reduce the PRank search space. Then, we seamlessly integrate these pruning heuristics into the PRank query procedure. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed approach in answering PRank queries, in terms of both wall clock time and the number of candidates to be refined.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "10597721f4f4fb83fc205ef61b0054487eb0622e",
            "title": "Collaborative Search in Large-scale Unstructured Peer-to-Peer Networks",
            "abstract": "Searching in large-scale unstructured peer-to-peer networks is challenging due to the lack of effective hint information to guide queries. In this paper, we propose POP, a Parallel, collaborative and Probabilistic search mechanism, in which query messages are viewed as search units to collaborate with each other and aggregate the distributed hints during the search process. A scheme called distributed Bloom filter (DBF) is presented to propagate the hints with a bandwidth-aware manner, in which a node divides the received Bloom filter vector into subvectors and disseminates the fragments to its neighbors according to their bandwidth capacity. The effectiveness of POP is demonstrated through theoretical analysis and extensive simulations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1908353",
                    "name": "Yiming Zhang"
                },
                {
                    "authorId": "2124829514",
                    "name": "Dongsheng Li"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "7828826",
                    "name": "Xicheng Lu"
                }
            ]
        },
        {
            "paperId": "4f62d84f1171ec81519ecbe573fc617e174dd753",
            "title": "Indexable PLA for Efficient Similarity Search",
            "abstract": "Similarity-based search over time-series databases has been a hot research topic for a long history, which is widely used in many applications, including multimedia retrieval, data mining, web search and retrieval, and so on. However, due to high dimensionality (i.e. length) of the time series, the similarity search over directly indexed time series usually encounters a serious problem, known as the \"dimensionality curse\". Thus, many dimensionality reduction techniques are proposed to break such curse by reducing the dimensionality of time series. Among all the proposed methods, only Piecewise Linear Approximation (PLA) does not have indexing mechanisms to support similarity queries, which prevents it from efficiently searching over very large time-series databases. Our initial studies on the effectiveness of different reduction methods, however, show that PLA performs no worse than others. Motivated by this, in this paper, we re-investigate PLA for approximating and indexing time series. Specifically, we propose a novel distance function in the reduced PLA-space, and prove that this function indeed results in a lower bound of the Euclidean distance between the original time series, which can lead to no false dismissals during the similarity search. As a second step, we develop an effective approach to index these lower bounds to improve the search efficiency. Our extensive experiments over a wide spectrum of real and synthetic data sets have demonstrated the efficiency and effectiveness of PLA together with the newly proposed lower bound distance, in terms of both pruning power and wall clock time, compared with two state-of-the-art reduction methods, Adaptive Piecewise Constant Approximation (APCA) and Chebyshev Polynomials (CP).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143716572",
                    "name": "Qiuxia Chen"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                }
            ]
        },
        {
            "paperId": "683f3a5d0e2e56d1fbb8f8d26dcdcb06bb87e113",
            "title": "A push-based prefetching for cooperative caching RAM Grid",
            "abstract": "As an innovative distributed computing technique for sharing the memory resources in high-speed network, RAM Grid exploits the distributed free nodes, and provides remote memory for the nodes which are short of memory. One of the RAM Grid systems named DRACO, tries to provide cooperative caching to improve the performance of the user node which has mass disk I/O but lacks local memory. However, the performance of DRACO is constrained with the network communication cost. In order to hide the latency of remote memory access and improve the caching performance, we proposed using push- based prefetching to enable the caching providers to push the potential useful memory pages to the user nodes. Specifically, for each caching provider, it employs sequential pattern mining techniques, which adapts to the characteristics of memory page access sequences, on locating useful memory pages for prefetching. We have verified the effectiveness of the proposed method through system analysis and trace-driven simulations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3216580",
                    "name": "Rui Chu"
                },
                {
                    "authorId": "1730284",
                    "name": "Nong Xiao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "7828826",
                    "name": "Xicheng Lu"
                }
            ]
        },
        {
            "paperId": "a56f0695fbe214143012cd32b2c4c0ac2e9c1551",
            "title": "Probabilistic correlation-based similarity measure of unstructured records",
            "abstract": "Computing the similarity between unstructured records is a fundamental function in multiple applications. Approximate string matching and full text retrieval techniques do not show the best performance when applied directly, since the information are limited in unstructured records of short record length. In this paper, we propose a novel probabilistic correlation-based similarity measure. Rather than simply conducting the exact matching tokens of two records, our similarity evaluation enriches the information of records by considering the correlations of tokens. We define the probabilistic correlation between tokens as the probability that these tokens appear in the same records. Then we compute the weight of tokens and discover the correlations of records based on the probabilistic correlations of tokens. Finally, we present extensive experimental results to demonstrate the effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225865",
                    "name": "Shaoxu Song"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "a7a70bb859adb99c7be4f8728f5db8a29fa69358",
            "title": "Optimal proactive caching in peer-to-peer network: analysis and application",
            "abstract": "As a promising new technology with the unique properties like high efficiency, scalability and fault tolerance, Peer-to-Peer (P2P) technology is used as the underlying network to build new Internet-scale applications. However, one of the well known issues in such an application (for example WWW) is that the distribution of data popularities is heavily tailed with a Zipf-like distribution. With consideration of the skewed popularity we adopt a proactive caching approach to handle the challenge, and focus on two key problems: where (i.e. the placement strategy: where to place the replicas) and how (i.e. the degree problem: how many replicas are assigned to one specific content)? For the where problem, we propose a novel approach which can be generally applied to structured P2P networks. Next, we solve two optimization objectives related to the how problem: MAX_PERF and MIN_COST. Our solution is called <B>PoPCache</B>, and we discover two interesting properties: (1) the number of replicas assigned to each content is proportional to its popularity; (2) the derived optimal solutions are related to the entropy of popularity. To our knowledge, none of the previous works has mentioned such results. Finally, we apply the results of PoPCache to propose a P2P base web caching, called as Web-PoPCache. By means of web cache trace driven simulation, our extensive evaluation results demonstrate the advantages of PoPCache and Web-PoPCache.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144084147",
                    "name": "Weixiong Rao"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1699363",
                    "name": "A. Fu"
                },
                {
                    "authorId": "1925686",
                    "name": "Yingyi Bu"
                }
            ]
        },
        {
            "paperId": "c525ae091934a5e73e0726d045ff2b60d21477e7",
            "title": "A Weighted Moving Average-based Approach for Cleaning Sensor Data",
            "abstract": "Nowadays, wireless sensor networks have been widely used in many monitoring applications. Due to the low quality of sensors and random effects of the environments, however, it is well known that the collected sensor data are noisy. Therefore, it is very critical to clean the sensor data before using them to answer queries or conduct data analysis. Popular data cleaning approaches, such as the moving average, cannot meet the requirements of both energy efficiency and quick response time in many sensor related applications. In this paper, we propose a hybrid sensor data cleaning approach with confidence. Specifically, we propose a smart weighted moving average (WMA) algorithm that collects confidence data from sensors and computes the weighted moving average. The rationale behind the WMA algorithm is to draw more samples for a particular value that is of great importance to the moving average, and provide higher confidence weight for this value, such that this important value can be quickly reflected in the moving average. Based on our extensive simulation results, we demonstrate that, compared to the simple moving average (SMA), our WMA approach can effectively clean data and offer quick response time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2766544",
                    "name": "Y. Zhuang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2118774785",
                    "name": "X. Wang"
                },
                {
                    "authorId": "2056408901",
                    "name": "Jie Lian"
                }
            ]
        },
        {
            "paperId": "ce89fcf2a7887657ffa729532b777fd4bfd9b665",
            "title": "Mining Frequent Trajectory Patterns for Activity Monitoring Using Radio Frequency Tag Arrays",
            "abstract": "Activity monitoring, a crucial task in many applications, is often conducted expensively using video cameras. Also, effectively monitoring a large field by analyzing images from multiple cameras remains a challenging problem. In this paper, we introduce a novel application of the recently developed RFID technology: using RF tag arrays for activity monitoring, where data mining techniques play a critical role. The RFID technology provides an economically attractive solution due to the low cost of RF tags and readers. Another novelty of this design is that the tracking objects do not need to attach any transmitters or receivers, such as tags or readers. By developing a practical fault-tolerant method, we offset the noise of RF tag data and mine frequent trajectory patterns as models of regular activities. Our empirical study using real RFID systems and data sets verifies the feasibility and the effectiveness of our design",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145525190",
                    "name": "J. Pei"
                },
                {
                    "authorId": "2143716572",
                    "name": "Qiuxia Chen"
                },
                {
                    "authorId": "2134770433",
                    "name": "Yiyang Zhao"
                }
            ]
        },
        {
            "paperId": "d1b305a3ede4266d2bcb32e03733e4548a50e049",
            "title": "Similarity Match Over High Speed Time-Series Streams",
            "abstract": "Similarity-based time series retrieval has been a subject of long term study due to its wide usage in many applications, such as financial data analysis, weather data forecasting, and multimedia data retrieval. Its original task was to find those time series similar to a pattern (query) time series data, where both the pattern and data time series are static. Recently, with an increasing demand on stream data management, similarity-based stream time series retrieval has raised new research issues due to its unique requirements during the stream processing, such as one-pass search and fast response. In this paper, we address the problem of matching patterns over high-speed stream time series data. We will develop a novel representation, called multi-scaled segment mean (MSM), for stream time series data, which can be incrementally computed and thus perfectly adapted to the stream characteristics. Most importantly, we propose a novel multi-step filtering mechanism over the multi-scaled representation. Analysis indicates that the mechanism can greatly prune the search space and thus offer fast response. Extensive experiments show the multi-scaled representation together with the multi-step filtering scheme can efficiently filter out false candidates and detect patterns, compared to the multiscaled wavelet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                },
                {
                    "authorId": "145024112",
                    "name": "Ge Yu"
                }
            ]
        },
        {
            "paperId": "d589b209cf4837325a6c48642cf216611befd31e",
            "title": "Non-Threshold based Event Detection for 3D Environment Monitoring in Sensor Networks",
            "abstract": "Event detection is a crucial task for wireless sensor network applications, especially environment monitoring. Existing approaches for event detection are mainly based on some predefined threshold values, and thus are often inaccurate and incapable of capturing complex events. For example, in coal mine monitoring scenarios, gas leakage or water osmosis can hardly be described by the overrun of specified attribute thresholds, but some complex pattern in the full-scale view of the environmental data. To address this issue, we propose a non-threshold based approach for the real 3D sensor monitoring environment. We employ energy-efficient methods to collect a time series of data maps from the sensor network and detect complex events through matching the gathered data to spatio-temporal data patterns. Finally, we conduct trace driven simulations to prove the efficacy and efficiency of this approach on detecting events of complex phenomena from real-life records.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112155230",
                    "name": "Mo Li"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "f998ffe2addcc3844a56b7cc231d2d4192219667",
            "title": "A General Cost Model for Dimensionality Reduction in High Dimensional Spaces",
            "abstract": "Similarity search usually encounters a serious problem in the high dimensional space, known as the \"curse of dimensionality\". In order to speed up the retrieval efficiency, previous approaches usually reduce the dimensionality of the entire data set to a fixed lower value before building indexes (referred to as global dimensionality reduction (GDR)). More recent works focus on locally reducing the dimensionality of data to different values (called the local dimensionality reduction (LDR)). However, so far little work has formally evaluated the effectiveness and efficiency of both GDR and LDR for range queries. Motivated by this, in this paper, we propose a general cost model for both GDR and LDR, in light of which we introduce a novel LDR method, PRANS. It can achieve high retrieval efficiency with the guarantee of optimality given by the formal model. Finally, a B+-tree index is constructed over the reduced partitions for fast similarity search. Extensive experiments validate the correctness of our cost model on both real and synthetic data sets, and demonstrate the efficiency and effectiveness of the proposed PRANS method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "ff9bfd92118bf09ba26eb411cd2d89d19579c08e",
            "title": "Top-k subgraph matching query in a large graph",
            "abstract": "Recently, due to its wide applications, subgraph search has attracted a lot of attention from database and data mining community. Sub-graph search is defined as follows: given a query graph Q, we report all data graphs containing Q in the database. However, there is little work about sub-graph search in a single large graph, which has been used in many applications, such as biological network and social network.\n In this paper, we address top-k sub-graph matching query problem, which is defined as follows: given a query graph Q, we locate top-k matchings of Q in a large data graph G according to a score function. The score function is defined as the sum of the pairwise similarity between a vertex in Q and its matching vertex in G. Specifically, we first design a balanced tree (that is G-Tree) to index the large data graph. Then, based on G-Tree, we propose an efficient query algorithm (that is Ranked Matching algorithm). Our extensive experiment results show that, due to efficiency of pruning strategy, given a query with up to 20 vertices, we can locate the top-100 matchings in less than 10 seconds in a large data graph with 100K vertices. Furthermore, our approach outperforms the alternative method by orders of magnitude.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144554647",
                    "name": "Lei Zou"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1691412",
                    "name": "Yansheng Lu"
                }
            ]
        },
        {
            "paperId": "174b8ec5a3a4aea9a3f50cf14c0bb4313d58ed5f",
            "title": "Efficient Methods on Predictions for Similarity Search over Stream Time Series",
            "abstract": "Due to the wide usage of stream time series, an efficient and effective similarity search over stream data becomes essential for many applications. Although many approaches have been proposed for searching through archived data, because of the unique characteristics of the stream, for example, data are frequently updated, traditional methods may not work for the stream time series. Especially, for the cases where the arrival of data is often delayed for various reasons, for example, the communication congestion or batch processing and so on, queries on such incomplete time series or even future time series may result in inaccuracy. Therefore, in this paper we propose two approaches, polynomial and probabilistic, to predict the unknown values that have not arrived at the system. We also present efficient indexes, that is, a multidimensional hash index and B+-tree, to facilitate the prediction and similarity search on future time series, respectively. Extensive experiments demonstrate the efficiency and effectiveness of our methods in terms of I/O, prediction and query accuracy",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1800925",
                    "name": "Xiang Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "86adcad240e1b7192118d7a7e6c67880c44b7426",
            "title": "Mixed Type Audio Classification with Support Vector Machine",
            "abstract": "Content-based classification of audio data is an important problem for various applications such as overall analysis of audio-visual streams, boundary detection of video story segment, extraction of speech segments from video, and content-based video retrieval. Though the classification of audio into single type such as music, speech, environmental sound and silence is well studied, classification of mixed type audio data, such as clips having speech with music as background, is still considered a difficult problem. In this paper, we present a mixed type audio classification system based on support vector machine (SVM). In order to capture characteristics of different types of audio data, besides selecting audio features, we also design four different representation formats for each feature. Our SVM-based audio classifier can classify audio data into five types: music, speech, environment sound, speech mixed with music, and music mixed with environment sound. The experimental results show that our system outperforms other classification systems using k nearest neighbor (k-NN), neural network (NN), and Naive Bayes (NB)",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "7782730",
                    "name": "\u015e. \u00d6\u011f\u00fcd\u00fcc\u00fc"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "aaf8d2feb846635456017db63ee21fbf88d9dad4",
            "title": "Contour map matching for event detection in sensor networks",
            "abstract": "Many sensor network applications, such as object tracking and disaster monitoring, require effective techniques for event detection. In this paper, we propose a novel event detection mechanism based on matching the contour maps of in-network sensory data distribution. Our key observation is that events in sensor networks can be abstracted into spatio-temporal patterns of sensory data and that pattern matching can be done efficiently through contour map matching. Therefore, we propose simple SQL extensions to allow users to specify common types of events as patterns in contour maps and study energy-efficient techniques of contour map construction and maintenance for our pattern-based event detection. Our experiments with synthetic workloads derived from a real-world coal mine surveillance application validate the effectiveness and efficiency of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2869921",
                    "name": "Wenwei Xue"
                },
                {
                    "authorId": "153889459",
                    "name": "Qiong Luo"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2051647248",
                    "name": "Yunhao Liu"
                }
            ]
        },
        {
            "paperId": "b556e3ec83ae0eb40342ed256dc27e4d5d61d4d4",
            "title": "Enhancing Edit Distance on Real Sequences Filters using Histogram Distance on Fixed Reference Ordering",
            "abstract": "Distance functions are the main tools to measure similarity of two sequences and to search the closest sequences to given query sequence. Several well known distance functions, however, have asymptotical time complexity of O(mn) which cannot be fully afforded by systems that deal with large volumes of data. These distance functions, including edit distance on real sequences (EDR) (L. Chen et al., 2005), have pruning methods to reduce execution time by dismissing false candidates as early as possible. In this paper, we propose the histogram distance on fixed reference (HDFR) ordering, with various reference histogram construction methods, to improve the filtering power of the pruning methods in EDR. Experiments show that a decrease in EDR execution time is observed after HDFR is applied. While we base our experiments on EDR, HDFR can also be applied to other distance functions with appropriate pruning methods",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3295547",
                    "name": "Prima Chairunnanda"
                },
                {
                    "authorId": "1744041",
                    "name": "Vivekanand Gopalkrishnan"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "d140a16e502554edab6e76567c422b67ab348d3d",
            "title": "BBS: An Energy Efficient Localized Routing Scheme for Query Processing in Wireless Sensor Networks",
            "abstract": "A wireless sensor network (WSNET) can support various types of queries. The energy resource of sensors constrains the total number of query responses, called query capacity, received by the sink. There are four problems in the existing approaches for energy-efficient query processing in WSNETs: the fact that sensors near the sink drain their energy much faster than distant sensors has been overlooked, routing trees (RT) are rooted at the sink, and therefore, aggregative queries are less energy-efficient, data reception cost has been ignored, and flooding is used in query distribution or RT construction. In this paper, we propose a Broadcasting-Based query Scheme (BBS) to address the above problems. BBS reduces the energy depletion rate of sensors near the sink, builds different localized RTs for different query types, and eliminates the flooding cost of query distribution. Compared to the existing approaches, simulation studies show that BBS produces significant improvement in the query capacity for non-holistic queries (10%\u2014100% capacity improvement) and holistic queries (up to an order of magnitude of capacity improvement).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056408901",
                    "name": "Jie Lian"
                },
                {
                    "authorId": "145431598",
                    "name": "S. Naik"
                },
                {
                    "authorId": "2456942",
                    "name": "G. Agnew"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "f9ac48ab70d4fb1ad01a1bf954bc1c5d5b9ebeca",
            "title": "In-network Outlier Cleaning for Data Collection in Sensor Networks",
            "abstract": "Outliers are very common in the environmental data monitored by a sensor network consisting of many inexpensive, low fidelity, and frequently failed sensors. The limited battery power and costly data transmission have introduced a new challenge for outlier cleaning in sensor networks: it must be done innetwork to avoid spending energy on transmitting outliers. In this paper, we propose an in-network outlier cleaning approach, including wavelet based outlier correction and neighboring DTW(Dynamic Time Warping) distance-based outlier removal. The cleaning process is accomplished during multi-hop data forwarding process, and makes use of the neighboring relation in the hop-count based routing algorithm. Our approach guarantees that most of the outliers can be either corrected, or removed from further transmission within 2 hops. We have simulated a spatialtemporal correlated environmental area, and evaluated the outlier cleaning approach in it. The results show that our approach can effectively clean the sensing data and reduce outlier traffic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2766544",
                    "name": "Y. Zhuang"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "0a74634cb9d002169345726112eada1e3dbc8fca",
            "title": "Localized routing trees for query processing in sensor networks",
            "abstract": "In this paper, we propose a novel energy-efficient approach, a localized routing tree (LRT) coupled with a route redirection (RR) strategy, to support various types of queries. LRTs take care of the sensors near the sink and reduce the energy consumption of these sensors, and RR reduces the energy cost of data receptions. Compared to the existing approaches, simulation studies show that LRT together with RR has significant improvement on the query capacity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056408901",
                    "name": "Jie Lian"
                },
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "145431598",
                    "name": "S. Naik"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                },
                {
                    "authorId": "2456942",
                    "name": "G. Agnew"
                }
            ]
        },
        {
            "paperId": "b3880255595e88d2a560431970f79cf6e432ba77",
            "title": "Using Multi-Scale Histograms to Answer Pattern Existence and Shape Match Queries",
            "abstract": "Similarity-based querying of time series data can be categorized as pattern existence queries and shape match queries. Pattern existence queries find the time series data with certain patterns while shape match queries look for the time series data that have similar movement shapes. Existing proposals address one of these or the other. In this paper, we propose multi-scale time series histograms that can be used to answer both types of queries, thus offering users more flexibility. Multiple histogram levels allow querying at various precision levels. Most importantly, the distanc es of time series histograms at lower scale are lower bounds of the distances at higher scale, which guarantees that no false dismissals will be introduced when a multi-step filter ing process is used in answering shape match queries. We further propose to use averages of time series histograms to reduce the dimensionality and avoid computing the distances of full time series histograms. The experimental results show that multi-scale histograms can effectively find the patterns in time series data and answer shape match queries, even when the data contain noise, time shifting and scaling, or amplitude shifting and scaling.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                },
                {
                    "authorId": "1693176",
                    "name": "Vincent Oria"
                }
            ]
        },
        {
            "paperId": "efac1cdecf594038530fd4eebcec54bcfff4a36a",
            "title": "Robust and fast similarity search for moving object trajectories",
            "abstract": "An important consideration in similarity-based retrieval of moving object trajectories is the definition of a distance function. The existing distance functions are usually sensitive to noise, shifts and scaling of data that commonly occur due to sensor failures, errors in detection techniques, disturbance signals, and different sampling rates. Cleaning data to eliminate these is not always possible. In this paper, we introduce a novel distance function, Edit Distance on Real sequence (EDR) which is robust against these data imperfections. Analysis and comparison of EDR with other popular distance functions, such as Euclidean distance, Dynamic Time Warping (DTW), Edit distance with Real Penalty (ERP), and Longest Common Subsequences (LCSS), indicate that EDR is more robust than Euclidean distance, DTW and ERP, and it is on average 50% more accurate than LCSS. We also develop three pruning techniques to improve the retrieval efficiency of EDR and show that these techniques can be combined effectively in a search, increasing the pruning power significantly. The experimental results confirm the superior efficiency of the combined methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                },
                {
                    "authorId": "1693176",
                    "name": "Vincent Oria"
                }
            ]
        },
        {
            "paperId": "b44cbc7183003bc87b8929e580dd87d6d7656005",
            "title": "Multi-scale histograms for answering queries over time series data",
            "abstract": "Similarity-based time series data retrieval has been used in many real world applications, such as stock data or weather data analysis. Two types of queries on time series data are generally studied: pattern existence queries and exact match queries. Here, we describe a technique to answer both pattern existence queries and exact match queries. A typical application that needs answers to both queries is an interactive analysis of time series data. We propose a histogram-based representation to approximate time series data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "d971327934f58b0f6ba1beb4f242db600b7cc23c",
            "title": "Symbolic representation and retrieval of moving object trajectories",
            "abstract": "Searching moving object trajectories of video databases has been applied to many fields, such as video data analysis, content-based video retrieval, video scene classification. In this paper, we propose a novel representation of trajectories, called <i>movement pattern strings</i>, which convert the trajectories into symbolic representations. Movement pattern strings encode both the movement direction and the movement distance information of the trajectories. The distances that are computed in a symbolic space are lower bounds of the distances of original trajectory data, which guarantees that no false dismissals will be introduced using movement pattern strings to retrieve trajectories. In order to improve the retrieval efficiency, we define a <i>modified frequency distance</i> for frequency vectors that are obtained from movement pattern strings to reduce the dimensionality and the computation cost. The experimental results show that using movement pattern strings is almost as effective as using raw trajectories. In addition, the cost of retrieving similar trajectories can greatly be reduced when the modified frequency distance is used as a filter",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                },
                {
                    "authorId": "1693176",
                    "name": "Vincent Oria"
                }
            ]
        },
        {
            "paperId": "5b120962e9b4d55bfdd8cc4aa325d64bb4bccf82",
            "title": "Modeling Video Data for Content Based Queries: Extending the DISIMA Image Data Model",
            "abstract": "We present an efficient video data model that extends the DISIMA image data model by adding the video components and setting up links between image and video data. Many video data models have been proposed, most of which describe video data independently of image data and therefore fail to consider the relationship between videos and images. Our proposed model expresses the semantics of video data content by means of salient objects and relationships among them. Connections between video data and DISIMA images are made through key frames, which are extracted from each shot. Based on these connections, techniques used to query image data may be used to query video data. In addition, a set of new predicates has been defined to describe the spatio-temporal characteristics of salient objects in the video data. MOQL is used as a query language, with which we present example queries that can be posed on the proposed video data model..",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                },
                {
                    "authorId": "1693176",
                    "name": "Vincent Oria"
                }
            ]
        },
        {
            "paperId": "a6e145cef798bca21f372ba24cdbf95c53c7537d",
            "title": "Incorporating Audio Cues into Dialog and Action Scene Extraction",
            "abstract": "In this paper, we present an approach to extract scenes in video. The approach is top-down and uses video editing rules and audio cues to extract simple dialog and action scenes. The underlying model is a finite state machine coupled with audio cues that are determined using an audio classifier.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "11606206",
                    "name": "Shariq J. Rizvi"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "18c60e623ca76fa2cd24d6d44120e2af902ad269",
            "title": "A Multi-Level Index Structure for Video Databases",
            "abstract": "Several salient object-based data models have been proposed to model the video data, however, none of them proposed an index structure to handle the salient object-based queries efficiently. There are several indexing schemes that have been proposed for spatio-temporal relationships among objects and they are used to optimize timestamp and interval queries, which are rarely used in video database. Moreover, these index structures are designed without consideration of the granularity levels of constraints in salient objects and the characteristics of the video data. In this paper, we propose a multi-level index structure to efficiently handle the salient object-based quires with different levels of constraints. The characteristics of video data are also captured in the second level of the index structure designed to reduce the storage requirement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1693176",
                    "name": "Vincent Oria"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "59f2f7ee64c5d7e55889de82b85f08fdb4d1dbf0",
            "title": "Rule-based scene extraction from video",
            "abstract": "Instead of clustering video shots into scenes using low level image features, in this paper, we propose a rule-based model to extract simple dialog or action scenes. Through analyzing video editing rules and observing temporal appearance patterns of shots in dialog scenes of movies, we deduce a set of rules to recognize dialog or action scenes. Based on these rides, a finite state machine is designed to extract dialog or action scenes from videos automatically.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        },
        {
            "paperId": "634b4feafe670879cd9a39f84017f4f437f61a95",
            "title": "Modeling of video objects in a video databases",
            "abstract": "In this paper, we present an efficient video data model to represent moving trajectories of video objects and spatiotemporal relationships among the video objects. A video clip is segmented into a set of common appearance intervals (CAIs). A CAI is a time interval that video objects appear together. Transitions among CAIs record the appearance/disappearance of video objects. Depending on the properties of video objects, they are classified as foreground and background video objects. Foreground video objects are further divided into moving video objects and static video objects. Different models are designed to capture these video objects and spatio-temporal relationships among foreground video objects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49330176",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "1705151",
                    "name": "M. Tamer \u00d6zsu"
                }
            ]
        }
    ]
}