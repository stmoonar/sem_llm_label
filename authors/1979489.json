{
    "authorId": "1979489",
    "papers": [
        {
            "paperId": "c43a6f12b062a50617244611af180a8146e792de",
            "title": "Learning from Natural Language Feedback",
            "abstract": "Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human evaluation. Here, we propose to learn from natural language feedback, which conveys more information per human evaluation. We learn from language feedback on model outputs using a three-step learning algorithm. First, we condition the language model on the initial output and feedback to generate many refinements. Second, we choose the refinement with the highest similarity to the feedback. Third, we finetune a language model to maximize the likelihood of the chosen refinement given the input. In synthetic experiments, we first evaluate whether language models accurately incorporate feedback to produce refinements, finding that only large language models (175B parameters) do so. Using only 100 samples of human-written feedback, our learning algorithm fine-tunes a GPT-3 model to roughly human-level summarization ability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2070850274",
                    "name": "J'er'emy Scheurer"
                },
                {
                    "authorId": "1602998334",
                    "name": "Jon Ander Campos"
                },
                {
                    "authorId": "2163950075",
                    "name": "Jun Shern Chan"
                },
                {
                    "authorId": "13336152",
                    "name": "Angelica Chen"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                },
                {
                    "authorId": "3439053",
                    "name": "Ethan Perez"
                }
            ]
        },
        {
            "paperId": "01bc697a530bf5b15ec0f20b6419947d66af4d83",
            "title": "Latent State Models of Training Dynamics",
            "abstract": "The impact of randomness on model training is poorly understood. How do differences in data order and initialization actually manifest in the model, such that some training runs outperform others or converge faster? Furthermore, how can we interpret the resulting training dynamics and the phase transitions that characterize different trajectories? To understand the effect of randomness on the dynamics and outcomes of neural network training, we train models multiple times with different random seeds and compute a variety of metrics throughout training, such as the $L_2$ norm, mean, and variance of the neural network's weights. We then fit a hidden Markov model (HMM) over the resulting sequences of metrics. The HMM represents training as a stochastic process of transitions between latent states, providing an intuitive overview of significant changes during training. Using our method, we produce a low-dimensional, discrete representation of training dynamics on grokking tasks, image classification, and masked language modeling. We use the HMM representation to study phase transitions and identify latent\"detour\"states that slow down convergence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145919775",
                    "name": "Michael Y Hu"
                },
                {
                    "authorId": "13336152",
                    "name": "Angelica Chen"
                },
                {
                    "authorId": "2362960",
                    "name": "Naomi Saphra"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                }
            ]
        },
        {
            "paperId": "12a3d43d9bc97b7d0e93893b98b5ed1c54c1ced2",
            "title": "Improving Joint Speech-Text Representations Without Alignment",
            "abstract": "The last year has seen astonishing progress in text-prompted image generation premised on the idea of a cross-modal representation space in which the text and image domains are represented jointly. In ASR, this idea has found application as joint speech-text encoders that can scale to the capacities of very large parameter models by being trained on both unpaired speech and text. While these methods show promise, they have required special treatment of the sequence-length mismatch inherent in speech and text, either by up-sampling heuristics or an explicit alignment model. In this work, we offer evidence that joint speech-text encoders naturally achieve consistent representations across modalities by disregarding sequence length, and argue that consistency losses could forgive length differences and simply assume the best alignment. We show that such a loss improves downstream WER in both a large-parameter monolingual and multilingual system.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "150062790",
                    "name": "Cal Peyser"
                },
                {
                    "authorId": "134905390",
                    "name": "Zhong Meng"
                },
                {
                    "authorId": "2055735413",
                    "name": "Ke Hu"
                },
                {
                    "authorId": "2557391",
                    "name": "Rohit Prabhavalkar"
                },
                {
                    "authorId": "3998980",
                    "name": "A. Rosenberg"
                },
                {
                    "authorId": "1784851",
                    "name": "Tara N. Sainath"
                },
                {
                    "authorId": "1774515",
                    "name": "M. Picheny"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                }
            ]
        },
        {
            "paperId": "2dffb901382c6055a4d6bc9d07f4e9f6ae0e520e",
            "title": "Leveraging Implicit Feedback from Deployment Data in Dialogue",
            "abstract": "We study improving social conversational agents by learning from natural dialogue between users and a deployed model, without extra annotations. To implicitly measure the quality of a machine-generated utterance, we leverage signals like user response length, sentiment and reaction of the future human utterances in the collected dialogue episodes. Our experiments use the publicly released deployment data from BlenderBot (Xu et al., 2023). Human evaluation indicates improvements in our new models over baseline responses; however, we find that some proxy signals can lead to more generations with undesirable properties as well. For example, optimizing for conversation length can lead to more controversial or unfriendly generations compared to the baseline, whereas optimizing for positive sentiment or reaction can decrease these behaviors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46230016",
                    "name": "Richard Yuanzhe Pang"
                },
                {
                    "authorId": "3849208",
                    "name": "Stephen Roller"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                },
                {
                    "authorId": "144533687",
                    "name": "He He"
                },
                {
                    "authorId": "145183709",
                    "name": "J. Weston"
                }
            ]
        },
        {
            "paperId": "2e5a62dc622f67ff6adf8ebb4aecde812b6e4b28",
            "title": "Detecting incidental correlation in multimodal learning via latent variable modeling",
            "abstract": "Multimodal neural networks often fail to utilize all modalities. They subsequently generalize worse than their unimodal counterparts, or make predictions that only depend on a subset of modalities. We refer to this problem as modality underutilization . Existing work has addressed this issue by ensuring that there are no systematic biases in dataset creation, or that our neural network architectures and optimization algorithms are capable of learning modality interactions. We demonstrate that even when these favorable conditions are met, modality underutilization can still occur in the small data regime. To explain this phenomenon, we put forth a concept that we call incidental correlation . It is a spurious correlation that emerges in small datasets, despite not being a part of the underlying data generating process (DGP). We develop our argument using a DGP under which multimodal neural networks must utilize all modalities, since all paths between the inputs and target are causal. This represents an idealized scenario that often fails to materialize. Instead, due to incidental correlation, small datasets sampled from this DGP have higher likelihood under an alternative DGP with spurious paths between the inputs and target. Multimodal neural networks that use these spurious paths for prediction fail to utilize all modalities. Given its harmful effects, we propose to detect incidental correlation via latent variable modeling. We specify an identifiable variational autoencoder such that the latent posterior encodes the spurious correlations between the inputs and target. This allows us to interpret the Kullback-Leibler divergence between the latent posterior and prior as the severity of incidental correlation. We use an ablation study to show that identifiability is important in this context, since we derive our conclusions from the latent posterior. Using experiments with synthetic data, as well as with VQA v2.0 and NLVR2, we demonstrate that incidental correlation emerges in the small data regime, and leads to modality underutilization. Practitioners of multimodal learning can use our method to detect whether incidental correlation is present in their datasets, and determine whether they should collect additional data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2099390928",
                    "name": "Taro Makino"
                },
                {
                    "authorId": "2247396517",
                    "name": "Yixin Wang"
                },
                {
                    "authorId": "2376144",
                    "name": "Krzysztof J. Geras"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                }
            ]
        },
        {
            "paperId": "3200a0d6fef7164f0341cf1938f584da6057ffd6",
            "title": "Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs",
            "abstract": "Large language models (LLMs) have achieved widespread success on a variety of in-context few-shot tasks, but this success is typically evaluated via correctness rather than consistency. We argue that self-consistency is an important criteria for valid multi-step reasoning in tasks where the solution is composed of the answers to multiple sub-steps. We propose two types of self-consistency that are particularly important for multi-step reasoning -- hypothetical consistency (a model's ability to predict what its output would be in a hypothetical other context) and compositional consistency (consistency of a model's final outputs when intermediate sub-steps are replaced with the model's outputs for those steps). We demonstrate that multiple variants of the GPT-3/-4 models exhibit poor consistency rates across both types of consistency on a variety of tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "13336152",
                    "name": "Angelica Chen"
                },
                {
                    "authorId": "80842917",
                    "name": "Jason Phang"
                },
                {
                    "authorId": "119389860",
                    "name": "Alicia Parrish"
                },
                {
                    "authorId": "2044959912",
                    "name": "Vishakh Padmakumar"
                },
                {
                    "authorId": "145756130",
                    "name": "Chen Zhao"
                },
                {
                    "authorId": "1799822",
                    "name": "Sam Bowman"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                }
            ]
        },
        {
            "paperId": "42bf5abb8e8fb3acaac03509d4deac7caba232ca",
            "title": "A Comparison of Semi-Supervised Learning Techniques for Streaming ASR at Scale",
            "abstract": "Unpaired text and audio injection have emerged as dominant methods for improving ASR performance in the absence of a large labeled corpus. However, little guidance exists on deploying these methods to improve production ASR systems that are trained on very large supervised corpora and with realistic requirements like a constrained model size and CPU budget, streaming capability, and a rich lattice for rescoring and for downstream NLU tasks. In this work, we compare three state-of-the-art semi-supervised methods encompassing both unpaired text and audio as well as several of their combinations in a controlled setting using joint training. We find that in our setting these methods offer many improvements beyond raw WER, including substantial gains in tail-word WER, decoder computation during inference, and lattice density.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "150062790",
                    "name": "Cal Peyser"
                },
                {
                    "authorId": "1774515",
                    "name": "M. Picheny"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                },
                {
                    "authorId": "2557391",
                    "name": "Rohit Prabhavalkar"
                },
                {
                    "authorId": "2162739754",
                    "name": "Ronny Huang"
                },
                {
                    "authorId": "1784851",
                    "name": "Tara N. Sainath"
                }
            ]
        },
        {
            "paperId": "512d4baaacb28578464db7697a49ee5dcd69f915",
            "title": "Blind Biological Sequence Denoising with Self-Supervised Set Learning",
            "abstract": "Biological sequence analysis relies on the ability to denoise the imprecise output of sequencing platforms. We consider a common setting where a short sequence is read out repeatedly using a high-throughput long-read platform to generate multiple subreads, or noisy observations of the same sequence. Denoising these subreads with alignment-based approaches often fails when too few subreads are available or error rates are too high. In this paper, we propose a novel method for blindly denoising sets of sequences without directly observing clean source sequence labels. Our method, Self-Supervised Set Learning (SSSL), gathers subreads together in an embedding space and estimates a single set embedding as the midpoint of the subreads in both the latent and sequence spaces. This set embedding represents the\"average\"of the subreads and can be decoded into a prediction of the clean sequence. In experiments on simulated long-read DNA data, SSSL methods denoise small reads of $\\leq 6$ subreads with 17% fewer errors and large reads of $>6$ subreads with 8% fewer errors compared to the best baseline. On a real dataset of antibody sequences, SSSL improves over baselines on two self-supervised metrics, with a significant improvement on difficult small reads that comprise over 60% of the test set. By accurately denoising these reads, SSSL promises to better realize the potential of high-throughput DNA sequencing data for downstream scientific applications.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "144055124",
                    "name": "Nathan Ng"
                },
                {
                    "authorId": "2116023523",
                    "name": "Ji Won Park"
                },
                {
                    "authorId": "2164381430",
                    "name": "Jae Hyeon Lee"
                },
                {
                    "authorId": "2237787661",
                    "name": "Ryan Lewis Kelly"
                },
                {
                    "authorId": "4195120",
                    "name": "Stephen Ra"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                }
            ]
        },
        {
            "paperId": "911175afe6c3e7a412f5e6244c4844fd74919b78",
            "title": "Unsupervised Learning of Initialization in Deep Neural Networks via Maximum Mean Discrepancy",
            "abstract": "Despite the recent success of stochastic gradient descent in deep learning, it is often difficult to train a deep neural network with an inappropriate choice of its initial parameters. Even if training is successful, it has been known that the initial parameter configuration may negatively impact generalization. In this paper, we propose an unsupervised algorithm to find good initialization for input data, given that a downstream task is d-way classification. We first notice that each parameter configuration in the parameter space corresponds to one particular downstream task of d-way classification. We then conjecture that the success of learning is directly related to how diverse downstream tasks are in the vicinity of the initial parameters. We thus design an algorithm that encourages small perturbation to the initial parameter configuration leads to a diverse set of d-way classification tasks. In other words, the proposed algorithm ensures a solution to any downstream task to be near the initial parameter configuration. We empirically evaluate the proposed algorithm on various tasks derived from MNIST with a fully connected network. In these experiments, we observe that our algorithm improves average test accuracy across most of these tasks, and that such improvement is greater when the number of labelled examples is small.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "81275395",
                    "name": "Cheolhyoung Lee"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                }
            ]
        },
        {
            "paperId": "c11810fa8887b678facea62da4607c4898360308",
            "title": "Training Language Models with Language Feedback at Scale",
            "abstract": "Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate ILF's effectiveness on a carefully-controlled toy task and a realistic summarization task. Our experiments demonstrate that large language models accurately incorporate feedback and that finetuning with ILF scales well with the dataset size, even outperforming finetuning on human summaries. Learning from both language and comparison feedback outperforms learning from each alone, achieving human-level summarization performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2070850274",
                    "name": "J'er'emy Scheurer"
                },
                {
                    "authorId": "1602998334",
                    "name": "Jon Ander Campos"
                },
                {
                    "authorId": "30023177",
                    "name": "Tomasz Korbak"
                },
                {
                    "authorId": "2163950075",
                    "name": "Jun Shern Chan"
                },
                {
                    "authorId": "13336152",
                    "name": "Angelica Chen"
                },
                {
                    "authorId": "1979489",
                    "name": "Kyunghyun Cho"
                },
                {
                    "authorId": "3439053",
                    "name": "Ethan Perez"
                }
            ]
        }
    ]
}