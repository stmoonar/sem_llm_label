{
    "authorId": "143712374",
    "papers": [
        {
            "paperId": "cd4e49a4b3f9ab6c3737a1691bbf7281027932a6",
            "title": "StructSum: Summarization via Structured Representations",
            "abstract": "Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges: (i) layout bias: they overfit to the style of training corpora; (ii) limited abstractiveness: they are optimized to copying n-grams from the source rather than generating novel abstractive summaries; (iii) lack of transparency: they are not interpretable. In this work, we propose a framework based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoder-decoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN/DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "51152502",
                    "name": "Artidoro Pagnoni"
                },
                {
                    "authorId": "11073942",
                    "name": "Jay Yoon Lee"
                },
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                },
                {
                    "authorId": "145317727",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "2583e7e279e2969493c3290c8f300ab32da40bf9",
            "title": "Improving Candidate Generation for Low-resource Cross-lingual Entity Linking",
            "abstract": "Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention. Approaches based on resources from Wikipedia have proven successful in the realm of relatively high-resource languages, but these do not extend well to low-resource languages with few, if any, Wikipedia pages. Recently, transfer learning methods have been shown to reduce the demand for resources in the low-resource languages by utilizing resources in closely related languages, but the performance still lags far behind their high-resource counterparts. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios. The methods are simple, but effective: We experiment with our approach on seven XEL datasets and find that they yield an average gain of 16.9% in Top-30 gold candidate recall, compared with state-of-the-art baselines. Our improved model also yields an average gain of 7.9% in in-KB accuracy of end-to-end XEL.1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149163534",
                    "name": "Shuyan Zhou"
                },
                {
                    "authorId": "1518268393",
                    "name": "Shruti Rijhawani"
                },
                {
                    "authorId": "1771118",
                    "name": "J. Wieting"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "5585c7fcbda5d94e946fe9091860e2e574927ed8",
            "title": "Efficient Meta Lifelong-Learning with Limited Memory",
            "abstract": "Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as lifelong learning. State-of-the-art lifelong language learning methods store past examples in episodic memory and replay them at both training and inference time. However, as we show later in our experiments, there are three significant impediments: (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed. In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion. To achieve sample efficiency, our method trains the model in a manner that it learns a better initialization for local adaptation. Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1% memory size and narrowing the gap with multi-task learning. We further show that our method alleviates both catastrophic forgetting and negative transfer at the same time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2331539",
                    "name": "Zirui Wang"
                },
                {
                    "authorId": "47613860",
                    "name": "Sanket Vaibhav Mehta"
                },
                {
                    "authorId": "1719347",
                    "name": "B. P\u00f3czos"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                }
            ]
        },
        {
            "paperId": "581eb3f1d9fb1f4c613b6f2473c0330d9aa76e08",
            "title": "Empirical Evidence for Derivational Analogy",
            "abstract": "Analogical problem solving is mostly described as transfer of a source solution to a target problem based on the structural correspondences (mapping) between source and target. Derivational analogy (Carbonell, 1986) proposes an alternative view: A target problem is solved by replaying a remembered problem solving episode. Thus, the experience with the source problem is used to guide the search for the target solution by applying the same solution technique rather than by a transferring the complete solution. We report an empirical study using the network problems presented in Novick and Hmelo (1994) as material. We can show that for this domain subjects exhibit a derivational rather than a transformational solution strategy. The empirical results are in correspondence with a theoretical analysis of mapping vs replay effort for this problem domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1727734",
                    "name": "Ute Schmid"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                }
            ]
        },
        {
            "paperId": "719af99501c8b2435cec86547afbca6b224bb2ea",
            "title": "Semi-Supervised Learning on Meta Structure: Multi-Task Tagging and Parsing in Low-Resource Scenarios",
            "abstract": "Multi-view learning makes use of diverse models arising from multiple sources of input or different feature subsets for the same task. For example, a given natural language processing task can combine evidence from models arising from character, morpheme, lexical, or phrasal views. The most common strategy with multi-view learning, especially popular in the neural network community, is to unify multiple representations into one unified vector through concatenation, averaging, or pooling, and then build a single-view model on top of the unified representation. As an alternative, we examine whether building one model per view and then unifying the different models can lead to improvements, especially in low-resource scenarios. More specifically, taking inspiration from co-training methods, we propose a semi-supervised learning approach based on multi-view models through consensus promotion, and investigate whether this improves overall performance. To test the multi-view hypothesis, we use moderately low-resource scenarios for nine languages and test the performance of the joint model for part-of-speech tagging and dependency parsing. The proposed model shows significant improvements across the test cases, with average gains of -0.9 \u223c +9.3 labeled attachment score (LAS) points. We also investigate the effect of unlabeled data on the proposed model by varying the amount of training data and by using different domains of unlabeled data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8068510",
                    "name": "Kyungtae Lim"
                },
                {
                    "authorId": "11073942",
                    "name": "Jay Yoon Lee"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                },
                {
                    "authorId": "1736763",
                    "name": "T. Poibeau"
                }
            ]
        },
        {
            "paperId": "98caf4eb79208cf4bbfe20bde37bc1b6ded6d6de",
            "title": "Soft Gazetteers for Low-Resource Named Entity Recognition",
            "abstract": "Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance. Although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated their utility for named entity recognition on English data. However, designing such features for low-resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages. To address this problem, we propose a method of \u201csoft gazetteers\u201d that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7391530",
                    "name": "Shruti Rijhwani"
                },
                {
                    "authorId": "2149163534",
                    "name": "Shuyan Zhou"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                }
            ]
        },
        {
            "paperId": "c18b96b51ee70ab14c575e5ba24e17d79a94b59c",
            "title": "Harnessing Code Switching to Transcend the Linguistic Barrier",
            "abstract": "Code mixing (or code switching) is a common phenomenon observed in social-media content generated by a linguistically diverse user-base. Studies show that in the Indian sub-continent, a substantial fraction of social media posts exhibit code switching. While the difficulties posed by code mixed documents to further downstream analyses are well-understood, lending visibility to code mixed documents under certain scenarios may have utility that has been previously overlooked. For instance, a document written in a mixture of multiple languages can be partially accessible to a wider audience; this could be particularly useful if a considerable fraction of the audience lacks fluency in one of the component languages. In this paper, we provide a systematic approach to sample code mixed documents leveraging a polyglot embedding based method that requires minimal supervision. In the context of the 2019 India-Pakistan conflict triggered by the Pulwama terror attack, we demonstrate an untapped potential of harnessing code mixing for human well-being: starting from an existing hostility diffusing hope speech classifier solely trained on English documents, code mixed documents are utilized to perform cross-lingual sampling and retrieve hope speech content written in a low-resource but widely used language - Romanized Hindi. Our proposed pipeline requires minimal supervision and holds promise in substantially reducing web moderation efforts. A further exploratory study on a new COVID-19 data set introduced in this paper demonstrates the generalizability of our cross-lingual sampling technique.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2856003",
                    "name": "Ashiqur R. KhudaBukhsh"
                },
                {
                    "authorId": "2911934",
                    "name": "Shriphani Palakodety"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                }
            ]
        },
        {
            "paperId": "e66e443219af0f57fedc3ee12b1cf583bc357faf",
            "title": "Mining Insights from Large-Scale Corpora Using Fine-Tuned Language Models",
            "abstract": ". Mining insights from large volume of social media texts with minimal supervision is a highly challenging Natural Language Processing (NLP) task. While Language Models\u2019 (LMs) ef\ufb01cacy in several downstream tasks is well-studied, assessing their applicability in answering relational questions, tracking perception or mining deeper insights is under-explored. Few recent lines of work have scratched the surface by studying pre-trained LMs\u2019 (e.g., BERT ) capability in answering relational questions through \u201c\ufb01ll-in-the-blank\u201d cloze statements (e.g., [Dante was born in MASK] ). BERT predicts the MASK-ed word with a list of words ranked by probability (in this case, BERT successfully predicts Florence with the highest probability). In this paper, we conduct a feasibility study of \ufb01ne-tuned LMs with a different focus on tracking polls, tracking community perception and mining deeper insights typically obtained through costly surveys. Our main focus is on a substantial corpus of video comments extracted from YouTube videos (6,182,868 comments on 130,067 videos by 1,518,077 users) posted within 100 days prior to the 2019 Indian General Election. Using \ufb01ll-in-the-blank cloze statements against a recent high-performance language modeling algorithm, BERT , we present a novel application of this family of tools that is able to (1) aggregate political sentiment (2) reveal community perception and (3) track evolving national priorities and issues of interest.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2911934",
                    "name": "Shriphani Palakodety"
                },
                {
                    "authorId": "2856003",
                    "name": "Ashiqur R. KhudaBukhsh"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                }
            ]
        },
        {
            "paperId": "f184908270fc934ab74438a0aaac7a43a5eae6d2",
            "title": "StructSum: Incorporating Latent and Explicit Sentence Dependencies for Single Document Summarization",
            "abstract": "Traditional preneural approaches to single document summarization relied on modeling the intermediate structure of a document before generating the summary. In contrast, the current state of the art neural summarization models do not preserve any intermediate structure, resorting to encoding the document as a sequence of tokens. The goal of this work is two-fold: to improve the quality of generated summaries and to learn interpretable document representations for summarization. To this end, we propose incorporating latent and explicit sentence dependencies into single-document summarization models. We use structure-aware encoders to induce latent sentence relations, and inject explicit coreferring mention graph across sentences to incorporate explicit structure. On the CNN/DM dataset, our model outperforms standard baselines and provides intermediate latent structures for analysis. We present an extensive analysis of our summaries and show that modeling document structure reduces copying long sequences and incorporates richer content from the source document while maintaining comparable summary lengths and an increased degree of abstraction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "51152502",
                    "name": "Artidoro Pagnoni"
                },
                {
                    "authorId": "11073942",
                    "name": "Jay Yoon Lee"
                },
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                },
                {
                    "authorId": "145317727",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "0805cb1b26577f08f84190445992f7f0584e4742",
            "title": "OPERA: Operations-oriented Probabilistic Extraction, Reasoning, and Analysis",
            "abstract": "The OPERA system of CMU and USC/ISI performs end-to-end information extraction from multiple media and languages (English, Russian, Ukrainian), integrates the results, builds Knowledge Bases about the domain, and does hypothesis creation and reasoning to answer questions. ",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                },
                {
                    "authorId": "143712374",
                    "name": "J. Carbonell"
                },
                {
                    "authorId": "2284176",
                    "name": "Hans Chalupsky"
                },
                {
                    "authorId": "145001267",
                    "name": "A. Gershman"
                },
                {
                    "authorId": "7661726",
                    "name": "Alexander Hauptmann"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "1706595",
                    "name": "T. Mitamura"
                },
                {
                    "authorId": "38599655",
                    "name": "Zaid A. W. Sheikh"
                },
                {
                    "authorId": "1741515",
                    "name": "Ankit Dangi"
                },
                {
                    "authorId": "51250894",
                    "name": "Aditi Chaudhary"
                },
                {
                    "authorId": "2135112672",
                    "name": "Xianyang Chen"
                },
                {
                    "authorId": "97791350",
                    "name": "Xiang Kong"
                },
                {
                    "authorId": "1410241246",
                    "name": "Bernie Huang"
                },
                {
                    "authorId": "34777258",
                    "name": "Salvador Medina"
                },
                {
                    "authorId": "2109279237",
                    "name": "H. Liu"
                },
                {
                    "authorId": "2378954",
                    "name": "Xuezhe Ma"
                },
                {
                    "authorId": "1410648718",
                    "name": "Maria Ryskina"
                },
                {
                    "authorId": "2075461978",
                    "name": "Ramon Sanabria"
                },
                {
                    "authorId": "2126048085",
                    "name": "Varun Gangal"
                }
            ]
        }
    ]
}