{
    "authorId": "2261426672",
    "papers": [
        {
            "paperId": "36c87f474378d3b352dc125d22cec672fd3c54c3",
            "title": "MultiFS: Automated Multi-Scenario Feature Selection in Deep Recommender Systems",
            "abstract": "Multi-scenario recommender systems (MSRSs) have been increasingly used in real-world industrial platforms for their excellent advantages in mitigating data sparsity and reducing maintenance costs. However, conventional MSRSs usually use all relevant features indiscriminately and ignore that different kinds of features have varying importance under different scenarios, which may cause confusion and performance degradation. In addition, existing feature selection methods for deep recommender systems may lack the exploration of scenario relations. In this paper, we propose a novel automated multi-scenario feature selection (MultiFS) framework to bridge this gap, which is able to consider scenario relations and utilize a hierarchical gating mechanism to select features for each scenario. Specifically, MultiFS first efficiently obtains feature importance across all the scenarios through a scenario-shared gate. Then, some scenario-specific gate aims to identify feature importance to individual scenarios from a subset of the former with lower importance. Subsequently, MultiFS imposes constraints on the two gates to make the learning mechanism more feasible and combines the two to select exclusive features for different scenarios. We evaluate MultiFS and demonstrate its ability to enhance the multi-scenario model performance through experiments over two public multi-scenario benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66953961",
                    "name": "Dugang Liu"
                },
                {
                    "authorId": "2290696975",
                    "name": "Chaohua Yang"
                },
                {
                    "authorId": "2240611823",
                    "name": "Xing Tang"
                },
                {
                    "authorId": "2162455919",
                    "name": "Yejing Wang"
                },
                {
                    "authorId": "1704274486",
                    "name": "Fuyuan Lyu"
                },
                {
                    "authorId": "2261426672",
                    "name": "Weihong Luo"
                },
                {
                    "authorId": "2261430143",
                    "name": "Xiuqiang He"
                },
                {
                    "authorId": "2240535483",
                    "name": "Zhong Ming"
                },
                {
                    "authorId": "2116711669",
                    "name": "Xiangyu Zhao"
                }
            ]
        },
        {
            "paperId": "9ea30b735ce4b530e40c5dcff24cb1a0dcf4f76e",
            "title": "FedBAT: Communication-Efficient Federated Learning via Learnable Binarization",
            "abstract": "Federated learning is a promising distributed machine learning paradigm that can effectively exploit large-scale data without exposing users' privacy. However, it may incur significant communication overhead, thereby potentially impairing the training efficiency. To address this challenge, numerous studies suggest binarizing the model updates. Nonetheless, traditional methods usually binarize model updates in a post-training manner, resulting in significant approximation errors and consequent degradation in model accuracy. To this end, we propose Federated Binarization-Aware Training (FedBAT), a novel framework that directly learns binary model updates during the local training process, thus inherently reducing the approximation errors. FedBAT incorporates an innovative binarization operator, along with meticulously designed derivatives to facilitate efficient learning. In addition, we establish theoretical guarantees regarding the convergence of FedBAT. Extensive experiments are conducted on four popular datasets. The results show that FedBAT significantly accelerates the convergence and exceeds the accuracy of baselines by up to 9\\%, even surpassing that of FedAvg in some cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2195780326",
                    "name": "Shiwei Li"
                },
                {
                    "authorId": "2314844128",
                    "name": "Wenchao Xu"
                },
                {
                    "authorId": "51175126",
                    "name": "Haozhao Wang"
                },
                {
                    "authorId": "2315079186",
                    "name": "Xing Tang"
                },
                {
                    "authorId": "2278194140",
                    "name": "Yining Qi"
                },
                {
                    "authorId": "2315248559",
                    "name": "Shijie Xu"
                },
                {
                    "authorId": "2261426672",
                    "name": "Weihong Luo"
                },
                {
                    "authorId": "2315068141",
                    "name": "Yuhua Li"
                },
                {
                    "authorId": "2261430143",
                    "name": "Xiuqiang He"
                },
                {
                    "authorId": "2260298889",
                    "name": "Ruixuan Li"
                }
            ]
        },
        {
            "paperId": "ca457e9b0ba0680659ed02c9d12a48c8f7d8cab9",
            "title": "Mixed-Precision Embeddings for Large-Scale Recommendation Models",
            "abstract": "Embedding techniques have become essential components of large databases in the deep learning era. By encoding discrete entities, such as words, items, or graph nodes, into continuous vector spaces, embeddings facilitate more efficient storage, retrieval, and processing in large databases. Especially in the domain of recommender systems, millions of categorical features are encoded as unique embedding vectors, which facilitates the modeling of similarities and interactions among features. However, numerous embedding vectors can result in significant storage overhead. In this paper, we aim to compress the embedding table through quantization techniques. Given that features vary in importance levels, we seek to identify an appropriate precision for each feature to balance model accuracy and memory usage. To this end, we propose a novel embedding compression method, termed Mixed-Precision Embeddings (MPE). Specifically, to reduce the size of the search space, we first group features by frequency and then search precision for each feature group. MPE further learns the probability distribution over precision levels for each feature group, which can be used to identify the most suitable precision with a specially designed sampling strategy. Extensive experiments on three public datasets demonstrate that MPE significantly outperforms existing embedding compression methods. Remarkably, MPE achieves about 200x compression on the Criteo dataset without comprising the prediction accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2195780326",
                    "name": "Shiwei Li"
                },
                {
                    "authorId": "2323521242",
                    "name": "Zhuoqi Hu"
                },
                {
                    "authorId": "1704274486",
                    "name": "Fuyuan Lyu"
                },
                {
                    "authorId": "2315079186",
                    "name": "Xing Tang"
                },
                {
                    "authorId": "2312867341",
                    "name": "Haozhao Wang"
                },
                {
                    "authorId": "2315248559",
                    "name": "Shijie Xu"
                },
                {
                    "authorId": "2261426672",
                    "name": "Weihong Luo"
                },
                {
                    "authorId": "2315068141",
                    "name": "Yuhua Li"
                },
                {
                    "authorId": "2188246843",
                    "name": "Xue Liu"
                },
                {
                    "authorId": "2261430143",
                    "name": "Xiuqiang He"
                },
                {
                    "authorId": "2283487404",
                    "name": "Ruixuan Li"
                }
            ]
        },
        {
            "paperId": "eea4b6975f0ca364c2f6df132901d17cf5816d8c",
            "title": "Masked Random Noise for Communication Efficient Federaetd Learning",
            "abstract": "Federated learning is a promising distributed training paradigm that effectively safeguards data privacy. However, it may involve significant communication costs, which hinders training efficiency. In this paper, we aim to enhance communication efficiency from a new perspective. Specifically, we request the distributed clients to find optimal model updates relative to global model parameters within predefined random noise. For this purpose, we propose Federated Masked Random Noise (FedMRN), a novel framework that enables clients to learn a 1-bit mask for each model parameter and apply masked random noise (i.e., the Hadamard product of random noise and masks) to represent model updates. To make FedMRN feasible, we propose an advanced mask training strategy, called progressive stochastic masking (PSM). After local training, each client only need to transmit local masks and a random seed to the server. Additionally, we provide theoretical guarantees for the convergence of FedMRN under both strongly convex and non-convex assumptions. Extensive experiments are conducted on four popular datasets. The results show that FedMRN exhibits superior convergence speed and test accuracy compared to relevant baselines, while attaining a similar level of accuracy as FedAvg.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2195780326",
                    "name": "Shiwei Li"
                },
                {
                    "authorId": "2315337822",
                    "name": "Yingyi Cheng"
                },
                {
                    "authorId": "51175126",
                    "name": "Haozhao Wang"
                },
                {
                    "authorId": "2315079186",
                    "name": "Xing Tang"
                },
                {
                    "authorId": "2315248559",
                    "name": "Shijie Xu"
                },
                {
                    "authorId": "2261426672",
                    "name": "Weihong Luo"
                },
                {
                    "authorId": "2315068141",
                    "name": "Yuhua Li"
                },
                {
                    "authorId": "2257136810",
                    "name": "Dugang Liu"
                },
                {
                    "authorId": "2261430143",
                    "name": "Xiuqiang He"
                },
                {
                    "authorId": "2283487404",
                    "name": "Ruixuan Li"
                }
            ]
        },
        {
            "paperId": "ab308ccd8648382e87fe3e8ea2bb5bd2009054e8",
            "title": "Towards Hybrid-grained Feature Interaction Selection for Deep Sparse Network",
            "abstract": "Deep sparse networks are widely investigated as a neural network architecture for prediction tasks with high-dimensional sparse features, with which feature interaction selection is a critical component. While previous methods primarily focus on how to search feature interaction in a coarse-grained space, less attention has been given to a finer granularity. In this work, we introduce a hybrid-grained feature interaction selection approach that targets both feature field and feature value for deep sparse networks. To explore such expansive space, we propose a decomposed space which is calculated on the fly. We then develop a selection algorithm called OptFeature, which efficiently selects the feature interaction from both the feature field and the feature value simultaneously. Results from experiments on three large real-world benchmark datasets demonstrate that OptFeature performs well in terms of accuracy and efficiency. Additional studies support the feasibility of our method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1704274486",
                    "name": "Fuyuan Lyu"
                },
                {
                    "authorId": "2109888596",
                    "name": "Xing Tang"
                },
                {
                    "authorId": "2257136810",
                    "name": "Dugang Liu"
                },
                {
                    "authorId": "2261661544",
                    "name": "Chen Ma"
                },
                {
                    "authorId": "2261426672",
                    "name": "Weihong Luo"
                },
                {
                    "authorId": "2261475293",
                    "name": "Liang Chen"
                },
                {
                    "authorId": "2261430143",
                    "name": "Xiuqiang He"
                },
                {
                    "authorId": "2188246843",
                    "name": "Xue Liu"
                }
            ]
        }
    ]
}