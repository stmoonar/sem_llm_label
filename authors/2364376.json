{
    "authorId": "2364376",
    "papers": [
        {
            "paperId": "740eca781733175a8ecf7cf60e4d3d4d901fec24",
            "title": "Content-Aware Rectified Activation for Zero-Shot Fine-Grained Image Retrieval",
            "abstract": "Fine-grained image retrieval mainly focuses on learning salient features from the seen subcategories as discriminative embedding while neglecting the problems behind zero-shot settings. We argue that retrieving fine-grained objects from unseen subcategories may rely on more diverse clues, which are easily restrained by the salient features learnt from seen subcategories. To address this issue, we propose a novel Content-aware Rectified Activation model, which enables this model to suppress the activation on salient regions while preserving their discrimination, and spread activation to adjacent non-salient regions, thus mining more diverse discriminative features for retrieving unseen subcategories. Specifically, we construct a content-aware rectified prototype (CARP) by perceiving semantics of salient regions. CARP acts as a channel-wise non-destructive activation upper bound and can be selectively used to suppress salient regions for obtaining the rectified features. Moreover, two regularizations are proposed: 1) a semantic coherency constraint that imposes a restriction on semantic coherency of CARP and salient regions, aiming at propagating the discriminative ability of salient regions to CARP, 2) a feature-navigated constraint to further guide the model to adaptively balance the discrimination power of rectified features and the suppression power of salient features. Experimental results on fine-grained and product retrieval benchmarks demonstrate that our method consistently outperforms the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2272104691",
                    "name": "Shijie Wang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2228348127",
                    "name": "Zhihui Wang"
                },
                {
                    "authorId": "2261841626",
                    "name": "Haojie Li"
                },
                {
                    "authorId": "2038729691",
                    "name": "Wangli Ouyang"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "dfae1d7e72e5ffae46b53866cb3b5f151420a77a",
            "title": "Structure Aware Multi-Graph Network for Multi-Modal Emotion Recognition in Conversations",
            "abstract": "Multi-Modal Emotion Recognition in Conversations (MMERC) is an increasingly active research field that leverages multi-modal signals to understand the feelings behind each utterance. Modeling contextual interactions and multi-modal fusion lie at the heart of this field, with graph-based models recently being widely used for MMERC to capture global multi-modal contextual information. However, these models generally mix all modality representations in a single graph, and utterances in each modality are fully connected, potentially ignoring three problems: 1) the heterogeneity of the multi-modal context, 2) the redundancy of contextual information, and 3) over-smoothing of the graph networks. To address these problems, we propose a Structure Aware Multi-Graph Network (SAMGN) for MMERC. Specifically, we construct multiple modality-specific graphs to model the heterogeneity of the multi-modal context. Instead of fully connecting the utterances in each modality, we design a structure learning module that determines whether edges exist between the utterances. This module reduces redundancy by forcing each utterance to focus on the contextual ones that contribute to its emotion recognition, acting like a message propagating reducer to alleviate over-smoothing. Then, we develop the SAMGN via Dual-Stream Propagation (DSP), which contains two propagation streams, i.e., intra- and inter-modal, performed in parallel to aggregate the heterogeneous modality information from multi-graphs. DSP also contains a gating unit that adaptively integrates the co-occurrence information from the above two propagations for emotion recognition. Experiments on two popular MMERC datasets demonstrate that SAMGN achieves new State-Of-The-Art (SOTA) results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "70123445",
                    "name": "Duzhen Zhang"
                },
                {
                    "authorId": "49102717",
                    "name": "Feilong Chen"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "30917866",
                    "name": "Xiuyi Chen"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "0340c850e033abbf71c7214e403c8fe2be5ef91f",
            "title": "Visual Tuning",
            "abstract": "Fine-tuning visual models has been widely shown promising performance on many downstream visual tasks. With the surprising development of pre-trained visual foundation models, visual tuning jumped out of the standard modus operandi that fine-tunes the whole pre-trained model or just the fully connected layer. Instead, recent advances can achieve superior performance than full-tuning the whole pre-trained parameters by updating far fewer parameters, enabling edge devices and downstream applications to reuse the increasingly large foundation models deployed on the cloud. With the aim of helping researchers get the full picture and future directions of visual tuning, this survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models. Specifically, it provides a detailed background of visual tuning and categorizes recent visual tuning techniques into five groups: fine-tuning, prompt tuning, adapter tuning, parameter tuning, and remapping tuning. Meanwhile, it offers some exciting research directions for prospective pre-training and various interactions in visual tuning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48317098",
                    "name": "Bruce X. B. Yu"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2146017850",
                    "name": "Lin Liu"
                },
                {
                    "authorId": "2108620644",
                    "name": "Shijie Wang"
                },
                {
                    "authorId": "2216713990",
                    "name": "Zhiyu Wang"
                },
                {
                    "authorId": "2110810106",
                    "name": "Junfan Lin"
                },
                {
                    "authorId": "3041937",
                    "name": "Lingxi Xie"
                },
                {
                    "authorId": "2198533822",
                    "name": "Haojie Li"
                },
                {
                    "authorId": "33383055",
                    "name": "Zhouchen Lin"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                },
                {
                    "authorId": "2118428500",
                    "name": "Chang Wen Chen"
                }
            ]
        },
        {
            "paperId": "051549d8ef56937b2f4d113afdcf8c7586d3770b",
            "title": "Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models",
            "abstract": "The AI community has been pursuing algorithms known as artificial general intelligence (AGI) that apply to any kind of real-world problem. Recently, chat systems powered by large language models (LLMs) emerge and rapidly become a promising direction to achieve AGI in natural language processing (NLP), but the path towards AGI in computer vision (CV) remains unclear. One may owe the dilemma to the fact that visual signals are more complex than language signals, yet we are interested in finding concrete reasons, as well as absorbing experiences from GPT and LLMs to solve the problem. In this paper, we start with a conceptual definition of AGI and briefly review how NLP solves a wide range of tasks via a chat system. The analysis inspires us that unification is the next important goal of CV. But, despite various efforts in this direction, CV is still far from a system like GPT that naturally integrates all tasks. We point out that the essential weakness of CV lies in lacking a paradigm to learn from environments, yet NLP has accomplished the task in the text world. We then imagine a pipeline that puts a CV algorithm (i.e., an agent) in world-scale, interactable environments, pre-trains it to predict future frames with respect to its action, and then fine-tunes it with instruction to accomplish various tasks. We expect substantial research and engineering efforts to push the idea forward and scale it up, for which we share our perspectives on future research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3041937",
                    "name": "Lingxi Xie"
                },
                {
                    "authorId": "26351715",
                    "name": "Longhui Wei"
                },
                {
                    "authorId": "2108250420",
                    "name": "Xiaopeng Zhang"
                },
                {
                    "authorId": "94365920",
                    "name": "Kaifeng Bi"
                },
                {
                    "authorId": "7787721",
                    "name": "Xiaotao Gu"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "0c7ce5898dab92da540457b754254d72b8592fc2",
            "title": "Parameter-efficient Tuning of Large-scale Multimodal Foundation Model",
            "abstract": "Driven by the progress of large-scale pre-training, parameter-efficient transfer learning has gained immense popularity across different subfields of Artificial Intelligence. The core is to adapt the model to downstream tasks with only a small set of parameters. Recently, researchers have leveraged such proven techniques in multimodal tasks and achieve promising results. However, two critical issues remain unresolved: how to further reduce the complexity with lightweight design and how to boost alignment between modalities under extremely low parameters. In this paper, we propose A graceful prompt framework for cross-modal transfer (Aurora) to overcome these challenges. Considering the redundancy in existing architectures, we first utilize the mode approximation to generate 0.1M trainable parameters to implement the multimodal prompt tuning, which explores the low intrinsic dimension with only 0.04% parameters of the pre-trained model. Then, for better modality alignment, we propose the Informative Context Enhancement and Gated Query Transformation module under extremely few parameters scenes. A thorough evaluation on six cross-modal benchmarks shows that it not only outperforms the state-of-the-art but even outperforms the full fine-tuning approach. Our code is available at: https://github.com/WillDreamer/Aurora.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2150441370",
                    "name": "Xinlong Yang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "2168258085",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "29a7644583d7042c4476af126f3fe0a372897abe",
            "title": "LION: Implicit Vision Prompt Tuning",
            "abstract": "Despite recent promising performances across a range of vision tasks, vision Transformers still have an issue of high computational costs.\nRecently, vision prompt learning has provided an economical solution to this problem without fine-tuning the whole large-scale model. \nHowever, the efficiency and effectiveness of existing models are still far from satisfactory due to the parameter cost of extensive prompt blocks and tricky prompt framework designs. \nIn this paper, we propose a light-weight prompt framework named impLicit vIsion prOmpt tuNing (LION), which is motivated by deep implicit models with stable low memory costs for various complex tasks.\nIn particular, we merely insect two equilibrium implicit layers in two ends of the pre-trained backbone with parameters frozen. Moreover, according to the lottery hypothesis, we further prune the parameters to relieve the computation burden in implicit layers. Various experiments have validated that our LION obtains promising performances on a wide range of datasets. Most importantly, LION reduces up to 11.5 % of training parameter numbers while obtaining higher performance than the state-of-the-art VPT, especially under challenging scenes. Furthermore, we find that our proposed LION has an excellent generalization performance, making it an easy way to boost transfer learning in the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2168258085",
                    "name": "Xiao Luo"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "33383055",
                    "name": "Zhouchen Lin"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "47dc9942a83fee414c3944a0ca69987edc44795a",
            "title": "Open-Set Fine-Grained Retrieval via Prompting Vision-Language Evaluator",
            "abstract": "Open-set fine-grained retrieval is an emerging challenge that requires an extra capability to retrieve unknown subcategories during evaluation. However, current works focus on close-set visual concepts, where all the subcategories are pre-defined, and make it hard to capture discriminative knowledge from unknown subcategories, consequently failing to handle unknown subcategories in open-world scenarios. In this work, we propose a novel Prompting vision-Language Evaluator (PLEor) framework based on the recently introduced contrastive language-image pretraining (CLIP) model, for open-set fine-grained retrieval. PLEor could leverage pre-trained CLIP model to infer the discrepancies encompassing both pre-defined and unknown subcategories, called category-specific discrepancies, and transfer them to the backbone network trained in the close-set scenarios. To make pre-trained CLIP model sensitive to category-specific discrepancies, we design a dual prompt scheme to learn a vision prompt specifying the categoryspecific discrepancies, and turn random vectors with category names in a text prompt into category-specific discrepancy descriptions. Moreover, a vision-language evaluator is proposed to semantically align the vision and text prompts based on CLIP model, and reinforce each other. In addition, we propose an open-set knowledge transfer to transfer the category-specific discrepancies into the backbone network using knowledge distillation mechanism. Quantitative and qualitative experiments show that our PLEor achieves promising performance on open-set fine-grained datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108620644",
                    "name": "Shijie Wang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2198533822",
                    "name": "Haojie Li"
                },
                {
                    "authorId": "47196393",
                    "name": "Zhihui Wang"
                },
                {
                    "authorId": "3001348",
                    "name": "Wanli Ouyang"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "501364e655182cc8cf6ab7cb37137ac9ea2403be",
            "title": "Constraint and Union for Partially-Supervised Temporal Sentence Grounding",
            "abstract": "Temporal sentence grounding aims to detect the event timestamps described by the natural language query from given untrimmed videos. The existing fully-supervised setting achieves great performance but requires expensive annotation costs; while the weakly-supervised setting adopts cheap labels but performs poorly. To pursue high performance with less annotation cost, this paper introduces an intermediate partially-supervised setting, i.e., only short-clip or even single-frame labels are available during training. To take full advantage of partial labels, we propose a novel quadruple constraint pipeline to comprehensively shape event-query aligned representations, covering intra- and inter-samples, uni- and multi-modalities. The former raises intra-cluster compactness and inter-cluster separability; while the latter enables event-background separation and event-query gather. To achieve more powerful performance with explicit grounding optimization, we further introduce a partial-full union framework, i.e., bridging with an additional fully-supervised branch, to enjoy its impressive grounding bonus, and be robust to partial annotations. Extensive experiments and ablations on Charades-STA and ActivityNet Captions demonstrate the significance of partial supervision and our superior performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059175825",
                    "name": "Chen Ju"
                },
                {
                    "authorId": "2109252575",
                    "name": "Haicheng Wang"
                },
                {
                    "authorId": "2047214830",
                    "name": "Jinxian Liu"
                },
                {
                    "authorId": "2124034538",
                    "name": "Chaofan Ma"
                },
                {
                    "authorId": "46868037",
                    "name": "Ya Zhang"
                },
                {
                    "authorId": "2283619",
                    "name": "Peisen Zhao"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "5133f4d4293e894a9bd666fa0083eafd1764850d",
            "title": "Learning to Parameterize Visual Attributes for Open-set Fine-grained Retrieval",
            "abstract": "Open-set fine-grained retrieval is an emerging challenging task that allows to retrieve unknown categories beyond the training set. The best solution for handling unknown categories is to represent them using a set of visual attributes learnt from known categories, as widely used in zero-shot learning. Though important, attribute modeling usually requires significant manual annotations and thus is labor-intensive. Therefore, it is worth to investigate how to transform retrieval models trained by image-level supervision from category semantic extraction to attribute modeling. To this end, we propose a novel Visual Attribute Parameterization Network (VAPNet) to learn visual attributes from known categories and parameterize them into the retrieval model, without the involvement of any attribute annotations. In this way, VAPNet could utilize its parameters to parse a set of visual attributes from unknown categories and precisely represent them. Technically, VAPNet explicitly attains some semantics with rich details via making use of local image patches and distills the visual attributes from these discovered semantics. Additionally, it integrates the online refinement of these visual attributes into the training process to iteratively enhance their quality. Simultaneously, VAPNet treats these attributes as supervisory signals to tune the retrieval models, thereby achieving attribute parameterization. Extensive experiments on open-set fine-grained retrieval datasets validate the superior performance of our VAPNet over existing solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2272104691",
                    "name": "Shijie Wang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2198533822",
                    "name": "Haojie Li"
                },
                {
                    "authorId": "2228348127",
                    "name": "Zhihui Wang"
                },
                {
                    "authorId": "2257000758",
                    "name": "Wanli Ouyang"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        },
        {
            "paperId": "852ab98a42204ea905ba41b3b1354820be4f201c",
            "title": "When Parameter-efficient Tuning Meets General-purpose Vision-language Models",
            "abstract": "Instruction tuning has shown promising potential for developing general-purpose AI capabilities by using large-scale pre-trained models and boosts growing research to integrate multimodal information for creative applications. However, existing works still face two main limitations: the high training costs and heavy computing resource dependence of full model fine-tuning, and the lack of semantic information in instructions, which hinders multimodal alignment. Addressing these challenges, this paper proposes a novel approach to utilize Parameter-Efficient Tuning for generAl-purpose vision-Language models, namely PETAL. PETAL revolutionizes the training process by requiring only 0.5% of the total parameters, achieved through a unique mode approximation technique, which significantly reduces the training costs and reliance on heavy computing resources. Furthermore, PETAL enhances the semantic depth of instructions in two innovative ways: 1) by introducing adaptive instruction mixture-of-experts(MOEs), and 2) by fortifying the score-based linkage between parameter-efficient tuning and mutual information. Our extensive experiments across five multimodal downstream benchmarks reveal that PETAL not only outperforms current state-of-the-art methods in most scenarios but also surpasses full fine-tuning models in effectiveness. Additionally, our approach demonstrates remarkable advantages in few-shot settings, backed by comprehensive visualization analyses. Our source code is available at: https://github. com/melonking32/PETAL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275115509",
                    "name": "Yihang Zhai"
                },
                {
                    "authorId": "2109591599",
                    "name": "Haixin Wang"
                },
                {
                    "authorId": "2364376",
                    "name": "Jianlong Chang"
                },
                {
                    "authorId": "2265575121",
                    "name": "Xinlong Yang"
                },
                {
                    "authorId": "2262309",
                    "name": "Jinan Sun"
                },
                {
                    "authorId": "2184249162",
                    "name": "Shikun Zhang"
                },
                {
                    "authorId": "2149898830",
                    "name": "Qi Tian"
                }
            ]
        }
    ]
}