{
    "authorId": "3025780",
    "papers": [
        {
            "paperId": "146efbdd6b4ca18c4d00880b8e06bde72644d49f",
            "title": "Retrieve, Merge, Predict: Augmenting Tables with Data Lakes",
            "abstract": "We present an in-depth analysis of data discovery in data lakes, focusing on table augmentation for given machine learning tasks. We analyze alternative methods used in the three main steps: retrieving joinable tables, merging information, and predicting with the resultant table. As data lakes, the paper uses YADL (Yet Another Data Lake) -- a novel dataset we developed as a tool for benchmarking this data discovery task -- and Open Data US, a well-referenced real data lake. Through systematic exploration on both lakes, our study outlines the importance of accurately retrieving join candidates and the efficiency of simple merging methods. We report new insights on the benefits of existing solutions and on their limitations, aiming at guiding future research in this space.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51031408",
                    "name": "Riccardo Cappuzzo"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                },
                {
                    "authorId": "2283770826",
                    "name": "Aimee Coelho"
                },
                {
                    "authorId": "2265384723",
                    "name": "Paolo Papotti"
                }
            ]
        },
        {
            "paperId": "1961368c26f42f0e6777d34b6e9c40e179651bb1",
            "title": "Learning High-Quality and General-Purpose Phrase Representations",
            "abstract": "Phrase representations play an important role in data science and natural language processing, benefiting various tasks like Entity Alignment, Record Linkage, Fuzzy Joins, and Paraphrase Classification.The current state-of-the-art method involves fine-tuning pre-trained language models for phrasal embeddings using contrastive learning. However, we have identified areas for improvement. First, these pre-trained models tend to be unnecessarily complex and require to be pre-trained on a corpus with context sentences.Second, leveraging the phrase type and morphology gives phrase representations that are both more precise and more flexible.We propose an improved framework to learn phrase representations in a context-free fashion.The framework employs phrase type classification as an auxiliary task and incorporates character-level information more effectively into the phrase representation.Furthermore, we design three granularities of data augmentation to increase the diversity of training samples.Our experiments across a wide range of tasks reveal that our approach generates superior phrase embeddings compared to previous methods while requiring a smaller model size.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260594517",
                    "name": "Lihu Chen"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                },
                {
                    "authorId": "2277286707",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "3d6be47c043d4ec1111e376a3e52eb879c814cb3",
            "title": "Confidence intervals uncovered: Are we ready for real-world medical imaging AI?",
            "abstract": "Medical imaging is spearheading the AI transformation of healthcare. Performance reporting is key to determine which methods should be translated into clinical practice. Frequently, broad conclusions are simply derived from mean performance values. In this paper, we argue that this common practice is often a misleading simplification as it ignores performance variability. Our contribution is threefold. (1) Analyzing all MICCAI segmentation papers (n = 221) published in 2023, we first observe that more than 50% of papers do not assess performance variability at all. Moreover, only one (0.5%) paper reported confidence intervals (CIs) for model performance. (2) To address the reporting bottleneck, we show that the unreported standard deviation (SD) in segmentation papers can be approximated by a second-order polynomial function of the mean Dice similarity coefficient (DSC). Based on external validation data from 56 previous MICCAI challenges, we demonstrate that this approximation can accurately reconstruct the CI of a method using information provided in publications. (3) Finally, we reconstructed 95% CIs around the mean DSC of MICCAI 2023 segmentation papers. The median CI width was 0.03 which is three times larger than the median performance gap between the first and second ranked method. For more than 60% of papers, the mean performance of the second-ranked method was within the CI of the first-ranked method. We conclude that current publications typically do not provide sufficient evidence to support which models could potentially be translated into clinical practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51248761",
                    "name": "E. Christodoulou"
                },
                {
                    "authorId": "47131776",
                    "name": "Annika Reinke"
                },
                {
                    "authorId": "104862189",
                    "name": "Rola Houhou"
                },
                {
                    "authorId": "2322985965",
                    "name": "Piotr Kalinowski"
                },
                {
                    "authorId": "2322986563",
                    "name": "Selen Erkan"
                },
                {
                    "authorId": "2282769611",
                    "name": "Carole H. Sudre"
                },
                {
                    "authorId": "2267484563",
                    "name": "Ninon Burgos"
                },
                {
                    "authorId": "2322989899",
                    "name": "Sofiene Boutaj"
                },
                {
                    "authorId": "2129489721",
                    "name": "Sophie Loizillon"
                },
                {
                    "authorId": "2203368148",
                    "name": "Maelys Solal"
                },
                {
                    "authorId": "3180607",
                    "name": "Nicola Rieke"
                },
                {
                    "authorId": "2225413",
                    "name": "V. Cheplygina"
                },
                {
                    "authorId": "51504413",
                    "name": "M. Antonelli"
                },
                {
                    "authorId": "2322991030",
                    "name": "Leon D. Mayer"
                },
                {
                    "authorId": "14332753",
                    "name": "M. Tizabi"
                },
                {
                    "authorId": "2260353658",
                    "name": "M. J. Cardoso"
                },
                {
                    "authorId": "2322933295",
                    "name": "Amber Simpson"
                },
                {
                    "authorId": "1661131601",
                    "name": "Paul F. Jager"
                },
                {
                    "authorId": "2255453145",
                    "name": "Annette Kopp-Schneider"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                },
                {
                    "authorId": "2333839",
                    "name": "O. Colliot"
                },
                {
                    "authorId": "2307371712",
                    "name": "Lena Maier-Hein"
                }
            ]
        },
        {
            "paperId": "4791862758956b4e3da1629d0f2c9dae8019a8e1",
            "title": "Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI",
            "abstract": "With the growing attention and investment in recent AI approaches such as large language models, the narrative that the larger the AI system the more valuable, powerful and interesting it is is increasingly seen as common sense. But what is this assumption based on, and how are we measuring value, power, and performance? And what are the collateral consequences of this race to ever-increasing scale? Here, we scrutinize the current scaling trends and trade-offs across multiple axes and refute two common assumptions underlying the 'bigger-is-better' AI paradigm: 1) that improved performance is a product of increased scale, and 2) that all interesting problems addressed by AI require large-scale models. Rather, we argue that this approach is not only fragile scientifically, but comes with undesirable consequences. First, it is not sustainable, as its compute demands increase faster than model performance, leading to unreasonable economic requirements and a disproportionate environmental footprint. Second, it implies focusing on certain problems at the expense of others, leaving aside important applications, e.g. health, education, or the climate. Finally, it exacerbates a concentration of power, which centralizes decision-making in the hands of a few actors while threatening to disempower others in the context of shaping both AI research and its applications throughout society.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                },
                {
                    "authorId": "2993731",
                    "name": "A. Luccioni"
                },
                {
                    "authorId": "2322449176",
                    "name": "Meredith Whittaker"
                }
            ]
        },
        {
            "paperId": "659fe890e963c574c083f1b60754a071d945b5b2",
            "title": "CARTE: pretraining and transfer for tabular learning",
            "abstract": "Pretrained deep-learning models are the go-to solution for images or text. However, for tabular data the standard is still to train tree-based models. Indeed, transfer learning on tables hits the challenge of data integration: finding correspondences, correspondences in the entries (entity matching) where different words may denote the same entity, correspondences across columns (schema matching), which may come in different orders, names... We propose a neural architecture that does not need such correspondences. As a result, we can pretrain it on background data that has not been matched. The architecture -- CARTE for Context Aware Representation of Table Entries -- uses a graph representation of tabular (or relational) data to process tables with different columns, string embedding of entries and columns names to model an open vocabulary, and a graph-attentional network to contextualize entries with column names and neighboring entries. An extensive benchmark shows that CARTE facilitates learning, outperforming a solid set of baselines including the best tree-based models. CARTE also enables joint learning across tables with unmatched columns, enhancing a small table with bigger ones. CARTE opens the door to large pretrained models for tabular data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275709676",
                    "name": "Myung Jun Kim"
                },
                {
                    "authorId": "1509420956",
                    "name": "L\u00e9o Grinsztajn"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                }
            ]
        },
        {
            "paperId": "72db0c6e52adf34435bd396e211f96c6d023b42a",
            "title": "Teaching Models To Survive: Proper Scoring Rule and Stochastic Optimization with Competing Risks",
            "abstract": "When data are right-censored, i.e. some outcomes are missing due to a limited period of observation, survival analysis can compute the\"time to event\". Multiple classes of outcomes lead to a classification variant: predicting the most likely event, known as competing risks, which has been less studied. To build a loss that estimates outcome probabilities for such settings, we introduce a strictly proper censoring-adjusted separable scoring rule that can be optimized on a subpart of the data because the evaluation is made independently of observations. It enables stochastic optimization for competing risks which we use to train gradient boosting trees. Compared to 11 state-of-the-art models, this model, MultiIncidence, performs best in estimating the probability of outcomes in survival and competing risks. It can predict at any time horizon and is much faster than existing alternatives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307471824",
                    "name": "Julie Alberge"
                },
                {
                    "authorId": "2307471586",
                    "name": "Vincent Maladiere"
                },
                {
                    "authorId": "2958756",
                    "name": "O. Grisel"
                },
                {
                    "authorId": "2307473070",
                    "name": "Judith Ab'ecassis"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                }
            ]
        },
        {
            "paperId": "754f9c903754d909cb754364f4d6416ada0ab2b5",
            "title": "Reconfidencing LLMs from the Grouping Loss Perspective",
            "abstract": "Large Language Models (LLMs), including ChatGPT and LLaMA, are susceptible to generating hallucinated answers in a confident tone. While efforts to elicit and calibrate confidence scores have proven useful, recent findings show that controlling uncertainty must go beyond calibration: predicted scores may deviate significantly from the actual posterior probabilities due to the impact of grouping loss. In this work, we construct a new evaluation dataset derived from a knowledge base to assess confidence scores given to answers of Mistral and LLaMA. Experiments show that they tend to be overconfident. Further, we show that they are more overconfident on some answers than others, \\emph{eg} depending on the nationality of the person in the query. In uncertainty-quantification theory, this is grouping loss. To address this, we propose a solution to reconfidence LLMs, canceling not only calibration but also grouping loss. The LLMs, after the reconfidencing process, indicate improved confidence alignment with the accuracy of their responses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260594517",
                    "name": "Lihu Chen"
                },
                {
                    "authorId": "2149725904",
                    "name": "Alexandre Perez-Lebel"
                },
                {
                    "authorId": "2277286707",
                    "name": "Fabian M. Suchanek"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                }
            ]
        },
        {
            "paperId": "9369e4d7bd45a522878950b8259651cf1f4c717a",
            "title": "Imputation for prediction: beware of diminishing returns",
            "abstract": "Missing values are prevalent across various fields, posing challenges for training and deploying predictive models. In this context, imputation is a common practice, driven by the hope that accurate imputations will enhance predictions. However, recent theoretical and empirical studies indicate that simple constant imputation can be consistent and competitive. This empirical study aims at clarifying if and when investing in advanced imputation methods yields significantly better predictions. Relating imputation and predictive accuracies across combinations of imputation and predictive models on 20 datasets, we show that imputation accuracy matters less i) when using expressive models, ii) when incorporating missingness indicators as complementary inputs, iii) matters much more for generated linear outcomes than for real-data outcomes. Interestingly, we also show that the use of the missingness indicator is beneficial to the prediction performance, even in MCAR scenarios. Overall, on real-data with powerful models, improving imputation only has a minor effect on prediction performance. Thus, investing in better imputations for improved predictions often offers limited benefits.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "19288464",
                    "name": "Marine Le Morvan"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                }
            ]
        },
        {
            "paperId": "e79981c91c1ac40d747377b4af7409793d8e7350",
            "title": "What is the Role of Small Models in the LLM Era: A Survey",
            "abstract": "Large Language Models (LLMs) have made significant progress in advancing artificial general intelligence (AGI), leading to the development of increasingly large models such as GPT-4 and LLaMA-405B. However, scaling up model sizes results in exponentially higher computational costs and energy consumption, making these models impractical for academic researchers and businesses with limited resources. At the same time, Small Models (SMs) are frequently used in practical settings, although their significance is currently underestimated. This raises important questions about the role of small models in the era of LLMs, a topic that has received limited attention in prior research. In this work, we systematically examine the relationship between LLMs and SMs from two key perspectives: Collaboration and Competition. We hope this survey provides valuable insights for practitioners, fostering a deeper understanding of the contribution of small models and promoting more efficient use of computational resources. The code is available at https://github.com/tigerchen52/role_of_small_models",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260594517",
                    "name": "Lihu Chen"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                }
            ]
        },
        {
            "paperId": "2305016782c2d6eaa07e38d0614eafbd44c643d3",
            "title": "How to select predictive models for causal inference?",
            "abstract": "As predictive models -- e.g., from machine learning -- give likely outcomes, they may be used to reason on the effect of an intervention, a causal-inference task. The increasing complexity of health data has opened the door to a plethora of models, but also the Pandora box of model selection: which of these models yield the most valid causal estimates? Here we highlight that classic machine-learning model selection does not select the best outcome models for causal inference. Indeed, causal model selection should control both outcome errors for each individual, treated or not treated, whereas only one outcome is observed. Theoretically, simple risks used in machine learning do not control causal effects when treated and non-treated population differ too much. More elaborate risks build proxies of the causal error using ``nuisance'' re-weighting to compute it on the observed data. But does computing these nuisance adds noise to model selection? Drawing from an extensive empirical study, we outline a good causal model-selection procedure: using the so-called $R\\text{-risk}$; using flexible estimators to compute the nuisance models on the train set; and splitting out 10\\% of the data to compute risks.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "80844951",
                    "name": "M. Doutreligne"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                }
            ]
        }
    ]
}