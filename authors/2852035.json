{
    "authorId": "2852035",
    "papers": [
        {
            "paperId": "846768c90137ecbf08ab6630bb894bb349096bc3",
            "title": "DocSynthv2: A Practical Autoregressive Modeling for Document Generation",
            "abstract": "While the generation of document layouts has been extensively explored, comprehensive document generation encompassing both layout and content presents a more complex challenge. This paper delves into this advanced domain, proposing a novel approach called DocSynthv2 through the development of a simple yet effective autoregressive structured model. Our model, distinct in its integration of both layout and textual cues, marks a step beyond existing layout-generation approaches. By focusing on the relationship between the structural elements and the textual content within documents, we aim to generate cohesive and contextually relevant documents without any reliance on visual components. Through experimental studies on our curated benchmark for the new task, we demonstrate the ability of our model combining layout and textual information in enhancing the generation quality and relevance of documents, opening new pathways for research in document creation and automated design. Our findings emphasize the effectiveness of autoregressive models in handling complex document generation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150473007",
                    "name": "Sanket Biswas"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "26360698",
                    "name": "Curtis Wigington"
                },
                {
                    "authorId": "2190051546",
                    "name": "Tongfei Sun"
                },
                {
                    "authorId": "2305681954",
                    "name": "Josep Llad'os"
                }
            ]
        },
        {
            "paperId": "88334b1040eb7bbd587af95ee60e62f7761e24ab",
            "title": "TutoAI: a cross-domain framework for AI-assisted mixed-media tutorial creation on physical tasks",
            "abstract": "Mixed-media tutorials, which integrate videos, images, text, and diagrams to teach procedural skills, offer more browsable alternatives than timeline-based videos. However, manually creating such tutorials is tedious, and existing automated solutions are often restricted to a particular domain. While AI models hold promise, it is unclear how to effectively harness their powers, given the multi-modal data involved and the vast landscape of models. We present TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial creation on physical tasks. First, we distill common tutorial components by surveying existing work; then, we present an approach to identify, assemble, and evaluate AI models for component extraction; finally, we propose guidelines for designing user interfaces (UI) that support tutorial creation based on AI-generated components. We show that TutoAI has achieved higher or similar quality compared to a baseline model in preliminary user studies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265816484",
                    "name": "Yuexi Chen"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "2291068632",
                    "name": "Anh Truong"
                },
                {
                    "authorId": "2291054565",
                    "name": "Zhicheng Liu"
                }
            ]
        },
        {
            "paperId": "b3795d73cadfacfb6aa0f723978915012070a3d5",
            "title": "DocScript: Document-level Script Event Prediction",
            "abstract": "We present a novel task of document-level script event prediction, which aims to predict the next event given a candidate list of narrative events in long-form documents. To enable this, we introduce DocSEP, a challenging dataset in two new domains - contractual documents and Wikipedia articles, where timeline events may be paragraphs apart and may require multi-hop temporal and causal reasoning. We benchmark existing baselines and present a novel architecture called DocScript to learn sequential ordering between events at the document scale. Our experimental results on the DocSEP dataset demonstrate that learning longer-range dependencies between events is a key challenge and show that contemporary LLMs such as ChatGPT and FlanT5 struggle to solve this task, indicating their lack of reasoning abilities for understanding causal relationships and temporal sequences within long texts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "31099365",
                    "name": "Aparna Garimella"
                },
                {
                    "authorId": "2301580599",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2273677165",
                    "name": "Dinesh Manocha"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                }
            ]
        },
        {
            "paperId": "1d22ee803dd4c7a4c7dfdc21d53331fa949cf2a8",
            "title": "DocDancer: Authoring Ultra-Responsive Documents with Layout Generation",
            "abstract": "Responsive design enhances user experience by adapting layout and content to different display factors. However, existing tools for authoring responsive design primarily adapt to screen width only, and they either rely on predefined templates or require significant manual effort. To expedite responsive design creation, we introduce an authoring tool called DocDancer. DocDancer supports creating ultra-responsive documents where both layout and content adapt to multiple factors (screen width, font properties, customer segments, etc), and provides layout suggestions based on user-provided content and popular responsive patterns. A comparative user study with 16 participants shows that authoring responsive documents in DocDancer takes significantly less time and effort than a commercial tool.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265816484",
                    "name": "Yuexi Chen"
                },
                {
                    "authorId": "2265717775",
                    "name": "Zhicheng Liu"
                },
                {
                    "authorId": "2265677478",
                    "name": "Christopher Tensmeyer"
                },
                {
                    "authorId": "2265676726",
                    "name": "Niklas Elmqvist"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                }
            ]
        },
        {
            "paperId": "383db93b039c2f7d743a06dc62a8db2ff1ea33f7",
            "title": "DocEdit: Language-Guided Document Editing",
            "abstract": "Professional document editing tools require a certain level of expertise to perform complex edit operations. To make editing tools accessible to increasingly novice users, we investigate intelligent document assistant systems that can make or suggest edits based on a user's natural language request. Such a system should be able to understand the user's ambiguous requests and contextualize them to the visual cues and textual content found in a document image to edit localized unstructured text and structured layouts. To this end, we propose a new task of language-guided localized document editing, where the user provides a document and an open vocabulary editing request, and the intelligent system produces a command that can be used to automate edits in real-world document editing software. In support of this task, we curate the DocEdit dataset, a collection of approximately 28K instances of user edit requests over PDF and design templates along with their corresponding ground truth software executable commands. To our knowledge, this is the first dataset that provides a diverse mix of edit operations with direct and indirect references to the embedded text and visual objects such as paragraphs, lists, tables, etc. We also propose DocEditor, a Transformer-based localization-aware multimodal (textual, spatial, and visual) model that performs the new task. The model attends to both document objects and related text contents which may be referred to in a user edit request, generating a multimodal embedding that is used to predict an edit command and associated bounding box localizing it. Our proposed model empirically outperforms other baseline deep learning approaches by 15-18%, providing a strong starting point for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "2301580599",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2172597446",
                    "name": "Dinesh Manocha"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                }
            ]
        },
        {
            "paperId": "40419c73884c5bdeaee3d5166299977e201a9a0b",
            "title": "A Critical Analysis of Document Out-of-Distribution Detection",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "2143847067",
                    "name": "Yifei Ming"
                },
                {
                    "authorId": "2273755938",
                    "name": "Yi Zhou"
                },
                {
                    "authorId": "1859486",
                    "name": "Jason Kuen"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "2273802124",
                    "name": "Handong Zhao"
                },
                {
                    "authorId": "2261751659",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "1598478975",
                    "name": "Nikolaos Barmpalios"
                },
                {
                    "authorId": "2273985067",
                    "name": "Anqi Liu"
                },
                {
                    "authorId": "2273756302",
                    "name": "Yixuan Li"
                },
                {
                    "authorId": "2274099393",
                    "name": "Tong Sun"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                }
            ]
        },
        {
            "paperId": "91ac4c094a2936c292b5dec56fd44a913f183083",
            "title": "LayerDoc: Layer-wise Extraction of Spatial Hierarchical Structure in Visually-Rich Documents",
            "abstract": "Digital documents often contain images and scanned text. Parsing such visually-rich documents is a core task for work-flow automation, but it remains challenging since most documents do not encode explicit layout information, e.g., how characters and words are grouped into boxes and ordered into larger semantic entities. Current state-of-the-art layout extraction methods are challenged by such documents as they rely on word sequences to have correct reading order and do not exploit their hierarchical structure. We propose LayerDoc, an approach that uses visual features, textual semantics, and spatial coordinates along with constraint inference to extract the hierarchical layout structure of documents in a bottom-up layer-wise fashion. LayerDoc recursively groups smaller regions into larger semantic elements in 2D to infer complex nested hierarchies. Experiments show that our approach outperforms competitive baselines by 10-15% on three diverse datasets of forms and mobile app screen layouts for the tasks of spatial region classification, higher-order group identification, layout hierarchy extraction, reading order detection, and word grouping.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2064021145",
                    "name": "Ashutosh Mehra"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "2301580599",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2101319527",
                    "name": "Anandhavelu N"
                },
                {
                    "authorId": "2536742",
                    "name": "Quan Hung Tran"
                },
                {
                    "authorId": "1419671559",
                    "name": "Verena Kaynig-Fittkau"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                },
                {
                    "authorId": "2172597446",
                    "name": "Dinesh Manocha"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                }
            ]
        },
        {
            "paperId": "b0f6f1c464700c79fa20fb7b2e3975c60208a559",
            "title": "Improving Cross-Domain Detection with Self-Supervised Learning",
            "abstract": "Cross-Domain Detection (XDD) aims to train a domain-adaptive object detector using unlabeled images from a target domain and labeled images from a source domain. Existing approaches achieve this either by transferring the style of source images to that of target images, or by aligning the features of images from the two domains. In this paper, rather than proposing another method following the existing lines, we introduce a new framework complementary to existing methods. Our framework unifies some popular Self-Supervised Learning (SSL) techniques (e.g., rotation angle prediction, strong/weak data augmentation, mean teacher modeling) and adapts them to the XDD task. Our basic idea is to leverage the unsupervised nature of these SSL techniques and apply them simultaneously across domains (source and target) and models (student and teacher). These SSL techniques can thus serve as shared bridges that facilitate knowledge transfer between domains. More importantly, as these techniques are independently applied in each domain, they are complementary to existing domain alignment techniques that relies on interactions between domains (e.g., adversarial alignment). We perform extensive analyses on these SSL techniques and show that they significantly improve the performance of existing methods. In addition, we reach comparable or even better performance than the state-of-the-art methods when integrating our framework with an old well-established method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "94451829",
                    "name": "K. Li"
                },
                {
                    "authorId": "26360698",
                    "name": "Curtis Wigington"
                },
                {
                    "authorId": "67319819",
                    "name": "Chris Tensmeyer"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "7574699",
                    "name": "Handong Zhao"
                },
                {
                    "authorId": "1977256",
                    "name": "Varun Manjunatha"
                },
                {
                    "authorId": "1598478975",
                    "name": "Nikolaos Barmpalios"
                },
                {
                    "authorId": "46956675",
                    "name": "Y. Fu"
                }
            ]
        },
        {
            "paperId": "fc1400e02aa3b1ffb07628dc152a9157feac4782",
            "title": "Improving a Named Entity Recognizer Trained on Noisy Data with a Few Clean Instances",
            "abstract": "To achieve state-of-the-art performance, one still needs to train NER models on large-scale, high-quality annotated data, an asset that is both costly and time-intensive to accumulate. In contrast, real-world applications often resort to massive low-quality labeled data through non-expert annotators via crowdsourcing and external knowledge bases via distant supervision as a cost-effective alternative. However, these annotation methods result in noisy labels, which in turn lead to a notable decline in performance. Hence, we propose to denoise the noisy NER data with guidance from a small set of clean instances. Along with the main NER model we train a discriminator model and use its outputs to recalibrate the sample weights. The discriminator is capable of detecting both span and category errors with different discriminative prompts. Results on public crowdsourcing and distant supervision datasets show that the proposed method can consistently improve performance with a small guidance set.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261494597",
                    "name": "Zhendong Chu"
                },
                {
                    "authorId": "2261751659",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                }
            ]
        },
        {
            "paperId": "182ba87b08726063765f3bed20ca53adf6e1f472",
            "title": "End-to-end Document Recognition and Understanding with Dessurt",
            "abstract": "We introduce Dessurt, a relatively simple document understanding transformer capable of being fine-tuned on a greater variety of document tasks than prior methods. It receives a document image and task string as input and generates arbitrary text autoregressively as output. Because Dessurt is an end-to-end architecture that performs text recognition in addition to the document understanding, it does not require an external recognition model as prior methods do. Dessurt is a more flexible model than prior methods and is able to handle a variety of document domains and tasks. We show that this model is effective at 9 different dataset-task combinations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145566717",
                    "name": "Brian L. Davis"
                },
                {
                    "authorId": "2877830",
                    "name": "B. Morse"
                },
                {
                    "authorId": "31844147",
                    "name": "Brian L. Price"
                },
                {
                    "authorId": "67319819",
                    "name": "Chris Tensmeyer"
                },
                {
                    "authorId": "26360698",
                    "name": "Curtis Wigington"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                }
            ]
        }
    ]
}