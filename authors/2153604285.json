{
    "authorId": "2153604285",
    "papers": [
        {
            "paperId": "2e58aa10ab8fb29b2a8fa91220ccc707e6b19f0d",
            "title": "AutoOD: Automatic Outlier Detection",
            "abstract": "Outlier detection is critical in real world. Due to the existence of many outlier detection techniques which often return different results for the same data set, the users have to address the problem of determining which among these techniques is the best suited for their task and tune its parameters. This is particularly challenging in the unsupervised setting, where no labels are available for cross-validation needed for such method and parameter optimization. In this work, we propose AutoOD which uses the existing unsupervised detection techniques to automatically produce high quality outliers without any human tuning. AutoOD's fundamentally new strategy unifies the merits of unsupervised outlier detection and supervised classification within one integrated solution. It automatically tests a diverse set of unsupervised outlier detectors on a target data set, extracts useful signals from their combined detection results to reliably capture key differences between outliers and inliers. It then uses these signals to produce a \"custom outlier classifier\" to classify outliers, with its accuracy comparable to supervised outlier classification models trained with ground truth labels - without having access to the much needed labels. On a diverse set of benchmark outlier detection datasets, AutoOD consistently outperforms the best unsupervised outlier detector selected from hundreds of detectors. It also outperforms other tuning-free approaches from 12 to 97 points (out of 100) in the F-1 score.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153604285",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "40457242",
                    "name": "Yu Wang"
                }
            ]
        },
        {
            "paperId": "506d5864304a9907d14fc02fd2e3c22bb5bb036f",
            "title": "Label Denoising through Cross-Model Agreement",
            "abstract": "Learning from corrupted labels is very common in real-world machine-learning applications. Memorizing such noisy labels could affect the learning of the model, leading to sub-optimal performances. In this work, we propose a novel framework to learn robust machine-learning models from noisy labels. Through an empirical study, we find that different models make relatively similar predictions on clean examples, while the predictions on noisy examples vary much more across different models. Motivated by this observation, we propose \\em denoising with cross-model agreement \\em (DeCA) which aims to minimize the KL-divergence between the true label distributions parameterized by two machine learning models while maximizing the likelihood of data observation. We employ the proposed DeCA on both the binary label scenario and the multiple label scenario. For the binary label scenario, we select implicit feedback recommendation as the downstream task and conduct experiments with four state-of-the-art recommendation models on four datasets. For the multiple-label scenario, the downstream application is image classification on two benchmark datasets. Experimental results demonstrate that the proposed methods significantly improve the model performance compared with normal training and other denoising methods on both binary and multiple-label scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153604285",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2113821128",
                    "name": "Xin Xin"
                },
                {
                    "authorId": "3451645",
                    "name": "Zaiqiao Meng"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                },
                {
                    "authorId": "50686770",
                    "name": "J. Jose"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "71b99a53892409720dc8867afffe64bf3632af6b",
            "title": "Learning Concise and Descriptive Attributes for Visual Recognition",
            "abstract": "Recent advances in foundation models present new opportunities for interpretable visual recognition \u2013 one can first query Large Language Models (LLMs) to obtain a set of attributes that describe each class, then apply vision-language models to classify images via these attributes. Pioneering work shows that querying thousands of attributes can achieve performance competitive with image features. However, our further investigation on 8 datasets reveals that LLM-generated attributes in a large quantity perform almost the same as random words. This surprising finding suggests that significant noise may be present in these attributes. We hypothesize that there exist subsets of attributes that can maintain the classification performance with much smaller sizes, and propose a novel learning-to-search method to discover those concise sets of attributes. As a result, on the CUB dataset, our method achieves performance close to that of massive LLM-generated attributes (e.g., 10k attributes for CUB), yet using only 32 attributes in total to distinguish 200 bird species. Furthermore, our new paradigm demonstrates several additional benefits: higher interpretability and interactivity for humans, and the ability to summarize knowledge for a recognition task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064233490",
                    "name": "Andy Yan"
                },
                {
                    "authorId": "2153604285",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "1828787912",
                    "name": "Yiwu Zhong"
                },
                {
                    "authorId": "2113540861",
                    "name": "Chengyu Dong"
                },
                {
                    "authorId": "2116458151",
                    "name": "Zexue He"
                },
                {
                    "authorId": "47006228",
                    "name": "Yujie Lu"
                },
                {
                    "authorId": "2187907974",
                    "name": "William Wang"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                }
            ]
        },
        {
            "paperId": "f547449b45681a08959d1dfac9c118831f79893e",
            "title": "A Targeted Accuracy Diagnostic for Variational Approximations",
            "abstract": "Variational Inference (VI) is an attractive alternative to Markov Chain Monte Carlo (MCMC) due to its computational efficiency in the case of large datasets and/or complex models with high-dimensional parameters. However, evaluating the accuracy of variational approximations remains a challenge. Existing methods characterize the quality of the whole variational distribution, which is almost always poor in realistic applications, even if specific posterior functionals such as the component-wise means or variances are accurate. Hence, these diagnostics are of practical value only in limited circumstances. To address this issue, we propose the TArgeted Diagnostic for Distribution Approximation Accuracy (TADDAA), which uses many short parallel MCMC chains to obtain lower bounds on the error of each posterior functional of interest. We also develop a reliability check for TADDAA to determine when the lower bounds should not be trusted. Numerical experiments validate the practical utility and computational efficiency of our approach on a range of synthetic distributions and real-data examples, including sparse logistic regression and Bayesian neural network models.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2153604285",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "151119660",
                    "name": "Mikolaj Kasprzak"
                },
                {
                    "authorId": "39256289",
                    "name": "Jonathan Huggins"
                }
            ]
        },
        {
            "paperId": "3ced5335b973fa9d4e537376c02c2df22dd5631c",
            "title": "Improving Out-of-Distribution Robustness via Selective Augmentation",
            "abstract": "Machine learning algorithms typically assume that training and test examples are drawn from the same distribution. However, distribution shift is a common problem in real-world applications and can cause models to perform dramatically worse at test time. In this paper, we specifically consider the problems of subpopulation shifts (e.g., imbalanced data) and domain shifts. While prior works often seek to explicitly regularize internal representations or predictors of the model to be domain invariant, we instead aim to learn invariant predictors without restricting the model's internal representations or predictors. This leads to a simple mixup-based technique which learns invariant predictors via selective augmentation called LISA. LISA selectively interpolates samples either with the same labels but different domains or with the same domain but different labels. Empirically, we study the effectiveness of LISA on nine benchmarks ranging from subpopulation shifts to domain shifts, and we find that LISA consistently outperforms other state-of-the-art methods and leads to more invariant predictors. We further analyze a linear setting and theoretically show how LISA leads to a smaller worst-group error.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "18307037",
                    "name": "Huaxiu Yao"
                },
                {
                    "authorId": "2153604285",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2116585805",
                    "name": "Sai Li"
                },
                {
                    "authorId": "10537441",
                    "name": "Linjun Zhang"
                },
                {
                    "authorId": "151253861",
                    "name": "Weixin Liang"
                },
                {
                    "authorId": "145085305",
                    "name": "James Y. Zou"
                },
                {
                    "authorId": "46881670",
                    "name": "Chelsea Finn"
                }
            ]
        },
        {
            "paperId": "5b9e36dd98c0811cdeaed25d3867e03ad7fe0ca3",
            "title": "Differentiable Invariant Causal Discovery",
            "abstract": "Learning causal structure from observational data is a fundamental challenge in machine learning. However, the majority of commonly used differentiable causal discovery methods are non-identifiable, turning this problem into a continuous optimization task prone to data biases. In many real-life situations, data is collected from different environments, in which the functional relations remain consistent across environments, while the distribution of additive noises may vary. This paper proposes Differentiable Invariant Causal Discovery (DICD), utilizing the multi-environment information based on a differentiable framework to avoid learning spurious edges and wrong causal directions. Specifically, DICD aims to discover the environment-invariant causation while removing the environment-dependent correlation. We further formulate the constraint that enforces the target structure equation model to maintain optimal across the environments. Theoretical guarantees for the identifiability of proposed DICD are provided under mild conditions with enough environments. Extensive experiments on synthetic and real-world datasets verify that DICD outperforms state-of-the-art causal discovery methods up to 36% in SHD. Our code will be open-sourced.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2153604285",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2153659066",
                    "name": "An Zhang"
                },
                {
                    "authorId": "98285513",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        },
        {
            "paperId": "6ee75df901a5b8f9e048402bb41cffdd1f6b178f",
            "title": "Controlling Bias Exposure for Fair Interpretable Predictions",
            "abstract": "Recent work on reducing bias in NLP models usually focuses on protecting or isolating information related to a sensitive attribute (like gender or race). However, when sensitive information is semantically entangled with the task information of the input, e.g., gender information is predictive for a profession, a fair trade-off between task performance and bias mitigation is difficult to achieve. Existing approaches perform this trade-off by eliminating bias information from the latent space, lacking control over how much bias is necessarily required to be removed. We argue that a favorable debiasing method should use sensitive information 'fairly', rather than blindly eliminating it (Caliskan et al., 2017; Sun et al., 2019; Bogen et al., 2020). In this work, we provide a novel debiasing algorithm by adjusting the predictive model's belief to (1) ignore the sensitive information if it is not useful for the task; (2) use sensitive information minimally as necessary for the prediction (while also incurring a penalty). Experimental results on two text classification tasks (influenced by gender) and an open-ended generation task (influenced by race) indicate that our model achieves a desirable trade-off between debiasing and task performance along with producing debiased rationales as evidence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116458151",
                    "name": "Zexue He"
                },
                {
                    "authorId": "2153604285",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "35660011",
                    "name": "Julian McAuley"
                },
                {
                    "authorId": "3165738",
                    "name": "Bodhisattwa Prasad Majumder"
                }
            ]
        },
        {
            "paperId": "757ce2ffd64fe6bf6f2b3fdb5bd0db5aa0873357",
            "title": "Learning Robust Recommenders through Cross-Model Agreement",
            "abstract": "Learning from implicit feedback is one of the most common cases in the application of recommender systems. Generally speaking, interacted examples are considered as positive while negative examples are sampled from uninteracted ones. However, noisy examples are prevalent in real-world implicit feedback. A noisy positive example could be interacted but it actually leads to negative user preference. A noisy negative example which is uninteracted because of user unawareness could also denote potential positive user preference. Conventional training methods overlook these noisy examples, leading to sub-optimal recommendations. In this work, we propose a general framework to learn robust recommenders from implicit feedback. Through an empirical study, we find that different models make relatively similar predictions on clean examples which denote the real user preference, while the predictions on noisy examples vary much more across different models. Motivated by this observation, we propose denoising with cross-model agreement (DeCA) which minimizes the KL-divergence between the real user preference distributions parameterized by two recommendation models while maximizing the likelihood of data observation. We instantiate DeCA on four representative recommendation models, empirically demonstrating its superiority over normal training and existing denoising methods. Codes are available at https://github.com/wangyu-ustc/DeCA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153604285",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2113821128",
                    "name": "Xin Xin"
                },
                {
                    "authorId": "3451645",
                    "name": "Zaiqiao Meng"
                },
                {
                    "authorId": "50686770",
                    "name": "J. Jose"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "f2693c6c9a601d5a40a57db2d2e67aebf4b73b87",
            "title": "Meta-learning with an Adaptive Task Scheduler",
            "abstract": "To benefit the learning of a new task, meta-learning has been proposed to transfer a well-generalized meta-model learned from various meta-training tasks. Existing meta-learning algorithms randomly sample meta-training tasks with a uniform probability, under the assumption that tasks are of equal importance. However, it is likely that tasks are detrimental with noise or imbalanced given a limited number of meta-training tasks. To prevent the meta-model from being corrupted by such detrimental tasks or dominated by tasks in the majority, in this paper, we propose an adaptive task scheduler (ATS) for the meta-training process. In ATS, for the first time, we design a neural scheduler to decide which meta-training tasks to use next by predicting the probability being sampled for each candidate task, and train the scheduler to optimize the generalization capacity of the meta-model to unseen tasks. We identify two meta-model-related factors as the input of the neural scheduler, which characterize the difficulty of a candidate task to the meta-model. Theoretically, we show that a scheduler taking the two factors into account improves the meta-training loss and also the optimization landscape. Under the setting of meta-learning with noise and limited budgets, ATS improves the performance on both miniImageNet and a real-world drug discovery benchmark by up to 13% and 18%, respectively, compared to state-of-the-art task schedulers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "18307037",
                    "name": "Huaxiu Yao"
                },
                {
                    "authorId": "2153604285",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2112556840",
                    "name": "Ying Wei"
                },
                {
                    "authorId": "144259957",
                    "name": "P. Zhao"
                },
                {
                    "authorId": "1694826",
                    "name": "M. Mahdavi"
                },
                {
                    "authorId": "1862782",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "46881670",
                    "name": "Chelsea Finn"
                }
            ]
        }
    ]
}