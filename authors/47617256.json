{
    "authorId": "47617256",
    "papers": [
        {
            "paperId": "a157b81f9b121c957ed24d6b9ad57ccd445a2d19",
            "title": "Towards Sequential Counterfactual Learning to Rank",
            "abstract": "Counterfactual evaluation plays a crucial role in learning-to-rank problems, as it addresses the discrepancy between the data logging policy and the policy being evaluated, due to the presence of presentation bias. Existing counterfactual methods, which are based on the empirical risk minimization framework, aim to evaluate the ability of a ranking policy to produce optimal results for a single query using implicit feedback from logged data. In real-world scenarios, however, users often reformulate their queries multiple times until they find what they are looking for and then provide a feedback signal. In such circumstances, current counterfactual approaches cannot assess a policy\u2019s effectiveness in delivering satisfactory results to the user over consecutive queries during a search session. Taking sequential search behavior into account, we propose the first counterfactual estimator for session ranking metrics under sequential position-based models and conduct preliminary experiments to shed light on further research in this direction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267725093",
                    "name": "Tesi Xiao"
                },
                {
                    "authorId": "1681967",
                    "name": "B. Kveton"
                },
                {
                    "authorId": "47617256",
                    "name": "S. Katariya"
                },
                {
                    "authorId": "3366663",
                    "name": "Tanmay Gangwani"
                },
                {
                    "authorId": "51110343",
                    "name": "A. Rangi"
                }
            ]
        },
        {
            "paperId": "adeea51d2851c60bce90bbc9e34540180900188c",
            "title": "Selective Uncertainty Propagation in Offline RL",
            "abstract": "We consider the finite-horizon offline reinforcement learning (RL) setting, and are motivated by the challenge of learning the policy at any step h in dynamic programming (DP) algorithms. To learn this, it is sufficient to evaluate the treatment effect of deviating from the behavioral policy at step h after having optimized the policy for all future steps. Since the policy at any step can affect next-state distributions, the related distributional shift challenges can make this problem far more statistically hard than estimating such treatment effects in the stochastic contextual bandit setting. However, the hardness of many real-world RL instances lies between the two regimes. We develop a flexible and general method called selective uncertainty propagation for confidence interval construction that adapts to the hardness of the associated distribution shift challenges. We show benefits of our approach on toy environments and demonstrate the benefits of these techniques for offline policy learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47318645",
                    "name": "Sanath Kumar Krishnamurthy"
                },
                {
                    "authorId": "3366663",
                    "name": "Tanmay Gangwani"
                },
                {
                    "authorId": "47617256",
                    "name": "S. Katariya"
                },
                {
                    "authorId": "1681967",
                    "name": "B. Kveton"
                },
                {
                    "authorId": "51110343",
                    "name": "A. Rangi"
                }
            ]
        },
        {
            "paperId": "bee752f8f18ad764d84da3d841d77d42f465a458",
            "title": "Logarithmic Bayes Regret Bounds",
            "abstract": "We derive the first finite-time logarithmic regret bounds for Bayesian bandits. For Gaussian bandits, we obtain a $O(c_h \\log^2 n)$ bound, where $c_h$ is a prior-dependent constant. This matches the asymptotic lower bound of Lai (1987). Our proofs mark a technical departure from prior works, and are simple and general. To show generality, we apply our technique to linear bandits. Our bounds shed light on the value of the prior in the Bayesian setting, both in the objective and as a side information given to the learner. They significantly improve the $\\tilde{O}(\\sqrt{n})$ bounds, that despite the existing lower bounds, have become standard in the literature.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2060208885",
                    "name": "Alexia Atsidakou"
                },
                {
                    "authorId": "1681967",
                    "name": "B. Kveton"
                },
                {
                    "authorId": "47617256",
                    "name": "S. Katariya"
                },
                {
                    "authorId": "1764494",
                    "name": "C. Caramanis"
                },
                {
                    "authorId": "1701677",
                    "name": "S. Sanghavi"
                }
            ]
        },
        {
            "paperId": "f52e64dd23ed080ca1a39fddce8e74105a9bc895",
            "title": "Finite-Time Logarithmic Bayes Regret Upper Bounds",
            "abstract": "We derive the first finite-time logarithmic Bayes regret upper bounds for Bayesian bandits. In a multi-armed bandit, we obtain $O(c_\\Delta \\log n)$ and $O(c_h \\log^2 n)$ upper bounds for an upper confidence bound algorithm, where $c_h$ and $c_\\Delta$ are constants depending on the prior distribution and the gaps of bandit instances sampled from it, respectively. The latter bound asymptotically matches the lower bound of Lai (1987). Our proofs are a major technical departure from prior works, while being simple and general. To show the generality of our techniques, we apply them to linear bandits. Our results provide insights on the value of prior in the Bayesian setting, both in the objective and as a side information given to the learner. They significantly improve upon existing $\\tilde{O}(\\sqrt{n})$ bounds, which have become standard in the literature despite the logarithmic lower bound of Lai (1987).",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2060208885",
                    "name": "Alexia Atsidakou"
                },
                {
                    "authorId": "1681967",
                    "name": "B. Kveton"
                },
                {
                    "authorId": "47617256",
                    "name": "S. Katariya"
                },
                {
                    "authorId": "1764494",
                    "name": "C. Caramanis"
                },
                {
                    "authorId": "1701677",
                    "name": "S. Sanghavi"
                }
            ]
        },
        {
            "paperId": "0c9d7e59734cba9aaefd8756b957116318029fd3",
            "title": "Multi-Task Off-Policy Learning from Bandit Feedback",
            "abstract": "Many practical applications, such as recommender systems and learning to rank, involve solving multiple similar tasks. One example is learning of recommendation policies for users with similar movie preferences, where the users may still rank the individual movies slightly differently. Such tasks can be organized in a hierarchy, where similar tasks are related through a shared structure. In this work, we formulate this problem as a contextual off-policy optimization in a hierarchical graphical model from logged bandit feedback. To solve the problem, we propose a hierarchical off-policy optimization algorithm (HierOPO), which estimates the parameters of the hierarchical model and then acts pessimistically with respect to them. We instantiate HierOPO in linear Gaussian models, for which we also provide an efficient implementation and analysis. We prove per-task bounds on the suboptimality of the learned policies, which show a clear improvement over not using the hierarchical model. We also evaluate the policies empirically. Our theoretical and empirical results show a clear advantage of using the hierarchy over solving each task independently.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110597723",
                    "name": "Joey Hong"
                },
                {
                    "authorId": "1681967",
                    "name": "B. Kveton"
                },
                {
                    "authorId": "47617256",
                    "name": "S. Katariya"
                },
                {
                    "authorId": "1771307",
                    "name": "M. Zaheer"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                }
            ]
        },
        {
            "paperId": "108558590429d66537042c01ae7285ea9a95a4e7",
            "title": "Bayesian Fixed-Budget Best-Arm Identification",
            "abstract": "Fixed-budget best-arm identification (BAI) is a bandit problem where the agent maximizes the probability of identifying the optimal arm within a fixed budget of observations. In this work, we study this problem in the Bayesian setting. We propose a Bayesian elimination algorithm and derive an upper bound on its probability of misidentifying the optimal arm. The bound reflects the quality of the prior and is the first distribution-dependent bound in this setting. We prove it using a frequentist-like argument, where we carry the prior through, and then integrate out the bandit instance at the end. We also provide a lower bound on the probability of misidentification in a $2$-armed Bayesian bandit and show that our upper bound (almost) matches it for any budget. Our experiments show that Bayesian elimination is superior to frequentist methods and competitive with the state-of-the-art Bayesian algorithms that have no guarantees in our setting.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2060208885",
                    "name": "Alexia Atsidakou"
                },
                {
                    "authorId": "47617256",
                    "name": "S. Katariya"
                },
                {
                    "authorId": "1701677",
                    "name": "S. Sanghavi"
                },
                {
                    "authorId": "1681967",
                    "name": "B. Kveton"
                }
            ]
        },
        {
            "paperId": "2254e24e716f53dcef722085239b00d5edc15cc8",
            "title": "ANTHEM: Attentive Hyperbolic Entity Model for Product Search",
            "abstract": "Product search is a fundamentally challenging problem due to the large-size of product catalogues and the complexity of extracting semantic information from products. In addition to this, the black-box nature of most search systems also hamper a smooth customer experience. Current approaches in this area utilize lexical and semantic product information to match user queries against products. However, these models lack (i) a hierarchical query representation, (ii) a mechanism to detect and capture inter-entity relationships within a query, and (iii) a query composition method specific to e-commerce domain. To address these challenges, in this paper, we propose an AtteNTive Hyperbolic Entity Model (ANTHEM), a novel attention-based product search framework that models query entities as two-vector hyperboloids, learns inter-entity intersections and utilizes attention to unionize individual entities and inter-entity intersections to predict product matches from the search space. ANTHEM utilizes the first and second vector of hyperboloids to determine the query's semantic position and to tune its surrounding search volume, respectively. The attention networks capture the significance of intra-entity and inter-entity intersections to the final query space. Additionally, we provide a mechanism to comprehend ANTHEM and understand the significance of query entities towards the final resultant products. We evaluate the performance of our model on real data collected from popular e-commerce sites. Our experimental study on the offline data demonstrates compelling evidence of ANTHEM's superior performance over state-of-the-art product search methods with an improvement of more than 10% on various metrics. We also demonstrate the quality of ANTHEM's query encoder using a query matching task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2726036",
                    "name": "Nurendra Choudhary"
                },
                {
                    "authorId": "145850291",
                    "name": "Nikhil S. Rao"
                },
                {
                    "authorId": "47617256",
                    "name": "S. Katariya"
                },
                {
                    "authorId": "2691095",
                    "name": "Karthik Subbian"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "2997bb1ca697cdb658b25ae684b882fe42efd783",
            "title": "Mixed-Effect Thompson Sampling",
            "abstract": "A contextual bandit is a popular framework for online learning to act under uncertainty. In practice, the number of actions is huge and their expected rewards are correlated. In this work, we introduce a general framework for capturing such correlations through a mixed-effect model where actions are related through multiple shared effect parameters. To explore efficiently using this structure, we propose Mixed-Effect Thompson Sampling (meTS) and bound its Bayes regret. The regret bound has two terms, one for learning the action parameters and the other for learning the shared effect parameters. The terms reflect the structure of our model and the quality of priors. Our theoretical findings are validated empirically using both synthetic and real-world problems. We also propose numerous extensions of practical interest. While they do not come with guarantees, they perform well empirically and show the generality of the proposed framework.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2121261507",
                    "name": "Imad Aouali"
                },
                {
                    "authorId": "1681967",
                    "name": "B. Kveton"
                },
                {
                    "authorId": "47617256",
                    "name": "S. Katariya"
                }
            ]
        },
        {
            "paperId": "78e3657bff17a0880c9e4dbd921c28e019ce625c",
            "title": "Deep Hierarchy in Bandits",
            "abstract": "Mean rewards of actions are often correlated. The form of these correlations may be complex and unknown a priori, such as the preferences of a user for recommended products and their categories. To maximize statistical efficiency, it is important to leverage these correlations when learning. We formulate a bandit variant of this problem where the correlations of mean action rewards are represented by a hierarchical Bayesian model with latent variables. Since the hierarchy can have multiple layers, we call it deep. We propose a hierarchical Thompson sampling algorithm (HierTS) for this problem, and show how to implement it efficiently for Gaussian hierarchies. The efficient implementation is possible due to a novel exact hierarchical representation of the posterior, which itself is of independent interest. We use this exact posterior to analyze the Bayes regret of HierTS in Gaussian bandits. Our analysis reflects the structure of the problem, that the regret decreases with the prior width, and also shows that hierarchies reduce the regret by non-constant factors in the number of actions. We confirm these theoretical findings empirically, in both synthetic and real-world experiments.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2110597723",
                    "name": "Joey Hong"
                },
                {
                    "authorId": "1681967",
                    "name": "B. Kveton"
                },
                {
                    "authorId": "47617256",
                    "name": "S. Katariya"
                },
                {
                    "authorId": "1771307",
                    "name": "M. Zaheer"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                }
            ]
        },
        {
            "paperId": "ad831132bbff89a21302f3917fb1985f90127254",
            "title": "Meta-Learning for Simple Regret Minimization",
            "abstract": "We develop a meta-learning framework for simple regret minimization in bandits. In this framework, a learning agent interacts with a sequence of bandit tasks, which are sampled i.i.d. from an unknown prior distribution, and learns its meta-parameters to perform better on future tasks. We propose the first Bayesian and frequentist meta-learning algorithms for this setting. The Bayesian algorithm has access to a prior distribution over the meta-parameters and its meta simple regret over m bandit tasks with horizon n is mere O(m / \u221an). On the other hand, the meta simple regret of the frequentist algorithm is O(n\u221am + m/ \u221an). While its regret is worse, the frequentist algorithm is more general because it does not need a prior distribution over the meta-parameters. It can also be analyzed in more settings. We instantiate our algorithms for several classes of bandit problems. Our algorithms are general and we complement our theory by evaluating them empirically in several environments.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "27585106",
                    "name": "Javad Azizi"
                },
                {
                    "authorId": "1681967",
                    "name": "B. Kveton"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                },
                {
                    "authorId": "47617256",
                    "name": "S. Katariya"
                }
            ]
        }
    ]
}