{
    "authorId": "39598299",
    "papers": [
        {
            "paperId": "1fdc07b3152bd0e3213b8122224fc96f3070f2f5",
            "title": "Improved Neural Network with Spatial Pyramid Pooling and Online Datasets Preprocessing for Underwater Target Detection Based on Side Scan Sonar Imagery",
            "abstract": "Fast and high-accuracy detection of underwater targets based on side scan sonar images has great potential for marine fisheries, underwater security, marine mapping, underwater engineering and other applications. The following problems, however, must be addressed when using low-resolution side scan sonar images for underwater target detection: (1) the detection performance is limited due to the restriction on the input of multi-scale images; (2) the widely used deep learning algorithms have a low detection effect due to their complex convolution layer structures; (3) the detection performance is limited due to insufficient model complexity in training process; and (4) the number of samples is not enough because of the bad dataset preprocessing methods. To solve these problems, an improved neural network for underwater target detection\u2014which is based on side scan sonar images and fully utilizes spatial pyramid pooling and online dataset preprocessing based on the You Look Only Once version three (YOLO V3) algorithm\u2014is proposed. The methodology of the proposed approach is as follows: (1) the AlexNet, GoogleNet, VGGNet and the ResNet networks and an adopted YOLO V3 algorithm were the backbone networks. The structure of the YOLO V3 model is more mature and compact and has higher target detection accuracy and better detection efficiency than the other models; (2) spatial pyramid pooling was added at the end of the convolution layer to improve detection performance. Spatial pyramid pooling breaks the scale restrictions when inputting images to improve feature extraction because spatial pyramid pooling enables the backbone network to learn faster at high accuracy; and (3) online dataset preprocessing based on YOLO V3 with spatial pyramid pooling increases the number of samples and improves the complexity of the model to further improve detection process performance. Three-side scan imagery datasets were used for training and were tested in experiments. The quantitative evaluation using Accuracy, Recall, Precision, mAP and F1-Score metrics indicates that: for the AlexNet, GoogleNet, VGGNet and ResNet algorithms, when spatial pyramid pooling is added to their backbone networks, the average detection accuracy of the three sets of data was improved by 2%, 4%, 2% and 2%, respectively, as compared to their original formulations. Compared with the original YOLO V3 model, the proposed ODP+YOLO V3+SPP underwater target detection algorithm model has improved detection performance through the mAP qualitative evaluation index has increased by 6%, the Precision qualitative evaluation index has increased by 13%, and the detection efficiency has increased by 9.34%. These demonstrate that adding spatial pyramid pooling and online dataset preprocessing can improve the target detection accuracy of these commonly used algorithms. The proposed, improved neural network with spatial pyramid pooling and online dataset preprocessing based on the YOLO V3 method achieves the highest scores for underwater target detection results for sunken ships, fish flocks and seafloor topography, with mAP scores of 98%, 91% and 96% for the above three kinds of datasets, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145465764",
                    "name": "Jinrui Li"
                },
                {
                    "authorId": "2122090299",
                    "name": "Libin Chen"
                },
                {
                    "authorId": "2115733546",
                    "name": "Jian Shen"
                },
                {
                    "authorId": "2018051",
                    "name": "Xiongwu Xiao"
                },
                {
                    "authorId": "2110654417",
                    "name": "Xiaosong Liu"
                },
                {
                    "authorId": "2109230283",
                    "name": "Xin Sun"
                },
                {
                    "authorId": "144129720",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "39598299",
                    "name": "Deren Li"
                }
            ]
        },
        {
            "paperId": "521b8383818ee1050fefd05ae9f9df4ef2ff2175",
            "title": "AFDE-Net: Building Change Detection Using Attention-Based Feature Differential Enhancement for Satellite Imagery",
            "abstract": "Building change detection (BCD) from satellite imagery is critical for monitoring urbanization, managing agricultural land, and updating geospatial databases. However, complex variations in building roofs that resemble the background of their surroundings pose challenges for deep-learning-based change detection methods due to their focus on color and texture. Additionally, downsampling can result in the loss of spatial information, leading to incomplete buildings and irregular output boundaries. To address these challenges, a novel Siamese network called AFDE-Net is proposed, which combines differential image features and attention modules using a learnable parameter. The AFDE-Net employs an ensemble spatial-channel attention fusion (ESCAF) module, along with a deep supervision (DS) module, to mitigate the loss of spatial information and refine deep features in high-dimensional inputs. Besides, we have created a new dataset (EGY-BCD) comprising high-resolution and multitemporal satellite images captured in four urban and coastal areas in Egypt to detect building changes. The EGY-BCD dataset includes images with complex types of change, such as tall and dense buildings with roofs that resemble the background of their surroundings, which is a challenge for deep-learning algorithms. The proposed method outperforms other methods on the EGY-BCD dataset with an overall accuracy (OA) of 94.3%, an F1-score of 88.8%, and an mIoU of 86.6%. The datasets and codes will be released at https://github.com/oshholail/EGY-BCD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219396609",
                    "name": "S. Holail"
                },
                {
                    "authorId": "2067344277",
                    "name": "T. Saleh"
                },
                {
                    "authorId": "2018051",
                    "name": "Xiongwu Xiao"
                },
                {
                    "authorId": "39598299",
                    "name": "Deren Li"
                }
            ]
        },
        {
            "paperId": "86514eb10f93ae046f7aec9fd1a06304de4eadc1",
            "title": "Radiometric and Polarimetric Quality Validation of Gaofen-3 over a Five-Year Operation Period",
            "abstract": "GaoFen-3 was the first Chinese civilian C-band synthetic aperture radar (SAR) satellite, launched in August 2016. The need for monitoring the satellite\u2019s image quality has been boosted by its widespread applications in various fields. The efficient and scientific assessment of the system\u2019s radiometric and polarimetric performance has been essential in its more than five years of service. The authors collected 90 images of the Inner Mongolia calibration site, 888 images of the Amazon rainforest, and 39,929 images of the Chinese mainland from 2017 to 2021. This was achieved whilst covering the leading imaging modes, such as the spotlight mode, stripmap mode, ultra-fine mode, wave imaging mode, etc. In this study, we derive a framework that incorporates the man-made corner reflectors (CRs) in Mongolia, the traditional Amazon rainforest datasets, and even the long-strip data in the Chinese mainland (known as CRAS) for the purposes of GaoFen-3 radiometric quality analysis and polarimetric validation over its five years of operation. Polarimetric calibration without recourse to the CRs is utilized to measure the polarimetric distortions regardless of the region, and thus requires a higher calibration accuracy for the GaoFen-3 polarimetric monitoring task. Consequently, the modified Quegan method is developed by relaxing the target azimuth symmetry constraint with the Amazon forest datasets. The experiments based on the CRAS demonstrate that the main radiometric characteristics could reach the international level, with an estimated noise-equivalent sigma zero of approximately \u221230 dB, a radiometric resolution that is better than 2.9 dB, and a single-imagery relative radiation accuracy that is better than 0.51 dB. For polarimetric validation, the modified Quegan method was utilized to measure the crosstalk for quad-pol products to ensure that it was than \u221240 dB. Meanwhile, non-negligible channel imbalance errors were found in the QPSII and WAV modes, and they were effectively well-calibrated with strip estimators to satisfy the system design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145453255",
                    "name": "Le Yang"
                },
                {
                    "authorId": "34834923",
                    "name": "Lei Shi"
                },
                {
                    "authorId": "2153198679",
                    "name": "Weidong Sun"
                },
                {
                    "authorId": "2118580330",
                    "name": "Jie Yang"
                },
                {
                    "authorId": "1709508",
                    "name": "Pingxiang Li"
                },
                {
                    "authorId": "39598299",
                    "name": "Deren Li"
                },
                {
                    "authorId": "8954444",
                    "name": "Shanwei Liu"
                },
                {
                    "authorId": "48096594",
                    "name": "Lingli Zhao"
                }
            ]
        },
        {
            "paperId": "26d51dfd19a9c038a1b1b0993207f41051edb291",
            "title": "Integrated Imaging Mission Planning Modeling Method for Multi-Type Targets for Super-Agile Earth Observation Satellite",
            "abstract": "A unified description for the imaging mission of different types of targets is lacking, making overall optimization of imaging missions of complex multitype targets (point, curve, and area) within a single pass difficult when using traditional satellite imaging mission planning. We propose an imaging mission planning modeling method based on the optimal mission decomposition/merge (MD/M) strategy for imaging missions of multitype targets within a single pass of SA-EOSs. This method transforms the imaging missions of multitype targets into an atomic mission set that can be described uniformly for integrated optimization scheduling. First, an optimal MD/M strategy was proposed for different types of target imaging (point, curve, and area) based on the characteristics of dynamic imaging. A mission optimization model was then constructed, with imaging coverage benefit and mission execution time as the objective functions, and an improved particle swarm optimization algorithm was used to solve the model. Finally, the proposed optimal MD/M strategy and mission planning modeling method were tested by setting up seven groups of imaging mission simulation experiments with different multi-type target combinations. The result showed that the proposed \u201ctwo-stage\u201d optimization method achieved integrated optimization of multimode imaging missions in dynamic imaging. The proposed optimal MD/M strategy can be applied in integrated modeling of imaging missions of multitype targets. Moreover, the imaging mission planning model constructed with time-stamped strips as atomic tasks can perform efficient integrated planning of imaging mission of complex multitype targets and ensures the effective performance of SA-EOSs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1517974354",
                    "name": "Zezhong Lu"
                },
                {
                    "authorId": "2111113703",
                    "name": "Xin Shen"
                },
                {
                    "authorId": "39598299",
                    "name": "Deren Li"
                },
                {
                    "authorId": "2109279294",
                    "name": "Yaxin Chen"
                }
            ]
        },
        {
            "paperId": "348f998da2bb6f286730337fff14a1055ef9a14c",
            "title": "A Survey on Visual Navigation and Positioning for Autonomous UUVs",
            "abstract": "Autonomous navigation and positioning are key to the successful performance of unmanned underwater vehicles (UUVs) in environmental monitoring, oceanographic mapping, and critical marine infrastructure inspections in the sea. Cameras have been at the center of attention as an underwater sensor due to the advantages of low costs and rich content information in high visibility ocean waters, especially in the fields of underwater target recognition, navigation, and positioning. This paper is not only a literature overview of the vision-based navigation and positioning of autonomous UUVs but also critically evaluates the methodologies which have been developed and that directly affect such UUVs. In this paper, the visual navigation and positioning algorithms are divided into two categories: geometry-based methods and deep learning-based. In this paper, the two types of SOTA methods are compared experimentally and quantitatively using a public underwater dataset and their potentials and shortcomings are analyzed, providing a panoramic theoretical reference and technical scheme comparison for UUV visual navigation and positioning research in the highly dynamic and three-dimensional ocean environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1490728402",
                    "name": "J. Qin"
                },
                {
                    "authorId": "144168273",
                    "name": "Ming Li"
                },
                {
                    "authorId": "39598299",
                    "name": "Deren Li"
                },
                {
                    "authorId": "1490719214",
                    "name": "J. Zhong"
                },
                {
                    "authorId": "2119301417",
                    "name": "Ke Yang"
                }
            ]
        },
        {
            "paperId": "45fbde2ee30962599beb401ac895b2474f0fead3",
            "title": "Semantic Change Detection Based on a New Chinese Satellite Dataset and a Deep Conditional Random Field Framework",
            "abstract": "In this paper, a new semantic change detection (CD) dataset based on Chinese Gaofen-2 (GF-2) satellite images with high spatial resolution (HSR) namely Wuhan Urban Semantic Understanding (WUSU) dataset is built up and a CD framework combining binary and semantic CD tasks based on deep learning and a conditional random field model (SDCRF) is proposed. Existing CD datasets mostly focus on \u201cchange/no change\u201d. Traditional CD methods pay attention only on either of the binary CD task or the semantic CD task. Although there are methods to handle both tasks simultaneously but they ignore the inconsistency between the two tasks. In the SDCRF framework, any state-of-the-art feature extraction model can be used to extract the class and change probabilities as the unary potential of a fully connected conditional random field (FC-CRF) model which is adopted as a post-processing to enhance the location information of deep networks and reduce outlier noise.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2037792915",
                    "name": "Sunan Shi"
                },
                {
                    "authorId": "2798207",
                    "name": "Yanfei Zhong"
                },
                {
                    "authorId": "2108068894",
                    "name": "Yinhe Liu"
                },
                {
                    "authorId": "2109764075",
                    "name": "Jue Wang"
                },
                {
                    "authorId": "39598299",
                    "name": "Deren Li"
                }
            ]
        },
        {
            "paperId": "6b48cdc307ec0ab3282f676c47c4af5b7a7527c4",
            "title": "Aerial Scene Parsing: From Tile-level Scene Classification to Pixel-wise Semantic Labeling",
            "abstract": "Given an aerial image, aerial scene parsing (ASP) targets to interpret the semantic structure of the image content, e.g., by assigning a semantic label to every pixel of the image. With the popularization of data-driven methods, the past decades have witnessed promising progress on ASP by approaching the problem with the schemes of tile-level scene classification or segmentation-based image analysis, when using high-resolution aerial images. However, the former scheme often produces results with tile-wise boundaries, while the latter one needs to handle the complex modeling process from pixels to semantics, which often requires large-scale and well-annotated image samples with pixel-wise semantic labels. In this paper, we address these issues in ASP, with perspectives from tile-level scene classification to pixel-wise semantic labeling. Specifically, we first revisit aerial image interpretation by a literature review. We then present a large-scale scene classification dataset that contains one million aerial images termed Million-AID. With the presented dataset, we also report benchmarking experiments using classical convolutional neural networks (CNNs). Finally, we perform ASP by unifying the tile-level scene classification and object-based image analysis to achieve pixel-wise semantic labeling. Intensive experiments show that Million-AID is a challenging yet useful dataset, which can serve as a benchmark for evaluating newly developed algorithms. When transferring knowledge from Million-AID, fine-tuning CNN models pretrained on Million-AID perform consistently better than those pretrained ImageNet for aerial scene classification. Moreover, our designed hierarchical multi-task learning method achieves the state-of-the-art pixel-wise classification on the challenging GID, bridging the tile-level scene classification toward pixel-wise semantic labeling for aerial image interpretation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072952146",
                    "name": "Yang Long"
                },
                {
                    "authorId": "39943835",
                    "name": "Gui-Song Xia"
                },
                {
                    "authorId": "1720539",
                    "name": "L. Zhang"
                },
                {
                    "authorId": "2064868033",
                    "name": "Gong Cheng"
                },
                {
                    "authorId": "39598299",
                    "name": "Deren Li"
                }
            ]
        },
        {
            "paperId": "80a89961c479514fcd2a6af7a19d4a5f4e739a4f",
            "title": "Population, GDP, and Carbon Emissions as Revealed by SNPP-VIIRS Nighttime Light Data in China With Different Scales",
            "abstract": "Satellite-based artificial nighttime brightness observations are typically considered proxy measures of socioeconomic indicators at large scales, such as population, gross domestic product (GDP), and carbon emissions. However, few studies have explored and compared the correlations between Suomi National Polar Orbiting Partnership-Visible Infrared Imaging Radiometer Suite (SNPP-VIIRS) nighttime light (NTL) data and socioeconomic indicators from administrative scale to grid scale and further analyzed the potential mechanisms for the dissimilar correlations at different grid scales. Using the regression model, dissimilarity index, and relief amplitude, the quantitative relationship and potential influence mechanism across different scales were investigated in this letter. Results show that the finer the scale is, the lower the correlations between total NTLs and socioeconomic indicators when comparing 1 km, town, and county scales. The determination coefficient (<inline-formula> <tex-math notation=\"LaTeX\">${R} ^{\\mathbf {2}}$ </tex-math></inline-formula>) values of the NTL-socioeconomic indicator correlations increase sharply with the increase of grid scale at 1\u201310 km scale. The <inline-formula> <tex-math notation=\"LaTeX\">${R} ^{\\mathbf {2}}$ </tex-math></inline-formula> values increase volatilely between 10 and 30 km but are relatively stable above 30 km. The differences in <inline-formula> <tex-math notation=\"LaTeX\">${R} ^{\\mathbf {2}}$ </tex-math></inline-formula> values may be attributed to the diversity and distribution balance of industrial types and relief amplitude at different scales. This letter provides new insights into estimating and predicting population, GDP, and carbon emissions by using SNPP-VIIRS data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21413452",
                    "name": "Kaifang Shi"
                },
                {
                    "authorId": "2145019424",
                    "name": "Yizhen Wu"
                },
                {
                    "authorId": "39598299",
                    "name": "Deren Li"
                },
                {
                    "authorId": "2116226619",
                    "name": "Xi Li"
                }
            ]
        },
        {
            "paperId": "94bad93aab64561ce02ea0f1bf9077f5dba9973e",
            "title": "Transformer-Driven Semantic Relation Inference for Multilabel Classification of High-Resolution Remote Sensing Images",
            "abstract": "It is hard to use a single label to describe an image for the complexity of remote sensing scenes. Thus, it is a more general and practical choice to use multilabel image classification for high-resolution remote sensing (HRS) images. How to construct the relation between categories is a vital problem for multilabel classification. Some researchers use the recurrent neural network (RNN) or long short-term memory (LSTM) to exploit label relations over the last years. However, the RNN or LSTM could model such category dependence in a chain propagation manner. The performance of the RNN/LSTM might be questioned when a specific category is improperly inferred. To address this, we propose a novel HRS image multilabel classification network, transformer-driven semantic relation inference network. The network comprises two modules: semantic sensitive module (SSM) and semantic relation-building module (SRBM). The SSM locates the semantic attentional regions in the features extracted by a deep convolutional neural network and generates a discriminative content-aware category representation (CACR). The SRBM uses label relation inference from outputs of the SSM to predict final results. The characteristic of the proposed method is that it can extract semantic attentional regions relevant to the category and generate a discriminative CACR and natural and interpretable reasoning about label relations. Experiments were performed on the public UCM multilabel and MLRSNet datasets. Quantitative and qualitative analyses on state-of-the-art multilabel benchmarks proved that the proposed method could effectively locate semantic regions and build relationships between categories with better robustness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "92272628",
                    "name": "Xiaowei Tan"
                },
                {
                    "authorId": "51308516",
                    "name": "Zhifeng Xiao"
                },
                {
                    "authorId": "2155238520",
                    "name": "Jianjun Zhu"
                },
                {
                    "authorId": "49959423",
                    "name": "Qiao Wan"
                },
                {
                    "authorId": "2148897909",
                    "name": "Kai Wang"
                },
                {
                    "authorId": "39598299",
                    "name": "Deren Li"
                }
            ]
        },
        {
            "paperId": "a889004576b9abc9e1f86d904f191e3c57db916e",
            "title": "Oil Spill Contextual and Boundary-Supervised Detection Network Based on Marine SAR Images",
            "abstract": "Oil spills have caused serious harm to the marine environment. Remote sensing technology is one of the important tools for marine environment monitoring. Synthetic aperture radar (SAR) has become an important technology for detecting marine pollution. Identifying dark spots is essential for oil spill detection based on SAR images. Dark spots\u2019 detection can be achieved using image segmentation techniques. However, natural phenomena, such as waves and currents, can also cause dark spots, resulting in consistently uneven intensity, high noise, and blurred boundaries in oil spill images. In addition, existing oil spill detection models often perform well for large targets but have poor detection accuracy for small targets. To solve the above problems, the oil spill contextual and boundary-supervised detection network (CBD-Net) is proposed to extract refined oil spill regions by fusing multiscale features. To improve the internal consistency of oil spill regions, the spatial and channel squeeze excitation (scSE) block is introduced. In CBD-Net, boundary details are enhanced with optimized edge supervision. In addition, a manually labeled dataset is proposed, Deep-SAR Oil Spill (SOS) dataset, aiming to solve the problem of insufficient existing oil spill detection dataset. Experimental results demonstrate that CBD-Net outperforms other comparative models and is able to extract robust and accurate oil spill regions from complex SAR images. The highest mIoU of 83.42% and the highest F1 score of 87.87% were achieved on the SOS dataset. The CBD-Net model proposed in this article can play a guiding role in the marine oil spill decision support system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "147865102",
                    "name": "Qiqi Zhu"
                },
                {
                    "authorId": null,
                    "name": "Yanan Zhang"
                },
                {
                    "authorId": "2118275024",
                    "name": "Ziqi Li"
                },
                {
                    "authorId": "2152614161",
                    "name": "Xiaorui Yan"
                },
                {
                    "authorId": "35796399",
                    "name": "Qingfeng Guan"
                },
                {
                    "authorId": "2798207",
                    "name": "Yanfei Zhong"
                },
                {
                    "authorId": "9802604",
                    "name": "Liangpei Zhang"
                },
                {
                    "authorId": "39598299",
                    "name": "Deren Li"
                }
            ]
        }
    ]
}