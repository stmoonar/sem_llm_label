{
    "authorId": "145434248",
    "papers": [
        {
            "paperId": "2ceb3f9f4333ad39a176c19d66546c4dd7c161a1",
            "title": "Report on the 46th European Conference on Information Retrieval (ECIR 2024)",
            "abstract": "\n The 46\n th\n European Conference on Information Retrieval (ECIR 2024) was held in Glasgow, Scotland, during 24\n th\n -28\n th\n March 2024. The conference brought together over four hundred researchers from the UK, Europe and abroad. ECIR 2024 was a fully in-person conference with a total of 417 attendees, the largest number of in-person attendees of any ECIR. The conference received over 700 submissions, not including submissions to the workshops (280 Full Paper and 184 Short paper submissions). ECIR 2024 introduced a number of novelties, including a new Findings track, an IR4Good track, a new innovation called the \"Collab-athon\" to foster collaborations within the community, and the Keith van Rijsbergen Award to recognise researchers who have made significant contributions in using theory to advance the field of information retrieval. This report details the conference programme and events.\n \n \n Date\n : 24--28 March 2024.\n \n \n Website\n : https://www.ecir2024.org.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38657905",
                    "name": "Graham Mcdonald"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "1698205",
                    "name": "I. Ounis"
                }
            ]
        },
        {
            "paperId": "32d8b4794691e972a0d939d5f8faf2617b2955dd",
            "title": "Personalised Multi-Modal Interactive Recommendation with Hierarchical State Representations",
            "abstract": "Multi-modal interactive recommender systems (MMIRS) can effectively guide users towards their desired items through multi-turn interactions by leveraging the users\u2019 real-time feedback (in the form of natural-language critiques) on previously recommended items (such as images of fashion products). In this scenario, the users\u2019 preferences can be expressed by both the users\u2019 past interests from their historical interactions and their current needs from the real-time interactions. However, it is typically challenging to make satisfactory personalised recommendations across multi-turn interactions due to the difficulty in balancing the users\u2019 past interests and the current needs for generating the users\u2019 state (i.e. current preferences) representations over time. On the other hand, hierarchical reinforcement learning has been successfully applied in various fields by decomposing a complex task into a hierarchy of more easily addressed subtasks. In this journal article, we propose a novel personalised multi-modal interactive recommendation model (PMMIR) using hierarchical reinforcement learning to more effectively incorporate the users\u2019 preferences from both their past and real-time interactions. In particular, PMMIR decomposes the personalised interactive recommendation process into a sequence of two subtasks with hierarchical state representations: a first subtask where a history encoder learns the users\u2019 past interests with the hidden states of history for providing personalised initial recommendations, and a second subtask where a state tracker estimates the current needs with the real-time estimated states for updating the subsequent recommendations. The history encoder and the state tracker are jointly optimised with a single objective by maximising the users\u2019 future satisfaction with the recommendations. Following previous work, we train and evaluate our PMMIR model using a user simulator that can generate natural-language critiques about the recommendations as a surrogate for real human users. Experiments conducted on two derived fashion datasets from two well-known public datasets demonstrate that our proposed PMMIR model yields significant improvements in comparison to the existing state-of-the-art baseline models. The datasets and code are publicly available at: https://github.com/yashonwu/pmmir.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1924079377",
                    "name": "Yaxiong Wu"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "1698205",
                    "name": "I. Ounis"
                }
            ]
        },
        {
            "paperId": "24fcd29e28a27efb06647ae42a64e50c11d49840",
            "title": "Effective Contrastive Weighting for Dense Query Expansion",
            "abstract": "Verbatim queries submitted to search engines often do not sufficiently describe the user\u2019s search intent. Pseudo-relevance feedback (PRF) techniques, which modify a query\u2019srepresentation using the top-ranked documents, have been shown to overcome such inadequacies and improve retrieval effectiveness for both lexical methods (e.g., BM25) and dense methods (e.g., ANCE, ColBERT). For instance, the recent ColBERT-PRF approach heuristically chooses new embeddings to add to the query representation using the inverse document frequency (IDF) of the underlying tokens. However, this heuristic potentially ignores the valuable context encoded by the embeddings. In this work, we present a contrastive solution that learns to select the most useful embeddings for expansion. More specifically, a deep language model-based contrastive weighting model, called CWPRF, is trained to learn to discriminate between relevant and non-relevant documents for semantic search. Our experimental results show that our contrastive weighting model can aid to select useful expansion embeddings and outperform various baselines. In particular, CWPRF can improve nDCG@10 by upto to 4.1% compared to an existing PRF approach for ColBERT while maintaining its efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144550968",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "1698205",
                    "name": "I. Ounis"
                }
            ]
        },
        {
            "paperId": "393542144c73ebe34a339fa4bcdfa7c4f82e5a3f",
            "title": "Goal-Oriented Multi-Modal Interactive Recommendation with Verbal and Non-Verbal Relevance Feedback",
            "abstract": "Interactive recommendation enables users to provide verbal and non-verbal relevance feedback (such as natural-language critiques and likes/dislikes) when viewing a ranked list of recommendations (such as images of fashion products), in order to guide the recommender system towards their desired items (i.e. goals) across multiple interaction turns. Such a multi-modal interactive recommendation (MMIR) task has been successfully formulated with deep reinforcement learning (DRL) algorithms by simulating the interactions between an environment (i.e. a user) and an agent (i.e. a recommender system). However, it is typically challenging and unstable to optimise the agent to improve the recommendation quality associated with implicit learning of multi-modal representations in an end-to-end fashion in DRL. This is known as the coupling of policy optimisation and representation learning. To address this coupling issue, we propose a novel goal-oriented multi-modal interactive recommendation model (GOMMIR) that uses both verbal and non-verbal relevance feedback to effectively incorporate the users\u2019 preferences over time. Specifically, our GOMMIR model employs a multi-task learning approach to explicitly learn the multi-modal representations using a multi-modal composition network when optimising the recommendation agent. Moreover, we formulate the MMIR task using goal-oriented reinforcement learning and enhance the optimisation objective by leveraging non-verbal relevance feedback for hard negative sampling and providing extra goal-oriented rewards to effectively optimise the recommendation agent. Following previous work, we train and evaluate our GOMMIR model by using user simulators that can generate natural-language feedback about the recommendations as a surrogate for real human users. Experiments conducted on four well-known fashion datasets demonstrate that our proposed GOMMIR model yields significant improvements in comparison to the existing state-of-the-art baseline models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1924079377",
                    "name": "Yaxiong Wu"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "1698205",
                    "name": "I. Ounis"
                }
            ]
        },
        {
            "paperId": "62977d919255c6e818461927fb26fafe4c53d8d4",
            "title": "Multi-Task Learning of Query Generation and Classification for Generative Conversational Question Rewriting",
            "abstract": "In conversational search settings, users ask questions and receive answers as part of a conversation. The ambiguity in the questions is a common challenge, which can be effectively addressed by leveraging contextual information from the conversation history. In this context, determining topic continuity and reformulating questions into well-defined queries are crucial tasks. Previous approaches have typically addressed these tasks either as a classification task in the case of topic continuity or as a text generation task for question reformulation. However, no prior work has combined both tasks to effectively identify ambiguous questions as part of a conversation. In this paper, we propose a Multi-Task Learning (MTL) approach that uses a text generation model for both question rewriting and classification. Our models, based on BART and T5, are trained to rewrite conversational questions and identify follow-up questions simultaneously. We evaluate our approach on multiple test sets and demonstrate that it out-performs single-task learning baselines on the three LIF test sets, with statistically significant improvements ranging from +3.5% to +10.5% in terms of F1 and Micro-F1 scores. We also show that our approach outperforms single-task question rewriting models in passage retrieval on a large OR-QuAC test set.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2533998",
                    "name": "S. Kongyoung"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "1698205",
                    "name": "I. Ounis"
                }
            ]
        },
        {
            "paperId": "6b913e84066186f57e0210728872159cc47f52ef",
            "title": "Generative Query Reformulation for Effective Adhoc Search",
            "abstract": "Performing automatic reformulations of a user's query is a popular paradigm used in information retrieval (IR) for improving effectiveness -- as exemplified by the pseudo-relevance feedback approaches, which expand the query in order to alleviate the vocabulary mismatch problem. Recent advancements in generative language models have demonstrated their ability in generating responses that are relevant to a given prompt. In light of this success, we seek to study the capacity of such models to perform query reformulation and how they compare with long-standing query reformulation methods that use pseudo-relevance feedback. In particular, we investigate two representative query reformulation frameworks, GenQR and GenPRF. GenQR directly reformulates the user's input query, while GenPRF provides additional context for the query by making use of pseudo-relevance feedback information. For each reformulation method, we leverage different techniques, including fine-tuning and direct prompting, to harness the knowledge of language models. The reformulated queries produced by the generative models are demonstrated to markedly benefit the effectiveness of a state-of-the-art retrieval pipeline on four TREC test collections (varying from TREC 2004 Robust to the TREC 2019 Deep Learning). Furthermore, our results indicate that our studied generative models can outperform various statistical query expansion approaches while remaining comparable to other existing complex neural query reformulation models, with the added benefit of being simpler to implement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144550968",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "1698205",
                    "name": "I. Ounis"
                }
            ]
        },
        {
            "paperId": "7b2e78d4e7986914ae633fa6b30e73bad8a2b2c1",
            "title": "Doc2Query-: When Less is More",
            "abstract": "Doc2Query -- the process of expanding the content of a document before indexing using a sequence-to-sequence model -- has emerged as a prominent technique for improving the first-stage retrieval effectiveness of search engines. However, sequence-to-sequence models are known to be prone to\"hallucinating\"content that is not present in the source text. We argue that Doc2Query is indeed prone to hallucination, which ultimately harms retrieval effectiveness and inflates the index size. In this work, we explore techniques for filtering out these harmful queries prior to indexing. We find that using a relevance model to remove poor-quality queries can improve the retrieval effectiveness of Doc2Query by up to 16%, while simultaneously reducing mean query execution time by 23% and cutting the index size by 33%. We release the code, data, and a live demonstration to facilitate reproduction and further exploration at https://github.com/terrierteam/pyterrier_doc2query.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153377082",
                    "name": "Mitko Gospodinov"
                },
                {
                    "authorId": "22214396",
                    "name": "Sean MacAvaney"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                }
            ]
        },
        {
            "paperId": "8282c6b141c335fc144818f20d86e68800691a01",
            "title": "Reproducibility, Replicability, and Insights into Dense Multi-Representation Retrieval Models: from ColBERT to Col*",
            "abstract": "Dense multi-representation retrieval models, exemplified as ColBERT, estimate the relevance between a query and a document based on the similarity of their contextualised token-level embeddings. Indeed, by using contextualised token embeddings, dense retrieval, conducted as either exact or semantic matches, can result in increased effectiveness for both in-domain and out-of-domain retrieval tasks, indicating that it is an important model to study. However, the exact role that these semantic matches play is not yet well investigated. For instance, although tokenisation is one of the crucial design choices for various pretrained language models, its impact on the matching behaviour has not been examined in detail. In this work, we inspect the reproducibility and replicability of the contextualised late interaction mechanism by extending ColBERT to Col\u22c6 which implements the late interaction mechanism across various pretrained models and different types of tokenisers. As different tokenisation methods can directly impact the matching behaviour within the late interaction mechanism, we study the nature of matches occurring in different Col\u22c6 models, and further quantify the contribution of lexical and semantic matching on retrieval effectiveness. Overall, our experiments successfully reproduce the performance of ColBERT on various query sets, and replicate the late interaction mechanism upon different pretrained models with different tokenisers. Moreover, our experimental results yield new insights, such as: (i) semantic matching behaviour varies across different tokenisers; (ii) more specifically, high-frequency tokens tend to perform semantic matching than other token families; (iii) late interaction mechanism benefits more from lexical matching than semantic matching; (iv) special tokens, such as [CLS], play a very important role in late interaction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144550968",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "2783910",
                    "name": "N. Tonellotto"
                },
                {
                    "authorId": "1698205",
                    "name": "I. Ounis"
                }
            ]
        },
        {
            "paperId": "99335b0ca039ea2f59ccbe8bd182acb3e5c705d6",
            "title": "A Social-aware Gaussian Pre-trained Model for Effective Cold-start Recommendation",
            "abstract": "The use of pre-training is an emerging technique to enhance a neural model's performance, which has been shown to be effective for many neural language models such as BERT. This technique has also been used to enhance the performance of recommender systems. In such recommender systems, pre-training models are used to learn a better initialisation for both users and items. However, recent existing pre-trained recommender systems tend to only incorporate the user interaction data at the pre-training stage, making it difficult to deliver good recommendations, especially when the interaction data is sparse. To alleviate this common data sparsity issue, we propose to pre-train the recommendation model not only with the interaction data but also with other available information such as the social relations among users, thereby providing the recommender system with a better initialisation compared with solely relying on the user interaction data. We propose a novel recommendation model, the Social-aware Gaussian Pre-trained model (SGP), which encodes the user social relations and interaction data at the pre-training stage in a Graph Neural Network (GNN). Afterwards, in the subsequent fine-tuning stage, our SGP model adopts a Gaussian Mixture Model (GMM) to factorise these pre-trained embeddings for further training, thereby benefiting the cold-start users from these pre-built social relations. Our extensive experiments on three public datasets show that, in comparison to 16 competitive baselines, our SGP model significantly outperforms the best baseline by upto 7.7% in terms of NDCG@10. In addition, we show that SGP permits to effectively alleviate the cold-start problem, especially when users newly register to the system through their friends' suggestions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108636639",
                    "name": "Siwei Liu"
                },
                {
                    "authorId": "2268631177",
                    "name": "Xi Wang"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "1698205",
                    "name": "I. Ounis"
                }
            ]
        },
        {
            "paperId": "9c2413c5a10de6eaad0f86225ff4e6cb72bb6e1b",
            "title": "Large Multi-modal Encoders for Recommendation",
            "abstract": "In recent years, the rapid growth of online multimedia services, such as e-commerce platforms, has necessitated the development of personalised recommendation approaches that can encode diverse content about each item. Indeed, modern multi-modal recommender systems exploit diverse features obtained from raw images and item descriptions to enhance the recommendation performance. However, the existing multi-modal recommenders primarily depend on the features extracted individually from different media through pre-trained modality-specific encoders, and exhibit only shallow alignments between different modalities - limiting these systems' ability to capture the underlying relationships between the modalities. In this paper, we investigate the usage of large multi-modal encoders within the specific context of recommender systems, as these have previously demonstrated state-of-the-art effectiveness when ranking items across various domains. Specifically, we tailor two state-of-the-art multi-modal encoders (CLIP and VLMo) for recommendation tasks using a range of strategies, including the exploration of pre-trained and fine-tuned encoders, as well as the assessment of the end-to-end training of these encoders. We demonstrate that pre-trained large multi-modal encoders can generate more aligned and effective user/item representations compared to existing modality-specific encoders across three multi-modal recommendation datasets. Furthermore, we show that fine-tuning these large multi-modal encoders with recommendation datasets leads to an enhanced recommendation performance. In terms of different training paradigms, our experiments highlight the essential role of the end-to-end training of large multi-modal encoders in multi-modal recommendation systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30678834",
                    "name": "Zixuan Yi"
                },
                {
                    "authorId": "114180634",
                    "name": "Zijun Long"
                },
                {
                    "authorId": "1698205",
                    "name": "I. Ounis"
                },
                {
                    "authorId": "145434248",
                    "name": "Craig Macdonald"
                },
                {
                    "authorId": "1740893",
                    "name": "R. McCreadie"
                }
            ]
        }
    ]
}