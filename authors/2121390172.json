{
    "authorId": "2121390172",
    "papers": [
        {
            "paperId": "0130f2fab75c30c26635be32fdafa6a03c35af6a",
            "title": "GraphStorm: all-in-one graph machine learning framework for industry applications",
            "abstract": "Graph machine learning (GML) is effective in many business applications. However, making GML easy to use and applicable to industry applications with massive datasets remain challenging. We developed GraphStorm, which provides an end-to-end solution for scalable graph construction, graph model training and inference. GraphStorm has the following desirable properties: (a) Easy to use: it can perform graph construction and model training and inference with just a single command; (b) Expert-friendly: GraphStorm contains many advanced GML modeling techniques to handle complex graph data and improve model performance; (c) Scalable: every component in GraphStorm can operate on graphs with billions of nodes and can scale model training and inference to different hardware without changing any code. GraphStorm has been used and deployed for over a dozen billion-scale industry applications after its release in May 2023. It is open-sourced in Github: https://github.com/awslabs/graphstorm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283934850",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "2284037254",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "2299062897",
                    "name": "Qi Zhu"
                },
                {
                    "authorId": "2284069643",
                    "name": "Jian Zhang"
                },
                {
                    "authorId": "1812965",
                    "name": "Theodore Vasiloudis"
                },
                {
                    "authorId": "2146354747",
                    "name": "Runjie Ma"
                },
                {
                    "authorId": "2280741034",
                    "name": "Houyu Zhang"
                },
                {
                    "authorId": "2255392614",
                    "name": "Zichen Wang"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "10429687",
                    "name": "Israt Nisa"
                },
                {
                    "authorId": "3125115",
                    "name": "Alejandro Mottini"
                },
                {
                    "authorId": "2305618086",
                    "name": "Qingjun Cui"
                },
                {
                    "authorId": "145344187",
                    "name": "H. Rangwala"
                },
                {
                    "authorId": "2305616323",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "2263543517",
                    "name": "Christos Faloutsos"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "72adedc0d50fc50ee4f1bcc1ad70b72d6b1a573f",
            "title": "NetInfoF Framework: Measuring and Exploiting Network Usable Information",
            "abstract": "Given a node-attributed graph, and a graph task (link prediction or node classification), can we tell if a graph neural network (GNN) will perform well? More specifically, do the graph structure and the node features carry enough usable information for the task? Our goals are (1) to develop a fast tool to measure how much information is in the graph structure and in the node features, and (2) to exploit the information to solve the task, if there is enough. We propose NetInfoF, a framework including NetInfoF_Probe and NetInfoF_Act, for the measurement and the exploitation of network usable information (NUI), respectively. Given a graph data, NetInfoF_Probe measures NUI without any model training, and NetInfoF_Act solves link prediction and node classification, while two modules share the same backbone. In summary, NetInfoF has following notable advantages: (a) General, handling both link prediction and node classification; (b) Principled, with theoretical guarantee and closed-form solution; (c) Effective, thanks to the proposed adjustment to node similarity; (d) Scalable, scaling linearly with the input size. In our carefully designed synthetic datasets, NetInfoF correctly identifies the ground truth of NUI and is the only method being robust to all graph scenarios. Applied on real-world datasets, NetInfoF wins in 11 out of 12 times on link prediction compared to general GNN baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151121638",
                    "name": "Meng-Chieh Lee"
                },
                {
                    "authorId": "2284130224",
                    "name": "Haiyang Yu"
                },
                {
                    "authorId": "2284069643",
                    "name": "Jian Zhang"
                },
                {
                    "authorId": "40043851",
                    "name": "V. Ioannidis"
                },
                {
                    "authorId": "2284037254",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "2283934850",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "2256724188",
                    "name": "Christos Faloutsos"
                }
            ]
        },
        {
            "paperId": "a6d2eb2e7bf518e93595e80e61127176f36204f5",
            "title": "Hierarchical Compression of Text-Rich Graphs via Large Language Models",
            "abstract": "Text-rich graphs, prevalent in data mining contexts like e-commerce and academic graphs, consist of nodes with textual features linked by various relations. Traditional graph machine learning models, such as Graph Neural Networks (GNNs), excel in encoding the graph structural information, but have limited capability in handling rich text on graph nodes. Large Language Models (LLMs), noted for their superior text understanding abilities, offer a solution for processing the text in graphs but face integration challenges due to their limitation for encoding graph structures and their computational complexities when dealing with extensive text in large neighborhoods of interconnected nodes. This paper introduces ``Hierarchical Compression'' (HiCom), a novel method to align the capabilities of LLMs with the structure of text-rich graphs. HiCom processes text in a node's neighborhood in a structured manner by organizing the extensive textual information into a more manageable hierarchy and compressing node text step by step. Therefore, HiCom not only preserves the contextual richness of the text but also addresses the computational challenges of LLMs, which presents an advancement in integrating the text processing power of LLMs with the structural complexities of text-rich graphs. Empirical results show that HiCom can outperform both GNNs and LLM backbones for node classification on e-commerce and citation graphs. HiCom is especially effective for nodes from a dense region in a graph, where it achieves a 3.48% average performance improvement on five datasets while being more efficient than LLM backbones.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "2283934850",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "2258747730",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "2299062897",
                    "name": "Qi Zhu"
                },
                {
                    "authorId": "2284037254",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "2263543517",
                    "name": "Christos Faloutsos"
                },
                {
                    "authorId": "2064547804",
                    "name": "George Karypis"
                },
                {
                    "authorId": "2279734673",
                    "name": "Yizhou Sun"
                }
            ]
        },
        {
            "paperId": "4008f607e29cfe6c0cce0b5ae119827380b99031",
            "title": "PaGE-Link: Path-based Graph Neural Network Explanation for Heterogeneous Link Prediction",
            "abstract": "Transparency and accountability have become major concerns for black-box machine learning (ML) models. Proper explanations for the model behavior increase model transparency and help researchers develop more accountable models. Graph neural networks (GNN) have recently shown superior performance in many graph ML problems than traditional methods, and explaining them has attracted increased interest. However, GNN explanation for link prediction (LP) is lacking in the literature. LP is an essential GNN task and corresponds to web applications like recommendation and sponsored search on web. Given existing GNN explanation methods only address node/graph-level tasks, we propose Path-based GNN Explanation for heterogeneous Link prediction (PaGE-Link) that generates explanations with connection interpretability, enjoys model scalability, and handles graph heterogeneity. Qualitatively, PaGE-Link can generate explanations as paths connecting a node pair, which naturally captures connections between the two nodes and easily transfer to human-interpretable explanations. Quantitatively, explanations generated by PaGE-Link improve AUC for recommendation on citation and user-item graphs by 9 - 35% and are chosen as better by 78.79% of responses in human evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145408511",
                    "name": "Shichang Zhang"
                },
                {
                    "authorId": "73329314",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "2118943843",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "2109461904",
                    "name": "Yizhou Sun"
                }
            ]
        },
        {
            "paperId": "51484cf02592a3551f944b7c6bf94fe902c0aa66",
            "title": "Train Your Own GNN Teacher: Graph-Aware Distillation on Textual Graphs",
            "abstract": "How can we learn effective node representations on textual graphs? Graph Neural Networks (GNNs) that use Language Models (LMs) to encode textual information of graphs achieve state-of-the-art performance in many node classification tasks. Yet, combining GNNs with LMs has not been widely explored for practical deployments due to its scalability issues. In this work, we tackle this challenge by developing a Graph-Aware Distillation framework (GRAD) to encode graph structures into an LM for graph-free, fast inference. Different from conventional knowledge distillation, GRAD jointly optimizes a GNN teacher and a graph-free student over the graph's nodes via a shared LM. This encourages the graph-free student to exploit graph information encoded by the GNN teacher while at the same time, enables the GNN teacher to better leverage textual information from unlabeled nodes. As a result, the teacher and the student models learn from each other to improve their overall performance. Experiments in eight node classification benchmarks in both transductive and inductive settings showcase GRAD's superiority over existing distillation approaches for textual graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1944251405",
                    "name": "Costas Mavromatis"
                },
                {
                    "authorId": "40043851",
                    "name": "V. Ioannidis"
                },
                {
                    "authorId": "2151226309",
                    "name": "Shen Wang"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "65743795",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "1390716752",
                    "name": "Han Zhao"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "5bf14dda76156d62a9b3b9ff59dba90ff7b9923d",
            "title": "OrthoReg: Improving Graph-regularized MLPs via Orthogonality Regularization",
            "abstract": "Graph Neural Networks (GNNs) are currently dominating in modeling graph-structure data, while their high reliance on graph structure for inference significantly impedes them from widespread applications. By contrast, Graph-regularized MLPs (GR-MLPs) implicitly inject the graph structure information into model weights, while their performance can hardly match that of GNNs in most tasks. This motivates us to study the causes of the limited performance of GR-MLPs. In this paper, we first demonstrate that node embeddings learned from conventional GR-MLPs suffer from dimensional collapse, a phenomenon in which the largest a few eigenvalues dominate the embedding space, through empirical observations and theoretical analysis. As a result, the expressive power of the learned node representations is constrained. We further propose OrthoReg, a novel GR-MLP model to mitigate the dimensional collapse issue. Through a soft regularization loss on the correlation matrix of node embeddings, OrthoReg explicitly encourages orthogonal node representations and thus can naturally avoid dimensionally collapsed representations. Experiments on traditional transductive semi-supervised classification tasks and inductive node classification for cold-start scenarios demonstrate its effectiveness and superiority.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35466544",
                    "name": "Hengrui Zhang"
                },
                {
                    "authorId": "2151226033",
                    "name": "Shen Wang"
                },
                {
                    "authorId": "40043851",
                    "name": "V. Ioannidis"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "73329314",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "2099585332",
                    "name": "Xiao Qin"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                },
                {
                    "authorId": "152297693",
                    "name": "Philip S. Yu"
                }
            ]
        },
        {
            "paperId": "9b7f7e8df0e8d8dfb95222d80c7269259fa3ae40",
            "title": "ScatterSample: Diversified Label Sampling for Data Efficient Graph Neural Network Learning",
            "abstract": "What target labels are most effective for graph neural network (GNN) training? In some applications where GNNs excel-like drug design or fraud detection, labeling new instances is expensive. We develop a data-efficient active sampling framework, ScatterSample, to train GNNs under an active learning setting. ScatterSample employs a sampling module termed DiverseUncertainty to collect instances with large uncertainty from different regions of the sample space for labeling. To ensure diversification of the selected nodes, DiverseUncertainty clusters the high uncertainty nodes and selects the representative nodes from each cluster. Our ScatterSample algorithm is further supported by rigorous theoretical analysis demonstrating its advantage compared to standard active sampling methods that aim to simply maximize the uncertainty and not diversify the samples. In particular, we show that ScatterSample is able to efficiently reduce the model uncertainty over the whole sample space. Our experiments on five datasets show that ScatterSample significantly outperforms the other GNN active learning baselines, specifically it reduces the sampling cost by up to 50% while achieving the same test accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2075325665",
                    "name": "Zhenwei Dai"
                },
                {
                    "authorId": "28310338",
                    "name": "Vasileios Ioannidis"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "2169555999",
                    "name": "Zak Jost"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "abc3f04cf373a03178be1b81e11eec14a40c83e9",
            "title": "PropInit: Scalable Inductive Initialization for Heterogeneous Graph Neural Networks",
            "abstract": "Graph Neural Networks (GNNs) require that all nodes have initial representations which are usually derived from the node features. When the node features are absent, GNNs can learn node embeddings with an embedding layer or use pre-trained network embeddings for the initial node representations. However, these approaches are limited because i) they cannot be easily extended to initialize new nodes that are added to the graph for inference after training and ii) they are memory intensive and store a fixed representation for every node in the graph. In this work, we present PropInit a scalable node representation initialization method for training GNNs and other Graph Machine Learning (ML) models on heterogeneous graphs where some or all node types have no natural features. Unlike existing methods that learn a fixed embedding vector for each node, PropInit learns an inductive function that leverages the metagraph to initialize node representations. As a result, PropInit is fully inductive and can be applied, without retraining, to new nodes without features that are added to the graph. PropInit also scales to large graphs as it requires only a small fraction of the memory requirements of existing methods. On public benchmark heterogeneous graph datasets, using various GNN models, PropInit achieves comparable or better performance to other competing approaches while needing only 0.01% to 2% of their memory consumption for representing node embeddings. We also demonstrate PropInit's effectiveness on an industry heterogeneous graph dataset for fraud detection and achieve better classification accuracy than learning full embeddings while reducing the embedding memory footprint during training and inference by 99.99%",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "2151810148",
                    "name": "Jian Zhang"
                },
                {
                    "authorId": "2169156186",
                    "name": "Muhyun Kim"
                },
                {
                    "authorId": "2139389377",
                    "name": "Min Chen"
                },
                {
                    "authorId": "7320311",
                    "name": "Rizal Fathony"
                },
                {
                    "authorId": "2204840154",
                    "name": "Advitiya Vashisht"
                },
                {
                    "authorId": "2117930025",
                    "name": "Jia Chen"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        },
        {
            "paperId": "07fe1f1a76595413f70594aff33c4a194fa2eab7",
            "title": "Relational Graph Neural Networks for Fraud Detection in a Super-App environment",
            "abstract": "Large digital platforms create environments where different types of user interactions are captured, these relationships offer a novel source of information for fraud detection problems. In this paper we propose a framework of relational graph convolutional networks methods for fraudulent behaviour prevention in the financial services of a Super-App. To this end, we apply the framework on different heterogeneous graphs of users, devices, and credit cards; and finally use an interpretability algorithm for graph neural networks to determine the most important relations to the classification task of the users. Our results show that there is an added value when considering models that take advantage of the alternative data of the Super-App and the interactions found in their high connectivity, further proofing how they can leverage that into better decisions and fraud detection strategies.",
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "authors": [
                {
                    "authorId": "2121397875",
                    "name": "Jaime D. Acevedo-Viloria"
                },
                {
                    "authorId": "87088870",
                    "name": "Luisa Roa"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "2121395937",
                    "name": "Cesar Charalla Olazo"
                },
                {
                    "authorId": "2121390163",
                    "name": "Andr\u00e9s Rodr\u00edguez-Rey"
                },
                {
                    "authorId": "2142591264",
                    "name": "Jose Alberto Ramos"
                },
                {
                    "authorId": "2297216",
                    "name": "Alejandro Correa Bahnsen"
                }
            ]
        },
        {
            "paperId": "1f836ccdd422f35c1e2e470b1bcc7201b863cc63",
            "title": "TempoQR: Temporal Question Reasoning over Knowledge Graphs",
            "abstract": "Knowledge Graph Question Answering (KGQA) involves retrieving facts from a Knowledge Graph (KG) using natural language queries. A KG is a curated set of facts consisting of entities linked by relations. Certain facts include also temporal information forming a Temporal KG (TKG). Although many natural questions involve explicit or implicit time constraints, question answering (QA) over TKGs has been a relatively unexplored area. Existing solutions are mainly designed for simple temporal questions that can be answered directly by a single TKG fact.\n This paper puts forth a comprehensive embedding-based framework for answering complex questions over TKGs. Our method termed temporal question reasoning (TempoQR) exploits TKG embeddings to ground the question to the specific entities and time scope it refers to. It does so by augmenting the question embeddings with context, entity and time-aware information by employing three specialized modules. The first computes a textual representation of a given question, the second combines it with the entity embeddings for entities involved in the question, and the third generates question-specific time embeddings. Finally, a transformer-based encoder learns to fuse the generated temporal information with the question representation, which is used for answer predictions. Extensive experiments show that TempoQR improves accuracy by 25--45 percentage points on complex temporal questions over state-of-the-art approaches and it generalizes better to unseen question types.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1944251405",
                    "name": "Costas Mavromatis"
                },
                {
                    "authorId": "2145157804",
                    "name": "Prasanna Lakkur Subramanyam"
                },
                {
                    "authorId": "40043851",
                    "name": "V. Ioannidis"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "48575315",
                    "name": "Phillip Howard"
                },
                {
                    "authorId": "2145154001",
                    "name": "Tetiana Grinberg"
                },
                {
                    "authorId": "2058623624",
                    "name": "Nagib Hakim"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                }
            ]
        }
    ]
}