{
    "authorId": "47331186",
    "papers": [
        {
            "paperId": "1b9ce0e84228882e912e8c526eca0911860929bd",
            "title": "Big Data Analytics from the Rich Cloud to the Frugal Edge",
            "abstract": "Modern systems and applications generate and consume an enormous amount of data from different sources, including mobile edge computing and IoT systems. Our ability to locate and analyze these massive amounts of data will shape the future, building next-generation Big Data Analytics (BDA) and artificial intelligence systems in critical domains. Traditionally, big data materialize in a centralized repository (e.g., the cloud) for running sophisticated analytics using decent computation. Nevertheless, many modern applications and critical domains require low-latency data analysis with the right decision at the right time standard for building trust. With the advent of edge computing, that traditional deployment model shifted closer to the data sources at the network\u2019s edge. Such a shift was motivated by minimized latency, increased uptime, and enhanced efficiencies. This paper studies the BDA building blocks, analyzes the deployment requirements for edge-based BDA QoS, and drafts future trends. It also discusses critical open issues and further research directions for the next step of edge-based BDA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17663383",
                    "name": "Feras M. Awaysheh"
                },
                {
                    "authorId": "50271459",
                    "name": "Riccardo Tommasini"
                },
                {
                    "authorId": "47331186",
                    "name": "Ahmed Awad"
                }
            ]
        },
        {
            "paperId": "960093070ba735676203a4dca6a7e033f82440ab",
            "title": "I Will Survive: An Event-driven Conformance Checking Approach Over Process Streams",
            "abstract": "Online conformance checking deals with finding discrepancies between real-life and modeled behavior on data streams. The current state-of-the-art output of online conformance checking is a prefix-alignment, which is used for pinpointing the exact deviations in terms of the trace and the model while accommodating a trace's unknown termination in an online setting. Current methods for producing prefix-alignments are computationally expensive and hinder the applicability in real-life settings. This paper introduces a new approximate algorithm - I Will Survive (IWS). The algorithm utilizes the trie data structure to improve the calculation speed, while remaining memory-efficient. Comparative analysis on real-life and synthetic datasets shows that the IWS algorithm can achieve an order of magnitude faster execution time while having a smaller error cost, compared to the current state of the art. In extreme cases, the IWS finds prefix-alignments roughly three orders of magnitude faster than previous approximate methods. The IWS algorithm includes a discounted decay time setting for more efficient memory usage and a look-ahead limit for improving computation time. Finally, the algorithm is stress tested for performance using a simulation of high-traffic event streams.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "147246577",
                    "name": "Kristo Raun"
                },
                {
                    "authorId": "50271459",
                    "name": "Riccardo Tommasini"
                },
                {
                    "authorId": "47331186",
                    "name": "Ahmed Awad"
                }
            ]
        },
        {
            "paperId": "983aea938bbb16f59f3797c7c7ff553a062d54c2",
            "title": "On the Shift to Decentralised Identity Management in Distributed Data Exchange Systems",
            "abstract": "The commonly used centralised trust and centralised identity management make information systems and organisations prone to a single point of failure. Therefore, decentralised identity management has appeared as an alternative solution to mitigate the weaknesses of centralised identity. In this paper, we propose an approach of system analysis that should guide organisations that considers the transition to decentralised identity management. The approach aims to support decision-making about the usefulness of the transition based on the created assessment model. The approach is validated through a case study of the X-Road ecosystem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117252837",
                    "name": "Mariia Bakhtina"
                },
                {
                    "authorId": "2112115",
                    "name": "Raimundas Matulevi\u010dius"
                },
                {
                    "authorId": "47331186",
                    "name": "Ahmed Awad"
                },
                {
                    "authorId": "2219554602",
                    "name": "Petteri Kivim\u00e4ki"
                }
            ]
        },
        {
            "paperId": "d47ba8c72521964ea1de7197e8885fb91aa2e42b",
            "title": "A Decentralised Public Key Infrastructure for X-Road",
            "abstract": "X-Road is an open-source solution that acts as a data exchange layer and enables secure data exchange between organisations. X-Road serves as the backbone of digital infrastructure in the public sector (e.g., enabling Estonia\u2019s digital public services) and private sector (e.g., enabling clients\u2019 data exchange in the Japanese energy sector). An approach and architecture were recently proposed for the X-Road data exchange systems to move from public key infrastructure (PKI) with centralised certification authorities to decentralised PKI (DPKI). In this paper, we develop a proof of concept for the designed DPKI-based architecture that leverages distributed ledger-based identifiers and verifiable credentials to establish trust between information systems using Hyperledger Indy and Hyperledger Aries. We evaluate the proof of concept implementation against the design and functional requirements. The results show that the proposed system architecture is technically feasible and satisfies the identified design goals and functional requirements. To the best of our knowledge, this paper presents the first open-access system prototype for an organisation\u2019s identity management following self-sovereign identity principles. The presented proof of concept proves that DPKI helps to address some of the scalability issues of PKI, improve control over identity and mitigate replay attacks and a single point of failure in the X-Road system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117252837",
                    "name": "Mariia Bakhtina"
                },
                {
                    "authorId": "2228363192",
                    "name": "Kin Long Leung"
                },
                {
                    "authorId": "2112115",
                    "name": "Raimundas Matulevi\u010dius"
                },
                {
                    "authorId": "47331186",
                    "name": "Ahmed Awad"
                },
                {
                    "authorId": "1804733",
                    "name": "P. \u0160venda"
                }
            ]
        },
        {
            "paperId": "6a894c1842b54f6c708f43645c375bb645f48b53",
            "title": "Optimizing ADWIN for steady streams",
            "abstract": "With the ever-growing data generation rates and stringent constraints on the latency of analyzing such data, stream analytics is overtaking. Learning from data streams, aka online machine learning, is no exception. However, online machine learning comes with many challenges for the different aspects of the learning process, starting from the algorithm design to the evaluation method. One of these challenges is the ability of a learning system to adapt to the change in data distribution, known as concept drift, to maintain the accuracy of the predictions. Over time, several drift detection approaches have been proposed. A prominent approach is adaptive windowing (ADWIN) which can detect changes in features data distribution without explicit feedback on the correctness of the prediction. Several variants for ADWIN have been proposed to enhance its runtime performance, w.r.t throughput, and latency. However, the drift detection accuracy of these variants was not compared with the original algorithm. Moreover, there is no study concerning the memory consumption of the variants and the original algorithm. Additionally, the evaluation was done on synthetic datasets with a considerable number of drifts not covering all types of drifts or steady streams, those that do not have drifts at all or almost negligible drifts. The contribution of this paper is two-fold. First, we compare the original Adaptive Window (ADWIN) and its variants: Serial, HalfCut, and Optimistic in terms of drift detection accuracy, detection speed, and memory consumption, represented in the internal window size. We compare them using synthetic data sets covering different types of concept drifts, namely: incremental, gradual, abrupt, and steady. We also use two real-life datasets whose drifts are unknown. Second, we present ADWIN++. We use an adaptive bucket dropping technique to control window size. We evaluate our technique on the same data sets above and new datasets with fewer drifts. Experiments show that our approach saves about 80% of memory consumption. Moreover, it takes less time to detect concept drift and maintains the drift detection accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2164258242",
                    "name": "Hassan Moharram"
                },
                {
                    "authorId": "47331186",
                    "name": "Ahmed Awad"
                },
                {
                    "authorId": "143633033",
                    "name": "Passent El-Kafrawy"
                }
            ]
        },
        {
            "paperId": "be24df1fcf4235a51a59e4908a7951fb60b4968c",
            "title": "A Distributed Real-Time Recommender System for Big Data Streams",
            "abstract": "In today's data-driven world, recommender systems (RS) play a crucial role to support the decision-making process. As users become continuously connected to the internet, they become less patient and less tolerant to obsolete recommendations made by an RS, e.g., movie recommendations on Netflix or books to read on Amazon. This, in turn, requires continuous training of the RS to cope with both the online fashion of data and the changing nature of user tastes and interests, known as concept drift. Streaming (online) RS has to address three requirements: continuous training and recommendation, handling concept drifts, and ability to scale. Streaming recommender systems proposed in the literature mostly, address the first two requirements and do not consider scalability. That is because they run the training process on a single machine. Such a machine, no matter how powerful it is, will eventually fail to cope with the volume of the data, a lesson learned from big data processing. To tackle the third challenge, we propose a Splitting and Replication mechanism for building distributed streaming recommender systems. Our mechanism is inspired by the successful shared-nothing architecture that underpins contemporary big data processing systems. We have applied our mechanism to two well-known approaches for online recommender systems, namely, matrix factorization and item-based collaborative filtering. We have implemented our mechanism on top of Apache Flink. We conducted experiments comparing the performance of the baseline (single machine) approach with our distributed approach. Evaluating different data sets, improvement in processing latency, throughput, and accuracy have been observed. Our experiments show online recall improvement by 40\\% with more than 50\\% less memory consumption.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1586543759",
                    "name": "Heidy Hazem"
                },
                {
                    "authorId": "47331186",
                    "name": "Ahmed Awad"
                },
                {
                    "authorId": "144890499",
                    "name": "A. Hassan"
                }
            ]
        },
        {
            "paperId": "748238c6df7bfca8cdaa71aca6b0fbb08886f70d",
            "title": "Commit Message Generation from Code Differences using Hidden Markov Models",
            "abstract": "Commit messages are developer-written messages that document code changes. Such change might be adding features, fixing bugs or simply code updates. Although these messages help in understanding the evolution of any software, it is quite often that developers disregard the process of writing these messages, when making a change. Many automated methods have been proposed to generate commit messages. Due to the inability of those techniques to represent higher order understanding of code changes, the quality of these messages in terms of logic and context representation is very low as opposed to developer written messages. To solve this problem, previous work used deep learning models -specifically, sequence-to-sequence models- were used to automate that task. This model delivered promising results on translating code differences to commit messages. However, after the model's performance was thoroughly investigated in previous work. It was found out that code differences corresponding to almost every high quality commit messages generated by the model were very similar to one or more training sample code differences on a token level. Motivated by that observation, a k-nearest neighbor algorithm that outputs the same exact message of the nearest code difference was proposed in previous work. Inspired by the traditional solution to sequence modeling; Hidden Markov Models, we show that HMMs outperforms sequence-to-sequence models without outputting the same exact message of the nearest code diff, our experiments show an enhancement of 4% against sequence to sequence models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47331186",
                    "name": "Ahmed Awad"
                },
                {
                    "authorId": "8908312",
                    "name": "K. Nagaty"
                }
            ]
        },
        {
            "paperId": "2f4e6dabde13412127bb2394c12f58b009eec999",
            "title": "Enforcing Customization in e-Learning Systems: an ontology and product line-based approach",
            "abstract": "In the era of e-Learning, educational materials are considered a crucial point for all the stakeholders. On the one hand, instructors aim at creating learning materials that meet the needs and expectations of learners easily and effec-tively; On the other hand, learners want to acquire knowledge in a way that suits their characteristics and preferences. Consequently, the provision and customization of educational materials to meet the needs of learners is a constant challenge and is currently synonymous with technological devel-opment. Promoting the personalization of learning materials, especially dur-ing their development, will help to produce customized learning materials for specific learners' needs. The main objective of this thesis is to reinforce and strengthen Reuse, Cus-tomization and Ease of Production issues in e-Learning materials during the development process. The thesis deals with the design of a framework based on ontologies and product lines to develop customized Learning Objects (LOs). With this framework, the development of learning materials has the following advantages: (i) large-scale production, (ii) faster development time, (iii) greater (re) use of resources. The proposed framework is the main contribution of this thesis, and is char-acterized by the combination of three models: the Content Model, which addresses important points related to the structure of learning materials, their granularity and levels of aggregation; the Customization Model, which con-siders specific learner characteristics and preferences to customize the learn-ing materials; and the LO Product Line (LOPL) model, which handles the subject of variability and creates matter-them in an easy and flexible way. With these models, instructors can not only develop learning materials, but also reuse and customize them during development. An additional contribution is the Customization Model, which is based on the Learning Style Model (LSM) concept. Based on the study of seven of them, a Global Learning Style Model Ontology (GLSMO) has been con-structed to help instructors with information on the apprentice's characteris-tics and to recommend appropriate LOs for customization. The results of our work have been reflected in the design of an authoring tool for learning materials called LOAT. They have described their require-ments, the elements of their architecture, and some details of their user inter-face. As an example of its use, it includes a case study that shows how its use in the development of some learning components.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47331186",
                    "name": "Ahmed Awad"
                }
            ]
        }
    ]
}