{
    "authorId": "1801149",
    "papers": [
        {
            "paperId": "8bb8784903bbaa24e4606b49cbd0859e595c78e7",
            "title": "Confidence Calibration and Rationalization for LLMs via Multi-Agent Deliberation",
            "abstract": "Uncertainty estimation is a significant issue for current large language models (LLMs) that are generally poorly calibrated and over-confident, especially with reinforcement learning from human feedback (RLHF). Unlike humans, whose decisions and confidences not only stem from intrinsic beliefs but can also be adjusted through daily observations, existing calibration methods for LLMs focus on estimating or eliciting individual confidence without taking full advantage of the\"Collective Wisdom\": the interaction among multiple LLMs that can collectively improve both accuracy and calibration. In this work, we propose Collaborative Calibration, a post-hoc training-free calibration strategy that leverages the collaborative and expressive capabilities of multiple tool-augmented LLM agents in a simulated group deliberation process. We demonstrate the effectiveness of Collaborative Calibration on generative QA tasks across various domains, showing its potential in harnessing the rationalization of collectively calibrated confidence assessments and improving the reliability of model predictions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2297329510",
                    "name": "Ruixin Yang"
                },
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "2281641852",
                    "name": "S. Hayati"
                },
                {
                    "authorId": "2296769037",
                    "name": "Bin Hu"
                },
                {
                    "authorId": "48493368",
                    "name": "Dongyeop Kang"
                }
            ]
        },
        {
            "paperId": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "title": "How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!",
            "abstract": "Collecting diverse human opinions is costly and challenging. This leads to a recent trend in collaborative efforts between humans and Large Language Models (LLMs) for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate a new problem of maximum diversity extraction from LLMs. Motivated by how humans develop their opinions through their values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative manner. As we apply our methods to various tasks, indeed we find that LLMs can generate diverse opinions according to the degree of task subjectivity",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31998283",
                    "name": "Shirley Anugrah Hayati"
                },
                {
                    "authorId": "2187932371",
                    "name": "Minhwa Lee"
                },
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "48493368",
                    "name": "Dongyeop Kang"
                }
            ]
        },
        {
            "paperId": "f0ed191bdcbd56610ab4d95c5a80d5ac31ffc074",
            "title": "AutoMix: Automatically Mixing Language Models",
            "abstract": "Large language models (LLMs) are now available from cloud API providers in various sizes and configurations. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present Automix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to Automix are two key technical contributions. First, it has a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring extensive training. Second, given that self-verification can be noisy, it employs a POMDP based router that can effectively select an appropriately sized model, based on answer confidence. Experiments across five language models and five challenging datasets show that Automix consistently surpasses strong baselines, reducing computational cost by over 50% for comparable performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21626987",
                    "name": "Aman Madaan"
                },
                {
                    "authorId": "2114841965",
                    "name": "Pranjal Aggarwal"
                },
                {
                    "authorId": "2275111393",
                    "name": "Ankit Anand"
                },
                {
                    "authorId": "52226135",
                    "name": "Srividya Pranavi Potharaju"
                },
                {
                    "authorId": "1817207",
                    "name": "Swaroop Mishra"
                },
                {
                    "authorId": "1557324013",
                    "name": "Pei Zhou"
                },
                {
                    "authorId": "2254858472",
                    "name": "Aditya Gupta"
                },
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "2202787",
                    "name": "K. Kappaganthu"
                },
                {
                    "authorId": "2260433687",
                    "name": "Yiming Yang"
                },
                {
                    "authorId": "2254265068",
                    "name": "Shyam Upadhyay"
                },
                {
                    "authorId": "2260340500",
                    "name": "Mausam"
                },
                {
                    "authorId": "1779225",
                    "name": "Manaal Faruqui"
                }
            ]
        },
        {
            "paperId": "1b1b6e429c022fecb524f755e3cf36961566bcb7",
            "title": "StyLEx: Explaining Style Using Human Lexical Annotations",
            "abstract": "Large pre-trained language models have achieved impressive results on various style classification tasks, but they often learn spurious domain-specific words to make predictions (Hayati et al., 2021). While human explanation highlights stylistic tokens as important features for this task, we observe that model explanations often do not align with them. To tackle this issue, we introduce StyLEx, a model that learns from human annotated explanations of stylistic features and jointly learns to perform the task and predict these features as model explanations. Our experiments show that StyLEx can provide human like stylistic lexical explanations without sacrificing the performance of sentence-level style prediction on both in-domain and out-of-domain datasets. Explanations from StyLEx show significant improvements in explanation metrics (sufficiency, plausibility) and when evaluated with human annotations. They are also more understandable by human judges compared to the widely-used saliency-based explanation baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31998283",
                    "name": "Shirley Anugrah Hayati"
                },
                {
                    "authorId": "2152042873",
                    "name": "Kyumin Park"
                },
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "1717822",
                    "name": "Lyle Ungar"
                },
                {
                    "authorId": "48493368",
                    "name": "Dongyeop Kang"
                }
            ]
        },
        {
            "paperId": "1cdcf5f527a887cb935eaf2a0109b40d2fc90fdb",
            "title": "Conditional set generation using Seq2seq models",
            "abstract": "Conditional set generation learns a mapping from an input sequence of tokens to a set. Several NLP tasks, such as entity typing and dialogue emotion tagging, are instances of set generation. Seq2Seq models are a popular choice to model set generation but they treat a set as a sequence and do not fully leverage its key properties, namely order-invariance and cardinality. We propose a novel algorithm for effectively sampling informative orders over the combinatorial space of label orders. Further, we jointly model the set cardinality and output by listing the set size as the first element and taking advantage of the autoregressive factorization used by Seq2Seq models. Our method is a model-independent data augmentation approach that endows any Seq2Seq model with the signals of order-invariance and cardinality. Training a Seq2Seq model on this new augmented data (without any additional annotations), gets an average relative improvement of 20% for four benchmarks datasets across models spanning from BART-base, T5-11B, and GPT-3. We will release all code and data upon acceptance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21626987",
                    "name": "Aman Madaan"
                },
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "1721168",
                    "name": "Niket Tandon"
                },
                {
                    "authorId": "46286308",
                    "name": "Yiming Yang"
                },
                {
                    "authorId": "2691021",
                    "name": "Antoine Bosselut"
                }
            ]
        },
        {
            "paperId": "51e549e1bb49032ea24bfe74dfa30d52c1172ae4",
            "title": "Counterfactual Data Augmentation improves Factuality of Abstractive Summarization",
            "abstract": "Abstractive summarization systems based on pretrained language models often generate coherent but factually inconsistent sentences. In this paper, we present a counterfactual data augmentation approach where we augment data with perturbed summaries that increase the training data diversity. Specifically, we present three augmentation approaches based on replacing (i) entities from other and the same category and (ii) nouns with their corresponding WordNet hypernyms. We show that augmenting the training data with our approach improves the factual correctness of summaries without significantly affecting the ROUGE score. We show that in two commonly used summarization datasets (CNN/Dailymail and XSum), we improve the factual correctness by about 2.5 points on average",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "2944868",
                    "name": "Siamak Shakeri"
                },
                {
                    "authorId": "1790831",
                    "name": "C. D. Santos"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                },
                {
                    "authorId": "2152948655",
                    "name": "Chung-Ching Chang"
                }
            ]
        },
        {
            "paperId": "5e5b0979e5c532cf494d79ab07bb1eb75d21c3bf",
            "title": "StyLEx: Explaining Styles with Lexicon-Based Human Perception",
            "abstract": "Style plays a signi\ufb01cant role in how humans express themselves and communicate with others. Large pre-trained language models produce impressive results on various style clas-si\ufb01cation tasks. However, they often learn spurious domain-speci\ufb01c words to make predictions. This incorrect word importance learned by the model often leads to ambiguous token-level explanations which do not align with human perception of linguistic styles. To tackle this challenge, we introduce StyLEx, a model that learns annotated human perceptions of stylistic lexica and uses these stylistic words as additional information for predicting the style of a sentence. Our experiments show that StyLEx can provide human-like stylistic lexical explanations without sacri\ufb01cing the performance of sentence-level style prediction on both original and out-of-domain datasets. Explanations from StyLEx show higher suf-\ufb01ciency, and plausibility when compared to human annotations, and are also more under-standable by human judges compared to the existing widely-used saliency baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31998283",
                    "name": "Shirley Anugrah Hayati"
                },
                {
                    "authorId": "2152042873",
                    "name": "Kyumin Park"
                },
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "1717822",
                    "name": "Lyle Ungar"
                },
                {
                    "authorId": "48493368",
                    "name": "Dongyeop Kang"
                }
            ]
        },
        {
            "paperId": "8842a96d21340af84dc068d7b74fc44c0f36583e",
            "title": "One Document, Many Revisions: A Dataset for Classification and Description of Edit Intents",
            "abstract": "Document authoring involves a lengthy revision process, marked by individual edits that are frequently linked to comments. Modeling the relationship between edits and comments leads to a better understanding of document evolution, potentially benefiting applications such as content summarization, and task triaging. Prior work on understanding revisions has primarily focused on classifying edit intents, but falling short of a deeper understanding of the nature of these edits. In this paper, we present explore the challenge of describing an edit at two levels: identifying the edit intent, and describing the edit using free-form text. We begin by defining a taxonomy of general edit intents and introduce a new dataset of full revision histories of Wikipedia pages, annotated with each revision\u2019s edit intent. Using this dataset, we train a classifier that achieves a 90% accuracy in identifying edit intent. We use this classifier to train a distantly-supervised model that generates a high-level description of a revision in free-form text. Our experimental results show that incorporating edit intent information aids in generating better edit descriptions. We establish a set of baselines for the edit description task, achieving a best score of 28 ROUGE, thus demonstrating the effectiveness of our layered approach to edit understanding.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "2048981220",
                    "name": "Xuchao Zhang"
                },
                {
                    "authorId": "2417334",
                    "name": "Michael Gamon"
                },
                {
                    "authorId": "3001990",
                    "name": "S. Jauhar"
                },
                {
                    "authorId": "2143919864",
                    "name": "Diyi Yang"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                }
            ]
        },
        {
            "paperId": "1a0d8dbd0252193abe9d64f72fc56cc1f05ed3eb",
            "title": "CURIE: An Iterative Querying Approach for Reasoning About Situations",
            "abstract": "Predicting the effects of unexpected situations is an important reasoning task, e.g., would cloudy skies help or hinder plant growth? Given a context, the goal of such situational reasoning is to elicit the consequences of a new situation (st) that arises in that context. We propose CURIE, a method to iteratively build a graph of relevant consequences explicitly in a structured situational graph (st graph) using natural language queries over a finetuned language model. Across multiple domains, CURIE generates st graphs that humans find relevant and meaningful in eliciting the consequences of a new situation (75% of the graphs were judged correct by humans). We present a case study of a situation reasoning end task (WIQA-QA), where simply augmenting their input with st graphs improves accuracy by 3 points. We show that these improvements mainly come from a hard subset of the data, that requires background knowledge and multi-hop reasoning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "21626987",
                    "name": "Aman Madaan"
                },
                {
                    "authorId": "1721168",
                    "name": "Niket Tandon"
                },
                {
                    "authorId": "46286308",
                    "name": "Yiming Yang"
                },
                {
                    "authorId": "9358910",
                    "name": "Shrimai Prabhumoye"
                },
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "48323507",
                    "name": "Peter Clark"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                }
            ]
        },
        {
            "paperId": "21bd7486c693aeb17ada3102ed400fb9233a6cb0",
            "title": "Template Filling for Controllable Commonsense Reasoning",
            "abstract": "Large-scale sequence-to-sequence models have shown to be adept at both multiple-choice and open-domain commonsense reasoning tasks. However, the current systems do not provide the ability to control the various attributes of the reasoning chain. To enable better controllability, we propose to study the commonsense reasoning as a template filling task (TemplateCSR) -- where the language models fills reasoning templates with the given constraints as control factors. As an approach to TemplateCSR, we (i) propose a dataset of commonsense reasoning template-expansion pairs and (ii) introduce POTTER, a pretrained sequence-to-sequence model using prompts to perform commonsense reasoning across concepts. Our experiments show that our approach outperforms baselines both in generation metrics and factuality metrics. We also present a detailed error analysis on our approach's ability to reliably perform commonsense reasoning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1801149",
                    "name": "Dheeraj Rajagopal"
                },
                {
                    "authorId": "3638684",
                    "name": "Vivek Khetan"
                },
                {
                    "authorId": "2666791",
                    "name": "Bogdan Sacaleanu"
                },
                {
                    "authorId": "145001267",
                    "name": "A. Gershman"
                },
                {
                    "authorId": "151352049",
                    "name": "Andy E. Fano"
                },
                {
                    "authorId": "144547315",
                    "name": "E. Hovy"
                }
            ]
        }
    ]
}