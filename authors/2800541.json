{
    "authorId": "2800541",
    "papers": [
        {
            "paperId": "1d59fae73c33b9f7c3c41927f21fdc53e4d7355d",
            "title": "Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval",
            "abstract": "Large language models (LLMs) have demonstrated significant potential in clinical decision support. Yet LLMs still suffer from hallucinations and lack fine-grained contextual medical knowledge, limiting their high-stake healthcare applications such as clinical diagnosis. Traditional retrieval-augmented generation (RAG) methods attempt to address these limitations but frequently retrieve sparse or irrelevant information, undermining prediction accuracy. We introduce KARE, a novel framework that integrates knowledge graph (KG) community-level retrieval with LLM reasoning to enhance healthcare predictions. KARE constructs a comprehensive multi-source KG by integrating biomedical databases, clinical literature, and LLM-generated insights, and organizes it using hierarchical graph community detection and summarization for precise and contextually relevant information retrieval. Our key innovations include: (1) a dense medical knowledge structuring approach enabling accurate retrieval of relevant information; (2) a dynamic knowledge retrieval mechanism that enriches patient contexts with focused, multi-faceted medical insights; and (3) a reasoning-enhanced prediction framework that leverages these enriched contexts to produce both accurate and interpretable clinical predictions. Extensive experiments demonstrate that KARE outperforms leading models by up to 10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and readmission predictions. In addition to its impressive prediction accuracy, our framework leverages the reasoning capabilities of LLMs, enhancing the trustworthiness of clinical predictions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Pengcheng Jiang"
                },
                {
                    "authorId": null,
                    "name": "Cao Xiao"
                },
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "2303385525",
                    "name": "Parminder Bhatia"
                },
                {
                    "authorId": "1405125430",
                    "name": "Taha A. Kass-Hout"
                },
                {
                    "authorId": "2249887353",
                    "name": "Jimeng Sun"
                },
                {
                    "authorId": null,
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "b2fda33b7c122c044a7faa185d250d59ce9e4453",
            "title": "Investigating Data Contamination for Pre-training Language Models",
            "abstract": "Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \\textit{data contamination} -- in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \\textit{from scratch}. We highlight the effect of both text contamination (\\textit{i.e.}\\ input text of the evaluation samples) and ground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks. Additionally, we examine the prevailing n-gram-based definitions of contamination within current LLM reports, pinpointing their limitations and inadequacy. Our findings offer new insights into data contamination's effects on language model capabilities and underscore the need for independent, comprehensive contamination assessments in LLM studies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "2298016051",
                    "name": "Ken Ziyu Liu"
                },
                {
                    "authorId": "1606040932",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "1749176844",
                    "name": "Rylan Schaeffer"
                },
                {
                    "authorId": "2260339714",
                    "name": "Siru Ouyang"
                },
                {
                    "authorId": "2259869648",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "123593472",
                    "name": "Sanmi Koyejo"
                }
            ]
        },
        {
            "paperId": "5272b872f8f9a6c89880f0fe6b7e8ad47333b7ec",
            "title": "PromptClass: Weakly-Supervised Text Classification with Prompting Enhanced Noise-Robust Self-Training",
            "abstract": "Recently proposed weakly-supervised text classification settings train a classifier using the label name of each target class as the only super-vision. Such weakly-supervised settings have been gaining increasing attention since they can largely reduce human annotation efforts compared to fully-supervised and semi-supervised settings. Most existing methods follow the strategy that first uses the label names as static features to generate pseudo labels, which are then used for classifier training. While reasonable, such a commonly adopted framework suffers from two limitations: (1) words can have different meanings in different contexts, so using label names for context-free matching can induce very noisy pseudo labels; and (2) the errors made in the pseudo label generation stage will directly propagate to the classifier training stage without a chance of being corrected. In this paper, we propose a new method, PromptClass, consisting of two modules: (1) a pseudo label acquisition module that uses zero-shot prompting of pre-trained language models (PLM) to get pseudo labels based on contextualized text understanding, and (2) a noise-robust self-training module that iteratively trains the classifier and updates pseudo labels by utilizing two PLM fine-tuning strategies that regularize each other. Extensive experiments show that PromptClass achieves overall better performance than existing strong baselines on four benchmark datasets and even achieves similar performance to fully-supervised classifiers on sentiment classification tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48379289",
                    "name": "Yunyi Zhang"
                },
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "2261413719",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "2257136881",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "9dd8d14842256e16dcc2ababcd5dddbeb8bee726",
            "title": "OntoType: Ontology-Guided and Pre-Trained Language Model Assisted Fine-Grained Entity Typing",
            "abstract": "Fine-grained entity typing (FET), which assigns entities in text with context-sensitive, fine-grained semantic types, is a basic but important task for knowledge extraction from unstructured text. FET has been studied extensively in natural language processing and typically relies on human-annotated corpora for training, which is costly and difficult to scale. Recent studies explore the utilization of pre-trained language models (PLMs) as a knowledge base to generate rich and context-aware weak supervision for FET. However, a PLM still requires direction and guidance to serve as a knowledge base as they often generate a mixture of rough and fine-grained types, or tokens unsuitable for typing. In this study, we vision that an ontology provides a semantics-rich, hierarchical structure, which will help select the best results generated by multiple PLM models and head words. Specifically, we propose a novel annotation-free, ontology-guided FET method, OntoType, which follows a type ontological structure, from coarse to fine, ensembles multiple PLM prompting results to generate a set of type candidates, and refines its type resolution, under the local context with a natural language inference model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their associated ontological structures demonstrate that our method outperforms the state-of-the-art zero-shot fine-grained entity typing methods as well as a typical LLM method, ChatGPT. Our error analysis shows that refinement of the existing ontology structures will further improve fine-grained entity typing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113611114",
                    "name": "Tanay Komarlu"
                },
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "2154990549",
                    "name": "Xuan Wang"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "9ecf184dd657640cbd1c4cc0f3c801ebd9d53162",
            "title": "ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision",
            "abstract": "Structured chemical reaction information plays a vital role for chemists engaged in laboratory work and advanced endeavors such as computer-aided drug design. Despite the importance of extracting structured reactions from scientific literature, data annotation for this purpose is cost-prohibitive due to the significant labor required from domain experts. Consequently, the scarcity of sufficient training data poses an obstacle to the progress of related models in this domain. In this paper, we propose ReactIE, which combines two weakly supervised approaches for pre-training. Our method utilizes frequent patterns within the text as linguistic cues to identify specific characteristics of chemical reactions. Additionally, we adopt synthetic data from patent records as distant supervision to incorporate domain knowledge into the model. Experiments demonstrate that ReactIE achieves substantial improvements and outperforms all existing baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1606040932",
                    "name": "Ming Zhong"
                },
                {
                    "authorId": "2042897657",
                    "name": "Siru Ouyang"
                },
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "50161308",
                    "name": "Vivian Hu"
                },
                {
                    "authorId": "1381900594",
                    "name": "Yizhu Jiao"
                },
                {
                    "authorId": "2154990549",
                    "name": "Xuan Wang"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "a1c297c2b0d54817670532edfa937444e595249f",
            "title": "OntoType: Ontology-Guided Zero-Shot Fine-Grained Entity Typing with Weak Supervision from Pre-Trained Language Models",
            "abstract": "Fine-grained entity typing (FET), which assigns entities in text with context-sensitive, \ufb01ne-grained semantic types, will play an important role in natural language understanding. A supervised FET method, which typically relies on human-annotated corpora for training, is costly and dif\ufb01cult to scale. Recent studies leverage pre-trained language models (PLMs) to generate rich and context-aware weak supervision for FET. However, a PLM may still generate a mixture of rough and \ufb01ne-grained types, or tokens unsuitable for typing. In this study, we vision that an ontology provides a semantics-rich, hierarchical structure, which will help select the best results generated by multiple PLM models and head words. Specifically, we propose a novel zero-shot, ontology-guided FET method, O NTO T YPE , which follows a type ontological structure, from coarse to \ufb01ne, ensembles multiple PLM prompting results to generate a set of type candidates, and re\ufb01nes its type resolution, under the lo-cal context with a natural language inference model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their associated ontological structures demonstrate that our method outperforms the state-of-the-art zero-shot \ufb01ne-grained entity typing methods. Our error analysis shows that re\ufb01nement of the existing ontology structures will further improve \ufb01ne-grained entity typing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113611114",
                    "name": "Tanay Komarlu"
                },
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "2265623885",
                    "name": "Xuan Wang"
                },
                {
                    "authorId": "2265725240",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "a5960c6674f26118e1e81b95d5c2482dce159bfb",
            "title": "PIEClass: Weakly-Supervised Text Classification with Prompting and Noise-Robust Iterative Ensemble Training",
            "abstract": "Weakly-supervised text classification trains a classifier using the label name of each target class as the only supervision, which largely reduces human annotation efforts. Most existing methods first use the label names as static keyword-based features to generate pseudo labels, which are then used for final classifier training. While reasonable, such a commonly adopted framework suffers from two limitations: (1) keywords can have different meanings in different contexts and some text may not have any keyword, so keyword matching can induce noisy and inadequate pseudo labels; (2) the errors made in the pseudo label generation stage will directly propagate to the classifier training stage without a chance of being corrected. In this paper, we propose a new method, PIEClass, consisting of two modules: (1) a pseudo label acquisition module that uses zero-shot prompting of pre-trained language models (PLM) to get pseudo labels based on contextualized text understanding beyond static keyword matching, and (2) a noise-robust iterative ensemble training module that iteratively trains classifiers and updates pseudo labels by utilizing two PLM fine-tuning methods that regularize each other. Extensive experiments show that PIEClass achieves overall better performance than existing strong baselines on seven benchmark datasets and even achieves similar performance to fully-supervised classifiers on sentiment classification tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48379289",
                    "name": "Yunyi Zhang"
                },
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "49891156",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "c0c5e1284df896fcec61e5b50b9a5668214d0b7a",
            "title": "Geospatial Knowledge Hypercube",
            "abstract": "Today a tremendous amount of geospatial knowledge is hidden in massive volumes of text data. To facilitate flexible and powerful geospatial analysis and applications, we introduce a new architecture: geospatial knowledge hypercube, a multi-scale, multidimensional knowledge structure that integrates information from geospatial dimensions, thematic themes and diverse application semantics, extracted and computed from spatial-related text data. To construct such a knowledge hypercube, weakly supervised language models are leveraged for automatic, dynamic and incremental extraction of heterogeneous geospatial data, thematic themes, latent connections and relationships, and application semantics, through combining a variety of information from unstructured text, structured tables, and maps. The hypercube lays a foundation for many knowledge discovery and in-depth spatial analysis, and other advanced applications. We have deployed a prototype web application of proposed geospatial knowledge hypercube for public access at: https://hcwebapp.cigi.illinois.edu/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257245358",
                    "name": "Zhaonan Wang"
                },
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "2276366678",
                    "name": "Wei Hu"
                },
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "2276004425",
                    "name": "Seungyeon Kang"
                },
                {
                    "authorId": "46947755",
                    "name": "Zhiyuan Li"
                },
                {
                    "authorId": "2187778761",
                    "name": "Sizhe Zhou"
                },
                {
                    "authorId": "2257136881",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "2238440774",
                    "name": "Shaowen Wang"
                }
            ]
        },
        {
            "paperId": "4086fc2bc4b7966698e31c0d5ddc19f31b4702c4",
            "title": "REACTCLASS: Cross-Modal Supervision for Subword-Guided Reactant Entity Classification",
            "abstract": "We propose REACTCLASS that automatically maps the low-level concrete chemical entities into the high-level reactant groups without human effort for training data annotation. REACTCLASS is designed to take two special characteristics of the chemical molecules into consideration. The first characteristic is that each chemical molecule can be represented in two modalities: a chemical name in the text and a molecular structure in the graph. We propose to use cross-modal supervision to automatically create the training data for chemical name classification in the text via molecular structure matching in the graph. The second characteristic is that there is a knowledge-aware subword correlation between the surface names of the chemical entities to be classified and that of the reactant groups as class labels. We propose to train a classification model based on the subword cross-attention map between each chemical name and the corresponding reaction group. Experiments demonstrate that REACTCLASS is highly effective, achieving state-of-the-art performance in classifying the chemical names into human-defined reactant groups without requiring human effort for training data annotation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2154990549",
                    "name": "Xuan Wang"
                },
                {
                    "authorId": "50161308",
                    "name": "Vivian Hu"
                },
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "49891156",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "46851930",
                    "name": "Jinfeng Xiao"
                },
                {
                    "authorId": "1499269725",
                    "name": "Danielle C. Loving"
                },
                {
                    "authorId": "2181650518",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2199258773",
                    "name": "Martin Burke"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "82cb295f1d5c3e736b77d8a3fe61c07cf61eb5f5",
            "title": "TaxoEnrich: Self-Supervised Taxonomy Completion via Structure-Semantic Representations",
            "abstract": "Taxonomies are fundamental to many real-world applications in various domains, serving as structural representations of knowledge. To deal with the increasing volume of new concepts needed to be organized as taxonomies, researchers turn to automatically completion of an existing taxonomy with new concepts. In this paper, we propose TaxoEnrich, a new taxonomy completion framework, which effectively leverages both semantic features and structural information in the existing taxonomy and offers a better representation of candidate position to boost the performance of taxonomy completion. Specifically, TaxoEnrichconsists of four components: (1) taxonomy-contextualized embedding which incorporates both semantic meanings of concept and taxonomic relations based on powerful pretrained language models; (2) a taxonomy-aware sequential encoder which learns candidate position representations by encoding the structural information of taxonomy; (3) a query-aware sibling encoder which adaptively aggregates candidate siblings to augment candidate position representations based on their importance to the query-position matching; (4) a query-position matching model which extends existing work with our new candidate position representations. Extensive experiments on four large real-world datasets from different domains show that TaxoEnrichachieves the best performance among all evaluation metrics and outperforms previous state-of-the-art methods by a large margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2800541",
                    "name": "Minhao Jiang"
                },
                {
                    "authorId": "19214393",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "47540245",
                    "name": "Jieyu Zhang"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        }
    ]
}