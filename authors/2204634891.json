{
    "authorId": "2204634891",
    "papers": [
        {
            "paperId": "208e861d55125a57c943b666b0bcaed879aa540a",
            "title": "PAMI: partition input and aggregate outputs for model interpretation",
            "abstract": "There is an increasing demand for interpretation of model predictions especially in high-risk applications. Various visualization approaches have been proposed to estimate the part of input which is relevant to a specific model prediction. However, most approaches require model structure and parameter details in order to obtain the visualization results, and in general much effort is required to adapt each approach to multiple types of tasks particularly when model backbone and input format change over tasks. In this study, a simple yet effective visualization framework called PAMI is proposed based on the observation that deep learning models often aggregate features from local regions for model predictions. The basic idea is to mask majority of the input and use the corresponding model output as the relative contribution of the preserved input part to the original model prediction. For each input, since only a set of model outputs are collected and aggregated, PAMI does not require any model detail and can be applied to various prediction tasks with different model backbones and input formats. Extensive experiments on multiple tasks confirm the proposed method performs better than existing visualization approaches in more precisely finding class-specific input regions, and when applied to different model backbones and input formats. The source code will be released publicly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156688166",
                    "name": "Wei Shi"
                },
                {
                    "authorId": "2204634891",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2152975427",
                    "name": "Weishi Zheng"
                },
                {
                    "authorId": "2192231278",
                    "name": "Ruixuan Wang"
                }
            ]
        },
        {
            "paperId": "26f7785ef8da35820599799549152b9ef695dae2",
            "title": "GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation",
            "abstract": "Recent advancements in Natural Language Processing (NLP) have led to the development of NLP-based recommender systems that have shown superior performance. However, current models commonly treat items as mere IDs and adopt discriminative modeling, resulting in limitations of (1) fully leveraging the content information of items and the language modeling capabilities of NLP models; (2) interpreting user interests to improve relevance and diversity; and (3) adapting practical circumstances such as growing item inventories. To address these limitations, we present GPT4Rec, a novel and flexible generative framework inspired by search engines. It first generates hypothetical\"search queries\"given item titles in a user's history, and then retrieves items for recommendation by searching these queries. The framework overcomes previous limitations by learning both user and item embeddings in the language space. To well-capture user interests with different aspects and granularity for improving relevance and diversity, we propose a multi-query generation technique with beam search. The generated queries naturally serve as interpretable representations of user interests and can be searched to recommend cold-start items. With GPT-2 language model and BM25 search engine, our framework outperforms state-of-the-art methods by $75.7\\%$ and $22.2\\%$ in Recall@K on two public datasets. Experiments further revealed that multi-query generation with beam search improves both the diversity of retrieved items and the coverage of a user's multi-interests. The adaptiveness and interpretability of generated queries are discussed with qualitative case studies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115977668",
                    "name": "Jinming Li"
                },
                {
                    "authorId": "2204634891",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2048439240",
                    "name": "Tiantian Wang"
                },
                {
                    "authorId": "46827326",
                    "name": "Guanglei Xiong"
                },
                {
                    "authorId": "145604369",
                    "name": "Alan Lu"
                },
                {
                    "authorId": "70874887",
                    "name": "G. Medioni"
                }
            ]
        },
        {
            "paperId": "3a755f8dcc9af9304c2cbd3a00e42e66feec1d5d",
            "title": "Patton: Language Model Pretraining on Text-Rich Networks",
            "abstract": "A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).Text documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval.However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks.Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration.To this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.Patton includes two pretraining strategies: network-contextualized masked language modeling and masked node prediction, to capture the inherent dependency between textual attributes and network structure.We conduct experiments on four downstream tasks in five datasets from both academic and e-commerce domains, where Patton outperforms baselines significantly and consistently.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "2204634891",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "49891156",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "2108262811",
                    "name": "Xinyang Zhang"
                },
                {
                    "authorId": "2152206948",
                    "name": "Qi Zhu"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "cd5f7e8ae82f0aea3cd8b16177b5d5c7d51d27df",
            "title": "Fairness-aware Maximal Biclique Enumeration on Bipartite Graphs",
            "abstract": "Maximal biclique enumeration is a fundamental problem in bipartite graph data analysis. Existing biclique enumeration methods mainly focus on non-attributed bipartite graphs and also ignore the fairness of graph attributes. In this paper, we introduce the concept of fairness into the biclique model for the first time and study the problem of fairness-aware biclique enumeration. Specifically, we propose two fairness-aware biclique models, called single-side fair biclique and bi-side fair biclique respectively. To efficiently enumerate all single-side fair bicliques, we first present two non-trivial pruning techniques, called fair \u03b1-\u03b2 core pruning and colorful fair \u03b1-\u03b2 core pruning, to reduce the graph size without losing accuracy. Then, we develop a branch and bound algorithm, called FairBCEM, to enumerate all single-side fair bicliques on the reduced bipartite graph. To further improve the efficiency, we propose an efficient branch and bound algorithm with a carefully-designed combinatorial enumeration technique. Note that all of our techniques can also be extended to enumerate all bi-side fair bicliques. We also extend the two fairness-aware biclique models by constraining the ratio of the number of vertices of each attribute to the total number of vertices and present corresponding enumeration algorithms. Extensive experimental results on five large real-world datasets demonstrate our methods\u2019 efficiency, effectiveness, and scalability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27036277",
                    "name": "Ziqi Yin"
                },
                {
                    "authorId": "2145908257",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "2204634891",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "2121775",
                    "name": "Ronghua Li"
                },
                {
                    "authorId": "8349792",
                    "name": "Guoren Wang"
                }
            ]
        },
        {
            "paperId": "e7528dedc94310200189128e57ef8e07a63f595e",
            "title": "Semantic-aware Node Synthesis for Imbalanced Heterogeneous Information Networks",
            "abstract": "Heterogeneous graph neural networks (HGNNs) have exhibited exceptional efficacy in modeling the complex heterogeneity in heterogeneous information networks (HINs). The critical advantage of HGNNs is their ability to handle diverse node and edge types in HINs by extracting and utilizing the abundant semantic information for effective representation learning. As a widespread phenomenon in many real-world scenarios, the class-imbalance distribution in HINs creates a performance bottleneck for existing HGNNs. Apart from the node imbalance in quantity, the more crucial and distinctive challenge in HINs is semantic imbalance. Minority classes in HINs often lack diverse and sufficient neighbor nodes, resulting in biased and incomplete semantic information. This semantic imbalance further compounds the difficulty of accurately classifying minority nodes, leading to the performance degradation of HGNNs. However, existing remedies are either tailored for non-graph data or designed specifically for homogeneous graphs, thus overlooking the inherent semantic imbalance in HINs. To tackle the imbalance of minority classes and supplement their inadequate semantics, we present the first method for the semantic imbalance problem in imbalanced HINs named Semantic-aware Node Synthesis (SNS). By assessing the influence on minority classes, SNS adaptively selects the heterogeneous neighbor nodes and augments the network with synthetic nodes while preserving the minority semantics. In addition, we introduce two dedicated regularization approaches for HGNNs that explore the inter-type and intra-type information and constrain the representation of synthetic nodes from both semantic and class perspectives to effectively suppress the potential noises from synthetic nodes, facilitating more expressive embeddings for classification. The comprehensive experimental study demonstrates that SNS consistently outperforms existing methods in different benchmark datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192083394",
                    "name": "Xin Gao"
                },
                {
                    "authorId": "2204634891",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "1490931831",
                    "name": "Tong Chen"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                },
                {
                    "authorId": "2260715383",
                    "name": "Hung Quoc Viet Nguyen"
                },
                {
                    "authorId": "2416851",
                    "name": "Hongzhi Yin"
                }
            ]
        },
        {
            "paperId": "50b030ef00b24ef8cbba797f17c2b37b452ef25a",
            "title": "Two-Level Supervised Contrastive Learning for Response Selection in Multi-Turn Dialogue",
            "abstract": "Selecting an appropriate response from many candidates given the utterances in a multi-turn dialogue is the key problem for a retrieval-based dialogue system. Existing work formalizes the task as matching between the utterances and a candidate and uses the cross-entropy loss in learning of the model. This paper applies contrastive learning to the problem by using the supervised contrastive loss. In this way, the learned representations of positive examples and representations of negative examples can be more distantly separated in the embedding space, and the performance of matching can be enhanced. We further develop a new method for supervised contrastive learning, referred to as two-level supervised contrastive learning, and employ the method in response selection in multi-turn dialogue. Our method exploits two techniques: sentence token shuffling (STS) and sentence re-ordering (SR) for supervised contrastive learning. Experimental results on three benchmark datasets demonstrate that the proposed method significantly outperforms the contrastive learning baseline and the state-of-the-art methods for the task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2204634891",
                    "name": "Wentao Zhang"
                },
                {
                    "authorId": "9521918",
                    "name": "Shuangyin Xu"
                },
                {
                    "authorId": "2039788",
                    "name": "Haoran Huang"
                }
            ]
        }
    ]
}