{
    "authorId": "2285630243",
    "papers": [
        {
            "paperId": "2b17b4a5af897ef108386643cda60ebfe493fddb",
            "title": "Linear-Time Graph Neural Networks for Scalable Recommendations",
            "abstract": "In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users. The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions. Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems. Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages. Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods. In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender systems to achieve comparable scalability as classic MF approaches while maintaining GNNs' powerful expressiveness for superior prediction accuracy. Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm. Our implementation based on PyTorch is available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284929250",
                    "name": "Jiahao Zhang"
                },
                {
                    "authorId": "2066270999",
                    "name": "Rui Xue"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2285630243",
                    "name": "Xin Xu"
                },
                {
                    "authorId": "2286126060",
                    "name": "Qing Li"
                },
                {
                    "authorId": "2284871288",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "2272987756",
                    "name": "Xiaorui Liu"
                }
            ]
        },
        {
            "paperId": "ab028c75d4c83e4100f7c88b082e50aaa1bc4c94",
            "title": "A Survey of Mamba",
            "abstract": "As one of the most representative DL techniques, Transformer architecture has empowered numerous advanced models, especially the large language models (LLMs) that comprise billions of parameters, becoming a cornerstone in deep learning. Despite the impressive achievements, Transformers still face inherent limitations, particularly the time-consuming inference resulting from the quadratic computation complexity of attention calculation. Recently, a novel architecture named Mamba, drawing inspiration from classical state space models (SSMs), has emerged as a promising alternative for building foundation models, delivering comparable modeling abilities to Transformers while preserving near-linear scalability concerning sequence length. This has sparked an increasing number of studies actively exploring Mamba's potential to achieve impressive performance across diverse domains. Given such rapid evolution, there is a critical need for a systematic review that consolidates existing Mamba-empowered models, offering a comprehensive understanding of this emerging model architecture. In this survey, we therefore conduct an in-depth investigation of recent Mamba-associated studies, covering three main aspects: the advancements of Mamba-based models, the techniques of adapting Mamba to diverse data, and the applications where Mamba can excel. Specifically, we first review the foundational knowledge of various representative deep learning models and the details of Mamba-1&2 as preliminaries. Then, to showcase the significance of Mamba for AI, we comprehensively review the related studies focusing on Mamba models' architecture design, data adaptability, and applications. Finally, we present a discussion of current limitations and explore various promising research directions to provide deeper insights for future investigations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2306950042",
                    "name": "Haohao Qu"
                },
                {
                    "authorId": "2301015154",
                    "name": "Liang-bo Ning"
                },
                {
                    "authorId": "2314692092",
                    "name": "Rui An"
                },
                {
                    "authorId": "2291324376",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2314693283",
                    "name": "Tyler Derr"
                },
                {
                    "authorId": "2316828277",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "2285630243",
                    "name": "Xin Xu"
                },
                {
                    "authorId": "2314786918",
                    "name": "Qing Li"
                }
            ]
        }
    ]
}