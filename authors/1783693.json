{
    "authorId": "1783693",
    "papers": [
        {
            "paperId": "678b05603697e4923a139ed9ac1755941f5ed11d",
            "title": "AutoMLBench: A Comprehensive Experimental Evaluation of Automated Machine Learning Frameworks",
            "abstract": "With the booming demand for machine learning applications, it has been recognized that the number of knowledgeable data scientists can not scale with the growing data volumes and application needs in our digital world. In response to this demand, several automated machine learning (AutoML) frameworks have been developed to fill the gap of human expertise by automating the process of building machine learning pipelines. Each framework comes with different heuristics-based design decisions. In this study, we present a comprehensive evaluation and comparison of the performance characteristics of six popular AutoML frameworks, namely, AutoWeka, AutoSKlearn, TPOT, Recipe, ATM, and SmartML, across 100 data sets from established AutoML benchmark suites. Our experimental evaluation considers different aspects for its comparison, including the performance impact of several design decisions, including time budget, size of search space, meta-learning, and ensemble construction. The results of our study reveal various interesting insights that can significantly guide and impact the design of AutoML frameworks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47706442",
                    "name": "Hassan Eldeeb"
                },
                {
                    "authorId": "2097409976",
                    "name": "M. Maher"
                },
                {
                    "authorId": "2077516967",
                    "name": "Oleh Matsuk"
                },
                {
                    "authorId": "2162778897",
                    "name": "Abdelrahman Aldallal"
                },
                {
                    "authorId": "1805967",
                    "name": "Radwa El Shawi"
                },
                {
                    "authorId": "1783693",
                    "name": "Sherif Sakr"
                }
            ]
        },
        {
            "paperId": "9461cc2c34c77c1c371ecb509e6240675e577eef",
            "title": "cSmartML-Glassbox: Increasing Transparency and Controllability in Automated Clustering",
            "abstract": "Machine learning algorithms have been widely employed in various applications and fields. Novel technologies in automated machine learning (AutoML) ease algorithm selection and hyperparameter optimization complexity. AutoML frame-works have achieved notable success in hyperparameter tuning and surpassed the performance of human experts. However, depending on such frameworks as black-box can leave machine learning practitioners without insights into the inner working of the AutoML process and hence influence their trust in the models produced. In addition, excluding humans from the loop creates several limitations. For example, most of the current AutoML frameworks ignore the user preferences on defining or controlling the search space, which consequently can impact the performance of the models produced and the acceptance of these models by the end-users. The research in the area of transparency and controllability of AutoML has attracted much interest lately, both in academia and industry. However, existing tools are usually restricted to supervised learning tasks such as classification and regression, while unsupervised learning, particularly clustering, remains a largely unexplored problem. Motivated by these shortcomings, we design and implement cSmartML-GlassBox, an interactive visualization tool that en-ables users to refine the search space of AutoML and analyze the results. cSmartML-GlassBox is equipped with a recommendation engine to recommend a time budget that is likely adequate for a new dataset to obtain well-performing pipeline. In addition, the tool supports multi-granularity visualization to enable machine learning practitioners to monitor the AutoML process, analyze the explored configurations and refine/control the search space. Furthermore, cSmartML-GlassBox is equipped with a logging mechanism such that repeated runs on the same dataset can be more effective by avoiding evaluating the same previously considered configurations. We demonstrate the effectiveness and usability of the cSmartML-GlassBox through a user evaluation study with 23 participants and an expert-based usability study based on four experts. We find that the proposed tool increases users' understanding and trust in the AutoML frameworks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1805967",
                    "name": "Radwa El Shawi"
                },
                {
                    "authorId": "1783693",
                    "name": "Sherif Sakr"
                }
            ]
        },
        {
            "paperId": "d35de742f28190d650c3b4e7ccedf338c8eefffa",
            "title": "TPE-AutoClust: A Tree-based Pipline Ensemble Framework for Automated Clustering",
            "abstract": "Novel technologies in automated machine learning ease the complexity of building well-performed machine learning pipelines. However, these are usually restricted to supervised learning tasks such as classification and regression, while unsu-pervised learning, particularly clustering, remains a largely un-explored problem due to the ambiguity involved when evaluating the clustering solutions. Motivated by this shortcoming, in this paper, we introduce TPE-AutoClust, a genetic programming-based automated machine learning framework for clustering. TPE-AutoCl ust optimizes a series of feature preprocessors and machine learning models to optimize the performance on an unsupervised clustering task. TPE-AutoClust mainly consists of three main phases: meta-learning phase, optimization phase and clustering ensemble construction phase. The meta-learning phase suggests some instantiations of pipelines that are likely to perform well on a new dataset. These pipelines are used to warmstart the optimization phase that adopts a multi-objective optimization technique to select pipelines based on the Pareto front of the trade-off between the pipeline length and performance. The ensemble construction phase develops a collaborative mechanism based on a clustering ensemble to combine optimized pipelines based on different internal cluster validity indices and construct a well-performing solution for a new dataset. The proposed framework is based on scikit-learn with 4 preprocessors and 6 clustering algorithms. Extensive experiments are conducted on 27 real and synthetic benchmark datasets to validate the superiority of TPE-AutoCl ust. The results show that TPE-AutoClust outperforms the state-of-the-art techniques for building automated clustering solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1805967",
                    "name": "Radwa El Shawi"
                },
                {
                    "authorId": "1783693",
                    "name": "Sherif Sakr"
                }
            ]
        },
        {
            "paperId": "59dd9998e071d62a54d1747c5c8109a7d274d4d4",
            "title": "cSmartML: A Meta Learning-Based Framework for Automated Selection and Hyperparameter Tuning for Clustering",
            "abstract": "Novel technologies in automated machine learning ease the complexity of algorithm selection and hyper-parameter optimization. However, these are usually restricted to supervised learning tasks such as classification and regression, while unsupervised learning remains a largely unexplored problem. In this paper, we offer a solution for automating machine learning specifically for the case of unsupervised learning with clustering, in a domain-agnostic manner. This is achieved through a combination of state-of-the-art processes based on meta-learning for algorithm and evaluation criteria selection, and evolutionary algorithm for hyper-parameter tuning. We introduce a robust and scalable interactive tool, named cSmartML, built on scikit-learn with 8 clustering algorithms. In order to capture more than a single measure of goodness of the output clustering solution, cSmartML optimizes multiple objective functions. A pareto-approach evaluates each objective simultaneously for each clustering solution. On each of the 27 real and synthetic benchmark datasets, we show that the performance of cSmartML is often much better than using standard selection and hyper-parameter optimization methods. In addition, experimentation reveals that cSmartML takes advantage of the defined objective functions on multi-objective functions framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1805967",
                    "name": "Radwa El Shawi"
                },
                {
                    "authorId": "2149994691",
                    "name": "H. Lekunze"
                },
                {
                    "authorId": "1783693",
                    "name": "Sherif Sakr"
                }
            ]
        },
        {
            "paperId": "6175f5cff07b7a129c450c63baa142976d53db14",
            "title": "To tune or not to tune? An Approach for Recommending Important Hyperparameters",
            "abstract": "Novel technologies in automated machine learning ease the complexity of algorithm selection and hyperparameter optimization. Hyperparameters are important for machine learning models as they significantly influence the performance of machine learning models. Many optimization techniques have achieved notable success in hyperparameter tuning and surpassed the performance of human experts. However, depending on such techniques as blackbox algorithms can leave machine learning practitioners without insight into the relative importance of different hyperparameters. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters to discover the trend and gain insights, with empirical results based on six classifiers and 200 datasets. Our results enable users to decide whether it is worth conducting a possibly time-consuming tuning strategy, to focus on the most important hyperparameters, and to choose adequate hyperparameter spaces for tuning. The results of our experiments show that gradient boosting and Adaboost outperform other classifiers across 200 problems. However, they need tuning to boost their performance. Overall, the results obtained from this study provide a quantitative basis to focus efforts toward guided automated hyperparameter optimization and contribute toward the development of better-automated machine learning frameworks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068489494",
                    "name": "Mohamad Bahmani"
                },
                {
                    "authorId": "1805967",
                    "name": "Radwa El Shawi"
                },
                {
                    "authorId": "2124981562",
                    "name": "Nshan Potikyan"
                },
                {
                    "authorId": "1783693",
                    "name": "Sherif Sakr"
                }
            ]
        },
        {
            "paperId": "90e271126c7200d5ed76fc86a66a6a33a859c508",
            "title": "Towards Automated Concept-based Decision TreeExplanations for CNNs",
            "abstract": "Currently, deep learning models have been widely used in different application domains due to their notable performance. Ex-plaining the decisions made by deep learning models is important for end-users to enable them to comprehend and diagnose the trustworthiness of the model. Most of the current interpretability techniques provide explanations in the form of importance score for the input pixels or features. However, summarizing such importance scores for input features to provide human-interpretable explanations is challenging. To this end, we propose Automated Concept-based Decision Tree Explanations (ACDTE), a novel lo-cal explanation framework that provides human-understandable and concept-based explanations for classification networks. Our framework provides end users with the flexibility of customizing the explanations by allowing users to provide the dataset in which visual human-understandable concepts are automatically extracted. Then, such concepts are interpreted through a shallow decision tree that includes concepts that are deemed important to the model in predicting the decision of specific instance. In addition, ACDTE generates counterfactual explanations, suggesting the the minimum changes in the instance\u2019s concept-based explanation that lead to a different prediction. Our experiments demonstrate that such a shallow decision tree is faithful to the original neural network at low tree depth. The human inter-pretability of the explanations provided from our framework is evaluated through humans experiments, showing that our framework generates faithful and interpretable explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1805967",
                    "name": "Radwa El Shawi"
                },
                {
                    "authorId": "2068579152",
                    "name": "Youssef Mohamed"
                },
                {
                    "authorId": "1783693",
                    "name": "Sherif Sakr"
                }
            ]
        },
        {
            "paperId": "bdc97cc86dcd11daba6bba26a4ae82b1c4f345b9",
            "title": "The impact of Auto-Sklearn's Learning Settings: Meta-learning, Ensembling, Time Budget, and Search Space Size",
            "abstract": "With the booming demand for machine learning (ML) applications, it is recognized that the number of knowledgeable data scientists cannot scale with the growing data volumes and application needs in our digital world. Therefore, several automated machine learning (AutoML) frameworks have been developed to fill the gap of human expertise by automating most of the process of building a ML pipeline. In this paper, we present a micro-level analysis of the AutoML process by empirically evaluating and analyzing the impact of several learning settings and parameters, i.e., meta-learning , ensembling , time budget and size of search space on the performance. Particularly, we focus on AutoSklearn, the state-of-the-art AutoML framework. Our study reveals that no single configuration of these design decisions achieves the best performance across all conditions and datasets. However, some design parameters have a statistically consistent improvement over the performance, such as using ensemble models. Some others are conditionally effective, e.g., meta-learning adds a statistically significant improvement, only with a small time budget.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47706442",
                    "name": "Hassan Eldeeb"
                },
                {
                    "authorId": "2077516967",
                    "name": "Oleh Matsuk"
                },
                {
                    "authorId": "2097409976",
                    "name": "M. Maher"
                },
                {
                    "authorId": "1470840298",
                    "name": "Abdelrhman Eldallal"
                },
                {
                    "authorId": "1783693",
                    "name": "Sherif Sakr"
                }
            ]
        },
        {
            "paperId": "0e74d9dea6398e3eb6290ac8ea792692357dab95",
            "title": "Towards making sense of Spark-SQL performance for processing vast distributed RDF datasets",
            "abstract": "Recently, a wide range of Web applications (e.g. DBPedia, Uniprot, and Probase) are built on top of vast RDF knowledge bases and using the SPARQL query language. The continuous growth of these knowledge bases led to the investigation of new paradigms and technologies for storing, accessing, and querying RDF data. In practice, modern big data systems (e.g, Hadoop, Spark) can handle vast relational repositories, however, their application in the Semantic Web context is still limited. One possible reason is that such frameworks rely on distributed systems, which are good for relational data, however, their performance on dealing with graph data models like RDF has not been well-studied yet. In this paper, we present a systematic evaluation of the performance of SparkSQL engine for processing SPARQL queries. We stated it using three relevant RDF relational schemas, and two different storage backends, namely, Hive, and HDFS. In addition, we show the impact of using three different RDF-based partitioning techniques with our relational scenario. Additionally, we discuss the results of our experiments: (i) we present insights about the trade-offs that characterize different experimental configurations, and (ii) we identify the best and the worst ones for the SP2Bench's benchmark scenario.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065777089",
                    "name": "Mohamed Ragab"
                },
                {
                    "authorId": "50271459",
                    "name": "Riccardo Tommasini"
                },
                {
                    "authorId": "1716216761",
                    "name": "Sadiq Eyvazov"
                },
                {
                    "authorId": "1783693",
                    "name": "Sherif Sakr"
                }
            ]
        },
        {
            "paperId": "1c1a28dad77bf3510626aa56f92a2a66c9a00b0e",
            "title": "Graph Generators",
            "abstract": "The abundance of interconnected data has fueled the design and implementation of graph generators reproducing real-world linking properties or gauging the effectiveness of graph algorithms, techniques, and applications manipulating these data. We consider graph generation across multiple subfields, such as Semantic Web, graph databases, social networks, and community detection, along with general graphs. Despite the disparate requirements of modern graph generators throughout these communities, we analyze them under a common umbrella, reaching out the functionalities, the practical usage, and their supported operations. We argue that this classification is serving the need of providing scientists, researchers, and practitioners with the right data generator at hand for their work. This survey provides a comprehensive overview of the state-of-the-art graph generators by focusing on those that are pertinent and suitable for several data-intensive tasks. Finally, we discuss open challenges and missing requirements of current graph generators along with their future extensions to new emerging fields.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1699192",
                    "name": "A. Bonifati"
                },
                {
                    "authorId": "1722720",
                    "name": "I. Holubov\u00e1"
                },
                {
                    "authorId": "1403335961",
                    "name": "Arnau Prat-P\u00e9rez"
                },
                {
                    "authorId": "1783693",
                    "name": "Sherif Sakr"
                }
            ]
        },
        {
            "paperId": "354458830b08d05a87947e9c8fb0d24ef0a36112",
            "title": "Declarative Languages for Big Streaming Data",
            "abstract": "The Big Data movement proposes data streaming systems to tame velocity and to enable reactive decision making. However, approaching such systems is still too complex due to the paradigm shift they require, i.e., moving from scalable batch processing to continuous data analysis and pattern detection. Recently, declarative Languages are playing a crucial role in fostering the adoption of Stream Processing solutions. In particular, several key players introduce SQL extensions for stream processing. These new languages are currently playing a central role in fostering the stream processing paradigm shift. In this tutorial, we give an overview of the various languages for declarative querying interfaces big streaming data. To this extent, we discuss how the different Big Stream Processing Engines (BigSPE) interpret, execute, and optimize continuous queries expressed with SQL-like languages such as KSQL , Flink-SQL , and Spark SQL . Finally, we present the open research challenges in the domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50271459",
                    "name": "Riccardo Tommasini"
                },
                {
                    "authorId": "1783693",
                    "name": "Sherif Sakr"
                },
                {
                    "authorId": "2539248",
                    "name": "Emanuele Della Valle"
                },
                {
                    "authorId": "2041269",
                    "name": "Hojjat Jafarpour"
                }
            ]
        }
    ]
}