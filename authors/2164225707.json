{
    "authorId": "2164225707",
    "papers": [
        {
            "paperId": "1e9b3d236f15b27856f5551ad23965fac84ff6e5",
            "title": "Hierarchical Projection Enhanced Multi-behavior Recommendation",
            "abstract": "Various types of user behaviors are recorded in most real-world recommendation scenarios. To fully utilize the multi-behavior information, the exploration of multiplex interaction among them is essential. Many multi-task learning based multi-behavior methods are proposed recently to use multiple types of supervision signals and perform information transfer among them. Despite the great successes, these methods fail to design prediction tasks comprehensively, leading to insufficient utilization of multi-behavior correlative information. Besides, these methods are either based on the weighting of expert information extracted from the coupled input or modeling of information transfer between multiple behavior levels through task-specific extractors, which are usually accompanied by negative transfer phenomenon1. To address the above problems, we propose a multi-behavior recommendation framework, called Hierarchical Projection Enhanced Multi-behavior Recommendation (HPMR). The key module, Projection-based Transfer Network (PTN), uses the projection mechanism to \"explicitly\" model the correlations of upstream and downstream behaviors, refines the upstream behavior representations, and fully uses the refined representations to enhance the learning of downstream tasks. Offline experiments on public and industrial datasets and online A/B test further verify the effectiveness of HPMR in modeling the associations from upstream to downstream and alleviating the negative transfer. The source code and datasets are available at https://github.com/MC-CV/HPMR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2180167214",
                    "name": "Chang Meng"
                },
                {
                    "authorId": "2187872267",
                    "name": "Hengyu Zhang"
                },
                {
                    "authorId": "2109155646",
                    "name": "Wei Guo"
                },
                {
                    "authorId": "3339005",
                    "name": "Huifeng Guo"
                },
                {
                    "authorId": null,
                    "name": "Haotian Liu"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2181501520",
                    "name": "Hongkun Zheng"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "2164225707",
                    "name": "Xiu Li"
                },
                {
                    "authorId": "144142354",
                    "name": "Rui Zhang"
                }
            ]
        },
        {
            "paperId": "5545f3edc6b5eb2847cb9e5fd27ce212c27a76c7",
            "title": "Dual-Path Reconstruction Guided Segmentation Network for Unsupervised Anomaly Detection and Localization",
            "abstract": "Visual anomaly detection methods with localization are critically important for industrial manufacturing quality control. Because of the rarity of anomalies and the irregular variation of anomaly patterns, unsupervised methods have been widely explored. For the anomaly detection and localization tasks, the image reconstruction-based approaches have shown competitive performance. However, reconstruction results of those methods are coarse and visually blurred, which leads to a high rate of false detection and false pixel-level localization. To address those issues, we propose a framework called Dual-Path Re-construction Guided Segmentation Network (DRGS-Net), which determines the abnormal regions by segmenting the anomaly image with its reconstructed result as the reference template. DRGS-Net mainly consists of a novel dual-path reconstruction sub-network and a specifically designed anomaly segmentation sub-network. They are jointly trained end-to-end, with the former fusing texture repair and image reconstruction information to obtain fine-grained reconstruction results and the latter learning a decision boundary between normal and anomalous regions based on reconstruction results. On the standard benchmark dataset MVTecAD and an additional dataset DAGM, DRGS-Net shows competitive performance in image-level detection and achieves outstanding improvement in pixel-level localization. Further experiments with the few samples setting demonstrate that DRGS-Net retains strong performance with only few anomaly-free training images.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2165889378",
                    "name": "Junwei Xiao"
                },
                {
                    "authorId": "2114190756",
                    "name": "Lei Deng"
                },
                {
                    "authorId": "1723853",
                    "name": "Z. Chen"
                },
                {
                    "authorId": "2164225707",
                    "name": "Xiu Li"
                },
                {
                    "authorId": "2108426292",
                    "name": "Baohua Chen"
                },
                {
                    "authorId": "2165879253",
                    "name": "Hanxi Yin"
                }
            ]
        },
        {
            "paperId": "7ee02a0b3a0b39d3f44704eab82a405a44d84c3a",
            "title": "D2Animator: Dual Distillation of StyleGAN For High-Resolution Face Animation",
            "abstract": "The style-based generator architectures (e.g. StyleGAN v1, v2) largely promote the controllability and explainability of Generative Adversarial Networks (GANs). Many researchers have applied the pretrained style-based generators to image manipulation and video editing by exploring the correlation between linear interpolation in the latent space and semantic transformation in the synthesized image manifold. However, most previous studies focused on manipulating separate discrete attributes, which is insufficient to animate a still image to generate videos with complex and diverse poses and expressions. In this work, we devise a dual distillation strategy (D2Animator) for generating animated high-resolution face videos conditioned on identities and poses from different images. Specifically, we first introduce a Clustering-based Distiller (CluDistiller) to distill diverse interpolation directions in the latent space, and synthesize identity-consistent faces with various poses and expressions, such as blinking, frowning, looking up/down, etc. Then we propose an Augmentation-based Distiller (AugDistiller) that learns to encode arbitrary face deformation into a combination of interpolation directions via training on augmentation samples synthesized by CluDistiller. Through assembling the two distillation methods, D2Animator can generate high-resolution face animation videos without training on video sequences. Extensive experiments on self-driving, cross-identity and sequence-driving tasks demonstrate the superiority of the proposed D2Animator over existing StyleGAN manipulation and face animation methods in both generation quality and animation fidelity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111499679",
                    "name": "Zhuo Chen"
                },
                {
                    "authorId": "2518211",
                    "name": "Chaoyue Wang"
                },
                {
                    "authorId": "11359740",
                    "name": "Haimei Zhao"
                },
                {
                    "authorId": "2055907880",
                    "name": "Bo Yuan"
                },
                {
                    "authorId": "2164225707",
                    "name": "Xiu Li"
                }
            ]
        },
        {
            "paperId": "8db962faf0ec4708f0e14410eabd23521d244619",
            "title": "Visual Knowledge Graph for Human Action Reasoning in Videos",
            "abstract": "Action recognition has been traditionally treated as a high-level video classification problem. However, such a manner lacks the detailed and semantic understanding of body movement, which is the critical knowledge to explain and infer complex human actions. To fill this gap, we propose to summarize a novel visual knowledge graph from over 15M detailed human annotations, for describing action as the distinct composition of body parts, part movements and interactive objects in videos. Based on it, we design a generic multi-modal Action Knowledge Understanding (AKU) framework, which can progressively infer human actions from body part movements in the videos, with assistance of visual-driven semantic knowledge mining. Finally, we validate AKU on the recent Kinetics-TPS benchmark, which contains body part parsing annotations for detailed understanding of human action in videos. The results show that, our AKU significantly boosts various video backbones with explainable action knowledge in both supervised and few shot settings, and outperforms the recent knowledge-based action recognition framework, e.g., our AKU achieves 83.9% accuracy on Kinetics-TPS while PaStaNet achieves 63.8% accuracy under the same backbone. The codes and models will be released at https://github.com/mayuelala/AKU.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2165983663",
                    "name": "Yue Ma"
                },
                {
                    "authorId": "47903936",
                    "name": "Yali Wang"
                },
                {
                    "authorId": "46220633",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "1920802076",
                    "name": "Ziyu Lyu"
                },
                {
                    "authorId": "2187493699",
                    "name": "Siran Chen"
                },
                {
                    "authorId": "2164225707",
                    "name": "Xiu Li"
                },
                {
                    "authorId": "2093202334",
                    "name": "Yu Qiao"
                }
            ]
        },
        {
            "paperId": "c273d1c5d4670fe45ffa71f18361bdeba945864e",
            "title": "A Medical Semantic-Assisted Transformer for Radiographic Report Generation",
            "abstract": "Automated radiographic report generation is a challenging cross-domain task that aims to automatically generate accurate and semantic-coherence reports to describe medical images. Despite the recent progress in this field, there are still many challenges at least in the following aspects. First, radiographic images are very similar to each other, and thus it is difficult to capture the fine-grained visual differences using CNN as the visual feature extractor like many existing methods. Further, semantic information has been widely applied to boost the performance of generation tasks (e.g. image captioning), but existing methods often fail to provide effective medical semantic features. Toward solving those problems, in this paper, we propose a memory-augmented sparse attention block utilizing bilinear pooling to capture the higher-order interactions between the input fine-grained image features while producing sparse attention. Moreover, we introduce a novel Medical Concepts Generation Network (MCGN) to predict fine-grained semantic concepts and incorporate them into the report generation process as guidance. Our proposed method shows promising performance on the recently released largest benchmark MIMIC-CXR. It outperforms multiple state-of-the-art methods in image captioning and medical report generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135223272",
                    "name": "Zhanyu Wang"
                },
                {
                    "authorId": "2152363018",
                    "name": "Mingkang Tang"
                },
                {
                    "authorId": "2152505258",
                    "name": "Lei Wang"
                },
                {
                    "authorId": "2164225707",
                    "name": "Xiu Li"
                },
                {
                    "authorId": "6578587",
                    "name": "Luping Zhou"
                }
            ]
        },
        {
            "paperId": "cd0fb61797175ca4dba28aa40ed21a243bdb6422",
            "title": "Automated Radiographic Report Generation Purely on Transformer: A Multicriteria Supervised Approach",
            "abstract": "Automated radiographic report generation is challenging in at least two aspects. First, medical images are very similar to each other and the visual differences of clinic importance are often fine-grained. Second, the disease-related words may be submerged by many similar sentences describing the common content of the images, causing the abnormal to be misinterpreted as the normal in the worst case. To tackle these challenges, this paper proposes a pure transformer-based framework to jointly enforce better visual-textual alignment, multi-label diagnostic classification, and word importance weighting, to facilitate report generation. To the best of our knowledge, this is the first pure transformer-based framework for medical report generation, which enjoys the capacity of transformer in learning long range dependencies for both image regions and sentence words. Specifically, for the first challenge, we design a novel mechanism to embed an auxiliary image-text matching objective into the transformer\u2019s encoder-decoder structure, so that better correlated image and text features could be learned to help a report to discriminate similar images. For the second challenge, we integrate an additional multi-label classification task into our framework to guide the model in making correct diagnostic predictions. Also, a term-weighting scheme is proposed to reflect the importance of words for training so that our model would not miss key discriminative information. Our work achieves promising performance over the state-of-the-arts on two benchmark datasets, including the largest dataset MIMIC-CXR.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2135223272",
                    "name": "Zhanyu Wang"
                },
                {
                    "authorId": "50434256",
                    "name": "Hongwei Han"
                },
                {
                    "authorId": "2152505258",
                    "name": "Lei Wang"
                },
                {
                    "authorId": "2164225707",
                    "name": "Xiu Li"
                },
                {
                    "authorId": "6578587",
                    "name": "Luping Zhou"
                }
            ]
        },
        {
            "paperId": "f21738d328965c48148f68fb3f13d49da8015c22",
            "title": "Disentangling Past-Future Modeling in Sequential Recommendation via Dual Networks",
            "abstract": "Sequential recommendation (SR) plays an important role in personalized recommender systems because it captures dynamic and diverse preferences from users' real-time increasing behaviors. Unlike the standard autoregressive training strategy, future data (also available during training) has been used to facilitate model training as it provides richer signals about users' current interests and can be used to improve the recommendation quality. However, existing methods suffer from a severe training-inference gap, i.e., both past and future contexts are modeled by the same encoder when training, while only historical behaviors are available during inference. This discrepancy leads to potential performance degradation. To alleviate the training-inference gap, we propose a new framework DualRec, which achieves past-future disentanglement and past-future mutual enhancement by a novel dual network. Specifically, a dual network structure is exploited to model the past and future context separately.And a bi-directional knowledge transferring mechanism enhances the knowledge learnt by the dual network. Extensive experiments on four real-world datasets demonstrate the superiority of our approach over baseline methods. Besides, we demonstrate the compatibility of DualRec by instantiating using different backbones. Further empirical analysis verifies the high utility of modeling future contexts under our DualRec framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187872267",
                    "name": "Hengyu Zhang"
                },
                {
                    "authorId": "1601384914",
                    "name": "Enming Yuan"
                },
                {
                    "authorId": "2109155646",
                    "name": "Wei Guo"
                },
                {
                    "authorId": "2175453540",
                    "name": "Zhicheng He"
                },
                {
                    "authorId": "79494403",
                    "name": "Jiarui Qin"
                },
                {
                    "authorId": "3339005",
                    "name": "Huifeng Guo"
                },
                {
                    "authorId": "92633145",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2164225707",
                    "name": "Xiu Li"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                }
            ]
        }
    ]
}