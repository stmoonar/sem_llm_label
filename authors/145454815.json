{
    "authorId": "145454815",
    "papers": [
        {
            "paperId": "7ac3e09d74ca15fdafa5f730617cd65059a144f3",
            "title": "Weakly Supervised Anomaly Detection: A Survey",
            "abstract": "Anomaly detection (AD) is a crucial task in machine learning with various applications, such as detecting emerging diseases, identifying financial frauds, and detecting fake news. However, obtaining complete, accurate, and precise labels for AD tasks can be expensive and challenging due to the cost and difficulties in data annotation. To address this issue, researchers have developed AD methods that can work with incomplete, inexact, and inaccurate supervision, collectively summarized as weakly supervised anomaly detection (WSAD) methods. In this study, we present the first comprehensive survey of WSAD methods by categorizing them into the above three weak supervision settings across four data modalities (i.e., tabular, graph, time-series, and image/video data). For each setting, we provide formal definitions, key algorithms, and potential future directions. To support future research, we conduct experiments on a selected setting and release the source code, along with a collection of WSAD methods and data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152154941",
                    "name": "Minqi Jiang"
                },
                {
                    "authorId": "2204929921",
                    "name": "Chaochuan Hou"
                },
                {
                    "authorId": "2204948431",
                    "name": "Ao Zheng"
                },
                {
                    "authorId": "35346885",
                    "name": "Xiyang Hu"
                },
                {
                    "authorId": "2004577591",
                    "name": "Songqiao Han"
                },
                {
                    "authorId": "2146285145",
                    "name": "Hailiang Huang"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                },
                {
                    "authorId": "152297693",
                    "name": "Philip S. Yu"
                },
                {
                    "authorId": "145454815",
                    "name": "Yue Zhao"
                }
            ]
        },
        {
            "paperId": "94247afc9114dcbc7dee149f74ff42524edf493c",
            "title": "The Need for Unsupervised Outlier Model Selection: A Review and Evaluation of Internal Evaluation Strategies",
            "abstract": "Given an unsupervised outlier detection task, how should one select i) a detection algorithm, and ii) associated hyperparameter values (jointly called a model)? E ective outlier model selection is essential as di erent algorithms may work well for varying detection tasks, and moreover their performance can be quite sensitive to the values of the hyperparameters (HPs). On the other hand, unsupervised model selection is notoriously difficult, in the absence of hold-out validation data with ground-truth labels. Therefore, the problem is vastly understudied in the outlier mining literature. There exists a body of work that propose internal model evaluation strate- gies for selecting a model. These so-called internal strategies solely rely on the input data (without labels) and the output (outlier scores) of the candidate models. In this paper, we rst survey internal model evaluation strategies including both those proposed speci cally for outlier detection, as well as those that can be adapted from the unsupervised deep representation learning literature. Then, we investigate their e ectiveness empirically in comparison to simple baselines such as random selection and the popular state-of-the-art detector Isolation Forest (iForest) with default HPs. To this end, we set up (and open-source) a large testbed with 39 detection tasks and 297 candidate models comprised of 8 different detectors and various HP con gurations. We evaluate internal strategies from 7 di erent families on their ability to discriminate between models w.r.t. detection performance, without using any labels. Our study reports a striking nding, that none of the existing and adapted strategies would be practically useful: stand-alone ones are not signi cantly di erent from random, and consensus-based ones do not outperform iForest (w/ default HPs) while being more expensive (as all candidate models need to be trained for evaluation). Our survey stresses the importance of and the standing need for e ective unsupervised outlier model selection, and acts as a call for future work on the problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1384374825",
                    "name": "Martin Q. Ma"
                },
                {
                    "authorId": "145454815",
                    "name": "Yue Zhao"
                },
                {
                    "authorId": "2124956562",
                    "name": "Xiaorong Zhang"
                },
                {
                    "authorId": "3255268",
                    "name": "L. Akoglu"
                }
            ]
        },
        {
            "paperId": "c1a13ed7ef722e6442ef4f3ffdd7614b9d8dc8ac",
            "title": "Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks",
            "abstract": "Neural architecture search (NAS) for Graph neural networks (GNNs), called NAS-GNNs, has achieved significant performance over manually designed GNN architectures. However, these methods inherit issues from the conventional NAS methods, such as high computational cost and optimization difficulty. More importantly, previous NAS methods have ignored the uniqueness of GNNs, where GNNs possess expressive power without training. With the randomly-initialized weights, we can then seek the optimal architecture parameters via the sparse coding objective and derive a novel NAS-GNNs method, namely neural architecture coding (NAC). Consequently, our NAC holds a no-update scheme on GNNs and can efficiently compute in linear time. Empirical evaluations on multiple GNN benchmark datasets demonstrate that our approach leads to state-of-the-art performance, which is up to $200\\times$ faster and $18.8\\%$ more accurate than the strong baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087067253",
                    "name": "Peng Xu"
                },
                {
                    "authorId": "2217098681",
                    "name": "Lin Zhang"
                },
                {
                    "authorId": "26966619",
                    "name": "Xuan Liu"
                },
                {
                    "authorId": "2188389873",
                    "name": "Jiaqi Sun"
                },
                {
                    "authorId": "145454815",
                    "name": "Yue Zhao"
                },
                {
                    "authorId": "2218651621",
                    "name": "Haiqing Yang"
                },
                {
                    "authorId": "2218064535",
                    "name": "Bei Yu"
                }
            ]
        },
        {
            "paperId": "c504e0fd14f2f364d1b51139f2e4b28123725e64",
            "title": "ADGym: Design Choices for Deep Anomaly Detection",
            "abstract": "Deep learning (DL) techniques have recently found success in anomaly detection (AD) across various fields such as finance, medical services, and cloud computing. However, most of the current research tends to view deep AD algorithms as a whole, without dissecting the contributions of individual design choices like loss functions and network architectures. This view tends to diminish the value of preliminary steps like data preprocessing, as more attention is given to newly designed loss functions, network architectures, and learning paradigms. In this paper, we aim to bridge this gap by asking two key questions: (i) Which design choices in deep AD methods are crucial for detecting anomalies? (ii) How can we automatically select the optimal design choices for a given AD dataset, instead of relying on generic, pre-existing solutions? To address these questions, we introduce ADGym, a platform specifically crafted for comprehensive evaluation and automatic selection of AD design elements in deep methods. Our extensive experiments reveal that relying solely on existing leading methods is not sufficient. In contrast, models developed using ADGym significantly surpass current state-of-the-art techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2249727206",
                    "name": "Minqi Jiang"
                },
                {
                    "authorId": "2204929921",
                    "name": "Chaochuan Hou"
                },
                {
                    "authorId": "2204948431",
                    "name": "Ao Zheng"
                },
                {
                    "authorId": "2004577591",
                    "name": "Songqiao Han"
                },
                {
                    "authorId": "117894122",
                    "name": "Hailiang Huang"
                },
                {
                    "authorId": "2247916993",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "35346885",
                    "name": "Xiyang Hu"
                },
                {
                    "authorId": "145454815",
                    "name": "Yue Zhao"
                }
            ]
        },
        {
            "paperId": "f03cec99e75dc74ab653b2598da16dc606c15da5",
            "title": "DSV: An Alignment Validation Loss for Self-supervised Outlier Model Selection",
            "abstract": "Self-supervised learning (SSL) has proven effective in solving various problems by generating internal supervisory signals. Unsupervised anomaly detection, which faces the high cost of obtaining true labels, is an area that can greatly benefit from SSL. However, recent literature suggests that tuning the hyperparameters (HP) of data augmentation functions is crucial to the success of SSL-based anomaly detection (SSAD), yet a systematic method for doing so remains unknown. In this work, we propose DSV (Discordance and Separability Validation), an unsupervised validation loss to select high-performing detection models with effective augmentation HPs. DSV captures the alignment between an augmentation function and the anomaly-generating mechanism with surrogate losses, which approximate the discordance and separability of test data, respectively. As a result, the evaluation via DSV leads to selecting an effective SSAD model exhibiting better alignment, which results in high detection accuracy. We theoretically derive the degree of approximation conducted by the surrogate losses and empirically show that DSV outperforms a wide range of baselines on 21 real-world tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31888223",
                    "name": "Jaemin Yoo"
                },
                {
                    "authorId": "145454815",
                    "name": "Yue Zhao"
                },
                {
                    "authorId": "21613538",
                    "name": "Lingxiao Zhao"
                },
                {
                    "authorId": "3255268",
                    "name": "L. Akoglu"
                }
            ]
        },
        {
            "paperId": "0bff4af924788d9779041513b6894385eac51ffd",
            "title": "ADBench: Anomaly Detection Benchmark",
            "abstract": "Given a long list of anomaly detection algorithms developed in the last few decades, how do they perform with regard to (i) varying levels of supervision, (ii) different types of anomalies, and (iii) noisy and corrupted data? In this work, we answer these key questions by conducting (to our best knowledge) the most comprehensive anomaly detection benchmark with 30 algorithms on 57 benchmark datasets, named ADBench. Our extensive experiments (98,436 in total) identify meaningful insights into the role of supervision and anomaly types, and unlock future directions for researchers in algorithm selection and design. With ADBench, researchers can easily conduct comprehensive and fair evaluations for newly proposed methods on the datasets (including our contributed ones from natural language and computer vision domains) against the existing baselines. To foster accessibility and reproducibility, we fully open-source ADBench and the corresponding results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2004577591",
                    "name": "Songqiao Han"
                },
                {
                    "authorId": "35346885",
                    "name": "Xiyang Hu"
                },
                {
                    "authorId": "2146285145",
                    "name": "Hailiang Huang"
                },
                {
                    "authorId": "35072998",
                    "name": "Mingqi Jiang"
                },
                {
                    "authorId": "145454815",
                    "name": "Yue Zhao"
                }
            ]
        },
        {
            "paperId": "155d9c8000e7e862886e01ea9b8011652673d9ae",
            "title": "Toward Unsupervised Outlier Model Selection",
            "abstract": "Today there exists no shortage of outlier detection algorithms in the literature, yet the complementary and critical problem of unsupervised outlier model selection (UOMS) is vastly understudied. In this work, we propose ELECT, a new approach to select an effective candidate model, i.e. an outlier detection algorithm and its hyperparameter(s), to employ on a new dataset without any labels. At its core, ELECT is based on meta-learning; transferring prior knowledge (e.g. model performance) on historical datasets that are similar to the new one to facilitate UOMS. Uniquely, it employs a dataset similarity measure that is performance-based, which is more direct and goal-driven than other measures used in the past. ELECT adaptively searches for similar historical datasets, as such, it can serve an output on-demand, being able to accommodate varying time budgets. Extensive experiments show that ELECT significantly outperforms a wide range of basic UOMS baselines, including no model selection (always using the same popular model such as iForest) as well as more recent selection strategies based on meta-features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145454815",
                    "name": "Yue Zhao"
                },
                {
                    "authorId": "2107968036",
                    "name": "Sean Zhang"
                },
                {
                    "authorId": "3255268",
                    "name": "L. Akoglu"
                }
            ]
        },
        {
            "paperId": "1d4709b83ad9dbf9f2f92b63944d4469bedd4c73",
            "title": "CanMethdb: a database for genome-wide DNA methylation annotation in cancers",
            "abstract": "Abstract Motivation DNA methylation within gene body and promoters in cancer cells is well documented. An increasing number of studies showed that cytosine\u2013phosphate\u2013guanine (CpG) sites falling within other regulatory elements could also regulate target gene activation, mainly by affecting transcription factors (TFs) binding in human cancers. This led to the urgent need for comprehensively and effectively collecting distinct cis-regulatory elements and TF-binding sites (TFBS) to annotate DNA methylation regulation. Results We developed a database (CanMethdb, http://meth.liclab.net/CanMethdb/) that focused on the upstream and downstream annotations for CpG\u2013genes in cancers. This included upstream cis-regulatory elements, especially those involving distal regions to genes, and TFBS annotations for the CpGs and downstream functional annotations for the target genes, computed through integrating abundant DNA methylation and gene expression profiles in diverse cancers. Users could inquire CpG\u2013target gene pairs for a cancer type through inputting a genomic region, a CpG, a gene name, or select hypo/hypermethylated CpG sets. The current version of CanMethdb documented a total of 38 986 060 CpG\u2013target gene pairs (with 6 769 130 unique pairs), involving 385 217 CpGs and 18 044 target genes, abundant cis-regulatory elements and TFs for 33 TCGA cancer types. CanMethdb might help biologists perform in-depth studies of target gene regulations based on DNA methylations in cancer. Availability and implementation The main program is available at https://github.com/chunquanlipathway/CanMethdb. Supplementary information Supplementary data are available at Bioinformatics online.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48018908",
                    "name": "Jianmei Zhao"
                },
                {
                    "authorId": "51245008",
                    "name": "Fengcui Qian"
                },
                {
                    "authorId": "2241825",
                    "name": "Xuecang Li"
                },
                {
                    "authorId": "15523151",
                    "name": "Zhengming Yu"
                },
                {
                    "authorId": "2146281313",
                    "name": "Jiang Zhu"
                },
                {
                    "authorId": "2112771606",
                    "name": "Rui Yu"
                },
                {
                    "authorId": "145454815",
                    "name": "Yue Zhao"
                },
                {
                    "authorId": "2171122232",
                    "name": "Ke Ding"
                },
                {
                    "authorId": "50024059",
                    "name": "Yanyu Li"
                },
                {
                    "authorId": "2143013732",
                    "name": "Yongsan Yang"
                },
                {
                    "authorId": "2057913890",
                    "name": "Qi Pan"
                },
                {
                    "authorId": "1391202748",
                    "name": "Jiaxin Chen"
                },
                {
                    "authorId": "2116332092",
                    "name": "Chao Song"
                },
                {
                    "authorId": "50621406",
                    "name": "Qiuyu Wang"
                },
                {
                    "authorId": "50561852",
                    "name": "Jian Zhang"
                },
                {
                    "authorId": "2145474480",
                    "name": "Guohua Wang"
                },
                {
                    "authorId": "2195812226",
                    "name": "Chunquan Li"
                }
            ]
        },
        {
            "paperId": "2ffd543dbf1d8ce20ee97098c0edf557b0f7f256",
            "title": "Benchmarking Node Outlier Detection on Graphs",
            "abstract": "Graph outlier detection is an emerging but crucial machine learning task with numerous applications. Despite the proliferation of algorithms developed in recent years, the lack of a standard and uni\ufb01ed setting for performance evaluation limits their advancement and usage in real-world applications. To tap the gap, we present, (to our best knowledge) the \ufb01rst comprehensive unsupervised node outlier detection benchmark for graphs called UNOD, with the following highlights: (1) evaluating fourteen methods with backbone spanning from classical matrix factorization to the latest graph neural networks; (2) benchmarking the method performance with different types of injected outliers and organic outliers on real-world datasets; (3) comparing the ef\ufb01ciency and scalability of the algorithms by runtime and GPU memory usage on synthetic graphs at different scales. Based on the analyses of extensive experimental results, we discuss the pros and cons of current UNOD methods and point out multiple crucial and promising future research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2163663039",
                    "name": "Kay Liu"
                },
                {
                    "authorId": "8729899",
                    "name": "Yingtong Dou"
                },
                {
                    "authorId": "145454815",
                    "name": "Yue Zhao"
                },
                {
                    "authorId": "151257348",
                    "name": "Xueying Ding"
                },
                {
                    "authorId": "35346885",
                    "name": "Xiyang Hu"
                },
                {
                    "authorId": "2110035651",
                    "name": "Ruitong Zhang"
                },
                {
                    "authorId": "66807781",
                    "name": "Kaize Ding"
                },
                {
                    "authorId": "2163546329",
                    "name": "Canyu Chen"
                },
                {
                    "authorId": "2138443697",
                    "name": "Hao Peng"
                },
                {
                    "authorId": "145800151",
                    "name": "Kai Shu"
                },
                {
                    "authorId": "46732871",
                    "name": "Lichao Sun"
                },
                {
                    "authorId": "2040455",
                    "name": "Jundong Li"
                },
                {
                    "authorId": "27694416",
                    "name": "George H. Chen"
                },
                {
                    "authorId": "2072782550",
                    "name": "Zhihao Jia"
                },
                {
                    "authorId": "152297693",
                    "name": "Philip S. Yu"
                }
            ]
        },
        {
            "paperId": "3e814bd1160e2df9ef2c647cf9fea0d5018b336c",
            "title": "CEDA: integrating gene expression data with CRISPR-pooled screen data identifies essential genes with higher expression",
            "abstract": "MOTIVATION\nCRISPR-based genetic perturbation screen is a powerful tool to probe gene function. However, experimental noises, especially for the lowly expressed genes, need to be accounted for to maintain proper control of false positive rate.\n\n\nMETHOD\nWe develop a statistical method, named CRISPR screen with Expression Data Analysis (CEDA), to integrate gene expression profiles and CRISPR screen data for identifying essential genes. CEDA stratifies genes based on expression level and adopts a three-component mixture model for the log-fold change of single-guide RNAs (sgRNAs). Empirical Bayesian prior and Expectation-Maximization algorithm are used for parameter estimation and false discovery rate inference.\n\n\nRESULTS\nTaking advantage of gene expression data, CEDA identifies essential genes with higher expression. Compared to existing methods, CEDA shows comparable reliability but higher sensitivity in detecting essential genes with moderate sgRNA fold change. Therefore, using the same CRISPR data, CEDA generates an additional hit gene list.\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "145454815",
                    "name": "Yue Zhao"
                },
                {
                    "authorId": "39575399",
                    "name": "Lianbo Yu"
                },
                {
                    "authorId": "98314898",
                    "name": "Xue Wu"
                },
                {
                    "authorId": "2145539865",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "2246953",
                    "name": "K. Coombes"
                },
                {
                    "authorId": "5419411",
                    "name": "K. Au"
                },
                {
                    "authorId": "47768853",
                    "name": "Lijun Cheng"
                },
                {
                    "authorId": "2177231960",
                    "name": "Lang Li"
                }
            ]
        }
    ]
}