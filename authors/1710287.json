{
    "authorId": "1710287",
    "papers": [
        {
            "paperId": "21b4777948797377deedf4a9f1f58ad13f6b8b5d",
            "title": "Overview of the Tenth Dialog System Technology Challenge: DSTC10",
            "abstract": "This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                },
                {
                    "authorId": "2150275",
                    "name": "Satwik Kottur"
                },
                {
                    "authorId": "2887412",
                    "name": "Jinchao Li"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2066415714",
                    "name": "Zhengcong Fei"
                },
                {
                    "authorId": "2109965103",
                    "name": "Zekang Li"
                },
                {
                    "authorId": "27672597",
                    "name": "Jinchao Zhang"
                },
                {
                    "authorId": "2257374643",
                    "name": "Yang Feng"
                },
                {
                    "authorId": "2116575668",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "145916630",
                    "name": "Karthik Gopalakrishnan"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "1979505",
                    "name": "A. Geramifard"
                },
                {
                    "authorId": "1765212",
                    "name": "Chiori Hori"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1694652",
                    "name": "Rafael E. Banchs"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ]
        },
        {
            "paperId": "027ec9a2aaa81b01d190e8607b2250779e5834dd",
            "title": "Selective In-Context Data Augmentation for Intent Detection using Pointwise V-Information",
            "abstract": "This work focuses on in-context data augmentation for intent detection. Having found that augmentation via in-context prompting of large pre-trained language models (PLMs) alone does not improve performance, we introduce a novel approach based on PLMs and pointwise V-information (PVI), a metric that can measure the usefulness of a datapoint for training a model. Our method first fine-tunes a PLM on a small seed of training data and then synthesizes new datapoints - utterances that correspond to given intents. It then employs intent-aware filtering, based on PVI, to remove datapoints that are not helpful to the downstream intent classifier. Our method is thus able to leverage the expressive power of large language models to produce diverse training data. Empirical results demonstrate that our method can produce synthetic training data that achieve state-of-the-art performance on three challenging intent detection datasets under few-shot settings (1.28% absolute improvement in 5-shot and 1.18% absolute in 10-shot, on average) and perform on par with the state-of-the-art in full-shot settings (within 0.01% absolute, on average).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1557269413",
                    "name": "Yen-Ting Lin"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2108230831",
                    "name": "Sungjin Lee"
                },
                {
                    "authorId": "8223433",
                    "name": "Devamanyu Hazarika"
                },
                {
                    "authorId": "2171886",
                    "name": "Mahdi Namazifar"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2152802138",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "2ad96c799edcbba5b097c8842461f0d9da4aa1cc",
            "title": "Identifying Entrainment in Task-Oriented Conversations",
            "abstract": "Human interlocutors adapt their behavior to each other in a conversation through entrainment. While entrainment has been found in long chit-chat conversations, much less research has been conducted on task-oriented dialogs. In this paper, we investigate short task-oriented Wizard-of-Oz conversations for acoustic-prosodic and lexical entrainment. We conduct significance tests that reveal changes in speech pitch and frequent words as important indicators of entrainment. Our findings will guide user-entraining dialog systems to improve the quality of conversations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188747624",
                    "name": "Run Chen"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "32057282",
                    "name": "J. Hirschberg"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "6f9d3c3a628e8d842de83f4ba2e41dd1b4b404ac",
            "title": "PLACES: Prompting Language Models for Social Conversation Synthesis",
            "abstract": "Collecting high quality conversational data can be very expensive for most applications and infeasible for others due to privacy, ethical, or similar concerns. A promising direction to tackle this problem is to generate synthetic dialogues by prompting large language models. In this work, we use a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting. We perform several thorough evaluations of our synthetic conversations compared to human-collected conversations. This includes various dimensions of conversation quality with human evaluation directly on the synthesized conversations, and interactive human evaluation of chatbots fine-tuned on the synthetically generated dataset. We additionally demonstrate that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi-party tasks. Our synthetic multi-party conversations were rated more favorably across all measured dimensions compared to conversation excerpts sampled from a human-collected multi-party dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66615738",
                    "name": "Maximillian Chen"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "46387857",
                    "name": "Chenyang Tao"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "146177177",
                    "name": "Andrew Rosenbaum"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2167255986",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "a30ee4285c17d027d89659b539bd6d909258b137",
            "title": "\u201cWhat do others think?\u201d: Task-Oriented Conversational Modeling with Subjective Knowledge",
            "abstract": "Task-oriented Dialogue (TOD) Systems aim to build dialogue systems that assist users in accomplishing specific goals, such as booking a hotel or a restaurant. Traditional TODs rely on domain-specific APIs/DBs or external factual knowledge to generate responses, which cannot accommodate subjective user requests (e.g.,\u201dIs the WIFI reliable?\u201d or \u201cDoes the restaurant have a good atmosphere?\u201d). To address this issue, we propose a novel task of subjective-knowledge-based TOD (SK-TOD). We also propose the first corresponding dataset, which contains subjective knowledge-seeking dialogue contexts and manually annotated responses grounded in subjective knowledge sources. When evaluated with existing TOD approaches, we find that this task poses new challenges such as aggregating diverse opinions from multiple knowledge snippets. We hope this task and dataset can promote further research on TOD and subjective content understanding. The code and the dataset are available at https://github.com/alexa/dstc11-track5.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151480423",
                    "name": "Chao Zhao"
                },
                {
                    "authorId": "2921001",
                    "name": "Spandana Gella"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "8223433",
                    "name": "Devamanyu Hazarika"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "2171886",
                    "name": "Mahdi Namazifar"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "f1eeba530be0ac537bcdf633ee6c462045c97fb5",
            "title": "Towards Credible Human Evaluation of Open-Domain Dialog Systems Using Interactive Setup",
            "abstract": "Evaluating open-domain conversation models has been an open challenge due to the open-ended nature of conversations. In addition to static evaluations, recent work has started to explore a variety of per-turn and per-dialog interactive evaluation mechanisms and provide advice on the best setup. In this work, we adopt the interactive evaluation framework and further apply to multiple models with a focus on per-turn evaluation techniques. Apart from the widely used setting where participants select the best response among different candidates at each turn, one more novel per-turn evaluation setting is adopted, where participants can select all appropriate responses with different fallback strategies to continue the conversation when no response is selected. We evaluate these settings based on sensitivity and consistency using four GPT2-based models that differ in model sizes or fine-tuning data. To better generalize to any model groups with no prior assumptions on their rankings and control evaluation costs for all setups, we also propose a methodology to estimate the required sample size given a minimum performance gap of interest before running most experiments. Our comprehensive human evaluation results shed light on how to conduct credible human evaluations of open domain dialog systems using the interactive setup, and suggest additional future directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2221300824",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "26882347",
                    "name": "P. Lange"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "145305185",
                    "name": "A. Wirth"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "9c3ee6cc47ff463e15590c012dbd133598dc5743",
            "title": "Knowledge-Grounded Conversational Data Augmentation with Generative Conversational Networks",
            "abstract": "While rich, open-domain textual data are generally available and may include interesting phenomena (humor, sarcasm, empathy, etc.) most are designed for language processing tasks, and are usually in a non-conversational format. In this work, we take a step towards automatically generating conversational data using Generative Conversational Networks, aiming to benefit from the breadth of available language and knowledge data, and train open domain social conversational agents. We evaluate our approach on conversations with and without knowledge on the Topical Chat dataset using automatic metrics and human evaluators. Our results show that for conversations without knowledge grounding, GCN can generalize from the seed data, producing novel conversations that are less relevant but more engaging and for knowledge-grounded conversations, it can produce more knowledge-focused, fluent, and engaging conversations. Specifically, we show that for open-domain conversations with 10% of seed data, our approach performs close to the baseline that uses 100% of the data, while for knowledge-grounded conversations, it achieves the same using only 1% of the data, on human ratings of engagingness, fluency, and relevance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1557269413",
                    "name": "Yen-Ting Lin"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "9ed047bb4ef6ddc296d473bf4f3b55488aeba350",
            "title": "GEMv2: Multilingual NLG Benchmarking in a Single Line of Code",
            "abstract": "Evaluation in machine learning is usually informed by past choices, for example which datasets or metrics to use. This standardization enables the comparison on equal footing using leaderboards, but the evaluation choices become sub-optimal as better alternatives arise. This problem is especially pertinent in natural language generation which requires ever-improving suites of datasets, metrics, and human evaluation to make definitive claims. To make following best model evaluation practices easier, we introduce GEMv2. The new version of the Generation, Evaluation, and Metrics Benchmark introduces a modular infrastructure for dataset, model, and metric developers to benefit from each others work. GEMv2 supports 40 documented datasets in 51 languages. Models for all datasets can be evaluated online and our interactive data card creation and rendering tools make it easier to add new datasets to the living benchmark.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3159346",
                    "name": "Sebastian Gehrmann"
                },
                {
                    "authorId": "49785688",
                    "name": "Abhik Bhattacharjee"
                },
                {
                    "authorId": "2143239611",
                    "name": "Abinaya Mahendiran"
                },
                {
                    "authorId": "2115765629",
                    "name": "Alex Wang"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "21626987",
                    "name": "Aman Madaan"
                },
                {
                    "authorId": "1584940075",
                    "name": "Angelina McMillan-Major"
                },
                {
                    "authorId": "46270987",
                    "name": "Anna Shvets"
                },
                {
                    "authorId": "84414257",
                    "name": "Ashish Upadhyay"
                },
                {
                    "authorId": "1490485182",
                    "name": "Bingsheng Yao"
                },
                {
                    "authorId": "150048491",
                    "name": "Bryan Wilie"
                },
                {
                    "authorId": "1857797",
                    "name": "Chandra Bhagavatula"
                },
                {
                    "authorId": "2172174590",
                    "name": "Chaobin You"
                },
                {
                    "authorId": "2122535749",
                    "name": "Craig Thomson"
                },
                {
                    "authorId": "3360992",
                    "name": "Cristina Garbacea"
                },
                {
                    "authorId": "36743361",
                    "name": "Dakuo Wang"
                },
                {
                    "authorId": "145346875",
                    "name": "Daniel Deutsch"
                },
                {
                    "authorId": "2694222",
                    "name": "Deyi Xiong"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2921637",
                    "name": "Dimitra Gkatzia"
                },
                {
                    "authorId": "9215251",
                    "name": "Dragomir R. Radev"
                },
                {
                    "authorId": "144354055",
                    "name": "Elizabeth Clark"
                },
                {
                    "authorId": "41152329",
                    "name": "Esin Durmus"
                },
                {
                    "authorId": "8759332",
                    "name": "Faisal Ladhak"
                },
                {
                    "authorId": "1694491",
                    "name": "Filip Ginter"
                },
                {
                    "authorId": "9162688",
                    "name": "Genta Indra Winata"
                },
                {
                    "authorId": "2879705",
                    "name": "Hendrik Strobelt"
                },
                {
                    "authorId": "50376014",
                    "name": "Hiroaki Hayashi"
                },
                {
                    "authorId": "2848048",
                    "name": "Jekaterina Novikova"
                },
                {
                    "authorId": "1776599",
                    "name": "Jenna Kanerva"
                },
                {
                    "authorId": "2164872258",
                    "name": "Jenny Chim"
                },
                {
                    "authorId": "2112249795",
                    "name": "Jiawei Zhou"
                },
                {
                    "authorId": "2133312617",
                    "name": "Jordan Clive"
                },
                {
                    "authorId": "2124977868",
                    "name": "Joshua Maynez"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "47080255",
                    "name": "Juraj Juraska"
                },
                {
                    "authorId": "4834571",
                    "name": "Kaustubh D. Dhole"
                },
                {
                    "authorId": "37619618",
                    "name": "Khyathi Raghavi Chandu"
                },
                {
                    "authorId": "10430740",
                    "name": "Leonardo F. R. Ribeiro"
                },
                {
                    "authorId": "2072250824",
                    "name": "Lewis Tunstall"
                },
                {
                    "authorId": "72436283",
                    "name": "Li Zhang"
                },
                {
                    "authorId": "51478016",
                    "name": "Mahima Pushkarna"
                },
                {
                    "authorId": "2219854",
                    "name": "Mathias Creutz"
                },
                {
                    "authorId": "145713719",
                    "name": "Michael White"
                },
                {
                    "authorId": "26688118",
                    "name": "Mihir Kale"
                },
                {
                    "authorId": "2065867306",
                    "name": "Moussa Kamal Eddine"
                },
                {
                    "authorId": "2048028927",
                    "name": "Nico Daheim"
                },
                {
                    "authorId": "34202134",
                    "name": "Nishant Subramani"
                },
                {
                    "authorId": "2544049",
                    "name": "Ondrej Dusek"
                },
                {
                    "authorId": "28130078",
                    "name": "P. Liang"
                },
                {
                    "authorId": "1451644426",
                    "name": "Pawan Sasanka Ammanamanchi"
                },
                {
                    "authorId": "2152208184",
                    "name": "Qinqin Zhu"
                },
                {
                    "authorId": "2990638",
                    "name": "Ratish Puduppully"
                },
                {
                    "authorId": "46218926",
                    "name": "Reno Kriz"
                },
                {
                    "authorId": "2046603",
                    "name": "Rifat Shahriyar"
                },
                {
                    "authorId": "144969436",
                    "name": "Ronald Cardenas"
                },
                {
                    "authorId": "2221260",
                    "name": "Saad Mahamood"
                },
                {
                    "authorId": "1486204986",
                    "name": "Salomey Osei"
                },
                {
                    "authorId": "2220548276",
                    "name": "Samuel Cahyawijaya"
                },
                {
                    "authorId": "102379512",
                    "name": "S. vStajner"
                },
                {
                    "authorId": "46185285",
                    "name": "S\u00e9bastien Montella"
                },
                {
                    "authorId": "89189891",
                    "name": "Shailza"
                },
                {
                    "authorId": "51228129",
                    "name": "Shailza Jolly"
                },
                {
                    "authorId": "2738095",
                    "name": "Simon Mille"
                },
                {
                    "authorId": "1400373232",
                    "name": "Tahmid Hasan"
                },
                {
                    "authorId": "2057973326",
                    "name": "Tianhao Shen"
                },
                {
                    "authorId": "51221489",
                    "name": "Tosin P. Adewumi"
                },
                {
                    "authorId": "24025563",
                    "name": "Vikas Raunak"
                },
                {
                    "authorId": "2831377",
                    "name": "Vipul Raheja"
                },
                {
                    "authorId": "48942032",
                    "name": "Vitaly Nikolaev"
                },
                {
                    "authorId": "40052131",
                    "name": "V. Tsai"
                },
                {
                    "authorId": "2262249",
                    "name": "Yacine Jernite"
                },
                {
                    "authorId": "2110290513",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "31470750",
                    "name": "Yisi Sang"
                },
                {
                    "authorId": "2108176413",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "39517968",
                    "name": "Yufang Hou"
                }
            ]
        },
        {
            "paperId": "a4c4093802e2084553781d700566c60a2dcc4e7a",
            "title": "What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation",
            "abstract": "Accurate automatic evaluation metrics for open-domain dialogs are in high demand. Existing model-based metrics for system response evaluation are trained on human annotated data, which is cumbersome to collect. In this work, we propose to use information that can be automatically extracted from the next user utterance, such as its sentiment or whether the user explicitly ends the conversation, as a proxy to measure the quality of the previous system response. This allows us to train on a massive set of dialogs with weak supervision, without requiring manual system turn quality annotations. Experiments show that our model is comparable to models trained on human annotated data. Furthermore, our model generalizes across both spoken and written open-domain dialog corpora collected from real and paid users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3022427",
                    "name": "Sarik Ghazarian"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "c70e19518f7016d01d67884d06de4523b92dd867",
            "title": "Understanding How People Rate Their Conversations",
            "abstract": "User ratings play a signi\ufb01cant role in spoken dialogue systems. Typically, such ratings tend to be averaged across all users and then utilized as feedback to improve the system or personalize its behavior. While this method can be useful to understand broad, general issues with the system and its behavior, it does not take into account differences between users that affect their ratings. In this work, we conduct a study to better understand how people rate their interactions with conversational agents. One macro-level characteristic that has been shown to cor-relate with how people perceive their inter-personal communication is personality [1, 2, 12]. We speci\ufb01cally focus on agreeableness and extraversion as variables that may explain variation in ratings and therefore provide a more meaningful signal for training or personalization. In order to elicit those personality traits during an interaction with a conversational agent, we designed and validated a \ufb01ctional story, grounded in prior work in psychology. We then implemented the story into an experimental conversational agent that allowed users to opt-in to hearing the story. Our results suggest that for human-conversational agent interactions, extraversion may play a role in user ratings, but more data is needed to determine if the relationship is signi\ufb01cant. Agreeableness, on the other hand, plays a statistically signi\ufb01cant role in conversation ratings: users who are more agreeable are more likely to provide a higher rating for their interaction. In addition, we found that users who opted to hear the story were, in general, more likely to rate their conversational experience higher than those who did not.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "147657571",
                    "name": "Nicole Chartier"
                },
                {
                    "authorId": "29153729",
                    "name": "Pankaj Rajan"
                },
                {
                    "authorId": "32057282",
                    "name": "J. Hirschberg"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        }
    ]
}