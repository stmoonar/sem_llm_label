{
    "authorId": "46334890",
    "papers": [
        {
            "paperId": "79d11dd9c97e8727eed7ea31e31fb561e42c9523",
            "title": "Survey for Landing Generative AI in Social and E-commerce Recsys - the Industry Perspectives",
            "abstract": "Recently, generative AI (GAI), with their emerging capabilities, have presented unique opportunities for augmenting and revolutionizing industrial recommender systems (Recsys). Despite growing research efforts at the intersection of these fields, the integration of GAI into industrial Recsys remains in its infancy, largely due to the intricate nature of modern industrial Recsys infrastructure, operations, and product sophistication. Drawing upon our experiences in successfully integrating GAI into several major social and e-commerce platforms, this survey aims to comprehensively examine the underlying system and AI foundations, solution frameworks, connections to key research advancements, as well as summarize the practical insights and challenges encountered in the endeavor to integrate GAI into industrial Recsys. As pioneering work in this domain, we hope outline the representative developments of relevant fields, shed lights on practical GAI adoptions in the industry, and motivate future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2305646985",
                    "name": "Da Xu"
                },
                {
                    "authorId": "46334890",
                    "name": "Danqing Zhang"
                },
                {
                    "authorId": "2305704544",
                    "name": "Guangyu Yang"
                },
                {
                    "authorId": "2305986643",
                    "name": "Bo Yang"
                },
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "2305922750",
                    "name": "Lingling Zheng"
                },
                {
                    "authorId": "2305745246",
                    "name": "Cindy Liang"
                }
            ]
        },
        {
            "paperId": "35544f161c8efa6134960bf0cc770defb13e6b56",
            "title": "Exploiting Intent Evolution in E-commercial Query Recommendation",
            "abstract": "Aiming at a better understanding of the search goals in the user search sessions, recent query recommender systems explicitly model the reformulations of queries, which hopes to estimate the intents behind these reformulations and thus benefit the next-query recommendation. However, in real-world e-commercial search scenarios, user intents are much more complicated and may evolve dynamically. Existing methods merely consider trivial reformulation intents from semantic aspects and fail to model dynamic reformulation intent flows in search sessions, leading to sub-optimal capacities to recommend desired queries. To deal with these limitations, we first explicitly define six types of query reformulation intents according to the desired products of two consecutive queries. We then apply two self-attentive encoders on top of two pre-trained large language models to learn the transition dynamics from semantic query and intent reformulation sequences, respectively. We develop an intent-aware query decoder to utilize the predicted intents for suggesting the next queries. We instantiate such a framework with an Intent-aware Variational AutoEncoder (IVAE) under deployment at Amazon. We conduct comprehensive experiments on two real-world e-commercial datasets from Amazon and one public dataset from BestBuy. Specifically, IVAE improves the Recall@15 by 25.44% and 60.47% on two Amazon datasets and 13.91% on BestBuy, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153606201",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "8492168",
                    "name": "Zhengyang Wang"
                },
                {
                    "authorId": "35466544",
                    "name": "Hengrui Zhang"
                },
                {
                    "authorId": "3393799",
                    "name": "Qingyu Yin"
                },
                {
                    "authorId": "48784944",
                    "name": "Xianfeng Tang"
                },
                {
                    "authorId": "2107962433",
                    "name": "Yinghan Wang"
                },
                {
                    "authorId": "46334890",
                    "name": "Danqing Zhang"
                },
                {
                    "authorId": "3122003",
                    "name": "Limeng Cui"
                },
                {
                    "authorId": "2227491227",
                    "name": "M. Cheng"
                },
                {
                    "authorId": "2021632793",
                    "name": "Bing Yin"
                },
                {
                    "authorId": "2893721",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "2721708",
                    "name": "P. Yu"
                }
            ]
        },
        {
            "paperId": "200c92418ccba212e6f974665af407f06f6ccabf",
            "title": "Condensing Graphs via One-Step Gradient Matching",
            "abstract": "As training deep learning models on large dataset takes a lot of time and resources, it is desired to construct a small synthetic dataset with which we can train deep learning models sufficiently. There are recent works that have explored solutions on condensing image datasets through complex bi-level optimization. For instance, dataset condensation (DC) matches network gradients w.r.t. large-real data and small-synthetic data, where the network weights are optimized for multiple steps at each outer iteration. However, existing approaches have their inherent limitations: (1) they are not directly applicable to graphs where the data is discrete; and (2) the condensation process is computationally expensive due to the involved nested optimization. To bridge the gap, we investigate efficient dataset condensation tailored for graph datasets where we model the discrete graph structure as a probabilistic model. We further propose a one-step gradient matching scheme, which performs gradient matching for only one single step without training the network weights. Our theoretical analysis shows this strategy can generate synthetic graphs that lead to lower classification loss on real graphs. Extensive experiments on various graph datasets demonstrate the effectiveness and efficiency of the proposed method. In particular, we are able to reduce the dataset size by 90% while approximating up to 98% of the original performance and our method is significantly faster than multi-step gradient matching (e.g. $15$\u00d7 in CIFAR10 for synthesizing 500 graphs).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144767914",
                    "name": "Wei Jin"
                },
                {
                    "authorId": "48784944",
                    "name": "Xianfeng Tang"
                },
                {
                    "authorId": "5795999",
                    "name": "Haoming Jiang"
                },
                {
                    "authorId": "2146249169",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "46334890",
                    "name": "Danqing Zhang"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "2170538349",
                    "name": "Bin Ying"
                }
            ]
        },
        {
            "paperId": "716f9d0f6e96f437e127de90c87f7b2f7a6c8f12",
            "title": "SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models",
            "abstract": "Recent research showed promising results on combining pretrained language models (LMs) with canonical utterance for few-shot semantic parsing. The canonical utterance is often lengthy and complex due to the compositional structure of formal languages. Learning to generate such canonical utterance requires significant amount of data to reach high performance. Fine-tuning with only few-shot samples, the LMs can easily forget pretrained knowledge, overfit spurious biases, and suffer from compositionally out-of-distribution generalization errors. To tackle these issues, we propose a novel few-shot semantic parsing method -- SeqZero. SeqZero decomposes the problem into a sequence of sub-problems, which correspond to the sub-clauses of the formal language. Based on the decomposition, the LMs only need to generate short answers using prompts for predicting sub-clauses. Thus, SeqZero avoids generating a long canonical utterance at once. Moreover, SeqZero employs not only a few-shot model but also a zero-shot model to alleviate the overfitting. In particular, SeqZero brings out the merits from both models via ensemble equipped with our proposed constrained rescaling. SeqZero achieves SOTA performance of BART-based models on GeoQuery and EcommerceQuery, which are two few-shot datasets with compositional data split.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7788583",
                    "name": "Jingfeng Yang"
                },
                {
                    "authorId": "5795999",
                    "name": "Haoming Jiang"
                },
                {
                    "authorId": "3393799",
                    "name": "Qingyu Yin"
                },
                {
                    "authorId": "46334890",
                    "name": "Danqing Zhang"
                },
                {
                    "authorId": "2021632793",
                    "name": "Bing Yin"
                },
                {
                    "authorId": "2143919864",
                    "name": "Diyi Yang"
                }
            ]
        },
        {
            "paperId": "940d32e2f5ac604393e8a9ef9195f53e00fc20ad",
            "title": "Short Text Pre-training with Extended Token Classification for E-commerce Query Understanding",
            "abstract": "E-commerce query understanding is the process of inferring the shopping intent of customers by extracting semantic meaning from their search queries. The recent progress of pre-trained masked language models (MLM) in natural language processing is extremely attractive for developing effective query understanding models. Specifically, MLM learns contextual text embedding via recovering the masked tokens in the sentences. Such a pre-training process relies on the sufficient contextual information. It is, however, less effective for search queries, which are usually short text. When applying masking to short search queries, most contextual information is lost and the intent of the search queries may be changed. To mitigate the above issues for MLM pre-training on search queries, we propose a novel pre-training task specifically designed for short text, called Extended Token Classification (ETC). Instead of masking the input text, our approach extends the input by inserting tokens via a generator network, and trains a discriminator to identify which tokens are inserted in the extended input. We conduct experiments in an E-commerce store to demonstrate the effectiveness of ETC.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5795999",
                    "name": "Haoming Jiang"
                },
                {
                    "authorId": "2151070",
                    "name": "Tianyu Cao"
                },
                {
                    "authorId": "2146249169",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "143884453",
                    "name": "Cheng-hsin Luo"
                },
                {
                    "authorId": "48784944",
                    "name": "Xianfeng Tang"
                },
                {
                    "authorId": "3393799",
                    "name": "Qingyu Yin"
                },
                {
                    "authorId": "46334890",
                    "name": "Danqing Zhang"
                },
                {
                    "authorId": "3057049",
                    "name": "R. Goutam"
                },
                {
                    "authorId": "2021632793",
                    "name": "Bing Yin"
                }
            ]
        },
        {
            "paperId": "c1359ebf6cfac857517e2303fe54e366639b4366",
            "title": "RETE: Retrieval-Enhanced Temporal Event Forecasting on Unified Query Product Evolutionary Graph",
            "abstract": "With the increasing demands on e-commerce platforms, numerous user action history is emerging. Those enriched action records are vital to understand users\u2019 interests and intents. Recently, prior works for user behavior prediction mainly focus on the interactions with product-side information. However, the interactions with search queries, which usually act as a bridge between users and products, are still under investigated. In this paper, we explore a new problem named temporal event forecasting, a generalized user behavior prediction task in a unified query product evolutionary graph, to embrace both query and product recommendation in a temporal manner. To fulfill this setting, there involves two challenges: (1) the action data for most users is scarce; (2) user preferences are dynamically evolving and shifting over time. To tackle those issues, we propose a novel Retrieval-Enhanced Temporal Event (RETE) forecasting framework. Unlike existing methods that enhance user representations via roughly absorbing information from connected entities in the whole graph, RETE efficiently and dynamically retrieves relevant entities centrally on each user as high-quality subgraphs, preventing the noise propagation from the densely evolutionary graph structures that incorporate abundant search queries. And meanwhile, RETE autoregressively accumulates retrieval-enhanced user representations from each time step, to capture evolutionary patterns for joint query and product prediction. Empirically, extensive experiments on both the public benchmark and four real-world industrial datasets demonstrate the effectiveness of the proposed RETE method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "2146249169",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "46334890",
                    "name": "Danqing Zhang"
                },
                {
                    "authorId": "3393799",
                    "name": "Qingyu Yin"
                },
                {
                    "authorId": "2088210968",
                    "name": "Tong Zhao"
                },
                {
                    "authorId": "2021632793",
                    "name": "Bing Yin"
                },
                {
                    "authorId": "1730531",
                    "name": "T. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "f43e2a64358e2f0f420183f317c636f7df3fdffd",
            "title": "AutoGDA: Automated Graph Data Augmentation for Node Classification",
            "abstract": "Graph data augmentation has been used to improve generalizability of graph machine learning. However, by only applying \ufb01xed augmentation operations on entire graphs, existing methods overlook the unique characteristics of communities which naturally exist in the graphs. For example, different communities can have various degree distributions and homophily ratios. Ignoring such discrepancy with uni\ufb01ed augmentation strategies on the entire graph could lead to sub-optimal performance for graph data augmentation methods. In this paper, we study a novel problem of automated graph data augmentation for node classi\ufb01cation from the localized perspective of communities. We formulate it as a bilevel optimization problem: \ufb01nding a set of augmentation strategies for each community, which maximizes the performance of graph neural networks on node classi\ufb01cation. As the bilevel optimization is hard to solve directly and the search space for community-customized augmentations strategy is huge, we propose a reinforcement learning framework AutoGDA that learns the local-optimal augmentation strategy for each community sequentially. Our proposed approach outperforms established and popular baselines on public node classi\ufb01cation benchmarks as well as real industry e-commerce networks by up to +12.5% accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1742573",
                    "name": "Tong Zhao"
                },
                {
                    "authorId": "48784944",
                    "name": "Xianfeng Tang"
                },
                {
                    "authorId": "46334890",
                    "name": "Danqing Zhang"
                },
                {
                    "authorId": "5795999",
                    "name": "Haoming Jiang"
                },
                {
                    "authorId": "145850291",
                    "name": "Nikhil S. Rao"
                },
                {
                    "authorId": "2115714424",
                    "name": "Yiwei Song"
                },
                {
                    "authorId": "2155993722",
                    "name": "Pallav Agrawal"
                },
                {
                    "authorId": "2691095",
                    "name": "Karthik Subbian"
                },
                {
                    "authorId": "2021632793",
                    "name": "Bing Yin"
                },
                {
                    "authorId": "144812586",
                    "name": "Meng Jiang"
                }
            ]
        },
        {
            "paperId": "3da2195d841db8a061b849b50b5078189f468ea5",
            "title": "MetaTS: Meta Teacher-Student Network for Multilingual Sequence Labeling with Minimal Supervision",
            "abstract": "Sequence labeling aims to predict a fine-grained sequence of labels for the text. However, such formulation hinders the effectiveness of supervised methods due to the lack of token-level annotated data. This is exacerbated when we meet a diverse range of languages. In this work, we explore multilingual sequence labeling with minimal supervision using a single unified model for multiple languages. Specifically, we propose a Meta Teacher-Student (MetaTS) Network, a novel meta learning method to alleviate data scarcity by leveraging large multilingual unlabeled data. Prior teacher-student frameworks of self-training rely on rigid teaching strategies, which may hardly produce high-quality pseudo-labels for consecutive and interdependent tokens. On the contrary, MetaTS allows the teacher to dynamically adapt its pseudo-annotation strategies by the student\u2019s feedback on the generated pseudo-labeled data of each language and thus mitigate error propagation from noisy pseudo-labels. Extensive experiments on both public and real-world multilingual sequence labeling datasets empirically demonstrate the effectiveness of MetaTS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146249169",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "46334890",
                    "name": "Danqing Zhang"
                },
                {
                    "authorId": "2151070",
                    "name": "Tianyu Cao"
                },
                {
                    "authorId": "2112556840",
                    "name": "Ying Wei"
                },
                {
                    "authorId": "2115714424",
                    "name": "Yiwei Song"
                },
                {
                    "authorId": "2021632793",
                    "name": "Bing Yin"
                }
            ]
        },
        {
            "paperId": "7990f22dc8d1ad2114fd5c9c36bd8ecdcf4f77dc",
            "title": "Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data",
            "abstract": "Weak supervision has shown promising results in many natural language processing tasks, such as Named Entity Recognition (NER). Existing work mainly focuses on learning deep NER models only with weak supervision, i.e., without any human annotation, and shows that by merely using weakly labeled data, one can achieve good performance, though still underperforms fully supervised NER with manually/strongly labeled data. In this paper, we consider a more practical scenario, where we have both a small amount of strongly labeled data and a large amount of weakly labeled data. Unfortunately, we observe that weakly labeled data does not necessarily improve, or even deteriorate the model performance (due to the extensive noise in the weak labels) when we train deep NER models over a simple or weighted combination of the strongly labeled and weakly labeled data. To address this issue, we propose a new multi-stage computational framework \u2013 NEEDLE with three essential ingredients: (1) weak label completion, (2) noise-aware loss function, and (3) final fine-tuning over the strongly labeled data. Through experiments on E-commerce query NER and Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise of the weak labels and outperforms existing methods. In particular, we achieve new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74, BC5CDR-disease 90.69, NCBI-disease 92.28.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152630772",
                    "name": "Haoming Jiang"
                },
                {
                    "authorId": "46334890",
                    "name": "Danqing Zhang"
                },
                {
                    "authorId": "2151070",
                    "name": "Tianyu Cao"
                },
                {
                    "authorId": "2021632793",
                    "name": "Bing Yin"
                },
                {
                    "authorId": "36345161",
                    "name": "T. Zhao"
                }
            ]
        },
        {
            "paperId": "d0c1b11072cdcf71fd2eec5aedb53aaec361171c",
            "title": "QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction",
            "abstract": "We study the problem of query attribute value extraction, which aims to identify named entities from user queries as diverse surface form attribute values and afterward transform them into formally canonical forms. Such a problem consists of two phases: named entity recognition (NER) and attribute value normalization (AVN). However, existing works only focus on the NER phase but neglect equally important AVN. To bridge this gap, this paper proposes a unified query attribute value extraction system in e-commerce search named QUEACO, which involves both two phases. Moreover, by leveraging large-scale weakly-labeled behavior data, we further improve the extraction performance with less supervision cost. Specifically, for the NER phase, QUEACO adopts a novel teacher-student network, where a teacher network that is trained on the strongly-labeled data generates pseudo-labels to refine the weakly-labeled data for training a student network. Meanwhile, the teacher network can be dynamically adapted by the feedback of the student's performance on strongly-labeled data to maximally denoise the noisy supervisions from the weak labels. For the AVN phase, we also leverage the weakly-labeled query-to-attribute behavior data to normalize surface form attribute values from queries into canonical forms from products. Extensive experiments on a real-world large-scale E-commerce dataset demonstrate the effectiveness of QUEACO.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46334890",
                    "name": "Danqing Zhang"
                },
                {
                    "authorId": "2146249169",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2151070",
                    "name": "Tianyu Cao"
                },
                {
                    "authorId": "2112751280",
                    "name": "Chen Luo"
                },
                {
                    "authorId": "2143626319",
                    "name": "Tony Wu"
                },
                {
                    "authorId": "1694235",
                    "name": "Hanqing Lu"
                },
                {
                    "authorId": "2115714424",
                    "name": "Yiwei Song"
                },
                {
                    "authorId": "2021632793",
                    "name": "Bing Yin"
                },
                {
                    "authorId": "2153707398",
                    "name": "Tuo Zhao"
                },
                {
                    "authorId": "153096457",
                    "name": "Qiang Yang"
                }
            ]
        }
    ]
}