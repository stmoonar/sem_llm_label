{
    "authorId": "145755155",
    "papers": [
        {
            "paperId": "0132648c16e85a0cdacbf1700ed18ea44a51dce6",
            "title": "Dependency Dialogue Acts - Annotation Scheme and Case Study",
            "abstract": "In this paper, we introduce Dependency Dialogue Acts (DDA), a novel framework for capturing the structure of speaker-intentions in multi-party dialogues. DDA combines and adapts features from existing dialogue annotation frameworks, and emphasizes the multi-relational response structure of dialogues in addition to the dialogue acts and rhetorical relations. It represents the functional, discourse, and response structure in multi-party multi-threaded conversations. A few key features distinguish DDA from existing dialogue annotation frameworks such as SWBD-DAMSL and the ISO 24617-2 standard. First, DDA prioritizes the relational structure of the dialogue units and the dialog context, annotating both dialog acts and rhetorical relations as response relations to particular utterances. Second, DDA embraces overloading in dialogues, encouraging annotators to specify multiple response relations and dialog acts for each dialog unit. Lastly, DDA places an emphasis on adequately capturing how a speaker is using the full dialog context to plan and organize their speech. With these features, DDA is highly expressive and recall-oriented with regard to conversation dynamics between multiple speakers. In what follows, we present the DDA annotation framework and case studies annotating DDA structures in multi-party, multi-threaded conversations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2130777703",
                    "name": "Jon Z. Cai"
                },
                {
                    "authorId": "39326238",
                    "name": "Brendan King"
                },
                {
                    "authorId": "2162960361",
                    "name": "Margaret Perkoff"
                },
                {
                    "authorId": "2732135",
                    "name": "Shiran Dudy"
                },
                {
                    "authorId": "144089400",
                    "name": "Jie Cao"
                },
                {
                    "authorId": "2209985949",
                    "name": "Marie Grace"
                },
                {
                    "authorId": "2209984741",
                    "name": "Natalia Wojarnik"
                },
                {
                    "authorId": "47079359",
                    "name": "Ananya Ganesh"
                },
                {
                    "authorId": "10796472",
                    "name": "James H. Martin"
                },
                {
                    "authorId": "145755155",
                    "name": "Martha Palmer"
                },
                {
                    "authorId": "2234088162",
                    "name": "M. Walker"
                },
                {
                    "authorId": "144683841",
                    "name": "Jeffrey Flanigan"
                }
            ]
        },
        {
            "paperId": "14964a54fcd86f7f08afc2d26a48dde828276b07",
            "title": "CRAPES:Cross-modal Annotation Projection for Visual Semantic Role Labeling",
            "abstract": "Automatic image comprehension is an important yet challenging task that includes identifying actions in an image and corresponding action participants. Most current approaches to this task, now termed Grounded Situation Recognition (GSR), start by predicting a verb that describes the action and then predict the nouns that can participate in the action as arguments to the verb. This problem formulation limits each image to a single action even though several actions could be depicted. In contrast, text-based Semantic Role Labeling (SRL) aims to label all actions in a sentence, typically resulting in at least two or three predicate argument structures per sentence. We hypothesize that expanding GSR to follow the more liberal SRL text-based approach to action and participant identification could improve image comprehension results. To test this hypothesis and to preserve generalization capabilities, we use general-purpose vision and language components as a front-end. This paper presents our results, a substantial 28.6 point jump in performance on the SWiG dataset, which confirm our hypothesis. We also discuss the benefits of loosely coupled broad-coverage off-the-shelf components which generalized well to out of domain images, and can decrease the need for manual image semantic role annotation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "23483233",
                    "name": "A. Bhattacharyya"
                },
                {
                    "authorId": "145755155",
                    "name": "Martha Palmer"
                },
                {
                    "authorId": "40517238",
                    "name": "C. Heckman"
                }
            ]
        },
        {
            "paperId": "18fcf14bdd51a242b3ac306cc5a026991f58f719",
            "title": "Mind the Gap between the Application Track and the Real World",
            "abstract": "Recent advances in NLP have led to a rise in inter-disciplinary and application-oriented research. While this demonstrates the growing real-world impact of the field, research papers frequently feature experiments that do not account for the complexities of realistic data and environments. To explore the extent of this gap, we investigate the relationship between the real-world motivations described in NLP papers and the models and evaluation which comprise the proposed solution. We first survey papers from the NLP Applications track from ACL 2020 and EMNLP 2020, asking which papers have differences between their stated motivation and their experimental setting, and if so, mention them. We find that many papers fall short of considering real-world input and output conditions due to adopting simplified modeling or evaluation settings. As a case study, we then empirically show that the performance of an educational dialog understanding system deteriorates when used in a realistic classroom environment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47079359",
                    "name": "Ananya Ganesh"
                },
                {
                    "authorId": "144089400",
                    "name": "Jie Cao"
                },
                {
                    "authorId": "1698446599",
                    "name": "E. M. Perkoff"
                },
                {
                    "authorId": "119407580",
                    "name": "Rosy Southwell"
                },
                {
                    "authorId": "145755155",
                    "name": "Martha Palmer"
                },
                {
                    "authorId": "3422953",
                    "name": "Katharina Kann"
                }
            ]
        },
        {
            "paperId": "1b0378d52d8988ffd5ecfb507e23420985171306",
            "title": "A Comparative Analysis of Automatic Speech Recognition Errors in Small Group Classroom Discourse",
            "abstract": "In collaborative learning environments, effective intelligent learning systems need to accurately analyze and understand the collaborative discourse between learners (i.e., group modeling) to provide adaptive support. We investigate how automatic speech recognition (ASR) errors influence discourse models of small group collaboration in noisy real-world classrooms. Our dataset consisted of 30 students recorded by consumer off-the-shelf microphones (Yeti Blue) while engaging in dyadic- and triadic- collaborative learning in a multi-day STEM curriculum unit. We found that two state-of-the-art ASR systems (Google Speech and OpenAI Whisper) yielded very high word error rates (0.822, 0.847) but very different profiles of error with Google being more conservative, rejecting 38% of utterances instead of 12% for Whisper. Next, we examined how these ASR errors influenced down-stream small group modeling based on pre-trained large language models for three tasks: Abstract Meaning Representation parsing (AMRParsing), on-task/off-task detection (OnTask), and Accountable Productive Talk prediction (TalkMove). As expected, models trained on clean human transcripts yielded degraded performance on all three tasks, measured by the transfer ratio (TR). However, the TR of the specific sentence-level AMRParsing task (.39 - .62) was much lower than that of the abstract discourse-level OnTask (.63- .94) and TalkMove tasks (.64-.72). Furthermore, different training strategies that incorporated ASR transcripts alone or as augmentations of human transcripts increased accuracy for the discourse-level tasks (OnTask and TalkMove) but not AMRParsing. Simulation experiments suggested that the models were tolerant of missing utterances in the dialog context, and that jointly improving ASR accuracy on important word classes (e.g., verbs and nouns) can improve performance across all tasks. Overall, our results provide insights into how different types of NLP-based tasks might be tolerant of ASR errors under extremely noisy conditions and provide suggestions for how to improve accuracy in small group modeling settings for a more equitable, engaging, and adaptive collaborative learning environment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144089400",
                    "name": "Jie Cao"
                },
                {
                    "authorId": "47079359",
                    "name": "Ananya Ganesh"
                },
                {
                    "authorId": "2130777703",
                    "name": "Jon Z. Cai"
                },
                {
                    "authorId": "119407580",
                    "name": "Rosy Southwell"
                },
                {
                    "authorId": "1698446599",
                    "name": "E. M. Perkoff"
                },
                {
                    "authorId": "145666891",
                    "name": "Michael Regan"
                },
                {
                    "authorId": "3422953",
                    "name": "Katharina Kann"
                },
                {
                    "authorId": "2110110561",
                    "name": "James H. Martin"
                },
                {
                    "authorId": "145755155",
                    "name": "Martha Palmer"
                },
                {
                    "authorId": "1383996606",
                    "name": "S. D\u2019Mello"
                }
            ]
        },
        {
            "paperId": "3b28c6f08feaf8e87dff17c999bd112b95a8bd11",
            "title": "Event Semantic Knowledge in Procedural Text Understanding",
            "abstract": "The task of entity state tracking aims to automatically analyze procedural texts \u2013 texts that describe a step-by-step process (e.g. a baking recipe). Specifically, the goal is to track various states of the entities participating in a given process. Some of the challenges for this NLP task include annotated data scarcity and annotators\u2019 reliance on commonsense knowledge to annotate implicit state information. Zhang et al. (2021) successfully incorporated commonsense entity-centric knowledge from ConceptNet into their BERT-based neural-symbolic architecture. Since English mostly encodes state change information in verbs, we attempted to test whether injecting semantic knowledge of events (retrieved from the state-of-the-art VerbNet parser) into a neural model can also improve the performance on this task. To achieve this, we adapt the methodology introduced by Zhang et al. (2021) for incorporating symbolic entity information from ConceptNet to the incorporation of VerbNet event semantics. We evaluate the performance of our model on the ProPara dataset (Mishra et al., 2018). In addition, we introduce a purely symbolic model for entity state tracking that uses a simple set of case statements, and is informed mostly by linguistic knowledge retrieved from various computational lexical resources. Our approach is inherently domain-agnostic, and our model is explainable and achieves state-of-the-art results on the Recipes dataset (Bosselut et al., 2017).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51203051",
                    "name": "Ghazaleh Kazeminejad"
                },
                {
                    "authorId": "145755155",
                    "name": "Martha Palmer"
                }
            ]
        },
        {
            "paperId": "7030fbc2e043d3389d716f0592a381eeab99c627",
            "title": "Learning Semantic Role Labeling from Compatible Label Sequences",
            "abstract": "Semantic role labeling (SRL) has multiple disjoint label sets, e.g., VerbNet and PropBank. Creating these datasets is challenging, therefore a natural question is how to use each one to help the other. Prior work has shown that cross-task interaction helps, but only explored multitask learning so far. A common issue with multi-task setup is that argument sequences are still separately decoded, running the risk of generating structurally inconsistent label sequences (as per lexicons like Semlink). In this paper, we eliminate such issue with a framework that jointly models VerbNet and PropBank labels as one sequence. In this setup, we show that enforcing Semlink constraints during decoding constantly improves the overall F1. With special input constructions, our joint model infers VerbNet arguments from given PropBank arguments with over 99 F1. For learning, we propose a constrained marginal model that learns with knowledge defined in Semlink to further benefit from the large amounts of PropBank-only data. On the joint benchmark based on CoNLL05, our models achieve state-of-the-art F1's, outperforming the prior best in-domain model by 3.5 (VerbNet) and 0.8 (PropBank). For out-of-domain generalization, our models surpass the prior best by 3.4 (VerbNet) and 0.2 (PropBank).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47387745",
                    "name": "Tao Li"
                },
                {
                    "authorId": "51203051",
                    "name": "Ghazaleh Kazeminejad"
                },
                {
                    "authorId": "1783500",
                    "name": "S. Brown"
                },
                {
                    "authorId": "145755155",
                    "name": "Martha Palmer"
                },
                {
                    "authorId": "3052879",
                    "name": "Vivek Srikumar"
                }
            ]
        },
        {
            "paperId": "990f0f881bfd50eff5a0f083dd04f37ba198f338",
            "title": "Leveraging Active Learning to Minimise SRL Annotation Across Corpora",
            "abstract": "In this paper we investigate the application of active learning to semantic role labeling (SRL) using Bayesian Active Learning by Disagreement (BALD). Our new predicate-focused selection method quickly improves efficiency on three different specialised domain corpora. This is encouraging news for researchers wanting to port SRL to domain specific applications. Interestingly, with the large and diverse \\textit{OntoNotes} corpus, the sentence selection approach, that collects a larger number of predicates, taking more time to annotate, fares better than the predicate approach. In this paper, we analyze both the selections made by our two selections methods for the various domains and the differences between these corpora in detail.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1380290459",
                    "name": "Skatje Myers"
                },
                {
                    "authorId": "145755155",
                    "name": "Martha Palmer"
                }
            ]
        },
        {
            "paperId": "a2e112f16d18a6af6752e7a49c30972898f44258",
            "title": "Widely Interpretable Semantic Representation: Frameless Meaning Representation for Broader Applicability",
            "abstract": "This paper presents a novel semantic representation, WISeR, that overcomes challenges for Abstract Meaning Representation (AMR). Despite its strengths, AMR is not easily applied to languages or domains without predefined semantic frames, and its use of numbered arguments results in semantic role labels, which are not directly interpretable and are semantically overloaded for parsers. We examine the numbered arguments of predicates in AMR and convert them to thematic roles that do not require reference to semantic frames. We create a new corpus of 1K English dialogue sentences annotated in both WISeR and AMR. WISeR shows stronger inter-annotator agreement for beginner and experienced annotators, with beginners becoming proficient in WISeR annotation more quickly. Finally, we train a state-of-the-art parser on the AMR 3.0 corpus and a WISeR corpus converted from AMR 3.0. The parser is evaluated on these corpora and our dialogue corpus. The WISeR model exhibits higher accuracy than its AMR counterpart across the board, demonstrating that WISeR is easier for parsers to learn.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144456145",
                    "name": "W. Bruce Croft"
                },
                {
                    "authorId": "144002335",
                    "name": "Jan Hajic"
                },
                {
                    "authorId": "145755155",
                    "name": "Martha Palmer"
                }
            ]
        },
        {
            "paperId": "a5ebebcf0d17d08bfa2895533b121a4411c35685",
            "title": "GLEN: General-Purpose Event Detection for Thousands of Types",
            "abstract": "The progress of event extraction research has been hindered by the absence of wide-coverage, large-scale datasets. To make event extraction systems more accessible, we build a general-purpose event detection dataset GLEN, which covers 205K event mentions with 3,465 different types, making it more than 20x larger in ontology than today's largest event dataset. GLEN is created by utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables us to use the abundant existing annotation for PropBank as distant supervision. In addition, we also propose a new multi-stage event detection model CEDAR specifically designed to handle the large ontology size in GLEN. We show that our model exhibits superior performance compared to a range of baselines including InstructGPT. Finally, we perform error analysis and show that label noise is still the largest challenge for improving performance for this new dataset. Our dataset, code, and models are released at \\url{https://github.com/ZQS1943/GLEN}.}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167027235",
                    "name": "Qiusi Zhan"
                },
                {
                    "authorId": "2109154767",
                    "name": "Sha Li"
                },
                {
                    "authorId": "39996046",
                    "name": "Kathryn Conger"
                },
                {
                    "authorId": "145755155",
                    "name": "Martha Palmer"
                },
                {
                    "authorId": "2181650518",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "d44996a04e77c990177b589261ceebee5a80ef01",
            "title": "Human-in-the-loop Schema Induction",
            "abstract": "Schema induction builds a graph representation explaining how events unfold in a scenario. Existing approaches have been based on information retrieval (IR) and information extraction (IE), often with limited human curation. We demonstrate a human-in-the-loop schema induction system powered by GPT-3. We first describe the different modules of our system, including prompting to generate schematic elements, manual edit of those elements, and conversion of those into a schema graph. By qualitatively comparing our system to previous ones, we show that our system not only transfers to new domains more easily than previous approaches, but also reduces efforts of human curation thanks to our interactive interface.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123437034",
                    "name": "Tianyi Zhang"
                },
                {
                    "authorId": "2065421682",
                    "name": "Isaac Tham"
                },
                {
                    "authorId": "2165225503",
                    "name": "Zhaoyi Hou"
                },
                {
                    "authorId": "2111472779",
                    "name": "J. Ren"
                },
                {
                    "authorId": "2202592542",
                    "name": "Liyang Zhou"
                },
                {
                    "authorId": "2283763073",
                    "name": "Hainiu Xu"
                },
                {
                    "authorId": "72436283",
                    "name": "Li Zhang"
                },
                {
                    "authorId": "145262322",
                    "name": "Lara J. Martin"
                },
                {
                    "authorId": "3372941",
                    "name": "Rotem Dror"
                },
                {
                    "authorId": "2109154767",
                    "name": "Sha Li"
                },
                {
                    "authorId": "2072975661",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "145755155",
                    "name": "Martha Palmer"
                },
                {
                    "authorId": "1783500",
                    "name": "S. Brown"
                },
                {
                    "authorId": "2209984833",
                    "name": "Reece Suchocki"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                }
            ]
        }
    ]
}