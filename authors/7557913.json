{
    "authorId": "7557913",
    "papers": [
        {
            "paperId": "016e238ff916a5d1134bd202bc4ee2013aa248bf",
            "title": "Graph Contrastive Backdoor Attacks",
            "abstract": "Graph Contrastive Learning (GCL) has attracted considerable interest due to its impressive node representation learning capability. Despite the wide application of GCL techniques, little attention has been paid to the security of GCL. In this paper, we systematically study the vulnerability of GCL in the presence of malicious backdoor adversaries. In particular, we propose GCBA , the first backdoor attack for graph contrastive learning. GCBA incorporates three attacks: poisoning, crafting, and natural backdoor, each targeting one stage of the GCL pipeline. We formulate our attacks as optimization problems and solve them with a novel discrete optimization technique to overcome the discrete nature of graph-structured data. By extensively evaluating GCBA on multiple datasets and GCL methods, we show that our attack can achieve high attack success rates while preserving stealthiness. We further consider potential countermeasures to our attack and conclude that existing defenses are insufficient to mitigate GCBA. We show that as a complex paradigm involving data and model republishing, GCL is vulnerable to backdoor attacks, and specifically designed defenses are needed to mitigate the backdoor attacks on GCL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "15115915",
                    "name": "Hangfan Zhang"
                },
                {
                    "authorId": "7557913",
                    "name": "Jinghui Chen"
                },
                {
                    "authorId": "2186399968",
                    "name": "Lu Lin"
                },
                {
                    "authorId": "143987304",
                    "name": "Jinyuan Jia"
                },
                {
                    "authorId": "2178945545",
                    "name": "Di Wu"
                }
            ]
        },
        {
            "paperId": "05c286b7d149fd5edd542f5765ca2a66f6219854",
            "title": "PAT: Geometry-Aware Hard-Label Black-Box Adversarial Attacks on Text",
            "abstract": "Despite a plethora of prior explorations, conducting text adversarial attacks in practical settings is still challenging with the following constraints: black box -- the inner structure of the victim model is unknown; hard label -- the attacker only has access to the top-1 prediction results; and semantic preservation - the perturbation needs to preserve the original semantics. In this paper, we present PAT, a novel adversarial attack method employed under all these constraints. Specifically, PAT explicitly models the adversarial and non-adversarial prototypes and incorporates them to measure semantic changes for replacement selection in the hard-label black-box setting to generate high-quality samples. In each iteration, PAT finds original words that can be replaced back and selects better candidate words for perturbed positions in a geometry-aware manner guided by this estimation, which maximally improves the perturbation construction and minimally impacts the original semantics. Extensive evaluation with benchmark datasets and state-of-the-art models shows that PAT outperforms existing text adversarial attacks in terms of both attack effectiveness and semantic preservation. Moreover, we validate the efficacy of PAT against industry-leading natural language processing platforms in real-world settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1898157310",
                    "name": "Muchao Ye"
                },
                {
                    "authorId": "7557913",
                    "name": "Jinghui Chen"
                },
                {
                    "authorId": "49434299",
                    "name": "Chenglin Miao"
                },
                {
                    "authorId": "2215701914",
                    "name": "Han Liu"
                },
                {
                    "authorId": "2155394026",
                    "name": "Ting Wang"
                },
                {
                    "authorId": "2068198592",
                    "name": "Fenglong Ma"
                }
            ]
        },
        {
            "paperId": "41cc3338635cde85316a0bcb934bffc73008761a",
            "title": "Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks",
            "abstract": "Pre-trained language models (PLMs) have demonstrated remarkable performance as few-shot learners. However, their security risks under such settings are largely unexplored. In this work, we conduct a pilot study showing that PLMs as few-shot learners are highly vulnerable to backdoor attacks while existing defenses are inadequate due to the unique challenges of few-shot scenarios. To address such challenges, we advocate MDP, a novel lightweight, pluggable, and effective defense for PLMs as few-shot learners. Specifically, MDP leverages the gap between the masking-sensitivity of poisoned and clean samples: with reference to the limited few-shot data as distributional anchors, it compares the representations of given samples under varying masking and identifies poisoned samples as ones with significant variations. We show analytically that MDP creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness. The empirical evaluation using benchmark datasets and representative attacks validates the efficacy of MDP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51195718",
                    "name": "Zhaohan Xi"
                },
                {
                    "authorId": "2056897605",
                    "name": "Tianyu Du"
                },
                {
                    "authorId": "2145413923",
                    "name": "Changjiang Li"
                },
                {
                    "authorId": "80866871",
                    "name": "Ren Pang"
                },
                {
                    "authorId": "2081160",
                    "name": "S. Ji"
                },
                {
                    "authorId": "7557913",
                    "name": "Jinghui Chen"
                },
                {
                    "authorId": "2068197354",
                    "name": "Fenglong Ma"
                },
                {
                    "authorId": "2155394026",
                    "name": "Ting Wang"
                }
            ]
        },
        {
            "paperId": "cd29c25c489562b409a60f83365f93f33ee1a0a1",
            "title": "Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM",
            "abstract": "Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments on open-source large language models, we demonstrate that RA-LLM can successfully defend against both state-of-the-art adversarial prompts and popular handcrafted jailbreaking prompts by reducing their attack success rates from nearly 100% to around 10% or less.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Bochuan Cao"
                },
                {
                    "authorId": "143695559",
                    "name": "Yu Cao"
                },
                {
                    "authorId": "2110924166",
                    "name": "Lu Lin"
                },
                {
                    "authorId": "7557913",
                    "name": "Jinghui Chen"
                }
            ]
        },
        {
            "paperId": "efaab402913dbec3a01a503803dda48554be97e8",
            "title": "On the Vulnerability of Backdoor Defenses for Federated Learning",
            "abstract": "Federated learning (FL) is a popular distributed machine learning paradigm which enables jointly training a global model without sharing clients' data. However, its repetitive server-client communication gives room for possible backdoor attacks which aims to mislead the global model into a targeted misprediction when a specific trigger pattern is presented. In response to such backdoor threats on federated learning, various defense measures have been proposed. In this paper, we study whether the current defense mechanisms truly neutralize the backdoor threats from federated learning in a practical setting by proposing a new federated backdoor attack framework for possible countermeasures. Different from traditional training (on triggered data) and rescaling (the malicious client model) based backdoor injection, the proposed backdoor attack framework (1) directly modifies (a small proportion of) local model weights to inject the backdoor trigger via sign flips; (2) jointly optimize the trigger pattern with the client model, thus is more persistent and stealthy for circumventing existing defenses. In a case study, we examine the strength and weaknesses of several recent federated backdoor defenses from three major categories and provide suggestions to the practitioners when training federated models in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2070258658",
                    "name": "Pei Fang"
                },
                {
                    "authorId": "7557913",
                    "name": "Jinghui Chen"
                }
            ]
        },
        {
            "paperId": "03013e291fb3192b286147f5bdb5770e434f91b2",
            "title": "Do Language Models Plagiarize?",
            "abstract": "Past literature has illustrated that language models (LMs) often memorize parts of training instances and reproduce them in natural language generation (NLG) processes. However, it is unclear to what extent LMs \u201creuse\u201d a training corpus. For instance, models can generate paraphrased sentences that are contextually similar to training samples. In this work, therefore, we study three types of plagiarism (i.e., verbatim, paraphrase, and idea) among GPT-2 generated texts, in comparison to its training data, and further analyze the plagiarism patterns of fine-tuned LMs with domain-specific corpora which are extensively used in practice. Our results suggest that (1) three types of plagiarism widely exist in LMs beyond memorization, (2) both size and decoding methods of LMs are strongly associated with the degrees of plagiarism they exhibit, and (3) fine-tuned LMs\u2019 plagiarism patterns vary based on their corpus similarity and homogeneity. Given that a majority of LMs\u2019 training data is scraped from the Web without informing content owners, their reiteration of words, phrases, and even core ideas from training sets into generated texts has ethical implications. Their patterns are likely to exacerbate as both the size of LMs and their training data increase, raising concerns about indiscriminately pursuing larger models with larger training corpora. Plagiarized content can also contain individuals\u2019 personal and sensitive information. These findings overall cast doubt on the practicality of current LMs in mission-critical writing tasks and urge more discussions around the observed phenomena. Data and source code are available at https://github.com/Brit7777/LM-plagiarism.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116713871",
                    "name": "Jooyoung Lee"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "7557913",
                    "name": "Jinghui Chen"
                },
                {
                    "authorId": "2158951945",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "3f8af18256f5b158510ec248974568db25495e53",
            "title": "Learnability Lock: Authorized Learnability Control Through Adversarial Invertible Transformations",
            "abstract": "Owing much to the revolution of information technology, the recent progress of deep learning benefits incredibly from the vastly enhanced access to data available in various digital formats. However, in certain scenarios, people may not want their data being used for training commercial models and thus studied how to attack the learnability of deep learning models. Previous works on learnability attack only consider the goal of preventing unauthorized exploitation on the specific dataset but not the process of restoring the learnability for authorized cases. To tackle this issue, this paper introduces and investigates a new concept called\"learnability lock\"for controlling the model's learnability on a specific dataset with a special key. In particular, we propose adversarial invertible transformation, that can be viewed as a mapping from image to image, to slightly modify data samples so that they become\"unlearnable\"by machine learning models with negligible loss of visual features. Meanwhile, one can unlock the learnability of the dataset and train models normally using the corresponding key. The proposed learnability lock leverages class-wise perturbation that applies a universal transformation function on data samples of the same label. This ensures that the learnability can be easily restored with a simple inverse transformation while remaining difficult to be detected or reverse-engineered. We empirically demonstrate the success and practicability of our method on visual classification tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2067863326",
                    "name": "Weiqi Peng"
                },
                {
                    "authorId": "7557913",
                    "name": "Jinghui Chen"
                }
            ]
        },
        {
            "paperId": "43d65091532f280990245ca1dc73706d480656dc",
            "title": "RoCourseNet: Distributionally Robust Training of a Prediction Aware Recourse Model",
            "abstract": "Counterfactual (CF) explanations for machine learning (ML) models are preferred by end-users, as they explain the predictions of ML models by providing a recourse case to individuals who are adversely impacted by predicted outcomes. Existing CF explanation methods generate recourses under the assumption that the underlying target ML model remains stationary over time. However, due to commonly occurring distributional shifts in training data, ML models constantly get updated in practice, which might render previously generated recourses invalid and diminish end-users trust in our algorithmic framework. To address this problem, we propose RoCourseNet, a training framework that jointly optimizes for predictions and robust recourses to future data shifts. We have three main contributions: (i) We propose a novel virtual data shift (VDS) algorithm to \ufb01nd worst-case shifted ML models by explicitly considering the worst-case data shift in the training dataset. (ii) We leverage adversarial training to solve a novel tri-level optimization problem inside RoCourseNet, which simultaneously generates predictions and corresponding robust recourses. (iii) Finally, we evaluate RoCourseNet\u2019s performance on three real-world datasets and show that RoCourseNet outperforms state-of-the-art baselines by \u223c 10% in generating robust CF explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110878688",
                    "name": "Hangzhi Guo"
                },
                {
                    "authorId": "2007511920",
                    "name": "Feiran Jia"
                },
                {
                    "authorId": "7557913",
                    "name": "Jinghui Chen"
                },
                {
                    "authorId": "1692654",
                    "name": "A. Squicciarini"
                },
                {
                    "authorId": "34903979",
                    "name": "A. Yadav"
                }
            ]
        },
        {
            "paperId": "4fac6a8d86261484983729f3c7d466677d4cf359",
            "title": "Spectral Augmentation for Self-Supervised Learning on Graphs",
            "abstract": "Graph contrastive learning (GCL), as an emerging self-supervised learning technique on graphs, aims to learn representations via instance discrimination. Its performance heavily relies on graph augmentation to reflect invariant patterns that are robust to small perturbations; yet it still remains unclear about what graph invariance GCL should capture. Recent studies mainly perform topology augmentations in a uniformly random manner in the spatial domain, ignoring its influence on the intrinsic structural properties embedded in the spectral domain. In this work, we aim to find a principled way for topology augmentations by exploring the invariance of graphs from the spectral perspective. We develop spectral augmentation which guides topology augmentations by maximizing the spectral change. Extensive experiments on both graph and node classification tasks demonstrate the effectiveness of our method in self-supervised representation learning. The proposed method also brings promising generalization capability in transfer learning, and is equipped with intriguing robustness property under adversarial attacks. Our study sheds light on a general principle for graph topology augmentation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9538465",
                    "name": "Lu Lin"
                },
                {
                    "authorId": "7557913",
                    "name": "Jinghui Chen"
                },
                {
                    "authorId": "2108883660",
                    "name": "Hongning Wang"
                }
            ]
        },
        {
            "paperId": "6d01891e71f99b8365292689f7792839bed2a305",
            "title": "One-shot Neural Backdoor Erasing via Adversarial Weight Masking",
            "abstract": "Recent studies show that despite achieving high accuracy on a number of real-world applications, deep neural networks (DNNs) can be backdoored: by injecting triggered data samples into the training dataset, the adversary can mislead the trained model into classifying any test data to the target class as long as the trigger pattern is presented. To nullify such backdoor threats, various methods have been proposed. Particularly, a line of research aims to purify the potentially compromised model. However, one major limitation of this line of work is the requirement to access sufficient original training data: the purifying performance is a lot worse when the available training data is limited. In this work, we propose Adversarial Weight Masking (AWM), a novel method capable of erasing the neural backdoors even in the one-shot setting. The key idea behind our method is to formulate this into a min-max optimization problem: first, adversarially recover the trigger patterns and then (soft) mask the network weights that are sensitive to the recovered patterns. Comprehensive evaluations of several benchmark datasets suggest that AWM can largely improve the purifying effects over other state-of-the-art methods on various available training dataset sizes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2124028323",
                    "name": "Shuwen Chai"
                },
                {
                    "authorId": "7557913",
                    "name": "Jinghui Chen"
                }
            ]
        }
    ]
}