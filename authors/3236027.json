{
    "authorId": "3236027",
    "papers": [
        {
            "paperId": "0549fbdd35a010ae1b8eebc18975144a35d45e1c",
            "title": "Asd-Transformer: Efficient Active Speaker Detection Using Self And Multimodal Transformers",
            "abstract": "Multimodal active speaker detection (ASD) methods assign a speaking/not-speaking label per individual in a video clip. ASD is critical for applications such as natural human-computer interaction, speaker diarization, and video reframing. Recent work has shown the success of transformers in multimodal settings, thus we propose a novel framework that leverages modern transformer and concatenation mechanisms to efficiently capture the interaction between audio and video modalities for ASD. We achieve mAP similar to state-of-the-art (93.0% vs 93.5%) on the AVA-ActiveSpeaker dataset. Further, our model has ~3\u00d7 smaller size (15.23MB vs 49.82MB), reduced FLOPs count (11.8 vs 14.3), and lower training time (15h vs 38h). To verify our model is making predictions from the right visual cues, we computed saliency maps over input images. We found that in addition to mouth regions, the nose, cheek, and area under the eye were helpful in identifying active speakers. Our ablation study reveals that the mouth region alone achieved lower mAP (91.9% vs 93.0%) compared to full face region, supporting our hypothesis that facial expressions in addition to mouth region are useful for ASD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1382640493",
                    "name": "G. Datta"
                },
                {
                    "authorId": "51207767",
                    "name": "Tyler Etchart"
                },
                {
                    "authorId": "2145574137",
                    "name": "Vivek Yadav"
                },
                {
                    "authorId": "3236027",
                    "name": "Varsha Hedau"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "2122374530",
                    "name": "Shih-Fu Chang"
                }
            ]
        },
        {
            "paperId": "7168266cd8a4bc171e61a0c881857f1676d85807",
            "title": "Few-Shot Gaze Estimation with Model Offset Predictors",
            "abstract": "Due to the variance of optical properties across different people, the performance of a person-agnostic gaze estimation model may not generalize well on a specific person. Though one may achieve better performance by training a person-specific model, it typically requires a large number of samples which is not available in real-life scenarios. Hence, few-shot gaze estimation method is preferred for the small number of samples from a target person. However, the key question is how to close the performance gap between a \"few-shot\" model and the \"many-shot\" model. In this paper, we propose to learn a person-specific offset predictor which outputs the difference between the person-agnostic model and the many-shot person-specific model with as few as one training sample. We adapt the knowledge to a new person by using the average of meta-learned offset predictors parameters as the initialization of the new offset predictor. Experiments show that the proposed few-shot person-specific model is not only closer to the corresponding many-shot person-specific model but also has better accuracy than the SOTA few-shot gaze estimation methods in multiple gaze datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152320135",
                    "name": "Jiawei Ma"
                },
                {
                    "authorId": "2115462987",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "2109035901",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "3236027",
                    "name": "Varsha Hedau"
                },
                {
                    "authorId": "2122374530",
                    "name": "Shih-Fu Chang"
                }
            ]
        },
        {
            "paperId": "9690e7056c972e298c6696fa4b1f485b82ded433",
            "title": "FashionVLP: Vision Language Transformer for Fashion Retrieval with Feedback",
            "abstract": "Fashion image retrieval based on a query pair of reference image and natural language feedback is a challenging task that requires models to assess fashion related information from visual and textual modalities simultaneously. We propose a new vision-language transformer based model, FashionVLP, that brings the prior knowledge contained in large image-text corpora to the domain of fashion image retrieval, and combines visual information from multiple levels of context to effectively capture fashion-related information. While queries are encoded through the transformer layers, our asymmetric design adopts a novel attention-based approach for fusing target image features without involving text or transformer layers in the process. Extensive results show that FashionVLP achieves the state-of-the-art performance on benchmark datasets, with a large 23% relative improvement on the challenging FashionIQ dataset, which contains complex natural language feedback.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2096063076",
                    "name": "Sonam Goenka"
                },
                {
                    "authorId": "29962444",
                    "name": "Zhao-Heng Zheng"
                },
                {
                    "authorId": "26393556",
                    "name": "Ayush Jaiswal"
                },
                {
                    "authorId": "147887099",
                    "name": "Rakesh Chada"
                },
                {
                    "authorId": "1887625",
                    "name": "Yuehua Wu"
                },
                {
                    "authorId": "3236027",
                    "name": "Varsha Hedau"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                }
            ]
        },
        {
            "paperId": "e1d53567b15d50cc1f58729083cbff0460cd395b",
            "title": "Enhancing Fairness in Face Detection in Computer Vision Systems by Demographic Bias Mitigation",
            "abstract": "Fairness has become an important agenda in computer vision and artificial intelligence. Recent studies have shown that many computer vision models and datasets exhibit demographic biases and proposed mitigation strategies. These works attempt to address accuracy disparity, spurious correlations, or unbalanced representations in datasets in tasks such as face recognition, verification and expression and attribute classification. These tasks, however, all require face detection as the first preprocessing step, and surprisingly, there has been little effort in identifying or mitigating biases in face detection. Biased face detectors themselves pose a threat against fair and ethical AI systems, and their biases may be further passed on to subsequent downstream tasks such as face recognition in a computer vision pipeline. This paper therefore investigates the problem of biases in face detection, focusing on accuracy disparity of detectors between demographic groups including gender, age group, and skin tone. We collect perceived demographic attributes on a popular face detection benchmark dataset, WIDER FACE, report skewed demographic distributions, and compare detection performance between groups. In order to mitigate the biases, we apply three mitigation methods that have been introduced in the recent literature and also propose two novel methods. Experimental results show that these methods are effective in reducing demographic biases. We also discuss how the effectiveness varies by demographic attributes, detection easiness, and multiple detectors, which will shed light on this new topic of addressing face detection bias.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116468252",
                    "name": "Yu Yang"
                },
                {
                    "authorId": "50178621",
                    "name": "Aayush Gupta"
                },
                {
                    "authorId": "1384556269",
                    "name": "Jianfeng Feng"
                },
                {
                    "authorId": "3222821",
                    "name": "Prateek Singhal"
                },
                {
                    "authorId": "2145574137",
                    "name": "Vivek Yadav"
                },
                {
                    "authorId": "2109035901",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "3236027",
                    "name": "Varsha Hedau"
                },
                {
                    "authorId": "1834047",
                    "name": "Jungseock Joo"
                }
            ]
        },
        {
            "paperId": "5f6ec91e7de9a53942d3b168b3b856cb6ea0c5bf",
            "title": "ANTNets: Mobile Convolutional Neural Networks for Resource Efficient Image Classification",
            "abstract": "Deep convolutional neural networks have achieved remarkable success in computer vision. However, deep neural networks require large computing resources to achieve high performance. Although depthwise separable convolution can be an efficient module to approximate a standard convolution, it often leads to reduced representational power of networks. In this paper, under budget constraints such as computational cost (MAdds) and the parameter count, we propose a novel basic architectural block, ANTBlock. It boosts the representational power by modeling, in a high dimensional space, interdependency of channels between a depthwise convolution layer and a projection layer in the ANTBlocks. Our experiments show that ANTNet built by a sequence of ANTBlocks, consistently outperforms state-of-the-art low-cost mobile convolutional neural networks across multiple datasets. On CIFAR100, our model achieves 75.7% top-1 accuracy, which is 1.5% higher than MobileNetV2 with 8.3% fewer parameters and 19.6% less computational cost. On ImageNet, our model achieves 72.8% top-1 accuracy, which is 0.8% improvement, with 157.7ms (20% faster) on iPhone 5s over MobileNetV2.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2760194",
                    "name": "Yunyang Xiong"
                },
                {
                    "authorId": "2154858223",
                    "name": "Hyunwoo Kim"
                },
                {
                    "authorId": "3236027",
                    "name": "Varsha Hedau"
                }
            ]
        },
        {
            "paperId": "7f2f299e56c591df8ba7f1fb8e98d78eb9237a31",
            "title": "Recovering free space of indoor scenes from a single image",
            "abstract": "In this paper we consider the problem of recovering the free space of an indoor scene from its single image. We show that exploiting the box like geometric structure of furniture and constraints provided by the scene, allows us to recover the extent of major furniture objects in 3D. Our \u201cboxy\u201d detector localizes box shaped objects oriented parallel to the scene across different scales and object types, and thus blocks out the occupied space in the scene. To localize the objects more accurately in 3D we introduce a set of specially designed features that capture the floor contact points of the objects. Image based metrics are not very indicative of performance in 3D. We make the first attempt to evaluate single view based occupancy estimates for 3D errors and propose several task driven performance measures towards it. On our dataset of 592 indoor images marked with full 3D geometry of the scene, we show that: (a) our detector works well using image based metrics; (b) our refinement method produces significant improvements in localization in 3D; and (c) if one evaluates using 3D metrics, our method offers major improvements over other single view based scene geometry estimation methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3236027",
                    "name": "Varsha Hedau"
                },
                {
                    "authorId": "2433269",
                    "name": "Derek Hoiem"
                },
                {
                    "authorId": "144016256",
                    "name": "D. Forsyth"
                }
            ]
        },
        {
            "paperId": "e514643aa5f819e3ceca36aed01ebeb05caeae4e",
            "title": "3D City Modeling from Street-Level Data for Augmented Reality Applications",
            "abstract": "We present a method for automatically creating compact and accurate 3D city models needed for enhanced Augmented Reality applications. The input data are panorama images and LIDAR scans collected at street level and positioned using an IMU and a GPS. Our method corrects for the GPS error and the IMU drift to produce a globally consistent and well registered dataset for the whole city. We use structure from motion and skyline detection to complement the limited range of LIDAR data. Additionally, we propose a novel reconstruction technique that exploits architectural properties of urban environments to create an accurate 3D city model from incomplete data. Our method is able to process an entire city, or several terabytes of data, in a matter of days. We show that our reconstruction achieves higher accuracy than a commercial solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2453822",
                    "name": "Timo Pylv\u00e4n\u00e4inen"
                },
                {
                    "authorId": "3255177",
                    "name": "J. Berclaz"
                },
                {
                    "authorId": "1795753",
                    "name": "Thommen Korah"
                },
                {
                    "authorId": "3236027",
                    "name": "Varsha Hedau"
                },
                {
                    "authorId": "2297496",
                    "name": "Mridul Aanjaneya"
                },
                {
                    "authorId": "2026212",
                    "name": "R. Grzeszczuk"
                }
            ]
        },
        {
            "paperId": "08aedeb74dda306a14c699ffcef4f434a60f34e8",
            "title": "3D spatial layout and geometric constraints for scene understanding",
            "abstract": "An image is nothing but a projection of the physical world around us, where objects do not occur randomly but follow certain spatial rules. Many existing computer vision approaches tend to ignore this aspect of understanding images. In this work, we build representations and propose strategies for exploiting such constraints towards extracting a 3D understanding of a scene from its single image. \nWe model a scene in terms of its spatial layout abstracted as a box, object cuboids, camera viewpoint, and interactions between them. We take a supervised approach towards estimation, and learn models from training data that is fully annotated with the 3D spatial extent of objects, walls, and floor. We assume the world is populated with axis aligned objects and surfaces, and exploit constrained appearance models which use geometric cues from the scene. Our methods are tailored towards indoor scenes that are highly structured and require careful spatial reasoning. \nWe show that our box layout representation is able to capture the full spatial extent of a 3D scene, which we can successfully estimate even for heavily cluttered rooms. Similarly, by exploiting the geometric constraints offered by the scene, we can approximate the extent of the objects as cuboids in 3D. The box layout provides rich contextual information for detecting objects. We show that modeling the 3D interactions between object cuboids and scene layout improves object detection. Finally, we show how to use our 3D spatial layout models together with object cuboid models to predict the free space in the scene.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3236027",
                    "name": "Varsha Hedau"
                }
            ]
        },
        {
            "paperId": "66bb1604482741a16f3abb1bfb1c531790c66a57",
            "title": "Rendering synthetic objects into legacy photographs",
            "abstract": "We propose a method to realistically insert synthetic objects into existing photographs without requiring access to the scene or any additional scene measurements. With a single image and a small amount of annotation, our method creates a physical model of the scene that is suitable for realistically rendering synthetic objects with diffuse, specular, and even glowing materials while accounting for lighting interactions between the objects and the scene. We demonstrate in a user study that synthetic images produced by our method are confusable with real scenes, even for people who believe they are good at telling the difference. Further, our study shows that our method is competitive with other insertion methods while requiring less scene information. We also collected new illumination and reflectance datasets; renderings produced by our system compare well to ground truth. Our system has applications in the movie and gaming industry, as well as home decorating and user content creation, among others.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37615584",
                    "name": "Kevin Karsch"
                },
                {
                    "authorId": "3236027",
                    "name": "Varsha Hedau"
                },
                {
                    "authorId": "144016256",
                    "name": "D. Forsyth"
                },
                {
                    "authorId": "2433269",
                    "name": "Derek Hoiem"
                }
            ]
        },
        {
            "paperId": "451a06626afe8dd70099c7dfec86de7af909a062",
            "title": "Recovering the spatial layout of cluttered rooms",
            "abstract": "In this paper, we consider the problem of recovering the spatial layout of indoor scenes from monocular images. The presence of clutter is a major problem for existing single-view 3D reconstruction algorithms, most of which rely on finding the ground-wall boundary. In most rooms, this boundary is partially or entirely occluded. We gain robustness to clutter by modeling the global room space with a parameteric 3D \u201cbox\u201d and by iteratively localizing clutter and refitting the box. To fit the box, we introduce a structured learning algorithm that chooses the set of parameters to minimize error, based on global perspective cues. On a dataset of 308 images, we demonstrate the ability of our algorithm to recover spatial layout in cluttered rooms and show several examples of estimated free space.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3236027",
                    "name": "Varsha Hedau"
                },
                {
                    "authorId": "2433269",
                    "name": "Derek Hoiem"
                },
                {
                    "authorId": "144016256",
                    "name": "D. Forsyth"
                }
            ]
        }
    ]
}