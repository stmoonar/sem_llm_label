{
    "authorId": "1830448175",
    "papers": [
        {
            "paperId": "34ee38fac516e35a312ff67d02fc1c3355173266",
            "title": "Debiasing Counterfactual Context With Causal Inference for Multi-Turn Dialogue Reasoning",
            "abstract": "In the multi-turn dialogue reasoning task, existing models conduct word-level interaction on the entire context to gather reasoning evidence, which aims to select the logically correct one from the candidate response options. Observing the fact that the salient reasoning evidence usually comes from certain snippets of the whole dialogue session, one promising study direction is to explicitly identify the candidate reasoning contexts correlated with the dialogue reasoning options, called option-related contexts, and then make logical inference among them. However, such option-related contexts are stained with noisy information. As a result, existing models may reason unfairly with biased context and select wrong options. To tackle the context bias problem, in this article, we propose a novel CounterFactual learning framework for Dialogue Reasoning, named CF-DialReas, which mitigates the bias information by subtracting the counterfactual representation from the total causal representation. Specifically, we consider two scenarios, i.e., factual dialogue reasoning where the whole context is available to estimate the total causal representation, and the counterfactual dialogue reasoning, which firstly utilizes three different types of utterance selectors to select option-unrelated context, and then only the option-unrelated context is available to guess the counterfactual representation. Experimental results on two public dialogue reasoning datasets show that the model with our mechanism can obtain higher ranking measures, validating the effectiveness of counterfactual learning of CF-DialReas. Further analysis on the generality of CF-DialReas shows that our counterfactual learning mechanism is generally effective to the widely-used models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108601408",
                    "name": "Xu Wang"
                },
                {
                    "authorId": "2108769177",
                    "name": "Hainan Zhang"
                },
                {
                    "authorId": "46250837",
                    "name": "Shuai Zhao"
                },
                {
                    "authorId": "1830448175",
                    "name": "Hongshen Chen"
                },
                {
                    "authorId": "1775738",
                    "name": "Zhuoye Ding"
                },
                {
                    "authorId": "2166257158",
                    "name": "Zhiguo Wan"
                },
                {
                    "authorId": "2055922628",
                    "name": "Bo Cheng"
                },
                {
                    "authorId": "2248140925",
                    "name": "Yanyan Lan"
                }
            ]
        },
        {
            "paperId": "22ed27ccf4223e95a4e165cadbbbaec3102b49a2",
            "title": "Contrastive Learning with Dialogue Attributes for Neural Dialogue Generation",
            "abstract": "Designing an effective learning method remains a challenge in neural dialogue generation systems as it requires the training objective to well approximate the intrinsic human-preferred dialogue properties. Conventional training approaches such as maximum likelihood estimation focus on modeling general syntactic patterns and may fail to capture intricate conversational characteristics. Contrastive dialogue learning offers an effective training schema by explicitly training a neural dialogue model on multiple positive and negative conversational pairs. However, constructing contrastive learning pairs is non-trivial, and multiple dialogue attributes have been found to be crucial for governing the human judgments of conversations. This paper proposes to guide the response generation with attribute-aware contrastive learning to improve the overall quality of the generated responses, where contrastive learning samples are generated according to various important dialogue attributes each specializing in a different principle of conversation. Extensive experiments show that our proposed techniques are crucial to achieving superior model performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216390603",
                    "name": "Jie Tan"
                },
                {
                    "authorId": "22561596",
                    "name": "Hengyi Cai"
                },
                {
                    "authorId": "1830448175",
                    "name": "Hongshen Chen"
                },
                {
                    "authorId": "2148630945",
                    "name": "Hong Cheng"
                },
                {
                    "authorId": "1702243",
                    "name": "Helen M. Meng"
                },
                {
                    "authorId": "1775738",
                    "name": "Zhuoye Ding"
                }
            ]
        },
        {
            "paperId": "3eb9879355059f19a2d07e862e54699b278328e5",
            "title": "Vague Preference Policy Learning for Conversational Recommendation",
            "abstract": "Conversational recommendation systems (CRS) commonly assume users have clear preferences, leading to potential over-filtering of relevant alternatives. However, users often exhibit vague, non-binary preferences. We introduce the Vague Preference Multi-round Conversational Recommendation (VPMCR) scenario, employing a soft estimation mechanism to accommodate users' vague and dynamic preferences while mitigating over-filtering. In VPMCR, we propose Vague Preference Policy Learning (VPPL), consisting of Ambiguity-aware Soft Estimation (ASE) and Dynamism-aware Policy Learning (DPL). ASE captures preference vagueness by estimating scores for clicked and non-clicked options, using a choice-based approach and time-aware preference decay. DPL leverages ASE's preference distribution to guide the conversation and adapt to preference changes for recommendations or attribute queries. Extensive experiments demonstrate VPPL's effectiveness within VPMCR, outperforming existing methods and setting a new benchmark. Our work advances CRS by accommodating users' inherent ambiguity and relative decision-making processes, improving real-world applicability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47776511",
                    "name": "Gangyi Zhang"
                },
                {
                    "authorId": "31446099",
                    "name": "Chongming Gao"
                },
                {
                    "authorId": "39165620",
                    "name": "Wenqiang Lei"
                },
                {
                    "authorId": "46909769",
                    "name": "Xiaojie Guo"
                },
                {
                    "authorId": "2137366124",
                    "name": "Shijun Li"
                },
                {
                    "authorId": "2116666963",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "1830448175",
                    "name": "Hongshen Chen"
                },
                {
                    "authorId": "2219273831",
                    "name": "Zhuozhi Ding"
                },
                {
                    "authorId": "1752741172",
                    "name": "Sulong Xu"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "ab4ccd52d0b32437399a9658549e175cd84bbfa9",
            "title": "Automatic Marketing Theme and Commodity Construction System for E-commerce",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2271209966",
                    "name": "Zhiping Wang"
                },
                {
                    "authorId": "2273589248",
                    "name": "Peng Lin"
                },
                {
                    "authorId": "2108769177",
                    "name": "Hainan Zhang"
                },
                {
                    "authorId": "1830448175",
                    "name": "Hongshen Chen"
                },
                {
                    "authorId": "2272187064",
                    "name": "Tianhao Li"
                },
                {
                    "authorId": "2240540876",
                    "name": "Zhuoye Ding"
                },
                {
                    "authorId": "2239394494",
                    "name": "Sulong Xu"
                },
                {
                    "authorId": "2271810352",
                    "name": "Jinghe Hu"
                }
            ]
        },
        {
            "paperId": "c37162adaca1d8e697d6531321d6566ea28e9be4",
            "title": "Embracing Uncertainty: Adaptive Vague Preference Policy Learning for Multi-round Conversational Recommendation",
            "abstract": "Conversational recommendation systems (CRS) effectively address information asymmetry by dynamically eliciting user preferences through multi-turn interactions. Existing CRS widely assumes that users have clear preferences, i.e., users have a firm belief about the fine-grained preference for one or multiple target items. Under this assumption, the agent will completely trust the user feedback and treat the accepted or rejected signals as strong indicators to filter items and reduce the candidate space, which may lead to the problem of over-filtering. However, in reality, users\u2019 preferences are often vague and volatile, with uncertainty about their desires and changing decisions during interactions. To address this issue, we introduce a novel scenario called Vague Preference Multi-round Conversational Recommendation (VPMCR), whichconsidersusers\u2019vagueandvolatilepreferencesinCRS.VPMCR employsasoftestimationmechanismtoassignanon-zeroconfi-dencescoreforallcandidateitemstobedisplayed,naturallyavoid-ingtheover-filteringproblem.IntheVPMCRsetting,weintro-duceansolutioncalledAdaptiveVaguePreferencePolicyLearning (AVPPL),whichconsistsoftwomaincomponents:Uncertainty-awareSoftEstimation(USE)andUncertainty-awarePolicyLearn-ing(UPL).USEestimatestheuncertaintyofusers\u2019vaguefeedback andcapturestheirdynamicpreferencesusingachoice-basedprefer-encesextractionmoduleandatime-awaredecayingstrategy.UPL leveragesthepreferencedistributionestimatedbyUSEtoguidethe conversationandadapttochangesinusers\u2019preferencestomake recommendationsoraskforattributes. Ourextensiveexperimentsdemonstratetheeffectivenessofour methodintheVPMCRscenario,highlightingitspotentialforprac- ticalapplicationsandimprovingtheoverallperformanceandappli-cabilityofCRSinreal-worldsettings,particularlyforuserswith vagueordynamicpreferences",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47776511",
                    "name": "Gangyi Zhang"
                },
                {
                    "authorId": "31446099",
                    "name": "Chongming Gao"
                },
                {
                    "authorId": "39165620",
                    "name": "Wenqiang Lei"
                },
                {
                    "authorId": "33465926",
                    "name": "Xiaojie Guo"
                },
                {
                    "authorId": "2137366124",
                    "name": "Shijun Li"
                },
                {
                    "authorId": "2257138073",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "1830448175",
                    "name": "Hongshen Chen"
                },
                {
                    "authorId": "2219273831",
                    "name": "Zhuozhi Ding"
                },
                {
                    "authorId": "2239394494",
                    "name": "Sulong Xu"
                },
                {
                    "authorId": "2239071206",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "ffadd54ed8fc11a5a03ca95c302e526a68b98ca5",
            "title": "From spoken dialogue to formal summary: An utterance rewriting for dialogue summarization",
            "abstract": "Due to the dialogue characteristics of unstructured contexts and multi-parties with first-person perspective, many successful text summarization works have failed when dealing with dialogue summarization. In dialogue summarization task, the input dialogue is usually spoken style with ellipsis and co-references but the output summaries are more formal and complete. Therefore, the dialogue summarization model should be able to complete the ellipsis content and co-reference information and then produce a suitable summary accordingly. However, the current state-of-the-art models pay more attention on the topic or structure of summary, rather than the consistency of dialogue summary with its input dialogue context, which may suffer from the personal and logical inconsistency problem. In this paper, we propose a new model, named ReWriteSum, to tackle this problem. Firstly, an utterance rewriter is conducted to complete the ellipsis content of dialogue content and then obtain the rewriting utterances. Then, the co-reference data augmentation mechanism is utilized to replace the referential person name with its specific name to enhance the personal information. Finally, the rewriting utterances and the co-reference replacement data are used in the standard BART model. Experimental results on both SAMSum and DialSum datasets show that our ReWriteSum significantly outperforms baseline models, in terms of both metric-based and human evaluations. Further analysis on multi-speakers also shows that ReWriteSum can obtain relatively higher improvement with more speakers, validating the correctness and property of ReWriteSum.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112809665",
                    "name": "Yue Fang"
                },
                {
                    "authorId": "2108769177",
                    "name": "Hainan Zhang"
                },
                {
                    "authorId": "1830448175",
                    "name": "Hongshen Chen"
                },
                {
                    "authorId": "1775738",
                    "name": "Zhuoye Ding"
                },
                {
                    "authorId": "2175483055",
                    "name": "Bo Long"
                },
                {
                    "authorId": "37510256",
                    "name": "Yanyan Lan"
                },
                {
                    "authorId": "7743551",
                    "name": "Yanquan Zhou"
                }
            ]
        },
        {
            "paperId": "0f88bb2df2c7c2e0ee5ec95cf7459f07b9395fb4",
            "title": "Augmenting Knowledge-grounded Conversations with Sequential Knowledge Transition",
            "abstract": "Knowledge data are massive and widespread in the real-world, which can serve as good external sources to enrich conversations. However, in knowledge-grounded conversations, current models still lack the fine-grained control over knowledge selection and integration with dialogues, which finally leads to the knowledge-irrelevant response generation problems: 1) knowledge selection merely relies on the dialogue context, ignoring the inherent knowledge transitions along with conversation flows; 2) the models often over-fit during training, resulting with incoherent response by referring to unrelated tokens from specific knowledge content in the testing phase; 3) although response is generated upon the dialogue history and knowledge, the models often tend to overlook the selected knowledge, and hence generates knowledge-irrelevant response. To address these problems, we proposed to explicitly model the knowledge transition in sequential multi-turn conversations by abstracting knowledge into topic tags. Besides, to fully utilizing the selected knowledge in generative process, we propose pre-training a knowledge-aware response generator to pay more attention on the selected knowledge. In particular, a sequential knowledge transition model equipped with a pre-trained knowledge-aware response generator (SKT-KG) formulates the high-level knowledge transition and fully utilizes the limited knowledge data. Experimental results on both structured and unstructured knowledge-grounded dialogue benchmarks indicate that our model achieves better performance over baseline models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "146950185",
                    "name": "Haolan Zhan"
                },
                {
                    "authorId": "2108769177",
                    "name": "Hainan Zhang"
                },
                {
                    "authorId": "1830448175",
                    "name": "Hongshen Chen"
                },
                {
                    "authorId": "1775738",
                    "name": "Zhuoye Ding"
                },
                {
                    "authorId": "1830443292",
                    "name": "Yongjun Bao"
                },
                {
                    "authorId": "37510256",
                    "name": "Yanyan Lan"
                }
            ]
        },
        {
            "paperId": "269bd9452a29dcdcb30488a18712453eebf2c632",
            "title": "Identifying Untrustworthy Samples: Data Filtering for Open-domain Dialogues with Bayesian Optimization",
            "abstract": "Being able to reply with a related, fluent, and informative response is an indispensable requirement for building high-quality conversational agents. In order to generate better responses, some approaches have been proposed, such as feeding extra information by collecting large-scale datasets with human annotations, designing neural conversational models (NCMs) with complex architecture and loss functions, or filtering out untrustworthy samples based on a dialogue attribute, e.g., Relatedness or Genericness. In this paper, we follow the third research branch and present a data filtering method for open-domain dialogues, which identifies untrustworthy samples from training data with a quality measure that linearly combines seven dialogue attributes. The attribute weights are obtained via Bayesian Optimization (BayesOpt) that aims to optimize an objective function for dialogue generation iteratively on the validation set. Then we score training samples with the quality measure, sort them in descending order, and filter out those at the bottom. Furthermore, to accelerate the \"filter-train-evaluate'' iterations involved in BayesOpt on large-scale datasets, we propose a training framework that integrates maximum likelihood estimation (MLE) and negative training method (NEG). The training method updates parameters of a trained NCMs on two small sets with newly maintained and removed samples, respectively. Specifically, MLE is applied to maximize the log-likelihood of newly maintained samples, while NEG is used to minimize the log-likelihood of newly removed ones. Experimental results on two datasets show that our method can effectively identify untrustworthy samples, and NCMs trained on the filtered datasets achieve better performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Lei Shen"
                },
                {
                    "authorId": "146950185",
                    "name": "Haolan Zhan"
                },
                {
                    "authorId": null,
                    "name": "Xin Shen"
                },
                {
                    "authorId": "1830448175",
                    "name": "Hongshen Chen"
                },
                {
                    "authorId": "9272754",
                    "name": "Xiaofang Zhao"
                },
                {
                    "authorId": "1854999",
                    "name": "Xiao-Dan Zhu"
                }
            ]
        },
        {
            "paperId": "29e815c583f62735821e5fab1c743347e8cb3bcc",
            "title": "Probing Product Description Generation via Posterior Distillation",
            "abstract": "In product description generation (PDG), the user-cared aspect is critical for the recommendation system, which can not only improve user's experiences but also obtain more clicks. High-quality customer reviews can be considered as an ideal source to mine user-cared aspects. However, in reality, a large number of new products (known as long-tailed commodities) cannot gather sufficient amount of customer reviews, which brings a big challenge in the product description generation task. Existing works tend to generate the product description solely based on item information, i.e., product attributes or title words, which leads to tedious contents and cannot attract customers effectively. To tackle this problem, we propose an adaptive posterior network based on Transformer architecture that can utilize user-cared information from customer reviews. Specifically, we first extend the self-attentive Transformer encoder to encode product titles and attributes. Then, we apply an adaptive posterior distillation module to utilize useful review information, which integrates user-cared aspects to the generation process. Finally, we apply a Transformer-based decoding phase with copy mechanism to automatically generate the product description. Besides, we also collect a large-scare Chinese product description dataset to support our work and further research in this field. Experimental results show that our model is superior to traditional generative models in both automatic indicators and human evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "146950185",
                    "name": "Haolan Zhan"
                },
                {
                    "authorId": "2108769177",
                    "name": "Hainan Zhang"
                },
                {
                    "authorId": "1830448175",
                    "name": "Hongshen Chen"
                },
                {
                    "authorId": "2115887820",
                    "name": "Lei Shen"
                },
                {
                    "authorId": "1775738",
                    "name": "Zhuoye Ding"
                },
                {
                    "authorId": "1830443292",
                    "name": "Yongjun Bao"
                },
                {
                    "authorId": "46704879",
                    "name": "Weipeng P. Yan"
                },
                {
                    "authorId": "37510256",
                    "name": "Yanyan Lan"
                }
            ]
        },
        {
            "paperId": "2a4f240bbc33900208a0c37467f17a6933d2eb1e",
            "title": "CoLV: A Collaborative Latent Variable Model for Knowledge-Grounded Dialogue Generation",
            "abstract": "Knowledge-grounded dialogue generation has achieved promising performance with the engagement of external knowledge sources. Typical approaches towards this task usually perform relatively independent two sub-tasks, i.e., knowledge selection and knowledge-aware response generation. In this paper, in order to improve the diversity of both knowledge selection and knowledge-aware response generation, we propose a collaborative latent variable (CoLV) model to integrate these two aspects simultaneously in separate yet collaborative latent spaces, so as to capture the inherent correlation between knowledge selection and response generation. During generation, our proposed model firstly draws knowledge candidate from the latent space conditioned on the dialogue context, and then samples a response from another collaborative latent space conditioned on both the context and the selected knowledge. Experimental results on two widely-used knowledge-grounded dialogue datasets show that our model outperforms previous methods on both knowledge selection and response generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "146950185",
                    "name": "Haolan Zhan"
                },
                {
                    "authorId": null,
                    "name": "Lei Shen"
                },
                {
                    "authorId": "1830448175",
                    "name": "Hongshen Chen"
                },
                {
                    "authorId": "2108769177",
                    "name": "Hainan Zhang"
                }
            ]
        }
    ]
}