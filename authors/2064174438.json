{
    "authorId": "2064174438",
    "papers": [
        {
            "paperId": "e705cac894c62be9f866528b3250321de5f51b43",
            "title": "Driven to Distraction: Examining the Influence of Distractors on Search Behaviours, Performance and Experience",
            "abstract": "Advertisements, sponsored links, clickbait, in-house recommendations and similar elements pervasively shroud featured content. Such elements vie for people\u2019s attention, potentially distracting people from their task at hand. The effects of such \u201cdistractors\u201d is likely to increase people\u2019s cognitive workload and reduce their performance as they need to work harder to discern the relevant from non-relevant. In this paper, we investigate how people of varying cognitive abilities (measured using Perceptual Speed and Cognitive Failure instruments) are affected by these different types of distractions when completing search tasks. We performed a crowdsourced within-subjects user study, where 102 participants completed four search tasks using our news search engine over four different interface conditions: (i) one with no additional distractors; (ii) one with advertisements; (iii) one with sponsored links; and (iv) one with in-house recommendations. Our results highlight a number of important trends and findings. Participants perceived the interface condition without distractors as significantly better across numerous dimensions. Participants reported higher satisfaction, lower workload, higher topic recall, and found it easier to concentrate. Behaviourally, participants issued queries faster and clicked results earlier when compared to the interfaces with distractors. When using the interfaces with distractors, one in ten participants clicked on a distractor\u2014and despite engaging with a distractor for less than twenty seconds, their task time increased by approximately two minutes. We found that the effects were magnified depending on cognitive abilities\u2014with a greater impact of distractors on participants with lower perceptual speed, and for those with a higher propensity of cognitive failures. Distractors\u2014regardless of their type\u2014have negative consequences on a user\u2019s search experience and performance. As a consequence, interfaces containing visually distracting elements are creating poorer search experiences due to the \u201cdistractor tax\u201d being placed on people\u2019s limited attention.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1716332",
                    "name": "L. Azzopardi"
                },
                {
                    "authorId": "2064174438",
                    "name": "David Maxwell"
                },
                {
                    "authorId": "1767534",
                    "name": "Martin Halvey"
                },
                {
                    "authorId": "2731925",
                    "name": "C. Hauff"
                }
            ]
        },
        {
            "paperId": "ef33b3c1768dd8e6ad931f4a81e11aef0b545a5b",
            "title": "Hear Me Out: A Study on the Use of the Voice Modality for Crowdsourced Relevance Assessments",
            "abstract": "The creation of relevance assessments by human assessors (often nowadays crowdworkers) is a vital step when building IR test collections. Prior works have investigated assessor quality & behaviour, and tooling to support assessors in their task. We have few insights though into the impact of a document's presentation modality on assessor efficiency and effectiveness. Given the rise of voice-based interfaces, we investigate whether it is feasible for assessors to judge the relevance of text documents via a voice-based interface. We ran a user study (n = 49) on a crowdsourcing platform where participants judged the relevance of short and long documents- sampled from the TREC Deep Learning corpus-presented to them either in the text or voice modality. We found that: (i) participants are equally accurate in their judgements across both the text and voice modality; (ii) with increased document length it takes partic- ipants significantly longer (for documents of length > 120 words it takes almost twice as much time) to make relevance judgements in the voice condition; and (iii) the ability of assessors to ignore stimuli that are not relevant (i.e., inhibition) impacts the assessment quality in the voice modality-assessors with higher inhibition are significantly more accurate than those with lower inhibition. Our results indicate that we can reliably leverage the voice modality as a means to effectively collect relevance labels from crowdworkers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49217627",
                    "name": "Nirmal Roy"
                },
                {
                    "authorId": "9572457",
                    "name": "Agathe Balayn"
                },
                {
                    "authorId": "2064174438",
                    "name": "David Maxwell"
                },
                {
                    "authorId": "2731925",
                    "name": "C. Hauff"
                }
            ]
        },
        {
            "paperId": "038d7b3348496bd56c61a873a2df4d0edd5e0bbd",
            "title": "First Early Career Researchers\u2019 Roundtable for Information Access Research: CHIIR 2022 Full Day Workshop",
            "abstract": "The COVID-19 pandemic has changed the way we work, study, and conduct research. Ongoing stresses and uncertainties of the pandemic have impacted research activities and collaborations, especially for graduate researchers1 and Early Career Researchers (ECRs)2. It has also changed the way we connect with the broader research communities. For example, in the last year, conferences were either postponed or held online. Even though many conferences implemented social activities, connecting online with peers is hard. Thus, serendipity and forming new bonds or research connections at conferences have been more complex. Indeed, graduate researchers and ECRs have increased challenges connecting and establishing new research connections in online driven environments. This workshops aims to empower graduate and ECRs, make new research connections, and foster a sense of belonging. The First ECRs Roundtable on Information Access Research workshop at ACM CHIIR\u201922 looks into the future of research, collaborations, and self-development to ask the following. The workshop is hands-on and interactive\u2014with two key talks to kick-start discussion. Rather than a series of technical talks, we solicit position statements from attendees on opportunities, problems, and solutions on (post-)pandemic research on information access within the wider CHIIR community. Building on work presented at ACM CHIIR 2021 [10], this workshop empowers attendees to share their do\u2019s and don\u2019ts, review their practices for success, and refine which strategies work for them. The workshop provides a neutral platform for an open and honest discussion about the lessons learned from working in a pandemic. Outcomes include a technical report written by the attendees.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2528063",
                    "name": "Johanne R. Trippas"
                },
                {
                    "authorId": "2064174438",
                    "name": "David Maxwell"
                }
            ]
        },
        {
            "paperId": "990d8ce3592a8cab879b80b14010085315a4e96b",
            "title": "Users and Contemporary SERPs: A (Re-)Investigation",
            "abstract": "TheSearch Engine Results Page (SERP) has evolved significantly over the last two decades, moving away from the simple ten blue links paradigm to considerably more complex presentations that contain results from multiple verticals and granularities of textual information. Prior works have investigated how user interactions on the SERP are influenced by the presence or absence of heterogeneous content (e.g., images, videos, or news content), the layout of the SERP (\\emphlist vs. grid layout), and task complexity. In this paper, we reproduce the user studies conducted in prior works---specifically those of~\\citetarguello2012task and~\\citetsiu2014first ---to explore to what extent the findings from research conducted five to ten years ago still hold today as the average web user has become accustomed to SERPs with ever-increasing presentational complexity. To this end, we designed and ran a user study with four different SERP interfaces:(i) ~\\empha heterogeneous grid ;(ii) ~\\empha heterogeneous list ;(iii) ~\\empha simple grid ; and(iv) ~\\empha simple list. We collected the interactions of $41$ study participants over $12$ search tasks for our analyses. We observed that SERP types and task complexity affect user interactions with search results. We also find evidence to support most (6 out of 8) observations from~\\citearguello2012task,siu2014first indicating that user interactions with different interfaces and to solve tasks of different complexity have remained mostly similar over time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49217627",
                    "name": "Nirmal Roy"
                },
                {
                    "authorId": "2064174438",
                    "name": "David Maxwell"
                },
                {
                    "authorId": "2731925",
                    "name": "C. Hauff"
                }
            ]
        },
        {
            "paperId": "5685c6546a35716705c0e5dae85a088a88bd0b71",
            "title": "The PhD Journey: Reaching Out and Lending a Hand",
            "abstract": "Undertaking a PhD is a challenging yet fulfilling experience. PhD candidates become deeply involved in developing a myriad of skills over many vital facets, including (but not limited to): (i) the development of their research ideas; (ii) learning how to conduct their research; (iii) engaging with others about their research - both locally and internationally; (iv) developing a profile as an independent researcher; and (v) developing their teaching portfolio. Of course, a candidate is likely to encounter many highs and lows during their candidature. Periods of turbulence can be overcome through the application of various techniques to adapt and learn from these experiences. This tutorial will partly aim to introduce attendees to several techniques to help them advance in the PhD process. It will be presented by two recent PhD graduates in the field of Interactive Information Retrieval (IIR), who are both close enough to their respective times as PhD students to remember the highs and lows of PhD life, yet be far enough removed from the process that they can adequately reflect and provide insights into their own experiences - both good and bad. This tutorial will empower attendees to share their own do's and don'ts, review their practices for success, and refine what productivity strategies work for them. It will provide an impartial platform for an open and honest discussion about the journey of undertaking a PhD, led by the presenters without judgement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2528063",
                    "name": "Johanne R. Trippas"
                },
                {
                    "authorId": "2064174438",
                    "name": "David Maxwell"
                }
            ]
        },
        {
            "paperId": "6acd379323b0a9dc379ae99d883da1a8c84c61d1",
            "title": "Report on the 1st simulation for information retrieval workshop (Sim4IR 2021) at SIGIR 2021",
            "abstract": "Simulation is used as a low-cost and repeatable means of experimentation. As Information Retrieval (IR) researchers, we are no strangers to the idea of using simulation within our own field---such as the traditional means of IR system evaluation as manifested through the Cranfield paradigm. While simulation has been used in other areas of IR research (such as the study of user behaviours), we argue that the potential for using simulation has been recognised by relatively few IR researchers so far. To this end, the Sim4IR workshop was held online on July 15th, 2021 in conjunction with ACM SIGIR 2021. Building on past efforts, the goal of the workshop was to create a forum for researchers and practitioners to promote methodology and development of more widespread use of simulation for IR evaluation. Around 80 participants took part over two sessions. A total of two keynotes, three original paper presentations, and eight 'encore talks' were presented. The main conclusions from the resultant discussion were that simulation has the potential to offer solutions to the limitations of existing evaluation methodologies, but there is more research needed toward developing realistic user simulators; and the development and sharing of simulators, in the form of toolkits and online services, is critical for successful uptake. Date: 15 July, 2021. Website: https://sim4ir.org.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1680484",
                    "name": "K. Balog"
                },
                {
                    "authorId": "2064174438",
                    "name": "David Maxwell"
                },
                {
                    "authorId": "2147292438",
                    "name": "P. Thomas"
                },
                {
                    "authorId": "1390852432",
                    "name": "Shuo Zhang"
                }
            ]
        },
        {
            "paperId": "8d7a9884b49135935de08a454828c6fe1b92c198",
            "title": "The Impact of Entity Cards on Learning-Oriented Search Tasks",
            "abstract": "Entity cards are a common occurrence in today's web Search Engine Results Pages (SERPs). SERPs provide information on a complex information object in a structured manner. Typically, they combine data from several search verticals. They have been shown to: (i) increase users' engagement with the SERP; and (ii) improve decision making for certain types of searches (such as health searches). In this paper, we investigate whether the benefits of showing entity cards also extend to the Search as Learning (SAL) domain. Do learners learn more when entity cards are present on the SERP? To answer this question, we designed a series of learning-oriented search tasks (with a minimum search time of 15 minutes), and conducted a crowdsourced Interactive Information Retrieval (IIR) user study (N=144) with four interface conditions: (i) a control with no entity cards; (ii) displaying relevant entity cards; (iii) displaying somewhat relevant entity cards; and (iv) displaying non-relevant entity cards. Our results show that (i) entity cards do not have an effect on participants' learning, but (ii) they do significantly impact participants' search behaviours across a range of dimensions (such as the dwell time and search session duration).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1584456706",
                    "name": "Sara Salimzadeh"
                },
                {
                    "authorId": "2064174438",
                    "name": "David Maxwell"
                },
                {
                    "authorId": "2731925",
                    "name": "C. Hauff"
                }
            ]
        },
        {
            "paperId": "8e02e2707fdd82e578aa840cebf9d0a333c42d5d",
            "title": "Developing ContemporaryWeb-Based Interaction Logging Infrastructure: The Design and Challenges of LogUI",
            "abstract": "Studies involving user interfaces typically involve the capturing and recording (logging) of key user interactions between the user and the system being examined. However, anecdotal evidence suggests that researchers often implement their own logging infrastructure\u2014sometimes in a piecemeal fashion\u2014which can lead to numerous implementation mistakes (due to misunderstanding or ignoring differences between web browsers , for example). While efforts have been made to develop interaction logging solutions for experimentation and commercial use, many solutions either use obsolete technology, are prohibitively expensive, are complex to use (and require extensive programming knowledge), or have no source code available. To address these issues, we have developed LogUI , an easy-to-use yet powerful interaction logging framework that can capture virtually any user interaction within a web-based environment. LogUI has been successfully used in several user studies since its launch. This paper provides an in-depth discussion into how we have designed LogUI , and provides narrative on the key challenges that we are looking to address moving forward.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064174438",
                    "name": "David Maxwell"
                },
                {
                    "authorId": "2731925",
                    "name": "C. Hauff"
                }
            ]
        },
        {
            "paperId": "94638f6c7d4140dbebb50ce31c49f9c13080ab83",
            "title": "Sim4IR: The SIGIR 2021 Workshop on Simulation for Information Retrieval Evaluation",
            "abstract": "The use of simulation techniques is not foreign to information retrieval. In the past, simulation has been employed, for example, for constructing test collections and for model performance prediction and analysis in a broad array of information access scenarios. Nevertheless, a standardized methodology for performance evaluation via simulation has not yet been developed. The goal of this workshop is to create a forum for researchers and practitioners to promote methodology development and more widespread use of simulation for evaluation by: (1) identifying problem settings and application scenarios; (2) sharing tools, techniques, and experiences; (3) characterizing potentials and limitations; and (4) developing a research agenda.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1680484",
                    "name": "K. Balog"
                },
                {
                    "authorId": "2064174438",
                    "name": "David Maxwell"
                },
                {
                    "authorId": "41202995",
                    "name": "Paul Thomas"
                },
                {
                    "authorId": "2144324539",
                    "name": "Shuo Zhang"
                }
            ]
        },
        {
            "paperId": "97b11f811cf6e3b59157187d20d5d9913dd806b1",
            "title": "Incorporating Widget Positioning in Interaction Models of Search Behaviour",
            "abstract": "Models developed to simulate user interactions with search interfaces typically do not consider the visual layout and presentation of a Search Engine Results Page (SERP). In particular, the position and size of interfacewidgets ---such as entity cards and query suggestions---are usually considered a negligible constant. In contrast, in this work, we investigate the impact of widget positioning on user behaviour. To this end, we focus on one specific widget: the Query History Widget (QHW). It allows users to see (and thus reflect) on their recently issued queries. We build a novel simulation model based on Search Economic Theory (SET) that considers how users behave when faced with such a widget by incorporating its positioning on the SERP. We derive five hypotheses from our model and experimentally validate them based on user interaction data gathered for an ad-hoc search task, run across five different placements of the \\qhw on the SERP. We find partial support for three of the five hypotheses, and indeed observe that a widget's location has a significant impact on search behaviour.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49217627",
                    "name": "Nirmal Roy"
                },
                {
                    "authorId": "133572799",
                    "name": "A. C\u00e2mara"
                },
                {
                    "authorId": "2064174438",
                    "name": "David Maxwell"
                },
                {
                    "authorId": "2731925",
                    "name": "C. Hauff"
                }
            ]
        }
    ]
}