{
    "authorId": "47139979",
    "papers": [
        {
            "paperId": "1f1d14555d28c7ca469ba33ca63116467b9f3357",
            "title": "Generative Model for Decision Trees",
            "abstract": "Decision trees are among the most popular supervised models due to their interpretability and knowledge representation resembling human reasoning. Commonly-used decision tree induction algorithms are based on greedy top-down strategies. Although these approaches are known to be an efficient heuristic, the resulting trees are only locally optimal and tend to have overly complex structures. On the other hand, optimal decision tree algorithms attempt to create an entire decision tree at once to achieve global optimality. We place our proposal between these approaches by designing a generative model for decision trees. Our method first learns a latent decision tree space through a variational architecture using pre-trained decision tree models. Then, it adopts a genetic procedure to explore such latent space to find a compact decision tree with good predictive performance. We compare our proposal against classical tree induction methods, optimal approaches, and ensemble models. The results show that our proposal can generate accurate and shallow, i.e., interpretable, decision trees.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261748854",
                    "name": "Riccardo Guidotti"
                },
                {
                    "authorId": "2296962",
                    "name": "A. Monreale"
                },
                {
                    "authorId": "47139979",
                    "name": "Mattia Setzu"
                },
                {
                    "authorId": "2293419211",
                    "name": "Giulia Volpi"
                }
            ]
        },
        {
            "paperId": "343565145bd1d5cad907dec579491b719f1004a3",
            "title": "AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems",
            "abstract": "Everyday we increasingly rely on machine learning models to automate and support high-stake tasks and decisions. This growing presence means that humans are now constantly interacting with machine learning-based systems, training and using models everyday. Several different techniques in computer science literature account for the human interaction with machine learning systems, but their classification is sparse and the goals varied. This survey proposes a taxonomy of Hybrid Decision Making Systems, providing both a conceptual and technical framework for understanding how current computer science literature models interaction between humans and machines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261749096",
                    "name": "Clara Punzi"
                },
                {
                    "authorId": "24269254",
                    "name": "Roberto Pellungrini"
                },
                {
                    "authorId": "47139979",
                    "name": "Mattia Setzu"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                }
            ]
        },
        {
            "paperId": "8da6e7da2e2338edcb1c815d70497a3c28dbfa7d",
            "title": "FairBelief - Assessing Harmful Beliefs in Language Models",
            "abstract": "Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing.This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions. With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs\u2019 outputs\u2019 hurtfulness.Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models.We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing tasks, they show hurtful beliefs about specific genders. Interestingly, training procedure and dataset, model scale, and architecture induce beliefs of different degrees of hurtfulness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47139979",
                    "name": "Mattia Setzu"
                },
                {
                    "authorId": "2121386115",
                    "name": "Marta Marchiori Manerba"
                },
                {
                    "authorId": "2261279220",
                    "name": "Pasquale Minervini"
                },
                {
                    "authorId": "2287829560",
                    "name": "Debora Nozza"
                }
            ]
        },
        {
            "paperId": "4b3c8f3cc8760b8f95546431b4fe635b8a0f0e18",
            "title": "HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis",
            "abstract": "Authorship Analysis, also known as stylometry, has been an essential aspect of Natural Language Processing (NLP) for a long time. Likewise, the recent advancement of Large Language Models (LLMs) has made authorship analysis increasingly crucial for distinguishing between human-written and AI-generated texts. However, these authorship analysis tasks have primarily been focused on written texts, not considering spoken texts. Thus, we introduce the largest benchmark for spoken texts - HANSEN (Human ANd ai Spoken tExt beNchmark). HANSEN encompasses meticulous curation of existing speech datasets accompanied by transcripts, alongside the creation of novel AI-generated spoken text datasets. Together, it comprises 17 human datasets, and AI-generated spoken texts created using 3 prominent LLMs: ChatGPT, PaLM2, and Vicuna13B. To evaluate and demonstrate the utility of HANSEN, we perform Authorship Attribution (AA)&Author Verification (AV) on human-spoken datasets and conducted Human vs. AI spoken text detection using state-of-the-art (SOTA) models. While SOTA methods, such as, character ngram or Transformer-based model, exhibit similar AA&AV performance in human-spoken datasets compared to written ones, there is much room for improvement in AI-generated spoken text detection. The HANSEN benchmark is available at: https://huggingface.co/datasets/HANSEN-REPO/HANSEN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66674465",
                    "name": "Nafis Irtiza Tripto"
                },
                {
                    "authorId": "150035131",
                    "name": "Adaku Uchendu"
                },
                {
                    "authorId": "2260345102",
                    "name": "Thai Le"
                },
                {
                    "authorId": "47139979",
                    "name": "Mattia Setzu"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "2158951945",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "9fea134cdcb4e325152bcce18f1aa34ebee79f88",
            "title": "Correlation and Unintended Biases on Univariate and Multivariate Decision Trees",
            "abstract": "Decision Trees are accessible, interpretable, and well-performing classification models. A plethora of variants with increasing expressiveness has been proposed in the last forty years. We contrast the two families of univariate DTs, whose split functions partition data through axis-parallel hyperplanes, and multivariate DTs, whose splits instead partition data through oblique hyperplanes. The latter include the former, hence multivariate DTs are in principle more powerful. Surprisingly enough, however, univariate DTs consistently show comparable performances in the literature. We analyze the reasons behind this, both with synthetic and real-world benchmark datasets. Our research questions test whether the pre-processing phase of removing correlation among features in datasets has an impact on the relative performances of univariate vs multivariate DTs. We find that existing benchmark datasets are likely biased towards favoring univariate DTs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47139979",
                    "name": "Mattia Setzu"
                },
                {
                    "authorId": "2243467268",
                    "name": "Salvatore Ruggieri"
                }
            ]
        },
        {
            "paperId": "b5596d12d8acd2af34ea2db0795c469aa70e621d",
            "title": "Trustworthy AI at KDD Lab",
            "abstract": "This document summarizes the activities regarding the development of Responsible AI (Responsible Artificial Intelligence) conducted by the Knowledge Discovery and Data mining group (KDD-Lab), a joint research group of the Institute of Information Science and Technologies \u201cAlessandro Faedo\u201d (ISTI) of the National Research Council of Italy (CNR), the Department of Computer Science of the University of Pisa, and the Scuola Normale Superiore of Pisa.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1704327",
                    "name": "Riccardo Guidotti"
                },
                {
                    "authorId": "2296962",
                    "name": "A. Monreale"
                },
                {
                    "authorId": "3145906",
                    "name": "Luca Pappalardo"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                },
                {
                    "authorId": "24269254",
                    "name": "Roberto Pellungrini"
                },
                {
                    "authorId": "33769943",
                    "name": "Francesca Pratesi"
                },
                {
                    "authorId": "2120595",
                    "name": "S. Rinzivillo"
                },
                {
                    "authorId": "2243467268",
                    "name": "Salvatore Ruggieri"
                },
                {
                    "authorId": "47139979",
                    "name": "Mattia Setzu"
                },
                {
                    "authorId": "2243462850",
                    "name": "Rosaria Deluca"
                }
            ]
        },
        {
            "paperId": "b6a90162584b50b8ef92950695901fd09cbea46e",
            "title": "Explainable Authorship Identification in Cultural Heritage Applications",
            "abstract": "While a substantial amount of work has recently been devoted to improving the accuracy of computational Authorship Identification (AId) systems for textual data, little to no attention has been paid to endowing AId systems with the ability to explain the reasons behind their predictions. This substantially hinders the practical application of AId methods, since the predictions returned by such systems are hardly useful unless they are supported by suitable explanations. In this paper, we explore the applicability of existing general-purpose eXplainable Artificial Intelligence (XAI) techniques to AId, with a focus on explanations addressed to scholars working in cultural heritage. In particular, we assess the relative merits of three different types of XAI techniques (feature ranking, probing, factual and counterfactual selection) on three different AId tasks (authorship attribution, authorship verification, same-authorship verification) by running experiments on real AId textual data. Our analysis shows that, while these techniques make important first steps towards explainable Authorship Identification, more work remains to be done in order to provide tools that can be profitably integrated in the workflows of scholars.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47139979",
                    "name": "Mattia Setzu"
                },
                {
                    "authorId": "1379629622",
                    "name": "Silvia Corbara"
                },
                {
                    "authorId": "2296962",
                    "name": "A. Monreale"
                },
                {
                    "authorId": "38218211",
                    "name": "Alejandro Moreo"
                },
                {
                    "authorId": "2256999539",
                    "name": "Fabrizio Sebastiani"
                }
            ]
        },
        {
            "paperId": "2f0440fd9ace0ef669f9510be4e669e106bfb023",
            "title": "TRIPLEx: Triple Extraction for Explanation",
            "abstract": "Transformer-based models are used to solve a va-riety of Natural Language Processing tasks. Still, these models are opaque and poorly understandable for their users. Current approaches to explainability focus on token importance, in which the explanation consists of a set of tokens relevant to the prediction, and natural language explanations, in which the explanation is a generated piece of text. The latter are usually learned by design with models traind end-to-end to provide a prediction and an explanation, or rely on powerful external text generators to do the heavy lifting for them. In this paper we present TRIPLEX, an explainability algorithm for Transformer-based models fine-tuned on Natural Language Inference, Semantic Text Similarity, or Text Classification tasks. TRIPLEX explains Transformers-based models by extracting a set of facts from the input data, subsuming it by abstraction, and generating a set of weighted triples as explanation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47139979",
                    "name": "Mattia Setzu"
                },
                {
                    "authorId": "2296962",
                    "name": "A. Monreale"
                },
                {
                    "authorId": "3051815",
                    "name": "Pasquale Minervini"
                }
            ]
        },
        {
            "paperId": "5d01af387b05bc49455d2b421d16b2aaeec94704",
            "title": "SPARQL Queries over Source Code",
            "abstract": "We introduce a framework to extract and parse Java source code, serialize it into RDF triples by applying an appropriate ontology and then analyze the resulting structured code information by using standard SPARQL queries. We present our experiments on a sample of 134 Java repositories collected from Github, obtaining 17 Million triples about methods, input and output types, comments, and other source code information. Experiments also address the scalability of the framework. We finally provide examples of the level of expressivity that can be achieved with SPARQL by using our proposed ontology and semantic technologies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47139979",
                    "name": "Mattia Setzu"
                },
                {
                    "authorId": "2413538",
                    "name": "Maurizio Atzori"
                }
            ]
        }
    ]
}