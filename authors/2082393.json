{
    "authorId": "2082393",
    "papers": [
        {
            "paperId": "9dea7ec34ff7dfc6e62286643da012fd8f07eca2",
            "title": "Algorithm-Assisted Decision Making and Racial Disparities in Housing: A Study of the Allegheny Housing Assessment Tool",
            "abstract": "The demand for housing assistance across the United States far exceeds the supply, leaving housing providers the task of prioritizing clients for receipt of this limited resource. To be eligible for federal funding, local homelessness systems are required to implement assessment tools as part of their prioritization processes. The Vulnerability Index Service Prioritization Decision Assistance Tool (VI-SPDAT) is the most commonly used assessment tool nationwide. Recent studies have criticized the VI-SPDAT as exhibiting racial bias, which may lead to unwarranted racial disparities in housing provision. In response to these criticisms, some jurisdictions have developed alternative tools, such as the Allegheny Housing Assessment (AHA), which uses algorithms to assess clients' risk levels. Drawing on data from its deployment, we conduct descriptive and quantitative analyses to evaluate whether replacing the VI-SPDAT with the AHA affects racial disparities in housing allocation. We find that the VI-SPDAT tended to assign higher risk scores to white clients and lower risk scores to Black clients, and that white clients were served at a higher rates pre-AHA deployment. While post-deployment service decisions became better aligned with the AHA score, and the distribution of AHA scores is similar across racial groups, we do not find evidence of a corresponding decrease in disparities in service rates. We attribute the persistent disparity to the use of Alt-AHA, a survey-based tool that is used in cases of low data quality, as well as group differences in eligibility-related factors, such as chronic homelessness and veteran status. We discuss the implications for housing service systems seeking to reduce racial disparities in their service delivery.",
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "authors": [
                {
                    "authorId": "2314072248",
                    "name": "Lingwei Cheng"
                },
                {
                    "authorId": "2313913563",
                    "name": "Cameron Drayton"
                },
                {
                    "authorId": "2082393",
                    "name": "A. Chouldechova"
                },
                {
                    "authorId": "4549041",
                    "name": "R. Vaithianathan"
                }
            ]
        },
        {
            "paperId": "238b2cfebdd144f04645420f2364024b1a48e152",
            "title": "Multi-Target Multiplicity: Flexibility and Fairness in Target Specification under Resource Constraints",
            "abstract": "Prediction models have been widely adopted as the basis for decision-making in domains as diverse as employment, education, lending, and health. Yet, few real world problems readily present themselves as precisely formulated prediction tasks. In particular, there are often many reasonable target variable options. Prior work has argued that this is an important and sometimes underappreciated choice, and has also shown that target choice can have a significant impact on the fairness of the resulting model. However, the existing literature does not offer a formal framework for characterizing the extent to which target choice matters in a particular task. Our work fills this gap by drawing connections between the problem of target choice and recent work on predictive multiplicity. Specifically, we introduce a conceptual and computational framework for assessing how the choice of target affects individuals\u2019 outcomes and selection rate disparities across groups. We call this multi-target multiplicity. Along the way, we refine the study of single-target multiplicity by introducing notions of multiplicity that respect resource constraints\u2014a feature of many real-world tasks that isn\u2019t captured by existing notions of predictive multiplicity. We apply our methods on a healthcare dataset, and show that the level of multiplicity that stems from target variable choice can be greater than that stemming from nearly-optimal models of a single target.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388406103",
                    "name": "J. Watson-Daniels"
                },
                {
                    "authorId": "2881033",
                    "name": "Solon Barocas"
                },
                {
                    "authorId": "40368603",
                    "name": "Jake M. Hofman"
                },
                {
                    "authorId": "2082393",
                    "name": "A. Chouldechova"
                }
            ]
        },
        {
            "paperId": "346e4f35a5a81ef893792133ec1fec18f23c1768",
            "title": "Examining risks of racial biases in NLP tools for child protective services",
            "abstract": "Although much literature has established the presence of demographic bias in natural language processing (NLP) models, most work relies on curated bias metrics that may not be reflective of real-world applications. At the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in NLP. In this work, we focus on one such setting: child protective services (CPS). CPS workers often write copious free-form text notes about families they are working with, and CPS agencies are actively seeking to deploy NLP models to leverage these data. Given well-established racial bias in this setting, we investigate possible ways deployed NLP is liable to increase racial disparities. We specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (NER). We document consistent algorithmic unfairness in NER models, possible algorithmic unfairness in coreference resolution models, and little evidence of exacerbated racial bias in risk prediction. While there is existing pronounced criticism of risk prediction, our results expose previously undocumented risks of racial bias in realistic information extraction systems, highlighting potential concerns in deploying them, even though they may appear more benign. Our work serves as a rare realistic examination of NLP algorithmic fairness in a potential deployed setting and a timely investigation of a specific risk associated with deploying NLP in CPS settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49713890",
                    "name": "Anjalie Field"
                },
                {
                    "authorId": "48577290",
                    "name": "Amanda Coston"
                },
                {
                    "authorId": "47404598",
                    "name": "Nupoor Gandhi"
                },
                {
                    "authorId": "2082393",
                    "name": "A. Chouldechova"
                },
                {
                    "authorId": "1398859796",
                    "name": "Emily Putnam-Hornstein"
                },
                {
                    "authorId": "2082303978",
                    "name": "David Steier"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "c5e3c812047219faf80a3fe01a7c70fd41b3fac3",
            "title": "Overcoming Algorithm Aversion: A Comparison between Process and Outcome Control",
            "abstract": "Algorithm aversion occurs when humans are reluctant to use algorithms despite their superior performance. Studies show that giving users outcome control by providing agency over how models\u2019 predictions are incorporated into decision-making mitigates algorithm aversion. We study whether algorithm aversion is mitigated by process control, wherein users can decide what input factors and algorithms to use in model training. We conduct a replication study of outcome control, and test novel process control study conditions on Amazon Mechanical Turk (MTurk) and Prolific. Our results partly confirm prior findings on the mitigating effects of outcome control, while also forefronting reproducibility challenges. We find that process control in the form of choosing the training algorithm mitigates algorithm aversion, but changing inputs does not. Furthermore, giving users both outcome and process control does not reduce algorithm aversion more than outcome or process control alone. This study contributes to design considerations around mitigating algorithm aversion.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123962244",
                    "name": "Ling-chi Cheng"
                },
                {
                    "authorId": "2082393",
                    "name": "A. Chouldechova"
                }
            ]
        },
        {
            "paperId": "5d079f68a6b5635efd8d8357a22a18b9143d3e72",
            "title": "Heterogeneity in Algorithm-Assisted Decision-Making: A Case Study in Child Abuse Hotline Screening",
            "abstract": "Algorithmic risk assessment tools are now commonplace in public sector domains such as criminal justice and human services. These tools are intended to aid decision makers in systematically using rich and complex data captured in administrative systems. In this study we investigate sources of heterogeneity in the alignment between worker decisions and algorithmic risk scores in the context of a real world child abuse hotline screening use case. Specifically, we focus on heterogeneity related to worker experience. We find that senior workers are far more likely to screen in referrals for investigation, even after we control for the observed algorithmic risk score and other case characteristics. We also observe that the decisions of less-experienced workers are more closely aligned with algorithmic risk scores than those of senior workers who had decision-making experience prior to the tool being introduced. While screening decisions vary across child race, we do not find evidence of racial differences in the relationship between worker experience and screening decisions. Our findings indicate that it is important for agencies and system designers to consider ways of preserving institutional knowledge when introducing algorithms into high employee turnover settings such as child welfare call screening.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123962244",
                    "name": "Ling-chi Cheng"
                },
                {
                    "authorId": "2082393",
                    "name": "A. Chouldechova"
                }
            ]
        },
        {
            "paperId": "980e008db6c56db71089da0d2af4295c28a07ab3",
            "title": "Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models",
            "abstract": "This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity\u2014appropriately accounting for relevant differences across individuals\u2014which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods\u2014as opposed to simpler models\u2014shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39579691",
                    "name": "Emily Black"
                },
                {
                    "authorId": "51247229",
                    "name": "Hadi Elzayn"
                },
                {
                    "authorId": "2082393",
                    "name": "A. Chouldechova"
                },
                {
                    "authorId": "82126230",
                    "name": "Jacob Goldin"
                },
                {
                    "authorId": "2056459943",
                    "name": "Daniel E. Ho"
                }
            ]
        },
        {
            "paperId": "9f1753227e7dbbc3b79dec494982e96f1604b98e",
            "title": "Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness",
            "abstract": "Much of machine learning research focuses on predictive accuracy: given a task, create a machine learning model (or algorithm) that maximizes accuracy. In many settings, however, the final prediction or decision of a system is under the control of a human, who uses an algorithm\u2019s output along with their own personal expertise in order to produce a combined prediction. One ultimate goal of such collaborative systems is complementarity: that is, to produce lower loss (equivalently, greater payoff or utility) than either the human or algorithm alone. However, experimental results have shown that even in carefully-designed systems, complementary performance can be elusive. Our work provides three key contributions. First, we provide a theoretical framework for modeling simple human-algorithm systems and demonstrate that multiple prior analyses can be expressed within it. Next, we use this model to prove conditions where complementarity is impossible, and give constructive examples of where complementarity is achievable. Finally, we discuss the implications of our findings, especially with respect to the fairness of a classifier. In sum, these results deepen our understanding of key factors influencing the combined performance of human-algorithm systems, giving insight into how algorithmic tools can best be designed for collaborative environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "122495956",
                    "name": "Kate Donahue"
                },
                {
                    "authorId": "2082393",
                    "name": "A. Chouldechova"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                }
            ]
        },
        {
            "paperId": "b1c2a257a1b74416a15663e6d975f95a5f050c3a",
            "title": "Imagining new futures beyond predictive systems in child welfare: A qualitative study with impacted stakeholders",
            "abstract": "Child welfare agencies across the United States are turning to data-driven predictive technologies (commonly called predictive analytics) which use government administrative data to assist workers\u2019 decision-making. While some prior work has explored impacted stakeholders\u2019 concerns with current uses of data-driven predictive risk models (PRMs), less work has asked stakeholders whether such tools ought to be used in the first place. In this work, we conducted a set of seven design workshops with 35 stakeholders who have been impacted by the child welfare system or who work in it to understand their beliefs and concerns around PRMs, and to engage them in imagining new uses of data and technologies in the child welfare system. We found that participants worried current PRMs perpetuate or exacerbate existing problems in child welfare. Participants suggested new ways to use data and data-driven tools to better support impacted communities and suggested paths to mitigate possible harms of these tools. Participants also suggested low-tech or no-tech alternatives to PRMs to address problems in child welfare. Our study sheds light on how researchers and designers can work in solidarity with impacted communities, possibly to circumvent or oppose child welfare agencies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "114587077",
                    "name": "Logan Stapleton"
                },
                {
                    "authorId": "2165656539",
                    "name": "Min Hun Lee"
                },
                {
                    "authorId": "2161433875",
                    "name": "Diana Qing"
                },
                {
                    "authorId": "143661720",
                    "name": "Mary-Frances Wright"
                },
                {
                    "authorId": "2082393",
                    "name": "A. Chouldechova"
                },
                {
                    "authorId": "2257955034",
                    "name": "Kenneth Holstein"
                },
                {
                    "authorId": "1768074",
                    "name": "Zhiwei Steven Wu"
                },
                {
                    "authorId": "1742431",
                    "name": "Haiyi Zhu"
                }
            ]
        },
        {
            "paperId": "cf2d9203513ce0b559627b4e9eeb22eb1912c2d4",
            "title": "Doubting AI Predictions: Influence-Driven Second Opinion Recommendation",
            "abstract": "Effective human-AI collaboration requires a system design that provides humans with meaningful ways to make sense of and critically evaluate algorithmic recommendations. In this paper, we propose a way to augment human-AI collaboration by building on a common organizational practice: identifying experts who are likely to provide complementary opinions. When machine learning algorithms are trained to predict human-generated assessments, experts' rich multitude of perspectives is frequently lost in monolithic algorithmic recommendations. The proposed approach aims to leverage productive disagreement by (1) identifying whether some experts are likely to disagree with an algorithmic assessment and, if so, (2) recommend an expert to request a second opinion from.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1406443102",
                    "name": "Maria De-Arteaga"
                },
                {
                    "authorId": "2082393",
                    "name": "A. Chouldechova"
                },
                {
                    "authorId": "2064128275",
                    "name": "A. Dubrawski"
                }
            ]
        },
        {
            "paperId": "3321c947a4a399803592f26879927e58f587fd74",
            "title": "The Impact of Algorithmic Risk Assessments on Human Predictions and its Analysis via Crowdsourcing Studies",
            "abstract": "As algorithmic risk assessment instruments (RAIs) are increasingly adopted to assist decision makers, their predictive performance and potential to promote inequity have come under scrutiny. However, while most studies examine these tools in isolation, researchers have come to recognize that assessing their impact requires understanding the behavior of their human interactants. In this paper, building off of several recent crowdsourcing works focused on criminal justice, we conduct a vignette study in which laypersons are tasked with predicting future re-arrests. Our key findings are as follows: (1) Participants often predict that an offender will be rearrested even when they deem the likelihood of re-arrest to be well below 50%; (2) Participants do not anchor on the RAI's predictions; (3) The time spent on the survey varies widely across participants and most cases are assessed in less than 10 seconds; (4) Judicial decisions, unlike participants' predictions, depend in part on factors that are orthogonal to the likelihood of re-arrest. These results highlight the influence of several crucial but often overlooked design decisions and concerns around generalizability when constructing crowdsourcing studies to analyze the impacts of RAI",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1396268881",
                    "name": "Riccardo Fogliato"
                },
                {
                    "authorId": "2082393",
                    "name": "A. Chouldechova"
                },
                {
                    "authorId": "32219137",
                    "name": "Zachary Chase Lipton"
                }
            ]
        }
    ]
}