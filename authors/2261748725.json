{
    "authorId": "2261748725",
    "papers": [
        {
            "paperId": "6f918de95b8a3619e6eb00dbc16a3da43d6fa571",
            "title": "Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization",
            "abstract": "This study addresses the challenge of noise in training datasets for Direct Preference Optimization (DPO), a method for aligning Large Language Models (LLMs) with human preferences. We categorize noise into pointwise noise, which includes low-quality data points, and pairwise noise, which encompasses erroneous data pair associations that affect preference rankings. Utilizing Distributionally Robust Optimization (DRO), we enhance DPO's resilience to these types of noise. Our theoretical insights reveal that DPO inherently embeds DRO principles, conferring robustness to pointwise noise, with the regularization coefficient $\\beta$ playing a critical role in its noise resistance. Extending this framework, we introduce Distributionally Robustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing against worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr. DPO allows for fine-tuned control over data pair reliability, providing a strategic balance between exploration and exploitation in noisy training environments. Empirical evaluations demonstrate that Dr. DPO substantially improves the quality of generated text and response accuracy in preference datasets, showcasing enhanced performance in both noisy and noise-free settings. The code is available at https://github.com/junkangwu/Dr_DPO.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260622979",
                    "name": "Junkang Wu"
                },
                {
                    "authorId": "2298950582",
                    "name": "Yuexiang Xie"
                },
                {
                    "authorId": "2261748725",
                    "name": "Zhengyi Yang"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2259837759",
                    "name": "Jiawei Chen"
                },
                {
                    "authorId": "2237951623",
                    "name": "Jinyang Gao"
                },
                {
                    "authorId": "2265900833",
                    "name": "Bolin Ding"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "2240825631",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "81448c69a0b900f3721596c635c849987eec1a4b",
            "title": "MolTC: Towards Molecular Relational Modeling In Language Models",
            "abstract": "Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the issue of information underutilization, as it hinders the sharing of interaction mechanism learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which effectively integrate graphical information of two molecules in pair. To train MolTC efficiently, we introduce a Multi-hierarchical CoT concept to refine its training paradigm, and conduct a comprehensive Molecular Interactive Instructions dataset for the development of biochemical LLMs involving MRL. Our experiments, conducted across various datasets involving over 4,000,000 molecular pairs, exhibit the superiority of our method over current GNN and LLM-based baselines. Code is available at https://github.com/MangoKiller/MolTC.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "2159830802",
                    "name": "Junfeng Fang"
                },
                {
                    "authorId": "2280045614",
                    "name": "Shuai Zhang"
                },
                {
                    "authorId": "2283505020",
                    "name": "Chang Wu"
                },
                {
                    "authorId": "2261748725",
                    "name": "Zhengyi Yang"
                },
                {
                    "authorId": "1390625267",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2261249035",
                    "name": "Sihang Li"
                },
                {
                    "authorId": "2283296635",
                    "name": "Kun Wang"
                },
                {
                    "authorId": "2282958808",
                    "name": "Wenjie Du"
                },
                {
                    "authorId": "2283443460",
                    "name": "Xiang Wang"
                }
            ]
        },
        {
            "paperId": "81571f64e0b4b75fedae6a4279290e59e4143976",
            "title": "Item-side Fairness of Large Language Model-based Recommendation System",
            "abstract": "Recommendation systems for Web content distribution intricately connect to the information access and exposure opportunities for vulnerable populations. The emergence of Large Language Models-based Recommendation System (LRS) may introduce additional societal challenges to recommendation systems due to the inherent biases in Large Language Models (LLMs). From the perspective of item-side fairness, there remains a lack of comprehensive investigation into the item-side fairness of LRS given the unique characteristics of LRS compared to conventional recommendation systems. To bridge this gap, this study examines the property of LRS with respect to item-side fairness and reveals the influencing factors of both historical users' interactions and inherent semantic biases of LLMs, shedding light on the need to extend conventional item-side fairness methods for LRS. Towards this goal, we develop a concise and effective framework called IFairLRS to enhance the item-side fairness of an LRS. IFairLRS covers the main stages of building an LRS with specifically adapted strategies to calibrate the recommendations of LRS. We utilize IFairLRS to fine-tune LLaMA, a representative LLM, on MovieLens and Steam datasets, and observe significant item-side fairness improvements. The code can be found in https://github.com/JiangM-C/IFairLRS.git.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225076028",
                    "name": "Meng Jiang"
                },
                {
                    "authorId": "2188063534",
                    "name": "Keqin Bao"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2261748725",
                    "name": "Zhengyi Yang"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2239071206",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "c93d7e26ba36eef361bc1011a3a0a30c5233893c",
            "title": "On Softmax Direct Preference Optimization for Recommendation",
            "abstract": "Recommender systems aim to predict personalized rankings based on user preference data. With the rise of Language Models (LMs), LM-based recommenders have been widely explored due to their extensive world knowledge and powerful reasoning abilities. Most of the LM-based recommenders convert historical interactions into language prompts, pairing with a positive item as the target response and fine-tuning LM with a language modeling loss. However, the current objective fails to fully leverage preference data and is not optimized for personalized ranking tasks, which hinders the performance of LM-based recommenders. Inspired by the current advancement of Direct Preference Optimization (DPO) in human preference alignment and the success of softmax loss in recommendations, we propose Softmax-DPO (S-DPO) to instill ranking information into the LM to help LM-based recommenders distinguish preferred items from negatives, rather than solely focusing on positives. Specifically, we incorporate multiple negatives in user preference data and devise an alternative version of DPO loss tailored for LM-based recommenders, connected to softmax sampling strategies. Theoretically, we bridge S-DPO with the softmax loss over negative sampling and find that it has a side effect of mining hard negatives, which assures its exceptional capabilities in recommendation tasks. Empirically, extensive experiments conducted on three real-world datasets demonstrate the superiority of S-DPO to effectively model user preference and further boost recommendation performance while mitigating the data likelihood decline issue of DPO. Our codes are available at https://github.com/chenyuxin1999/S-DPO.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258784735",
                    "name": "Yuxin Chen"
                },
                {
                    "authorId": "2308069999",
                    "name": "Junfei Tan"
                },
                {
                    "authorId": "2153659066",
                    "name": "An Zhang"
                },
                {
                    "authorId": "2261748725",
                    "name": "Zhengyi Yang"
                },
                {
                    "authorId": "2258731846",
                    "name": "Leheng Sheng"
                },
                {
                    "authorId": "2261086256",
                    "name": "Enzhi Zhang"
                },
                {
                    "authorId": "2257436645",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "2257036129",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "f4a305280da8cc9ad798e1b4ebc7e3e8d1754b82",
            "title": "\u03b2-DPO: Direct Preference Optimization with Dynamic \u03b2",
            "abstract": "Direct Preference Optimization (DPO) has emerged as a compelling approach for training Large Language Models (LLMs) to adhere to human preferences. However, the performance of DPO is sensitive to the fine-tuning of its trade-off parameter $\\beta$, as well as to the quality of the preference data. We analyze the impact of $\\beta$ and data quality on DPO, uncovering that optimal $\\beta$ values vary with the informativeness of pairwise data. Addressing the limitations of static $\\beta$ values, we introduce a novel framework that dynamically calibrates $\\beta$ at the batch level, informed by data quality considerations. Additionally, our method incorporates $\\beta$-guided data filtering to safeguard against the influence of outliers. Through empirical evaluation, we demonstrate that our dynamic $\\beta$ adjustment technique significantly improves DPO's performance across a range of models and datasets, offering a more robust and adaptable training paradigm for aligning LLMs with human feedback. The code is available at \\url{https://github.com/junkangwu/beta-DPO}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260622979",
                    "name": "Junkang Wu"
                },
                {
                    "authorId": "2298950582",
                    "name": "Yuexiang Xie"
                },
                {
                    "authorId": "2261748725",
                    "name": "Zhengyi Yang"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2237951623",
                    "name": "Jinyang Gao"
                },
                {
                    "authorId": "2265900833",
                    "name": "Bolin Ding"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "2240825631",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "1cb1a224535589139087124d28b964db453da76d",
            "title": "LLaRA: Large Language-Recommendation Assistant",
            "abstract": "Sequential recommendation aims to predict users' next interaction with items based on their past engagement sequence. Recently, the advent of Large Language Models (LLMs) has sparked interest in leveraging them for sequential recommendation, viewing it as language modeling. Previous studies represent items within LLMs' input prompts as either ID indices or textual metadata. However, these approaches often fail to either encapsulate comprehensive world knowledge or exhibit sufficient behavioral understanding. To combine the complementary strengths of conventional recommenders in capturing behavioral patterns of users and LLMs in encoding world knowledge about items, we introduce Large Language-Recommendation Assistant (LLaRA). Specifically, it uses a novel hybrid prompting method that integrates ID-based item embeddings learned by traditional recommendation models with textual item features. Treating the\"sequential behaviors of users\"as a distinct modality beyond texts, we employ a projector to align the traditional recommender's ID embeddings with the LLM's input space. Moreover, rather than directly exposing the hybrid prompt to LLMs, a curriculum learning strategy is adopted to gradually ramp up training complexity. Initially, we warm up the LLM using text-only prompts, which better suit its inherent language modeling ability. Subsequently, we progressively transition to the hybrid prompts, training the model to seamlessly incorporate the behavioral knowledge from the traditional sequential recommender into the LLM. Empirical results validate the effectiveness of our proposed framework. Codes are available at https://github.com/ljy0ustc/LLaRA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2269787162",
                    "name": "Jiayi Liao"
                },
                {
                    "authorId": "2261249035",
                    "name": "Sihang Li"
                },
                {
                    "authorId": "2261748725",
                    "name": "Zhengyi Yang"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2263441815",
                    "name": "Yancheng Yuan"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "2240825631",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "984583e8380fc8af8e20f32ed6ad9d7bc4888c6c",
            "title": "Large Language Model Can Interpret Latent Space of Sequential Recommender",
            "abstract": "Sequential recommendation is to predict the next item of interest for a user, based on her/his interaction history with previous items. In conventional sequential recommenders, a common approach is to model item sequences using discrete IDs, learning representations that encode sequential behaviors and reflect user preferences. Inspired by recent success in empowering large language models (LLMs) to understand and reason over diverse modality data (e.g., image, audio, 3D points), a compelling research question arises: ``Can LLMs understand and work with hidden representations from ID-based sequential recommenders?''.To answer this, we propose a simple framework, RecInterpreter, which examines the capacity of open-source LLMs to decipher the representation space of sequential recommenders. Specifically, with the multimodal pairs (\\ie representations of interaction sequence and text narrations), RecInterpreter first uses a lightweight adapter to map the representations into the token embedding space of the LLM. Subsequently, it constructs a sequence-recovery prompt that encourages the LLM to generate textual descriptions for items within the interaction sequence. Taking a step further, we propose a sequence-residual prompt instead, which guides the LLM in identifying the residual item by contrasting the representations before and after integrating this residual into the existing sequence. Empirical results showcase that our RecInterpreter enhances the exemplar LLM, LLaMA, to understand hidden representations from ID-based sequential recommenders, especially when guided by our sequence-residual prompts. Furthermore, RecInterpreter enables LLaMA to instantiate the oracle items generated by generative recommenders like DreamRec, concreting the item a user would ideally like to interact with next. Codes are available at https://github.com/YangZhengyi98/RecInterpreter.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261748725",
                    "name": "Zhengyi Yang"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2231664238",
                    "name": "Yancheng Luo"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2263441815",
                    "name": "Yancheng Yuan"
                },
                {
                    "authorId": "2153659066",
                    "name": "An Zhang"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "2240825631",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "c718c0a7185b16edc29432fe943489ca630c3c48",
            "title": "Generate What You Prefer: Reshaping Sequential Recommendation via Guided Diffusion",
            "abstract": "Sequential recommendation aims to recommend the next item that matches a user's interest, based on the sequence of items he/she interacted with before. Scrutinizing previous studies, we can summarize a common learning-to-classify paradigm -- given a positive item, a recommender model performs negative sampling to add negative items and learns to classify whether the user prefers them or not, based on his/her historical interaction sequence. Although effective, we reveal two inherent limitations:(1) it may differ from human behavior in that a user could imagine an oracle item in mind and select potential items matching the oracle; and (2) the classification is limited in the candidate pool with noisy or easy supervision from negative samples, which dilutes the preference signals towards the oracle item. Yet, generating the oracle item from the historical interaction sequence is mostly unexplored. To bridge the gap, we reshape sequential recommendation as a learning-to-generate paradigm, which is achieved via a guided diffusion model, termed DreamRec.Specifically, for a sequence of historical items, it applies a Transformer encoder to create guidance representations. Noising target items explores the underlying distribution of item space; then, with the guidance of historical interactions, the denoising process generates an oracle item to recover the positive item, so as to cast off negative sampling and depict the true preference of the user directly. We evaluate the effectiveness of DreamRec through extensive experiments and comparisons with existing methods. Codes and data are open-sourced at https://github.com/YangZhengyi98/DreamRec.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261748725",
                    "name": "Zhengyi Yang"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2177238211",
                    "name": "Zhicai Wang"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "2263441815",
                    "name": "Yancheng Yuan"
                },
                {
                    "authorId": "2240825631",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "d3db65aec6bc2e1bdb101658ed854ec972829032",
            "title": "Model-enhanced Contrastive Reinforcement Learning for Sequential Recommendation",
            "abstract": "Reinforcement learning (RL) has been widely applied in recommendation systems due to its potential in optimizing the long-term engagement of users. From the perspective of RL, recommendation can be formulated as a Markov decision process (MDP), where recommendation system (agent) can interact with users (environment) and acquire feedback (reward signals).However, it is impractical to conduct online interactions with the concern on user experience and implementation complexity, and we can only train RL recommenders with offline datasets containing limited reward signals and state transitions. Therefore, the data sparsity issue of reward signals and state transitions is very severe, while it has long been overlooked by existing RL recommenders.Worse still, RL methods learn through the trial-and-error mode, but negative feedback cannot be obtained in implicit feedback recommendation tasks, which aggravates the overestimation problem of offline RL recommender. To address these challenges, we propose a novel RL recommender named model-enhanced contrastive reinforcement learning (MCRL). On the one hand, we learn a value function to estimate the long-term engagement of users, together with a conservative value learning mechanism to alleviate the overestimation problem.On the other hand, we construct some positive and negative state-action pairs to model the reward function and state transition function with contrastive learning to exploit the internal structure information of MDP. Experiments demonstrate that the proposed method significantly outperforms existing offline RL and self-supervised RL methods with different representative backbone networks on two real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257039734",
                    "name": "Chengpeng Li"
                },
                {
                    "authorId": "2261748725",
                    "name": "Zhengyi Yang"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "2506522",
                    "name": "Dingxian Wang"
                },
                {
                    "authorId": "2240825631",
                    "name": "Xiangnan He"
                },
                {
                    "authorId": "2259678005",
                    "name": "Xiang Wang"
                }
            ]
        }
    ]
}