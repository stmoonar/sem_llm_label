{
    "authorId": "1763608",
    "papers": [
        {
            "paperId": "3c7a060f32d26f6463005f22183715aa4b369358",
            "title": "CoMo: Controllable Motion Generation through Language Guided Pose Code Editing",
            "abstract": "Text-to-motion models excel at efficient human motion generation, but existing approaches lack fine-grained controllability over the generation process. Consequently, modifying subtle postures within a motion or inserting new actions at specific moments remains a challenge, limiting the applicability of these methods in diverse scenarios. In light of these challenges, we introduce CoMo, a Controllable Motion generation model, adept at accurately generating and editing motions by leveraging the knowledge priors of large language models (LLMs). Specifically, CoMo decomposes motions into discrete and semantically meaningful pose codes, with each code encapsulating the semantics of a body part, representing elementary information such as\"left knee slightly bent\". Given textual inputs, CoMo autoregressively generates sequences of pose codes, which are then decoded into 3D motions. Leveraging pose codes as interpretable representations, an LLM can directly intervene in motion editing by adjusting the pose codes according to editing instructions. Experiments demonstrate that CoMo achieves competitive performance in motion generation compared to state-of-the-art models while, in human studies, CoMo substantially surpasses previous work in motion editing abilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273119677",
                    "name": "Yiming Huang"
                },
                {
                    "authorId": "2292409003",
                    "name": "Weilin Wan"
                },
                {
                    "authorId": "2109409802",
                    "name": "Yue Yang"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                },
                {
                    "authorId": "2064210",
                    "name": "Mark Yatskar"
                },
                {
                    "authorId": "2274948399",
                    "name": "Lingjie Liu"
                }
            ]
        },
        {
            "paperId": "50257a5a7f9c27835dd5fd774d333109c218f4f9",
            "title": "FanOutQA: A Multi-Hop, Multi-Document Question Answering Benchmark for Large Language Models",
            "abstract": "One type of question that is commonly found in day-to-day scenarios is ``fan-out'' questions, complex multi-hop, multi-document reasoning questions that require finding information about a large number of entities. However, there exist few resources to evaluate this type of question-answering capability among large language models. To evaluate complex reasoning in LLMs more fully, we present FanOutQA, a high-quality dataset of fan-out question-answer pairs and human-annotated decompositions with English Wikipedia as the knowledge base. We formulate three benchmark settings across our dataset and benchmark 7 LLMs, including GPT-4, LLaMA 2, Claude-2.1, and Mixtral-8x7B, finding that contemporary models still have room to improve reasoning over inter-document dependencies in a long context. We provide our dataset and open-source tools to run models to encourage evaluation at https://fanoutqa.com",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2159538234",
                    "name": "Andrew Zhu"
                },
                {
                    "authorId": "2284994660",
                    "name": "Alyssa Hwang"
                },
                {
                    "authorId": "83863037",
                    "name": "Liam Dugan"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                }
            ]
        },
        {
            "paperId": "75c5e94d79ada9016788c95551166b16f49858cf",
            "title": "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows",
            "abstract": "Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at https://github.com/datadreamer-dev/DataDreamer .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109171018",
                    "name": "Ajay Patel"
                },
                {
                    "authorId": "2269733851",
                    "name": "Colin Raffel"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                }
            ]
        },
        {
            "paperId": "7c23b7b3fddb89550598c4812ecf2475c1ecb0b9",
            "title": "Evaluating Vision-Language Models on Bistable Images",
            "abstract": "Bistable images, also known as ambiguous or reversible images, present visual stimuli that can be seen in two distinct interpretations, though not simultaneously, by the observer. In this study, we conduct the most extensive examination of vision-language models using bistable images to date. We manually gathered a dataset of 29 bistable images, along with their associated labels, and subjected them to 121 different manipulations in brightness, resolution, tint, and rotation. We evaluated twelve different models in both classification and generative tasks across six model architectures. Our findings reveal that, with the exception of models from the Idefics family and LLaVA1.5-13b, there is a pronounced preference for one interpretation over another among the models, and minimal variance under image manipulations, with few exceptions on image rotations. Additionally, we compared the models\u2019 preferences with humans, noting that the models do not exhibit the same continuity biases as humans and often diverge from human initial interpretations. We also investigated the influence of variations in prompts and the use of synonymous labels, discovering that these factors significantly affect model interpretations more than image manipulations showing a higher influence of the language priors on bistable image interpretations compared to image-text training data. All code and data is open sourced.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2074098576",
                    "name": "Artemis Panagopoulou"
                },
                {
                    "authorId": "2303843081",
                    "name": "Coby Melkin"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                }
            ]
        },
        {
            "paperId": "992c554b1bf343eef3509579930b2552f1b6f1db",
            "title": "Calibrating Large Language Models with Sample Consistency",
            "abstract": "Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1904906987",
                    "name": "Qing Lyu"
                },
                {
                    "authorId": "2266467511",
                    "name": "Kumar Shridhar"
                },
                {
                    "authorId": "8805254",
                    "name": "Chaitanya Malaviya"
                },
                {
                    "authorId": "2258934061",
                    "name": "Li Zhang"
                },
                {
                    "authorId": "51131518",
                    "name": "Yanai Elazar"
                },
                {
                    "authorId": "2261389843",
                    "name": "Niket Tandon"
                },
                {
                    "authorId": "2817917",
                    "name": "Marianna Apidianaki"
                },
                {
                    "authorId": "2790926",
                    "name": "Mrinmaya Sachan"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                }
            ]
        },
        {
            "paperId": "bbbbb6517d6b1d17b5d5ab7170103d2c8540ed30",
            "title": "PROC2PDDL: Open-Domain Planning Representations from Texts",
            "abstract": "Planning in a text-based environment continues to be a significant challenge for AI systems. Recent approaches have utilized language models to predict planning domain definitions (e.g., PDDL) but have only been evaluated in closed-domain simulated environments. To address this, we present Proc2PDDL, the first dataset containing open-domain procedural texts paired with expert-annotated PDDL representations. Using this dataset, we evaluate the task of predicting domain actions (parameters, preconditions, and effects). We experiment with various large language models (LLMs) and prompting mechanisms, including a novel instruction inspired by the zone of proximal development (ZPD), which reconstructs the task as incremental basic skills. Our results demonstrate that Proc2PDDL is highly challenging for end-to-end LLMs, with GPT-3.5\u2019s success rate close to 0% and GPT-4o\u2019s 38%. With ZPD instructions, GPT-4o\u2019s success rate increases to 45%, outperforming regular chain-of-thought prompting\u2019s 34%. Our analysis systematically examines both syntactic and semantic errors, providing insights into the strengths and weaknesses of language models in generating domain-specific programs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2289680500",
                    "name": "Tianyi Zhang"
                },
                {
                    "authorId": "2258934061",
                    "name": "Li Zhang"
                },
                {
                    "authorId": "2165225503",
                    "name": "Zhaoyi Hou"
                },
                {
                    "authorId": "2289780153",
                    "name": "Ziyu Wang"
                },
                {
                    "authorId": "2267160000",
                    "name": "Yuling Gu"
                },
                {
                    "authorId": "2289611026",
                    "name": "Peter Clark"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                },
                {
                    "authorId": "2261389843",
                    "name": "Niket Tandon"
                }
            ]
        },
        {
            "paperId": "c3752fd57d81da76f45bb61fc566093bdb38e5d8",
            "title": "Large Language Models Can Self-Improve At Web Agent Tasks",
            "abstract": "Training models to act as agents that can effectively navigate and perform actions in a complex environment, such as a web browser, has typically been challenging due to lack of training data. Large language models (LLMs) have recently demonstrated some capability to navigate novel environments as agents in a zero-shot or few-shot fashion, purely guided by natural language instructions as prompts. Recent research has also demonstrated LLMs have the capability to exceed their base performance through self-improvement, i.e. fine-tuning on data generated by the model itself. In this work, we explore the extent to which LLMs can self-improve their performance as agents in long-horizon tasks in a complex environment using the WebArena benchmark. In WebArena, an agent must autonomously navigate and perform actions on web pages to achieve a specified objective. We explore fine-tuning on three distinct synthetic training data mixtures and achieve a 31\\% improvement in task completion rate over the base model on the WebArena benchmark through a self-improvement procedure. We additionally contribute novel evaluation metrics for assessing the performance, robustness, capabilities, and quality of trajectories of our fine-tuned agent models to a greater degree than simple, aggregate-level benchmark scores currently used to measure self-improvement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109171018",
                    "name": "Ajay Patel"
                },
                {
                    "authorId": "66798949",
                    "name": "M. Hofmarcher"
                },
                {
                    "authorId": "2282138287",
                    "name": "Claudiu Leoveanu-Condrei"
                },
                {
                    "authorId": "1974372841",
                    "name": "Marius-Constantin Dinu"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                },
                {
                    "authorId": "3308557",
                    "name": "Sepp Hochreiter"
                }
            ]
        },
        {
            "paperId": "d0317495dc03c22501618db88e15acc714044298",
            "title": "PDDLEGO: Iterative Planning in Textual Environments",
            "abstract": "Planning in textual environments have been shown to be a long-standing challenge even for current models. A recent, promising line of work uses LLMs to generate a formal representation of the environment that can be solved by a symbolic planner. However, existing methods rely on a fully-observed environment where all entity states are initially known, so a one-off representation can be constructed, leading to a complete plan. In contrast, we tackle partially-observed environments where there is initially no sufficient information to plan for the end-goal. We propose PDDLEGO that iteratively construct a planning representation that can lead to a partial plan for a given sub-goal. By accomplishing the sub-goal, more information is acquired to augment the representation, eventually achieving the end-goal. We show that plans produced by few-shot PDDLEGO are 43% more efficient than generating plans end-to-end on the Coin Collector simulation, with strong performance (98%) on the more complex Cooking World simulation where end-to-end LLMs fail to generate coherent plans (4%).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258934061",
                    "name": "Li Zhang"
                },
                {
                    "authorId": "144949918",
                    "name": "Peter Alexander Jansen"
                },
                {
                    "authorId": "2289680500",
                    "name": "Tianyi Zhang"
                },
                {
                    "authorId": "2289611026",
                    "name": "Peter Clark"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                },
                {
                    "authorId": "2261389843",
                    "name": "Niket Tandon"
                }
            ]
        },
        {
            "paperId": "ffe71c6035e7e5b8f988dc131224e32f64a90f1d",
            "title": "PaCE: Parsimonious Concept Engineering for Large Language Models",
            "abstract": "Large Language Models (LLMs) are being used for a wide variety of tasks. While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations. Alignment methods are designed to reduce such undesirable output, via techniques such as fine-tuning, prompt engineering, and representation engineering. However, existing methods face several challenges: some require costly fine-tuning for every alignment task; some do not adequately remove undesirable concepts, failing alignment; some remove benign concepts, lowering the linguistic capabilities of LLMs. To address these issues, we propose Parsimonious Concept Engineering (PaCE), a novel activation engineering framework for alignment. First, to sufficiently model the concepts, we construct a large-scale concept dictionary in the activation space, in which each atom corresponds to a semantic concept. Then, given any alignment task, we instruct a concept partitioner to efficiently annotate the concepts as benign or undesirable. Finally, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activation as a linear combination of the benign and undesirable components. By removing the latter ones from the activation, we reorient the behavior of LLMs towards alignment goals. We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32020492",
                    "name": "Jinqi Luo"
                },
                {
                    "authorId": "2304953383",
                    "name": "Tianjiao Ding"
                },
                {
                    "authorId": "2275849013",
                    "name": "Kwan Ho Ryan Chan"
                },
                {
                    "authorId": "40494989",
                    "name": "D. Thaker"
                },
                {
                    "authorId": "2275201091",
                    "name": "Aditya Chattopadhyay"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                },
                {
                    "authorId": "2268674509",
                    "name": "Ren'e Vidal"
                }
            ]
        },
        {
            "paperId": "0d9ca76f2383712dc122ee59db2afa1485dac12d",
            "title": "Interpretable-by-Design Text Understanding with Iteratively Generated Concept Bottleneck",
            "abstract": "Black-box deep neural networks excel in text classification, yet their application in high-stakes domains is hindered by their lack of interpretability. To address this, we propose Text Bottleneck Models (TBM), an intrinsically interpretable text classification framework that offers both global and local explanations. Rather than directly predicting the output label, TBM predicts categorical values for a sparse set of salient concepts and uses a linear layer over those concept values to produce the final prediction. These concepts can be automatically discovered and measured by a Large Language Model (LLM) without the need for human curation. Experiments on 12 diverse text understanding datasets demonstrate that TBM can rival the performance of black-box baselines such as few-shot GPT-4 and finetuned DeBERTa while falling short against finetuned GPT-3.5. Comprehensive human evaluation validates that TBM can generate high-quality concepts relevant to the task, and the concept measurement aligns well with human judgments, suggesting that the predictions made by TBMs are interpretable. Overall, our findings suggest that TBM is a promising new framework that enhances interpretability with minimal performance tradeoffs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216604794",
                    "name": "Josh Magnus Ludan"
                },
                {
                    "authorId": "1904906987",
                    "name": "Qing Lyu"
                },
                {
                    "authorId": "2109409802",
                    "name": "Yue Yang"
                },
                {
                    "authorId": "83863037",
                    "name": "Liam Dugan"
                },
                {
                    "authorId": "2064210",
                    "name": "Mark Yatskar"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                }
            ]
        }
    ]
}