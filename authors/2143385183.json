{
    "authorId": "2143385183",
    "papers": [
        {
            "paperId": "385f613b281318b335075b22346476dffbb36c76",
            "title": "FairSample: Training Fair and Accurate Graph Convolutional Neural Networks Efficiently",
            "abstract": "Fairness in Graph Convolutional Neural Networks (GCNs) becomes a more and more important concern as GCNs are adopted in many crucial applications. Societal biases against sensitive groups may exist in many real world graphs. GCNs trained on those graphs may be vulnerable to being affected by such biases. In this paper, we adopt the well-known fairness notion of demographic parity and tackle the challenge of training fair and accurate GCNs efficiently. We present an in-depth analysis on how graph structure bias, node attribute bias, and model parameters may affect the demographic parity of GCNs. Our insights lead to FairSample, a framework that jointly mitigates the three types of biases. We employ two intuitive strategies to rectify graph structures. First, we inject edges across nodes that are in different sensitive groups but similar in node features. Second, to enhance model fairness and retain model quality, we develop a learnable neighbor sampling policy using reinforcement learning. To address the bias in node features and model parameters, FairSample is complemented by a regularization objective to optimize fairness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "121977751",
                    "name": "Zicun Cong"
                },
                {
                    "authorId": "2680278",
                    "name": "Baoxu Shi"
                },
                {
                    "authorId": "2119028796",
                    "name": "Shan Li"
                },
                {
                    "authorId": "32073252",
                    "name": "Jaewon Yang"
                },
                {
                    "authorId": "2152881560",
                    "name": "Qi He"
                },
                {
                    "authorId": "2143385183",
                    "name": "Jian Pei"
                }
            ]
        },
        {
            "paperId": "5ca04cde78542b8d58c4c5b2db91ddd65b56d917",
            "title": "LazyGNN: Large-Scale Graph Neural Networks via Lazy Propagation",
            "abstract": "Recent works have demonstrated the benefits of capturing long-distance dependency in graphs by deeper graph neural networks (GNNs). But deeper GNNs suffer from the long-lasting scalability challenge due to the neighborhood explosion problem in large-scale graphs. In this work, we propose to capture long-distance dependency in graphs by shallower models instead of deeper models, which leads to a much more efficient model, LazyGNN, for graph representation learning. Moreover, we demonstrate that LazyGNN is compatible with existing scalable approaches (such as sampling methods) for further accelerations through the development of mini-batch LazyGNN. Comprehensive experiments demonstrate its superior prediction performance and scalability on large-scale benchmarks. The implementation of LazyGNN is available at https://github.com/RXPHD/Lazy_GNN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066270999",
                    "name": "Rui Xue"
                },
                {
                    "authorId": "2049039664",
                    "name": "Haoyu Han"
                },
                {
                    "authorId": "2937550",
                    "name": "MohamadAli Torkamani"
                },
                {
                    "authorId": "2143385183",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "2124928119",
                    "name": "Xiaorui Liu"
                }
            ]
        },
        {
            "paperId": "aa797cb3e4e4f8a8508b69d5fdf075d671f983b1",
            "title": "Deep Learning on Graphs: Methods and Applications (DLG-KDD2023)",
            "abstract": "Deep Learning models are at the core of research in Artificial Intelligence research today. A tide in research for deep learning on graphs or graph neural networks. This wave of research at the intersection of graph theory and deep learning has also influenced other fields of science, including computer vision, natural language processing, program synthesis and analysis, financial security, Drug Discovery and so on. However, there are still many challenges regarding a broad range of the topics in deep learning on graphs, from methodologies to applications, and from foundations to the new frontiers of GNNs. This international workshop on \"Deep Learning on Graphs: Method and Applications (DLG-KDD'23)\" aims to bring together both academic researchers and industrial practitioners from different backgrounds and perspectives to above challenges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3008832",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "2143385183",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "35846319",
                    "name": "Yinglong Xia"
                },
                {
                    "authorId": "46909769",
                    "name": "Xiaojie Guo"
                }
            ]
        },
        {
            "paperId": "093e9a883e80e6bc76b48033a1e8cfa7031638da",
            "title": "Robust Self-Supervised Structural Graph Neural Network for Social Network Prediction",
            "abstract": "The self-supervised graph representation learning has achieved much success in recent web based research and applications, such as recommendation system, social networks, and anomaly detection. However, existing works suffer from two problems. Firstly, in social networks, the influential neighbors are important, but the overwhelming routine in graph representation-learning utilizes the node-wise similarity metric defined on embedding vectors that cannot exactly capture the subtle local structure and the network proximity. Secondly, existing works implicitly assume a universal distribution across datasets, which presumably leads to sub-optimal models considering the potential distribution shift. To address these problems, in this paper, we learn structural embeddings in which the proximity is characterized by 1-Wasserstein distance. We propose a distributionally robust self-supervised graph neural network framework to learn the representations. More specifically, in our method, the embeddings are computed based on subgraphs centering at the node of interest and represent both the node of interest and its neighbors, which better preserves the local structure of nodes. To make our model end-to-end trainable, we adopt a deep implicit layer to compute the Wasserstein distance, which can be formulated as a differentiable convex optimization problem. Meanwhile, our distributionally robust formulation explicitly constrains the maximal diversity for matched queries and keys. As such, our model is insensitive to the data distributions and has better generalization abilities. Extensive experiments demonstrate that the graph encoder learned by our approach can be utilized for various downstream analyses, including node classification, graph classification, and top-k similarity search. The results show our algorithm outperforms state-of-the-art baselines, and the ablation study validates the effectiveness of our design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49889644",
                    "name": "Yanfu Zhang"
                },
                {
                    "authorId": "2141896",
                    "name": "Hongchang Gao"
                },
                {
                    "authorId": "2143385183",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "145114933",
                    "name": "Heng Huang"
                }
            ]
        },
        {
            "paperId": "2cab0c17c29ac9479f12a552d50b6094865fcd0b",
            "title": "Revisiting Graph Contrastive Learning from the Perspective of Graph Spectrum",
            "abstract": "Graph Contrastive Learning (GCL), learning the node representations by augmenting graphs, has attracted considerable attentions. Despite the proliferation of various graph augmentation strategies, some fundamental questions still remain unclear: what information is essentially encoded into the learned representations by GCL? Are there some general graph augmentation rules behind different augmentations? If so, what are they and what insights can they bring? In this paper, we answer these questions by establishing the connection between GCL and graph spectrum. By an experimental investigation in spectral domain, we firstly find the General grAph augMEntation (GAME) rule for GCL, i.e., the difference of the high-frequency parts between two augmented graphs should be larger than that of low-frequency parts. This rule reveals the fundamental principle to revisit the current graph augmentations and design new effective graph augmentations. Then we theoretically prove that GCL is able to learn the invariance information by contrastive invariance theorem, together with our GAME rule, for the first time, we uncover that the learned representations by GCL essentially encode the low-frequency information, which explains why GCL works. Guided by this rule, we propose a spectral graph contrastive learning module (SpCo), which is a general and GCL-friendly plug-in. We combine it with different existing GCL models, and extensive experiments well demonstrate that it can further improve the performances of a wide variety of different GCL methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087010708",
                    "name": "Nian Liu"
                },
                {
                    "authorId": "2118449003",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "1491450638",
                    "name": "Deyu Bo"
                },
                {
                    "authorId": "2151458697",
                    "name": "Chuan Shi"
                },
                {
                    "authorId": "2143385183",
                    "name": "Jian Pei"
                }
            ]
        },
        {
            "paperId": "31bb493c4d3286562a5cbadce439bb591ba65d53",
            "title": "Toward Unified Data and Algorithm Fairness via Adversarial Data Augmentation and Adaptive Model Fine-tuning",
            "abstract": "There is some recent research interest in algorithmic fairness for biased data. There are a variety of pre-, in-, and post-processing methods designed for this problem. However, these methods are exclusively targeting data unfairness and algorithmic unfairness. In this paper, we propose a novel intra-processing method to broaden the application scenario of fairness methods, which can simultaneously address the two bias sources. Since training modern deep models from scratch is expensive due to the enormous training data and the complicated structures, we propose an augmentation and fine-tuning framework. First, we design an adversarial attack to generate weighted samples disentangled with the protected attribute. Next, we identify the fair sub-structure in the biased model and fine-tune the model via weight reactivation. At last, we provide an optional joint training scheme for the augmentation and the fine-tuning. Our method can be combined with a variety of fairness measures. We benchmark our method and some related baselines to show the advantage and the scalability. Experimental results on several standard datasets demonstrate that our approach can effectively learn fair augmentation and achieve superior results to the state-of-the-art baselines. Our method also generalizes well to different types of data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49889644",
                    "name": "Yanfu Zhang"
                },
                {
                    "authorId": "1491239652",
                    "name": "Runxue Bao"
                },
                {
                    "authorId": "2143385183",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "2118881829",
                    "name": "Heng Huang"
                }
            ]
        },
        {
            "paperId": "39b8ebdacbb37a09ff87d8c7d5cbf2e1c64e8ad3",
            "title": "Communication-Efficient Robust Federated Learning with Noisy Labels",
            "abstract": "Federated learning (FL) is a promising privacy-preserving machine learning paradigm over distributed located data. In FL, the data is kept locally by each user. This protects the user privacy, but also makes the server difficult to verify data quality, especially if the data are correctly labeled. Training with corrupted labels is harmful to the federated learning task; however, little attention has been paid to FL in the case of label noise. In this paper, we focus on this problem and propose a learning-based reweighting approach to mitigate the effect of noisy labels in FL. More precisely, we tuned a weight for each training sample such that the learned model has optimal generalization performance over a validation set. More formally, the process can be formulated as a Federated Bilevel Optimization problem. Bilevel optimization problem is a type of optimization problem with two levels of entangled problems. The non-distributed bilevel problems have witnessed notable progress recently with new efficient algorithms. However, solving bilevel optimization problems under the Federated Learning setting is under-investigated. We identify that the high communication cost in hypergradient evaluation is the major bottleneck. So we proposeComm-FedBiO to solve the general Federated Bilevel Optimization problems; more specifically, we propose two communication-efficient subroutines to estimate the hypergradient. Convergence analysis of the proposed algorithms is also provided. Finally, we apply the proposed algorithms to solve the noisy label problem. Our approach has shown superior performance on several real-world datasets compared to various baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108933511",
                    "name": "Junyi Li"
                },
                {
                    "authorId": "2143385183",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "2146258604",
                    "name": "Heng Huang"
                }
            ]
        },
        {
            "paperId": "b2ea3564e8d763a00d733a3dc44f85550a995fd0",
            "title": "Graph Neural Networks: Foundation, Frontiers and Applications",
            "abstract": "The field of graph neural networks (GNNs) has seen rapid and incredible strides over the recent years. Graph neural networks, also known as deep learning on graphs, graph representation learning, or geometric deep learning, have become one of the fastest-growing research topics in machine learning, especially deep learning. This wave of research at the intersection of graph theory and deep learning has also influenced other fields of science, including recommendation systems, computer vision, natural language processing, inductive logic programming, program synthesis, software mining, automated planning, cybersecurity, and intelligent transportation. However, as the field rapidly grows, it has been extremely challenging to gain a global perspective of the developments of GNNs. Therefore, we feel the urgency to bridge the above gap and have a comprehensive tutorial on this fast-growing yet challenging topic. This tutorial of Graph Neural Networks (GNNs): Foundation, Frontiers and Applications will cover a broad range of topics in graph neural networks, by reviewing and introducing the fundamental concepts and algorithms of GNNs, new research frontiers of GNNs, and broad and emerging applications with GNNs. In addition, rich tutorial materials will be included and introduced to help the audience gain a systematic understanding by using our recently published book-Graph Neural Networks (GNN): Foundation, Frontiers, and Applications [12], which can easily be accessed at https://graph-neural-networks.github.io/index.html.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116666963",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "2052469774",
                    "name": "P. Cui"
                },
                {
                    "authorId": "2143385183",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "144000223",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "46909769",
                    "name": "Xiaojie Guo"
                }
            ]
        },
        {
            "paperId": "ec7fca842668bfec92f762b6eba0ebf61b2369d2",
            "title": "TrustLOG: The First Workshop on Trustworthy Learning on Graphs",
            "abstract": "Learning on graphs (LOG) plays a pivotal role in various high-impact application domains. The past decades have developed tremendous theories, algorithms, and open-source systems in answering what/who questions on graphs. However, recent studies reveal that the state-of-the-art techniques for learning on graphs (LOG) are often not trustworthy in practice with respect to several social aspects (e.g., fairness, transparency, security). A natural research question to ask is: how can we make learning algorithms on graphs trustworthy? To answer this question, we propose a paradigm shift, from answering what and who LOG questions to understanding how and why LOG questions. The TrustLOG workshop provides a venue for presenting, discussing, and promoting frontier research on trustworthy learning on graphs. Moreover, TrustLOG will serve as an impulse for the LOG community to identify novel research problems and shed new light on future directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111625448",
                    "name": "Jian Kang"
                },
                {
                    "authorId": "2108433254",
                    "name": "Shuaicheng Zhang"
                },
                {
                    "authorId": "2155883057",
                    "name": "Bo Li"
                },
                {
                    "authorId": "31108652",
                    "name": "Jingrui He"
                },
                {
                    "authorId": "2143385183",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "49407567",
                    "name": "Dawei Zhou"
                }
            ]
        },
        {
            "paperId": "f525939cbdc719933dc7869e528dd6da302b21e4",
            "title": "Deep Learning on Graphs: Methods and Applications (DLG-KDD2022)",
            "abstract": "Deep Learning models are at the core of research in Artificial Intelligence research today. A tide in research for deep learning on graphs or graph neural networks. This wave of research at the intersection of graph theory and deep learning has also influenced other fields of science, including computer vision, natural language processing, program synthesis and analysis, financial security, Drug Discovery and so on. However, there are still many challenges regarding a broad range of the topics in deep learning on graphs, from methodologies to applications, and from foundations to the new frontiers of GNNs. This international workshop on \"Deep Learning on Graphs: Method and Applications (DLG-KDD'22)\" aims to bring together both academic researchers and industrial practitioners from different backgrounds and perspectives to above challenges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3008832",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "2143385183",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "35846319",
                    "name": "Yinglong Xia"
                },
                {
                    "authorId": "46909769",
                    "name": "Xiaojie Guo"
                }
            ]
        }
    ]
}