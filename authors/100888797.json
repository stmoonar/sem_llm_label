{
    "authorId": "100888797",
    "papers": [
        {
            "paperId": "7d3f0f83fa943529d1516030c6abebd5a89aa44c",
            "title": "High Precision Sound Event Detection based on Transfer Learning using Transposed Convolutions and Feature Pyramid Network",
            "abstract": "We introduce two models for high precision sound event detection leveraging transfer learning. The sound events we detect include \u201cspeech\u201d, \u201cmusic\u201d, and \u201cchime\u201d. Both models consist of a CNN backbone pre-trained using AudioSet for audio classification. To get high precision detection results, the first model employs transposed convolutional layers as the detection head, while the second model uses Feature Pyramid Network(FPN) as the detection head. Experimental results show 98.8% accuracy and 98.6% F1 score on a private test set, from the one using FPN. Both models outperform a two-stage model using LSTM, various model ensembles, and a pre-trained neural network model for audio classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114031488",
                    "name": "S. Luo"
                },
                {
                    "authorId": "100888797",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "152613893",
                    "name": "Z. Liu"
                },
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "67038534",
                    "name": "Bruce Ferry"
                }
            ]
        },
        {
            "paperId": "ee210be856f8f782308808775e3dbd64a3e3df45",
            "title": "International Workshop on Multimodal Learning - 2023 Theme: Multimodal Learning with Foundation Models",
            "abstract": "The recent advancements in machine learning and artificial intelligence (particularly foundation models such as BERT, GPT-3, T5, ResNet, etc.) have demonstrated remarkable capabilities and driven significant revolutionary changes to the way we make inferences from complex data. These models represent a fundamental shift in the way data are approached and offer exciting new research directions and opportunities for multimodal learning and data fusion. Given the potential of foundation models to transform the field of multimodal learning, there is a need to bring together experts and researchers to discuss the latest developments in this area, exchange ideas, and identify key research questions and challenges that need to be addressed. By hosting this workshop, we aim to create a forum for researchers to share their insights and expertise on multimodal data fusion and learning using foundation models, and to explore potential new research directions and applications in the rapidly evolving field. We expect contributions from interdisciplinary researchers to study and model interactions between (but not limited to) modalities of language, graphs, time-series, vision, tabular data, sensors, and more. Our workshop will emphasize interdisciplinary work and aim at seeding cross-team collaborations around new tasks, datasets, and models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "49604787",
                    "name": "Fanyou Wu"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "100888797",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "dc55a213cf37862af7f8d1675c732a0dd0f3bc76",
            "title": "Detect Audio-Video Temporal Synchronization Errors in Advertisements (Ads)",
            "abstract": "Detecting audio-video (A/V) synchronization error is important to measure end user experience. Today, researches in this domain are mainly focused on contents such as movies or sports. The state of art algorithms usually first detect a specific type of events and then correlate the A/V data within during these events, e.g., find the human chatting events and then correlate the vocals with the lip shapes. Detecting A/V sync errors during Ads, on the other hand, has not received a lot of attentions. Compared with contents, an Ads section do not contain a particular type of events that can be used to detect A/V sync error. For example, many vocals in Ads are either from background narrators or have a very short period of time, so that the popular lip-sync based algorithms won\u2019t work accurately. In this paper, we present a novel algorithm that uses the scene change time features: we first segment out individual Ad from a playback. Then for each pair of temporal adjacent Ads, we compute the scene change time for the video data and the audio data separately, and then build their time difference histogram. Next, we aggregate the histograms from all Ads pairs within one Ads section. Finally, we combine the aggregated histogram to compute the A/V off-sync time values. We show that compared with the traditional lip-sync based algorithms, the new algorithm not only significantly improves the prediction rate, but also increases the prediction accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152613893",
                    "name": "Z. Liu"
                },
                {
                    "authorId": "2193282077",
                    "name": "Devin Chen"
                },
                {
                    "authorId": "100888797",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "2114031488",
                    "name": "S. Luo"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "67038534",
                    "name": "Bruce Ferry"
                }
            ]
        },
        {
            "paperId": "e32ba95825c343a6acb1031f26ee4e547ea1bac7",
            "title": "A Two-Stage LSTM Based Approach for Voice Activity Detection with Sound Event Classification",
            "abstract": "We introduce a two-stage approach using LSTM for voice activity detection with sound event classification. This approach proves to be effective when training data is limited. Moreover, it achieves better performance than pre-trained model using large-scale data set (AudioSet). Apart from clip-level accuracy, we also introduce two metrics for evaluating overall audio segmentation accuracy: mean $\\mathbf{IoU}$),and mean front miss. On test set, our method achieves 98 % accuracy, 0.95 mean $\\mathbf{IoU}$ for speech and 0.99 mean $\\mathbf{IoU}$ for music, and 0.03 mean front miss for both speech and music.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100888797",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "152613893",
                    "name": "Z. Liu"
                },
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "67038534",
                    "name": "Bruce Ferry"
                }
            ]
        }
    ]
}