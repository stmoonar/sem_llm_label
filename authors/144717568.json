{
    "authorId": "144717568",
    "papers": [
        {
            "paperId": "2231af21d60dfabc806988f538630d3b936d0bd8",
            "title": "Algorithmic Arbitrariness in Content Moderation",
            "abstract": "Machine learning (ML) is widely used to moderate online content. Despite its scalability relative to human moderation, the use of ML introduces unique challenges to content moderation. One such challenge is predictive multiplicity: multiple competing models for content classification may perform equally well on average, yet assign conflicting predictions to the same content. This multiplicity can result from seemingly innocuous choices made during training, which do not meaningfully change the accuracy of the ML model, but can nevertheless change what the model gets wrong. We experimentally demonstrate how content moderation tools can arbitrarily classify samples as \u201ctoxic,\u201d leading to arbitrary restrictions on speech. We use the principles set by the International Covenant on Civil and Political Rights (ICCPR), namely freedom of expression, non-discrimination, and procedural justice to interpret the effects of these findings in terms of Human Rights. We analyze (i) the extent of predictive multiplicity among popular state-of-the-art LLMs used for detecting \u201ctoxic\u201d content; (ii) the disparate impact of this arbitrariness across social groups; and (iii) the magnitude of model multiplicity on content that is unanimously recognized as toxic by human annotators. Our findings indicate that the up-scaled algorithmic moderation risks legitimizing an \u201calgorithmic leviathan\u201d, where an algorithm disproportionately manages human rights. To mitigate such risks, our study underscores the need to identify and increase the transparency of arbitrariness in content moderation applications. Our findings have implications to content moderation and intermediary liability laws being discussed and passed in many countries, such as the Digital Services Act in the European Union, the Online Safety Act in the United Kingdom, and the recent TSE resolutions in Brazil.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287925488",
                    "name": "Juan Felipe Gomez"
                },
                {
                    "authorId": "2287844503",
                    "name": "Caio Vieira Machado"
                },
                {
                    "authorId": "153835786",
                    "name": "Lucas Monteiro Paes"
                },
                {
                    "authorId": "144717568",
                    "name": "F. Calmon"
                }
            ]
        },
        {
            "paperId": "5e0f70cdfa907fd64bffdbcd0f0bb851908ff7a9",
            "title": "Selective Explanations",
            "abstract": "Feature attribution methods explain black-box machine learning (ML) models by assigning importance scores to input features. These methods can be computationally expensive for large ML models. To address this challenge, there has been increasing efforts to develop amortized explainers, where a machine learning model is trained to predict feature attribution scores with only one inference. Despite their efficiency, amortized explainers can produce inaccurate predictions and misleading explanations. In this paper, we propose selective explanations, a novel feature attribution method that (i) detects when amortized explainers generate low-quality explanations and (ii) improves these explanations using a technique called explanations with initial guess. Our selective explanation method allows practitioners to specify the fraction of samples that receive explanations with initial guess, offering a principled way to bridge the gap between amortized explainers and their high-quality counterparts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153835786",
                    "name": "Lucas Monteiro Paes"
                },
                {
                    "authorId": "2304219801",
                    "name": "Dennis Wei"
                },
                {
                    "authorId": "144717568",
                    "name": "F. Calmon"
                }
            ]
        },
        {
            "paperId": "8362c45885738f5246e163a9763e0270d229ca6b",
            "title": "Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)",
            "abstract": "CLIP embeddings have demonstrated remarkable performance across a wide range of computer vision tasks. However, these high-dimensional, dense vector representations are not easily interpretable, restricting their usefulness in downstream applications that require transparency. In this work, we empirically show that CLIP's latent space is highly structured, and consequently that CLIP representations can be decomposed into their underlying semantic components. We leverage this understanding to propose a novel method, Sparse Linear Concept Embeddings (SpLiCE), for transforming CLIP representations into sparse linear combinations of human-interpretable concepts. Distinct from previous work, SpLiCE does not require concept labels and can be applied post hoc. Through extensive experimentation with multiple real-world datasets, we validate that the representations output by SpLiCE can explain and even replace traditional dense CLIP representations, maintaining equivalent downstream performance while significantly improving their interpretability. We also demonstrate several use cases of SpLiCE representations including detecting spurious correlations, model editing, and quantifying semantic shifts in datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2160885489",
                    "name": "Usha Bhalla"
                },
                {
                    "authorId": "2041293215",
                    "name": "Alexander X. Oesterling"
                },
                {
                    "authorId": "2822290",
                    "name": "Suraj Srinivas"
                },
                {
                    "authorId": "144717568",
                    "name": "F. Calmon"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ]
        },
        {
            "paperId": "a5380c02128f9210d680a042d0c0ab1c48a60d4c",
            "title": "Multi-Group Proportional Representation",
            "abstract": "Image search and retrieval tasks can perpetuate harmful stereotypes, erase cultural identities, and amplify social disparities. Current approaches to mitigate these representational harms balance the number of retrieved items across population groups defined by a small number of (often binary) attributes. However, most existing methods overlook intersectional groups determined by combinations of group attributes, such as gender, race, and ethnicity. We introduce Multi-Group Proportional Representation (MPR), a novel metric that measures representation across intersectional groups. We develop practical methods for estimating MPR, provide theoretical guarantees, and propose optimization algorithms to ensure MPR in retrieval. We demonstrate that existing methods optimizing for equal and proportional representation metrics may fail to promote MPR. Crucially, our work shows that optimizing MPR yields more proportional representation across multiple intersectional groups specified by a rich function class, often with minimal compromise in retrieval accuracy.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2041293215",
                    "name": "Alexander X. Oesterling"
                },
                {
                    "authorId": "52015628",
                    "name": "C. M. Verdun"
                },
                {
                    "authorId": "2220214891",
                    "name": "Carol Xuan Long"
                },
                {
                    "authorId": "2310697251",
                    "name": "Alex Glynn"
                },
                {
                    "authorId": "153835786",
                    "name": "Lucas Monteiro Paes"
                },
                {
                    "authorId": "1598458883",
                    "name": "Sajani Vithana"
                },
                {
                    "authorId": "2310697237",
                    "name": "Martina Cardone"
                },
                {
                    "authorId": "144717568",
                    "name": "F. Calmon"
                }
            ]
        },
        {
            "paperId": "e196db4db81db7ba8ca498e99dc9497941e9fc2b",
            "title": "Correlated Privacy Mechanisms for Differentially Private Distributed Mean Estimation",
            "abstract": "Differentially private distributed mean estimation (DP-DME) is a fundamental building block in privacy-preserving federated learning, where a central server estimates the mean of $d$-dimensional vectors held by $n$ users while ensuring $(\\epsilon,\\delta)$-DP. Local differential privacy (LDP) and distributed DP with secure aggregation (SecAgg) are the most common notions of DP used in DP-DME settings with an untrusted server. LDP provides strong resilience to dropouts, colluding users, and malicious server attacks, but suffers from poor utility. In contrast, SecAgg-based DP-DME achieves an $O(n)$ utility gain over LDP in DME, but requires increased communication and computation overheads and complex multi-round protocols to handle dropouts and malicious attacks. In this work, we propose CorDP-DME, a novel DP-DME mechanism that spans the gap between DME with LDP and distributed DP, offering a favorable balance between utility and resilience to dropout and collusion. CorDP-DME is based on correlated Gaussian noise, ensuring DP without the perfect conditional privacy guarantees of SecAgg-based approaches. We provide an information-theoretic analysis of CorDP-DME, and derive theoretical guarantees for utility under any given privacy parameters and dropout/colluding user thresholds. Our results demonstrate that (anti) correlated Gaussian DP mechanisms can significantly improve utility in mean estimation tasks compared to LDP -- even in adversarial settings -- while maintaining better resilience to dropouts and attacks compared to distributed DP.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1598458883",
                    "name": "Sajani Vithana"
                },
                {
                    "authorId": "3311731",
                    "name": "V. Cadambe"
                },
                {
                    "authorId": "144717568",
                    "name": "F. Calmon"
                },
                {
                    "authorId": "2136397119",
                    "name": "Haewon Jeong"
                }
            ]
        },
        {
            "paperId": "2a3d82d3af02d8c893397c45d1bf14042dd9d679",
            "title": "Aleatoric and Epistemic Discrimination: Fundamental Limits of Fairness Interventions",
            "abstract": "Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: aleatoric discrimination, which is inherent in the data distribution, and epistemic discrimination, which is due to decisions made during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell's results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model's accuracy when fairness constraints are applied and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing fairness interventions and investigate fairness risks in data with missing values. Our results indicate that state-of-the-art fairness interventions are effective at removing epistemic discrimination on standard (overused) tabular datasets. However, when data has missing values, there is still significant room for improvement in handling aleatoric discrimination.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2144219566",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2294507804",
                    "name": "Luxi He"
                },
                {
                    "authorId": "2263013291",
                    "name": "Rui Gao"
                },
                {
                    "authorId": "144717568",
                    "name": "F. Calmon"
                }
            ]
        },
        {
            "paperId": "4602fbe468f41fe826930e64e6578640613fe279",
            "title": "Aleatoric and Epistemic Discrimination in Classification",
            "abstract": "Machine learning (ML) models can underperform on certain population groups due to choices made during model development and bias inherent in the data. We categorize sources of discrimination in the ML pipeline into two classes: aleatoric discrimination , which is inherent in the data distribution, and epistemic discrimination , which is due to decisions during model development. We quantify aleatoric discrimination by determining the performance limits of a model under fairness constraints, assuming perfect knowledge of the data distribution. We demonstrate how to characterize aleatoric discrimination by applying Blackwell\u2019s results on comparing statistical experiments. We then quantify epistemic discrimination as the gap between a model\u2019s accuracy given fairness constraints and the limit posed by aleatoric discrimination. We apply this approach to benchmark existing interventions and investigate fairness risks in data with missing values. Our results indicate that state-of-the-art fairness interventions are effective at removing epistemic discrimination. However, when data has missing values, there is still signi\ufb01cant room for improvement in handling aleatoric discrimination.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144219566",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2031135372",
                    "name": "Luxi He"
                },
                {
                    "authorId": "1381646041",
                    "name": "Rui Gao"
                },
                {
                    "authorId": "144717568",
                    "name": "F. Calmon"
                }
            ]
        },
        {
            "paperId": "55dd34dfc448bb09e598a8bddb3a3fd10d4b47d0",
            "title": "Optimal Multidimensional Differentially Private Mechanisms in the Large-Composition Regime",
            "abstract": "We construct vector differentially-private (DP) mechanisms that are asymptotically optimal in the limit of the number of compositions growing without bound. First, we derive via the central limit theorem a reduction from DP to a KL-divergence minimization problem. Second, we formulate the general theory of spherically-symmetric DP mechanisms in the large-composition regime. Specifically, we show that additive, continuous, spherically-symmetric DP mechanisms are optimal if one considers a spherically-symmetric cost (e.g., bounded noise variance) and an \u21132 sensitivity metric. We then formulate a finite-dimensional problem that produces noise distributions that can get arbitrarily close to optimal among monotone mechanisms. Finally, we demonstrate numerically that our proposed mechanism achieves better DP parameters than the vector Gaussian mechanism for the same variance constraint.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46976603",
                    "name": "Wael Alghamdi"
                },
                {
                    "authorId": "3085739",
                    "name": "S. Asoodeh"
                },
                {
                    "authorId": "144717568",
                    "name": "F. Calmon"
                },
                {
                    "authorId": "2233156852",
                    "name": "Juan Felipe Gomez"
                },
                {
                    "authorId": "1808206",
                    "name": "O. Kosut"
                },
                {
                    "authorId": "144711127",
                    "name": "L. Sankar"
                }
            ]
        },
        {
            "paperId": "703a4bec14310de4810ea55971ae1e5ad1e24fad",
            "title": "Arbitrariness Lies Beyond the Fairness-Accuracy Frontier",
            "abstract": "Machine learning tasks may admit multiple competing models that achieve similar performance yet produce conflicting outputs for individual samples -- a phenomenon known as predictive multiplicity. We demonstrate that fairness interventions in machine learning optimized solely for group fairness and accuracy can exacerbate predictive multiplicity. Consequently, state-of-the-art fairness interventions can mask high predictive multiplicity behind favorable group fairness and accuracy metrics. We argue that a third axis of ``arbitrariness'' should be considered when deploying models to aid decision-making in applications of individual-level impact. To address this challenge, we propose an ensemble algorithm applicable to any fairness intervention that provably ensures more consistent predictions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2220214891",
                    "name": "Carol Xuan Long"
                },
                {
                    "authorId": "2064834600",
                    "name": "Hsiang Hsu"
                },
                {
                    "authorId": "46976603",
                    "name": "Wael Alghamdi"
                },
                {
                    "authorId": "144717568",
                    "name": "F. Calmon"
                }
            ]
        },
        {
            "paperId": "7254d202b3b606af52b64769be1b77c30dba9729",
            "title": "Schr\u00f6dinger Mechanisms: Optimal Differential Privacy Mechanisms for Small Sensitivity",
            "abstract": "We consider the problem of designing optimal differential privacy mechanisms with a favorable privacy-utility tradeoff in the limit of a large number n of compositions (i.e., sequential queries). Here, utility is measured by the average distance between the mechanism's input and output, evaluated by a cost function c. We show that if n is sufficiently large and the sensitivities of all queries are small, then the optimal additive noise mechanism has probability density function fully characterized by the ground-state eigenfunction of the Schr\u00f6dinger operator with potential c. This leads to a family of optimal mechanisms, dubbed the Schr\u00f6dinger mechanisms, depending on the choice of the cost function. Instantiating this result, we demonstrate that for c(x) = x2 the Gaussian mechanism is optimal, and for c(x) = |x|, the optimal mechanism is obtained by the Airy function, thereby leading to the Airy mechanism.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46976603",
                    "name": "Wael Alghamdi"
                },
                {
                    "authorId": "3085739",
                    "name": "S. Asoodeh"
                },
                {
                    "authorId": "144717568",
                    "name": "F. Calmon"
                },
                {
                    "authorId": "2233156852",
                    "name": "Juan Felipe Gomez"
                },
                {
                    "authorId": "1808206",
                    "name": "O. Kosut"
                },
                {
                    "authorId": "144711127",
                    "name": "L. Sankar"
                }
            ]
        }
    ]
}