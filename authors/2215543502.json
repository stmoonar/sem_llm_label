{
    "authorId": "2215543502",
    "papers": [
        {
            "paperId": "fdd7056aa4886f1ba692f5b84d73e50eea8966b7",
            "title": "GAUSS: GrAph-customized Universal Self-Supervised Learning",
            "abstract": "To make Graph Neural Networks (GNNs) meet the requirements of the Web, the universality and the generalization become two important research directions. On one hand, many universal GNNs are presented for semi-supervised tasks on both homophilic and non-homophilic graphs by distinguishing homophilic and heterophilic edges with the help of labels. On the other hand, self-supervised learning (SSL) algorithms on graphs are presented by leveraging the self-supervised learning schemes from computer vision and natural language processing. Unfortunately, graph universal self-supervised learning remains resolved. Most existing SSL methods on graphs, which often employ two-layer GCN as the encoder and train the mapping functions, can't alter the low-passing filtering characteristic of GCN. Therefore, to be universal, SSL must becustomized for the graph, i.e., learning the graph. However, learning the graph via universal GNNs is disabled in SSL, since their distinguishability on homophilic and heterophilic edges disappears without the labels. To overcome this difficulty, this paper proposes novel GrAph-customized Universal Self-Supervised Learning (GAUSS) by exploiting local attribute distribution. The main idea is to replace the global parameters with locally learnable propagation. To make the propagation matrix demonstrate the affinity between the nodes, the self-representative learning framework is employed with k-block diagonal regularization. Extensive experiments on synthetic and real-world datasets demonstrate its effectiveness, universality and robustness to noises.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258970173",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "2301000149",
                    "name": "Weixiao Hu"
                },
                {
                    "authorId": "2300507511",
                    "name": "Jizhong Xu"
                },
                {
                    "authorId": "2215543502",
                    "name": "Runjie Shi"
                },
                {
                    "authorId": "2258959961",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2259851578",
                    "name": "Chuan Wang"
                },
                {
                    "authorId": "2259620725",
                    "name": "Xiaochun Cao"
                },
                {
                    "authorId": "2259628664",
                    "name": "Zhen Wang"
                },
                {
                    "authorId": "2661834",
                    "name": "Bingxin Niu"
                },
                {
                    "authorId": "2255826390",
                    "name": "Yuanfang Guo"
                }
            ]
        },
        {
            "paperId": "3eb3cb33d2c807b88c78c12d48041ffeba2ef096",
            "title": "Graph Neural Networks without Propagation",
            "abstract": "Due to the simplicity, intuition and explanation, most Graph Neural Networks (GNNs) are proposed by following the pipeline of message passing. Although they achieve superior performances in many tasks, propagation-based GNNs possess three essential drawbacks. Firstly, the propagation tends to produce smooth effect, which meets the inductive bias of homophily, and causes two serious issues: over-smoothing issue and performance drop on networks with heterophily. Secondly, the propagations to each node are irrelevant, which prevents GNNs from modeling high-order relation, and cause the GNNs fragile to the attributes noises. Thirdly, propagation-based GNNs may be fragile to topology noise, since they heavily relay on propagation over the topology. Therefore, the propagation, as the key component of most GNNs, may be the essence of some serious issues in GNNs. To get to the root of these issue, this paper attempts to replace the propagation with a novel local operation. Quantitative experimental analysis reveals: 1) the existence of low-rank characteristic in the node attributes from ego-networks and 2) the performance improvement by reducing its rank. Motivated by this finding, this paper propose the Low-Rank GNNs, whose key component is the low-rank attribute matrix approximation in ego-network. The graph topology is employed to construct the ego-networks instead of message propagation, which is sensitive to topology noises. The proposed Low-Rank GNNs posses some attractive characteristics, including robust to topology and attribute noises, parameter-free and parallelizable. Experimental evaluations demonstrate the superior performance, robustness to noises and universality of the proposed Low-Rank GNNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143921529",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "2215584803",
                    "name": "Qiuliang Zhang"
                },
                {
                    "authorId": "2215543502",
                    "name": "Runjie Shi"
                },
                {
                    "authorId": "2160390715",
                    "name": "Wenmiao Zhou"
                },
                {
                    "authorId": "2661834",
                    "name": "Bingxin Niu"
                },
                {
                    "authorId": "2109150586",
                    "name": "Chuan Wang"
                },
                {
                    "authorId": "2149214322",
                    "name": "Xiaochun Cao"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2188012894",
                    "name": "Zhen Wang"
                },
                {
                    "authorId": "2613860",
                    "name": "Yuanfang Guo"
                }
            ]
        },
        {
            "paperId": "6d35b28af908c1f6e2c9de2f30e12a9573480ddc",
            "title": "Self-supervised Graph Neural Networks via Low-Rank Decomposition",
            "abstract": "Self-supervised learning is introduced to train graph neural networks (GNNs) by employing propagation-based GNNs designed for semi-supervised learning tasks. Unfortunately, this common choice tends to cause two serious issues. Firstly, global parameters cause the model lack the ability to capture the local property. Secondly, it is dif\ufb01cult to handle networks beyond homophily without label information. This paper tends to break through the common choice of employing propagation-based GNNs, which aggregate representations of nodes belonging to different classes and tend to lose discriminative information. If the propagation in each ego-network is just between the nodes from the same class, the obtained representation matrix should follow the low-rank characteristic. To meet this requirement, this paper proposes the Low-Rank Decomposition-based GNNs (LRD-GNN-Matrix) by employing Low-Rank Decomposition to the attribute matrix. Furthermore, to incorporate long-distance information, Low-Rank Tensor Decomposition-based GNN (LRD-GNN-Tensor) is proposed by constructing the node attribute tensor from selected similar ego-networks and performing Low-Rank Tensor Decomposition. The employed tensor nuclear norm facilitates the capture of the long-distance relationship between original and selected similar ego-networks. Extensive experiments demonstrate the superior performance and the robustness of LRD-GNNs",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258970173",
                    "name": "Liang Yang"
                },
                {
                    "authorId": "2215543502",
                    "name": "Runjie Shi"
                },
                {
                    "authorId": "2215584803",
                    "name": "Qiuliang Zhang"
                },
                {
                    "authorId": "2661834",
                    "name": "Bingxin Niu"
                },
                {
                    "authorId": "2259628664",
                    "name": "Zhen Wang"
                },
                {
                    "authorId": "2259851578",
                    "name": "Chuan Wang"
                },
                {
                    "authorId": "2259620725",
                    "name": "Xiaochun Cao"
                }
            ]
        }
    ]
}