{
    "authorId": "1753450171",
    "papers": [
        {
            "paperId": "43b39f9a59ff67f6626fece3e7cb6c34045afc26",
            "title": "End-to-End Query Term Weighting",
            "abstract": "Bag-of-words based lexical retrieval systems are still the most commonly used methods for real-world search applications. Recently deep learning methods have shown promising results to improve this retrieval performance but are expensive to run in an online fashion, non-trivial to integrate into existing production systems, and might not generalize well in out-of-domain retrieval scenarios. Instead, we build on top of lexical retrievers by proposing a Term Weighting BERT (TW-BERT) model. TW-BERT learns to predict the weight for individual n-gram (e.g., uni-grams and bi-grams) query input terms. These inferred weights and terms can be used directly by a retrieval system to perform a query search. To optimize these term weights, TW-BERT incorporates the scoring function used by the search engine, such as BM25, to score query-document pairs. Given sample query-document pairs we can compute a ranking loss over these matching scores, optimizing the learned query term weights in an end-to-end fashion. Aligning TW-BERT with search engine scorers minimizes the changes needed to integrate it into existing production applications, whereas existing deep learning based search methods would require further infrastructure optimization and hardware requirements. The learned weights can be easily utilized by standard lexical retrievers and by other retrieval techniques such as query expansion. We show that TW-BERT improves retrieval performance over strong term weighting baselines within MSMARCO and in out-of-domain retrieval on TREC datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51133706",
                    "name": "Karan Samel"
                },
                {
                    "authorId": "1753450171",
                    "name": "Cheng Li"
                },
                {
                    "authorId": "40414862",
                    "name": "Weize Kong"
                },
                {
                    "authorId": "2118213045",
                    "name": "Tao Chen"
                },
                {
                    "authorId": "1791201",
                    "name": "Mingyang Zhang"
                },
                {
                    "authorId": "2174538749",
                    "name": "S. Gupta"
                },
                {
                    "authorId": "51149228",
                    "name": "Swaraj Khadanga"
                },
                {
                    "authorId": "2199186",
                    "name": "Wensong Xu"
                },
                {
                    "authorId": "90002561",
                    "name": "Xingyu Wang"
                },
                {
                    "authorId": "2839851",
                    "name": "K. Kolipaka"
                },
                {
                    "authorId": "1815447",
                    "name": "Michael Bendersky"
                },
                {
                    "authorId": "1398342639",
                    "name": "Marc Najork"
                }
            ]
        },
        {
            "paperId": "71f833d6d7cdfb49c611ef4e0bf1887afdd3d201",
            "title": "SparseEmbed: Learning Sparse Lexical Representations with Contextual Embeddings for Retrieval",
            "abstract": "In dense retrieval, prior work has largely improved retrieval effectiveness using multi-vector dense representations, exemplified by ColBERT. In sparse retrieval, more recent work, such as SPLADE, demonstrated that one can also learn sparse lexical representations to achieve comparable effectiveness while enjoying better interpretability. In this work, we combine the strengths of both the sparse and dense representations for first-stage retrieval. Specifically, we propose SparseEmbed - a novel retrieval model that learns sparse lexical representations with contextual embeddings. Compared with SPLADE, our model leverages the contextual embeddings to improve model expressiveness. Compared with ColBERT, our sparse representations are trained end-to-end to optimize both efficiency and effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40414862",
                    "name": "Weize Kong"
                },
                {
                    "authorId": "40511532",
                    "name": "Jeffrey M. Dudek"
                },
                {
                    "authorId": "1753450171",
                    "name": "Cheng Li"
                },
                {
                    "authorId": "1791201",
                    "name": "Mingyang Zhang"
                },
                {
                    "authorId": "1815447",
                    "name": "Michael Bendersky"
                }
            ]
        },
        {
            "paperId": "8859be9f22a3c7dd7f8df1846754242df946cd41",
            "title": "Job Type Extraction for Service Businesses",
            "abstract": "Google My Business (GMB) is a platform that hosts business profiles, which will be displayed when a user issues a relevant query on Google Search or Google Maps. GMB businesses provide a wide variety of services, from home cleaning and repair, to legal consultation. However, the exact details of the service provided (a.k.a. job types), are often missing in business profiles. This places the burden of finding these details on the users. To alleviate this burden, we built a pipeline to automatically extract the job types from business websites. We share the various challenges we faced while developing this pipeline, and how we effectively addressed these challenges by (1) utilizing structured content to tackle the cold start problem for dataset collection; (2) exploiting context information to improve model performance without hurting scalability; and (3) formulating the extraction problem as a retrieval task to improve both generalizability, efficiency, and coverage. The pipeline has been deployed for over a year and is scalable enough to be periodically refreshed. The extracted job types are serving users of Google Search and Google Maps, with significant improvements in both precision and coverage.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1753450171",
                    "name": "Cheng Li"
                },
                {
                    "authorId": "2093494906",
                    "name": "Yaping Qi"
                },
                {
                    "authorId": "92761946",
                    "name": "Hayk Zakaryan"
                },
                {
                    "authorId": "2215378624",
                    "name": "Mingyang Zhang"
                },
                {
                    "authorId": "1815447",
                    "name": "Michael Bendersky"
                },
                {
                    "authorId": "2216034937",
                    "name": "Yonghua Wu"
                },
                {
                    "authorId": "1398342639",
                    "name": "Marc Najork"
                }
            ]
        },
        {
            "paperId": "91206346edbe28abb606d7b3425cd455d4019d4f",
            "title": "Scaling Relationship on Learning Mathematical Reasoning with Large Language Models",
            "abstract": "Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also find RFT brings more improvement for less performant LLMs. Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\\% significantly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112340945",
                    "name": "Zheng Yuan"
                },
                {
                    "authorId": "2114128654",
                    "name": "Hongyi Yuan"
                },
                {
                    "authorId": "1753450171",
                    "name": "Cheng Li"
                },
                {
                    "authorId": "51490462",
                    "name": "Guanting Dong"
                },
                {
                    "authorId": "2111727840",
                    "name": "Chuanqi Tan"
                },
                {
                    "authorId": "2192678144",
                    "name": "Chang Zhou"
                }
            ]
        },
        {
            "paperId": "a6020d419c12b76eb1bd60983651c96cc00bf042",
            "title": "Graph Neural Networks for Tabular Data Learning",
            "abstract": "Deep learning-based approaches to Tabular Data Learning (TDL) have shown promising performance compared to their conventional counterparts. However, these methods often fail to account for the latent correlation among data instances and feature values. Recently, graph neural networks (GNNs) have gained attention across various application domains, including TDL, for their ability to model relations and interactions between different data entities. By creating appropriate graph structures from the input tabular data and employing GNNs for learning, the performance of TDL can be improved significantly. In this tutorial, we systematically introduce the methodologies of designing and applying GNNs to TDL. Our discussion covers the foundations and overview of GNN-based TDL methods, with a focus on formulating TDL as different graph structures. We also provide a comprehensive taxonomy of constructing graph structures and representation learning in GNN-based TDL methods. We describe the TDL model training framework, which includes different auxiliary tasks and supports open-world learning. Additionally, we discuss how to apply GNNs to various TDL application scenarios and tasks. Finally, we outline the limitations of current research and future directions for this field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1753450171",
                    "name": "Cheng Li"
                },
                {
                    "authorId": "1896151979",
                    "name": "Yu-Che Tsai"
                },
                {
                    "authorId": "2030126978",
                    "name": "Jay Chiehen Liao"
                }
            ]
        },
        {
            "paperId": "aff3f5b090260646aad8c5abc61770640446a714",
            "title": "DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification",
            "abstract": "Neural Architecture Search (NAS) has shown promising capability in learning text representation. However, existing text-based NAS neither performs a learnable fusion of neural operations to optimize the architecture nor encodes the latent hierarchical categorization behind text input. This article presents a novel NAS method, Discretized Differentiable Neural Architecture Search (DDNAS), for text representation learning and classification. With the continuous relaxation of architecture representation, DDNAS can use gradient descent to optimize the search. We also propose a novel discretization layer via mutual information maximization, which is imposed on every search node to model the latent hierarchical categorization in text representation. Extensive experiments conducted on eight diverse real datasets exhibit that DDNAS can consistently outperform the state-of-the-art NAS methods. While DDNAS relies on only three basic operations, i.e., convolution, pooling, and none, to be the candidates of NAS building blocks, its promising performance is noticeable and extensible to obtain further improvement by adding more different operations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145578365",
                    "name": "Kuan-Yu Chen"
                },
                {
                    "authorId": "1753450171",
                    "name": "Cheng Li"
                },
                {
                    "authorId": "2223158659",
                    "name": "Kuo-Jung Lee"
                }
            ]
        },
        {
            "paperId": "b775e8ce595a5ca7a5263fdadde071087e655c5b",
            "title": "TabGSL: Graph Structure Learning for Tabular Data Prediction",
            "abstract": "This work presents a novel approach to tabular data prediction leveraging graph structure learning and graph neural networks. Despite the prevalence of tabular data in real-world applications, traditional deep learning methods often overlook the potentially valuable associations between data instances. Such associations can offer beneficial insights for classification tasks, as instances may exhibit similar patterns of correlations among features and target labels. This information can be exploited by graph neural networks, necessitating robust graph structures. However, existing studies primarily focus on improving graph structure from noisy data, largely neglecting the possibility of deriving graph structures from tabular data. We present a novel solution, Tabular Graph Structure Learning (TabGSL), to enhance tabular data prediction by simultaneously learning instance correlation and feature interaction within a unified framework. This is achieved through a proposed graph contrastive learning module, along with transformer-based feature extractor and graph neural network. Comprehensive experiments conducted on 30 benchmark tabular datasets demonstrate that TabGSL markedly outperforms both tree-based models and recent deep learning-based tabular models. Visualizations of the learned instance embeddings further substantiate the effectiveness of TabGSL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2030126978",
                    "name": "Jay Chiehen Liao"
                },
                {
                    "authorId": "1753450171",
                    "name": "Cheng Li"
                }
            ]
        },
        {
            "paperId": "c9be75a4547331022fdb10d60f7ade2f0a108ffc",
            "title": "SUVR: A Search-Based Approach to Unsupervised Visual Representation Learning",
            "abstract": "Unsupervised learning has grown in popularity because of the difficulty of collecting annotated data and the development of modern frameworks that allow us to learn from unlabeled data. Existing studies, however, either disregard variations at different levels of similarity or only consider negative samples from one batch. We argue that image pairs should have varying degrees of similarity, and the negative samples should be allowed to be drawn from the entire dataset. In this work, we propose Search-based Unsupervised Visual Representation Learning (SUVR) to learn better image representations in an unsupervised manner. We first construct a graph from the image dataset by the similarity between images, and adopt the concept of graph traversal to explore positive samples. In the meantime, we make sure that negative samples can be drawn from the full dataset. Quantitative experiments on five benchmark image classification datasets demonstrate that SUVR can significantly outperform strong competing methods on unsupervised embedding learning. Qualitative experiments also show that SUVR can produce better representations in which similar images are clustered closer together than unrelated images in the latent space.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2274706526",
                    "name": "Yizhan Xu"
                },
                {
                    "authorId": "2109675427",
                    "name": "Chih-Yao Chen"
                },
                {
                    "authorId": "1753450171",
                    "name": "Cheng Li"
                }
            ]
        },
        {
            "paperId": "ea29fde45ef2b9b83dadd89d2fd5d7284021a92b",
            "title": "Teach LLMs to Personalize - An Approach inspired by Writing Education",
            "abstract": "Personalized text generation is an emerging research area that has attracted much attention in recent years. Most studies in this direction focus on a particular domain by designing bespoke features or models. In this work, we propose a general approach for personalized text generation using large language models (LLMs). Inspired by the practice of writing education, we develop a multistage and multitask framework to teach LLMs for personalized generation. In writing instruction, the task of writing from sources is often decomposed into multiple steps that involve finding, evaluating, summarizing, synthesizing, and integrating information. Analogously, our approach to personalized text generation consists of multiple stages: retrieval, ranking, summarization, synthesis, and generation. In addition, we introduce a multitask setting that helps the model improve its generation ability further, which is inspired by the observation in education that a student's reading proficiency and writing ability are often correlated. We evaluate our approach on three public datasets, each of which covers a different and representative domain. Our results show significant improvements over a variety of baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1753450171",
                    "name": "Cheng Li"
                },
                {
                    "authorId": "1791201",
                    "name": "Mingyang Zhang"
                },
                {
                    "authorId": "1743469",
                    "name": "Qiaozhu Mei"
                },
                {
                    "authorId": "49416601",
                    "name": "Yaqing Wang"
                },
                {
                    "authorId": "2078501964",
                    "name": "Spurthi Amba Hombaiah"
                },
                {
                    "authorId": "2119123595",
                    "name": "Yi Liang"
                },
                {
                    "authorId": "1815447",
                    "name": "Michael Bendersky"
                }
            ]
        },
        {
            "paperId": "f40a60a240b6026266db73a432db2724693c14af",
            "title": "Unsupervised Paraphrasing under Syntax Knowledge",
            "abstract": "The soundness of syntax is an important issue for the paraphrase generation task. \nMost methods control the syntax of paraphrases by embedding the syntax and semantics in the generation process, which cannot guarantee the syntactical correctness of the results. \nDifferent from them, in this paper we investigate the structural patterns of word usages termed as the word composable knowledge and integrate it into the paraphrase generation to control the syntax in an explicit way.\nThis syntax knowledge is pretrained on a large corpus with the dependency relationships and formed as the probabilistic functions on the word-level syntactical soundness.\nFor the sentence-level correctness, we design a hierarchical syntax structure loss to quantitatively verify the syntactical soundness of the paraphrase against the given dependency template. \nThus, the generation process can select the appropriate words with consideration on both semantics and syntax. \nThe proposed method is evaluated on a few paraphrase datasets.\nThe experimental results show that the quality of paraphrases by our proposed method outperforms the compared methods, especially in terms of syntax correctness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151480473",
                    "name": "Tianyuan Liu"
                },
                {
                    "authorId": "47366519",
                    "name": "Yuqing Sun"
                },
                {
                    "authorId": "3306377",
                    "name": "JiaQi Wu"
                },
                {
                    "authorId": "2112429764",
                    "name": "Xi Xu"
                },
                {
                    "authorId": "2152494723",
                    "name": "Yuchen Han"
                },
                {
                    "authorId": "1753450171",
                    "name": "Cheng Li"
                },
                {
                    "authorId": "2298047366",
                    "name": "Bin Gong"
                }
            ]
        }
    ]
}