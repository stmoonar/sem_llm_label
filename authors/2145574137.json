{
    "authorId": "2145574137",
    "papers": [
        {
            "paperId": "0549fbdd35a010ae1b8eebc18975144a35d45e1c",
            "title": "Asd-Transformer: Efficient Active Speaker Detection Using Self And Multimodal Transformers",
            "abstract": "Multimodal active speaker detection (ASD) methods assign a speaking/not-speaking label per individual in a video clip. ASD is critical for applications such as natural human-computer interaction, speaker diarization, and video reframing. Recent work has shown the success of transformers in multimodal settings, thus we propose a novel framework that leverages modern transformer and concatenation mechanisms to efficiently capture the interaction between audio and video modalities for ASD. We achieve mAP similar to state-of-the-art (93.0% vs 93.5%) on the AVA-ActiveSpeaker dataset. Further, our model has ~3\u00d7 smaller size (15.23MB vs 49.82MB), reduced FLOPs count (11.8 vs 14.3), and lower training time (15h vs 38h). To verify our model is making predictions from the right visual cues, we computed saliency maps over input images. We found that in addition to mouth regions, the nose, cheek, and area under the eye were helpful in identifying active speakers. Our ablation study reveals that the mouth region alone achieved lower mAP (91.9% vs 93.0%) compared to full face region, supporting our hypothesis that facial expressions in addition to mouth region are useful for ASD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1382640493",
                    "name": "G. Datta"
                },
                {
                    "authorId": "51207767",
                    "name": "Tyler Etchart"
                },
                {
                    "authorId": "2145574137",
                    "name": "Vivek Yadav"
                },
                {
                    "authorId": "3236027",
                    "name": "Varsha Hedau"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "2122374530",
                    "name": "Shih-Fu Chang"
                }
            ]
        },
        {
            "paperId": "e1d53567b15d50cc1f58729083cbff0460cd395b",
            "title": "Enhancing Fairness in Face Detection in Computer Vision Systems by Demographic Bias Mitigation",
            "abstract": "Fairness has become an important agenda in computer vision and artificial intelligence. Recent studies have shown that many computer vision models and datasets exhibit demographic biases and proposed mitigation strategies. These works attempt to address accuracy disparity, spurious correlations, or unbalanced representations in datasets in tasks such as face recognition, verification and expression and attribute classification. These tasks, however, all require face detection as the first preprocessing step, and surprisingly, there has been little effort in identifying or mitigating biases in face detection. Biased face detectors themselves pose a threat against fair and ethical AI systems, and their biases may be further passed on to subsequent downstream tasks such as face recognition in a computer vision pipeline. This paper therefore investigates the problem of biases in face detection, focusing on accuracy disparity of detectors between demographic groups including gender, age group, and skin tone. We collect perceived demographic attributes on a popular face detection benchmark dataset, WIDER FACE, report skewed demographic distributions, and compare detection performance between groups. In order to mitigate the biases, we apply three mitigation methods that have been introduced in the recent literature and also propose two novel methods. Experimental results show that these methods are effective in reducing demographic biases. We also discuss how the effectiveness varies by demographic attributes, detection easiness, and multiple detectors, which will shed light on this new topic of addressing face detection bias.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116468252",
                    "name": "Yu Yang"
                },
                {
                    "authorId": "50178621",
                    "name": "Aayush Gupta"
                },
                {
                    "authorId": "1384556269",
                    "name": "Jianfeng Feng"
                },
                {
                    "authorId": "3222821",
                    "name": "Prateek Singhal"
                },
                {
                    "authorId": "2145574137",
                    "name": "Vivek Yadav"
                },
                {
                    "authorId": "2109035901",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "3236027",
                    "name": "Varsha Hedau"
                },
                {
                    "authorId": "1834047",
                    "name": "Jungseock Joo"
                }
            ]
        },
        {
            "paperId": "6cee833a5cceb821d998e0954503dbb607499dde",
            "title": "SGDOL: Self-evolving Generative and Discriminative Online Learning for Data Stream Classification",
            "abstract": "Data streams are usually non-stationary obtained from the same or different data sources. It needs to be processed sequentially, hence termed stream processing. Stream processing often demands evolving neural network architectures that can alter the number of nodes and layers on-demand to classify data streams in an online manner, known as evolving online learning. Traditional deep neural networks (DNNs) uses batch data processing, often limited by their static network structures and offline learning approaches while addressing data streams. In this work, we propose a novel evolving deep neural network framework, known as Self-evolving Generative and Discriminative Online Learning (SGDOL), which utilises an online learning approach to evolve both generator and discriminator network structure from scratch, and on-demand to classify data streams. The dynamic feature learning mechanism of autoencoder-based generative models have demonstrated its potential in learning latent feature representations from data streams. These latent features are fed to the evolving feed-forward DNN-based discriminator as input. The mechanism of adding or pruning nodes in the evolving architecture of discriminator supports in dealing with catastrophic forgetting problems; a new layer is added to the discriminator when a new concept appears in the data stream. To back these theoretical contributions of SGDOL, experiments were conducted using nine benchmark datasets and compared with ten state-of-the-art online learning algorithms. SGDOL performance measure of testing classification rates was better in seven datasets out of nine than the existing algorithms, which clearly indicates its ability to deal with the data stream.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053049239",
                    "name": "Deeksha Aggarwal"
                },
                {
                    "authorId": "1871629",
                    "name": "J. Senthilnath"
                },
                {
                    "authorId": "2058042792",
                    "name": "U. Kumar"
                },
                {
                    "authorId": "2145574137",
                    "name": "Vivek Yadav"
                },
                {
                    "authorId": "34727171",
                    "name": "Sushant Kulkarni"
                },
                {
                    "authorId": "32588693",
                    "name": "Md Meftahul Ferdaus"
                },
                {
                    "authorId": "66673843",
                    "name": "Liao Xiaoli"
                }
            ]
        },
        {
            "paperId": "eab8335cd22afe6fa6732ace47132046336c4504",
            "title": "Adversarial Mask Generation for Preserving Visual Privacy",
            "abstract": "We present a privacy preserving machine learning method for images that separates task-relevant information from task-irrelevant information. Our primary hypothesis is that by revealing the minimal number of pixels required for a task we can provide the most privacy preserving guarantees. Specifically, we propose an adversarial method that masks out task-irrelevant information from an image for preserving privacy. The proposed method only uses task-specific label information and no privacy annotations such as identity of the subject, gender, race, etc., are required. We validate the proposed method on face attribute prediction on the CelebA dataset and emotion recognition on the FER+ dataset, showing that we can preserve visual privacy with little degradation in the task performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50178621",
                    "name": "Aayush Gupta"
                },
                {
                    "authorId": "26393556",
                    "name": "Ayush Jaiswal"
                },
                {
                    "authorId": "1887625",
                    "name": "Yuehua Wu"
                },
                {
                    "authorId": "2145574137",
                    "name": "Vivek Yadav"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                }
            ]
        }
    ]
}