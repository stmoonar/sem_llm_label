{
    "authorId": "1443779643",
    "papers": [
        {
            "paperId": "6b1c1bafe67521f0fddd8ffb73700c88978e0a44",
            "title": "Knowledge Graph Empowered Machine Learning Pipelines for Improved Efficiency, Reusability, and Explainability",
            "abstract": "Artificial intelligence (AI) pipelines are complex, heavily parameterized, and expensive to execute in terms of time and computational resources. Consequently, it is onerous to run experiments with all possible parameter combinations to achieve an optimal solution. However, these AI experiments can be optimized by recommending relevant parameters to commence the experiments, reducing search space significantly, which can be fine tuned further. The relevant parameters can be identified by observing the metadata of pipelines executed in the past, and the relevant pipeline with relevant parameters can be recommended to the user. Currently, there are various metadata frameworks that automatically record the metadata of AI pipelines. Developing a recommendation system requires understanding pipeline metadata components and their interactions. There is a need to represent the metadata generated by these AI pipelines that capture the relationship among these pipeline entities. This article presents a knowledge-infused recommender that utilizes prior knowledge and metadata of already executed pipelines represented using the proposed metadata schema to recommend a relevant pipeline per user queries. Unlike black-box models, the use of knowledge graphs makes recommendations explainable, improving transparency and trustworthiness for the users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51180669",
                    "name": "R. Venkataramanan"
                },
                {
                    "authorId": "2643562",
                    "name": "Aalap Tripathy"
                },
                {
                    "authorId": "1443779643",
                    "name": "M. Foltin"
                },
                {
                    "authorId": "35655815",
                    "name": "H. Y. Yip"
                },
                {
                    "authorId": "2205970689",
                    "name": "Annmary Justine"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                },
                {
                    "authorId": "2064342729",
                    "name": "Amit P. Sheth"
                }
            ]
        },
        {
            "paperId": "6c57ce08546a5f996d9c487522bd9ba1d67c9eac",
            "title": "Dataset Efficient Training with Model Ensembling",
            "abstract": "We propose a dataset-efficient deep learning training method by ensembling multiple models trained on different subsets. The ensembling method leverages the difficulty level of data samples to select subsets that are representative and diverse. The approach involves building a common base model with a random subset of data and then allotting different subsets to the models in an ensemble. The models are trained with their own subsets and then merged into a single model. We design an multi-phase training strategy that aggregates models in the ensemble more frequently and prevents divergence. The experiments on ResNet18 and ImageNet show that ensembling outperforms the no-ensemble case and achieves 64.8% accuracy with only 30% of dataset, saving 20 hours of training time in a single V100 GPU training experiment with a mild accuracy drop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2462957",
                    "name": "Yeonju Ro"
                },
                {
                    "authorId": "2110087444",
                    "name": "Cong Xu"
                },
                {
                    "authorId": "51119838",
                    "name": "Agnieszka Ciborowska"
                },
                {
                    "authorId": "46508637",
                    "name": "S. Bhattacharya"
                },
                {
                    "authorId": "2228136830",
                    "name": "Frankie Li"
                },
                {
                    "authorId": "1443779643",
                    "name": "M. Foltin"
                }
            ]
        },
        {
            "paperId": "794ba58adbce4d1630bbf3e3001347c2643f9a65",
            "title": "Workflows Community Summit 2022: A Roadmap Revolution",
            "abstract": "Scientific workflows have become integral tools in broad scientific computing use cases. Science discovery is increasingly dependent on workflows to orchestrate large and complex scientific experiments that range from execution of a cloud-based data preprocessing pipeline to multi-facility instrument-to-edge-to-HPC computational workflows. Given the changing landscape of scientific computing and the evolving needs of emerging scientific applications, it is paramount that the development of novel scientific workflows and system functionalities seek to increase the efficiency, resilience, and pervasiveness of existing systems and applications. Specifically, the proliferation of machine learning/artificial intelligence (ML/AI) workflows, need for processing large scale datasets produced by instruments at the edge, intensification of near real-time data processing, support for long-term experiment campaigns, and emergence of quantum computing as an adjunct to HPC, have significantly changed the functional and operational requirements of workflow systems. Workflow systems now need to, for example, support data streams from the edge-to-cloud-to-HPC enable the management of many small-sized files, allow data reduction while ensuring high accuracy, orchestrate distributed services (workflows, instruments, data movement, provenance, publication, etc.) across computing and user facilities, among others. Further, to accelerate science, it is also necessary that these systems implement specifications/standards and APIs for seamless (horizontal and vertical) integration between systems and applications, as well as enabling the publication of workflows and their associated products according to the FAIR principles. This document reports on discussions and findings from the 2022 international edition of the Workflows Community Summit that took place on November 29 and 30, 2022.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2140039842",
                    "name": "Rafael Ferreira da Silva"
                },
                {
                    "authorId": "1799983",
                    "name": "Rosa M. Badia"
                },
                {
                    "authorId": "2213299279",
                    "name": "Venkat Bala"
                },
                {
                    "authorId": "152409031",
                    "name": "Deborah Bard"
                },
                {
                    "authorId": "145466013",
                    "name": "P. Bremer"
                },
                {
                    "authorId": "2213305232",
                    "name": "Ian Buckley"
                },
                {
                    "authorId": "2304561021",
                    "name": "Silvina Ca\u00edno-Lores"
                },
                {
                    "authorId": "3091414",
                    "name": "K. Chard"
                },
                {
                    "authorId": "46555127",
                    "name": "C. Goble"
                },
                {
                    "authorId": "1693678",
                    "name": "S. Jha"
                },
                {
                    "authorId": "2059296518",
                    "name": "D. Katz"
                },
                {
                    "authorId": "33947553",
                    "name": "D. Laney"
                },
                {
                    "authorId": "145203664",
                    "name": "M. Parashar"
                },
                {
                    "authorId": "35531371",
                    "name": "F. Suter"
                },
                {
                    "authorId": "50329328",
                    "name": "N. Tyler"
                },
                {
                    "authorId": "2066071",
                    "name": "T. Uram"
                },
                {
                    "authorId": "1729053",
                    "name": "I. Altintas"
                },
                {
                    "authorId": "121299213",
                    "name": "S. Andersson"
                },
                {
                    "authorId": "144110414",
                    "name": "W. Arndt"
                },
                {
                    "authorId": "2057561277",
                    "name": "J. Aznar"
                },
                {
                    "authorId": "2062906007",
                    "name": "Jonathan Bader"
                },
                {
                    "authorId": "1960359",
                    "name": "B. Bali\u015b"
                },
                {
                    "authorId": "108377253",
                    "name": "Chris E. Blanton"
                },
                {
                    "authorId": "2199664",
                    "name": "K. Braghetto"
                },
                {
                    "authorId": "2651403",
                    "name": "Aharon Brodutch"
                },
                {
                    "authorId": "2213427261",
                    "name": "Paul Brunk"
                },
                {
                    "authorId": "1707417",
                    "name": "H. Casanova"
                },
                {
                    "authorId": "104449865",
                    "name": "Alba Cervera Lierta"
                },
                {
                    "authorId": "2213298896",
                    "name": "Justin Chigu"
                },
                {
                    "authorId": "1572245697",
                    "name": "T. Coleman"
                },
                {
                    "authorId": "2065988217",
                    "name": "Nick Collier"
                },
                {
                    "authorId": "2120657141",
                    "name": "Iacopo Colonnelli"
                },
                {
                    "authorId": "144108029",
                    "name": "Frederik Coppens"
                },
                {
                    "authorId": "144361904",
                    "name": "M. Crusoe"
                },
                {
                    "authorId": "2058475897",
                    "name": "W. Cunningham"
                },
                {
                    "authorId": "3362335",
                    "name": "B. Kinoshita"
                },
                {
                    "authorId": "3062856",
                    "name": "Paolo Di Tommaso"
                },
                {
                    "authorId": "2802678",
                    "name": "C. Doutriaux"
                },
                {
                    "authorId": "143916458",
                    "name": "M. Downton"
                },
                {
                    "authorId": "1799299",
                    "name": "W. Elwasif"
                },
                {
                    "authorId": "144504471",
                    "name": "B. Enders"
                },
                {
                    "authorId": "17606192",
                    "name": "Chris Erdmann"
                },
                {
                    "authorId": "1719226",
                    "name": "T. Fahringer"
                },
                {
                    "authorId": "84336109",
                    "name": "Ludmilla Figueiredo"
                },
                {
                    "authorId": "1698908",
                    "name": "Rosa Filgueira"
                },
                {
                    "authorId": "1443779643",
                    "name": "M. Foltin"
                },
                {
                    "authorId": "2850787",
                    "name": "A. Fouilloux"
                },
                {
                    "authorId": "2811678",
                    "name": "Luiz M. R. Gadelha"
                },
                {
                    "authorId": "2053600243",
                    "name": "Andrew Gallo"
                },
                {
                    "authorId": "2079451840",
                    "name": "A. G. Saez"
                },
                {
                    "authorId": "1398926410",
                    "name": "D. Garijo"
                },
                {
                    "authorId": "2027553659",
                    "name": "R. Gerlach"
                },
                {
                    "authorId": "2072840094",
                    "name": "Ryan E. Grant"
                },
                {
                    "authorId": "51124744",
                    "name": "Samuel Grayson"
                },
                {
                    "authorId": "2923900",
                    "name": "Patricia A. Grubel"
                },
                {
                    "authorId": "48985473",
                    "name": "Johan O. R. Gustafsson"
                },
                {
                    "authorId": "2308097253",
                    "name": "Val\u00e9rie Hayot-Sasson"
                },
                {
                    "authorId": "2053161325",
                    "name": "Oscar R. Hernandez"
                },
                {
                    "authorId": "1967862",
                    "name": "Marcus Hilbrich"
                },
                {
                    "authorId": "2205970689",
                    "name": "Annmary Justine"
                },
                {
                    "authorId": "152142617",
                    "name": "I. Laflotte"
                },
                {
                    "authorId": "151494370",
                    "name": "Fabian Lehmann"
                },
                {
                    "authorId": "50631038",
                    "name": "Andr\u00e9 Luckow"
                },
                {
                    "authorId": "2086648690",
                    "name": "Jakob Luettgau"
                },
                {
                    "authorId": "145341601",
                    "name": "K. Maheshwari"
                },
                {
                    "authorId": "3299313",
                    "name": "Motohiko Matsuda"
                },
                {
                    "authorId": "3421209",
                    "name": "Doriana Medic"
                },
                {
                    "authorId": "102153641",
                    "name": "P. Mendygral"
                },
                {
                    "authorId": "32733424",
                    "name": "M. Michalewicz"
                },
                {
                    "authorId": "2489735",
                    "name": "J. Nonaka"
                },
                {
                    "authorId": "40258733",
                    "name": "Maciej Pawlik"
                },
                {
                    "authorId": "34635073",
                    "name": "L. Pottier"
                },
                {
                    "authorId": "32809071",
                    "name": "Line C. Pouchard"
                },
                {
                    "authorId": "2213308235",
                    "name": "Mathias Putz"
                },
                {
                    "authorId": "46254996",
                    "name": "Santosh Kumar Radha"
                },
                {
                    "authorId": "1792683",
                    "name": "L. Ramakrishnan"
                },
                {
                    "authorId": "3445802",
                    "name": "S. Ristov"
                },
                {
                    "authorId": "1961788",
                    "name": "P. Romano"
                },
                {
                    "authorId": "21168032",
                    "name": "Daniel Rosendo"
                },
                {
                    "authorId": "3493555",
                    "name": "M. Ruefenacht"
                },
                {
                    "authorId": "2771040",
                    "name": "Katarzyna Rycerz"
                },
                {
                    "authorId": "2653506",
                    "name": "Nishant Saurabh"
                },
                {
                    "authorId": "144276915",
                    "name": "V. Savchenko"
                },
                {
                    "authorId": "2057456497",
                    "name": "Martin Schulz"
                },
                {
                    "authorId": "2037776232",
                    "name": "C. Simpson"
                },
                {
                    "authorId": "66188222",
                    "name": "R. Sirvent"
                },
                {
                    "authorId": "8884013",
                    "name": "Tyler J. Skluzacek"
                },
                {
                    "authorId": "1399487720",
                    "name": "S. Soiland-Reyes"
                },
                {
                    "authorId": "144712722",
                    "name": "Renan Souza"
                },
                {
                    "authorId": "1750800",
                    "name": "S. Sukumar"
                },
                {
                    "authorId": "48064827",
                    "name": "Ziheng Sun"
                },
                {
                    "authorId": "1746329",
                    "name": "A. Sussman"
                },
                {
                    "authorId": "1686699",
                    "name": "D. Thain"
                },
                {
                    "authorId": "2072191295",
                    "name": "Mikhail Titov"
                },
                {
                    "authorId": "50883695",
                    "name": "Benjam\u00edn Tovar"
                },
                {
                    "authorId": "2643562",
                    "name": "Aalap Tripathy"
                },
                {
                    "authorId": "1840916",
                    "name": "M. Turilli"
                },
                {
                    "authorId": "2165288240",
                    "name": "Bartosz Tuznik"
                },
                {
                    "authorId": "1942636",
                    "name": "H. V. Dam"
                },
                {
                    "authorId": "2006915860",
                    "name": "Aurelio Vivas"
                },
                {
                    "authorId": "47766095",
                    "name": "Logan T. Ward"
                },
                {
                    "authorId": "2946172",
                    "name": "Patrick M. Widener"
                },
                {
                    "authorId": "153569593",
                    "name": "Sean R. Wilkinson"
                },
                {
                    "authorId": "2169430585",
                    "name": "Justyna Zawalska"
                },
                {
                    "authorId": "2065914488",
                    "name": "M. Zulfiqar"
                }
            ]
        },
        {
            "paperId": "d5d981ac80c0280b2205d15706ec5e292ae93ccc",
            "title": "X-TIME: An in-memory engine for accelerating machine learning on tabular data with CAMs",
            "abstract": "Structured, or tabular, data is the most common format in data science. While deep learning models have proven formidable in learning from unstructured data such as images or speech, they are less accurate than simpler approaches when learning from tabular data. In contrast, modern tree-based Machine Learning (ML) models shine in extracting relevant information from structured data. An essential requirement in data science is to reduce model inference latency in cases where, for example, models are used in a closed loop with simulation to accelerate scientific discovery. However, the hardware acceleration community has mostly focused on deep neural networks and largely ignored other forms of machine learning. Previous work has described the use of an analog content addressable memory (CAM) component for efficiently mapping random forests. In this work, we focus on an overall analog-digital architecture implementing a novel increased precision analog CAM and a programmable network on chip allowing the inference of state-of-the-art tree-based ML models, such as XGBoost and CatBoost. Results evaluated in a single chip at 16nm technology show 119x lower latency at 9740x higher throughput compared with a state-of-the-art GPU, with a 19W peak power consumption.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47404896",
                    "name": "G. Pedretti"
                },
                {
                    "authorId": "2092597822",
                    "name": "John Moon"
                },
                {
                    "authorId": "35479075",
                    "name": "P. Bruel"
                },
                {
                    "authorId": "2400013",
                    "name": "S. Serebryakov"
                },
                {
                    "authorId": "2213392808",
                    "name": "Ron M. Roth"
                },
                {
                    "authorId": "51904636",
                    "name": "L. Buonanno"
                },
                {
                    "authorId": "46533048",
                    "name": "Tobias Ziegler"
                },
                {
                    "authorId": "2110086543",
                    "name": "Cong Xu"
                },
                {
                    "authorId": "1443779643",
                    "name": "M. Foltin"
                },
                {
                    "authorId": "1789618",
                    "name": "P. Faraboschi"
                },
                {
                    "authorId": "1742183159",
                    "name": "Jim Ignowski"
                },
                {
                    "authorId": "145961504",
                    "name": "Catherine E. Graves"
                }
            ]
        },
        {
            "paperId": "f3d0e3c342b0751fb3d1d6b725a5dcc3551b7343",
            "title": "Data-Aware Storage Tiering for Deep Learning",
            "abstract": "DNN models trained with very large datasets can perform rich deep learning tasks with high accuracy. However, feeding huge volumes of training data exerts significant pressure on IO subsystems as the entire data is re-loaded in random order on every iteration to enable convergence, with very little scope for reuse. To address this challenge, we co-optimize data tiering and iteration in DNN training for any given dataset and model with bandwidth and convergence conscious mini-epoch training (MET). This approach can substantially reduce the IO bandwidth required to provide sustained read throughput matching the processing speed of accelerators. Further, we introduce two different feedback mechanisms to adjust the repeating factor over each mini-epoch during the training. We have evaluated three different applications with MET. Most of them work outof-box with modest MET parameters. The adaptive repeating factor design was able to gain back most of the accuracy drop due lo large MET parameters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110086543",
                    "name": "Cong Xu"
                },
                {
                    "authorId": "2153402358",
                    "name": "S. Bhattacharya"
                },
                {
                    "authorId": "1443779643",
                    "name": "M. Foltin"
                },
                {
                    "authorId": "1749233",
                    "name": "S. Byna"
                },
                {
                    "authorId": "1789618",
                    "name": "P. Faraboschi"
                }
            ]
        },
        {
            "paperId": "02c0cd8886765463de4d7261bfbef1f4ccc20cb4",
            "title": "Memristor TCAMs Accelerate Regular Expression Matching for Network Intrusion Detection",
            "abstract": "We propose memristor-based TCAMs (Ternary Content Addressable Memory) circuits to accelerate Regular Expression (RegEx) matching through in memory processing of finite automata. RegEx matching is a key function in network security to find malicious actors. However, RegEx matching latency and power can be incredibly high and current proposals are challenged to perform wire-speed matching for large rulesets. Our approach dramatically decreases operating power, enables high throughput, and the use of nanoscale memristor TCAM circuits (mTCAMs) enables compression techniques to expand rulesets. We fabricated and demonstrated nanoscale memristor TCAM cells. SPICE simulations investigate performance at scale and a mTCAM dynamic power model using 16\u00a0nm layout parameters demonstrates <inline-formula><tex-math notation=\"LaTeX\">$\\sim$</tex-math></inline-formula>0.2\u00a0fJ/bit/search energy for a 36\u00a0\u00d7\u00a0250 mTCAM array. A tiled architecture is proposed to implement a Snort ruleset and assess application performance. Compared to a state-of-the-art FPGA approach (2\u00a0Gbps, <inline-formula><tex-math notation=\"LaTeX\">$\\sim$</tex-math></inline-formula>1\u00a0W), we show <inline-formula><tex-math notation=\"LaTeX\">$\\times 4$</tex-math></inline-formula> throughput (8 Gbps) at 55<inline-formula><tex-math notation=\"LaTeX\">$\\%$</tex-math></inline-formula> the power (0.55\u00a0W) without standard TCAM power-saving techniques. Our performance comparison improves further when striding (searching multiple characters at once) is considered, resulting in 47.2\u00a0Gbps at 1.2\u00a0W for our approach compared to 3.9\u00a0Gbps at 630\u00a0mW for strided FPGA NFA, demonstrating a promising path to wire-speed RegEx matching on large scale rulesets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145961504",
                    "name": "Catherine E. Graves"
                },
                {
                    "authorId": "2047503990",
                    "name": "Can Li"
                },
                {
                    "authorId": "71872160",
                    "name": "X. Sheng"
                },
                {
                    "authorId": "2111665710",
                    "name": "W. Ma"
                },
                {
                    "authorId": "1792792",
                    "name": "S. R. Chalamalasetti"
                },
                {
                    "authorId": "95838618",
                    "name": "Darrin Miller"
                },
                {
                    "authorId": "1742183159",
                    "name": "Jim Ignowski"
                },
                {
                    "authorId": "52306749",
                    "name": "B. Buchanan"
                },
                {
                    "authorId": "2118103124",
                    "name": "Le Zheng"
                },
                {
                    "authorId": "3405899",
                    "name": "Sity Lam"
                },
                {
                    "authorId": "2108274161",
                    "name": "Xuema Li"
                },
                {
                    "authorId": "2193958",
                    "name": "Lennie Kiyama"
                },
                {
                    "authorId": "1443779643",
                    "name": "M. Foltin"
                },
                {
                    "authorId": "34632500",
                    "name": "Matthew P. Hardy"
                },
                {
                    "authorId": "33102961",
                    "name": "J. Strachan"
                }
            ]
        },
        {
            "paperId": "67a586e80d9f29519d6218fa03355579f0d88230",
            "title": "PANTHER: A Programmable Architecture for Neural Network Training Harnessing Energy-Efficient ReRAM",
            "abstract": "The wide adoption of deep neural networks has been accompanied by ever-increasing energy and performance demands due to the expensive nature of training them. Numerous special-purpose architectures have been proposed to accelerate training: both digital and hybrid digital-analog using resistive RAM (ReRAM) crossbars. ReRAM-based accelerators have demonstrated the effectiveness of ReRAM crossbars at performing matrix-vector multiplication operations that are prevalent in training. However, they still suffer from inefficiency due to the use of serial reads and writes for performing the weight gradient and update step. A few works have demonstrated the possibility of performing outer products in crossbars, which can be used to realize the weight gradient and update step without the use of serial reads and writes. However, these works have been limited to low precision operations which are not sufficient for typical training workloads. Moreover, they have been confined to a limited set of training algorithms for fully-connected layers only. To address these limitations, we propose a bit-slicing technique for enhancing the precision of ReRAM-based outer products, which is substantially different from bit-slicing for matrix-vector multiplication only. We incorporate this technique into a crossbar architecture with three variants catered to different training algorithms. To evaluate our design on different types of layers in neural networks (fully-connected, convolutional, etc.) and training algorithms, we develop PANTHER, an ISA-programmable training accelerator with compiler support. Our design can also be integrated into other accelerators in the literature to enhance their efficiency. Our evaluation shows that PANTHER achieves up to <inline-formula><tex-math notation=\"LaTeX\">$8.02\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>8</mml:mn><mml:mo>.</mml:mo><mml:mn>02</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ankit-ieq1-2998456.gif\"/></alternatives></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$54.21\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>54</mml:mn><mml:mo>.</mml:mo><mml:mn>21</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ankit-ieq2-2998456.gif\"/></alternatives></inline-formula>, and <inline-formula><tex-math notation=\"LaTeX\">$103\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>103</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ankit-ieq3-2998456.gif\"/></alternatives></inline-formula> energy reductions as well as <inline-formula><tex-math notation=\"LaTeX\">$7.16\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>7</mml:mn><mml:mo>.</mml:mo><mml:mn>16</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ankit-ieq4-2998456.gif\"/></alternatives></inline-formula>, <inline-formula><tex-math notation=\"LaTeX\">$4.02\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>4</mml:mn><mml:mo>.</mml:mo><mml:mn>02</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ankit-ieq5-2998456.gif\"/></alternatives></inline-formula>, and <inline-formula><tex-math notation=\"LaTeX\">$16\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>16</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"ankit-ieq6-2998456.gif\"/></alternatives></inline-formula> execution time reductions compared to digital accelerators, ReRAM-based accelerators, and GPUs, respectively.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "3468125",
                    "name": "Aayush Ankit"
                },
                {
                    "authorId": "3077499",
                    "name": "I. E. Hajj"
                },
                {
                    "authorId": "1792792",
                    "name": "S. R. Chalamalasetti"
                },
                {
                    "authorId": "3491847",
                    "name": "S. Agarwal"
                },
                {
                    "authorId": "30782065",
                    "name": "M. Marinella"
                },
                {
                    "authorId": "1443779643",
                    "name": "M. Foltin"
                },
                {
                    "authorId": "33102961",
                    "name": "J. Strachan"
                },
                {
                    "authorId": "1712405",
                    "name": "D. Milojicic"
                },
                {
                    "authorId": "143668320",
                    "name": "Wen-mei W. Hwu"
                },
                {
                    "authorId": "2257216834",
                    "name": "K. Roy"
                }
            ]
        },
        {
            "paperId": "d74bdbc0bce8ff90e815c74368cdac49b0eb4185",
            "title": "PUMA: A Programmable Ultra-efficient Memristor-based Accelerator for Machine Learning Inference",
            "abstract": "Memristor crossbars are circuits capable of performing analog matrix-vector multiplications, overcoming the fundamental energy efficiency limitations of digital logic. They have been shown to be effective in special-purpose accelerators for a limited set of neural network applications. We present the Programmable Ultra-efficient Memristor-based Accelerator (PUMA) which enhances memristor crossbars with general purpose execution units to enable the acceleration of a wide variety of Machine Learning (ML) inference workloads. PUMA's microarchitecture techniques exposed through a specialized Instruction Set Architecture (ISA) retain the efficiency of in-memory computing and analog circuitry, without compromising programmability. We also present the PUMA compiler which translates high-level code to PUMA ISA. The compiler partitions the computational graph and optimizes instruction scheduling and register allocation to generate code for large and complex workloads to run on thousands of spatial cores. We have developed a detailed architecture simulator that incorporates the functionality, timing, and power models of PUMA's components to evaluate performance and energy consumption. A PUMA accelerator running at 1 GHz can reach area and power efficiency of 577 GOPS/s/mm 2 and 837~GOPS/s/W, respectively. Our evaluation of diverse ML applications from image recognition, machine translation, and language modelling (5M-800M synapses) shows that PUMA achieves up to 2,446\u00d7 energy and 66\u00d7 latency improvement for inference compared to state-of-the-art GPUs. Compared to an application-specific memristor-based accelerator, PUMA incurs small energy overheads at similar inference latency and added programmability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3468125",
                    "name": "Aayush Ankit"
                },
                {
                    "authorId": "3077499",
                    "name": "I. E. Hajj"
                },
                {
                    "authorId": "1792792",
                    "name": "S. R. Chalamalasetti"
                },
                {
                    "authorId": "2594412",
                    "name": "Geoffrey Ndu"
                },
                {
                    "authorId": "1443779643",
                    "name": "M. Foltin"
                },
                {
                    "authorId": "1761983",
                    "name": "R. S. Williams"
                },
                {
                    "authorId": "1789618",
                    "name": "P. Faraboschi"
                },
                {
                    "authorId": "143668320",
                    "name": "Wen-mei W. Hwu"
                },
                {
                    "authorId": "33102961",
                    "name": "J. Strachan"
                },
                {
                    "authorId": "2257216834",
                    "name": "K. Roy"
                },
                {
                    "authorId": "1712405",
                    "name": "D. Milojicic"
                }
            ]
        },
        {
            "paperId": "d9d92ac560774c164d918ca8d0fa1353cc27512d",
            "title": "FPGA Demonstrator of a Programmable Ultra-Efficient Memristor-Based Machine Learning Inference Accelerator",
            "abstract": "Hybrid analog-digital neuromorphic accelerators show promise for significant increase in performance per watt of deep learning inference and training as compared with conventional technologies. In this work we present an FPGA demonstrator of a programmable hybrid inferencing accelerator, with memristor analog dot product engines emulated by digital matrix-vector multiplication units employing FPGA SRAM memory for in-situ weight storage. The full-chip demonstrator interfaced to a host by PCIe interface serves as a software development platform and a vehicle for further hardware microarchitecture improvements. Implementation of compute cores, tiles, network on a chip, and the host interface is discussed. New pipelining scheme is introduced to achieve high utilization of matrix-vector multiplication units while reducing tile data memory size requirements for neural network layer activations. The data flow orchestration between the tiles is described, controlled by a RISC-V core. Inferencing accuracy analysis is presented for an example RNN and CNN models. The demonstrator is instrumented with hardware monitors to enable performance measurements and tuning. Performance projections for future memristor-based ASIC are also discussed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1443779643",
                    "name": "M. Foltin"
                },
                {
                    "authorId": "2047514551",
                    "name": "G. Aguiar"
                },
                {
                    "authorId": "2068518764",
                    "name": "Rodrigo Antunes"
                },
                {
                    "authorId": "2066129695",
                    "name": "P. Silveira"
                },
                {
                    "authorId": "72844688",
                    "name": "Gustavo Knuppe"
                },
                {
                    "authorId": "72833719",
                    "name": "J. Ambrosi"
                },
                {
                    "authorId": "2113355217",
                    "name": "Soumitra Chatterjee"
                },
                {
                    "authorId": "2086559746",
                    "name": "J. Kolhe"
                },
                {
                    "authorId": "1443780437",
                    "name": "Sunil Lakshiminarashimha"
                },
                {
                    "authorId": "1712405",
                    "name": "D. Milojicic"
                },
                {
                    "authorId": "33102961",
                    "name": "J. Strachan"
                },
                {
                    "authorId": "1443779635",
                    "name": "C. Warner"
                },
                {
                    "authorId": "2109649170",
                    "name": "Amit Sharma"
                },
                {
                    "authorId": "2115214941",
                    "name": "Eddie Lee"
                },
                {
                    "authorId": "1792792",
                    "name": "S. R. Chalamalasetti"
                },
                {
                    "authorId": "69925177",
                    "name": "C. Brueggen"
                },
                {
                    "authorId": "2110521171",
                    "name": "Charles Williams"
                },
                {
                    "authorId": "1443779581",
                    "name": "Nathaniel Jansen"
                },
                {
                    "authorId": "2083830016",
                    "name": "Felipe Saenz"
                },
                {
                    "authorId": "2114026531",
                    "name": "Luis Federico Li"
                }
            ]
        },
        {
            "paperId": "93c9278be6d884b346d42242ce4710f3c485069b",
            "title": "Regular Expression Matching with Memristor TCAMs",
            "abstract": "Regular expression (RegEx)matching is a key function in network security, where matching of packet data against known malicious signatures filters and alerts against active network intrusions. RegExs are widely used in open source and commercial network security systems as they easily and concisely represent complex patterns like those malicious signatures. However, the latency and power required to perform RegEx matching is incredibly high and approaches to this problem struggle to achieve > 1 Gbps on real-world rulesets while internet wirespeeds continue to increase > 100 Gbps. We propose performing RegEx matching using memristor-based ternary content addressable memories (mTCAMs)with compressed finite automata (CFA)to meet this challenge. In this work, we show fabrication of mTCAM circuits with excellent device properties from 100nm to 20nm device sizes and validate mTCAM operation. SPICE simulations investigate mTCAM performance at scale and a mTCAM dynamic power model using 16nm mTCAM layout parameters demonstrates 0.173 fJ/bit/search energy for a $36\\times 250$ mTCAM array. Using a tiled architecture to implement a Snort ruleset, we estimate performance of our mTCAM approach to be 47.2 Gbps at 1.21W dynamic search power (39 Gbps/W), compared to a state-of-the-art FPGA approach which achieves 3.9 Gbps at 630mW (6.2 Gbps/W). Preliminary error analysis shows the mTCAM approach allows for arbitrarily low false positive/negative rates using minimal and standard state refresh techniques. Dynamic search power is also calculated prior to applying standard TCAM power-reduction techniques capable of lowering power by $\\sim\\times 10$, further demonstrating the promise of mTCAM for wirespeed RegEx matching at low power.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145961504",
                    "name": "Catherine E. Graves"
                },
                {
                    "authorId": "2111665710",
                    "name": "W. Ma"
                },
                {
                    "authorId": "71872160",
                    "name": "X. Sheng"
                },
                {
                    "authorId": "52306749",
                    "name": "B. Buchanan"
                },
                {
                    "authorId": "2118103124",
                    "name": "Le Zheng"
                },
                {
                    "authorId": "3405899",
                    "name": "Sity Lam"
                },
                {
                    "authorId": "2108274161",
                    "name": "Xuema Li"
                },
                {
                    "authorId": "1792792",
                    "name": "S. R. Chalamalasetti"
                },
                {
                    "authorId": "2193958",
                    "name": "Lennie Kiyama"
                },
                {
                    "authorId": "1443779643",
                    "name": "M. Foltin"
                },
                {
                    "authorId": "34632500",
                    "name": "Matthew P. Hardy"
                },
                {
                    "authorId": "33102961",
                    "name": "J. Strachan"
                }
            ]
        }
    ]
}