{
    "authorId": "1977240",
    "papers": [
        {
            "paperId": "32b3db2ddeb22c9abe4e611da7f9ad6cf129cc9c",
            "title": "Robust Model-Based Clustering",
            "abstract": "We propose a class of Fisher-consistent robust estimators for mixture models. These estimators are then used to build a robust model-based clustering procedure. We study in detail the case of multivariate Gaussian mixtures and propose an algorithm, similar to the EM algorithm, to compute the proposed estimators and build the robust clusters. An extensive Monte Carlo simulation study shows that our proposal outperforms other robust and non robust, state of the art, model-based clustering procedures. We apply our proposal to a real data set and show that again it outperforms alternative procedures.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2115534656",
                    "name": "Juan D. Gonz\u00e1lez"
                },
                {
                    "authorId": "2595359",
                    "name": "R. Maronna"
                },
                {
                    "authorId": "1920126",
                    "name": "V. Yohai"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "b1fd54c0397d2d7eb3d41f5273e766106fc30fb2",
            "title": "Regularized K-means through hard-thresholding",
            "abstract": "We study a framework of regularized $K$-means methods based on direct penalization of the size of the cluster centers. Different penalization strategies are considered and compared through simulation and theoretical analysis. Based on the results, we propose HT $K$-means, which uses an $\\ell_0$ penalty to induce sparsity in the variables. Different techniques for selecting the tuning parameter are discussed and compared. The proposed method stacks up favorably with the most popular regularized $K$-means methods in an extensive simulation study. Finally, HT $K$-means is applied to several real data examples. Graphical displays are presented and used in these examples to gain more insight into the datasets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "18147491",
                    "name": "Jakob Raymaekers"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "2525db0bf23f1d98bb5f67869b0d11775c5a9263",
            "title": "Pooled variable scaling for cluster analysis",
            "abstract": "MOTIVATION\nMany popular clustering methods are not scale invariant because they are based on Euclidean distances. Even methods using scale invariant distances such as the Mahalanobis distance lose their scale invariance when combined with regularization and/or variable selection. Therefore, the results from these methods are very sensitive to the measurement units of the clustering variables. A simple way to achieve scale invariance is to scale the variables before clustering. However, scaling variables is a very delicate issue in cluster analysis: A bad choice of scaling can adversely affect the clustering results. On the other hand, reporting clustering results that depend on measurement units is not satisfactory. Hence, a safe and efficient scaling procedure is needed for applications in bioinformatics and medical sciences research.\n\n\nRESULTS\nWe propose a new approach for scaling prior to cluster analysis based on the concept of pooled variance. Unlike available scaling procedures such as the standard deviation and the range, our proposed scale avoids dampening the beneficial effect of informative clustering variables. We confirm through an extensive simulation study and applications to well known real data examples that the proposed scaling method is safe and generally useful. Finally, we use our approach to cluster a high dimensional genomic dataset consisting of gene expression data for several specimens of breast cancer cells tissue obtained from human patients.\n\n\nAVAILABILITY\nAn R-implementation of the algorithms presented is available at https://wis.kuleuven.be/statdatascience/robust/software.\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "18147491",
                    "name": "Jakob Raymaekers"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "5ceac1ef83b9cb5395a09f3e655f72e1b5993064",
            "title": "Empirical Dynamic Quantiles for Visualization of High-Dimensional Time Series",
            "abstract": "Abstract The empirical quantiles of independent data provide a good summary of the underlying distribution of the observations. For high-dimensional time series defined in two dimensions, such as in space and time, one can define empirical quantiles of all observations at a given time point, but such time-wise quantiles can only reflect properties of the data at that time point. They often fail to capture the dynamic dependence of the data. In this article, we propose a new definition of empirical dynamic quantiles (EDQ) for high-dimensional time series that mitigates this limitation by imposing that the quantile must be one of the observed time series. The word dynamic emphasizes the fact that these newly defined quantiles capture the time evolution of the data. We prove that the EDQ converge to the time-wise quantiles under some weak conditions as the dimension increases. A fast algorithm to compute the dynamic quantiles is presented and the resulting quantiles are used to produce summary plots for a collection of many time series. We illustrate with two real datasets that the time-wise and dynamic quantiles convey different and complementary information. We also briefly compare the visualization provided by EDQ with that obtained by functional depth. The R code and a vignette for computing and plotting EDQ are available at https://github.com/dpena157/HDts/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145993129",
                    "name": "D. Pe\u00f1a"
                },
                {
                    "authorId": "32360997",
                    "name": "R. Tsay"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "f1bb2c8f4705274d72f24df48d54a71413b3eaba",
            "title": "Robust Clustering Using Tau-Scales",
            "abstract": "K means is a popular non-parametric clustering procedure introduced by Steinhaus (1956) and further developed by MacQueen (1967). It is known, however, that K means does not perform well in the presence of outliers. Cuesta-Albertos et al (1997) introduced a robust alternative, trimmed K means, which can be tuned to be robust or efficient, but cannot achieve these two properties simultaneously in an adaptive way. To overcome this limitation we propose a new robust clustering procedure called K Tau Centers, which is based on the concept of Tau scale introduced by Yohai and Zamar (1988). We show that K Tau Centers performs well in extensive simulation studies and real data examples. We also show that the centers found by the proposed method are consistent estimators of the \"true\" centers defined as the minimizers of the the objective function at the population level.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2115534656",
                    "name": "Juan D. Gonz\u00e1lez"
                },
                {
                    "authorId": "1920126",
                    "name": "V. Yohai"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "73cfe120a3819a2c908675ed8d9e74a9eaf421c9",
            "title": "A Stepwise Approach for High-Dimensional Gaussian Graphical Models",
            "abstract": "We present a stepwise approach to estimate high dimensional Gaussian graphical models. We exploit the relation between the partial correlation coefficients and the distribution of the prediction errors, and parametrize the model in terms of the Pearson correlation coefficients between the prediction errors of the nodes\u2019 best linear predictors. We propose a novel stepwise algorithm for detecting pairs of conditionally dependent variables. We compare the proposed algorithm with existing methods including graphical lasso (Glasso), constrained `l1-minimization(CLIME) and equivalent partial correlation (EPC), via simulation studies and real life applications. In our simulation study we consider several model settings and report the results using different performance measures that look at desirable features of the recovered graph.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                },
                {
                    "authorId": "145091322",
                    "name": "M. Ruiz"
                },
                {
                    "authorId": "89067221",
                    "name": "G. Lafit"
                },
                {
                    "authorId": "2133568915",
                    "name": "Javier Nogales"
                }
            ]
        },
        {
            "paperId": "78880dc70d7b055bb8babc735c927b2041b35451",
            "title": "Exploring Rated Datasets with Rating Maps",
            "abstract": "Online rated datasets have become a source for large-scale population studies for analysts and a means for end-users to achieve routine tasks such as finding a book club. Existing systems however only provide limited insights into the opinions of different segments of the rater population. In this paper, we develop a framework for finding and exploring population segments and their opinions. We propose rating maps, a collection of (population segment, rating distribution) pairs, where a segment, e.g., {18-29 year old males in CA} has a rating distribution in the form of a histogram that aggregates its ratings for a set of items (e.g., movies starring Russel Crowe). We formalize the problem of building rating maps dynamically given desired input distributions. Our problem raises two challenges: (i) the choice of an appropriate measure for comparing rating distributions, and (ii) the design of efficient algorithms to find segments. We show that the Earth Mover's Distance (EMD) is well-adapted to comparing rating distributions and prove that finding segments whose rating distribution is close to input ones is NP-complete. We propose an efficient algorithm for building Partition Decision Trees and heuristics for combining the resulting partitions to further improve their quality. Our experiments on real and synthetic datasets validate the utility of rating maps for both analysts and end-users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "3036804",
                    "name": "Sofia Kleisarchaki"
                },
                {
                    "authorId": "9999137",
                    "name": "Naresh Kumar Kolloju"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "863c10bd01a9ea1244cc2b6b268886f2112de7cc",
            "title": "Regression Phalanxes",
            "abstract": "Tomal et al. (2015) introduced the notion of \"phalanxes\" in the context of rare-class detection in two-class classification problems. A phalanx is a subset of features that work well for classification tasks. In this paper, we propose a different class of phalanxes for application in regression settings. We define a \"Regression Phalanx\" - a subset of features that work well together for prediction. We propose a novel algorithm which automatically chooses Regression Phalanxes from high-dimensional data sets using hierarchical clustering and builds a prediction model for each phalanx for further ensembling. Through extensive simulation studies and several real-life applications in various areas (including drug discovery, chemical analysis of spectra data, microarray analysis and climate projections) we show that an ensemble of Regression Phalanxes improves prediction accuracy when combined with effective prediction methods like Lasso or Random Forests.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "40975176",
                    "name": "Hongyang Zhang"
                },
                {
                    "authorId": "145228938",
                    "name": "W. Welch"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "cad5f1f62f277b54b158c5bc60407ac6b3b723f2",
            "title": "Split Regularized Regression",
            "abstract": "Abstract We propose an approach for fitting linear regression models that splits the set of covariates into groups. The optimal split of the variables into groups and the regularized estimation of the regression coefficients are performed by minimizing an objective function that encourages sparsity within each group and diversity among them. The estimated coefficients are then pooled together to form the final fit. Our procedure works on top of a given penalized linear regression estimator (e.g., Lasso, elastic net) by fitting it to possibly overlapping groups of features, encouraging diversity among these groups to reduce the correlation of the corresponding predictions. For the case of two groups, elastic net penalty and orthogonal predictors, we give a closed form solution for the regression coefficients in each group. We establish the consistency of our method with the number of predictors possibly increasing with the sample size. An extensive simulation study and real-data applications show that in general the proposed method improves the prediction accuracy of the base estimator used in the procedure. Possible extensions to GLMs and other models are discussed. The supplemental material for this article, available online, contains the proofs of our theoretical results and the full results of our simulation study.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "89188121",
                    "name": "A. Christidis"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2024002",
                    "name": "Ezequiel Smucler"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "e4bf7d8428cf111af1170dea4638a373b877921c",
            "title": "Ensembles of phalanxes across assessment metrics for robust ranking of homologous proteins",
            "abstract": "Two proteins are homologous if they have a common evolutionary origin, and the binary classification problem is to identify proteins in a candidate set that are homologous to a particular native protein. The feature (explanatory) variables available for classification are various measures of similarity of proteins. There are multiple classification problems of this type for different native proteins and their respective candidate sets. Homologous proteins are rare in a single candidate set, giving a highly unbalanced two-class problem. The goal is to rank proteins in a candidate set according to the probability of being homologous to the set's native protein. An ideal classifier will place all the homologous proteins at the head of such a list. Our approach uses an ensemble of models in a classifier and an ensemble of assessment metrics. For a given metric a classifier combines models, each based on a subset of the available feature variables which we call phalanxes. The proposed ensemble of phalanxes identifies strong and diverse subsets of feature variables. A second phase of ensembling aggregates classifiers based on diverse evaluation metrics. The overall result is called an ensemble of phalanxes and metrics. It provide robustness against both close and distant homologues.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3408290",
                    "name": "J. Tomal"
                },
                {
                    "authorId": "145228938",
                    "name": "W. Welch"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "1a747b2a33c50c1e912e7cabadd7fc08f0e42ed3",
            "title": "Finding and Exploring Rating Distributions (Technical Report)",
            "abstract": "Online rated datasets have become a source for large-scale population studies for analysts and a means for end-users to achieve routine tasks such as finding a book club. Existing systems however only provide limited insights into the opinions of different segments of the rater population. In this technical report, we assume that a segment, e.g., $\\langle${\\em 18-29 year old males in CA}$\\rangle$ has a rating distribution in the form of a histogram that aggregates its ratings for a set of items (e.g., {\\em movies starring Russel Crowe}) and we are interested in comparing its distribution with a given desired input distribution. We use the Earth Mover's Distance ({\\tt EMD}) to comparing rating distributions and we prove that finding segments whose rating distribution is close to input ones is NP-complete.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "3036804",
                    "name": "Sofia Kleisarchaki"
                },
                {
                    "authorId": "9999137",
                    "name": "Naresh Kumar Kolloju"
                },
                {
                    "authorId": "103963722",
                    "name": "Laks V. S. Laskhmanan"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "433ac54874f273cf03b050aaa4015b907ae4d50e",
            "title": "Exploiting Multiple Descriptor Sets in QSAR Studies",
            "abstract": "A quantitative structure-activity relationship (QSAR) is a model relating a specific biological response to the chemical structures of compounds. There are many descriptor sets available to characterize chemical structure, raising the question of how to choose among them or how to use all of them for training a QSAR model. Making efficient use of all sets of descriptors is particularly problematic when active compounds are rare among the assay response data. We consider various strategies to make use of the richness of multiple descriptor sets when assay data are poor in active compounds. Comparisons are made using data from four bioassays, each with five sets of molecular descriptors. The recommended method takes all available descriptors from all sets and uses an algorithm to partition them into groups called phalanxes. Distinct statistical models are trained, each based on only the descriptors in one phalanx, and the models are then averaged in an ensemble of models. By giving the descriptors a chance to contribute in different models, the recommended method uses more of the descriptors in model averaging. This results in better ranking of active compounds to identify a shortlist of drug candidates for development.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "3408290",
                    "name": "J. Tomal"
                },
                {
                    "authorId": "145228938",
                    "name": "W. Welch"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "fbd3e29ae1a07b134ab76a8f26f28a2288352109",
            "title": "RSKC: An R Package for a Robust and Sparse K-Means Clustering Algorithm",
            "abstract": "Witten and Tibshirani (2010) proposed an algorithim to simultaneously find clusters and select clustering variables, called sparse K-means (SK-means). SK-means is particularly useful when the dataset has a large fraction of noise variables (that is, variables without useful information to separate the clusters). SK-means works very well on clean and complete data but cannot handle outliers nor missing data. To remedy these problems we introduce a new robust and sparse K-means clustering algorithm implemented in the R package RSKC. We demonstrate the use of our package on four datasets. We also conduct a Monte Carlo study to compare the performances of RSK-means and SK-means regarding the selection of important variables and identification of clusters. Our simulation study shows that RSK-means performs well on clean data and better than SK-means and other competitors on outlier-contaminated data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48358189",
                    "name": "Yumi Kondo"
                },
                {
                    "authorId": "1398955108",
                    "name": "M. Salibi\u00e1n\u2010Barrera"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "e65e5f8be9b2061608a8aedf82b4364dd7919622",
            "title": "A natural framework for sparse hierarchical clustering",
            "abstract": "There has been a surge in the number of large and flat data sets - data sets containing a large number of features and a relatively small number of observations - due to the growing ability to collect and store information in medical research and other fi?elds. Hierarchical clustering is a widely used clustering tool. In hierarchical clustering, large and flat data sets may allow for a better coverage of clustering features (features that help explain the true underlying clusters) but, such data sets usually include a large fraction of noise features (non-clustering features) that may hide the underlying clusters. Witten and Tibshirani (2010) proposed a sparse hierarchical clustering framework to cluster the observations using an adaptively chosen subset of the features, however, we show that this framework has some limitations when the data sets contain clustering features with complex structure. In this paper, another sparse hierarchical clustering (SHC) framework is proposed. We show that, using simulation studies and real data examples, the proposed framework produces superior feature selection and clustering performance comparing to the classical (of-the-shelf) hierarchical clustering and the existing sparse hierarchical clustering framework.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40975176",
                    "name": "Hongyang Zhang"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "218f491c84776070f1219bc43a629b81de319aac",
            "title": "A robust and sparse K-means clustering algorithm",
            "abstract": "AbstractIn many situations where the interest lies in identifying clusters one might expect that notall available variables carry information about these groups. Furthermore, data quality (e.g.outliers or missing entries) might present a serious and sometimes hard-to-assess problemfor large and complex datasets. In this paper we show that a small proportion of atypicalobservations might have serious adverse e ects on the solutions found by the sparse clusteringalgorithm of Witten and Tibshirani (2010). We propose a robusti cation of their sparse K-means algorithm based on the trimmed K-means algorithm of Cuesta-Albertos et al. (1997).Our proposal is also able to handle datasets with missing values. We illustrate the use of ourmethod on microarray data for cancer patients where we are able to identify strong biologicalclusters with a much reduced number of genes. Our simulation studies show that, when thereare outliers in the data, our robust sparse K-means algorithm performs better than othercompeting methods both in terms of the selection of features and also the identi ed clusters.This robust sparse K-means algorithm is implemented in the R package RSKC which is publiclyavailable from the CRAN repository.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48358189",
                    "name": "Yumi Kondo"
                },
                {
                    "authorId": "1398955108",
                    "name": "M. Salibi\u00e1n\u2010Barrera"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "a6316e72d84720bf70af0643113e55e65484b42a",
            "title": "Nearest\u2010neighbors medians clustering",
            "abstract": "We propose a nonparametric cluster algorithm based on local medians. Each observation is substituted by its local median and this new observation moves toward the peaks and away from the valleys of the distribution. The process is repeated until each observation converges to a fixpoint. We obtain a partition of the sample based on the convergence points. Our algorithm determines the number of clusters and the partition of the observations given the proportion \u03b1 of neighbors. A fast version of the algorithm where only a subset of the observations from the sample is processed is also proposed. A proof of the convergence from each point to its closest fixpoint and the existence and uniqueness of a fixpoint in a neighborhood of each mode is given for the univariate case. \u00a9 2012 Wiley Periodicals, Inc. Statistical Analysis and Data Mining, 2012",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145993129",
                    "name": "D. Pe\u00f1a"
                },
                {
                    "authorId": "2618814",
                    "name": "J\u00falia Viladomat"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "2787b4cf7fb6cddceeb0fdd8c94868875839518f",
            "title": "Automatic genotype calling of single nucleotide polymorphisms using a linear grouping algorithm",
            "abstract": "The use of single nucleotide polymorphisms (SNPs) has become increasingly important for a wide range of genetic studies. A high-throughput genotyping technology usually involves a statistical algorithm for automatic (non-manual) genotype calling. Most calling algorithms in the literature, using methods such as k-means and mixture-models, rely on elliptical structures of the genotyping intensity data. They may fail when the intensity data have linear patterns. We propose an automatic genotype calling algorithm by further developing a linear grouping algorithm (LGA). The proposed method clusters data points around lines as opposed to around centroids. The clusters are on lines because we do not normalize the intensities. In addition, we associate a quality value, silhouette width, with each DNA sample and with each whole plate. For a data set of 101 SNPs from the TaqMan platform (Applied Biosystems), the LGA algorithm has 100% automatic calling and 93% of samples pass a quality criterion and are assigned a genotype. For a subset of 30 SNPs where validated samples are available, the accuracy for called genotypes is over 98%. Thus, a key feature of applying LGA to unnormalized TaqMan SNP assay fluorescent signals is that it is able to call automatically and realiably a substantial proportion of samples, reducing the need for manual intervention. It could be potentially adapted to other fluorescent-based SNP genotyping technologies such as Invader Assay.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47126224",
                    "name": "G. Yan"
                },
                {
                    "authorId": "145228938",
                    "name": "W. Welch"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                },
                {
                    "authorId": "1833064",
                    "name": "L. Akhabir"
                },
                {
                    "authorId": "48340895",
                    "name": "Treena McDonald"
                }
            ]
        },
        {
            "paperId": "05112eb027e568b8f37149b5d2b1b666bd41dc2e",
            "title": "clues: An R Package for Nonparametric Clustering Based on Local Shrinking",
            "abstract": "Determining the optimal number of clusters appears to be a persistent and controversial issue in cluster analysis. Most existing R packages targeting clustering require the user to specify the number of clusters in advance. However, if this subjectively chosen number is far from optimal, clustering may produce seriously misleading results. In order to address this vexing problem, we develop the R package clues to automate and evaluate the selection of an optimal number of clusters, which is widely applicable in the field of clustering analysis. Package clues uses two main procedures, shrinking and partitioning, to estimate an optimal number of clusters by maximizing an index function, either the CH index or the Silhouette index, rather than relying on guessing a pre-specified number. Five agreement indices (Rand index, Hubert and Arabie's adjusted Rand index, Morey and Agresti's adjusted Rand index, Fowlkes and Mallows index and Jaccard index), which measure the degree of agreement between any two partitions, are also provided in clues. In addition to numerical evidence, clues also supplies a deeper insight into the partitioning process with trajectory plots.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064558952",
                    "name": "F. Chang"
                },
                {
                    "authorId": "2057145787",
                    "name": "Weiliang Qiu"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                },
                {
                    "authorId": "2208722150",
                    "name": "Ross Lazarus"
                },
                {
                    "authorId": "31843833",
                    "name": "Xiaogang Wang"
                }
            ]
        },
        {
            "paperId": "db3cd8a69556c800567c2a11d23327c8c33aa8e6",
            "title": "MDQC: a new quality assessment method for microarrays based on quality control reports",
            "abstract": "MOTIVATION\nThe process of producing microarray data involves multiple steps, some of which may suffer from technical problems and seriously damage the quality of the data. Thus, it is essential to identify those arrays with low quality. This article addresses two questions: (1) how to assess the quality of a microarray dataset using the measures provided in quality control (QC) reports; (2) how to identify possible sources of the quality problems.\n\n\nRESULTS\nWe propose a novel multivariate approach to evaluate the quality of an array that examines the 'Mahalanobis distance' of its quality attributes from those of other arrays. Thus, we call it Mahalanobis Distance Quality Control (MDQC) and examine different approaches of this method. MDQC flags problematic arrays based on the idea of outlier detection, i.e. it flags those arrays whose quality attributes jointly depart from those of the bulk of the data. Using two case studies, we show that a multivariate analysis gives substantially richer information than analyzing each parameter of the QC report in isolation. Moreover, once the QC report is produced, our quality assessment method is computationally inexpensive and the results can be easily visualized and interpreted. Finally, we show that computing these distances on subsets of the quality measures in the report may increase the method's ability to detect unusual arrays and helps to identify possible reasons of the quality problems.\n\n\nAVAILABILITY\nThe library to implement MDQC will soon be available from Bioconductor.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "11947538",
                    "name": "Gabriela V Cohen Freue"
                },
                {
                    "authorId": "2404531",
                    "name": "Z. Hollander"
                },
                {
                    "authorId": "2055348345",
                    "name": "E. Shen"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                },
                {
                    "authorId": "2357243",
                    "name": "R. Balshaw"
                },
                {
                    "authorId": "31714727",
                    "name": "A. Scherer"
                },
                {
                    "authorId": "144467314",
                    "name": "B. McManus"
                },
                {
                    "authorId": "2593157",
                    "name": "P. Keown"
                },
                {
                    "authorId": "2252227236",
                    "name": "W. McMaster"
                },
                {
                    "authorId": "2254917936",
                    "name": "Raymond T. Ng"
                }
            ]
        },
        {
            "paperId": "3ac2d60f1060f487c1357fc71f13071255d1622a",
            "title": "Extracting relational data from HTML repositories",
            "abstract": "There is a vast amount of valuable information in HTML documents, widely distributed across the World Wide Web and across corporate intranets. Unfortunately, HTML is mainly presentation oriented and hard to query. In this paper, we develop a system to extract desired information (records) from thousands of HTML documents, starting from a small set of examples. Duplicates in the result are automatically detected and eliminated. We propose a novel method to estimate the current coverage of results by the system, based on capture-recapture models with unequal capture probabilities. We also propose techniques for estimating the error rate of the extracted information and an interactive the technique for enhancing information quality. To evaluate the method and ideas proposed in this paper, we conducted an extensive set of experiments. Our experimental results validate the effectiveness and utility of our system, and demonstrate interesting tradeoffs between running time of information extraction and coverage of results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109960347",
                    "name": "Ruth Yuee Zhang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "466b0b7c2ac9e9137f63402b91b66c8f5dde37b2",
            "title": "Scalable robust covariance and correlation estimates for data mining",
            "abstract": "Covariance and correlation estimates have important applications in data mining. In the presence of outliers, classical estimates of covariance and correlation matrices are not reliable. A small fraction of outliers, in some cases even a single outlier, can distort the classical covariance and correlation estimates making them virtually useless. That is, correlations for the vast majority of the data can be very erroneously reported; principal components transformations can be misleading; and multidimensional outlier detection via Mahalanobis distances can fail to detect outliers. There is plenty of statistical literature on robust covariance and correlation matrix estimates with an emphasis on affine-equivariant estimators that possess high breakdown points and small worst case biases. All such estimators have unacceptable exponential complexity in the number of variables and quadratic complexity in the number of observations. In this paper we focus on several variants of robust covariance and correlation matrix estimates with quadratic complexity in the number of variables and linear complexity in the number of observations. These estimators are based on several forms of pairwise robust covariance and correlation estimates. The estimators studied include two fast estimators based on coordinate-wise robust transformations embedded in an overall procedure recently proposed by [14]. We show that the estimators have attractive robustness properties, and give an example that uses one of the estimators in the new Insightful Miner data mining product.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2286182252",
                    "name": "Fatemah A. Alqallaf"
                },
                {
                    "authorId": "2286035194",
                    "name": "Kjell P. Konis"
                },
                {
                    "authorId": "2256453976",
                    "name": "R. Douglas Martin"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "ba8f7111cacec9e7eb13f40f9abf369444172962",
            "title": "Robust Estimates of Location and Dispersion for High-Dimensional Datasets",
            "abstract": "The computing times of high-breakdown point estimates of multivariate location and scatter increase rapidly with the number of variables, which makes them impractical for high-dimensional datasets, such as those used in data mining. We propose an estimator of location and scatter based on a modified version of the Gnanadesikan\u2013Kettenring robust covariance estimate. We compare its behavior with that of the Stahel\u2013Donoho (SD) and Rousseeuw and Van Driessen's fast MCD (FMCD) estimates. In simulations with contaminated multivariate normal data, our estimate is almost as good as SD and clearly better than FMCD. It is much faster than both, especially for large dimension. We give examples with real data with dimensions between 5 and 93, in which the proposed estimate is as good as or better than SD and FMCD at detecting outliers and other structures, with much shorter computing times.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2595359",
                    "name": "R. Maronna"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "bf739095afadde5a6cc38c0eb39f6b863f02dd22",
            "title": "Robust space transformations for distance-based operations",
            "abstract": "For many KDD operations, such as nearest neighbor search, distance-based clustering, and outlier detection, there is an underlying &kgr;-D data space in which each tuple/object is represented as a point in the space. In the presence of differing scales, variability, correlation, and/or outliers, we may get unintuitive results if an inappropriate space is used.The fundamental question that this paper addresses is: \"What then is an appropriate space?\" We propose using a robust space transformation called the Donoho-Stahel estimator. In the first half of the paper, we show the key properties of the estimator. Of particular importance to KDD applications involving databases is the stability property, which says that in spite of frequent updates, the estimator does not: (a) change much, (b) lose its usefulness, or (c) require re-computation. In the second half, we focus on the computation of the estimator for high-dimensional databases. We develop randomized algorithms and evaluate how well they perform empirically. The novel algorithm we develop called the Hybrid-random algorithm is, in most cases, at least an order of magnitude faster than the Fixed-angle and Subsampling algorithms.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "39529307",
                    "name": "Edwin M. Knorr"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        }
    ]
}