{
    "authorId": "2209861054",
    "papers": [
        {
            "paperId": "38be57ad43ce2cefb7483702271dff1fbff2b844",
            "title": "Enhancing medical vision-language contrastive learning via inter-matching relation modelling",
            "abstract": "Medical image representations can be learned through medical vision-language contrastive learning (mVLCL) where medical imaging reports are used as weak supervision through image-text alignment. These learned image representations can be transferred to and benefit various downstream medical vision tasks such as disease classification and segmentation. Recent mVLCL methods attempt to align image sub-regions and the report keywords as local-matchings. However, these methods aggregate all local-matchings via simple pooling operations while ignoring the inherent relations between them. These methods therefore fail to reason between local-matchings that are semantically related, e.g., local-matchings that correspond to the disease word and the location word (semantic-relations), and also fail to differentiate such clinically important local-matchings from others that correspond to less meaningful words, e.g., conjunction words (importance-relations). Hence, we propose a mVLCL method that models the inter-matching relations between local-matchings via a relation-enhanced contrastive learning framework (RECLF). In RECLF, we introduce a semantic-relation reasoning module (SRM) and an importance-relation reasoning module (IRM) to enable more fine-grained report supervision for image representation learning. We evaluated our method using four public benchmark datasets on four downstream tasks, including segmentation, zero-shot classification, supervised classification, and cross-modal retrieval. Our results demonstrated the superiority of our RECLF over the state-of-the-art mVLCL methods with consistent improvements across single-modal and cross-modal tasks. These results suggest that our RECLF, by modelling the inter-matching relations, can learn improved medical image representations with better generalization capabilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2209861054",
                    "name": "Mingjian Li"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2110883",
                    "name": "M. Fulham"
                },
                {
                    "authorId": "2251669007",
                    "name": "David Dagan Feng"
                },
                {
                    "authorId": "49117537",
                    "name": "Lei Bi"
                },
                {
                    "authorId": "2110005686",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "b27152219dcd97b3c6b4ffa84b135adff0af983a",
            "title": "Dynamic Traceback Learning for Medical Report Generation",
            "abstract": "Automated medical report generation has the potential to significantly reduce the workload associated with the time-consuming process of medical reporting. Recent generative representation learning methods have shown promise in integrating vision and language modalities for medical report generation. However, when trained end-to-end and applied directly to medical image-to-text generation, they face two significant challenges: i) difficulty in accurately capturing subtle yet crucial pathological details, and ii) reliance on both visual and textual inputs during inference, leading to performance degradation in zero-shot inference when only images are available. To address these challenges, this study proposes a novel multi-modal dynamic traceback learning framework (DTrace). Specifically, we introduce a traceback mechanism to supervise the semantic validity of generated content and a dynamic learning strategy to adapt to various proportions of image and text input, enabling text generation without strong reliance on the input from both modalities during inference. The learning of cross-modal knowledge is enhanced by supervising the model to recover masked semantic information from a complementary counterpart. Extensive experiments conducted on two benchmark datasets, IU-Xray and MIMIC-CXR, demonstrate that the proposed DTrace framework outperforms state-of-the-art methods for medical report generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281398403",
                    "name": "Shuchang Ye"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2209861054",
                    "name": "Mingjian Li"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "da8103f9c09f20db5433b917e0a3adc9e0556cdc",
            "title": "SGSeg: Enabling Text-free Inference in Language-guided Segmentation of Chest X-rays via Self-guidance",
            "abstract": "Segmentation of infected areas in chest X-rays is pivotal for facilitating the accurate delineation of pulmonary structures and pathological anomalies. Recently, multi-modal language-guided image segmentation methods have emerged as a promising solution for chest X-rays where the clinical text reports, depicting the assessment of the images, are used as guidance. Nevertheless, existing language-guided methods require clinical reports alongside the images, and hence, they are not applicable for use in image segmentation in a decision support context, but rather limited to retrospective image analysis after clinical reporting has been completed. In this study, we propose a self-guided segmentation framework (SGSeg) that leverages language guidance for training (multi-modal) while enabling text-free inference (uni-modal), which is the first that enables text-free inference in language-guided segmentation. We exploit the critical location information of both pulmonary and pathological structures depicted in the text reports and introduce a novel localization-enhanced report generation (LERG) module to generate clinical reports for self-guidance. Our LERG integrates an object detector and a location-based attention aggregator, weakly-supervised by a location-aware pseudo-label extraction module. Extensive experiments on a well-benchmarked QaTa-COV19 dataset demonstrate that our SGSeg achieved superior performance than existing uni-modal segmentation methods and closely matched the state-of-the-art performance of multi-modal language-guided segmentation methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281398403",
                    "name": "Shuchang Ye"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2209861054",
                    "name": "Mingjian Li"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "5ce0e095e9f3e49eaa9a21647cda58e22ac8f22b",
            "title": "Remote Interactive Surgery Platform (RISP): Proof of Concept for an Augmented-Reality-Based Platform for Surgical Telementoring",
            "abstract": "The \u201cRemote Interactive Surgery Platform\u201d (RISP) is an augmented reality (AR)-based platform for surgical telementoring. It builds upon recent advances of mixed reality head-mounted displays (MR-HMD) and associated immersive visualization technologies to assist the surgeon during an operation. It enables an interactive, real-time collaboration with a remote consultant by sharing the operating surgeon\u2019s field of view through the Microsoft (MS) HoloLens2 (HL2). Development of the RISP started during the Medical Augmented Reality Summer School 2021 and is currently still ongoing. It currently includes features such as three-dimensional annotations, bidirectional voice communication and interactive windows to display radiographs within the sterile field. This manuscript provides an overview of the RISP and preliminary results regarding its annotation accuracy and user experience measured with ten participants.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "26338204",
                    "name": "Y. Kalbas"
                },
                {
                    "authorId": "123132879",
                    "name": "Hoijoon Jung"
                },
                {
                    "authorId": "2209799352",
                    "name": "J. Ricklin"
                },
                {
                    "authorId": "2209799563",
                    "name": "Ge Jin"
                },
                {
                    "authorId": "2209861054",
                    "name": "Mingjian Li"
                },
                {
                    "authorId": "1660521485",
                    "name": "T. Rauer"
                },
                {
                    "authorId": "1999206991",
                    "name": "Shervin Dehghani"
                },
                {
                    "authorId": "145587210",
                    "name": "Nassir Navab"
                },
                {
                    "authorId": "2146416796",
                    "name": "Jinman Kim"
                },
                {
                    "authorId": "2209799243",
                    "name": "Hans-Christoph Pape"
                },
                {
                    "authorId": "2385360",
                    "name": "S. Heining"
                }
            ]
        }
    ]
}