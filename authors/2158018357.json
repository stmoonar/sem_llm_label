{
    "authorId": "2158018357",
    "papers": [
        {
            "paperId": "3a6e41b3bb092157ef6af2dd630e24d52471a789",
            "title": "CSRec: Rethinking Sequential Recommendation from A Causal Perspective",
            "abstract": "The essence of sequential recommender systems (RecSys) lies in understanding how users make decisions. Most existing approaches frame the task as sequential prediction based on users' historical purchase records. While effective in capturing users' natural preferences, this formulation falls short in accurately modeling actual recommendation scenarios, particularly in accounting for how unsuccessful recommendations influence future purchases. Furthermore, the impact of the RecSys itself on users' decisions has not been appropriately isolated and quantitatively analyzed. To address these challenges, we propose a novel formulation of sequential recommendation, termed Causal Sequential Recommendation (CSRec). Instead of predicting the next item in the sequence, CSRec aims to predict the probability of a recommended item's acceptance within a sequential context and backtrack how current decisions are made. Critically, CSRec facilitates the isolation of various factors that affect users' final decisions, especially the influence of the recommender system itself, thereby opening new avenues for the design of recommender systems. CSRec can be seamlessly integrated into existing methodologies. Experimental evaluations on both synthetic and real-world datasets demonstrate that the proposed implementation significantly improves upon state-of-the-art baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2158018357",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "2261901774",
                    "name": "Jiaxin Yuan"
                },
                {
                    "authorId": "2266789873",
                    "name": "Yuhang Zhou"
                },
                {
                    "authorId": "2291078963",
                    "name": "Jingling Li"
                },
                {
                    "authorId": "2257407889",
                    "name": "Furong Huang"
                },
                {
                    "authorId": "2218202090",
                    "name": "Wei Ai"
                }
            ]
        },
        {
            "paperId": "3a74772a6011675ce2bdc87100fffcf4d18f5907",
            "title": "Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey",
            "abstract": "Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative Large Language Models (LLMs) has significantly impacted various NLP domains, particularly through their advanced reasoning capabilities. This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality. Meanwhile, LLMs' strong reasoning capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective potential to further the development of more advanced and equitable artificial intelligence systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2158018357",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "103226724",
                    "name": "Paiheng Xu"
                },
                {
                    "authorId": "2146666303",
                    "name": "Junda Wu"
                },
                {
                    "authorId": "2261901774",
                    "name": "Jiaxin Yuan"
                },
                {
                    "authorId": "2257361107",
                    "name": "Yifan Yang"
                },
                {
                    "authorId": "2266789873",
                    "name": "Yuhang Zhou"
                },
                {
                    "authorId": "52220309",
                    "name": "Fuxiao Liu"
                },
                {
                    "authorId": "2291143084",
                    "name": "Tianrui Guan"
                },
                {
                    "authorId": "2291313121",
                    "name": "Haoliang Wang"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "2284680226",
                    "name": "Julian McAuley"
                },
                {
                    "authorId": "2218202090",
                    "name": "Wei Ai"
                },
                {
                    "authorId": "2257407889",
                    "name": "Furong Huang"
                }
            ]
        },
        {
            "paperId": "3c15bd38cdfafee70d7798dfb2bec96259c4f61a",
            "title": "conv_einsum: A Framework for Representation and Fast Evaluation of Multilinear Operations in Convolutional Tensorial Neural Networks",
            "abstract": "Modern ConvNets continue to achieve state-of-the-art results over a vast array of vision and image classification tasks, but at the cost of increasing parameters. One strategy for compactifying a network without sacrificing much expressive power is to reshape it into a tensorial neural network (TNN), which is a higher-order tensorization of its layers, followed by a factorization, such as a CP-decomposition, which strips a weight down to its critical basis components. Passes through TNNs can be represented as sequences of multilinear operations (MLOs), where the evaluation path can greatly affect the number of floating point operations (FLOPs) incurred. While functions such as the popular einsum can evaluate simple MLOs such as contractions, existing implementations cannot process multi-way convolutions, resulting in scant assessments of how optimal evaluation paths through tensorized convolutional layers can improve training speed. In this paper, we develop a unifying framework for representing tensorial convolution layers as einsum-like strings and a meta-algorithm conv_einsum which is able to evaluate these strings in a FLOPs-minimizing manner. Comprehensive experiments, using our open-source implementation, over a wide range of models, tensor decompositions, and diverse tasks, demonstrate that conv_einsum significantly increases both computational and memory-efficiency of convolutional TNNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "71520803",
                    "name": "Tahseen Rabbani"
                },
                {
                    "authorId": "152324585",
                    "name": "Jiahao Su"
                },
                {
                    "authorId": "2158018357",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "2278434209",
                    "name": "David Chan"
                },
                {
                    "authorId": "2278432314",
                    "name": "Geoffrey Sangston"
                },
                {
                    "authorId": "2265619850",
                    "name": "Furong Huang"
                }
            ]
        },
        {
            "paperId": "6fa3544f42bc026ac684cf6c7a8cd50f59b3ee7d",
            "title": "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation",
            "abstract": "Large language models (LLMs) have significantly advanced various natural language processing tasks, but deploying them remains computationally expensive. Knowledge distillation (KD) is a promising solution, enabling the transfer of capabilities from larger teacher LLMs to more compact student models. Particularly, sequence-level KD, which distills rationale-based reasoning processes instead of merely final outcomes, shows great potential in enhancing students' reasoning capabilities. However, current methods struggle with sequence level KD under long-tailed data distributions, adversely affecting generalization on sparsely represented domains. We introduce the Multi-Stage Balanced Distillation (BalDistill) framework, which iteratively balances training data within a fixed computational budget. By dynamically selecting representative head domain examples and synthesizing tail domain examples, BalDistill achieves state-of-the-art performance across diverse long-tailed datasets, enhancing both the efficiency and efficacy of the distilled models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266789873",
                    "name": "Yuhang Zhou"
                },
                {
                    "authorId": "2307469621",
                    "name": "Jing Zhu"
                },
                {
                    "authorId": "103226724",
                    "name": "Paiheng Xu"
                },
                {
                    "authorId": "2158018357",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "2282902714",
                    "name": "Xiyao Wang"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                },
                {
                    "authorId": "2218202090",
                    "name": "Wei Ai"
                },
                {
                    "authorId": "2268686199",
                    "name": "Furong Huang"
                }
            ]
        },
        {
            "paperId": "f7f5836a74771a889e22ff37c4870889cf452d4e",
            "title": "Calibrated Dataset Condensation for Faster Hyperparameter Search",
            "abstract": "Dataset condensation can be used to reduce the computational cost of training multiple models on a large dataset by condensing the training dataset into a small synthetic set. State-of-the-art approaches rely on matching the model gradients between the real and synthetic data. However, there is no theoretical guarantee of the generalizability of the condensed data: data condensation often generalizes poorly across hyperparameters/architectures in practice. This paper considers a different condensation objective specifically geared toward hyperparameter search. We aim to generate a synthetic validation dataset so that the validation-performance rankings of the models, with different hyperparameters, on the condensed and original datasets are comparable. We propose a novel hyperparameter-calibrated dataset condensation (HCDC) algorithm, which obtains the synthetic validation dataset by matching the hyperparameter gradients computed via implicit differentiation and efficient inverse Hessian approximation. Experiments demonstrate that the proposed framework effectively maintains the validation-performance rankings of models and speeds up hyperparameter/architecture search for tasks on both images and graphs.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "52184822",
                    "name": "Mucong Ding"
                },
                {
                    "authorId": "2238278723",
                    "name": "Yuancheng Xu"
                },
                {
                    "authorId": "71520803",
                    "name": "Tahseen Rabbani"
                },
                {
                    "authorId": "2158018357",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "2290843949",
                    "name": "Brian J Gravelle"
                },
                {
                    "authorId": "1505823351",
                    "name": "Teresa M. Ranadive"
                },
                {
                    "authorId": "2303467705",
                    "name": "Tai-Ching Tuan"
                },
                {
                    "authorId": "2303418156",
                    "name": "Furong Huang"
                }
            ]
        },
        {
            "paperId": "01efb3fd2d3ae4b5f4389c916c94f2c6d9c11b81",
            "title": "Explore Spurious Correlations at the Concept Level in Language Models for Text Classification",
            "abstract": "Language models (LMs) have achieved notable success in numerous NLP tasks, employing both fine-tuning and in-context learning (ICL) methods. While language models demonstrate exceptional performance, they face robustness challenges due to spurious correlations arising from imbalanced label distributions in training data or ICL exemplars. Previous research has primarily concentrated on word, phrase, and syntax features, neglecting the concept level, often due to the absence of concept labels and difficulty in identifying conceptual content in input texts. This paper introduces two main contributions. First, we employ ChatGPT to assign concept labels to texts, assessing concept bias in models during fine-tuning or ICL on test data. We find that LMs, when encountering spurious correlations between a concept and a label in training or prompts, resort to shortcuts for predictions. Second, we introduce a data rebalancing technique that incorporates ChatGPT-generated counterfactual data, thereby balancing label distribution and mitigating spurious correlations. Our method's efficacy, surpassing traditional token removal approaches, is validated through extensive testing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145926835",
                    "name": "Yuhang Zhou"
                },
                {
                    "authorId": "103226724",
                    "name": "Paiheng Xu"
                },
                {
                    "authorId": "2158018357",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "2256993596",
                    "name": "Bang An"
                },
                {
                    "authorId": "2218202090",
                    "name": "Wei Ai"
                },
                {
                    "authorId": "2257407877",
                    "name": "Furong Huang"
                }
            ]
        },
        {
            "paperId": "465e1d23cd5ac234a6efbe747c74080503757d40",
            "title": "C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder",
            "abstract": "Representation learning assumes that real-world data is generated by a few semantically meaningful generative factors (i.e., sources of variation) and aims to discover them in the latent space. These factors are expected to be causally disentangled, meaning that distinct factors are encoded into separate latent variables, and changes in one factor will not affect the values of the others. Compared to statistical independence, causal disentanglement allows more controllable data generation, improved robustness, and better generalization. However, most existing work assumes unconfoundedness in the discovery process, that there are no common causes to the generative factors and thus obtain only statistical independence. In this paper, we recognize the importance of modeling confounders in discovering causal generative factors. Unfortunately, such factors are not identifiable without proper inductive bias. We fill the gap by introducing a framework entitled Confounded-Disentanglement (C-Disentanglement), the first framework that explicitly introduces the inductive bias of confounder via labels from domain expertise. In addition, we accordingly propose an approach to sufficiently identify the causally disentangled factors under any inductive bias of the confounder. We conduct extensive experiments on both synthetic and real-world datasets. Our method demonstrates competitive results compared to various SOTA baselines in obtaining causally disentangled features and downstream tasks under domain shifts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2158018357",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "2261901774",
                    "name": "Jiaxin Yuan"
                },
                {
                    "authorId": "2256993596",
                    "name": "Bang An"
                },
                {
                    "authorId": "2238278723",
                    "name": "Yuancheng Xu"
                },
                {
                    "authorId": "2257361107",
                    "name": "Yifan Yang"
                },
                {
                    "authorId": "2257407889",
                    "name": "Furong Huang"
                }
            ]
        },
        {
            "paperId": "338d0501947b5fc7d92d09eed9a3e299f7b48ec1",
            "title": "Tuformer: Data-driven Design of Transformers for Improved Generalization or Efficiency",
            "abstract": "CIFAR-10 datasets, the lower the better. Results show that Tuformers can be extended to image generation tasks and improve the performance of other ef\ufb01cient designs with linear computational and memory complexities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2158018357",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "152324585",
                    "name": "Jiahao Su"
                },
                {
                    "authorId": "2117426487",
                    "name": "Furong Huang"
                }
            ]
        },
        {
            "paperId": "f6f50b8d0b925801a121bfc9cebb873d06947483",
            "title": "Compact Neural Architecture Designs by Tensor Representations",
            "abstract": "We propose a framework of tensorial neural networks (TNNs) extending existing linear layers on low-order tensors to multilinear operations on higher-order tensors. TNNs have three advantages over existing networks: First, TNNs naturally apply to higher-order data without flattening, which preserves their multi-dimensional structures. Second, compressing a pre-trained network into a TNN results in a model with similar expressive power but fewer parameters. Finally, TNNs interpret advanced compact designs of network architectures, such as bottleneck modules and interleaved group convolutions. To learn TNNs, we derive their backpropagation rules using a novel suite of generalized tensor algebra. With backpropagation, we can either learn TNNs from scratch or pre-trained models using knowledge distillation. Experiments on VGG, ResNet, and Wide-ResNet demonstrate that TNNs outperform the state-of-the-art low-rank methods on a wide range of backbone networks and datasets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "152324585",
                    "name": "Jiahao Su"
                },
                {
                    "authorId": "8549223",
                    "name": "Jingling Li"
                },
                {
                    "authorId": "2158018357",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "1505823351",
                    "name": "Teresa M. Ranadive"
                },
                {
                    "authorId": "2065440596",
                    "name": "Christopher J. Coley"
                },
                {
                    "authorId": "35112632",
                    "name": "Tai-Ching Tuan"
                },
                {
                    "authorId": "2117426487",
                    "name": "Furong Huang"
                }
            ]
        }
    ]
}