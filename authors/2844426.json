{
    "authorId": "2844426",
    "papers": [
        {
            "paperId": "ff538891eb974fdcdebd1e4e2e1b7df4e9348a05",
            "title": "About Ergonomics of Computer Interfaces Designing a System: Designing a System for Human Computational Collective Use",
            "abstract": "This document discusses the ergonomic problems with currently available software products, and what in general is necessary to make an application pleasant to work with. The application of these principles to a new open-architecture user interface system, Views, is then described.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2844426",
                    "name": "C. Jouis"
                },
                {
                    "authorId": "1477336451",
                    "name": "Mercedes Or\u00fas-Lacort"
                },
                {
                    "authorId": "144615425",
                    "name": "S. Pemberton"
                }
            ]
        },
        {
            "paperId": "c84767de69c6de0ef95b67b9b94951d6b75bc3f4",
            "title": "Management of Big Textual Data in Qualitative Research: Organizing the Relationships in a Typology based on Logical Properties",
            "abstract": "In structured indexes, classification systems, thesauri, conceptual structures or semantic networks, relationships are too often vague. For instance, in terminology, the relationships between concepts are often reduced to the distinction established by standard between hierarchical relationships (genus-species relationships and part/whole relationships) and non-hierarchical relationships (\"time, space, causal relationships, etc.\"). The semantics of relationships are vague because the principal users of these relationships are industrial actors (translators of technical handbooks, terminologists, data-processing specialists, etc.). Nevertheless, the consistency of the models built must always be guaranteed... One possible approach to this problem consists in organizing the relationships in a typology based on logical properties. For instance, we typically use only the general relation \"Is-a\". It is too vague. We assume that general relation \"Is-a\" is characterized by asymmetry. This asymmetry is specified in: (1) the belonging of one individualizable entity to a distributive class, (2) Inclusion among distributive classes and (3) relation part of (or \"composition\"). With the view to better designing the knowledge structures underlying the concepts of a field, and more specifically, the indexing of documents and/or information retrieval, we propose a structured set of relationships, based on a linguistic model, the Applicative and Cognitive Grammar (ACG) of Jean-Pierre Descles [7], [8]. This model was extended to terminology by Christophe Jouis [32], and applied by Widad Mustafa and Christophe Jouis, [33], [34], [35] and [36].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2844426",
                    "name": "C. Jouis"
                },
                {
                    "authorId": "1477336451",
                    "name": "Mercedes Or\u00fas-Lacort"
                },
                {
                    "authorId": "13279363",
                    "name": "N. Durglishvili"
                },
                {
                    "authorId": "40054109",
                    "name": "R. Or\u00fas"
                }
            ]
        },
        {
            "paperId": "49606720279b044f778a8fbde86bd2ab56b342cf",
            "title": "Big textual data: how to find relevant information (with low cost)?",
            "abstract": "Obviously, it is not useful to accumulate large amounts of information if we cannot find a particular piece of information. Also, extracting relevant and targeted information from textual data on large digital media, and if they are heterogeneous and multilingual, is certainly not a new problem. However, the current methods prove to be expensive and the results are too often inappropriate, too numerous and not very presentable for the user. In addition to current methods, we propose an original method: Contextual Exploration. This is the EC3 software. EC3 does not need syntactic analysis, statistical analysis nor a \"general\" ontology. EC3 uses only small ontologies called \"linguistic ontologies\" that expresses the language of knowledge. This is why EC3 works very quickly on large corpus, which components can be both whole and short text: SMS to books. At the output, EC3 offers a dynamic visual representation of results. EC3 has been tested on very large digitized corpus provided by the French Labex OBVIL \"Observatory of the Literary Life\", in partnership with the National Library of France.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2844426",
                    "name": "C. Jouis"
                },
                {
                    "authorId": "2073312350",
                    "name": "Bilal Shafei"
                }
            ]
        },
        {
            "paperId": "6feed5b9000656911838fe5d9c2c65c1e558e726",
            "title": "Contextual Exploration (EC3): a strategy for the detection, extraction and visualization of target data",
            "abstract": "EC3 is intended to extract relevant information from large heterogeneous and multilingual text data, in particular in WEB 3.0. The project is based on an original method: contextual exploration. EC3 does not need syntactic analysis, statistical analysis or a \"general\" ontology. EC3 uses only small ontologies called \"linguistic ontologies\" that express the linguistic knowledge of a user who must concentrate on the relevant information from one point of view. This is why EC3 works very quickly on large corpus, whose components can be both whole books as well as \"short texts\": SMS to books. At the output, EC3 offers a visual representation of information using an original approach: the \"Memory Islands\". EC3 is implanted in the ACASA / LIP6 team. EC3 is tested on the large digitized corpus provided by the Labex OBVIL \u00abObservatoire de la Vie Litteraire\u00bb, in partnership with the Bibliotheque Nationale de France (http://obvil.paris-sorbonne.fr/). OBVIL intends to develop all the resources offered by digitization and computer applications to examine French literature from the sixteenth to the twentieth century, English and American literature, Italian literature, Spanish literature, in its most traditional formats and media Or the most innovative.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2085012164",
                    "name": "Hammou Fadili"
                },
                {
                    "authorId": "2844426",
                    "name": "C. Jouis"
                },
                {
                    "authorId": "1686738",
                    "name": "J. Ganascia"
                },
                {
                    "authorId": "1701200",
                    "name": "Iana Atanassova"
                },
                {
                    "authorId": "2118582147",
                    "name": "Bin Yang"
                },
                {
                    "authorId": "2008392",
                    "name": "Marc Bertin"
                }
            ]
        },
        {
            "paperId": "ea719f028d35d7b169df0f91b9c613566be99490",
            "title": "Towards an automatic analyze and standardization of unstructured data in the context of big and linked data",
            "abstract": "Unstructured data refers to information that either does not have a pre-defined data model or is not organized in a pre-defined manner. Many studies confirm that around 80--90% of all produced information is in unstructured form. So this kind of content, rich and most importantly too precious, must be integrated and taken into consideration for processing and exploitation: extraction of relevant information from heterogeneous textual data. The goal of the research described here is to present an approach for automating the detection and the extraction of meaning from unstructured Web using its normalized part: Web of data & Linked Open data (LOD) such as RDF WordNet, DBpedia, etc. The process follows a \"cyclical process\" that consists of two phases (a) creating & generating normalized smart data by the experts or automatically, (b) exploiting the created data in (a), as \"validated expert data\", to analyze the Big Data and generate automatically new ones by learning from Linked Open Data (LOD). The approach is based on a range of linguistic and ontological techniques, in the context of Big Data. A software, EC3, is being implemented and at LIP6. EC3 is actually tested on very large corpuses on electronic supports, provided by the labex OBVIL (http://obvil.paris-sorbonne.fr) and the BNF (National Library of France).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2085012164",
                    "name": "Hammou Fadili"
                },
                {
                    "authorId": "2844426",
                    "name": "C. Jouis"
                }
            ]
        },
        {
            "paperId": "c68edd075cb07fde293f86e432d63e3b6ebba26d",
            "title": "Discursive Mining Viewpoints in Building Multi-Document Synthesized Sheets",
            "abstract": "Multi-documents sheets are viewed as semantically structured representations of textual documents. The automatic construction of these sheets is based on the automatic annotation of textual documents according to a set of discursive categories called discursive mining viewpoints. The automatic annotation of a text is performed using the Contextual Exploration processing. It is a linguistic and computational method implemented in the EXCOM2 platform that allows the an-notation of segments (which can be a title, a paragraph, a sentence or a clause) according to a given discursive mining viewpoint.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2242365",
                    "name": "Olfa Makkaoui"
                },
                {
                    "authorId": "1727109",
                    "name": "Jean-Pierre Descl\u00e9s"
                },
                {
                    "authorId": "2008392",
                    "name": "Marc Bertin"
                },
                {
                    "authorId": "2844426",
                    "name": "C. Jouis"
                },
                {
                    "authorId": "1686738",
                    "name": "J. Ganascia"
                }
            ]
        },
        {
            "paperId": "d2204549234fa9bfaab8c3cd6fc022b216ffc6ae",
            "title": "Special Track on Semantic, Logics and Information Extraction in AI",
            "abstract": "This special track is a forum for discussing the latest approaches in computational linguistics related to cognitive semantics and to artificial intelligence. Its aim is also to exchange ideas concerning the way of building efficient systems for language analysis based on cognitive semantic models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2844426",
                    "name": "C. Jouis"
                },
                {
                    "authorId": "2917621",
                    "name": "Isma\u00efl Biskri"
                },
                {
                    "authorId": "34598153",
                    "name": "A. Jebali"
                },
                {
                    "authorId": "145795907",
                    "name": "A. Pascu"
                }
            ]
        },
        {
            "paperId": "01825c9435db493b4bacb840496389563f0fbd4e",
            "title": "A Neo-Topological Approach to Reasoning on Ontologies with Exceptions and Comparison with Defeasible Description Logics",
            "abstract": "This article compares Defeasible Description Logics (DDL) and Topological Approach to reason on Ontologies with exceptions. DDL is integration between Description Logics and Defeasible Logics to deal with monotonic and non-monotonic parts of the knowledge bases respectively. Topological approach tries to reason on inconsistent knowledge bases using the conventional topological operators e.g., interior, exterior, border and closure. We develop neo-Topology based on topological operators and we make major development and improvements of current Topological approach by properly introducing the \u201cThickness Border\u201d with strong inference rules. We proof the validity of the inference rules using set operations. We demonstrate both approaches with appropriate example. We show the advantages and disadvantages of both approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2844426",
                    "name": "C. Jouis"
                },
                {
                    "authorId": "2110138651",
                    "name": "Mohammed Yasin Rahman"
                },
                {
                    "authorId": "1686738",
                    "name": "J. Ganascia"
                }
            ]
        },
        {
            "paperId": "1870491e88efa057e4b31dc766d34f5238928828",
            "title": "Comparison between Defeasible Description Logics and neo-Topology to reason on ontologies with exceptions",
            "abstract": "This article compares Defeasible Description Logics (DDL) and Topological Approach to reason on Ontologies with exceptions. DDL is integration between Description Logics and Defeasible Logics to deal with monotonic and non-monotonic parts of the knowledge bases respectively. Topological approach tries to reason on inconsistent knowledge bases using the conventional topological operators e.g., interior, exterior, border and closure. We develop neo-Topology based on topological operators and we make major development and improvements of current Topological approach by properly introducing the ``Thickness Border'' with strong inference rules. We proof the validity of the inference rules using set operations. We demonstrate both approaches with appropriate example. We show the advantages and disadvantages of both approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2844426",
                    "name": "C. Jouis"
                },
                {
                    "authorId": "2110138651",
                    "name": "Mohammed Yasin Rahman"
                },
                {
                    "authorId": "1686738",
                    "name": "J. Ganascia"
                }
            ]
        },
        {
            "paperId": "3cca8f75344bcd0093322606fb32136a12be15c0",
            "title": "The Logic of Typical and Atypical Instances (LTA)",
            "abstract": "The difference between typical instances and atypical instances in a natural categorization process has been introduced by E. Rosh and studied by cognitive psychology and AI. A lot of the knowledge representation systems are expressed in using fuzzy concepts but a degree of membership raises some problem for natural categorizations (especially to classification problems in anthropology, ethnology, archeology, linguistics but also in ontologies), but atypical instances of a concept cannot be apprehended adequately by different degrees from a prototype. Other formal approaches, as paraconsistent logics or non monotonic logics, conceptualize often atypical objects as exceptions. It had yet been developed an alternative way with the logics of determination of the objects (LDO). In this paper, we present the logics of typical and atypical (LTA) in order to give directly a logical approach of typicality / atypicality associated to a concept by a more common way than in LDO, in using only classes and not determination operators. It is introduced a distinction between predicative property and concept defined with its intension and its essence, a part of intension. A typical instance of a concept inherits all properties of intension; a typical instance inherits only properties of essence but it is a full member of the category associated to a concept and not a member with a weak degree of membership. In natural categorization, there are often instances (the exceptions) which do not inherit some properties of the essence; they cannot be considered as atypical instance and belong to the boundary of the category.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1727109",
                    "name": "Jean-Pierre Descl\u00e9s"
                },
                {
                    "authorId": "145795907",
                    "name": "A. Pascu"
                },
                {
                    "authorId": "2844426",
                    "name": "C. Jouis"
                }
            ]
        }
    ]
}