{
    "authorId": "2108169214",
    "papers": [
        {
            "paperId": "0ff86630bf775f0510ef20b76352a0757a4ed70b",
            "title": "EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE",
            "abstract": "Building scalable vision-language models to learn from diverse, multimodal data remains an open challenge. In this paper, we introduce an Efficient Vision-languagE foundation model, namely EVE, which is one unified multimodal Transformer pre-trained solely by one unified pre-training task. Specifically, EVE encodes both vision and language within a shared Transformer network integrated with modality-aware sparse Mixture-of-Experts (MoE) modules, which capture modality-specific information by selectively switching to different experts. To unify pre-training tasks of vision and language, EVE performs masked signal modeling on image-text pairs to reconstruct masked signals, i.e., image pixels and text tokens, given visible signals. This simple yet effective pre-training objective accelerates training by 4x compared to the model pre-trained with Image-Text Contrastive and Image-Text Matching losses. Owing to the combination of the unified architecture and pre-training task, EVE is easy to scale up, enabling better downstream performance with fewer resources and faster training speed. Despite its simplicity, EVE achieves state-of-the-art performance on various vision-language downstream tasks, including visual question answering, visual reasoning, and image-text retrieval.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108169214",
                    "name": "Junyi Chen"
                },
                {
                    "authorId": "26982950",
                    "name": "Longteng Guo"
                },
                {
                    "authorId": "2047929346",
                    "name": "Jianxiang Sun"
                },
                {
                    "authorId": "2187301328",
                    "name": "Shuai Shao"
                },
                {
                    "authorId": "51305314",
                    "name": "Zehuan Yuan"
                },
                {
                    "authorId": "2148303328",
                    "name": "Liang Lin"
                },
                {
                    "authorId": "46334785",
                    "name": "Dongyu Zhang"
                }
            ]
        },
        {
            "paperId": "c55c82469faf31d12803c6ebbdf6ed6b5c2a5686",
            "title": "LPN: Label-Enhanced Prototypical Network for Legal Judgment Prediction",
            "abstract": "As one of the most critical tasks in legal artificial intelligence, legal judgment prediction (LJP) has garnered growing attention, especially in the civil law system. However, current methods often overlook the challenge of imbalanced label distributions, treating each label with equal importance, which can lead the model to be biased toward labels with high frequency. In this paper, we propose a label-enhanced prototypical network (LPN) suitable for LJP, that adopts a strategy of uniform encoding and separate decoding. Specifically, LPN adopts a multi-scale convolutional neural network to uniformly encode case factual description to capture long-distance features of the document. At the decoding end, a prototypical network incorporating label semantic features is used to guide the learning of prototype representations of high-frequency and low-frequency labels, respectively. At the same time, we also propose a prototype-prototype loss to optimize the prototypical representation. We conduct extensive experiments on two real datasets and show that our proposed method effectively improves the performance of LJP, with an average F1 of 1.23% and 1.13% higher than the state-of-the-art model on two subtasks, respectively.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108169214",
                    "name": "Junyi Chen"
                },
                {
                    "authorId": "49897177",
                    "name": "Yingjie Han"
                },
                {
                    "authorId": "1882859",
                    "name": "Xiabing Zhou"
                },
                {
                    "authorId": "144539290",
                    "name": "Hongying Zan"
                },
                {
                    "authorId": "2249679016",
                    "name": "Qinglei Zhou"
                }
            ]
        },
        {
            "paperId": "560e0114a023bdfd99eb60eb4d9d555a348600a0",
            "title": "PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals",
            "abstract": "We propose a PiggyBack, a Visual Question Answering platform that allows users to apply the state-of-the-art visual-language pretrained models easily. We integrate visual-language models, pretrained by HuggingFace, an open-source API platform of deep learning technologies; however, it cannot be runnable without programming skills or deep learning understanding. Hence, our PiggyBack supports an easy-to-use browser-based user interface with several deep-learning visual language pretrained models for general users and domain experts. The PiggyBack includes the following benefits: Portability due to web-based and thus runs on almost any platform, A comprehensive data creation and processing technique, and ease of use on visual language pretrained models. The demo video can be found at https://youtu.be/iz44RZ1lF4s.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2179013400",
                    "name": "Zhihao Zhang"
                },
                {
                    "authorId": "39209233",
                    "name": "Siwen Luo"
                },
                {
                    "authorId": "2108169214",
                    "name": "Junyi Chen"
                },
                {
                    "authorId": "2164243615",
                    "name": "Sijia Lai"
                },
                {
                    "authorId": "32545338",
                    "name": "Siqu Long"
                },
                {
                    "authorId": "3312958",
                    "name": "Hyunsuk Chung"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                }
            ]
        },
        {
            "paperId": "c45418475a4f78bb2bce77dab952ee80a39844db",
            "title": "Mulan: A Multiple Residual Article-Wise Attention Network for Legal Judgment Prediction",
            "abstract": "Legal judgment prediction (LJP) is used to predict judgment results based on the description of individual legal cases. In order to be more suitable for actual application scenarios in which the case has cited multiple articles and has multiple charges, we formulate legal judgment prediction as a multiple label learning problem and present a deep learning model that can effectively encode the content of each legal case via a multi-residual convolution neural network and the semantics of law articles via an article encoder. An article-wise attention mechanism is proposed to fuse the two types of encoded information. Experimental results derived on the CAIL2018 datasets show that our model provides a significant performance improvement over the existing neural models in predicting relevant law articles and charges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108169214",
                    "name": "Junyi Chen"
                },
                {
                    "authorId": "1723019",
                    "name": "Lan Du"
                },
                {
                    "authorId": "2112748109",
                    "name": "Ming Liu"
                },
                {
                    "authorId": "1882859",
                    "name": "Xiabing Zhou"
                }
            ]
        },
        {
            "paperId": "77582e19950efeb5c89d99b84f6a8f6abfaee90f",
            "title": "Towards an open science platform for the evaluation of data fusion",
            "abstract": "Combining the results of different search engines in order to improve upon their performance has been the subject of many research papers. This has become known as the \u201cData Fusion\u201d task, and has great promise in dealing with the vast quantity of unstructured textual data that is a feature of many Big Data scenarios. However, no universally-accepted evaluation methodology has emerged in the community. This makes it difficult to make meaningful comparisons between the various proposed techniques from reading the literature alone. Variations in the datasets, metrics, and baseline results have all contributed to this difficulty. This paper argues that a more unified approach is required, and that a centralised software platform should be developed to aid researchers in making comparisons between their algorithms and others. The desirable qualities of such a system have been identified and proposed, and an early prototype has been developed. Re-implementing algorithms published by other researchers is a great burden on those proposing new techniques. The prototype system has the potential to greatly reduce this burden and thus encourage more comparable results being generated and published more easily.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107223325",
                    "name": "W. Huang"
                },
                {
                    "authorId": "2108169214",
                    "name": "Junyi Chen"
                },
                {
                    "authorId": "46643379",
                    "name": "L. Meng"
                },
                {
                    "authorId": "143707305",
                    "name": "David Lillis"
                }
            ]
        }
    ]
}