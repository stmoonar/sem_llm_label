{
    "authorId": "31856827",
    "papers": [
        {
            "paperId": "73d2e0a9efd55c5ca460ce374f1745351f29bebd",
            "title": "D-Extract: Extracting Dimensional Attributes From Product Images",
            "abstract": "Product dimension is a crucial piece of information enabling customers make better buying decisions. E-commerce websites extract dimension attributes to enable customers filter the search results according to their requirements. The existing methods extract dimension attributes from textual data like title and product description. However, this textual information often exists in an ambiguous, disorganised structure. In comparison, images can be used to extract reliable and consistent dimensional information. With this motivation, we hereby propose two novel architecture to extract dimensional information from product images. The first namely Single-Box Classification Net-work is designed to classify each text token in the image, one at a time, whereas the second architecture namely Multi-Box Classification Network uses a transformer network to classify all the detected text tokens simultaneously. To attain better performance, the proposed architectures are also fused with statistical inferences derived from the product category which further increased the F1-score of the Single-Box Classification Network by 3.78% and Multi-Box Classification Network by \u2248 0.9%\u2248. We use distance super-vision technique to create a large scale automated dataset for pretraining purpose and notice considerable improvement when the models were pretrained on the large data before finetuning. The proposed model achieves a desirable precision of 91.54% at 89.75% recall and outperforms the other state of the art approaches by \u2248 4.76% in F1-score1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "103878325",
                    "name": "Pushpendu Ghosh"
                },
                {
                    "authorId": "31856827",
                    "name": "N. Wang"
                },
                {
                    "authorId": "31024864",
                    "name": "Promod Yenigalla"
                }
            ]
        },
        {
            "paperId": "46efbc5108e3176ebb6f4df74d38ba16f6d3ed0c",
            "title": "SemEval-2021 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS)",
            "abstract": "Understanding tables is an important and relevant task that involves understanding table structure as well as being able to compare and contrast information within cells. In this paper, we address this challenge by presenting a new dataset and tasks that addresses this goal in a shared task in SemEval 2020 Task 9: Fact Verification and Evidence Finding for Tabular Data in Scientific Documents (SEM-TAB-FACTS). Our dataset contains 981 manually-generated tables and an auto-generated dataset of 1980 tables providing over 180K statement and over 16M evidence annotations. SEM-TAB-FACTS featured two sub-tasks. In sub-task A, the goal was to determine if a statement is supported, refuted or unknown in relation to a table. In sub-task B, the focus was on identifying the specific cells of a table that provide evidence for the statement. 69 teams signed up to participate in the task with 19 successful submissions to subtask A and 12 successful submissions to subtask B. We present our results and main findings from the competition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31856827",
                    "name": "N. Wang"
                },
                {
                    "authorId": "40665082",
                    "name": "Diwakar Mahajan"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                },
                {
                    "authorId": "144063596",
                    "name": "Sara Rosenthal"
                }
            ]
        },
        {
            "paperId": "759314189ecdffdc979deebf334646e847d93b1b",
            "title": "KAAPA: Knowledge Aware Answers from PDF Analysis",
            "abstract": "We present KaaPa (Knowledge Aware Answers from Pdf Analysis), an integrated solution for machine reading comprehension over both text and tables extracted from PDFs. KaaPa enables interactive question refinement using facets generated from an automatically induced Knowledge Graph. In addition it provides a concise summary of the supporting evidence for the provided answers by aggregating information across multiple sources. KaaPa can be applied consistently to any collection of documents in English with zero domain adaptation effort. We showcase the use of KaaPa for QA on scientific literature using the COVID-19 Open Research Dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107085016",
                    "name": "Nicolas R. Fauceglia"
                },
                {
                    "authorId": "1888104",
                    "name": "Mustafa Canim"
                },
                {
                    "authorId": "1711133",
                    "name": "A. Gliozzo"
                },
                {
                    "authorId": "50685486",
                    "name": "Jennifer J. Liang"
                },
                {
                    "authorId": "31856827",
                    "name": "N. Wang"
                },
                {
                    "authorId": "143894459",
                    "name": "D. Burdick"
                },
                {
                    "authorId": "2689774",
                    "name": "Nandana Mihindukulasooriya"
                },
                {
                    "authorId": "2879453",
                    "name": "Vittorio Castelli"
                },
                {
                    "authorId": "2169082",
                    "name": "Guy Feigenblat"
                },
                {
                    "authorId": "1775524",
                    "name": "D. Konopnicki"
                },
                {
                    "authorId": "2208580",
                    "name": "Yannis Katsis"
                },
                {
                    "authorId": "1707117",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "1573872877",
                    "name": "Yunyao Li"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                },
                {
                    "authorId": "2707234",
                    "name": "Avirup Sil"
                }
            ]
        },
        {
            "paperId": "77c5ebea69b0c11653d79c2c721e456d6bcc8b3a",
            "title": "D2S: Document-to-Slide Generation Via Query-Based Text Summarization",
            "abstract": "Presentations are critical for communication in all areas of our lives, yet the creation of slide decks is often tedious and time-consuming. There has been limited research aiming to automate the document-to-slides generation process and all face a critical challenge: no publicly available dataset for training and benchmarking. In this work, we first contribute a new dataset, SciDuet, consisting of pairs of papers and their corresponding slides decks from recent years\u2019 NLP and ML conferences (e.g., ACL). Secondly, we present D2S, a novel system that tackles the document-to-slides task with a two-step approach: 1) Use slide titles to retrieve relevant and engaging text, figures, and tables; 2) Summarize the retrieved context into bullet points with long-form question answering. Our evaluation suggests that long-form QA outperforms state-of-the-art summarization baselines on both automated ROUGE metrics and qualitative human evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057901326",
                    "name": "Edward Sun"
                },
                {
                    "authorId": "39517968",
                    "name": "Yufang Hou"
                },
                {
                    "authorId": "36743361",
                    "name": "Dakuo Wang"
                },
                {
                    "authorId": "2108127520",
                    "name": "Yunfeng Zhang"
                },
                {
                    "authorId": "31856827",
                    "name": "N. Wang"
                }
            ]
        },
        {
            "paperId": "9dbb26e4f04f38aa2a11289cbc82a2d9ed533741",
            "title": "TableLab: An Interactive Table Extraction System with Adaptive Deep Learning",
            "abstract": "Table extraction from PDF and image documents is a ubiquitous task in the real-world. Perfect extraction quality is difficult to achieve with one single out-of-box model due to (1) the wide variety of table styles, (2) the lack of training data representing this variety and (3) the inherent ambiguity and subjectivity of table definitions between end-users. Meanwhile, building customized models from scratch can be difficult due to the expensive nature of annotating table data. We attempt to solve these challenges with TableLab by providing a system where users and models seamlessly work together to quickly customize high-quality extraction models with a few labelled examples for the user\u2019s document collection, which contains pages with tables. Given an input document collection, TableLab first detects tables with similar structures (templates) by clustering embeddings from the extraction model. Document collections often contain tables created with a limited set of templates or similar structures. It then selects a few representative table examples already extracted with a pre-trained base deep learning model. Via an easy-to-use user interface, users provide feedback to these selections without necessarily having to identify every single error. TableLab then applies such feedback to finetune the pre-trained model and returns the results of the finetuned model back to the user. The user can choose to repeat this process iteratively until obtaining a customized model with satisfactory performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31856827",
                    "name": "N. Wang"
                },
                {
                    "authorId": "143894459",
                    "name": "D. Burdick"
                },
                {
                    "authorId": "1718694",
                    "name": "Yunyao Li"
                }
            ]
        },
        {
            "paperId": "73a906a988e54defee536a120125f957059d595e",
            "title": "Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context",
            "abstract": "Documents are often used for knowledge sharing and preservation in business and science, within which are tables that capture most of the critical data. Unfortunately, most documents are stored and distributed as PDF or scanned images, which fail to preserve logical table structure. Recent vision-based deep learning approaches have been proposed to address this gap, but most still cannot achieve state-of-the-art results. We present Global Table Extractor (GTE), a vision-guided systematic framework for joint table detection and cell structured recognition, which could be built on top of any object detection model. With GTE-Table, we invent a new penalty based on the natural cell containment constraint of tables to train our table network aided by cell location predictions. GTE-Cell is a new hierarchical cell detection network that leverages table styles. Further, we design a method to automatically label table and cell structure in existing documents to cheaply create a large corpus of training and test data. We use this to enhance PubTabNet with cell labels and create FinTabNet, real-world and complex scientific and financial datasets with detailed table structure annotations to help train and test structure recognition. Our framework surpasses previous state-of-the-art results on the ICDAR 2013 and ICDAR 2019 table competition in both table detection and cell structure recognition. Further experiments demonstrate a greater than 45% improvement in cell structure recognition when compared to a vanilla RetinaNet object detection model in our new out-of-domain FinTabNet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1662767314",
                    "name": "Xinyi Zheng"
                },
                {
                    "authorId": "143894459",
                    "name": "D. Burdick"
                },
                {
                    "authorId": "145378077",
                    "name": "Lucian Popa"
                },
                {
                    "authorId": "31856827",
                    "name": "N. Wang"
                }
            ]
        },
        {
            "paperId": "751b3b064f41a78c267eb969f6b7617015b0a901",
            "title": "Table extraction and understanding for scientific and enterprise applications",
            "abstract": "Valuable high-precision data are often published in the form of tables in both scientific and business documents. While humans can easily identify, interpret and contextualize tables, developing general-purpose automated techniques for extraction of information from tables is difficult due to the wide variety of table formats employed across corpora. To extract useful data from tables, data cells must be correctly extracted and linked to all relevant headers, units of measure and in-text references. Table extraction involves identifying the border and cell structure for each document table, while table understanding provides context by linking cells with semantic information inside and outside the table, such as row and column headers, footnotes, titles, and references in surrounding text. The objective of this tutorial is to provide a detailed synopsis of existing approaches for table extraction and understanding, highlight open research problems, and provide an overview of potential applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143894459",
                    "name": "D. Burdick"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                },
                {
                    "authorId": "3351526",
                    "name": "A. Evfimievski"
                },
                {
                    "authorId": "2208580",
                    "name": "Yannis Katsis"
                },
                {
                    "authorId": "31856827",
                    "name": "N. Wang"
                }
            ]
        }
    ]
}