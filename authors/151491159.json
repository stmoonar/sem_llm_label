{
    "authorId": "151491159",
    "papers": [
        {
            "paperId": "c1f1c8d47643e0e564a1a04de4d766b0ba326905",
            "title": "Mapping Flood Exposure, Damage, and Population Needs Using Remote and Social Sensing: A Case Study of 2022 Pakistan Floods",
            "abstract": "The devastating 2022 floods in Pakistan resulted in a catastrophe impacting millions of people and destroying thousands of homes. While disaster management efforts were taken, crisis responders struggled to understand the country-wide flood extent, population exposure, urgent needs of affected people, and various types of damage. To tackle this challenge, we leverage remote and social sensing with geospatial data using state-of-the-art machine learning techniques for text and image processing. Our satellite-based analysis over a one-month period (25 Aug\u201325 Sep) revealed that 11.48% of Pakistan was inundated. When combined with geospatial data, this meant 18.9 million people were at risk across 160 districts in Pakistan, with adults constituting 50% of the exposed population. Our social sensing data analysis surfaced 106.7k reports pertaining to deaths, injuries, and concerns of the affected people. To understand the urgent needs of the affected population, we analyzed tweet texts and found that South Karachi, Chitral and North Waziristan required the most basic necessities like food and shelter. Further analysis of tweet images revealed that Lasbela, Rajanpur, and Jhal Magsi had the highest damage reports normalized by their population. These damage reports were found to correlate strongly with affected people reports and need reports, achieving an R-Square of 0.96 and 0.94, respectively. Our extensive study shows that combining remote sensing, social sensing, and geospatial data can provide accurate and timely information during a disaster event, which is crucial in prioritizing areas for immediate and gradual response.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30821216",
                    "name": "Zainab Akhtar"
                },
                {
                    "authorId": "2161481311",
                    "name": "Umair Qazi"
                },
                {
                    "authorId": "40898432",
                    "name": "Rizwan Sadiq"
                },
                {
                    "authorId": "2213594502",
                    "name": "Aya El-Sakka"
                },
                {
                    "authorId": "2065795043",
                    "name": "M. Sajjad"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                }
            ]
        },
        {
            "paperId": "446a44d50426d4d210c75d565fd4af98c3fb61dc",
            "title": "Bias-Aware Face Mask Detection Dataset",
            "abstract": "In December 2019, a novel coronavirus (COVID-19) spread so quickly around the world that many countries had to set mandatory face mask rules in public areas to reduce the transmission of the virus. To monitor public adherence, researchers aimed to rapidly develop efficient systems that can detect faces with masks automatically. However, the lack of representative and novel datasets posed challenges for training efficient models. Early attempts to collect face mask datasets did not account for potential race, gender, and age biases. Therefore, the resulting models show inherent biases toward specific race groups, such as Asian or Caucasian. In this work, we present a novel face mask detection dataset that contains images posted on Twitter during the pandemic from around the world. Unlike previous datasets, the proposed Bias-Aware Face Mask Detection (BAFMD) dataset contains more images from underrepresented races and age groups to mitigate the problem of the face mask detection task. We perform experiments to investigate potential biases in widely used face mask detection datasets and illustrate that the BAFMD dataset yields models with better performance and generalization ability. The dataset is publicly available at https://github.com/Alpkant/BAFMD.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1410151432",
                    "name": "Alperen Kantarci"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "3025777",
                    "name": "H. K. Ekenel"
                }
            ]
        },
        {
            "paperId": "52663ccd18921692b1efa53e9d7a35f6cd632d5a",
            "title": "Incidents1M: A Large-Scale Dataset of Images With Natural Disasters, Damage, and Incidents",
            "abstract": "Natural disasters, such as floods, tornadoes, or wildfires, are increasingly pervasive as the Earth undergoes global warming. It is difficult to predict when and where an incident will occur, so timely emergency response is critical to saving the lives of those endangered by destructive events. Fortunately, technology can play a role in these situations. Social media posts can be used as a low-latency data source to understand the progression and aftermath of a disaster, yet parsing this data is tedious without automated methods. Prior work has mostly focused on text-based filtering, yet image and video-based filtering remains largely unexplored. In this work, we present the Incidents1M Dataset, a large-scale multi-label dataset which contains 977,088 images, with 43 incident and 49 place categories. We provide details of the dataset construction, statistics and potential biases; introduce and train a model for incident detection; and perform image-filtering experiments on millions of images on Flickr and Twitter. We also present some applications on incident analysis to encourage and enable future work in computer vision for humanitarian aid. Code, data, and models are available at http://incidentsdataset.csail.mit.edu.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "143605244",
                    "name": "Ethan Weber"
                },
                {
                    "authorId": "1749373",
                    "name": "Dim P. Papadopoulos"
                },
                {
                    "authorId": "2677488",
                    "name": "\u00c0gata Lapedriza"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "143805211",
                    "name": "A. Torralba"
                }
            ]
        },
        {
            "paperId": "75fb040f5731bba853ecd092c768b56b92d72f83",
            "title": "AI for Disaster Rapid Damage Assessment from Microblogs",
            "abstract": "Formal response organizations perform rapid damage assessments after natural and human-induced disasters to measure the extent of damage to infrastructures such as roads, bridges, and buildings. This time-critical task, when performed using traditional approaches such as experts surveying the disaster areas, poses serious challenges and delays response. This paper presents an AI-based system that leverages citizen science to collect damage images reported on social media and perform rapid damage assessment in real-time. Several image processing models in the system tackle non-trivial challenges posed by social media as a data source, such as high-volume of redundant and irrelevant content. The system determines the severity of damage using a state-of-the-art computer vision model. Together with a response organization in the US, we deployed the system to identify damage reports during a major real-world disaster. We observe that almost 42% of the images are unique, 28% relevant, and more importantly, only 10% of them contain either mild or severe damage. Experts from our partner organization provided feedback on the system's mistakes, which we used to perform additional experiments to retrain the models. Consequently, the retrained models based on expert feedback on the target domain data helped us achieve significant performance improvements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "2161481311",
                    "name": "Umair Qazi"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "2053615208",
                    "name": "Steve Peterson"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                }
            ]
        },
        {
            "paperId": "0b0de688cf88182f4a5063b80ae6aca42751f388",
            "title": "Mapping Vulnerable Populations with AI",
            "abstract": "Humanitarian actions require accurate information to efficiently delegate support operations. Such information can be maps of building footprints, building functions, and population densities. While the access to this information is comparably easy in industrialized countries thanks to reliable census data and national geo-data infrastructures, this is not the case for developing countries, where that data is often incomplete or outdated. Building maps derived from remote sensing images may partially remedy this challenge in such countries, but are not always accurate due to different landscape configurations and lack of validation data. Even when they exist, building footprint layers usually do not reveal more fine-grained building properties, such as the number of stories or the building's function (e.g., office, residential, school, etc.). In this project we aim to automate building footprint and function mapping using heterogeneous data sources. In a first step, we intend to delineate buildings from satellite data, using deep learning models for semantic image segmentation. Building functions shall be retrieved by parsing social media data like for instance tweets, as well as ground-based imagery, to automatically identify different buildings functions and retrieve further information such as the number of building stories. Building maps augmented with those additional attributes make it possible to derive more accurate population density maps, needed to support the targeted provision of humanitarian aid.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "31394603",
                    "name": "B. Kellenberger"
                },
                {
                    "authorId": "1405141088",
                    "name": "John E. Vargas-Mu\u00f1oz"
                },
                {
                    "authorId": "2977931",
                    "name": "D. Tuia"
                },
                {
                    "authorId": "51262831",
                    "name": "R. C. Daudt"
                },
                {
                    "authorId": "144810819",
                    "name": "K. Schindler"
                },
                {
                    "authorId": "2121391358",
                    "name": "Thao T-T Whelan"
                },
                {
                    "authorId": "2007427293",
                    "name": "Brenda Ayo"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                }
            ]
        },
        {
            "paperId": "39b374b7060297dabd666e957f8de31699f2ea46",
            "title": "Towards Using Remote Sensing and Social Media Data for Flood Mapping",
            "abstract": "Ghana\u2019s capital, the Greater Accra Metropolitan Area, is vulnerable to flooding. This paper proposes a fusion of satellite imagery and social media data to derive informed flood extent maps and to understand affected population reports during a flood disaster. We use a change detection technique and present an automatic thresholding approach for flood extent mapping using Sentinel-1 images. We explore four different speckle filters and compare them using the VV, VH and VV/VH polarizations to determine the best polarization(s) for delineating flood extents. The VV and VH bands together on Perona-Malik filtered images achieved the highest accuracy with an F1-score of 81.6%. Moreover, we add social media data layers representing tweet text and images posted from the region to highlight different ways these heterogeneous data sources can be used. The obtained signals complement each other and help find flooded roads and areas which are not identified by the satellite imagery analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30821216",
                    "name": "Zainab Akhtar"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                }
            ]
        },
        {
            "paperId": "462cc2046ef4d48d844813b66d8a1ed6dfda3bc0",
            "title": "HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep Learning Benchmarks",
            "abstract": "Social networks are widely used for information consumption and dissemination, especially during time-critical events such as natural disasters. Despite its significantly large volume, social media content is often too noisy for direct use in any application. Therefore, it is important to filter, categorize, and concisely summarize the available content to facilitate effective consumption and decision-making. To address such issues automatic classification systems have been developed using supervised modeling approaches, thanks to the earlier efforts on creating labeled datasets. However, existing datasets are limited in different aspects (e.g., size, contains duplicates) and less suitable to support more advanced and data-hungry deep learning models. In this paper, we present a new large-scale dataset with ~77K human-labeled tweets, sampled from a pool of ~24 million tweets across 19 disaster events that happened between 2016 and 2019. Moreover, we propose a data collection and sampling pipeline, which is important for social media data sampling for human annotation. We report multiclass classification results using classic and deep learning (fastText and transformer) based models to set the ground for future studies. The dataset and associated resources are publicly available at https://crisisnlp.qcri.org/humaid_dataset.html.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "48166270",
                    "name": "U. Qazi"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                }
            ]
        },
        {
            "paperId": "6edf6f6548c6ad8299a85df1c4a90f6e5e6e345c",
            "title": "Fight Detection from Still Images in the Wild",
            "abstract": "Detecting fights from still images shared on social media is an important task required to limit the distribution of violent scenes in order to prevent their negative effects. For this reason, in this study, we address the problem of fight detection from still images collected from the web and social media. We explore how well one can detect fights from just a single still image. We also propose a new dataset, named Social Media Fight Images (SMFI), comprising real-world images of fight actions. Results of the extensive experiments on the proposed dataset show that fight actions can be recognized successfully from still images. That is, even without exploiting the temporal information, it is possible to detect fights with high accuracy by utilizing appearance only. We also perform cross-dataset experiments to evaluate the representation capacity of the collected dataset. These experiments indicate that, as in the other computer vision problems, there exists a dataset bias for the fight recognition problem. Although the methods achieve close to 100% accuracy when trained and tested on the same fight dataset, the cross-dataset accuracies are significantly lower, i.e., around 70% when more representative datasets are used for training. SMFI dataset is found to be one of the two most representative datasets among the utilized five fight datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1471439125",
                    "name": "Seymanur Akti"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "3025777",
                    "name": "H. K. Ekenel"
                }
            ]
        },
        {
            "paperId": "71f481010dcbc89b1e40196f2f0576297c99b695",
            "title": "Non-traditional data sources",
            "abstract": "THE WORLD IS facing enormous challenges, ranging from climate change to extreme poverty. The 2030 Agenda for Sustainable Development and its 17 Sustainable Development Goals (SDGs)a were adopted by United Nations Member States in 2015 as an operational framework to address these challenges. The SDGs include No Poverty, Quality Education, Gender Equality, Peace, Justice and Strong Institutions, among others, as well as a meta goal on Partnerships for the Goals. Despite limitations, the SDGs form a",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1684687",
                    "name": "Ingmar Weber"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "144229664",
                    "name": "F. Mrad"
                },
                {
                    "authorId": "147046671",
                    "name": "Jennifer Colville"
                },
                {
                    "authorId": "2057046985",
                    "name": "Mehdi Fathallah"
                },
                {
                    "authorId": "114079734",
                    "name": "A. Chaker"
                },
                {
                    "authorId": "2057047829",
                    "name": "Wigdan Seed Ahmed"
                }
            ]
        },
        {
            "paperId": "8233b42219fb46e3424e0b4ca4a553918a791c4a",
            "title": "Robust Training of Social Media Image Classification Models",
            "abstract": "Images shared on social media help crisis managers gain situational awareness and assess incurred damages, among other response tasks. As the volume and velocity of such content are typically high, real-time image classification has become an urgent need for faster disaster response. Recent advances in computer vision and deep neural networks have enabled the development of models for image classification for a number of tasks, including detecting crisis incidents, filtering irrelevant images, classifying images into specific humanitarian categories, and assessing the severity of the damage. To develop robust models, it is necessary to understand the capability of the publicly available pretrained models for these tasks, which remains to be underexplored in the crisis informatics literature. In this study, we address such limitations by investigating ten different network architectures for four different tasks using the largest publicly available datasets for these tasks. We also explore various data augmentation strategies, semisupervised techniques, and a multitask learning setup. In our extensive experiments, we achieve promising results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "1992544035",
                    "name": "Tanvirul Alam"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                }
            ]
        },
        {
            "paperId": "a4f8a5eae756e2149aaadbc4fbff4898414c76b9",
            "title": "Social Media Images Classification Models for Real-time Disaster Response",
            "abstract": "Images shared on social media help crisis managers in terms of gaining situational awareness and assessing incurred damages, among other response tasks. As the volume and velocity of such content are really high, therefore, real-time image classi\ufb01cation became an urgent need in order to take a faster response. Recent advances in computer vision and deep neural networks have enabled the development of models for real-time image classi\ufb01cation for a number of tasks, including detecting crisis incidents, \ufb01ltering irrelevant images, classifying images into speci\ufb01c humanitarian categories, and assessing the severity of the damage. For developing real-time robust models, it is necessary to understand the capability of the publicly available pretrained models for these tasks. In the current state-of-art of crisis informatics, it is under-explored. In this study, we address such limitations. We investigate ten di\ufb00erent architectures for four di\ufb00erent tasks using the largest publicly available datasets for these tasks. We also explore the data augmentation, semi-supervised techniques, and a multitask setup. In our extensive experiments, we achieve promising results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "1992544035",
                    "name": "Tanvirul Alam"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                }
            ]
        },
        {
            "paperId": "aff72adf305b71e73d5f4cdd3dc3f8c3aa18ede8",
            "title": "TBCOV: Two Billion Multilingual COVID-19 Tweets with Sentiment, Entity, Geo, and Gender Labels",
            "abstract": "As the world struggles with several compounded challenges caused by the COVID-19 pandemic in the health, economic, and social domains, timely access to disaggregated national and sub-national data are important to understand the emergent situation but it is difficult to obtain. The widespread usage of social networking sites, especially during mass convergence events, such as health emergencies, provides instant access to citizen-generated data offering rich information about public opinions, sentiments, and situational updates useful for authorities to gain insights. We offer a large-scale social sensing dataset comprising two billion multilingual tweets posted from 218 countries by 87 million users in 67 languages. We used state-of-the-art machine learning models to enrich the data with sentiment labels and named-entities. Additionally, a gender identification approach is proposed to segregate user gender. Furthermore, a geolocalization approach is devised to geotag tweets at country, state, county, and city granularities, enabling a myriad of data analysis tasks to understand real-world issues at national and sub-national levels. We believe this multilingual data with broader geographical and longer temporal coverage will be a cornerstone for researchers to study impacts of the ongoing global health catastrophe and to manage adverse consequences related to people\u2019s health, livelihood, and social well-being.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "48166270",
                    "name": "U. Qazi"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                }
            ]
        },
        {
            "paperId": "090bf8a2fc5286bd2e3194ced78e9a0a5dc73c4a",
            "title": "Rapid Damage Assessment Using Social Media Images by Combining Human and Machine Intelligence",
            "abstract": "Rapid damage assessment is one of the core tasks that response organizations perform at the onset of a disaster to understand the scale of damage to infrastructures such as roads, bridges, and buildings. This work analyzes the usefulness of social media imagery content to perform rapid damage assessment during a real-world disaster. An automatic image processing system, which was activated in collaboration with a volunteer response organization, processed ~280K images to understand the extent of damage caused by the disaster. The system achieved an accuracy of 76% computed based on the feedback received from the domain experts who analyzed ~29K system-processed images during the disaster. An extensive error analysis reveals several insights and challenges faced by the system, which are vital for the research community to advance this line of research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "48166270",
                    "name": "U. Qazi"
                },
                {
                    "authorId": "2053615208",
                    "name": "Steve Peterson"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                }
            ]
        },
        {
            "paperId": "12c12405c20bca738d070370f9ce6785e12e9c17",
            "title": "Deep Learning Benchmarks and Datasets for Social Media Image Classification for Disaster Response",
            "abstract": "During a disaster event, images shared on social media helps crisis managers gain situational awareness and assess incurred damages, among other response tasks. Recent advances in computer vision and deep neural networks have enabled the development of models for real-time image classification for a number of tasks, including detecting crisis incidents, filtering irrelevant images, classifying images into specific humanitarian categories, and assessing the severity of damage. Despite several efforts, past works mainly suffer from limited resources (i.e., labeled images) available to train more robust deep learning models. In this study, we propose new datasets for disaster type detection, and informativeness classification, and damage severity assessment. Moreover, we relabel existing publicly available datasets for new tasks. We identify exact- and near-duplicates to form non-overlapping data splits, and finally consolidate them to create larger datasets. In our extensive experiments, we benchmark several state-of-the-art deep learning models and achieve promising results. We release our datasets and models publicly, aiming to provide proper baselines as well as to spur further research in the crisis informatics community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "1992544035",
                    "name": "Tanvirul Alam"
                },
                {
                    "authorId": "48166270",
                    "name": "U. Qazi"
                }
            ]
        },
        {
            "paperId": "4a888afd8fb93e9d294acdc7022ee20a5db8d617",
            "title": "Analysis of Social Media Data using Multimodal Deep Learning for Disaster Response",
            "abstract": "Multimedia content in social media platforms provides significant information during disaster events. The types of information shared include reports of injured or deceased people, infrastructure damage, and missing or found people, among others. Although many studies have shown the usefulness of both text and image content for disaster response purposes, the research has been mostly focused on analyzing only the text modality in the past. In this paper, we propose to use both text and image modalities of social media data to learn a joint representation using state-of-the-art deep learning techniques. Specifically, we utilize convolutional neural networks to define a multimodal deep learning architecture with a modality-agnostic shared representation. Extensive experiments on real-world disaster datasets show that the proposed multimodal architecture yields better performance than models trained using a single modality (e.g., either text or image).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                }
            ]
        },
        {
            "paperId": "577cf6d39c04ee27db40c87c93b63300a787ff65",
            "title": "Standardizing and Benchmarking Crisis-related Social Media Datasets for Humanitarian Information Processing",
            "abstract": "Time-critical analysis of social media streams is important for humanitarian organizations to plan rapid response during disasters. The crisis informatics research community has developed several techniques and systems to process and classify big crisis related data posted on social media. However, due to the dispersed nature of the datasets used in the literature, it is not possible to compare the results and measure the progress made towards better models for crisis informatics. In this work, we attempt to bridge this gap by standardizing various existing crisis-related datasets. We consolidate labels of eight annotated data sources and provide 166.1k and 141.5k tweets for informativeness and humanitarian classification tasks, respectively. The consolidation results in a larger dataset that affords the ability to train more sophisticated models. To that end, we provide baseline results using CNN and BERT models. We make the dataset available at this https URL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "145775792",
                    "name": "Hassan Sajjad"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                }
            ]
        },
        {
            "paperId": "768e9954d407429f7dc5a13f7f1c12c980203f65",
            "title": "GeoCoV19",
            "abstract": "The past several years have witnessed a huge surge in the use of social media platforms during mass convergence events such as health emergencies, natural or human-induced disasters. These non-traditional data sources are becoming vital for disease forecasts and surveillance when preparing for epidemic and pandemic outbreaks. In this paper, we present GeoCoV19, a large-scale Twitter dataset containing more than 524 million multilingual tweets posted over a period of 90 days since February 1, 2020. Moreover, we employ a gazetteer-based approach to infer the geolocation of tweets. We postulate that this largescale, multilingual, geolocated social media data can empower the research communities to evaluate how societies are collectively coping with this unprecedented global crisis as well as to develop computational methods to address challenges such as identifying fake news, understanding communities' knowledge gaps, building disease forecast and surveillance models, among others.",
            "fieldsOfStudy": [
                "Computer Science",
                "Geography"
            ],
            "authors": [
                {
                    "authorId": "48166270",
                    "name": "U. Qazi"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "48046557",
                    "name": "Ferda Ofli"
                }
            ]
        },
        {
            "paperId": "48575a6f07629a021b41a38791b00f05d82bf7d9",
            "title": "Requirements for Software to Support the use of Social Media in Emergency Management: A Delphi Study",
            "abstract": "Social Media contain a wealth of information that could improve the situational awareness of Emergency Managers during a crisis, but many barriers stand in the way. These include information overload, making it impossible to deal with the flood of raw posts, and lack of trust in unverified crowdsourced data. The purpose of this project is to build a communications bridge between emergency responders and technologists who can provide the advances needed to realize social media\u2019s full potential. We are employing a Delphi study survey design, which is a technique for exploring and developing consensus among a group of experts around a particular topic. Participants include emergency managers and technologists with experience in software to support the use of social media in crisis response, from many countries. The topics of the study are described and preliminary, partial results presented for Round 1 of the study, based on 33 responses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1749200",
                    "name": "S. R. Hiltz"
                },
                {
                    "authorId": "2523772",
                    "name": "Amanda Lee Hughes"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "37617381",
                    "name": "L. Plotnick"
                },
                {
                    "authorId": "144122487",
                    "name": "R. Power"
                },
                {
                    "authorId": "1810042",
                    "name": "M. Turoff"
                }
            ]
        },
        {
            "paperId": "56694321e323ce250c09aea4d5c35fe694649abe",
            "title": "A Hybrid Domain Adaptation Approach for Identifying Crisis-Relevant Tweets",
            "abstract": "Huge amounts of data generated on social media during emergency situations is regarded as a trove of critical information. The use of supervised machine learning techniques in the early stages of a crisis is challenged by the lack of labeled data for that event. Furthermore, supervised models trained on labeled data from a prior crisis may not produce accurate results, due to inherent crisis variations. To address these challenges, the authors propose a hybrid feature-instance-parameter adaptation approach based on matrix factorization, k-nearest neighbors, and self-training. The proposed feature-instance adaptation selects a subset of the source crisis data that is representative for the target crisis data. The selected labeled source data, together with unlabeled target data, are used to learn self-training domain adaptation classifiers for the target crisis. Experimental results have shown that overall the hybrid domain adaptation classifiers perform better than the supervised classifiers learned from the original source data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145635275",
                    "name": "R. Mazloom"
                },
                {
                    "authorId": "2052120901",
                    "name": "Hongmin Li"
                },
                {
                    "authorId": "1736546",
                    "name": "Doina Caragea"
                },
                {
                    "authorId": "1690656",
                    "name": "Cornelia Caragea"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                }
            ]
        },
        {
            "paperId": "789ea84f44e03b3d50bb85987fb237b7765df317",
            "title": "CrisisDPS: Crisis Data Processing Services",
            "abstract": "Over the last few years, extensive research has been conducted to develop technologies to support humanitarian aid tasks. However, many technologies are still limited as they require both manual and automatic approaches, and more importantly, are not ready to be integrated into the disaster response work\ufb02ows. To tackle this limitation, we develop automatic data processing services that are freely and publicly available, and made to be simple, e\ufb03cient, and accessible to non-experts. Our services take textual messages (e.g., tweets, Facebook posts, SMS) as input to determine (i) which disaster type the message belongs to, (ii) whether it is informative or not, and (iii) what type of humanitarian information it conveys. We built our services upon machine learning classi\ufb01ers that are obtained from large-scale comparative experiments utilizing both classical and deep learning algorithms. Our services outperform state-of-the-art publicly available tools in terms of classi\ufb01cation accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                }
            ]
        },
        {
            "paperId": "7be15c84ace4fe135221cce26693a210d25d1f69",
            "title": "#CampFireMissing: An Analysis of Tweets About Missing and Found People From California Wildfires",
            "abstract": "Several research studies have shown the importance of social media data for humanitarian aid. Among others, the issue of missing and lost people during disasters and emergencies is crucial for disaster managers. This work analyzes Twitter data from a recent wildfire event to determine its usefulness for the mitigation of the missing and found people issue. Data analysis performed using various filtering techniques, and trend analysis revealed that Twitter contains important information potentially useful for emergency managers and volunteers to tackle this issue. Many tweets were found containing full names, partial names, location information, and other vital clues which could be useful for finding missing people.",
            "fieldsOfStudy": [
                "Computer Science",
                "Geography"
            ],
            "authors": [
                {
                    "authorId": "1392494624",
                    "name": "Humaira Waqas"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                }
            ]
        },
        {
            "paperId": "f2944e3c0fe4908afeb421fff3bc8965c44b6aae",
            "title": "Descriptive and visual summaries of disaster events using artificial intelligence techniques: case studies of Hurricanes Harvey, Irma, and Maria",
            "abstract": "ABSTRACT People increasingly use microblogging platforms such as Twitter during natural disasters and emergencies. Research studies have revealed the usefulness of the data available on Twitter for several disaster response tasks. However, making sense of social media data is a challenging task due to several reasons such as limitations of available tools to analyse high-volume and high-velocity data streams, dealing with information overload, among others. To eliminate such limitations, in this work, we first show that textual and imagery content on social media provide complementary information useful to improve situational awareness. We then explore ways in which various Artificial Intelligence techniques from Natural Language Processing and Computer Vision fields can exploit such complementary information generated during disaster events. Finally, we propose a methodological approach that combines several computational techniques effectively in a unified framework to help humanitarian organisations in their relief efforts. We conduct extensive experiments using textual and imagery content from millions of tweets posted during the three major disaster events in the 2017 Atlantic Hurricane season. Our study reveals that the distributions of various types of useful information can inform crisis managers and responders and facilitate the development of future automated systems for disaster management.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                }
            ]
        },
        {
            "paperId": "f888646ef8e8c0a466772a648aa6c1cc96d6b1da",
            "title": "Summarizing Situational Tweets in Crisis Scenarios: An Extractive-Abstractive Approach",
            "abstract": "Microblogging platforms such as Twitter are widely used by eyewitnesses and affected people to post situational updates during mass convergence events such as natural and man-made disasters. These crisis-related messages disperse among multiple classes/categories such as infrastructure damage, shelter needs, information about missing, injured, and dead people. Moreover, we observe that sometimes people post information about their missing relatives and friends with personal details such as names and last seen location. The information requirements of different stakeholders (government, NGOs, and rescue workers) also vary a lot. This brings twofold challenges: 1) extracting important high-level situational updates from these messages, assigning them appropriate categories, and finally summarizing big trove of information in each category and 2) extracting small-scale time-critical sparse updates related to missing or trapped people. In this article, we propose a classification-summarization framework which first assigns tweets into different situational classes and then summarizes those tweets. In the summarization phase, we propose a two-step extractive-abstractive summarization framework. In the first step, it extracts a set of important tweets from the whole set of information, develops a bigram-based word-graph from those tweets, and generates paths by traversing the word-graph. Next, it uses an optimization technique based on integer linear programming (ILP) to select the most important tweets and paths based on different optimization parameters such as informativeness and coverage of content words. Apart from general classwise summarization, we also show the customization of our summarization model to address time-critical sparse information needs (e.g., missing relatives). Our proposed method is time- and memory-efficient and shows better performance than state-of-the-art methods in terms of both quantitative and qualitative judgment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2042376",
                    "name": "Koustav Rudra"
                },
                {
                    "authorId": "51130504",
                    "name": "Pawan Goyal"
                },
                {
                    "authorId": "4213990",
                    "name": "Niloy Ganguly"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "143930195",
                    "name": "P. Mitra"
                }
            ]
        },
        {
            "paperId": "4caf1ce25f7076091be80f205196dd5f557a9a0f",
            "title": "Artificial Intelligence and Social Media to Aid Disaster Response and Management",
            "abstract": "Extended AbstractPeople increasingly use social media such as Facebook and Twitter during disasters and emergencies. Research studies have demonstrated the usefulness of social media information for a number of humanitarian relief operations ranging from situational awareness to actionable information extraction. Moreover, the use of social media platforms during sudden-onset disasters could potentially bridge the information scarcity issue, especially in the early hours when few other information sources are available. In this work, we analyzed Twitter content (textual messages and images) posted during the recent devastating hurricanes namely Harvey and Maria. We employed state of the art artificial intelligence techniques to process millions of textual messages and images shared on Twitter to understand the types of information available on social media and how emergency response organizations can leverage this information to aid their relief operations. Furthermore, we employed deep neural networks techniques to analyze the imagery content to assess the severity of damage shown in the images. Damage severity assessment is one of the core tasks for many humanitarian organization.To perform data collection and analysis, we employed our Artificial Intelligence for Digital Response (AIDR) technology. AIDR combines human computation and machine learning techniques to train machine learning models specialized to fulfill specific information needs of humanitarian organizations. Many humanitarian organizations such as UN OCHA, UNICEF have used the AIDR technology during many major disasters in the past including the 2015 Nepal earthquake, the 2014 typhoon Hagupit and typhoon Ruby, among others. Next, we provide a brief overview of our analysis during the two aforementioned hurricanes.Hurricane Harvey Case StudyHurricane Harvey was an extremely devastating storm that made landfall to Port Aransas and Port O'Connor, Texas, in the United States on August 24-25, 2017. We collected and analyzed around 4 million Twitter messages to determine how many of these messages are, for example, reporting some kind of infrastructure damage, or reports of injured or dead people, missing or found people, displacements and evacuation, donation and volunteers reports. Furthermore, we also analyzed geotagged tweets to determine the types of information originate from the disaster-hit areas compared to neighboring areas. For instance, we generated maps of different cities in the US in and around the hurricane hit areas. Figure 1 shows the map of geotagged tweets reporting different types of useful information from Florida, USA. According to the results obtained from the AIDR classifiers, both caution and advice and sympathy and support categories are more prominent than other informational categories such as donation and volunteering. In addition to the textual content processing of the collected tweets, we perform automatic image processing to collect and analyze imagery content posted on Twitter during Hurricane Harvey. For this purpose, we employ state-of-the-art deep learning techniques. One of the classifiers deployed in this case was the damage-level assessment. The damage-level assessment task aims to predict the level of damage in one out of three damage levels i.e., SEVERE damage, MILD damage, and NO damage. Our analysis revealed that most of the images (\u223c86%) do not contain any damage signs or considered irrelevant containing advertisements, cartoons, banners, and other irrelevant content. Of the remaining set, 10% of the images contain MILD damage, and only \u223c4% of them show SEVERE damage. However, finding these 10% (MILD) or 4% (SEVERE) useful images is like finding a needle in a giant haystack. Artificial intelligence techniques such as employed by the AIDR platform are hugely useful to overcome such information overload issues and help decision-makers to process large amounts of data in a timely manner.Fig. 1: Geotagged tweets from Florida, USA.Hurricane Maria Case StudyAn even more devastating hurricane than the Harvey that hit Puerto Rico and nearby areas was hurricane Maria. Damaged roofs, uprooted trees, widespread flooding were among the scenes on the path of Hurricane Maria, a Category 5 hurricane that slammed Dominica and Puerto Rico and has caused at least 78 deaths including 30 in Dominica and 34 in Puerto Rico, and many more left without homes, electricity, food, and drinking water.We activated AIDR on September 20, 2017 to collect tweets related to Hurricane Maria. More than 2 million tweets were collected. Figure 2 shows the distribution of the daily tweet counts. To understand what these tweets are about, we applied our tweet text classifier which was originally trained (F1\u00a0=\u00a00.64) on more than 30k human-labeled tweets from a number of past disasters. AIDR's image processing pipeline was also activated to identify images that show infrastructure damage due to Hurricane Maria. Around 80k tweets contained images. However, \u223c75% of these images were duplicate. The remaining 25% (\u223c20k) images were automatically classified by the AIDR's damage assessment classifier into three classes as before. Figure 2: Tweets count per dayWe believe that more information can be extracted from image about the devastation caused by the disaster than relying solely on the textual content provided by the users. Even though it is in the testing phase, our image processing pipeline does a decent job in identifying images that show MILD or SEVERE damage. Instead of trying to look at all the images, humanitarian organizations and emergency responders can simply take a look at the retained set of MILD or SEVERE damage images to get a quick sense of the level of destruction incurred by the disaster.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                },
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "1919784",
                    "name": "Micha\u00ebl Aupetit"
                }
            ]
        },
        {
            "paperId": "ac4350bd4eb2273809f7a39807a225b4f84fa626",
            "title": "Localizing and Quantifying Damage in Social Media Images",
            "abstract": "Traditional post-disaster assessment of damage heavily relies on expensive GIS data, especially remote sensing image data. In recent years, social media has become a rich source of disaster information that may be useful in assessing damage at a lower cost. Such information includes text (e.g., tweets) or images posted by eyewitnesses of a disaster. Most of the existing research explores the use of text in identifying situational awareness information useful for disaster response teams. The use of social media images to assess disaster damage is limited. In this paper, we propose a novel approach, based on convolutional neural networks and class activation maps, to locate damage in a disaster image and to quantify the degree of the damage. Our proposed approach enables the use of social network images for post-disaster damage assessment, and provides an inexpensive and feasible alternative to the more expensive GIS approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12092030",
                    "name": "Xukun Li"
                },
                {
                    "authorId": "151495735",
                    "name": "Xukun Li"
                },
                {
                    "authorId": "151505506",
                    "name": "Doina Caragea"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                }
            ]
        },
        {
            "paperId": "f8cd9e3da23d01ee7487be29ca251b6ad20889af",
            "title": "Combining Human Computing and Machine Learning to Make Sense of Big (Aerial) Data for Disaster Response",
            "abstract": "Aerial imagery captured via unmanned aerial vehicles (UAVs) is playing an increasingly important role in disaster response. Unlike satellite imagery, aerial imagery can be captured and processed within hours rather than days. In addition, the spatial resolution of aerial imagery is an order of magnitude higher than the imagery produced by the most sophisticated commercial satellites today. Both the United States Federal Emergency Management Agency (FEMA) and the European Commission's Joint Research Center (JRC) have noted that aerial imagery will inevitably present a big data challenge. The purpose of this article is to get ahead of this future challenge by proposing a hybrid crowdsourcing and real-time machine learning solution to rapidly process large volumes of aerial data for disaster response in a time-sensitive manner. Crowdsourcing can be used to annotate features of interest in aerial images (such as damaged shelters and roads blocked by debris). These human-annotated features can then be used to train a supervised machine learning system to learn to recognize such features in new unseen images. In this article, we describe how this hybrid solution for image analysis can be implemented as a module (i.e., Aerial Clicker) to extend an existing platform called Artificial Intelligence for Disaster Response (AIDR), which has already been deployed to classify microblog messages during disasters using its Text Clicker module and in response to Cyclone Pam, a category 5 cyclone that devastated Vanuatu in March 2015. The hybrid solution we present can be applied to both aerial and satellite imagery and has applications beyond disaster response such as wildlife protection, human rights, and archeological exploration. As a proof of concept, we recently piloted this solution using very high-resolution aerial photographs of a wildlife reserve in Namibia to support rangers with their wildlife conservation efforts (SAVMAP project, http://lasig.epfl.ch/savmap ). The results suggest that the platform we have developed to combine crowdsourcing and machine learning to make sense of large volumes of aerial images can be used for disaster response.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1727159",
                    "name": "Ferda Ofli"
                },
                {
                    "authorId": "49317293",
                    "name": "P. Meier"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2977931",
                    "name": "D. Tuia"
                },
                {
                    "authorId": "2070318733",
                    "name": "Nicolas Rey"
                },
                {
                    "authorId": "1382155500",
                    "name": "Julien Briant"
                },
                {
                    "authorId": "3348860",
                    "name": "P. Rudd"
                },
                {
                    "authorId": "144152549",
                    "name": "F. Reinhard"
                },
                {
                    "authorId": "144939013",
                    "name": "Matthew Parkan"
                },
                {
                    "authorId": "2436529",
                    "name": "S. Joost"
                }
            ]
        },
        {
            "paperId": "9397d5ae8c7d8119887819067d70107356fe33e6",
            "title": "AIDR: artificial intelligence for disaster response",
            "abstract": "We present AIDR (Artificial Intelligence for Disaster Response), a platform designed to perform automatic classification of crisis-related microblog communications. AIDR enables humans and machines to work together to apply human intelligence to large-scale data at high speed. The objective of AIDR is to classify messages that people post during disasters into a set of user-defined categories of information (e.g., \"needs\", \"damage\", etc.) For this purpose, the system continuously ingests data from Twitter, processes it (i.e., using machine learning classification techniques) and leverages human-participation (through crowdsourcing) in real-time. AIDR has been successfully tested to classify informative vs. non-informative tweets posted during the 2013 Pakistan Earthquake. Overall, we achieved a classification quality (measured using AUC) of 80%. AIDR is available at http://aidr.qcri.org/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "2081346",
                    "name": "J. Lucas"
                },
                {
                    "authorId": "49317293",
                    "name": "P. Meier"
                },
                {
                    "authorId": "3681090",
                    "name": "Sarah Vieweg"
                }
            ]
        },
        {
            "paperId": "d0d60c2bfca32b3e6d8feee815a3a132a2f47499",
            "title": "Practical extraction of disaster-relevant information from social media",
            "abstract": "During times of disasters online users generate a significant amount of data, some of which are extremely valuable for relief efforts. In this paper, we study the nature of social-media content generated during two different natural disasters. We also train a model based on conditional random fields to extract valuable information from such content. We evaluate our techniques over our two datasets through a set of carefully designed experiments. We also test our methods over a non-disaster dataset to show that our extraction model is useful for extracting information from socially-generated content in general.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "1863248",
                    "name": "Shady Elbassuoni"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                },
                {
                    "authorId": "145472333",
                    "name": "Fernando Diaz"
                },
                {
                    "authorId": "49317293",
                    "name": "P. Meier"
                }
            ]
        },
        {
            "paperId": "ef54bf37d60f37f07c958783fc56021311205d5c",
            "title": "Tweet4act: Using incident-specific profiles for classifying crisis-related messages",
            "abstract": "We present Tweet4act, a system to detect and classify crisis-related messages communicated over a microblogging platform. Our system relies on extracting content features from each message. These features and the use of an incident-specific dictionary allow us to determine the period type of an incident that each message belongs to. The period types are: pre-incident (messages talking about prevention, mitigation, and preparedness), during-incident (messages sent while the incident is taking place), and post-incident (messages related to the response, recovery, and reconstruction). We show that our detection method can effectively identify incident-related messages with high precision and recall, and that our incident-period classification method outperforms standard machine learning classification methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "37884573",
                    "name": "Soudip Roy Chowdhury"
                },
                {
                    "authorId": "151491159",
                    "name": "Muhammad Imran"
                },
                {
                    "authorId": "35065376",
                    "name": "M. R. Asghar"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "153191671",
                    "name": "Carlos Castillo"
                }
            ]
        }
    ]
}