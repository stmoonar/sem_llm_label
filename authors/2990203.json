{
    "authorId": "2990203",
    "papers": [
        {
            "paperId": "0d7f7043393fc4267c283ec35e31e5d720e7af7d",
            "title": "Bias, diversity, and challenges to fairness in classification and automated text analysis. From libraries to AI and back",
            "abstract": "Libraries are increasingly relying on computational methods, including methods from Artificial Intelligence (AI). This increasing usage raises concerns about the risks of AI that are currently broadly discussed in scientific literature, the media and law-making. In this article we investigate the risks surrounding bias and unfairness in AI usage in classification and automated text analysis within the context of library applications. We describe examples that show how the library community has been aware of such risks for a long time, and how it has developed and deployed countermeasures. We take a closer look at the notion of '(un)fairness' in relation to the notion of 'diversity', and we investigate a formalisation of diversity that models both inclusion and distribution. We argue that many of the unfairness problems of automated content analysis can also be regarded through the lens of diversity and the countermeasures taken to enhance diversity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                },
                {
                    "authorId": "2106778051",
                    "name": "\u00d6zg\u00fcr Karadeniz"
                },
                {
                    "authorId": "2199118261",
                    "name": "Sercan Kiyak"
                },
                {
                    "authorId": "47836888",
                    "name": "Stefan Mertens"
                },
                {
                    "authorId": "1389967827",
                    "name": "L. d\u2019Haenens"
                }
            ]
        },
        {
            "paperId": "675945cf520459ca048bcff2a1d26424f3095775",
            "title": "\u201cWe try to empower them\u201d - Exploring Future Technologies to Support Migrant Jobseekers",
            "abstract": "Previous work on technology in Public Employment Services and job market chances has focused on profiling systems that are intended for tasks such as assessing and classifying jobseekers. To integrate into the local job market, migrants and refugees seek support from the Public Employment Services (PES), but also non-profit, non-governmental organizations (herein referred to as third sector organizations, or TSOs). How do design visions for technologies to support jobseekers change when developed not under bureaucratic rules but by people interacting directly and informally with jobseekers? We focus on the perspectives of TSO workers assisting migrants and refugees seeking support for their job search. Through interviews and a design fiction exercise, we investigate (1) the role of TSO workers, (2) factors beyond those used in profiling systems that they consider relevant, and (3) their ideal technology. We describe how TSO workers contextualize formal criteria used in profiling systems while prioritising jobseekers\u2019 personal interests and strengths. Based on our findings on existing tools and methods, and imagined future technologies, we propose a software-based project that expands existing job taxonomies into a coordinated resource combining job characteristics, required competencies, and soft skills to support multiple informational tools for jobseekers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171416894",
                    "name": "Sonja Mei Wang"
                },
                {
                    "authorId": "2171370539",
                    "name": "Kristen M. Scott"
                },
                {
                    "authorId": "2219930032",
                    "name": "Margarita Artemenko"
                },
                {
                    "authorId": "1491520578",
                    "name": "Milagros Miceli"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                }
            ]
        },
        {
            "paperId": "82040a4a1b1bceb5216a9dd09e5d9c8fa3fdd83f",
            "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification",
            "abstract": "To mitigate gender bias in contextualized language models, different intrinsic mitigation strategies have been proposed, alongside many bias metrics. Considering that the end use of these language models is for downstream tasks like text classification, it is important to understand how these intrinsic bias mitigation strategies actually translate to fairness in downstream tasks and the extent of this.In this work, we design a probe to investigate the effects that some of the major intrinsic gender bias mitigation strategies have on downstream text classification tasks. We discover that instead of resolving gender bias, intrinsic mitigation techniques and metrics are able to hide it in such a way that significant gender information is retained in the embeddings. Furthermore, we show that each mitigation technique is able to hide the bias from some of the intrinsic bias measures but not all, and each intrinsic bias measure can be fooled by some mitigation techniques, but not all. We confirm experimentally, that none of the intrinsic mitigation techniques used without any other fairness intervention is able to consistently impact extrinsic bias. We recommend that intrinsic bias mitigation techniques should be combined with other fairness interventions for downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145259446",
                    "name": "E. Tokpo"
                },
                {
                    "authorId": "150258834",
                    "name": "Pieter Delobelle"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                },
                {
                    "authorId": "1709830",
                    "name": "T. Calders"
                }
            ]
        },
        {
            "paperId": "950a6812364995b13ee364901eae32ed32d5e398",
            "title": "Domain Adaptive Decision Trees: Implications for Accuracy and Fairness",
            "abstract": "In uses of pre-trained machine learning models, it is a known issue that the target population in which the model is being deployed may not have been reflected in the source population with which the model was trained. This can result in a biased model when deployed, leading to a reduction in model performance. One risk is that, as the population changes, certain demographic groups will be under-served or otherwise disadvantaged by the model, even as they become more represented in the target population. The field of domain adaptation proposes techniques for a situation where label data for the target population does not exist, but some information about the target distribution does exist. In this paper we contribute to the domain adaptation literature by introducing domain-adaptive decision trees (DADT). We focus on decision trees given their growing popularity due to their interpretability and performance relative to other more complex models. With DADT we aim to improve the accuracy of models trained in a source domain (or training data) that differs from the target domain (or test data). We propose an in-processing step that adjusts the information gain split criterion with outside information corresponding to the distribution of the target population. We demonstrate DADT on real data and find that it improves accuracy over a standard decision tree when testing in a shifted target population. We also study the change in fairness under demographic parity and equal opportunity. Results show an improvement in fairness with the use of DADT.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2974008",
                    "name": "J. \u00c1lvarez"
                },
                {
                    "authorId": "2171370539",
                    "name": "Kristen M. Scott"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                },
                {
                    "authorId": "2194703719",
                    "name": "Salvatore Ruggieri"
                }
            ]
        },
        {
            "paperId": "f7f2ae84378a9feaafc0714281c004a5e3873b54",
            "title": "ResumeTailor: Improving Resume Quality Through Co-Creative Tools",
            "abstract": ". Clear and well-written resumes can help jobseekers \ufb01nd better and better-suited jobs. However, many people struggle with writing their resumes, especially if they just entered the job market. Although many tools have been created to help write resumes, an analysis we conducted showed us that these tools focus mainly on layout and only give very limited content-related support. This paper presents a co-creative resume building tool that provides tailored advice to jobseekers. It is based on a comprehensive computational analysis of 444k resumes and the development of a Dutch language model, ResumeRobBERT, to provide contextual suggestions. Through the analysis of the resumes, we found that some expected sections, such as language pro\ufb01ciency, are often missing entirely, while conversely some resumes contain unexpected content, such as negative personality traits. This implies that jobseekers could bene\ufb01t from more guidance when writing resumes. We aim to support them in the resume-writing process through our tool ResumeTailor, a co-creative resume building tool that gives textual suggestions and provides a template for important resume sections.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150258834",
                    "name": "Pieter Delobelle"
                },
                {
                    "authorId": "2171416894",
                    "name": "Sonja Mei Wang"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                }
            ]
        },
        {
            "paperId": "058dee85d522f6565fe1502cafcf9a5e3f6a6f0e",
            "title": "Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models",
            "abstract": "An increasing awareness of biased patterns in natural language processing resources such as BERT has motivated many metrics to quantify \u2018bias\u2019 and \u2018fairness\u2019 in these resources. However, comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the literature on fairness metrics for pre-trained language models and experimentally evaluate compatibility, including both biases in language models and in their downstream tasks. We do this by combining traditional literature survey, correlation analysis and empirical evaluations. We find that many metrics are not compatible with each other and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. We also see no tangible evidence of intrinsic bias relating to extrinsic bias. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective. To improve future comparisons and fairness evaluations, we recommend to avoid embedding-based metrics and focus on fairness evaluations in downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150258834",
                    "name": "Pieter Delobelle"
                },
                {
                    "authorId": "2145259446",
                    "name": "E. Tokpo"
                },
                {
                    "authorId": "1709830",
                    "name": "T. Calders"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                }
            ]
        },
        {
            "paperId": "3c33ff87ea67c1d32fccacbd3c233fe4c8659483",
            "title": "5th Workshop on Fairness in User Modeling, Adaptation, and Personalization (FairUMAP 2022)",
            "abstract": "ACM Reference Format: Styliani Kleanthous, Bamshad Mobasher, Tsvika Kuflik, Bettina Berendt, Robin Burke, Jahna Otterbacher, Nasim Sonboli, and Avital Shulner Tal. 2022. 5th Workshop on Fairness in User Modeling, Adaptation, and Personalization (FairUMAP 2022). In Adjunct Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization (UMAP \u201922 Adjunct), July 4\u20137, 2022, Barcelona, Spain. ACM, New York, NY, USA, Article 111, 2 pages. https://doi.org/10.1145/3511047.3536348",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2478971",
                    "name": "S. Kleanthous"
                },
                {
                    "authorId": "1684679",
                    "name": "B. Mobasher"
                },
                {
                    "authorId": "1770193",
                    "name": "T. Kuflik"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                },
                {
                    "authorId": "2056064458",
                    "name": "R. Burke"
                },
                {
                    "authorId": "2216583",
                    "name": "Jahna Otterbacher"
                },
                {
                    "authorId": "14472354",
                    "name": "Nasim Sonboli"
                },
                {
                    "authorId": "2075154007",
                    "name": "Avital Shulner-Tal"
                }
            ]
        },
        {
            "paperId": "60d4da04889eb33268ebf58b3b8be730d2877ceb",
            "title": "RobBERT-2022: Updating a Dutch Language Model to Account for Evolving Language Use",
            "abstract": "Large transformer-based language models, e.g. BERT and GPT-3, outperform previous architectures on most natural language processing tasks. Such language models are first pre-trained on gigantic corpora of text and later used as base-model for finetuning on a particular task. Since the pre-training step is usually not repeated, base models are not up-to-date with the latest information. In this paper, we update RobBERT, a RoBERTa-based state-of-the-art Dutch language model, which was trained in 2019. First, the tokenizer of RobBERT is updated to include new high-frequent tokens present in the latest Dutch OSCAR corpus, e.g. corona-related words. Then we further pre-train the RobBERT model using this dataset. To evaluate if our new model is a plug-in replacement for RobBERT, we introduce two additional criteria based on concept drift of existing tokens and alignment for novel tokens.We found that for certain language tasks this update results in a significant performance increase. These results highlight the benefit of continually updating a language model to account for evolving language use.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150258834",
                    "name": "Pieter Delobelle"
                },
                {
                    "authorId": "144329659",
                    "name": "Thomas Winters"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                }
            ]
        },
        {
            "paperId": "8f6deba290850fb05ab0a65fff8605d75a87260f",
            "title": "Political representation bias in DBpedia and Wikidata as a challenge for downstream processing",
            "abstract": "Diversity Searcher is a tool originally developed to help analyse diversity in news media texts. It relies on a form of automated content analysis and thus rests on prior assumptions and depends on certain design choices related to diversity and fairness. One such design choice is the external knowledge source(s) used. In this article, we discuss implications that these sources can have on the results of content analysis. We compare two data sources that Diversity Searcher has worked with - DBpedia and Wikidata - with respect to their ontological coverage and diversity, and describe implications for the resulting analyses of text corpora. We describe a case study of the relative over- or under-representation of Belgian political parties between 1990 and 2020 in the English-language DBpedia, the Dutch-language DBpedia, and Wikidata, and highlight the many decisions needed with regard to the design of this data analysis and the assumptions behind it, as well as implications from the results. In particular, we came across a staggering over-representation of the political right in the English-language DBpedia.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2106778051",
                    "name": "\u00d6zg\u00fcr Karadeniz"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                },
                {
                    "authorId": "2199118261",
                    "name": "Sercan Kiyak"
                },
                {
                    "authorId": "47836888",
                    "name": "Stefan Mertens"
                },
                {
                    "authorId": "1389967827",
                    "name": "L. d\u2019Haenens"
                }
            ]
        },
        {
            "paperId": "91642d9c31bf9021cd02e50fa6049236de750702",
            "title": "FairDistillation: Mitigating Stereotyping in Language Models",
            "abstract": "Large pre-trained language models are successfully being used in a variety of tasks, across many languages. With this ever-increasing usage, the risk of harmful side effects also rises, for example by reproducing and reinforcing stereotypes. However, detecting and mitigating these harms is difficult to do in general and becomes computationally expensive when tackling multiple languages or when considering different biases. To address this, we present FairDistillation: a cross-lingual method based on knowledge distillation to construct smaller language models while controlling for specific biases. We found that our distillation method does not negatively affect the downstream performance on most tasks and successfully mitigates stereotyping and representational harms. We demonstrate that FairDistillation can create fairer language models at a considerably lower cost than alternative approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150258834",
                    "name": "Pieter Delobelle"
                },
                {
                    "authorId": "2990203",
                    "name": "Bettina Berendt"
                }
            ]
        }
    ]
}