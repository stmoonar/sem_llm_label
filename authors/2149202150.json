{
    "authorId": "2149202150",
    "papers": [
        {
            "paperId": "072d387ad3436f97bce984066c53c1b2e77b1ad9",
            "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents",
            "abstract": "Leveraging the rapid development of Large Language Models LLMs, LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis on the different forms of agent backdoor attacks. Specifically, from the perspective of the final attacking outcomes, the attacker can either choose to manipulate the final output distribution, or only introduce malicious behavior in the intermediate reasoning process, while keeping the final output correct. Furthermore, the former category can be divided into two subcategories based on trigger locations: the backdoor trigger can be hidden either in the user query or in an intermediate observation returned by the external environment. We propose the corresponding data poisoning mechanisms to implement the above variations of agent backdoor attacks on two typical agent tasks, web shopping and tool utilization. Extensive experiments show that LLM-based agents suffer severely from backdoor attacks, indicating an urgent need for further research on the development of defenses against backdoor attacks on LLM-based agents. Warning: This paper may contain biased content.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2120801160",
                    "name": "Wenkai Yang"
                },
                {
                    "authorId": "51255245",
                    "name": "Xiaohan Bi"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2111638473",
                    "name": "Sishuo Chen"
                },
                {
                    "authorId": "2266995673",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2130282986",
                    "name": "Xu Sun"
                }
            ]
        },
        {
            "paperId": "deb44bb1bc3b61a6b6a11d671947974da3ee4f73",
            "title": "Exploring Universal Intrinsic Task Subspace for Few-Shot Learning via Prompt Tuning",
            "abstract": "Why can pre-trained language models (PLMs) learn universal representations and effectively adapt to broad NLP tasks differing a lot superficially? In this work, we empirically find evidence indicating that the adaptations of PLMs to various few-shot tasks can be reparameterized as optimizing only a few free parameters in a unified low-dimensional intrinsic task subspace, which may help us understand why PLMs could easily adapt to various NLP tasks with small-scale data. To find such a subspace and examine its universality, we propose an analysis pipeline called intrinsic prompt tuning (IPT). Specifically, we resort to the recent success of prompt tuning and decompose the soft prompts of multiple NLP tasks into the same low-dimensional nonlinear subspace, then we learn to adapt the PLM to unseen data or tasks by only tuning parameters in this subspace. In the experiments, we study diverse few-shot NLP tasks and surprisingly find that in a 250-dimensional subspace found with 100 tasks, by only tuning 250 free parameters, we can recover 97% and 83% of the full prompt tuning performance for 100 seen tasks (using different training data) and 20 unseen tasks, respectively, showing great generalization ability of the found intrinsic task subspace. Besides being an analysis tool, IPTcould further help us improve the prompt tuning stability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "48631777",
                    "name": "Xiaozhi Wang"
                },
                {
                    "authorId": "48576745",
                    "name": "Yusheng Su"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2284766400",
                    "name": "Ning Ding"
                },
                {
                    "authorId": "2106388389",
                    "name": "Jing Yi"
                },
                {
                    "authorId": "2109136284",
                    "name": "Weize Chen"
                },
                {
                    "authorId": "2308485553",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2294164848",
                    "name": "Juanzi Li"
                },
                {
                    "authorId": "2301159894",
                    "name": "Lei Hou"
                },
                {
                    "authorId": "2209965245",
                    "name": "Peng Li"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                },
                {
                    "authorId": "2257088385",
                    "name": "Jie Zhou"
                }
            ]
        },
        {
            "paperId": "f840fdcd9e95bba5476030c000d82a172e984181",
            "title": "Exploring Backdoor Vulnerabilities of Chat Models",
            "abstract": "Recent researches have shown that Large Language Models (LLMs) are susceptible to a security threat known as Backdoor Attack. The backdoored model will behave well in normal cases but exhibit malicious behaviours on inputs inserted with a specific backdoor trigger. Current backdoor studies on LLMs predominantly focus on instruction-tuned LLMs, while neglecting another realistic scenario where LLMs are fine-tuned on multi-turn conversational data to be chat models. Chat models are extensively adopted across various real-world scenarios, thus the security of chat models deserves increasing attention. Unfortunately, we point out that the flexible multi-turn interaction format instead increases the flexibility of trigger designs and amplifies the vulnerability of chat models to backdoor attacks. In this work, we reveal and achieve a novel backdoor attacking method on chat models by distributing multiple trigger scenarios across user inputs in different rounds, and making the backdoor be triggered only when all trigger scenarios have appeared in the historical conversations. Experimental results demonstrate that our method can achieve high attack success rates (e.g., over 90% ASR on Vicuna-7B) while successfully maintaining the normal capabilities of chat models on providing helpful responses to benign user requests. Also, the backdoor can not be easily removed by the downstream re-alignment, highlighting the importance of continued research and attention to the security concerns of chat models. Warning: This paper may contain toxic content.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2295515003",
                    "name": "Yunzhuo Hao"
                },
                {
                    "authorId": "2120801160",
                    "name": "Wenkai Yang"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                }
            ]
        },
        {
            "paperId": "024a25b2445ecb3a181c5e2f39fbf8b73a4c1a6f",
            "title": "Emergent Modularity in Pre-trained Transformers",
            "abstract": "This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore finding a structure that groups neurons into modules by function, and each module works for its corresponding function. Given the enormous amount of possible structures, we focus on Mixture-of-Experts as a promising candidate, which partitions neurons into experts and usually activates different experts for different inputs. Experimental results show that there are functional experts, where clustered are the neurons specialized in a certain function. Moreover, perturbing the activations of functional experts significantly affects the corresponding function. Finally, we study how modularity emerges during pre-training, and find that the modular structure is stabilized at the early stage, which is faster than neuron stabilization. It suggests that Transformers first construct the modular structure and then learn fine-grained neuron functions. Our code and data are available at https://github.com/THUNLP/modularity-analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2621696",
                    "name": "Zhengyan Zhang"
                },
                {
                    "authorId": "2150468823",
                    "name": "Zhiyuan Zeng"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "51131083",
                    "name": "Chaojun Xiao"
                },
                {
                    "authorId": "48631777",
                    "name": "Xiaozhi Wang"
                },
                {
                    "authorId": "48506411",
                    "name": "Xu Han"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "3360722",
                    "name": "Ruobing Xie"
                },
                {
                    "authorId": "1753344",
                    "name": "Maosong Sun"
                },
                {
                    "authorId": "49640256",
                    "name": "Jie Zhou"
                }
            ]
        },
        {
            "paperId": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
            "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
            "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "2163374235",
                    "name": "Shi Liang"
                },
                {
                    "authorId": "2114059497",
                    "name": "Yining Ye"
                },
                {
                    "authorId": "2214586034",
                    "name": "Kunlun Zhu"
                },
                {
                    "authorId": "2214613855",
                    "name": "Lan Yan"
                },
                {
                    "authorId": "2191753738",
                    "name": "Ya-Ting Lu"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2214579778",
                    "name": "Xin Cong"
                },
                {
                    "authorId": "47274259",
                    "name": "Xiangru Tang"
                },
                {
                    "authorId": "2226120351",
                    "name": "Bill Qian"
                },
                {
                    "authorId": "2226184989",
                    "name": "Sihan Zhao"
                },
                {
                    "authorId": "2214603370",
                    "name": "Runchu Tian"
                },
                {
                    "authorId": "3360722",
                    "name": "Ruobing Xie"
                },
                {
                    "authorId": "49640256",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "152573911",
                    "name": "M. Gerstein"
                },
                {
                    "authorId": "2144118403",
                    "name": "Dahai Li"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "1753344",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "352420ee61a8da783ca7750170793613b18b8d9c",
            "title": "Tool Learning with Foundation Models",
            "abstract": "Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool learning framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate the generalization in tool learning. Considering the lack of a systematic tool learning evaluation in prior works, we experiment with 18 representative tools and show the potential of current foundation models in skillfully utilizing tools. Finally, we discuss several open problems that require further investigation for tool learning. In general, we hope this paper could inspire future research in integrating tools with foundation models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "1576223501",
                    "name": "Shengding Hu"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2109136284",
                    "name": "Weize Chen"
                },
                {
                    "authorId": "46649145",
                    "name": "Ning Ding"
                },
                {
                    "authorId": "52297757",
                    "name": "Ganqu Cui"
                },
                {
                    "authorId": "1633538428",
                    "name": "Zheni Zeng"
                },
                {
                    "authorId": "2115640120",
                    "name": "Yufei Huang"
                },
                {
                    "authorId": "51131083",
                    "name": "Chaojun Xiao"
                },
                {
                    "authorId": "2118642562",
                    "name": "Chi Han"
                },
                {
                    "authorId": "51135899",
                    "name": "Y. Fung"
                },
                {
                    "authorId": "48576745",
                    "name": "Yusheng Su"
                },
                {
                    "authorId": "2155242767",
                    "name": "Huadong Wang"
                },
                {
                    "authorId": "2082473972",
                    "name": "Cheng Qian"
                },
                {
                    "authorId": "2214603370",
                    "name": "Runchu Tian"
                },
                {
                    "authorId": "2214586034",
                    "name": "Kunlun Zhu"
                },
                {
                    "authorId": "2163374235",
                    "name": "Shi Liang"
                },
                {
                    "authorId": "145781166",
                    "name": "Xingyu Shen"
                },
                {
                    "authorId": "2052218689",
                    "name": "Bokai Xu"
                },
                {
                    "authorId": "2170500945",
                    "name": "Zhen Zhang"
                },
                {
                    "authorId": "2114059497",
                    "name": "Yining Ye"
                },
                {
                    "authorId": "2155882844",
                    "name": "Bo Li"
                },
                {
                    "authorId": "2214664440",
                    "name": "Ziwei Tang"
                },
                {
                    "authorId": "2106388389",
                    "name": "Jing Yi"
                },
                {
                    "authorId": "2109388429",
                    "name": "Yu Zhu"
                },
                {
                    "authorId": "2146517842",
                    "name": "Zhenning Dai"
                },
                {
                    "authorId": "2214613855",
                    "name": "Lan Yan"
                },
                {
                    "authorId": "2214579778",
                    "name": "Xin Cong"
                },
                {
                    "authorId": "2191753738",
                    "name": "Ya-Ting Lu"
                },
                {
                    "authorId": "2150606888",
                    "name": "Weilin Zhao"
                },
                {
                    "authorId": "2214586078",
                    "name": "Yuxiang Huang"
                },
                {
                    "authorId": "2213334016",
                    "name": "Jun-Han Yan"
                },
                {
                    "authorId": "48506411",
                    "name": "Xu Han"
                },
                {
                    "authorId": "2143570016",
                    "name": "Xian Sun"
                },
                {
                    "authorId": "2144118403",
                    "name": "Dahai Li"
                },
                {
                    "authorId": "80842917",
                    "name": "Jason Phang"
                },
                {
                    "authorId": "3443627",
                    "name": "Cheng Yang"
                },
                {
                    "authorId": "2116417519",
                    "name": "Tongshuang Wu"
                },
                {
                    "authorId": "2072975663",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "1753344",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "3aff8ba10ab822ff30d57f1d4a0c71af3facaca6",
            "title": "Towards Codable Text Watermarking for Large Language Models",
            "abstract": "As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by LLMs by injecting hidden patterns into the generated texts. However, we argue that existing watermarking methods for LLMs are encoding-inefficient (only contain one bit of information whether it is generated from an LLM or not) and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.) in different LLMs application scenarios. In this work, we conduct the first systematic study on the topic of Codable Text Watermarking for LLMs (CTWL) that allows text watermarks to carry more customizable information. First of all, we study the taxonomy of LLM watermarking technology and give a mathematical formulation for CTWL. Additionally, we provide a comprehensive evaluation system for CTWL: (1) watermarking success rate, (2) robustness against various corruptions, (3) coding rate of payload information, (4) encoding and decoding efficiency, (5) impacts on the quality of the generated text. To meet the requirements of these non-Paretoimproving metrics, we devise a CTWL method named Balance-Marking, based on the motivation of ensuring that available and unavailable vocabularies for encoding information have approximately equivalent probabilities. Compared to the random vocabulary partitioning extended from the existing work, a probabilitybalanced vocabulary partition can significantly improve the quality of the generated text. Extensive experimental results have shown that our method outperforms a direct baseline under comprehensive evaluation. We hope this work can raise the community\u2019s awareness of the importance of CTWL and inspire further research on designing more efficient, practical, and robust watermarking methods for LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189508204",
                    "name": "Lean Wang"
                },
                {
                    "authorId": "2120801160",
                    "name": "Wenkai Yang"
                },
                {
                    "authorId": "49025779",
                    "name": "Deli Chen"
                },
                {
                    "authorId": "2209520407",
                    "name": "Hao Zhou"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "33427918",
                    "name": "Fandong Meng"
                },
                {
                    "authorId": "2273496881",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2130282986",
                    "name": "Xu Sun"
                }
            ]
        },
        {
            "paperId": "3ff93fff401a89a3696c4b64372098da119ebe4f",
            "title": "Stochastic Bridges as Effective Regularizers for Parameter-Efficient Tuning",
            "abstract": "Parameter-efficient tuning methods (PETs) have achieved promising results in tuning large pre-trained language models (PLMs). By formalizing frozen PLMs and additional tunable parameters as systems and controls respectively, PETs can be theoretically grounded to optimal control and further viewed as optimizing the terminal cost and running cost in the optimal control literature. Despite the elegance of this theoretical grounding, in practice, existing PETs often ignore the running cost and only optimize the terminal cost, i.e., focus on optimizing the loss function of the output state, regardless of the running cost that depends on the intermediate states. Since it is non-trivial to directly model the intermediate states and design a running cost function, we propose to use latent stochastic bridges to regularize the intermediate states and use the regularization as the running cost of PETs. As the first work to propose regularized PETs that use stochastic bridges as the regularizers (running costs) for the intermediate states, we show the effectiveness and generality of this regularization across different tasks, PLMs and PETs. In view of the great potential and capacity, we believe more sophisticated regularizers can be designed for PETs and better performance can be achieved in the future. The code is released at \\url{https://github.com/thunlp/stochastic-bridge-pet/tree/main}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109136284",
                    "name": "Weize Chen"
                },
                {
                    "authorId": "48506411",
                    "name": "Xu Han"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "1753344",
                    "name": "Maosong Sun"
                },
                {
                    "authorId": "49640256",
                    "name": "Jie Zhou"
                }
            ]
        },
        {
            "paperId": "6e08eec2f487c092a86db6e9666e47d152eb97d6",
            "title": "Exploring the Impact of Model Scaling on Parameter-Efficient Tuning",
            "abstract": "Parameter-efficient tuning (PET) methods can effectively drive extremely large pre-trained language models (PLMs) by training only minimal parameters. Different PET methods utilize different manually designed tunable modules. In small PLMs, there are usually noticeable performance differences among PET methods. Nevertheless, as the model scale increases, the performance differences become marginal. Hence, we hypothesize that model scaling mitigates the impact of design differences on PET methods. To investigate this hypothesis, we introduce a more flexible PET method called Arbitrary PET (APET) method. The APET method is compatible with a tunable module, which consists of any number of parameters distributed in arbitrary positions. Then, we utilize it and conduct experiments on 11 NLP tasks across 3 representative PLMs. Our investigations reveal that model scaling (1) mitigates the effects of the positions of tunable parameters on performance, and (2) enables tuning methods to achieve performance comparable to full-parameter fine-tuning by optimizing fewer tunable parameters. Intriguingly, we also observe that tuning methods optimize the similar number of tunable parameters to exceed random guess performance on different tasks. We collectively discuss this phenomenon and the two aforementioned findings from an optimization perspective to understand the underlying mechanisms. These conclusions enhance our understanding of the impact of model scaling on PET and assist in designing more effective and efficient PET methods for PLMs of different scales. The source code can be obtained from this GitHub repository: \\url{https://github.com/yushengsu-thu/PET_Scaling}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48576745",
                    "name": "Yusheng Su"
                },
                {
                    "authorId": "2151547817",
                    "name": "Chi-Min Chan"
                },
                {
                    "authorId": "2210063335",
                    "name": "Jiali Cheng"
                },
                {
                    "authorId": "50625437",
                    "name": "Yujia Qin"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "1576223501",
                    "name": "Shengding Hu"
                },
                {
                    "authorId": "19343873",
                    "name": "Zonghan Yang"
                },
                {
                    "authorId": "46649145",
                    "name": "Ning Ding"
                },
                {
                    "authorId": "2141313179",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "1753344",
                    "name": "Maosong Sun"
                }
            ]
        },
        {
            "paperId": "729d4e0db88583d1add8681a34b28d1b1e84eb5d",
            "title": "User Behavior Simulation with Large Language Model based Agents",
            "abstract": "Simulating high quality user behavior data has always been a fundamental problem in human-centered applications, where the major difficulty originates from the intricate mechanism of human decision process. Recently, substantial evidences have suggested that by learning huge amounts of web knowledge, large language models (LLMs) can achieve human-like intelligence. We believe these models can provide significant opportunities to more believable user behavior simulation. To inspire such direction, we propose an LLM-based agent framework and design a sandbox environment to simulate real user behaviors. Based on extensive experiments, we find that the simulated behaviors of our method are very close to the ones of real humans. Concerning potential applications, we simulate and study two social phenomenons including (1) information cocoons and (2) user conformity behaviors. This research provides novel simulation paradigms for human-centered applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152509786",
                    "name": "Lei Wang"
                },
                {
                    "authorId": "2144163813",
                    "name": "Jingsen Zhang"
                },
                {
                    "authorId": "2257352534",
                    "name": "Hao Yang"
                },
                {
                    "authorId": "2241452075",
                    "name": "Zhiyuan Chen"
                },
                {
                    "authorId": "144010962",
                    "name": "Jiakai Tang"
                },
                {
                    "authorId": "2223760889",
                    "name": "Zeyu Zhang"
                },
                {
                    "authorId": "2144230136",
                    "name": "Xu Chen"
                },
                {
                    "authorId": "2149202150",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "35119829",
                    "name": "Ruihua Song"
                },
                {
                    "authorId": "2542603",
                    "name": "Wayne Xin Zhao"
                },
                {
                    "authorId": "2241424564",
                    "name": "Jun Xu"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2241311153",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "153693432",
                    "name": "Ji-rong Wen"
                }
            ]
        }
    ]
}