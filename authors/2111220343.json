{
    "authorId": "2111220343",
    "papers": [
        {
            "paperId": "11a4543838ded1e65e9767f3186e2835136181c9",
            "title": "Can Knowledge Graphs Simplify Text?",
            "abstract": "Knowledge Graph (KG)-to-Text Generation has seen recent improvements in generating fluent and informative sentences which describe a given KG. As KGs are widespread across multiple domains and contain important entity-relation information, and as text simplification aims to reduce the complexity of a text while preserving the meaning of the original text, we propose KGSimple, a novel approach to unsupervised text simplification which infuses KG-established techniques in order to construct a simplified KG path and generate a concise text which preserves the original input's meaning. Through an iterative and sampling KG-first approach, our model is capable of simplifying text when starting from a KG by learning to keep important information while harnessing KG-to-text generation to output fluent and descriptive sentences. We evaluate various settings of the KGSimple model on currently-available KG-to-text datasets, demonstrating its effectiveness compared to unsupervised text simplification models which start with a given complex text. Our code is available on GitHub.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1441128975",
                    "name": "Anthony Colas"
                },
                {
                    "authorId": "2110816708",
                    "name": "Haodi Ma"
                },
                {
                    "authorId": "2116986629",
                    "name": "Xuanli He"
                },
                {
                    "authorId": "1471403524",
                    "name": "Yang Bai"
                },
                {
                    "authorId": "2111220343",
                    "name": "D. Wang"
                }
            ]
        },
        {
            "paperId": "18a9128844d99f2fd68d74cbde191cf8f089ff4d",
            "title": "Learned Accelerator Framework for Angular-Distance-Based High-Dimensional DBSCAN",
            "abstract": "Density-based clustering is a commonly used tool in data science. Today many data science works are utilizing high-dimensional neural embeddings. However, traditional density-based clustering techniques like DBSCAN have a degraded performance on high-dimensional data. In this paper, we propose LAF, a generic learned accelerator framework to speed up the original DBSCAN and the sampling-based variants of DBSCAN on high-dimensional data with angular distance metric. This framework consists of a learned cardinality estimator and a post-processing module. The cardinality estimator can fast predict whether a data point is core or not to skip unnecessary range queries, while the post-processing module detects the false negative predictions and merges the falsely separated clusters. The evaluation shows our LAF-enhanced DBSCAN method outperforms the state-of-the-art efficient DBSCAN variants on both efficiency and quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115569383",
                    "name": "Yifan Wang"
                },
                {
                    "authorId": "2111220343",
                    "name": "D. Wang"
                }
            ]
        },
        {
            "paperId": "54b472bddc347ec415ea9dccb76c5bf2a9c547fc",
            "title": "Improving Rare Tree Species Classification Using Domain Knowledge",
            "abstract": "Forest inventory forms the foundation of forest management. Remote sensing (RS) is an efficient means of measuring forest parameters at scale. Remotely sensed species classification can be used to estimate species abundances, distributions, and to better approximate metrics such as aboveground biomass. State-of-the-art methods of RS species classification rely on deep-learning models such as convolutional neural networks (CNNs). These models have two major drawbacks: they require large samples of each species to classify well and they lack explainability. Therefore, rare species are poorly classified causing poor approximations of their associated parameters. We show that the classification of rare species can be improved by as much as eight F1-points using a neuro-symbolic (NS) approach that combines CNNs with an NS framework. The framework allows for the incorporation of domain knowledge into the model through the use of mathematically represented rules, improving model explainability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2123020285",
                    "name": "Ira Harmon"
                },
                {
                    "authorId": "50852908",
                    "name": "S. Marconi"
                },
                {
                    "authorId": "2059680546",
                    "name": "Ben Weinstein"
                },
                {
                    "authorId": "1471403524",
                    "name": "Yang Bai"
                },
                {
                    "authorId": "2111220343",
                    "name": "D. Wang"
                },
                {
                    "authorId": "145759281",
                    "name": "Ethan White"
                },
                {
                    "authorId": "3415991",
                    "name": "Stephanie A. Bohlman"
                }
            ]
        },
        {
            "paperId": "5dbffedcabe3fa43060ebbe2b1789500edfd871f",
            "title": "Reasoning with Language Model is Planning with World Model",
            "abstract": "Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\\textit{world model}$ to predict the world $\\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\\underline{R}$easoning vi$\\underline{a}$ $\\underline{P}$lanning $\\textbf{(RAP)}$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration $\\textit{vs.}$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33% relative improvement in a plan generation setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2128965713",
                    "name": "Shibo Hao"
                },
                {
                    "authorId": "2112578816",
                    "name": "Yi Gu"
                },
                {
                    "authorId": "2110816708",
                    "name": "Haodi Ma"
                },
                {
                    "authorId": "2218162745",
                    "name": "Joshua Jiahua Hong"
                },
                {
                    "authorId": "47197370",
                    "name": "Zhen Wang"
                },
                {
                    "authorId": "2111220343",
                    "name": "D. Wang"
                },
                {
                    "authorId": "2749311",
                    "name": "Zhiting Hu"
                }
            ]
        },
        {
            "paperId": "bc5709ea946de8445e9ecc9c2a6784432bb6996a",
            "title": "MythQA: Query-Based Large-Scale Check-Worthy Claim Detection through Multi-Answer Open-Domain Question Answering",
            "abstract": "Check-worthy claim detection aims at providing plausible misinformation to the downstream fact-checking systems or human experts to check. This is a crucial step toward accelerating the fact-checking process. Many efforts have been put into how to identify check-worthy claims from a small scale of pre-collected claims, but how to efficiently detect check-worthy claims directly from a large-scale information source, such as Twitter, remains underexplored. To fill this gap, we introduce MythQA, a new multi-answer open-domain question answering(QA) task that involves contradictory stance mining for query-based large-scale check-worthy claim detection. The idea behind this is that contradictory claims are a strong indicator of misinformation that merits scrutiny by the appropriate authorities. To study this task, we construct TweetMythQA, an evaluation dataset containing 522 factoid multi-answer questions based on controversial topics. Each question is annotated with multiple answers. Moreover, we collect relevant tweets for each distinct answer, then classify them into three categories: \"Supporting\", \"Refuting\", and \"Neutral\". In total, we annotated 5.3K tweets. Contradictory evidence is collected for all answers in the dataset. Finally, we present a baseline system for MythQA and evaluate existing NLP models for each system component using the TweetMythQA dataset. We provide initial benchmarks and identify key challenges for future models to improve upon. Code and data are available at: https://github.com/TonyBY/Myth-QA",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1471403524",
                    "name": "Yang Bai"
                },
                {
                    "authorId": "1441128975",
                    "name": "Anthony Colas"
                },
                {
                    "authorId": "2111220343",
                    "name": "D. Wang"
                }
            ]
        },
        {
            "paperId": "dccb0f9c87e81882eb2107ce84b8f2bc1bdfbac1",
            "title": "Simple Rule Injection for ComplEx Embeddings",
            "abstract": "Recent works in neural knowledge graph inference attempt to combine logic rules with knowledge graph embeddings to benefit from prior knowledge. However, they usually cannot avoid rule grounding, and injecting a diverse set of rules has still not been thoroughly explored. In this work, we propose InjEx, a mechanism to inject multiple types of rules through simple constraints, which capture definite Horn rules. To start, we theoretically prove that InjEx can inject such rules. Next, to demonstrate that InjEx infuses interpretable prior knowledge into the embedding space, we evaluate InjEx on both the knowledge graph completion (KGC) and few-shot knowledge graph completion (FKGC) settings. Our experimental results reveal that InjEx outperforms both baseline KGC models as well as specialized few-shot models while maintaining its scalability and efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110816708",
                    "name": "Haodi Ma"
                },
                {
                    "authorId": "1441128975",
                    "name": "Anthony Colas"
                },
                {
                    "authorId": "2115621078",
                    "name": "Yuejie Wang"
                },
                {
                    "authorId": "51283807",
                    "name": "A. Sadeghian"
                },
                {
                    "authorId": "2111220343",
                    "name": "D. Wang"
                }
            ]
        },
        {
            "paperId": "f9f49d9e8ff142c4dcbff82dfe27448f45408dea",
            "title": "A Survey On Few-shot Knowledge Graph Completion with Structural and Commonsense Knowledge",
            "abstract": "Knowledge graphs (KG) have served as the key component of various natural language processing applications. Commonsense knowledge graphs (CKG) are a special type of KG, where entities and relations are composed of free-form text. However, previous works in KG completion and CKG completion suffer from long-tail relations and newly-added relations which do not have many know triples for training. In light of this, few-shot KG completion (FKGC), which requires the strengths of graph representation learning and few-shot learning, has been proposed to challenge the problem of limited annotated data. In this paper, we comprehensively survey previous attempts on such tasks in the form of a series of methods and applications. Specifically, we first introduce FKGC challenges, commonly used KGs, and CKGs. Then we systematically categorize and summarize existing works in terms of the type of KGs and the methods. Finally, we present applications of FKGC models on prediction tasks in different areas and share our thoughts on future research directions of FKGC.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110816708",
                    "name": "Haodi Ma"
                },
                {
                    "authorId": "2111220343",
                    "name": "D. Wang"
                }
            ]
        },
        {
            "paperId": "49b960f6bc3f954c45a9d2b924e1832d3ca69252",
            "title": "Query-Driven Knowledge Base Completion using Multimodal Path Fusion over Multimodal Knowledge Graph",
            "abstract": "Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete, for example, over 70% of people in Freebase have no known place of birth. To solve this problem, we propose a query-driven knowledge base completion system with multimodal fusion of unstructured and structured information. To effectively fuse unstructured information from the Web and structured information in knowledge bases to achieve good performance, our system builds multimodal knowledge graphs based on question answering and rule inference. We propose a multimodal path fusion algorithm to rank candidate answers based on different paths in the multimodal knowledge graphs, achieving much better performance than question answering, rule inference and a baseline fusion algorithm. To improve system efficiency, query-driven techniques are utilized to reduce the runtime of our system, providing fast responses to user queries. Extensive experiments have been conducted to demonstrate the effectiveness and efficiency of our system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110929638",
                    "name": "Yang Peng"
                },
                {
                    "authorId": "2111220343",
                    "name": "D. Wang"
                }
            ]
        },
        {
            "paperId": "5333c7cd3590778eb4dee77f387082c8e7254b9e",
            "title": "LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense Passage Retrieval",
            "abstract": "Passage retrieval has been studied for decades, and many recent approaches of passage retrieval are using dense embeddings generated from deep neural models, called \"dense passage retrieval\". The state-of-the-art end-to-end dense passage retrieval systems normally deploy a deep neural model followed by an approximate nearest neighbor (ANN) search module. The model generates embeddings of the corpus and queries, which are then indexed and searched by the high-performance ANN module. With the increasing data scale, the ANN module unavoidably becomes the bottleneck on efficiency. An alternative is the learned index, which achieves significantly high search efficiency by learning the data distribution and predicting the target data location. But most of the existing learned indexes are designed for low dimensional data, which are not suitable for dense passage retrieval with high-dimensional dense embeddings.\n \n In this paper, we propose\n LIDER\n , an efficient high-dimensional\n L\n earned\n I\n ndex for large-scale\n DE\n nse passage\n R\n etrieval. LIDER has a clustering-based hierarchical architecture formed by two layers of core models. As the basic unit of LIDER to index and search data, a\n core model\n includes an adapted recursive model index (RMI) and a dimension reduction component which consists of an extended SortingKeys-LSH (SK-LSH) and a key re-scaling module. The dimension reduction component reduces the high-dimensional dense embeddings into one-dimensional keys and sorts them in a specific order, which are then used by the RMI to make fast prediction. Experiments show that LIDER has a higher search speed with high retrieval quality comparing to the state-of-the-art ANN indexes on passage retrieval tasks, e.g., on large-scale data it achieves 1.2x search speed and significantly higher retrieval quality than the fastest baseline in our evaluation. Furthermore, LIDER has a better capability of speed-quality trade-off.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115569215",
                    "name": "Yifan Wang"
                },
                {
                    "authorId": "2110816708",
                    "name": "Haodi Ma"
                },
                {
                    "authorId": "2111220343",
                    "name": "D. Wang"
                }
            ]
        },
        {
            "paperId": "5acfe1c57d2638a5589227740f5eecfa0dfccbfb",
            "title": "Extensible Database Simulator for Fast Prototyping In-Database Algorithms",
            "abstract": "With the rapid increasing of data scale, in-database analytics and learning has become one of the most studied topics in data science community, because of its significance on reducing the gap between the management and the analytics of data. By extending the capability of database on analytics and learning, data scientists can save much time on exchanging data between databases and external analytic tools. For this goal, researchers are attempting to integrate more data science algorithms into database. However, implementing the algorithms in mainstream databases is super time-consuming, since the developers often have to make a deep dive into the database kernels. Thus there are demands for an easy-to-extend database simulator to help fast prototype and verify the in-database algorithms before implementing them in real databases. In this demo, we present such an extensible relational database simulator, DBSim, to help data scientists prototype their in-database analytics and learning algorithms and verify the effectiveness of their ideas with minimal cost. DBSim simulates a real relational database with all the major components of the mainstream RDBMS, including SQL parser, relational operators, query optimizer, etc. In addition, DBSim provides various interfaces for users to flexibly plug their custom extension modules into any of the major components, without modifying the kernel. By those interfaces, DBSim supports easy extensions on SQL syntax, relational operators, query optimizer rules and cost models, and physical plan execution. To enable accurate evaluation on users' extensions, DBSim supports connecting to real RDBMS and using their real-world cost estimators to calculate the query plan cost. Furthermore, DBSim provides utilities to facilitate users' developing and debugging, like query plan visualizer and interactive analyzer on optimization rules. We develop DBSim using pure Python to support seamless implementation of most data science algorithms into it, since many of them are written in Python.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115569383",
                    "name": "Yifan Wang"
                },
                {
                    "authorId": "2111220343",
                    "name": "D. Wang"
                }
            ]
        }
    ]
}