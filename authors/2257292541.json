{
    "authorId": "2257292541",
    "papers": [
        {
            "paperId": "098be01c95b4c18e2c7e8b4164d29dbb0903e71f",
            "title": "Can a Multichoice Dataset be Repurposed for Extractive Question Answering?",
            "abstract": "The rapid evolution of Natural Language Processing (NLP) has favored major languages such as English, leaving a significant gap for many others due to limited resources. This is especially evident in the context of data annotation, a task whose importance cannot be underestimated, but which is time-consuming and costly. Thus, any dataset for resource-poor languages is precious, in particular when it is task-specific. Here, we explore the feasibility of repurposing existing datasets for a new NLP task: we repurposed the Belebele dataset (Bandarkar et al., 2023), which was designed for multiple-choice question answering (MCQA), to enable extractive QA (EQA) in the style of machine reading comprehension. We present annotation guidelines and a parallel EQA dataset for English and Modern Standard Arabic (MSA). We also present QA evaluation results for several monolingual and cross-lingual QA pairs including English, MSA, and five Arabic dialects. Our aim is to enable others to adapt our approach for the 120+ other language variants in Belebele, many of which are deemed under-resourced. We also conduct a thorough analysis and share our insights from the process, which we hope will contribute to a deeper understanding of the challenges and the opportunities associated with task reformulation in NLP research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298756162",
                    "name": "Teresa Lynn"
                },
                {
                    "authorId": "51935928",
                    "name": "Malik H. Altakrori"
                },
                {
                    "authorId": "148087360",
                    "name": "S. Magdy"
                },
                {
                    "authorId": "2211732585",
                    "name": "Rocktim Jyoti Das"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2056258384",
                    "name": "Mohamed Nasr"
                },
                {
                    "authorId": "2282523149",
                    "name": "Younes Samih"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2111356",
                    "name": "S. Godbole"
                },
                {
                    "authorId": "1781292",
                    "name": "S. Roukos"
                },
                {
                    "authorId": "2261287685",
                    "name": "Radu Florian"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                }
            ]
        },
        {
            "paperId": "9ff25b04f81d21f700deb5b386857840b81a1f23",
            "title": "ArabicMMLU: Assessing Massive Multitask Language Understanding in Arabic",
            "abstract": "The focus of language model evaluation has transitioned towards reasoning and knowledge-intensive tasks, driven by advancements in pretraining large models. While state-of-the-art models are partially trained on large Arabic texts, evaluating their performance in Arabic remains challenging due to the limited availability of relevant datasets. To bridge this gap, we present \\datasetname{}, the first multi-task language understanding benchmark for the Arabic language, sourced from school exams across diverse educational levels in different countries spanning North Africa, the Levant, and the Gulf regions. Our data comprises 40 tasks and 14,575 multiple-choice questions in Modern Standard Arabic (MSA) and is carefully constructed by collaborating with native speakers in the region. Our comprehensive evaluations of 35 models reveal substantial room for improvement, particularly among the best open-source models. Notably, BLOOMZ, mT0, LLaMA2, and Falcon struggle to achieve a score of 50%, while even the top-performing Arabic-centric model only achieves a score of 62.3%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2789148",
                    "name": "Fajri Koto"
                },
                {
                    "authorId": "2274084215",
                    "name": "Haonan Li"
                },
                {
                    "authorId": "2210193043",
                    "name": "Sara Shatnawi"
                },
                {
                    "authorId": "1724523030",
                    "name": "Jad Doughman"
                },
                {
                    "authorId": "2233498337",
                    "name": "Abdelrahman Boda Sadallah"
                },
                {
                    "authorId": "2284767001",
                    "name": "Aisha Alraeesi"
                },
                {
                    "authorId": "2284765679",
                    "name": "Khalid Almubarak"
                },
                {
                    "authorId": "25098419",
                    "name": "Zaid Alyafeai"
                },
                {
                    "authorId": "2284771123",
                    "name": "Neha Sengupta"
                },
                {
                    "authorId": "38510157",
                    "name": "Shady Shehata"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2256987316",
                    "name": "Timothy Baldwin"
                }
            ]
        },
        {
            "paperId": "c56eab12bd00e2fe28868af21d518044d66df00d",
            "title": "SemEval-2024 Task 8: Multidomain, Multimodel and Multilingual Machine-Generated Text Detection",
            "abstract": "We present the results and the main findings of SemEval-2024 Task 8: Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection. The task featured three subtasks. Subtask A is a binary classification task determining whether a text is written by a human or generated by a machine. This subtask has two tracks: a monolingual track focused solely on English texts and a multilingual track. Subtask B is to detect the exact source of a text, discerning whether it is written by a human or generated by a specific LLM. Subtask C aims to identify the changing point within a text, at which the authorship transitions from human to machine. The task attracted a large number of participants: subtask A monolingual (126), subtask A multilingual (59), subtask B (70), and subtask C (30). In this paper, we present the task, analyze the results, and discuss the system submissions and the methods they used. For all subtasks, the best systems used LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2218861055",
                    "name": "Jonibek Mansurov"
                },
                {
                    "authorId": "2058455381",
                    "name": "Petar Ivanov"
                },
                {
                    "authorId": "2265989879",
                    "name": "Jinyan Su"
                },
                {
                    "authorId": "1967424",
                    "name": "Artem Shelmanov"
                },
                {
                    "authorId": "2164381839",
                    "name": "Akim Tsvigun"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2218209429",
                    "name": "Tarek Mahmoud"
                },
                {
                    "authorId": "2284686859",
                    "name": "Giovanni Puccetti"
                },
                {
                    "authorId": "2284687590",
                    "name": "Thomas Arnold"
                },
                {
                    "authorId": "2161240241",
                    "name": "Chenxi Whitehouse"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "c59628de894a4aa7f91548bad5b4103b747256e8",
            "title": "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection",
            "abstract": "The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark based on a multilingual, multi-domain, and multi-generator corpus of MGTs -- M4GT-Bench. The benchmark is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection where one need to identify, which particular model generated the text; and (3) mixed human-machine text detection, where a word boundary delimiting MGT from human-written content should be determined. On the developed benchmark, we have tested several MGT detection baselines and also conducted an evaluation of human performance. We see that obtaining good performance in MGT detection usually requires an access to the training data from the same domain and generators. The benchmark is available at https://github.com/mbzuai-nlp/M4GT-Bench.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "2218861055",
                    "name": "Jonibek Mansurov"
                },
                {
                    "authorId": "2058455381",
                    "name": "Petar Ivanov"
                },
                {
                    "authorId": "2265989879",
                    "name": "Jinyan Su"
                },
                {
                    "authorId": "1967424",
                    "name": "Artem Shelmanov"
                },
                {
                    "authorId": "2164381839",
                    "name": "Akim Tsvigun"
                },
                {
                    "authorId": "2284688236",
                    "name": "Osama Mohanned Afzal"
                },
                {
                    "authorId": "2218209429",
                    "name": "Tarek Mahmoud"
                },
                {
                    "authorId": "2284686859",
                    "name": "Giovanni Puccetti"
                },
                {
                    "authorId": "2284687590",
                    "name": "Thomas Arnold"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "de2a01b40397944cea7eb1be41ec399f8b506b1a",
            "title": "HelloThere: A Corpus of Annotated Dialogues and Knowledge Bases of Time-Offset Avatars",
            "abstract": "A Time-Offset Interaction Application (TOIA) is a software system that allows people to engage in face-to-face dialogue with previously recorded videos of other people. There are two TOIA usage modes: (a) creation mode, where users pre-record video snippets of themselves representing their answers to possible questions someone may ask them, and (b) interaction mode, where other users of the system can choose to interact with created avatars. This paper presents the HelloThere corpus that has been collected from two user studies involving several people who recorded avatars and many more who engaged in dialogues with them. The interactions with avatars are annotated by people asking them questions through three modes (card selection, text search, and voice input) and rating the appropriateness of their answers on a 1 to 5 scale. The corpus, made available to the research community, comprises 26 avatars\u2019 knowledge bases and 317 dialogues between 64 interrogators and the avatars in text format.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266571657",
                    "name": "Alberto Chierici"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                }
            ]
        },
        {
            "paperId": "ea4c0ab66529cac83f0b2b50eaef305da6a297e1",
            "title": "LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection",
            "abstract": "The widespread accessibility of large language models (LLMs) to the general public has significantly amplified the dissemination of machine-generated texts (MGTs). Advancements in prompt manipulation have exacerbated the difficulty in discerning the origin of a text (human-authored vs machinegenerated). This raises concerns regarding the potential misuse of MGTs, particularly within educational and academic domains. In this paper, we present $\\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection. It is able to classify texts into four categories: human-written, machine-generated, machine-written machine-humanized, and human-written machine-polished. Contrary to previous MGT detectors that perform binary classification, introducing two additional categories in LLM-DetectiAIve offers insights into the varying degrees of LLM intervention during the text creation. This might be useful in some domains like education, where any LLM intervention is usually prohibited. Experiments show that LLM-DetectAIve can effectively identify the authorship of textual content, proving its usefulness in enhancing integrity in education, academia, and other domains. LLM-DetectAIve is publicly accessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video describing our system is available at https://youtu.be/E8eT_bE7k8c.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2315296727",
                    "name": "Mervat Abassy"
                },
                {
                    "authorId": "2315302985",
                    "name": "Kareem Elozeiri"
                },
                {
                    "authorId": "2315840157",
                    "name": "Alexander Aziz"
                },
                {
                    "authorId": "2315297170",
                    "name": "Minh Ngoc Ta"
                },
                {
                    "authorId": "2309163441",
                    "name": "Raj Vardhan Tomar"
                },
                {
                    "authorId": "2315301202",
                    "name": "Bimarsha Adhikari"
                },
                {
                    "authorId": "2315642904",
                    "name": "Saad El Dine Ahmed"
                },
                {
                    "authorId": "2241417701",
                    "name": "Yuxia Wang"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2315671993",
                    "name": "Zhuohan Xie"
                },
                {
                    "authorId": "2218861055",
                    "name": "Jonibek Mansurov"
                },
                {
                    "authorId": "2300660029",
                    "name": "Ekaterina Artemova"
                },
                {
                    "authorId": "51259225",
                    "name": "V. Mikhailov"
                },
                {
                    "authorId": "2308041454",
                    "name": "Rui Xing"
                },
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "2300555930",
                    "name": "Hasan Iqbal"
                },
                {
                    "authorId": "2266755049",
                    "name": "Zain Muhammad Mujahid"
                },
                {
                    "authorId": "2218209429",
                    "name": "Tarek Mahmoud"
                },
                {
                    "authorId": "2164381839",
                    "name": "Akim Tsvigun"
                },
                {
                    "authorId": "8129718",
                    "name": "Alham Fikri Aji"
                },
                {
                    "authorId": "1967424",
                    "name": "Artem Shelmanov"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                }
            ]
        },
        {
            "paperId": "26171fba71b714ab5fc4df09194766bc0978fdeb",
            "title": "Tell Me More, Tell Me More: AI-Generated Question Suggestions for the Creation of Interactive Video Recordings",
            "abstract": "Time-Offset Interaction Applications (TOIAs) are narrative-sharing systems that use databases of previously recorded videos of real people to mimic conversations with them. These video databases comprise large (the larger, the better) collections of videos of answers paired with specific questions. This paper focuses on a solution to the challenge of creating such databases without exhausting their creators\u2019 creativity, energy, and interest. We describe the design and development process of Question Suggester (QS)-an intelligent GPT-3-based service that generates suggested questions following up a conversation based on the history of recorded questions and answers. We conduct a user study to empirically evaluate the value of QS for reducing the effort to create a video database while creating an interaction that is enjoyable. The users\u2019 average experience rating for QS is 4.6 compared to 4.0 when QS is not used (on a 1-5 scale, p-value<0.05). The experience with interactions so created is more enjoyable, too (3.7vs. 3.3, p-value<0.05). The usage metrics and qualitative feedback confirm that QS is essential for interactive video-recording systems and for increasing their adoption.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266571657",
                    "name": "Alberto Chierici"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                }
            ]
        },
        {
            "paperId": "8255c2dda2903eb911cfaf7f141013558a94d094",
            "title": "Data Augmentation Techniques for Machine Translation of Code-Switched Texts: A Comparative Study",
            "abstract": "Code-switching (CSW) text generation has been receiving increasing attention as a solution to address data scarcity. In light of this growing interest, we need more comprehensive studies comparing different augmentation approaches. In this work, we compare three popular approaches: lexical replacements, linguistic theories, and back-translation (BT), in the context of Egyptian Arabic-English CSW. We assess the effectiveness of the approaches on machine translation and the quality of augmentations through human evaluation. We show that BT and CSW predictive-based lexical replacement, being trained on CSW parallel data, perform best on both tasks. Linguistic theories and random lexical replacement prove to be effective in the lack of CSW parallel data, where both approaches achieve similar results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3248560",
                    "name": "Injy Hamed"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                },
                {
                    "authorId": "2261387716",
                    "name": "Ngoc Thang Vu"
                }
            ]
        },
        {
            "paperId": "f6df2624d59cbcfc74a1bebd1d8b67227475b018",
            "title": "Boundless Conversations: AI-Powered Video Interactions across Domains, Languages, and Time",
            "abstract": "We present novel web applications that blend AI and video communication, offering an interactive way to engage with videos, transcending time and distance constraints. Known as Time Offset Interaction Application (TOIA), the platform transforms passive video viewing into dynamic conversations using personal recordings or YouTube clips. Our work expands TOIA with two unique experiences: 1) \"The Elephant in the Room\" project, stimulating dialogues on sensitive topics like childbirth, sex, and death; 2) Support for multilingual interactions, fostering a global collection of diverse personal narratives. Through AI and interactive media, we aim to broaden the horizons for sharing human experiences and perspectives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266571657",
                    "name": "Alberto Chierici"
                },
                {
                    "authorId": "2268205698",
                    "name": "Soojin Lee"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                },
                {
                    "authorId": "2268128626",
                    "name": "Aaron Sherwood"
                },
                {
                    "authorId": "2268129139",
                    "name": "Bishnu Dev"
                },
                {
                    "authorId": "2268188290",
                    "name": "Gautham Kumar"
                },
                {
                    "authorId": "2268173036",
                    "name": "Muhammad Ali"
                }
            ]
        },
        {
            "paperId": "5c0a7a3f4a7928e8d5fe0b05d73d53865f5afb3d",
            "title": "CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies",
            "abstract": "The Conference on Computational Natural Language Learning (CoNLL) features a shared task, in which participants train and test their learning systems on the same data sets. In 2017, the task was devoted to learning dependency parsers for a large number of languages, in a real-world setting without any gold-standard annotation on input. All test sets followed a unified annotation scheme, namely that of Universal Dependencies. In this paper, we define the task and evaluation methodology, describe how the data sets were prepared, report and analyze the main results, and provide a brief categorization of the different approaches of the participating systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2251519666",
                    "name": "Daniel Zeman"
                },
                {
                    "authorId": "3209310",
                    "name": "M. Popel"
                },
                {
                    "authorId": "2757959",
                    "name": "Milan Straka"
                },
                {
                    "authorId": "2250936695",
                    "name": "Jan Haji\u010d"
                },
                {
                    "authorId": "2256431387",
                    "name": "Joakim Nivre"
                },
                {
                    "authorId": "1694491",
                    "name": "Filip Ginter"
                },
                {
                    "authorId": "1794453",
                    "name": "Juhani Luotolahti"
                },
                {
                    "authorId": "1708916",
                    "name": "S. Pyysalo"
                },
                {
                    "authorId": "2257293575",
                    "name": "Slav Petrov"
                },
                {
                    "authorId": "2257289955",
                    "name": "Martin Potthast"
                },
                {
                    "authorId": "2241522772",
                    "name": "F. M. Tyers"
                },
                {
                    "authorId": "8634820",
                    "name": "E. Badmaeva"
                },
                {
                    "authorId": "7432509",
                    "name": "Memduh Gokirmak"
                },
                {
                    "authorId": "3045946",
                    "name": "A. Nedoluzhko"
                },
                {
                    "authorId": "2902216",
                    "name": "Silvie Cinkov\u00e1"
                },
                {
                    "authorId": "2250936695",
                    "name": "Jan Haji\u010d"
                },
                {
                    "authorId": "2489033",
                    "name": "Jaroslava Hlav\u00e1cov\u00e1"
                },
                {
                    "authorId": "2371631",
                    "name": "V\u00e1clava Kettnerov\u00e1"
                },
                {
                    "authorId": "3344673",
                    "name": "Zdenka Uresov\u00e1"
                },
                {
                    "authorId": "1776599",
                    "name": "Jenna Kanerva"
                },
                {
                    "authorId": "2659566",
                    "name": "Stina Ojala"
                },
                {
                    "authorId": "2813341",
                    "name": "Anna Missil\u00e4"
                },
                {
                    "authorId": "2257172629",
                    "name": "Christopher D. Manning"
                },
                {
                    "authorId": "145157639",
                    "name": "Sebastian Schuster"
                },
                {
                    "authorId": "2256975069",
                    "name": "Siva Reddy"
                },
                {
                    "authorId": "3405525",
                    "name": "Dima Taji"
                },
                {
                    "authorId": "2257292541",
                    "name": "Nizar Habash"
                },
                {
                    "authorId": "2257190247",
                    "name": "Herman Leung"
                },
                {
                    "authorId": "2241127",
                    "name": "M. Marneffe"
                },
                {
                    "authorId": "2040952",
                    "name": "M. Sanguinetti"
                },
                {
                    "authorId": "35184435",
                    "name": "M. Simi"
                },
                {
                    "authorId": "2257289114",
                    "name": "Hiroshi Kanayama"
                },
                {
                    "authorId": "2257291719",
                    "name": "Valeria de Paiva"
                },
                {
                    "authorId": "22341186",
                    "name": "Kira Droganova"
                },
                {
                    "authorId": "2257291828",
                    "name": "H\u00e9ctor Mart\u00ednez Alonso"
                },
                {
                    "authorId": "103304646",
                    "name": "\u00c7a\u011fr\u0131 \u00c7\u00f6ltekin"
                },
                {
                    "authorId": "3461272",
                    "name": "U. Sulubacak"
                },
                {
                    "authorId": "1781790",
                    "name": "H. Uszkoreit"
                },
                {
                    "authorId": "22330068",
                    "name": "Vivien Macketanz"
                },
                {
                    "authorId": "2495532",
                    "name": "A. Burchardt"
                },
                {
                    "authorId": "2055587557",
                    "name": "K. Harris"
                },
                {
                    "authorId": "71792678",
                    "name": "Katrin Marheinecke"
                },
                {
                    "authorId": "2257289372",
                    "name": "Georg Rehm"
                },
                {
                    "authorId": "73345985",
                    "name": "Tolga Kayadelen"
                },
                {
                    "authorId": "2257290146",
                    "name": "Mohammed Attia"
                },
                {
                    "authorId": "1405755816",
                    "name": "Ali El-Kahky"
                },
                {
                    "authorId": "2257433596",
                    "name": "Zhuoran Yu"
                },
                {
                    "authorId": "2585932",
                    "name": "Emily Pitler"
                },
                {
                    "authorId": "73041331",
                    "name": "Saran Lertpradit"
                },
                {
                    "authorId": "3848524",
                    "name": "M. Mandl"
                },
                {
                    "authorId": "49996036",
                    "name": "Jesse Kirchner"
                },
                {
                    "authorId": "2084848348",
                    "name": "Hector Fernandez Alcalde"
                },
                {
                    "authorId": "72151653",
                    "name": "Jana Strnadov\u00e1"
                },
                {
                    "authorId": "49320126",
                    "name": "Esha Banerjee"
                },
                {
                    "authorId": "2257289502",
                    "name": "Ruli Manurung"
                },
                {
                    "authorId": "2257289901",
                    "name": "Antonio Stella"
                },
                {
                    "authorId": "2257289912",
                    "name": "Atsuko Shimada"
                },
                {
                    "authorId": "144816713",
                    "name": "Sookyoung Kwak"
                },
                {
                    "authorId": "145454056",
                    "name": "Gustavo Mendon\u00e7a"
                },
                {
                    "authorId": "31682812",
                    "name": "Tatiana Lando"
                },
                {
                    "authorId": "2116910",
                    "name": "Rattima Nitisaroj"
                },
                {
                    "authorId": "150151373",
                    "name": "Josie Li"
                }
            ]
        }
    ]
}