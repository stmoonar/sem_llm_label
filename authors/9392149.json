{
    "authorId": "9392149",
    "papers": [
        {
            "paperId": "208cf9e3e62540bc99d9d2a226f3de0fe18c2836",
            "title": "Boosting Communication Efficiency of Federated Learning\u2019s Secure Aggregation",
            "abstract": "Federated Learning (FL) is a decentralized machine learning approach where client devices train models locally and send them to a server that performs aggregation to generate a global model. FL is vulnerable to model inversion attacks, where the server can infer sensitive client data from trained models. Google\u2019s Secure Aggregation (SecAgg) protocol addresses this data privacy issue by masking each client\u2019s trained model using shared secrets and individual elements generated locally on the client\u2019s device. Although SecAgg effectively preserves privacy, it imposes considerable communication and computation overhead, especially as network size increases. Building upon SecAgg, this poster introduces a Communication-Efficient Secure Aggregation (CESA) protocol that substantially reduces this overhead by using only two shared secrets per client to mask the model. We propose our method for stable networks with low delay variation and limited client dropouts. CESA is independent of the data distribution and network size (for higher than 6 nodes), preventing the honest-but-curious server from accessing unmasked models. Our initial evaluation reveals that CESA significantly reduces the communication cost compared to SecAgg.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293398307",
                    "name": "Niousha Nazemi"
                },
                {
                    "authorId": "3443570",
                    "name": "Omid Tavallaie"
                },
                {
                    "authorId": "2257570575",
                    "name": "Shuaijun Chen"
                },
                {
                    "authorId": "9392149",
                    "name": "Albert Y. Zomaya"
                },
                {
                    "authorId": "2293398147",
                    "name": "Ralph Holz"
                }
            ]
        },
        {
            "paperId": "799d56e08f2db61570105fa7b768f41cdeda7326",
            "title": "MESON: A Mobility-Aware Dependent Task Offloading Scheme for Urban Vehicular Edge Computing",
            "abstract": "Vehicular Edge Computing (VEC) is the transportation version of Mobile Edge Computing (MEC) in road scenarios. One key technology of VEC is task offloading, which allows vehicles to send their computation tasks to the surrounding Roadside Units (RSUs) or other vehicles for execution, thereby reducing computation delay and energy consumption. However, the existing task offloading schemes still have various gaps and face challenges that should be addressed because vehicles with time-varying trajectories need to process massive data with high complexity and diversity. In this paper, a VEC-based computation offloading model is developed with consideration of data dependency of tasks. The minimization of the average response time and average energy consumption of the system is defined as a combinatorial optimization problem. To solve this problem, we propose a Mobility-aware dependent task offloading (MESON) Scheme for urban VEC and develop a DRL-based algorithm to train the offloading strategy. To improve the training efficiency, a vehicle mobility detection algorithm is further designed to detect the communication time between vehicles and RSUs. In this way, MESON can avoid unreasonable decisions by lowering the size of the action space. Moreover, to improve the system stability and the offloading successful rate, we design a task priority determination scheme to prioritize the tasks in the waiting queue. The experimental results show that MESON is superior compared to other task offloading schemes in terms of the average response time, average system energy consumption, and offloading successful rate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144010790",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "2202431896",
                    "name": "Enchao Zhang"
                },
                {
                    "authorId": "2112899620",
                    "name": "Shaohua Wan"
                },
                {
                    "authorId": "2119430859",
                    "name": "Ammar Hawbani"
                },
                {
                    "authorId": "102161257",
                    "name": "A. Al-Dubai"
                },
                {
                    "authorId": "2165744386",
                    "name": "Geyong Min"
                },
                {
                    "authorId": "9392149",
                    "name": "Albert Y. Zomaya"
                }
            ]
        },
        {
            "paperId": "820c4a396ebd2aa603cb52ea056ba549721c587c",
            "title": "Digital Twin-Assisted, SFC-Enabled Service Provisioning in Mobile Edge Computing",
            "abstract": "Mobile Edge Computing (MEC) has been identified as a desirable computing paradigm that provides efficient and effective services for various applications, while meeting stringent service delay requirements. Orthogonal to the MEC computing paradigm, Network Function Virtualization (NFV) technology is another enabling technology that provides the network resource management with great flexibility and scalability, where the instances of Virtual Network Functions (VNFs) are deployed in edge servers as Service Function Chains (SFCs) for SFC-enabled services. Although reliable service provisioning in MEC environments is fundamentally important, the deployed VNF instances usually are not reliable, which can be affected by their software implementation, their execution duration, the workload among edge servers, and so on. Empowered by digital twin techniques, the states of VNF instances can be maintained by their digital twins in a real-time manner and their reliability can be accurately predicted through their digital twins. In this paper, we study digital twin-assisted, SFC-enabled reliable service provisioning in MEC networks by exploiting the dynamics of VNF instance reliability. We concentrate on two novel optimization problems of reliable service provisioning: the service cost minimization problem, and the dynamic service admission maximization problem. We first show their NP-hardness. We then formulate an Integer Linear Program (ILP) solution, and devise an approximation algorithm with a constant approximation ratio for the service cost minimization problem. We third provide an ILP solution to the offline version of the dynamic service admission maximization problem. Built upon this offline ILP solution, we also develop an online algorithm with a provable competitive ratio for the problem, by adopting the primal-dual dynamic updating technique. We finally evaluate the performance of the proposed algorithms via simulations. Simulation results demonstrate that the proposed algorithms outperform their comparison benchmarks, and improve the performance of their comparison counterparts by no less than <inline-formula><tex-math notation=\"LaTeX\">$10.2 \\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>10</mml:mn><mml:mo>.</mml:mo><mml:mn>2</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"liang-ieq1-3227248.gif\"/></alternatives></inline-formula>.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152909407",
                    "name": "Jing Li"
                },
                {
                    "authorId": "2115742147",
                    "name": "Song Guo"
                },
                {
                    "authorId": "145884252",
                    "name": "W. Liang"
                },
                {
                    "authorId": "2156536065",
                    "name": "Quan Chen"
                },
                {
                    "authorId": "2956815",
                    "name": "Zichuan Xu"
                },
                {
                    "authorId": "2766463",
                    "name": "Wenzheng Xu"
                },
                {
                    "authorId": "9392149",
                    "name": "Albert Y. Zomaya"
                }
            ]
        },
        {
            "paperId": "00789880b2f8c67953e5b145e140ce2540eebd2c",
            "title": "A Weighted Optimal Scheduling Scheme for Congestion Control in Cloud Data Center Networks",
            "abstract": "Cloud services are experiencing a remarkable increase in the number of users and the resource required over the past few years. Thus, it has become a great challenge for the internet vendors to make a robust framework to serve the customers with low cost and delay. Congestion control is one of the essential topics of routing algorithms in cloud data center networks. In this paper, we propose a weighted optimal scheduling scheme WSPR for congestion control in cloud data center networks which prevents the congestion in advance with the global view so that it can make good use of vacant network resources. We choose BCube as our network model and modify the network topology to fit software-defined networks so as to have a full view of the topology. First, we design the SP graph which contains all shortest paths between a source server and a destination server. Second, we propose WSPR to allocate the most appropriate path to each flow for congestion control. We implement a system to simulate a data center, and evaluate our proposed scheme WSPR by comparing WSPR with other classical methods. The experimental results demonstrate that our proposed scheme WSPR has the best performance in terms of the maximum delay, average delay, and throughput among all compared methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118835999",
                    "name": "Yun Li"
                },
                {
                    "authorId": "2058180229",
                    "name": "S. Jian"
                },
                {
                    "authorId": "145099880",
                    "name": "S. Hsieh"
                },
                {
                    "authorId": "14409291",
                    "name": "Wei-Kang Chung"
                },
                {
                    "authorId": "9392149",
                    "name": "Albert Y. Zomaya"
                }
            ]
        },
        {
            "paperId": "0ed9d51638450ed41b336dc1998c4b85c8d02482",
            "title": "CGS-Mask: Making Time Series Predictions Intuitive for All",
            "abstract": "Artificial intelligence (AI) has immense potential in time series prediction, but most explainable tools have limited capabilities in providing a systematic understanding of important features over time. These tools typically rely on evaluating a single time point, overlook the time ordering of inputs, and neglect the time-sensitive nature of time series applications. These factors make it difficult for users, particularly those without domain knowledge, to comprehend AI model decisions and obtain meaningful explanations. We propose CGS-Mask, a post-hoc and model-agnostic cellular genetic strip mask-based saliency approach to address these challenges. CGS-Mask uses consecutive time steps as a cohesive entity to evaluate the impact of features on the final prediction, providing binary and sustained feature importance scores over time. Our algorithm optimizes the mask population iteratively to obtain the optimal mask in a reasonable time. We evaluated CGS-Mask on synthetic and real-world datasets, and it outperformed state-of-the-art methods in elucidating the importance of features over time. According to our pilot user study via a questionnaire survey, CGS-Mask is the most effective approach in presenting easily understandable time series prediction results, enabling users to comprehend the decision-making process of AI models with ease.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262724470",
                    "name": "Feng Lu"
                },
                {
                    "authorId": "47777798",
                    "name": "Wei Li"
                },
                {
                    "authorId": "2222733404",
                    "name": "Yifei Sun"
                },
                {
                    "authorId": "2222871740",
                    "name": "Cheng Song"
                },
                {
                    "authorId": "2109978266",
                    "name": "Yufei Ren"
                },
                {
                    "authorId": "9392149",
                    "name": "Albert Y. Zomaya"
                }
            ]
        },
        {
            "paperId": "0f9ea7127a6c41d87d6f4728fe41a2047fe310e9",
            "title": "Achieving Sub-second Pairwise Query over Evolving Graphs",
            "abstract": "Many real-time OLAP systems have been proposed to query evolving data with sub-second latency. Although this feature is highly attractive, it is very hard to be achieved on analytic graph queries that can only be answered after accessing every connected vertex. Fortunately, researchers recently observed that answering pairwise queries is enough for many real-world scenarios. These pairwise queries avoid the exhaustive nature and hence may only need to access a small portion of the graph. Obviously, the crux of achieving low latency is to what extent the system can eliminate unnecessary computations. This pruning process, according to our investigation, is usually achieved by estimating certain upper bounds of the query result in existing systems. However, our evaluation results demonstrate that these existing upper-bound-only pruning techniques can only prune about half of the vertex activations, which is still far away from achieving the sub-second latency goal on large graphs. In contrast, we found that it is possible to substantially accelerate the processing if we are able to not only estimate the upper bounds, but also foresee a tighter lower bound for certain pairs of vertices in the graph. Our experiments show that only less than 1% of the vertices are activated via using this novel lower bound based pruning technique. Based on this observation, we build SGraph, a system that is able to answer dynamic pairwise queries over evolving graphs with sub-second latency. It can ingest millions of updates per second and simultaneously answer pairwise queries with a latency that is several orders of magnitude smaller than state-of-the-art systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2203414496",
                    "name": "Hongtao Chen"
                },
                {
                    "authorId": "145179162",
                    "name": "Mingxing Zhang"
                },
                {
                    "authorId": "2119301827",
                    "name": "Ke Yang"
                },
                {
                    "authorId": "1680073",
                    "name": "Kang Chen"
                },
                {
                    "authorId": "9392149",
                    "name": "Albert Y. Zomaya"
                },
                {
                    "authorId": "2115858706",
                    "name": "Yongwei Wu"
                },
                {
                    "authorId": "2288203548",
                    "name": "Xuehai Qian"
                }
            ]
        },
        {
            "paperId": "2178ba1d2984309e30012f1dd6de139feaabc838",
            "title": "Parallel Scientific Power Calculations in Cloud Data Center Based On Decomposition-Coordination Directed Acyclic Graph",
            "abstract": "With the expansion scale of interconnected power systems and refined state perception, scientific power calculations become more complex and diverse. They need faster computation speed and better scalability to support power flow calculation, reactive power optimization, and static/transient stability analysis for unit scheduling. Therefore, this article proposes a novel cloud data center task mapping algorithm of the Stoer-Wagner binary tree (SWBT) to support accelerated executions of these calculations. Firstly, based on the block bordered-diagonal form of the admittance matrix, high-time complexity scientific power calculations are transformed into a unified multi-task decomposition-coordination directed acyclic graph (DC-DAG). And then, the critical tasks in this DC-DAG are found and the virtual machines encapsulating them are matched with physical machines in the data center preferentially. Finally, on CloudSim, a cloud computing platform, the multi-job mixed experiments of 118-13659 bus power systems are carried out. In addition, real-time workload performance is enhanced in two very large real-world power systems. Studies illustrate that SWBT can improve the underlying physical machine resource utilization and reduce data interaction transmission hops to achieve better computing acceleration performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149250410",
                    "name": "Ting Yang"
                },
                {
                    "authorId": "2149224504",
                    "name": "Xutao Han"
                },
                {
                    "authorId": "2145536817",
                    "name": "Hao-Si Li"
                },
                {
                    "authorId": "39634635",
                    "name": "Wei Li"
                },
                {
                    "authorId": "9392149",
                    "name": "Albert Y. Zomaya"
                }
            ]
        },
        {
            "paperId": "2c8bccb96bc922eb57d824b573a417e31597fa39",
            "title": "Cloud-Native Computing: A Survey From the Perspective of Services",
            "abstract": "The development of cloud computing delivery models inspires the emergence of cloud-native computing. Cloud-native computing, as the most influential development principle for web applications, has already attracted increasingly more attention in both industry and academia. Despite the momentum in the cloud-native industrial community, a clear research roadmap on this topic is still missing. As a contribution to this knowledge, this article surveys key issues during the life cycle of cloud-native applications, from the perspective of services. Specifically, we elaborate on the research domains by decoupling the life cycle of cloud-native applications into four states: building, orchestration, operation, and maintenance. We also discuss the fundamental necessities and summarize the key performance metrics that play critical roles during the development and management of cloud-native applications. We highlight the key implications and limitations of existing works in each state. The challenges, future directions, and research opportunities are also discussed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145590434",
                    "name": "Shuiguang Deng"
                },
                {
                    "authorId": "144030613",
                    "name": "Hailiang Zhao"
                },
                {
                    "authorId": "46800058",
                    "name": "Binbin Huang"
                },
                {
                    "authorId": "2155988427",
                    "name": "Cheng Zhang"
                },
                {
                    "authorId": "16136372",
                    "name": "Feiyi Chen"
                },
                {
                    "authorId": "2220629497",
                    "name": "Yinuo Deng"
                },
                {
                    "authorId": "2116398505",
                    "name": "Jianwei Yin"
                },
                {
                    "authorId": "1691109",
                    "name": "S. Dustdar"
                },
                {
                    "authorId": "9392149",
                    "name": "Albert Y. Zomaya"
                }
            ]
        },
        {
            "paperId": "4f34bc29f4486343d64aae071e6ddc30a5c5e23f",
            "title": "Optimization of Federated Learning's Client Selection for Non-IID Data Based on Grey Relational Analysis",
            "abstract": "Federated learning (FL) is a novel distributed learning framework designed for applications with privacy-sensitive data. Without sharing data, FL trains local models on individual devices and constructs the global model on the server by performing model aggregation. However, to reduce the communication cost, the participants in each training round are randomly selected, which significantly decreases the training efficiency under data and device heterogeneity. To address this issue, in this paper, we introduce a novel approach that considers the data distribution and computational resources of devices to select the clients for each training round. Our proposed method performs client selection based on the Grey Relational Analysis (GRA) theory by considering available computational resources for each client, the training loss, and weight divergence. To examine the usability of our proposed method, we implement our contribution on Amazon Web Services (AWS) by using the TensorFlow library of Python. We evaluate our algorithm's performance in different setups by varying the learning rate, network size, the number of selected clients, and the client selection round. The evaluation results show that our proposed algorithm enhances the performance significantly in terms of test accuracy and the average client's waiting time compared to state-of-the-art methods, federated averaging and Pow-d.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257570575",
                    "name": "Shuaijun Chen"
                },
                {
                    "authorId": "3443570",
                    "name": "Omid Tavallaie"
                },
                {
                    "authorId": "2257347980",
                    "name": "Michael Henri Hambali"
                },
                {
                    "authorId": "7261022",
                    "name": "S. M. Zandavi"
                },
                {
                    "authorId": "2257347583",
                    "name": "Hamed Haddadi"
                },
                {
                    "authorId": "2257381216",
                    "name": "Song Guo"
                },
                {
                    "authorId": "9392149",
                    "name": "Albert Y. Zomaya"
                }
            ]
        },
        {
            "paperId": "514a96af287ab19a399479f91ae91a28b1593c5c",
            "title": "AI-Enabled Secure Microservices in Edge Computing: Opportunities and Challenges",
            "abstract": "The paradigm of edge computing has formed an innovative scope within the domain of the Internet of Things (IoT) through expanding the services of the cloud to the network edge to design distributed architectures and securely enhance decision-making applications. Due to the heterogeneous, distributed and resource-constrained essence of edge Computing, edge applications are required to be developed as a set of lightweight and interdependent modules. As this concept aligns with the objectives of microservice architecture, effective implementation of microservices-based edge applications within IoT networks has the prospective of fully leveraging edge nodes capabilities. Deploying microservices at IoT edge faces plenty of challenges associated with security and privacy. Advances in Artificial Intelligence (AI) (especially Machine Learning), and the easy access to resources with powerful computing providing opportunities for deriving precise models and developing different intelligent applications at the edge of network. In this study, an extensive survey is presented for securing edge computing-based AI Microservices to elucidate the challenges of IoT management and enable secure decision-making systems at the edge. We present recent research studies on edge AI and microservices orchestration and highlight key requirements as well as challenges of securing Microservices at IoT edge. We also propose a Microservices-based edge computing framework that provides secure edge AI algorithms as Microservices utilizing the containerization technology to offer automated and secure AI-based applications at the network edge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1410094857",
                    "name": "Firas Al-Doghman"
                },
                {
                    "authorId": "3193106",
                    "name": "Nour Moustafa"
                },
                {
                    "authorId": "143664230",
                    "name": "I. Khalil"
                },
                {
                    "authorId": "1699399",
                    "name": "Z. Tari"
                },
                {
                    "authorId": "9392149",
                    "name": "Albert Y. Zomaya"
                }
            ]
        }
    ]
}