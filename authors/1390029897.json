{
    "authorId": "1390029897",
    "papers": [
        {
            "paperId": "f9bb309310be4265ecdbe08b6a11baf260a21d96",
            "title": "Discovering Car-following Dynamics from Trajectory Data through Deep Learning",
            "abstract": "This study aims to discover the governing mathematical expressions of car-following dynamics from trajectory data directly using deep learning techniques. We propose an expression exploration framework based on deep symbolic regression (DSR) integrated with a variable intersection selection (VIS) method to find variable combinations that encourage interpretable and parsimonious mathematical expressions. In the exploration learning process, two penalty terms are added to improve the reward function: (i) a complexity penalty to regulate the complexity of the explored expressions to be parsimonious, and (ii) a variable interaction penalty to encourage the expression exploration to focus on variable combinations that can best describe the data. We show the performance of the proposed method to learn several car-following dynamics models and discuss its limitations and future research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2003203063",
                    "name": "Ohay Angah"
                },
                {
                    "authorId": "1390029897",
                    "name": "James Enouen"
                },
                {
                    "authorId": "2259132427",
                    "name": "Xuegang (Jeff) Ban"
                },
                {
                    "authorId": "2237953539",
                    "name": "Yan Liu"
                }
            ]
        },
        {
            "paperId": "896e5cee54d50d7a1f981823b4627948610d72a5",
            "title": "Estimating Treatment Effects from Irregular Time Series Observations with Hidden Confounders",
            "abstract": "Causal analysis for time series data, in particular estimating individualized treatment effect (ITE), is a key task in many real world applications, such as finance, retail, healthcare, etc. Real world time series, i.e., large-scale irregular or sparse and intermittent time series, raise significant challenges to existing work attempting to estimate treatment effects. Specifically, the existence of hidden confounders can lead to biased treatment estimates and complicate the causal inference process. In particular, anomaly hidden confounders which exceed the typical range can lead to high variance estimates. Moreover, in continuous time settings with irregular samples, it is challenging to directly handle the dynamics of causality. In this paper, we leverage recent advances in Lipschitz regularization and neural controlled differential equations (CDE) to develop an effective and scalable solution, namely LipCDE, to address the above challenges. LipCDE can directly model the dynamic causal relationships between historical data and outcomes with irregular samples by considering the boundary of hidden confounders given by Lipschitz constrained neural networks. Furthermore, we conduct extensive experiments on both synthetic and real world datasets to demonstrate the effectiveness and scalability of LipCDE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120783624",
                    "name": "Defu Cao"
                },
                {
                    "authorId": "1390029897",
                    "name": "James Enouen"
                },
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "19214393",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "27737939",
                    "name": "Chuizheng Meng"
                },
                {
                    "authorId": "2223921871",
                    "name": "Hao Niu"
                },
                {
                    "authorId": "47909531",
                    "name": "Yan Liu"
                }
            ]
        },
        {
            "paperId": "8b7d2029744569449d79162e701724b12b9bbdfe",
            "title": "Estimating Treatment Effects in Continuous Time with Hidden Confounders",
            "abstract": "Estimating treatment effects plays a crucial role in causal inference, having many real-world applications like policy analysis and decision making. Nevertheless, estimating treatment effects in the longitudinal setting in the presence of hidden confounders remains an extremely challenging problem. Recently, there is a growing body of work attempting to obtain unbiased ITE estimates from time-dynamic observational data by ignoring the possible existence of hidden confounders. Additionally, many existing works handling hidden confounders are not applicable for continuous-time settings. In this paper, we extend the line of work focusing on deconfounding in the dynamic time setting in the presence of hidden confounders. We leverage recent advancements in neural differential equations to build a latent factor model using a stochastic controlled differential equation and Lipschitz constrained convolutional operation in order to continuously incorporate information about ongoing interventions and irregularly sampled observations. Experiments on both synthetic and real-world datasets highlight the promise of continuous time methods for estimating treatment effects in the presence of hidden confounders.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "120783624",
                    "name": "Defu Cao"
                },
                {
                    "authorId": "1390029897",
                    "name": "James Enouen"
                },
                {
                    "authorId": "1679704",
                    "name": "Y. Liu"
                }
            ]
        },
        {
            "paperId": "c3ec2a49c5a09308a496ea21424dcd3eb812a754",
            "title": "TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents",
            "abstract": "Large language models (LLMs) have attracted huge interest in practical applications given their increasingly accurate responses and coherent reasoning abilities. Given their nature as black-boxes using complex reasoning processes on their inputs, it is inevitable that the demand for scalable and faithful explanations for LLMs' generated content will continue to grow. There have been major developments in the explainability of neural network models over the past decade. Among them, post-hoc explainability methods, especially Shapley values, have proven effective for interpreting deep learning models. However, there are major challenges in scaling up Shapley values for LLMs, particularly when dealing with long input contexts containing thousands of tokens and autoregressively generated output sequences. Furthermore, it is often unclear how to effectively utilize generated explanations to improve the performance of LLMs. In this paper, we introduce TextGenSHAP, an efficient post-hoc explanation method incorporating LM-specific techniques. We demonstrate that this leads to significant increases in speed compared to conventional Shapley value computations, reducing processing times from hours to minutes for token-level explanations, and to just seconds for document-level explanations. In addition, we demonstrate how real-time Shapley values can be utilized in two important scenarios, providing better understanding of long-document question answering by localizing important words and sentences; and improving existing document retrieval systems through enhancing the accuracy of selected passages and ultimately the final responses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1390029897",
                    "name": "James Enouen"
                },
                {
                    "authorId": "3346298",
                    "name": "Hootan Nakhost"
                },
                {
                    "authorId": "27556211",
                    "name": "Sayna Ebrahimi"
                },
                {
                    "authorId": "2676352",
                    "name": "Sercan \u00d6. Arik"
                },
                {
                    "authorId": "2257088730",
                    "name": "Yan Liu"
                },
                {
                    "authorId": "2264567300",
                    "name": "Tomas Pfister"
                }
            ]
        },
        {
            "paperId": "e706c89c914aaa5c58502fa69b2112ff15c4c40d",
            "title": "Measuring, Interpreting, and Improving Fairness of Algorithms using Causal Inference and Randomized Experiments",
            "abstract": "Algorithm fairness has become a central problem for the broad adoption of artificial intelligence. Although the past decade has witnessed an explosion of excellent work studying algorithm biases, achieving fairness in real-world AI production systems has remained a challenging task. Most existing works fail to excel in practical applications since either they have conflicting measurement techniques and/ or heavy assumptions, or require code-access of the production models, whereas real systems demand an easy-to-implement measurement framework and a systematic way to correct the detected sources of bias. In this paper, we leverage recent advances in causal inference and interpretable machine learning to present an algorithm-agnostic framework (MIIF) to Measure, Interpret, and Improve the Fairness of an algorithmic decision. We measure the algorithm bias using randomized experiments, which enables the simultaneous measurement of disparate treatment, disparate impact, and economic value. Furthermore, using modern interpretability techniques, we develop an explainable machine learning model which accurately interprets and distills the beliefs of a blackbox algorithm. Altogether, these techniques create a simple and powerful toolset for studying algorithm fairness, especially for understanding the cost of fairness in practical applications like e-commerce and targeted advertising, where industry A/B testing is already abundant.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1390029897",
                    "name": "James Enouen"
                },
                {
                    "authorId": "2238110900",
                    "name": "Tianshu Sun"
                },
                {
                    "authorId": "2237953539",
                    "name": "Yan Liu"
                }
            ]
        },
        {
            "paperId": "e8d9a0ca810bc4771a84a6681c4f4aec7345f54a",
            "title": "Sparse Interaction Additive Networks via Feature Interaction Detection and Sparse Selection",
            "abstract": "There is currently a large gap in performance between the statistically rigorous methods like linear regression or additive splines and the powerful deep methods using neural networks. Previous works attempting to close this gap have failed to fully investigate the exponentially growing number of feature combinations which deep networks consider automatically during training. In this work, we develop a tractable selection algorithm to efficiently identify the necessary feature combinations by leveraging techniques in feature interaction detection. Our proposed Sparse Interaction Additive Networks (SIAN) construct a bridge from these simple and interpretable models to fully connected neural networks. SIAN achieves competitive performance against state-of-the-art methods across multiple large-scale tabular datasets and consistently finds an optimal tradeoff between the modeling capacity of neural networks and the generalizability of simpler methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1390029897",
                    "name": "James Enouen"
                },
                {
                    "authorId": "2157170466",
                    "name": "Yan Liu"
                }
            ]
        },
        {
            "paperId": "1d7e5fea0979f697c951f94e6f5bf17a74218233",
            "title": "Interpretable Artificial Intelligence through the Lens of Feature Interaction",
            "abstract": "Interpretation of deep learning models is a very challenging problem because of their large number of parameters, complex connections between nodes, and unintelligible feature representations. Despite this, many view interpretability as a key solution to trustworthiness, fairness, and safety, especially as deep learning is applied to more critical decision tasks like credit approval, job screening, and recidivism prediction. There is an abundance of good research providing interpretability to deep learning models; however, many of the commonly used methods do not consider a phenomenon called\"feature interaction.\"This work first explains the historical and modern importance of feature interactions and then surveys the modern interpretability methods which do explicitly consider feature interactions. This survey aims to bring to light the importance of feature interactions in the larger context of machine learning interpretability, especially in a modern context where deep learning models heavily rely on feature interactions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1879592819",
                    "name": "Michael Tsang"
                },
                {
                    "authorId": "1390029897",
                    "name": "James Enouen"
                },
                {
                    "authorId": "47909587",
                    "name": "Yan Liu"
                }
            ]
        }
    ]
}