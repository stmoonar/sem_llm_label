{
    "authorId": "1864044654",
    "papers": [
        {
            "paperId": "3c2e5eb128af974ce8a21c189c6f06ebb671a2f8",
            "title": "Controlling High-Dimensional Data With Sparse Input",
            "abstract": "We address the problem of human-in-the-loop control for generating highly-structured data. This task is challenging because existing generative models lack an ef\ufb01cient interface through which users can modify the output. Users have the option to either manually explore a non-interpretable latent space, or to laboriously annotate the data with conditioning labels. To solve this, we introduce a novel framework whereby an encoder maps a sparse, human-interpretable control space onto the latent space of a generative model. We apply this framework to the task of controlling prosody in text-to-speech synthesis. We pro-pose a model, called Multiple-Instance CVAE (MICVAE), that is speci\ufb01cally designed to encode sparse prosodic features and output complete waveforms. We show empirically that MICVAE displays desirable qualities of a sparse human-in-the-loop control mechanism: ef\ufb01ciency, robustness, and faithfulness. With even a very small number of input values ( ~ 4), MICVAE enables users to improve the quality of the output signi\ufb01-cantly, in terms of listener preference (4:1).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66962453",
                    "name": "Dan-Andrei Iliescu"
                },
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "51263680",
                    "name": "Zack Hodari"
                }
            ]
        },
        {
            "paperId": "70dfac2c00f26ea3c372aab8213d8eb8b9ca2062",
            "title": "Ensemble Prosody Prediction For Expressive Speech Synthesis",
            "abstract": "Generating expressive speech with rich and varied prosody continues to be a challenge for Text-to-Speech. Most efforts have focused on sophisticated neural architectures intended to better model the data distribution. Yet, in evaluations it is generally found that no single model is preferred for all input texts. This suggests an approach that has rarely been used before for Text-to-Speech: an ensemble of models.We apply ensemble learning to prosody prediction. We construct simple ensembles of prosody predictors by varying either model architecture or model parameter values.To automatically select amongst the models in the ensemble when performing Text-to-Speech, we propose a novel, and computationally trivial, variance-based criterion. We demonstrate that even a small ensemble of prosody predictors yields useful diversity, which, combined with the proposed selection criterion, outperforms any individual model from the ensemble.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "50161308",
                    "name": "Vivian Hu"
                },
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "51263680",
                    "name": "Zack Hodari"
                },
                {
                    "authorId": "2213297845",
                    "name": "Christopher G. R. Wallis"
                },
                {
                    "authorId": "90190727",
                    "name": "Tom\u00e1s G\u00f3mez Ibarrondo"
                },
                {
                    "authorId": "1863935733",
                    "name": "Alexandra Torresquintero"
                },
                {
                    "authorId": "2213305354",
                    "name": "James Leoni"
                },
                {
                    "authorId": "1740397",
                    "name": "M. Gales"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "7b16c64fb656328e4fac6383ef3c290ff87e26ab",
            "title": "Controllable Prosody Generation with Partial Inputs",
            "abstract": "We address the problem of human-in-the-loop control for generating prosody in the context of text-to-speech synthesis. Controlling prosody is challenging because existing generative models lack an efficient interface through which users can modify the output quickly and precisely. To solve this, we introduce a novel framework whereby the user provides partial inputs and the generative model generates the missing features. We propose a model that is specifically designed to encode partial prosodic features and output complete audio. We show empirically that our model displays two essential qualities of a human-in-the-loop control mechanism: efficiency and robustness. With even a very small number of input values (~4), our model enables users to improve the quality of the output significantly in terms of listener preference (4:1).",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "66962453",
                    "name": "Dan-Andrei Iliescu"
                },
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "51263680",
                    "name": "Zack Hodari"
                }
            ]
        },
        {
            "paperId": "2758d54982811e19d63d6998014de94f4ac42da3",
            "title": "Performance Improvement in Speech Enabled IVR Systems Using Artificial Band Width Extension: Recent Development",
            "abstract": "The existing time-consuming menu-driven IVRs have been replaced by new blends of Speech Powered Interactive Voice Response (SEIVR) Systems. When transmitted via telecom networks that use narrowband codecs, however, the quality and intelligibility of the speech signal suffers significantly. Providing wideband quality signal without much modification of the existing network infrastructure can only be possible with a novel technique of Artificial Band Width Extension (ABWE). ABWE is implemented in this paper using a QMF filter bank that band splits the input speech into LF and HF components before compressing and encoding the HF components using a novel data hiding technique. A QMF synthesis filter bank is used in the reconstruction process to create an artificial wideband speech signal. For implementation of the proposed model, a client-server approach with socket programming on a single machine has been used assuming no noise and no transmission errors. A comparative analysis has also been done to find out the root cause for degradation in performances of SEIVR systems. The proposed SEIVR model with ABWE has shown a major improvement in speech recognition accuracy and overall efficiency in simulations. Based on the simulation findings, it can be concluded that the recognition accuracy of an artificially extended NB speech signal is superior to that of an NB speech signal and comparable to that of a WB speech signal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "2920646",
                    "name": "K. Sheela"
                }
            ]
        },
        {
            "paperId": "55e666881f2b51ba94ab504a0131707da4a21ed2",
            "title": "Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis",
            "abstract": "Text does not fully specify the spoken form, so text-to-speech models must be able to learn from speech data that vary in ways not explained by the corresponding text. One way to reduce the amount of unexplained variation in training data is to provide acoustic information as an additional learning signal. When generating speech, modifying this acoustic information enables multiple distinct renditions of a text to be produced. Since much of the unexplained variation is in the prosody, we propose a model that generates speech explicitly conditioned on the three primary acoustic correlates of prosody: $F_{0}$, energy and duration. The model is flexible about how the values of these features are specified: they can be externally provided, or predicted from text, or predicted then subsequently modified. Compared to a model that employs a variational auto-encoder to learn unsupervised latent features, our model provides more interpretable, temporally-precise, and disentangled control. When automatically predicting the acoustic features from text, it generates speech that is more natural than that from a Tacotron 2 model with reference encoder. Subsequent human-in-the-loop modification of the predicted acoustic features can significantly further increase naturalness.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "3330285",
                    "name": "Qinmin Hu"
                },
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "1863935733",
                    "name": "Alexandra Torresquintero"
                },
                {
                    "authorId": "23143903",
                    "name": "C. Wallis"
                },
                {
                    "authorId": "51251744",
                    "name": "Marlene Staib"
                },
                {
                    "authorId": "1863936190",
                    "name": "Lorenzo Foglianti"
                },
                {
                    "authorId": "8418520",
                    "name": "Jiameng Gao"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "5ca6643b4dde05a51035eb8f91d9a2dfc5964203",
            "title": "ADEPT: A Dataset for Evaluating Prosody Transfer",
            "abstract": "Text-to-speech is now able to achieve near-human naturalness and research focus has shifted to increasing expressivity. One popular method is to transfer the prosody from a reference speech sample. There have been considerable advances in using prosody transfer to generate more expressive speech, but the field lacks a clear definition of what successful prosody transfer means and a method for measuring it. We introduce a dataset of prosodically-varied reference natural speech samples for evaluating prosody transfer. The samples include global variations reflecting emotion and interpersonal attitude, and local variations reflecting topical emphasis, propositional attitude, syntactic phrasing and marked tonicity. The corpus only includes prosodic variations that listeners are able to distinguish with reasonable accuracy, and we report these figures as a benchmark against which text-to-speech prosody transfer can be compared. We conclude the paper with a demonstration of our proposed evaluation methodology, using the corpus to evaluate two text-to-speech models that perform prosody transfer.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1863935733",
                    "name": "Alexandra Torresquintero"
                },
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "40445010",
                    "name": "C. Wallis"
                },
                {
                    "authorId": "51251744",
                    "name": "Marlene Staib"
                },
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "50161308",
                    "name": "Vivian Hu"
                },
                {
                    "authorId": "1863936190",
                    "name": "Lorenzo Foglianti"
                },
                {
                    "authorId": "8418520",
                    "name": "Jiameng Gao"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "6a662878e7396f2507eecbe95fbb6162d0bf12ac",
            "title": "Phonological Features for 0-shot Multilingual Speech Synthesis",
            "abstract": "Code-switching---the intra-utterance use of multiple languages---is prevalent across the world. Within text-to-speech (TTS), multilingual models have been found to enable code-switching. By modifying the linguistic input to sequence-to-sequence TTS, we show that code-switching is possible for languages unseen during training, even within monolingual models. We use a small set of phonological features derived from the International Phonetic Alphabet (IPA), such as vowel height and frontness, consonant place and manner. This allows the model topology to stay unchanged for different languages, and enables new, previously unseen feature combinations to be interpreted by the model. We show that this allows us to generate intelligible, code-switched speech in a new language at test time, including the approximation of sounds never seen in training.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "51251744",
                    "name": "Marlene Staib"
                },
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "1863935733",
                    "name": "Alexandra Torresquintero"
                },
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "1863936190",
                    "name": "Lorenzo Foglianti"
                },
                {
                    "authorId": "1403266846",
                    "name": "R. Lenain"
                },
                {
                    "authorId": "8418520",
                    "name": "Jiameng Gao"
                }
            ]
        },
        {
            "paperId": "c316f7012cc6a459cc6044b7ac384e3c41739fc3",
            "title": "Incremental Text to Speech for Neural Sequence-to-Sequence Models using Reinforcement Learning",
            "abstract": "Modern approaches to text to speech require the entire input character sequence to be processed before any audio is synthesised. This latency limits the suitability of such models for time-sensitive tasks like simultaneous interpretation. Interleaving the action of reading a character with that of synthesising audio reduces this latency. However, the order of this sequence of interleaved actions varies across sentences, which raises the question of how the actions should be chosen. We propose a reinforcement learning based framework to train an agent to make this decision. We compare our performance against that of deterministic, rule-based systems. Our results demonstrate that our agent successfully balances the trade-off between the latency of audio generation and the quality of synthesised audio. More broadly, we show that neural sequence-to-sequence models can be adapted to run in an incremental manner.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "1403266846",
                    "name": "R. Lenain"
                },
                {
                    "authorId": "1863936190",
                    "name": "Lorenzo Foglianti"
                },
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "51251744",
                    "name": "Marlene Staib"
                },
                {
                    "authorId": "1863935733",
                    "name": "Alexandra Torresquintero"
                },
                {
                    "authorId": "8418520",
                    "name": "Jiameng Gao"
                }
            ]
        }
    ]
}