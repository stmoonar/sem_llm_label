{
    "authorId": "1485105824",
    "papers": [
        {
            "paperId": "115297df4ff7f75150752a707ef57e8726f81370",
            "title": "Intensity-free Integral-based Learning of Marked Temporal Point Processes",
            "abstract": "In the marked temporal point processes (MTPP), a core problem is to parameterize the conditional joint PDF (probability distribution function) $p^*(m,t)$ for inter-event time $t$ and mark $m$, conditioned on the history. The majority of existing studies predefine intensity functions. Their utility is challenged by specifying the intensity function's proper form, which is critical to balance expressiveness and processing efficiency. Recently, there are studies moving away from predefining the intensity function -- one models $p^*(t)$ and $p^*(m)$ separately, while the other focuses on temporal point processes (TPPs), which do not consider marks. This study aims to develop high-fidelity $p^*(m,t)$ for discrete events where the event marks are either categorical or numeric in a multi-dimensional continuous space. We propose a solution framework IFIB (\\underline{I}ntensity-\\underline{f}ree \\underline{I}ntegral-\\underline{b}ased process) that models conditional joint PDF $p^*(m,t)$ directly without intensity functions. It remarkably simplifies the process to compel the essential mathematical restrictions. We show the desired properties of IFIB and the superior experimental results of IFIB on real-world and synthetic datasets. The code is available at \\url{https://github.com/StepinSilence/IFIB}.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2108638322",
                    "name": "Sishun Liu"
                },
                {
                    "authorId": "2142590602",
                    "name": "Ke Deng"
                },
                {
                    "authorId": "1485105824",
                    "name": "Xiuzhen Zhang"
                },
                {
                    "authorId": "7800596",
                    "name": "Yongli Ren"
                }
            ]
        },
        {
            "paperId": "377bb27f5069dcd526f1c4bb1cab70fa471e3452",
            "title": "Modeling User Demand Evolution for Next-Basket Prediction",
            "abstract": "Users\u2019 purchase behaviors are complex and dynamic, which are usually driven by various personal demands evolving with time. According to psychology and economic theories, user demands can be satisfied with a sequence of purchase behaviors, resulting in a basket of items. However, most of the existing works simply predict the next basket from a shallow perspective of (purchase) sequence data modeling without deep insight into the underlying factors which drive user purchase behaviors. In fact, filling a basket with multiple items is a process to incrementally satisfy a user's demand. Therefore, the key challenges to predict a user's next basket lie in (1) how to track the changes of the user's demand, and (2) how to satisfy her demand at a given moment. To this end, we propose an Evolving DEmand SAtisfaction (EvoDESA) model to model a user's demand evolution for next-basket prediction. In EvoDESA, a demand evolution module learns the dynamics of user demand over a sequence of basket-purchase behaviors. Then, a next-basket planning module effectively packs an optimal combination of items to best satisfy the user's current demand. Extensive experiments on three real-world transaction datasets demonstrate the considerable superiority of EvoDESA over the state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116951322",
                    "name": "Shoujin Wang"
                },
                {
                    "authorId": "2152541925",
                    "name": "Yan Wang"
                },
                {
                    "authorId": "2118850040",
                    "name": "Liang Hu"
                },
                {
                    "authorId": "1485105824",
                    "name": "Xiuzhen Zhang"
                },
                {
                    "authorId": "2145908596",
                    "name": "Qi Zhang"
                },
                {
                    "authorId": "1713128",
                    "name": "Quan Z. Sheng"
                },
                {
                    "authorId": "145572420",
                    "name": "M. Orgun"
                },
                {
                    "authorId": "2184095458",
                    "name": "Longbin Cao"
                },
                {
                    "authorId": "1862782",
                    "name": "Defu Lian"
                }
            ]
        },
        {
            "paperId": "98f7607cab97329dd24510c70f2cc9b2e546effd",
            "title": "Task and Sentiment Adaptation for Appraisal Tagging",
            "abstract": "The Appraisal framework in linguistics defines the framework for fine-grained evaluations and opinions and has contributed to sentiment analysis and opinion mining. As developing appraisal-annotated resources requires tagging of several dimensions with distinct semantic taxonomies, it has been primarily conducted manually by human experts through expensive and time-consuming processes. In this paper, we study how to automatically identify and annotate text segments for appraisal. We formulate the problem as a sequence tagging problem and propose novel task and sentiment adapters based on language models for appraisal tagging. Our model, named Adaptive Appraisal (A$\u02c62$), achieves superior performance than baseline adapter-based models and other neural classification models, especially for cross-domain and cross-language settings. Source code for A$\u02c62$ is available at: https://github.com/ltian678/AA-code.git",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115833130",
                    "name": "Lin Tian"
                },
                {
                    "authorId": "1485105824",
                    "name": "Xiuzhen Zhang"
                },
                {
                    "authorId": "2171012896",
                    "name": "Myung Hee Kim"
                },
                {
                    "authorId": "2058850410",
                    "name": "J. Biggs"
                }
            ]
        },
        {
            "paperId": "a91d654c97ff69ea004c2bbb93eb6b58f8a9a8c9",
            "title": "Examining Bias in Opinion Summarisation through the Perspective of Opinion Diversity",
            "abstract": "Opinion summarisation is a task that aims to condense the information presented in the source documents while retaining the core message and opinions. A summary that only represents the majority opinions will leave the minority opinions unrepresented in the summary. In this paper, we use the stance towards a certain target as an opinion. We study bias in opinion summarisation from the perspective of opinion diversity, which measures whether the model generated summary can cover a diverse set of opinions. In addition, we examine opinion similarity, a measure of how closely related two opinions are in terms of their stance on a given topic, and its relationship with opinion diversity. Through the lense of stances towards a topic, we examine opinion diversity and similarity using three debatable topics under COVID-19. Experimental results on these topics revealed that a higher degree of similarity of opinions did not indicate good diversity or fairly cover the various opinions originally presented in the source documents. We found that BART and ChatGPT can better capture diverse opinions presented in the source documents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054020542",
                    "name": "Nannan Huang"
                },
                {
                    "authorId": "2115833130",
                    "name": "Lin Tian"
                },
                {
                    "authorId": "1822214",
                    "name": "Haytham M. Fayek"
                },
                {
                    "authorId": "1485105824",
                    "name": "Xiuzhen Zhang"
                }
            ]
        },
        {
            "paperId": "d440a180fdd177a455896bf5ae7de37cdc91630b",
            "title": "MetaTroll: Few-shot Detection of State-Sponsored Trolls with Transformer Adapters",
            "abstract": "State-sponsored trolls are the main actors of influence campaigns on social media and automatic troll detection is important to combat misinformation at scale. Existing troll detection models are developed based on training data for known campaigns (e.g. the influence campaign by Russia\u2019s Internet Research Agency on the 2016 US Election), and they fall short when dealing with novel campaigns with new targets. We propose MetaTroll, a text-based troll detection model based on the meta-learning framework that enables high portability and parameter-efficient adaptation to new campaigns using only a handful of labelled samples for few-shot transfer. We introduce campaign-specific transformer adapters to MetaTroll to \u201cmemorise\u201d campaign-specific knowledge so as to tackle catastrophic forgetting, where a model \u201cforgets\u201d how to detect trolls from older campaigns due to continual adaptation. Our experiments demonstrate that MetaTroll substantially outperforms baselines and state-of-the-art few-shot text classification models. Lastly, we explore simple approaches to extend MetaTroll to multilingual and multimodal detection. Source code for MetaTroll is available at: https://github.com/ltian678/metatroll-code.git",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115833130",
                    "name": "Lin Tian"
                },
                {
                    "authorId": "1485105824",
                    "name": "Xiuzhen Zhang"
                },
                {
                    "authorId": "1800564",
                    "name": "Jey Han Lau"
                }
            ]
        },
        {
            "paperId": "0ba43b22279123fb7cc4696c3da18a61fb3cff46",
            "title": "The Secondary Use of Electronic Health Records for Data Mining: Data Characteristics and Challenges",
            "abstract": "The primary objective of implementing Electronic Health Records (EHRs) is to improve the management of patients\u2019 health-related information. However, these records have also been extensively used for the secondary purpose of clinical research and to improve healthcare practice. EHRs provide a rich set of information that includes demographics, medical history, medications, laboratory test results, and diagnosis. Data mining and analytics techniques have extensively exploited EHR information to study patient cohorts for various clinical and research applications, such as phenotype extraction, precision medicine, intervention evaluation, disease prediction, detection, and progression. But the presence of diverse data types and associated characteristics poses many challenges to the use of EHR data. In this article, we provide an overview of information found in EHR systems and their characteristics that could be utilized for secondary applications. We first discuss the different types of data stored in EHRs, followed by the data transformations necessary for data analysis and mining. Later, we discuss the data quality issues and characteristics of the EHRs along with the relevant methods used to address them. Moreover, this survey also highlights the usage of various data types for different applications. Hence, this article can serve as a primer for researchers to understand the use of EHRs for data mining and analytics purposes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143900595",
                    "name": "Tabinda Sarwar"
                },
                {
                    "authorId": "2273903328",
                    "name": "Sattar Seifollahi"
                },
                {
                    "authorId": "50732243",
                    "name": "Jeffrey A Chan"
                },
                {
                    "authorId": "1485105824",
                    "name": "Xiuzhen Zhang"
                },
                {
                    "authorId": "2602767",
                    "name": "V. Aksakalli"
                },
                {
                    "authorId": "2150527392",
                    "name": "I. Hudson"
                },
                {
                    "authorId": "144765178",
                    "name": "Karin M. Verspoor"
                },
                {
                    "authorId": "1788025",
                    "name": "L. Cavedon"
                }
            ]
        },
        {
            "paperId": "1fc2a69e01dbbde7e39ddedf63da15c5641989e0",
            "title": "Mitigation of Rumours in Social Networks via Epidemic Model-based Reinforcement Learning",
            "abstract": "While detection of rumours in online social networks has been intensively studied in the literature, mitigation of the spread of rumours has only recently gained attention and remains a challenging task. Some studies developed user opinion models to find top influential users as debunkers to spread the truth to counter rumour spread. Other studies designed an intervention framework to optimize the mitigation activities for given debunkers. The issue of optimizing the selection of debunkers in a dynamic environment where users\u2019 beliefs and behaviour change remains under investigated. This paper addresses this issue by proposing a rumour mitigation approach based on the deep reinforcement learning framework. In particular, we model the changes in users\u2019 beliefs with an epidemic model. We further employ deep reinforcement learning to train an agent to learn a multi-stage policy for selecting the optimal debunkers to inject truthful information under a budget constraint. Our model selects debunkers to inject truthful information at multiple stages with an overall objective to maximize the number of users who will believe in the true information (a.k.a number of recovered nodes), such that the spread of rumours is minimized. Our experiments on synthetic and real-world social networks show that our proposed method for rumour mitigation can effectively minimize the spread of rumours.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39978184",
                    "name": "Hao Nie"
                },
                {
                    "authorId": "1485105824",
                    "name": "Xiuzhen Zhang"
                },
                {
                    "authorId": "2108928356",
                    "name": "Minyi Li"
                },
                {
                    "authorId": "2064071333",
                    "name": "Anil Dolgun"
                }
            ]
        },
        {
            "paperId": "22b5c54dd01c705f629332640274c0e9bf746c39",
            "title": "Identifying Cost-effective Debunkers for Multi-stage Fake News Mitigation Campaigns",
            "abstract": "Online social networks have become a fertile ground for spreading fake news. Methods to automatically mitigate fake news propagation have been proposed. Some studies focus on selecting top k influential users on social networks as debunkers, but the social influence of debunkers may not translate to wide mitigation information propagation as expected. Other studies assume a given set of debunkers and focus on optimizing intensity for debunkers to publish true news, but as debunkers are fixed, even if with high social influence and/or high intensity to post true news, the true news may not reach users exposed to fake news and therefore mitigation effect may be limited. In this paper, we propose the multi-stage fake news mitigation campaign where debunkers are dynamically selected within budget at each stage. We formulate it as a reinforcement learning problem and propose a greedy algorithm optimized by predicting future states so that the debunkers can be selected in a way that maximizes the overall mitigation effect. We conducted extensive experiments on synthetic and real-world social networks and show that our solution outperforms state-of-the-art baselines in terms of mitigation effect.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155346103",
                    "name": "Xiaofei Xu"
                },
                {
                    "authorId": "47826127",
                    "name": "Ke Deng"
                },
                {
                    "authorId": "1485105824",
                    "name": "Xiuzhen Zhang"
                }
            ]
        },
        {
            "paperId": "66240216d9c1cac7c3eec92f0c6815c482998a77",
            "title": "Data Science and Artificial Intelligence for Responsible Recommendations",
            "abstract": "With the advancement of data science and AI, more and more powerful and accurate recommender systems (RSs) have been developed. They provide recommendation services in various areas, including shopping, eating, travelling and entertainment. RSs have achieved a great success and benefted the society. However, most of the research on RS has focused on the improvement of the recommendation accuracy, while ignoring other important qualities, such as trustworthiness (robustness, fairness, explainability, privacy and security) and social impact (influence on users' recognition and behaviours) of the recommendations. These are important aspects and cannot be overlooked since they measure properties that determine whether the recommendation service is reliable, trustworthy and benefcial to individual users and society. In this work, responsible recommendations refer to trustworthy recommendation techniques and positive-social-impact recommendation results. This workshop aims to engage with active researchers from the RS community, and other communities, as social science, to discuss state-of-the-art research results related to the core challenges of responsible recommendation services. We will focus on two main topics of responsible RSs: (1) developing reliable and trustworthy RS models and algorithms, to provide reliable recommendation results when facing a complex, uncertain and dynamic scenario; (2) assessing the social influence of RSs on human's recognition and behaviours and ensuring the influence is positive to the society.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116951322",
                    "name": "Shoujin Wang"
                },
                {
                    "authorId": "47717322",
                    "name": "Ninghao Liu"
                },
                {
                    "authorId": "1485105824",
                    "name": "Xiuzhen Zhang"
                },
                {
                    "authorId": "2152541925",
                    "name": "Yan Wang"
                },
                {
                    "authorId": "40169260",
                    "name": "F. Ricci"
                },
                {
                    "authorId": "1684679",
                    "name": "B. Mobasher"
                }
            ]
        },
        {
            "paperId": "6853c273860838a9e41e48c24fc5968c31f21a9e",
            "title": "DUCK: Rumour Detection on Social Media by Modelling User and Comment Propagation Networks",
            "abstract": "Social media rumours, a form of misinformation, can mislead the public and cause significant economic and social disruption. Motivated by the observation that the user network \u2014 which captures \\textit{who} engage with a story \u2014 and the comment network \u2014 which captures \\textit{how} they react to it \u2014 provide complementary signals for rumour detection, in this paper, we propose DUCK (rumour \\underline{d}etection with \\underline{u}ser and \\underline{c}omment networ\\underline{k}s) for rumour detection on social media. We study how to leverage transformers and graph attention networks to jointly model the contents and structure of social media conversations, as well as the network of users who engaged in these conversations. Over four widely used benchmark rumour datasets in English and Chinese, we show that DUCK produces superior performance for detecting rumours, creating a new state-of-the-art. Source code for DUCK is available at: https://github.com/l tian678/DUCK-code.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Lin Tian"
                },
                {
                    "authorId": "1485105824",
                    "name": "Xiuzhen Zhang"
                },
                {
                    "authorId": "1800564",
                    "name": "Jey Han Lau"
                }
            ]
        }
    ]
}