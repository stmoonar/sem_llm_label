{
    "authorId": "3407646",
    "papers": [
        {
            "paperId": "11a571eaab42a6ffb1d938635a093315e392756d",
            "title": "ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages",
            "abstract": "Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240569372",
                    "name": "Nathaniel R. Robinson"
                },
                {
                    "authorId": "1988654955",
                    "name": "Perez Ogayo"
                },
                {
                    "authorId": "3407646",
                    "name": "David R. Mortensen"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "17fbffb05fa14e21d1c506fd5f0f568b955fe983",
            "title": "Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models",
            "abstract": "Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1452686038",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "51467955",
                    "name": "Sachin Kumar"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "11348687",
                    "name": "Jungo Kasai"
                },
                {
                    "authorId": "3407646",
                    "name": "David R. Mortensen"
                },
                {
                    "authorId": "144365875",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "377253f69af6699df9114d2819e03f341f83e8d3",
            "title": "African Substrates Rather Than European Lexifiers to Augment African-diaspora Creole Translation",
            "abstract": "Machine translation (MT) model training is difficult for low-resource languages. This is especially true for African-diaspora Creole languages because of data scarcity. Cross-lingual data augmentation methods with knowledge transfer from related high-resource languages are a common technique to overcome this disadvantage. For instance, practitioners may transfer knowledge from a language in the same language family as the low-resource language of interest. Africandiaspora Creole languages are low-resource and simultaneously have relationships with multiple language groups. These languages, such as Haitian Creole and Jamaican Patois, are typically lexified by colonial European languages, but they are structurally similar to African languages. We explore the advantages of transferring knowledge from the European lexifier language versus the phylogenetic and typological relatives of the African substrate languages. We analysed Haitian and Jamaican MT: both controlling tightly for data properties across compared transfer languages and later allowing use of all data we collected. Our inquiry demonstrates a significant advantage in using African transfer languages in some settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3407646",
                    "name": "David R. Mortensen"
                }
            ]
        },
        {
            "paperId": "659be1ff350634f50cc066d258ee6a45e697e552",
            "title": "SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing",
            "abstract": "In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2107034039",
                    "name": "Taiqi He"
                },
                {
                    "authorId": "2219036626",
                    "name": "Lindia Tjuatja"
                },
                {
                    "authorId": "2067645226",
                    "name": "Nathaniel R. Robinson"
                },
                {
                    "authorId": "1746678",
                    "name": "Shinji Watanabe"
                },
                {
                    "authorId": "3407646",
                    "name": "David R. Mortensen"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                },
                {
                    "authorId": "145585627",
                    "name": "Lori S. Levin"
                }
            ]
        },
        {
            "paperId": "7a08051aac75a809737096e39820bf836908d4e1",
            "title": "Construction Grammar Provides Unique Insight into Neural Language Models",
            "abstract": "Construction Grammar (CxG) has recently been used as the basis for probing studies that have investigated the performance of large pretrained language models (PLMs) with respect to the structure and meaning of constructions. In this position paper, we make suggestions for the continuation and augmentation of this line of research. We look at probing methodology that was not designed with CxG in mind, as well as probing methodology that was designed for specific constructions. We analyse selected previous work in detail, and provide our view of the most important challenges and research questions that this promising new field faces.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31832571",
                    "name": "Leonie Weissweiler"
                },
                {
                    "authorId": "2107034039",
                    "name": "Taiqi He"
                },
                {
                    "authorId": "145671279",
                    "name": "Naoki Otani"
                },
                {
                    "authorId": "3407646",
                    "name": "David R. Mortensen"
                },
                {
                    "authorId": "145585627",
                    "name": "Lori S. Levin"
                },
                {
                    "authorId": "144418438",
                    "name": "Hinrich Sch\u00fctze"
                }
            ]
        },
        {
            "paperId": "bf42c0462d1415cdde877c90d58da11545407b8a",
            "title": "Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation",
            "abstract": "Interlinear glossing provides a vital type of morphosyntactic annotation, both for linguists and language revitalists, and numerous conventions exist for representing it formally and computationally. Some of these formats are human readable; others are machine readable. Some are easy to edit with general-purpose tools. Few represent non-concatentative processes like infixation, reduplication, mutation, truncation, and tonal overwriting in a consistent and formally rigorous way (on par with affixation). We propose an annotation convention\u00e2\u20ac\u201dGeneralized Glossing Guidelines (GGG) that combines all of these positive properties using an Item-and-Process (IP) framework. We describe the format, demonstrate its linguistic adequacy, and compare it with two other interlinear glossed text annotation schemes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3407646",
                    "name": "David R. Mortensen"
                },
                {
                    "authorId": "2223096956",
                    "name": "Ela Gulsen"
                },
                {
                    "authorId": "2107034039",
                    "name": "Taiqi He"
                },
                {
                    "authorId": "2067645226",
                    "name": "Nathaniel R. Robinson"
                },
                {
                    "authorId": "1798729",
                    "name": "Jonathan D. Amith"
                },
                {
                    "authorId": "2219036626",
                    "name": "Lindia Tjuatja"
                },
                {
                    "authorId": "145585627",
                    "name": "Lori S. Levin"
                }
            ]
        },
        {
            "paperId": "c5c6d006e399386c99068daba138021a62d6cc17",
            "title": "Transformed Protoform Reconstruction",
            "abstract": "Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2221149804",
                    "name": "Young Min Kim"
                },
                {
                    "authorId": "2187555978",
                    "name": "Kalvin Chang"
                },
                {
                    "authorId": "31248497",
                    "name": "Chenxuan Cui"
                },
                {
                    "authorId": "3407646",
                    "name": "David R. Mortensen"
                }
            ]
        },
        {
            "paperId": "db14d05b18ec852f8afcd6d2d10bbd9eeaef8325",
            "title": "PWESuite: Phonetic Word Embeddings and Tasks They Facilitate",
            "abstract": "Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1429837660",
                    "name": "Vil\u00e9m Zouhar"
                },
                {
                    "authorId": "2187555978",
                    "name": "Kalvin Chang"
                },
                {
                    "authorId": "31248497",
                    "name": "Chenxuan Cui"
                },
                {
                    "authorId": "2190909003",
                    "name": "Nathaniel Carlson"
                },
                {
                    "authorId": "2067645226",
                    "name": "Nathaniel R. Robinson"
                },
                {
                    "authorId": "2790926",
                    "name": "Mrinmaya Sachan"
                },
                {
                    "authorId": "3407646",
                    "name": "David R. Mortensen"
                }
            ]
        },
        {
            "paperId": "11311c3688963a993c079ca3078c1fcf36b987e5",
            "title": "Phone Inventories and Recognition for Every Language",
            "abstract": "Identifying phone inventories is a crucial component in language documentation and the preservation of endangered languages. However, even the largest collection of phone inventory only covers about 2000 languages, which is only 1/4 of the total number of languages in the world. A majority of the remaining languages are endangered. In this work, we attempt to solve this problem by estimating the phone inventory for any language listed in Glottolog, which contains phylogenetic information regarding 8000 languages. In particular, we propose one probabilistic model and one non-probabilistic model, both using phylogenetic trees (\u201clanguage family trees\u201d) to measure the distance between languages. We show that our best model outperforms baseline models by 6.5 F1. Furthermore, we demonstrate that, with the proposed inventories, the phone recognition model can be customized for every language in the set, which improved the PER (phone error rate) in phone recognition by 25%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47058260",
                    "name": "Xinjian Li"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "3407646",
                    "name": "David R. Mortensen"
                },
                {
                    "authorId": "1690706",
                    "name": "A. Black"
                },
                {
                    "authorId": "1746678",
                    "name": "Shinji Watanabe"
                }
            ]
        },
        {
            "paperId": "12a0c4a3a7404858cc82990f21575cf2b3d7295c",
            "title": "When Is TTS Augmentation Through a Pivot Language Useful?",
            "abstract": "Developing Automatic Speech Recognition (ASR) for low-resource languages is a challenge due to the small amount of transcribed audio data. For many such languages, audio and text are available separately, but not audio with transcriptions. Using text, speech can be synthetically produced via text-to-speech (TTS) systems. However, many low-resource languages do not have quality TTS systems either. We propose an alter-native: produce synthetic audio by running text from the target language through a trained TTS system for a higher-resource pivot language. We investigate when and how this technique is most effective in low-resource settings. In our experiments, using several thousand synthetic TTS text-speech pairs and duplicating authentic data to balance yields optimal results. Our findings suggest that searching over a set of candidate pivot languages can lead to marginal improvements and that, surpris-ingly, ASR performance can by harmed by increases in measured TTS quality. Application of these findings improves ASR by 64.5% and 45.0% character error reduction rate (CERR) re-spectively for two low-resource languages: Guaran\u00b4\u0131 and Suba.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2067645226",
                    "name": "Nathaniel R. Robinson"
                },
                {
                    "authorId": "1988654955",
                    "name": "Perez Ogayo"
                },
                {
                    "authorId": "2175480195",
                    "name": "Swetha Gangu"
                },
                {
                    "authorId": "3407646",
                    "name": "David R. Mortensen"
                },
                {
                    "authorId": "1746678",
                    "name": "Shinji Watanabe"
                }
            ]
        }
    ]
}