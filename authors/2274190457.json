{
    "authorId": "2274190457",
    "papers": [
        {
            "paperId": "0cbc3ea797a994416867a76456c0c3e2528e20d6",
            "title": "LLaVA-Read: Enhancing Reading Ability of Multimodal Language Models",
            "abstract": "Large multimodal language models have demonstrated impressive capabilities in understanding and manipulating images. However, many of these models struggle with comprehending intensive textual contents embedded within the images, primarily due to the limited text recognition and layout understanding ability. To understand the sources of these limitations, we perform an exploratory analysis showing the drawbacks of classical visual encoders on visual text understanding. Hence, we present LLaVA-Read, a multimodal large language model that utilizes dual visual encoders along with a visual text encoder. Our model surpasses existing state-of-the-art models in various text-rich image understanding tasks, showcasing enhanced comprehension of textual content within images. Together, our research suggests visual text understanding remains an open challenge and an efficient visual text encoder is crucial for future successful multimodal systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283147651",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "1652060942",
                    "name": "Yufan Zhou"
                },
                {
                    "authorId": "2283210219",
                    "name": "Jian Chen"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "2283187567",
                    "name": "Changyou Chen"
                },
                {
                    "authorId": "2190051546",
                    "name": "Tongfei Sun"
                }
            ]
        },
        {
            "paperId": "0ce5e5ac7dfd61e05c42957553296a2034cdcd3e",
            "title": "MMR: Evaluating Reading Ability of Large Multimodal Models",
            "abstract": "Large multimodal models (LMMs) have demonstrated impressive capabilities in understanding various types of image, including text-rich images. Most existing text-rich image benchmarks are simple extraction-based question answering, and many LMMs now easily achieve high scores. This means that current benchmarks fail to accurately reflect performance of different models, and a natural idea is to build a new benchmark to evaluate their complex reasoning and spatial understanding abilities. In this work, we propose the Multi-Modal Reading (MMR) benchmark in 11 diverse tasks to evaluate LMMs for text-rich image understanding. MMR is the first text-rich image benchmark built on human annotations with the help of language models. By evaluating several state-of-the-art LMMs, including GPT-4o, it reveals the limited capabilities of existing LMMs underscoring the value of our benchmark.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283210219",
                    "name": "Jian Chen"
                },
                {
                    "authorId": "2283147651",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "1652060942",
                    "name": "Yufan Zhou"
                },
                {
                    "authorId": "2317012495",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "2283187567",
                    "name": "Changyou Chen"
                }
            ]
        },
        {
            "paperId": "144a6584af37eb750e637a96fd80668cbe0c4a44",
            "title": "Self-Cleaning: Improving a Named Entity Recognizer Trained on Noisy Data with a Few Clean Instances",
            "abstract": "To achieve state-of-the-art performance, one 001 still needs to train NER models on large-scale, 002 high-quality annotated data, an asset that is 003 both costly and time-intensive to accumulate. 004 In contrast, real-world applications often resort 005 to massive low-quality labeled data through 006 non-expert annotators via crowdsourcing and 007 external knowledge bases via distant supervi-008 sion as a cost-effective alternative. However, 009 these annotation methods result in noisy labels, 010 which in turn lead to a notable decline in per-011 formance. Hence, we propose to denoise the 012 noisy NER data with guidance from a small set 013 of clean instances. Along with the main NER 014 model we train a discriminator model and use 015 its outputs to recalibrate the sample weights. 016 The discriminator is capable of detecting both 017 span and category errors with different discrim-018 inative prompts. Results on public crowdsourc-019 ing and distant supervision datasets show that 020 the proposed method can consistently improve 021 performance with a small guidance set. 022",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261494597",
                    "name": "Zhendong Chu"
                },
                {
                    "authorId": "2283147661",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2307905181",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2313548885",
                    "name": "Vlad Morariu"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                }
            ]
        },
        {
            "paperId": "56ae7d5917654026fddba54026ef78ceeb6563d8",
            "title": "A Multi-LLM Debiasing Framework",
            "abstract": "Large Language Models (LLMs) are powerful tools with the potential to benefit society immensely, yet, they have demonstrated biases that perpetuate societal inequalities. Despite significant advancements in bias mitigation techniques using data augmentation, zero-shot prompting, and model fine-tuning, biases continuously persist, including subtle biases that may elude human detection. Recent research has shown a growing interest in multi-LLM approaches, which have been demonstrated to be effective in improving the quality of reasoning and factuality in LLMs. Building on this approach, we propose a novel multi-LLM debiasing framework aimed at reducing bias in LLMs. Our work is the first to introduce and evaluate two distinct approaches within this framework for debiasing LLMs: a centralized method, where the conversation is facilitated by a single central LLM, and a decentralized method, where all models communicate directly. Our findings reveal that our multi-LLM framework significantly reduces bias in LLMs, outperforming the baseline method across several social groups.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2322447547",
                    "name": "Deonna M. Owens"
                },
                {
                    "authorId": "2317012495",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "2299908109",
                    "name": "Sungchul Kim"
                },
                {
                    "authorId": "2301760185",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "2257962368",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2312278435",
                    "name": "Xiang Chen"
                },
                {
                    "authorId": "2283147661",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "2322441755",
                    "name": "Hanieh Deilamsalehy"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                }
            ]
        },
        {
            "paperId": "58030231466f852d877115c30a81be8076a18069",
            "title": "Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven Text-to-Image Generation",
            "abstract": "In subject-driven text-to-image generation, recent works have achieved superior performance by training the model on synthetic datasets containing numerous image pairs. Trained on these datasets, generative models can produce text-aligned images for specific subject from arbitrary testing image in a zero-shot manner. They even outperform methods which require additional fine-tuning on testing images. However, the cost of creating such datasets is prohibitive for most researchers. To generate a single training pair, current methods fine-tune a pre-trained text-to-image model on the subject image to capture fine-grained details, then use the fine-tuned model to create images for the same subject based on creative text prompts. Consequently, constructing a large-scale dataset with millions of subjects can require hundreds of thousands of GPU hours. To tackle this problem, we propose Toffee, an efficient method to construct datasets for subject-driven editing and generation. Specifically, our dataset construction does not need any subject-level fine-tuning. After pre-training two generative models, we are able to generate infinite number of high-quality samples. We construct the first large-scale dataset for subject-driven image editing and generation, which contains 5 million image pairs, text prompts, and masks. Our dataset is 5 times the size of previous largest dataset, yet our cost is tens of thousands of GPU hours lower. To test the proposed dataset, we also propose a model which is capable of both subject-driven image editing and generation. By simply training the model on our proposed dataset, it obtains competitive results, illustrating the effectiveness of the proposed dataset construction framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1652060942",
                    "name": "Yufan Zhou"
                },
                {
                    "authorId": "2283147651",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2253444275",
                    "name": "Kaizhi Zheng"
                },
                {
                    "authorId": "2295767319",
                    "name": "Nanxuan Zhao"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "2261732372",
                    "name": "Zichao Wang"
                },
                {
                    "authorId": "2256599634",
                    "name": "Xin Eric Wang"
                },
                {
                    "authorId": "2190051546",
                    "name": "Tongfei Sun"
                }
            ]
        },
        {
            "paperId": "68d22ee5154e42b2422683b854c3f0e19e3e9469",
            "title": "ARTIST: Improving the Generation of Text-rich Images by Disentanglement",
            "abstract": "Diffusion models have demonstrated exceptional capabilities in generating a broad spectrum of visual content, yet their proficiency in rendering text is still limited: they often generate inaccurate characters or words that fail to blend well with the underlying image. To address these shortcomings, we introduce a new framework named ARTIST . This framework incorporates a dedicated textual diffusion model to specifically focus on the learning of text structures. Initially, we pretrain this textual model to capture the intricacies of text representation. Subsequently, we finetune a visual diffusion model, enabling it to assimilate textual structure information from the pretrained textual model. This disentangled architecture design and thetraining strategy significantly enhance the text rendering ability of the diffusion models for text-rich image generation. Additionally, we leverage the capabilities of pretrained large language models to better interpret user intentions, contributing to improved generation quality. Empirical results on the MARIO-Eval benchmark underscore the effectiveness of the proposed method, showing an improvement of up to 15% in various metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265652686",
                    "name": "Jianyi Zhang"
                },
                {
                    "authorId": "1652060942",
                    "name": "Yufan Zhou"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "26360698",
                    "name": "Curtis Wigington"
                },
                {
                    "authorId": "2307905181",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "2265715182",
                    "name": "Yiran Chen"
                },
                {
                    "authorId": "2190051546",
                    "name": "Tongfei Sun"
                },
                {
                    "authorId": "2283147651",
                    "name": "Ruiyi Zhang"
                }
            ]
        },
        {
            "paperId": "784260e953b2ce34df52086804681e35d57e5843",
            "title": "TRINS: Towards Multimodal Language Models that Can Read",
            "abstract": "Large multimodal language models have shown remarkable proficiency in understanding and editing images. However, a majority of these visually-tuned models struggle to comprehend the textual content embedded in images, primar-ily due to the limitation of training data. In this work, we introduce TRINS: a Text-Rich image11In this work, we use the phrase \u201ctext-rich images\u201d to describe images with rich textual information, such as posters and book covers. INStruction dataset, with the objective of enhancing the reading ability of the multimodal large language model. TRINS is built upon LAION22Work done during Q3 2023. using hybrid data annotation strategies that include machine-assisted and human-assisted annotation process. It contains 39,153 text-rich images, captions, and 102,437 questions. Specifically, we show that the number of words per annotation in TRINS is significantly longer than that of related datasets, providing new challenges. Furthermore, we introduce a simple and effective architecture, called a Language-Vision Reading Assistant (LaRA), which is good at understanding textual content within images. LaRA outperforms existing state-of-the-art multimodal large language models on the TRINS dataset as well as other classical benchmarks. Lastly, we conducted a comprehensive evaluation with TRINS on various text-rich image understanding and generation tasks, demonstrating its effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283147651",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2121290295",
                    "name": "Yanzhe Zhang"
                },
                {
                    "authorId": "2283210219",
                    "name": "Jian Chen"
                },
                {
                    "authorId": "1652060942",
                    "name": "Yufan Zhou"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "2283187567",
                    "name": "Changyou Chen"
                },
                {
                    "authorId": "2190051546",
                    "name": "Tongfei Sun"
                }
            ]
        },
        {
            "paperId": "846768c90137ecbf08ab6630bb894bb349096bc3",
            "title": "DocSynthv2: A Practical Autoregressive Modeling for Document Generation",
            "abstract": "While the generation of document layouts has been extensively explored, comprehensive document generation encompassing both layout and content presents a more complex challenge. This paper delves into this advanced domain, proposing a novel approach called DocSynthv2 through the development of a simple yet effective autoregressive structured model. Our model, distinct in its integration of both layout and textual cues, marks a step beyond existing layout-generation approaches. By focusing on the relationship between the structural elements and the textual content within documents, we aim to generate cohesive and contextually relevant documents without any reliance on visual components. Through experimental studies on our curated benchmark for the new task, we demonstrate the ability of our model combining layout and textual information in enhancing the generation quality and relevance of documents, opening new pathways for research in document creation and automated design. Our findings emphasize the effectiveness of autoregressive models in handling complex document generation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150473007",
                    "name": "Sanket Biswas"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "26360698",
                    "name": "Curtis Wigington"
                },
                {
                    "authorId": "2190051546",
                    "name": "Tongfei Sun"
                },
                {
                    "authorId": "2305681954",
                    "name": "Josep Llad'os"
                }
            ]
        },
        {
            "paperId": "b3795d73cadfacfb6aa0f723978915012070a3d5",
            "title": "DocScript: Document-level Script Event Prediction",
            "abstract": "We present a novel task of document-level script event prediction, which aims to predict the next event given a candidate list of narrative events in long-form documents. To enable this, we introduce DocSEP, a challenging dataset in two new domains - contractual documents and Wikipedia articles, where timeline events may be paragraphs apart and may require multi-hop temporal and causal reasoning. We benchmark existing baselines and present a novel architecture called DocScript to learn sequential ordering between events at the document scale. Our experimental results on the DocSEP dataset demonstrate that learning longer-range dependencies between events is a key challenge and show that contemporary LLMs such as ChatGPT and FlanT5 struggle to solve this task, indicating their lack of reasoning abilities for understanding causal relationships and temporal sequences within long texts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "31099365",
                    "name": "Aparna Garimella"
                },
                {
                    "authorId": "2301580599",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2273677165",
                    "name": "Dinesh Manocha"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                }
            ]
        },
        {
            "paperId": "bb1737a475622be39d7845825c77c2a241f5cac3",
            "title": "ARTIST: Improving the Generation of Text-rich Images with Disentangled Diffusion Models",
            "abstract": "Diffusion models have demonstrated exceptional capabilities in generating a broad spectrum of visual content, yet their proficiency in rendering text is still limited: they often generate inaccurate characters or words that fail to blend well with the underlying image. To address these shortcomings, we introduce a new framework named ARTIST. This framework incorporates a dedicated textual diffusion model to specifically focus on the learning of text structures. Initially, we pretrain this textual model to capture the intricacies of text representation. Subsequently, we finetune a visual diffusion model, enabling it to assimilate textual structure information from the pretrained textual model. This disentangled architecture design and the training strategy significantly enhance the text rendering ability of the diffusion models for text-rich image generation. Additionally, we leverage the capabilities of pretrained large language models to better interpret user intentions, contributing to improved generation quality. Empirical results on the MARIO-Eval benchmark underscore the effectiveness of the proposed method, showing an improvement of up to 15\\% in various metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265652686",
                    "name": "Jianyi Zhang"
                },
                {
                    "authorId": "1652060942",
                    "name": "Yufan Zhou"
                },
                {
                    "authorId": "2274190457",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "26360698",
                    "name": "Curtis Wigington"
                },
                {
                    "authorId": "2307905181",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "2265715182",
                    "name": "Yiran Chen"
                },
                {
                    "authorId": "2190051546",
                    "name": "Tongfei Sun"
                },
                {
                    "authorId": "2283147651",
                    "name": "Ruiyi Zhang"
                }
            ]
        }
    ]
}