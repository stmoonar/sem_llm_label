{
    "authorId": "2268686199",
    "papers": [
        {
            "paperId": "4499afc74bda1c7d521a516df040facfe39943ed",
            "title": "Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement",
            "abstract": "Large vision-language models (LVLMs) have achieved impressive results in various visual question-answering and reasoning tasks through vision instruction tuning on specific datasets. However, there is still significant room for improvement in the alignment between visual and language modalities. Previous methods to enhance this alignment typically require external models or data, heavily depending on their capabilities and quality, which inevitably sets an upper bound on performance. In this paper, we propose SIMA, a framework that enhances visual and language modality alignment through self-improvement, eliminating the needs for external models or data. SIMA leverages prompts from existing vision instruction tuning datasets to self-generate responses and employs an in-context self-critic mechanism to select response pairs for preference tuning. The key innovation is the introduction of three vision metrics during the in-context self-critic process, which can guide the LVLM in selecting responses that enhance image comprehension. Through experiments across 14 hallucination and comprehensive benchmarks, we demonstrate that SIMA not only improves model performance across all benchmarks but also achieves superior modality alignment, outperforming previous approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2302790705",
                    "name": "Xiyao Wang"
                },
                {
                    "authorId": "1391200710",
                    "name": "Jiuhai Chen"
                },
                {
                    "authorId": "2303664290",
                    "name": "Zhaoyang Wang"
                },
                {
                    "authorId": "2266789873",
                    "name": "Yuhang Zhou"
                },
                {
                    "authorId": "2250759613",
                    "name": "Yiyang Zhou"
                },
                {
                    "authorId": "18307037",
                    "name": "Huaxiu Yao"
                },
                {
                    "authorId": "2303390941",
                    "name": "Tianyi Zhou"
                },
                {
                    "authorId": "2283845854",
                    "name": "Tom Goldstein"
                },
                {
                    "authorId": "2303385525",
                    "name": "Parminder Bhatia"
                },
                {
                    "authorId": "2268686199",
                    "name": "Furong Huang"
                },
                {
                    "authorId": "2303426039",
                    "name": "Cao Xiao"
                }
            ]
        },
        {
            "paperId": "6fa3544f42bc026ac684cf6c7a8cd50f59b3ee7d",
            "title": "Multi-Stage Balanced Distillation: Addressing Long-Tail Challenges in Sequence-Level Knowledge Distillation",
            "abstract": "Large language models (LLMs) have significantly advanced various natural language processing tasks, but deploying them remains computationally expensive. Knowledge distillation (KD) is a promising solution, enabling the transfer of capabilities from larger teacher LLMs to more compact student models. Particularly, sequence-level KD, which distills rationale-based reasoning processes instead of merely final outcomes, shows great potential in enhancing students' reasoning capabilities. However, current methods struggle with sequence level KD under long-tailed data distributions, adversely affecting generalization on sparsely represented domains. We introduce the Multi-Stage Balanced Distillation (BalDistill) framework, which iteratively balances training data within a fixed computational budget. By dynamically selecting representative head domain examples and synthesizing tail domain examples, BalDistill achieves state-of-the-art performance across diverse long-tailed datasets, enhancing both the efficiency and efficacy of the distilled models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266789873",
                    "name": "Yuhang Zhou"
                },
                {
                    "authorId": "2307469621",
                    "name": "Jing Zhu"
                },
                {
                    "authorId": "103226724",
                    "name": "Paiheng Xu"
                },
                {
                    "authorId": "2158018357",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "2282902714",
                    "name": "Xiyao Wang"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                },
                {
                    "authorId": "2218202090",
                    "name": "Wei Ai"
                },
                {
                    "authorId": "2268686199",
                    "name": "Furong Huang"
                }
            ]
        },
        {
            "paperId": "9445a7fcf495ba13169533d591cefaddadaf6f43",
            "title": "World Models with Hints of Large Language Models for Goal Achieving",
            "abstract": "Reinforcement learning struggles in the face of long-horizon tasks and sparse goals due to the difficulty in manual reward specification. While existing methods address this by adding intrinsic rewards, they may fail to provide meaningful guidance in long-horizon decision-making tasks with large state and action spaces, lacking purposeful exploration. Inspired by human cognition, we propose a new multi-modal model-based RL approach named Dreaming with Large Language Models (DLLM). DLLM integrates the proposed hinting subgoals from the LLMs into the model rollouts to encourage goal discovery and reaching in challenging tasks. By assigning higher intrinsic rewards to samples that align with the hints outlined by the language model during model rollouts, DLLM guides the agent toward meaningful and efficient exploration. Extensive experiments demonstrate that the DLLM outperforms recent methods in various challenging, sparse-reward environments such as HomeGrid, Crafter, and Minecraft by 27.7\\%, 21.1\\%, and 9.9\\%, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2305744595",
                    "name": "Zeyuan Liu"
                },
                {
                    "authorId": "2305685852",
                    "name": "Ziyu Huan"
                },
                {
                    "authorId": "2238396806",
                    "name": "Xiyao Wang"
                },
                {
                    "authorId": "2008151131",
                    "name": "Jiafei Lyu"
                },
                {
                    "authorId": "2219923621",
                    "name": "Jian Tao"
                },
                {
                    "authorId": "2306083369",
                    "name": "Xiu Li"
                },
                {
                    "authorId": "2268686199",
                    "name": "Furong Huang"
                },
                {
                    "authorId": "2255379295",
                    "name": "Huazhe Xu"
                }
            ]
        },
        {
            "paperId": "a649db6921c327b75df38d4b81e9c8b4173fb175",
            "title": "AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models",
            "abstract": "Large vision-language models (LVLMs) are prone to hallucinations, where certain contextual cues in an image can trigger the language module to produce overconfident and incorrect reasoning about abnormal or hypothetical objects. While some benchmarks have been developed to investigate LVLM hallucinations, they often rely on hand-crafted corner cases whose failure patterns may not generalize well. Additionally, fine-tuning on these examples could undermine their validity. To address this, we aim to scale up the number of cases through an automated approach, reducing human bias in crafting such corner cases. This motivates the development of AutoHallusion, the first automated benchmark generation approach that employs several key strategies to create a diverse range of hallucination examples. Our generated visual-question pairs pose significant challenges to LVLMs, requiring them to overcome contextual biases and distractions to arrive at correct answers. AutoHallusion enables us to create new benchmarks at the minimum cost and thus overcomes the fragility of hand-crafted benchmarks. It also reveals common failure patterns and reasons, providing key insights to detect, avoid, or control hallucinations. Comprehensive evaluations of top-tier LVLMs, e.g., GPT-4V(ision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, show a 97.7% and 98.7% success rate of hallucination induction on synthetic and real-world datasets of AutoHallusion, paving the way for a long battle against hallucinations. The codebase and data can be accessed at https://github.com/wuxiyang1996/AutoHallusion.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2249840699",
                    "name": "Xiyang Wu"
                },
                {
                    "authorId": "2291143084",
                    "name": "Tianrui Guan"
                },
                {
                    "authorId": "2306950674",
                    "name": "Dianqi Li"
                },
                {
                    "authorId": "2294631774",
                    "name": "Shuaiyi Huang"
                },
                {
                    "authorId": "2268723491",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "2268709343",
                    "name": "Xijun Wang"
                },
                {
                    "authorId": "2210731669",
                    "name": "Ruiqi Xian"
                },
                {
                    "authorId": "1781242",
                    "name": "Abhinav Shrivastava"
                },
                {
                    "authorId": "2268686199",
                    "name": "Furong Huang"
                },
                {
                    "authorId": "2267532838",
                    "name": "Jordan L. Boyd-Graber"
                },
                {
                    "authorId": "2303390941",
                    "name": "Tianyi Zhou"
                },
                {
                    "authorId": "2256714387",
                    "name": "Dinesh Manocha"
                }
            ]
        },
        {
            "paperId": "e0702a22e0841c54ab865b4996d7b07af192a3e1",
            "title": "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences",
            "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitative analysis and case studies identify three key factors impacting MLLMs' sequential image reasoning: the correlation between object and behavioral hallucinations, the influence of cooccurring behaviors, and the compounding impact of behavioral hallucinations. Our dataset is available at https://github.com/umd-huang-lab/Mementos.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2238396806",
                    "name": "Xiyao Wang"
                },
                {
                    "authorId": "2145926835",
                    "name": "Yuhang Zhou"
                },
                {
                    "authorId": "2268723491",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "2280118198",
                    "name": "Hongjin Lu"
                },
                {
                    "authorId": "2238278723",
                    "name": "Yuancheng Xu"
                },
                {
                    "authorId": "52220309",
                    "name": "Fuxiao Liu"
                },
                {
                    "authorId": "2280104027",
                    "name": "Feihong He"
                },
                {
                    "authorId": "2249842387",
                    "name": "Jaehong Yoon"
                },
                {
                    "authorId": "13563486",
                    "name": "Jaehong Yoon"
                },
                {
                    "authorId": "3313330",
                    "name": "Gedas Bertasius"
                },
                {
                    "authorId": "2276608813",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "2267311471",
                    "name": "Huaxiu Yao"
                },
                {
                    "authorId": "2268686199",
                    "name": "Furong Huang"
                }
            ]
        },
        {
            "paperId": "fb4dc0178e5d7347b1615c48caf05347b6e5eb48",
            "title": "TrustLLM: Trustworthiness in Large Language Models",
            "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257131651",
                    "name": "Lichao Sun"
                },
                {
                    "authorId": "2257084278",
                    "name": "Yue Huang"
                },
                {
                    "authorId": "2256769280",
                    "name": "Haoran Wang"
                },
                {
                    "authorId": "2254867423",
                    "name": "Siyuan Wu"
                },
                {
                    "authorId": "2254328621",
                    "name": "Qihui Zhang"
                },
                {
                    "authorId": "2279094112",
                    "name": "Chujie Gao"
                },
                {
                    "authorId": "2282234921",
                    "name": "Yixin Huang"
                },
                {
                    "authorId": "2279022836",
                    "name": "Wenhan Lyu"
                },
                {
                    "authorId": "2257107248",
                    "name": "Yixuan Zhang"
                },
                {
                    "authorId": "2118053386",
                    "name": "Xiner Li"
                },
                {
                    "authorId": "2145977326",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2254346817",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "2279093879",
                    "name": "Yijue Wang"
                },
                {
                    "authorId": "2275287781",
                    "name": "Zhikun Zhang"
                },
                {
                    "authorId": "1749353",
                    "name": "B. Kailkhura"
                },
                {
                    "authorId": "2266469680",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "2256992325",
                    "name": "Chaowei Xiao"
                },
                {
                    "authorId": "2268756316",
                    "name": "Chun-Yan Li"
                },
                {
                    "authorId": "2243234805",
                    "name": "Eric P. Xing"
                },
                {
                    "authorId": "2268686199",
                    "name": "Furong Huang"
                },
                {
                    "authorId": "2240876242",
                    "name": "Haodong Liu"
                },
                {
                    "authorId": "2271097936",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2254303011",
                    "name": "Hongyi Wang"
                },
                {
                    "authorId": "2237996727",
                    "name": "Huan Zhang"
                },
                {
                    "authorId": "18307037",
                    "name": "Huaxiu Yao"
                },
                {
                    "authorId": "2143693283",
                    "name": "M. Kellis"
                },
                {
                    "authorId": "2095762",
                    "name": "M. Zitnik"
                },
                {
                    "authorId": "2279159644",
                    "name": "Meng Jiang"
                },
                {
                    "authorId": "2253396640",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "2278917478",
                    "name": "James Zou"
                },
                {
                    "authorId": "2228505567",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "2238123544",
                    "name": "Jian Liu"
                },
                {
                    "authorId": "2256227183",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "2259869648",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "2266698166",
                    "name": "Jieyu Zhao"
                },
                {
                    "authorId": "2279062891",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "2145270616",
                    "name": "Jindong Wang"
                },
                {
                    "authorId": "2279260447",
                    "name": "John Mitchell"
                },
                {
                    "authorId": "2241470375",
                    "name": "Kai Shu"
                },
                {
                    "authorId": "2267887786",
                    "name": "Kaidi Xu"
                },
                {
                    "authorId": "2256646491",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2254874151",
                    "name": "Lifang He"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                },
                {
                    "authorId": "152981628",
                    "name": "M. Backes"
                },
                {
                    "authorId": "2249536787",
                    "name": "Neil Zhenqiang Gong"
                },
                {
                    "authorId": "2258679535",
                    "name": "Philip S. Yu"
                },
                {
                    "authorId": "2279077171",
                    "name": "Pin-Yu Chen"
                },
                {
                    "authorId": "2279024252",
                    "name": "Quanquan Gu"
                },
                {
                    "authorId": "2279097262",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "2279023269",
                    "name": "Rex Ying"
                },
                {
                    "authorId": "2279225650",
                    "name": "Shuiwang Ji"
                },
                {
                    "authorId": "39400201",
                    "name": "S. Jana"
                },
                {
                    "authorId": "2265221446",
                    "name": "Tian-Xiang Chen"
                },
                {
                    "authorId": "2254792886",
                    "name": "Tianming Liu"
                },
                {
                    "authorId": "2144116530",
                    "name": "Tianying Zhou"
                },
                {
                    "authorId": "2281072607",
                    "name": "William Wang"
                },
                {
                    "authorId": "2280943906",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2261601059",
                    "name": "Xiang-Yu Zhang"
                },
                {
                    "authorId": "2282386985",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2164984576",
                    "name": "Xingyao Xie"
                },
                {
                    "authorId": "2257123882",
                    "name": "Xun Chen"
                },
                {
                    "authorId": "2282196445",
                    "name": "Xuyu Wang"
                },
                {
                    "authorId": "2275033850",
                    "name": "Yan Liu"
                },
                {
                    "authorId": "2279157256",
                    "name": "Yanfang Ye"
                },
                {
                    "authorId": "2279101306",
                    "name": "Yinzhi Cao"
                },
                {
                    "authorId": "2254062898",
                    "name": "Yue Zhao"
                }
            ]
        },
        {
            "paperId": "0b395ed1c8b284e551172b728e83cf257e33729a",
            "title": "Hallusionbench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models",
            "abstract": "We introduce \u201cHALLUSIONBENCH11\u201cHallusion\u201d is a portmanteau of \u201challucination\u201d and \u201cillusion.\u201d,\u201d a comprehensive benchmark designed for the evaluation of image-context rea-soning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(ision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpre-tation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on Hallusion-bench, we benchmarked 15 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only high-lights the observed failure modes, including language hal-lucination and visual illusion but also deepens an under-standing of these pitfalls. Our comprehensive case studies within Hallusionbench shed light on the challenges of hallucination and illusion in LVLMs. Based on these in-sights, we suggest potential pathways for their future im-provement. The benchmark and codebase can be accessed at https://github.com/tianyi-labIHallusionBench.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "94955378",
                    "name": "Tianrui Guan"
                },
                {
                    "authorId": "52220309",
                    "name": "Fuxiao Liu"
                },
                {
                    "authorId": "2249840699",
                    "name": "Xiyang Wu"
                },
                {
                    "authorId": "2210731669",
                    "name": "Ruiqi Xian"
                },
                {
                    "authorId": "2261324984",
                    "name": "Zongxia Li"
                },
                {
                    "authorId": "2268723491",
                    "name": "Xiaoyu Liu"
                },
                {
                    "authorId": "2268709343",
                    "name": "Xijun Wang"
                },
                {
                    "authorId": "2108451006",
                    "name": "Lichang Chen"
                },
                {
                    "authorId": "2268686199",
                    "name": "Furong Huang"
                },
                {
                    "authorId": "2257181289",
                    "name": "Yaser Yacoob"
                },
                {
                    "authorId": "2256714387",
                    "name": "Dinesh Manocha"
                },
                {
                    "authorId": "2268389308",
                    "name": "Tianyi Zhou"
                }
            ]
        }
    ]
}