{
    "authorId": "144991764",
    "papers": [
        {
            "paperId": "245a3a6b108162cc14ec654a4b6858340288b507",
            "title": "Event-Triggered Optimal Adaptive Control of Partially Unknown Linear Continuous-Time Systems With State Delay",
            "abstract": "This article proposes an event-triggered optimal adaptive output-feedback control design approach by utilizing integral reinforcement learning (IRL) for linear time-invariant systems with state delay and uncertain internal dynamics. In the proposed approach, the general optimal control problem is formulated into the game-theoretic framework by treating the event-triggering threshold and the optimal control policy as players. A cost function is defined and a value functional, which includes the delayed system output, is considered. First, by using the value functional and applying stationarity conditions using the Hamiltonian function, the output game delay algebraic Riccati equation (OGDARE) and optimal control policy are derived when the internal system dynamics are available. Then to relax the knowledge of internal dynamics, a hybrid learning scheme using measured output is proposed for tuning the value function parameters, which in turn is employed to compute the estimated optimal control policy. The overall closed-loop system is shown to be asymptotically stable by selecting an appropriate event-triggering condition when the dynamics of the system are both known and partially uncertain. A simulation example is given to substantiate the efficacy of the theoretical claims.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144853180",
                    "name": "R. Moghadam"
                },
                {
                    "authorId": "46519350",
                    "name": "Vignesh Narayanan"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                }
            ]
        },
        {
            "paperId": "270ce3a187f91803b07fc3846b468f689c3402b1",
            "title": "Concurrent Learning-Based Neuroadaptive Robust Tracking Control of Wheeled Mobile Robot: An Event-Triggered Design",
            "abstract": "In this article, an event-based neuroadaptive robust tracking controller for a perturbed and networked differential drive mobile robot (DMR) is designed with concurrent learning. A radial basis function neural network (RBFNN), which approximates an unknown perturbation, is used to design an adaptive sliding mode controller. The RBFNN weights and sliding mode controller parameters are estimated online using an adaptive tuning law to ensure performance with reduced chattering. To improve the convergence of RBFNN weight estimation error, a concurrent learning-based adaptive law is derived, which uses measured online and recorded data. Furthermore, a suitable triggering condition is designed to achieve a reduced number of control computations while minimizing network resources without sacrificing the stability of the sampled data closed-loop control system. A finite sampling frequency is guaranteed for the designed triggering condition by establishing a positive lower bound on the inter-event execution time, which is equivalent to the Zeno-free behavior of the system. Finally, the proposed event-based neuroadaptive robust controller is implemented on a practical system (Q-bot 2e) to show the effectiveness of the proposed design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9099294",
                    "name": "Krishanu Nath"
                },
                {
                    "authorId": "30123675",
                    "name": "M. K. Bera"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                }
            ]
        },
        {
            "paperId": "d6d0f58e27015d49f8d44f8d64d4369ec2fc41bb",
            "title": "Filter-Based Fault Detection and Isolation in Distributed Parameter Systems Modeled by Parabolic Partial Differential Equations",
            "abstract": "This paper covers model-based fault detection and isolation for linear and nonlinear distributed parameter systems (DPS). The first part mainly deals with actuator, sensor and state fault detection and isolation for a class of DPS represented by a set of coupled linear partial differential equations (PDE). A filter based observer is designed based on the linear PDE representation using which a detection residual is generated. A fault is detected when the magnitude of the detection residual exceeds a detection threshold. Upon detection, several isolation estimators are designed using filters whose output residuals are compared with predefined isolation thresholds. A fault on a linear DPS is declared to be of certain type if the corresponding isolation estimator output residual is below its isolation threshold while the other fault isolation estimator output residual is above its threshold. Next, the fault location is determined when a state fault is identified. The second part of this paper focuses on fault detection and isolation of nonlinear DPS by using a Luenberger type observer. Here fault isolation framework is introduced to isolate actuator, sensor and state faults with isolability condition by using additional boundary measurements and without filters. Finally, the effectiveness of the proposed fault detection and isolation schemes for both linear and nonlinear DPS are demonstrated through simulation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1827915",
                    "name": "H. Ferdowsi"
                },
                {
                    "authorId": "2115669982",
                    "name": "Jia Cai"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                }
            ]
        },
        {
            "paperId": "f57b6b2483e27791aa14dd440447a96f529bfd1d",
            "title": "Continual Reinforcement Learning Formulation for Zero-Sum Game-Based Constrained Optimal Tracking",
            "abstract": "This study provides a novel reinforcement learning-based optimal tracking control of partially uncertain nonlinear discrete-time (DT) systems with state constraints using zero-sum game (ZSG) formulation. To address optimal tracking, a novel augmented system consisting of tracking error and its integral value, along with an uncertain desired trajectory, is constructed. A barrier function (BF) with a tradeoff factor is incorporated into the cost function to keep the state trajectories to remain within a compact set and to balance safety with optimality. Next, by using the modified value functional, the ZSG formulation is introduced wherein an actor-critic neural network (NN) framework is employed to approximate the value functional, optimal control input, and worst disturbance. The critic NN weights are tuned once at the sample instants and then iteratively within sampling instants. Using control input errors, the actor NN weights are adjusted once a sampling instant. The concurrent learning term in the critic weight tuning law overcomes the need for the persistency excitation (PE) condition. Further, a weight consolidation scheme is incorporated into the critic update law to attain lifelong learning by overcoming catastrophic forgetting. Finally, a numerical example supports the analytical claims.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9140170",
                    "name": "Behzad Farzanegan"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                }
            ]
        },
        {
            "paperId": "187121b5b47dfe6efe0ed7996503135a20f356d0",
            "title": "Online Optimal Adaptive Control of Partially Uncertain Nonlinear Discrete-Time Systems Using Multilayer Neural Networks",
            "abstract": "This article intends to address an online optimal adaptive regulation of nonlinear discrete-time systems in affine form and with partially uncertain dynamics using a multilayer neural network (MNN). The actor\u2013critic framework estimates both the optimal control input and value function. Instantaneous control input error and temporal difference are used to tune the weights of the critic and actor networks, respectively. The selection of the basis functions and their derivatives are not required in the proposed approach. The state vector, critic, and actor NN weights are proven to be bounded using the Lyapunov method. Our approach can be extended to neural networks with an arbitrary number of hidden layers. We have demonstrated our approach via a simulation example.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "144853180",
                    "name": "R. Moghadam"
                },
                {
                    "authorId": "1977855",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                }
            ]
        },
        {
            "paperId": "4acdb1d917584a631502a2f41be6f776ae1f3dd7",
            "title": "Optimal Adaptive Control of Uncertain Nonlinear Continuous-Time Systems With Input and State Delays",
            "abstract": "In this article, an actor-critic neural network (NN)-based online optimal adaptive regulation of a class of nonlinear continuous-time systems with known state and input delays and uncertain system dynamics is introduced. The temporal difference error (TDE), which is dependent upon state and input delays, is derived using actual and estimated value function and via integral reinforcement learning. The NN weights of the critic are tuned at every sampling instant as a function of the instantaneous integral TDE. A novel identifier, which is introduced to estimate the control coefficient matrices, is utilized to obtain the estimated control policy. The boundedness of the state vector, critic NN weights, identification error, and NN identifier weights are shown through the Lyapunov analysis. Simulation results are provided to illustrate the effectiveness of the proposed approach.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144853180",
                    "name": "R. Moghadam"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                }
            ]
        },
        {
            "paperId": "59303c4b130b4eeca8f131f4d3b5ae7cbf41163b",
            "title": "QC_SANE: Robust Control in DRL Using Quantile Critic With Spiking Actor and Normalized Ensemble",
            "abstract": "Recently introduced deep reinforcement learning (DRL) techniques in discrete-time have resulted in significant advances in online games, robotics, and so on. Inspired from recent developments, we have proposed an approach referred to as Quantile Critic with Spiking Actor and Normalized Ensemble (QC_SANE) for continuous control problems, which uses quantile loss to train critic and a spiking neural network (NN) to train an ensemble of actors. The NN does an internal normalization using a scaled exponential linear unit (SELU) activation function and ensures robustness. The empirical study on multijoint dynamics with contact (MuJoCo)-based environments shows improved training and test results than the state-of-the-art approach: population coded spiking actor network (PopSAN).",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49436759",
                    "name": "Surbhi Gupta"
                },
                {
                    "authorId": "2706436",
                    "name": "Gaurav Singal"
                },
                {
                    "authorId": "152392934",
                    "name": "Deepak Garg"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                }
            ]
        },
        {
            "paperId": "8a5c8c8ce4a2b3b40fb4d4dd5792e36f09d85c72",
            "title": "Flow-based attack detection and accommodation for networked control systems",
            "abstract": "This paper is concerned about detection and estimation of malicious attacks on the network and the linear physical system of a networked control system (NCS) by using linear matrix inequality (LMI)-based technique. Certain class of attacks on the communication network impacts the traffic flow causing network delays and packet losses to increase which in turn affects the stability of the NCS. Therefore in this paper, a novel observer-based scheme is proposed to capture the abnormal traffic flow at the bottleneck node of the communication network via the attack detection residual. An LMI-based design is proposed that ensures both system stability and H-infinity performance and also detects attacks on the network as well as on the physical system. Upon detection, the physical system is stabilised by adjusting the controller gains provided certain conditions are met. Both simulation and hardware implementation results are included to demonstrate the applicability of the proposed scheme.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39005895",
                    "name": "Haifeng Niu"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                }
            ]
        },
        {
            "paperId": "a74f279cc22c1f784991c1da6077ac86ca04090a",
            "title": "Cooperative Deep Q-Learning Framework for Environments Providing Image Feedback",
            "abstract": "In this article, we address two key challenges in deep reinforcement learning (DRL) setting, sample inefficiency and slow learning, with a dual-neural network (NN)-driven learning approach. In the proposed approach, we use two deep NNs with independent initialization to robustly approximate the action-value function in the presence of image inputs. In particular, we develop a temporal difference (TD) error-driven learning (EDL) approach, where we introduce a set of linear transformations of the TD error to directly update the parameters of each layer in the deep NN. We demonstrate theoretically that the cost minimized by the EDL regime is an approximation of the empirical cost, and the approximation error reduces as learning progresses, irrespective of the size of the network. Using simulation analysis, we show that the proposed methods enable faster learning and convergence and require reduced buffer size (thereby increasing the sample efficiency).",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "153726118",
                    "name": "Krishnan Raghavan"
                },
                {
                    "authorId": "46519350",
                    "name": "Vignesh Narayanan"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                }
            ]
        },
        {
            "paperId": "08d74f55bcd59c4b05e150b0d276a60ac46ec592",
            "title": "Distributed Min\u2013Max Learning Scheme for Neural Networks With Applications to High-Dimensional Classification",
            "abstract": "In this article, a novel learning methodology is introduced for the problem of classification in the context of high-dimensional data. In particular, the challenges introduced by high-dimensional data sets are addressed by formulating a $L_{1}$ regularized zero-sum game where optimal sparsity is estimated through a two-player game between the penalty coefficients/sparsity parameters and the deep neural network weights. In order to solve this game, a distributed learning methodology is proposed where additional variables are utilized to derive layerwise cost functions. Finally, an alternating minimization approach developed to solve the problem where the Nash solution provides optimal sparsity and compensation through the classifier. The proposed learning approach is implemented in a parallel and distributed environment through a novel computational algorithm. The efficiency of the approach is demonstrated both theoretically and empirically with nine data sets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "153726118",
                    "name": "Krishnan Raghavan"
                },
                {
                    "authorId": "50064464",
                    "name": "Shweta Garg"
                },
                {
                    "authorId": "144991764",
                    "name": "S. Jagannathan"
                },
                {
                    "authorId": "1993956",
                    "name": "V. A. Samaranayake"
                }
            ]
        }
    ]
}