{
    "authorId": "1739705",
    "papers": [
        {
            "paperId": "a8cb1c9b1836caa48f771e29dcc0df7840e4ec71",
            "title": "How does the Memorization of Neural Networks Impact Adversarial Robust Models?",
            "abstract": "Recent studies suggest that \"memorization\" is one necessary factor for overparameterized deep neural networks (DNNs) to achieve optimal performance. Specifically, the perfectly fitted DNNs can memorize the labels of many atypical samples, generalize their memorization to correctly classify test atypical samples and enjoy better test performance. While, DNNs which are optimized via adversarial training algorithms can also achieve perfect training performance by memorizing the labels of atypical samples, as well as the adversarially perturbed atypical samples. However, adversarially trained models always suffer from poor generalization, with both relatively low clean accuracy and robustness on the test set. In this work, we study the effect of memorization in adversarial trained DNNs and disclose two important findings: (a) Memorizing atypical samples is only effective to improve DNN's accuracy on clean atypical samples, but hardly improve their adversarial robustness and (b) Memorizing certain atypical samples will even hurt the DNN's performance on typical samples. Based on these two findings, we propose Benign Adversarial Training (BAT) which can facilitate adversarial training to avoid fitting \"harmful\" atypical samples and fit as more \"benign\" atypical samples as possible. In our experiments, we validate the effectiveness of BAT, and show that it can achieve better clean accuracy vs. robustness trade-off than baseline methods, in benchmark datasets for image classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2018756699",
                    "name": "Han Xu"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "2108329255",
                    "name": "Wentao Wang"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "1739705",
                    "name": "Anil K. Jain"
                },
                {
                    "authorId": "2115879611",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "ae4871a7ea5f0fb2808c32fb71e5d4673f04ed09",
            "title": "Robust Unsupervised Domain Adaptation from A Corrupted Source",
            "abstract": "Unsupervised Domain Adaptation (UDA) provides a promising solution for learning without supervision, which transfers knowledge from relevant source domains with accessible labeled training data. Existing UDA solutions hinge on clean training data with a short-tail distribution from the source domain, which can be fragile when the source domain data is corrupted either inherently or via adversarial attacks. In this work, we propose an effective framework to address the challenges of UDA from corrupted source domains in a principled manner. Specifically, we perform knowledge ensemble from multiple domain-invariant models that are learned on random partitions of training data. To further address the distribution shift from the source to the target domain, we refine each of the learned models via mutual information maximization, which adaptively obtains the predictive information of the target domain with high confidence. Extensive empirical studies demonstrate that the proposed approach is robust against various types of poisoned data attacks while achieving high asymptotic performance on the target domain.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116618893",
                    "name": "Shuyang Yu"
                },
                {
                    "authorId": "30451228",
                    "name": "Zhuangdi Zhu"
                },
                {
                    "authorId": "144281102",
                    "name": "Boyang Liu"
                },
                {
                    "authorId": "1739705",
                    "name": "Anil K. Jain"
                },
                {
                    "authorId": "2165563795",
                    "name": "Jiayu Zhou"
                }
            ]
        },
        {
            "paperId": "e138a7d3a694d1dd732b88660ab5b4d25d7b69c0",
            "title": "Minutiae-Guided Fingerprint Embeddings via Vision Transformers",
            "abstract": "Minutiae matching has long dominated the field of fingerprint recognition. However, deep networks can be used to extract fixed-length embeddings from fingerprints. To date, the few studies that have explored the use of CNN architectures to extract such embeddings have shown extreme promise. Inspired by these early works, we propose the first use of a Vision Transformer (ViT) to learn a discriminative fixed-length fingerprint embedding. We further demonstrate that by guiding the ViT to focus in on local, minutiae related features, we can boost the recognition performance. Finally, we show that by fusing embeddings learned by CNNs and ViTs we can reach near parity with a commercial state-of-the-art (SOTA) matcher. In particular, we obtain a TAR=94.23% @ FAR=0.1% on the NIST SD 302 public-domain dataset, compared to a SOTA commercial matcher which obtains TAR=96.71% @ FAR=0.1%. Additionally, our fixed-length embeddings can be matched orders of magnitude faster than the commercial system (2.5 million matches/second compared to 50K matches/second). We make our code and models publicly available to encourage further research on this topic: https://github.com/tba.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "73767136",
                    "name": "Steven A. Grosz"
                },
                {
                    "authorId": "13005592",
                    "name": "Joshua J. Engelsma"
                },
                {
                    "authorId": "48467498",
                    "name": "Rajeev Ranjan"
                },
                {
                    "authorId": "40292372",
                    "name": "Naveen Ramakrishnan"
                },
                {
                    "authorId": "2052181159",
                    "name": "Manoj Aggarwal"
                },
                {
                    "authorId": "3463966",
                    "name": "G. Medioni"
                },
                {
                    "authorId": "1739705",
                    "name": "Anil K. Jain"
                }
            ]
        },
        {
            "paperId": "47b43c2c24c0305861bb6ba89ff32c8a41814294",
            "title": "Towards the Memorization Effect of Neural Networks in Adversarial Training",
            "abstract": "Recent studies suggest that ``memorization'' is one important factor for overparameterized deep neural networks (DNNs) to achieve optimal performance. Specifically, the perfectly fitted DNNs can memorize the labels of many atypical samples, generalize their memorization to correctly classify test atypical samples and enjoy better test performance. While, DNNs which are optimized via adversarial training algorithms can also achieve perfect training performance by memorizing the labels of atypical samples, as well as the adversarially perturbed atypical samples. However, adversarially trained models always suffer from poor generalization, with both relatively low clean accuracy and robustness on the test set. In this work, we study the effect of memorization in adversarial trained DNNs and disclose two important findings: (a) Memorizing atypical samples is only effective to improve DNN's accuracy on clean atypical samples, but hardly improve their adversarial robustness and (b) Memorizing certain atypical samples will even hurt the DNN's performance on typical samples. Based on these two findings, we propose Benign Adversarial Training (BAT) which can facilitate adversarial training to avoid fitting ``harmful'' atypical samples and fit as more ``benign'' atypical samples as possible. In our experiments, we validate the effectiveness of BAT, and show it can achieve better clean accuracy vs. robustness trade-off than baseline methods, in benchmark datasets such as CIFAR100 and Tiny~ImageNet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2018756699",
                    "name": "Han Xu"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "2108329255",
                    "name": "Wentao Wang"
                },
                {
                    "authorId": "3068001",
                    "name": "Wenbiao Ding"
                },
                {
                    "authorId": "4574975",
                    "name": "Zhongqin Wu"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "1739705",
                    "name": "Anil K. Jain"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "8b365890c0224f17fffb90bf33da46fccacd9331",
            "title": "Trustworthy AI: A Computational Perspective",
            "abstract": "In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone\u2019s daily life and profoundly altering the course of human society. The intention behind developing AI was and is to benefit humans by reducing labor, increasing everyday conveniences, and promoting social good. However, recent research and AI applications indicate that AI can cause unintentional harm to humans by, for example, making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against a group or groups. Consequently, trustworthy AI has recently garnered increased attention regarding the need to avoid the adverse effects that AI could bring to people, so people can fully trust and live in harmony with AI technologies. A tremendous amount of research on trustworthy AI has been conducted and witnessed in recent years. In this survey, we present a comprehensive appraisal of trustworthy AI from a computational perspective to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex subject, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii) Nondiscrimination & Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability & Auditability, and (vi) Environmental Well-being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143856455",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "2108941389",
                    "name": "Yiqi Wang"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "1527096073",
                    "name": "Yaxin Li"
                },
                {
                    "authorId": "39720946",
                    "name": "Shaili Jain"
                },
                {
                    "authorId": "1739705",
                    "name": "Anil K. Jain"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        }
    ]
}