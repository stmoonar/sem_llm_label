{
    "authorId": "2069718816",
    "papers": [
        {
            "paperId": "c6d7074451e6036a311862b9f190d636b81f294d",
            "title": "Cooperative Multiagent Transfer Learning With Coalition Pattern Decomposition",
            "abstract": "Knowledge transfer in cooperative multiagent reinforcement learning (MARL) has drawn increasing attention in recent years. Unlike generalizing policies in single-agent tasks, it is more important to consider coordination knowledge than individual knowledge in multiagent transfer learning. However, most of the existing methods only focus on knowledge transfer of the individual agent policy, which leads to coordination bias, and finally, affects the final performance in cooperative MARL. In this article, we propose a level-adaptive MARL framework called \u201cLA-QTransformer,\u201d to realize the knowledge transfer on the coordination level via efficiently decomposing the agent coordination into multilevel coalition patterns for different agents. Compatible with centralized training with decentralized execution regime, LA-QTransformer utilizes the level-adaptive transformer to generate suitable coalition patterns, and then, realizes the credit assignment for each agent. Besides, to deal with unexpected changes in the number of agents in the coordination transfer phase, we design a policy network called \u201cpopulation invariant agent with transformer (PIT)\u201d to adapt dynamic observation and action space. We evaluate the LA-QTransformer and PIT in the StarCraft II micromanagement benchmark by comparing them with several state-of-the-art MARL baselines. The experimental results demonstrate the superiority of LA-QTransformer and PIT and verify the feasibility of coordination knowledge transfer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2106505654",
                    "name": "Tianze Zhou"
                },
                {
                    "authorId": "3440829",
                    "name": "Fubiao Zhang"
                },
                {
                    "authorId": "1704229853",
                    "name": "Kun Shao"
                },
                {
                    "authorId": "1382484054",
                    "name": "Zipeng Dai"
                },
                {
                    "authorId": "2158262854",
                    "name": "Kai Li"
                },
                {
                    "authorId": "1484970285",
                    "name": "Wenhan Huang"
                },
                {
                    "authorId": "2209431",
                    "name": "Weixun Wang"
                },
                {
                    "authorId": "2152593803",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2108821147",
                    "name": "Dong Li"
                },
                {
                    "authorId": "2046744137",
                    "name": "Wulong Liu"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                }
            ]
        },
        {
            "paperId": "0d983bfb3228941ee083828653081dccd167edc3",
            "title": "Generative Flow Networks for Precise Reward-Oriented Active Learning on Graphs",
            "abstract": "Many score-based active learning methods have been successfully applied to graph-structured data, aiming to reduce the number of labels and achieve better performance of graph neural networks based on predefined score functions. However, these algorithms struggle to learn policy distributions that are proportional to rewards and have limited exploration capabilities. In this paper, we innovatively formulate the graph active learning problem as a generative process, named GFlowGNN, which generates various samples through sequential actions with probabilities precisely proportional to a predefined reward function. Furthermore, we propose the concept of flow nodes and flow features to efficiently model graphs as flows based on generative flow networks, where the policy network is trained with specially designed rewards. Extensive experiments on real datasets show that the proposed approach has good exploration capability and transferability, outperforming various state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47002988",
                    "name": "Yinchuan Li"
                },
                {
                    "authorId": "46947185",
                    "name": "Zhigang Li"
                },
                {
                    "authorId": "2128179285",
                    "name": "Wenqian Li"
                },
                {
                    "authorId": "49713700",
                    "name": "Yunfeng Shao"
                },
                {
                    "authorId": "1752775197",
                    "name": "Yan Zheng"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                }
            ]
        },
        {
            "paperId": "172171f73e1c1b253ac7f330e83c28921b2ac51b",
            "title": "Learning from Hierarchical Structure of Knowledge Graph for Recommendation",
            "abstract": "Knowledge graphs (KGs) can help enhance recommendations, especially for the data-sparsity scenarios with limited user-item interaction data. Due to the strong power of representation learning of graph neural networks (GNNs), recent works of KG-based recommendation deploy GNN models to learn from both knowledge graph and user-item bipartite interaction graph. However, these works have not well considered the hierarchical structure of knowledge graph, leading to sub-optimal results. Despite the benefit of hierarchical structure, leveraging it is challenging since the structure is always partly-observed. In this work, we first propose to reveal unknown hierarchical structures with a supervised signal detection method and then exploit the hierarchical structure with disentangling representation learning. We conduct experiments on two large-scale datasets, of which the results well verify the superiority and rationality of the proposed method. Further experiments of ablation study with respect to key model designs have demonstrated the effectiveness and rationality of our proposed model. The code is available at https://github.com/tsinghua-fib-lab/HIKE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143528693",
                    "name": "Yingrong Qin"
                },
                {
                    "authorId": "49281242",
                    "name": "Chen Gao"
                },
                {
                    "authorId": "39283140",
                    "name": "Shuangqing Wei"
                },
                {
                    "authorId": "2118461593",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "49953590",
                    "name": "Depeng Jin"
                },
                {
                    "authorId": "2152086464",
                    "name": "Jian Yuan"
                },
                {
                    "authorId": "2143837262",
                    "name": "Lin Zhang"
                },
                {
                    "authorId": "2108821147",
                    "name": "Dong Li"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                },
                {
                    "authorId": "2154403926",
                    "name": "Yong Li"
                }
            ]
        },
        {
            "paperId": "26ebeeb1b9172df34ad21f1000bb6f3c374a222e",
            "title": "Structure Aware Incremental Learning with Personalized Imitation Weights for Recommender Systems",
            "abstract": "Recommender systems now consume large-scale data and play a significant role in improving user experience. Graph Neural Networks (GNNs) have emerged as one of the most effective recommender system models because they model the rich relational information. The ever-growing volume of data can make training GNNs prohibitively expensive. To address this, previous attempts propose to train the GNN models incrementally as new data blocks arrive. \nFeature and structure knowledge distillation techniques have been explored to allow the GNN model to train in a fast incremental fashion while alleviating the catastrophic forgetting problem. \nHowever, preserving the same amount of the historical information for all users is sub-optimal since it fails to take into account the dynamics of each user's change of preferences. \nFor the users whose interests shift substantially, retaining too much of the old knowledge can overly constrain the model, preventing it from quickly adapting to the users\u2019 novel interests. \nIn contrast, for users who have static preferences, model performance can benefit greatly from preserving as much of the user's long-term preferences as possible.\nIn this work, we propose a novel training strategy that adaptively learns personalized imitation weights for each user to balance the contribution from the recent data and the amount of knowledge to be distilled from previous time periods.\nWe demonstrate the effectiveness of learning imitation weights via a comparison on five diverse datasets for three state-of-art structure distillation based recommender systems. The performance shows consistent improvement over competitive incremental learning techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108711613",
                    "name": "Yuening Wang"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "1419478649",
                    "name": "Antonios Valkanas"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "74142381",
                    "name": "Chen Ma"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "2fd0e6aaccee819a880a55d9190700c6b754d32d",
            "title": "DAG Matters! GFlowNets Enhanced Explainer For Graph Neural Networks",
            "abstract": "Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over the years. Existing literature mainly focus on selecting a subgraph, through combinatorial optimization, to provide faithful explanations. However, the exponential size of candidate subgraphs limits the applicability of state-of-the-art methods to large-scale GNNs. We enhance on this through a different approach: by proposing a generative structure -- GFlowNets-based GNN Explainer (GFlowExplainer), we turn the optimization problem into a step-by-step generative problem. Our GFlowExplainer aims to learn a policy that generates a distribution of subgraphs for which the probability of a subgraph is proportional to its' reward. The proposed approach eliminates the influence of node sequence and thus does not need any pre-training strategies. We also propose a new cut vertex matrix to efficiently explore parent states for GFlowNets structure, thus making our approach applicable in a large-scale setting. We conduct extensive experiments on both synthetic and real datasets, and both qualitative and quantitative results show the superiority of our GFlowExplainer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2128179285",
                    "name": "Wenqian Li"
                },
                {
                    "authorId": "47002988",
                    "name": "Yinchuan Li"
                },
                {
                    "authorId": "46947185",
                    "name": "Zhigang Li"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                },
                {
                    "authorId": "2057013801",
                    "name": "Yan Pang"
                }
            ]
        },
        {
            "paperId": "393fa16f4076c460603b546fc9b86ad07f468ae7",
            "title": "Boosting Multiagent Reinforcement Learning via Permutation Invariant and Permutation Equivariant Networks",
            "abstract": "The state space in Multiagent Reinforcement Learning (MARL) grows exponentially with the agent number. Such a curse of dimensionality results in poor scalability and low sample efficiency, inhibiting MARL for decades. To break this curse, we propose a unified agent permutation framework that exploits the permutation invariance (PI) and permutation equivariance (PE) inductive biases to reduce the multiagent state space. Our insight is that permuting the order of entities in the factored multiagent state space does not change the information. Specifically, we propose two novel implementations: a Dynamic Permutation Network (DPN) and a Hyper Policy Network (HPN). The core idea is to build separate entity-wise PI input and PE output network modules to connect the entity-factored state space and action space in an end-to-end way. DPN achieves such connections by two separate module selection networks, which consistently assign the same input module to the same input entity (guarantee PI) and assign the same output module to the same entity-related output (guarantee PE). To enhance the representation capability, HPN replaces the module selection networks of DPN with hypernetworks to directly generate the corresponding module weights. Extensive experiments in SMAC, SMACv2, Google Research Football, and MPE validate that the proposed methods significantly boost the performance and the learning efficiency of existing MARL algorithms. Remarkably, in SMAC, we achieve 100% win rates in almost all hard and super-hard scenarios (never achieved before",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                },
                {
                    "authorId": "1491630925",
                    "name": "Xiaotian Hao"
                },
                {
                    "authorId": "8595845",
                    "name": "Hangyu Mao"
                },
                {
                    "authorId": "2209431",
                    "name": "Weixun Wang"
                },
                {
                    "authorId": "47796324",
                    "name": "Yaodong Yang"
                },
                {
                    "authorId": "2108821147",
                    "name": "Dong Li"
                },
                {
                    "authorId": "1752775197",
                    "name": "Yan Zheng"
                },
                {
                    "authorId": "2118454626",
                    "name": "Zhen Wang"
                }
            ]
        },
        {
            "paperId": "42fa70f18de0fb98381c76f624bd69e3a80d9cdd",
            "title": "MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL",
            "abstract": "Recently, diffusion model shines as a promising backbone for the sequence modeling paradigm in offline reinforcement learning(RL). However, these works mostly lack the generalization ability across tasks with reward or dynamics change. To tackle this challenge, in this paper we propose a task-oriented conditioned diffusion planner for offline meta-RL(MetaDiffuser), which considers the generalization problem as conditional trajectory generation task with contextual representation. The key is to learn a context conditioned diffusion model which can generate task-oriented trajectories for planning across diverse tasks. To enhance the dynamics consistency of the generated trajectories while encouraging trajectories to achieve high returns, we further design a dual-guided module in the sampling process of the diffusion model. The proposed framework enjoys the robustness to the quality of collected warm-start data from the testing task and the flexibility to incorporate with different task representation method. The experiment results on MuJoCo benchmarks show that MetaDiffuser outperforms other strong offline meta-RL baselines, demonstrating the outstanding conditional generation ability of diffusion architecture.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2060411245",
                    "name": "Fei Ni"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                },
                {
                    "authorId": "1675357512",
                    "name": "Yao Mu"
                },
                {
                    "authorId": "2112499487",
                    "name": "Yifu Yuan"
                },
                {
                    "authorId": "1752775197",
                    "name": "Yan Zheng"
                },
                {
                    "authorId": "3027441",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "47415635",
                    "name": "Zhixuan Liang"
                }
            ]
        },
        {
            "paperId": "43737655c34a6f2a1446d1574e7830560c34bd2f",
            "title": "CFlowNets: Continuous Control with Generative Flow Networks",
            "abstract": "Generative flow networks (GFlowNets), as an emerging technique, can be used as an alternative to reinforcement learning for exploratory control tasks. GFlowNet aims to generate distribution proportional to the rewards over terminating states, and to sample different candidates in an active learning fashion. GFlowNets need to form a DAG and compute the flow matching loss by traversing the inflows and outflows of each node in the trajectory. No experiments have yet concluded that GFlowNets can be used to handle continuous tasks. In this paper, we propose generative continuous flow networks (CFlowNets) that can be applied to continuous control tasks. First, we present the theoretical formulation of CFlowNets. Then, a training framework for CFlowNets is proposed, including the action selection process, the flow approximation algorithm, and the continuous flow matching loss function. Afterward, we theoretically prove the error bound of the flow approximation. The error decreases rapidly as the number of flow samples increases. Finally, experimental results on continuous control tasks demonstrate the performance advantages of CFlowNets compared to many reinforcement learning methods, especially regarding exploration ability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47002988",
                    "name": "Yinchuan Li"
                },
                {
                    "authorId": "151212797",
                    "name": "Shuang Luo"
                },
                {
                    "authorId": "2266273024",
                    "name": "Haozhi Wang"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                }
            ]
        },
        {
            "paperId": "4a810f17843145195d73d5c63dc0f1477ba9cafc",
            "title": "ChiPFormer: Transferable Chip Placement via Offline Decision Transformer",
            "abstract": "Placement is a critical step in modern chip design, aiming to determine the positions of circuit modules on the chip canvas. Recent works have shown that reinforcement learning (RL) can improve human performance in chip placement. However, such an RL-based approach suffers from long training time and low transfer ability in unseen chip circuits. To resolve these challenges, we cast the chip placement as an offline RL formulation and present ChiPFormer that enables learning a transferable placement policy from fixed offline data. ChiPFormer has several advantages that prior arts do not have. First, ChiPFormer can exploit offline placement designs to learn transferable policies more efficiently in a multi-task setting. Second, ChiPFormer can promote effective finetuning for unseen chip circuits, reducing the placement runtime from hours to minutes. Third, extensive experiments on 32 chip circuits demonstrate that ChiPFormer achieves significantly better placement quality while reducing the runtime by 10x compared to recent state-of-the-art approaches in both public benchmarks and realistic industrial tasks. The deliverables are released at https://sites.google.com/view/chipformer/home.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192588046",
                    "name": "Yao Lai"
                },
                {
                    "authorId": "49721910",
                    "name": "Jinxin Liu"
                },
                {
                    "authorId": "47347445",
                    "name": "Zhentao Tang"
                },
                {
                    "authorId": "37722675",
                    "name": "Bin Wang"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                },
                {
                    "authorId": "47571885",
                    "name": "Ping Luo"
                }
            ]
        },
        {
            "paperId": "4b80bd02a667a99e548709915017c80eff9d7011",
            "title": "Generalized Universal Domain Adaptation with Generative Flow Networks",
            "abstract": "We introduce a new problem in unsupervised domain adaptation, termed as Generalized Universal Domain Adaptation (GUDA), which aims to achieve precise prediction of all target labels including unknown categories. GUDA bridges the gap between label distribution shift-based and label space mismatch-based variants, essentially categorizing them as a unified problem, guiding to a comprehensive framework for thoroughly solving all the variants. The key challenge of GUDA is developing and identifying novel target categories while estimating the target label distribution. To address this problem, we take advantage of the powerful exploration capability of generative flow networks and propose an active domain adaptation algorithm named GFlowDA, which selects diverse samples with probabilities proportional to a reward function. To enhance the exploration capability and effectively perceive the target label distribution, we tailor the states and rewards, and introduce an efficient solution for parent exploration and state transition. We also propose a training paradigm for GUDA called Generalized Universal Adversarial Network (GUAN), which involves collaborative optimization between GUAN and GFlowNet. Theoretical analysis highlights the importance of exploration, and extensive experiments on benchmark datasets demonstrate the superiority of GFlowDA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2148404332",
                    "name": "Didi Zhu"
                },
                {
                    "authorId": "47002988",
                    "name": "Yinchuan Li"
                },
                {
                    "authorId": "49713700",
                    "name": "Yunfeng Shao"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                },
                {
                    "authorId": "93192602",
                    "name": "Fei Wu"
                },
                {
                    "authorId": "33870528",
                    "name": "Kun Kuang"
                },
                {
                    "authorId": "2111222729",
                    "name": "Jun Xiao"
                },
                {
                    "authorId": "2190938213",
                    "name": "Chao Wu"
                }
            ]
        }
    ]
}