{
    "authorId": "2110449137",
    "papers": [
        {
            "paperId": "bb16e1cc304fdbf2e5a7e034cb0235fa1aa37a76",
            "title": "IDGenRec: LLM-RecSys Alignment with Textual ID Learning",
            "abstract": "Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potential for a foundational generative recommendation model. Experiments show that our framework consistently surpasses existing models in sequential recommendation under standard experimental setting. Then, we explore the possibility of training a foundation recommendation model with the proposed method on data collected from 19 different datasets and tested its recommendation performance on 6 unseen datasets across different platforms under a completely zero-shot setting. The results show that the zero-shot performance of the pre-trained foundation model is comparable to or even better than some traditional recommendation models based on supervised training, showing the potential of the IDGen paradigm serving as the foundation model for generative recommendation. Code and data are open-sourced at https://github.com/agiresearch/IDGenRec.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "2007245028",
                    "name": "Wenyue Hua"
                },
                {
                    "authorId": "152988336",
                    "name": "Yingqiang Ge"
                },
                {
                    "authorId": "2109968285",
                    "name": "Zelong Li"
                },
                {
                    "authorId": "2145038716",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "0d230fe2af4fe4fd2257ba2a10cfeb91126e27a3",
            "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
            "abstract": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) are three prominent AI applications that have traditionally developed independently, resulting in disparate modeling and engineering methodologies. This has impeded the ability for these fields to directly benefit from each other's advancements. With the recent development of foundation models, large language models have emerged as a potential general-purpose interface for unifying different modalities and problem formulations. In light of this, we propose the development of a multimodal foundation model (MFM) considering visual, textual, and personalization modalities under the P5 recommendation paradigm, thus named VIP5 (Visual P5), to unify various modalities and recommendation tasks. This will enable the processing of multiple modalities in a shared architecture for improved recommendations. To achieve this, we introduce multimodal personalized prompts to accommodate multiple modalities under a shared format. Additionally, we propose a parameter-efficient training method for foundation models, which involves freezing the P5 backbone and fine-tuning lightweight adapters, resulting in improved recommendation performance and increased efficiency in terms of training time and memory usage. Code and data of VIP5 are available at https://github.com/jeykigung/VIP5.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1947101",
                    "name": "Shijie Geng"
                },
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "50152132",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "2011378",
                    "name": "Zuohui Fu"
                },
                {
                    "authorId": "1591136873",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "33f12f9c4ac4715dd643fcae2fa2764376bf1eaf",
            "title": "Towards More Robust and Accurate Sequential Recommendation with Cascade-guided Adversarial Training",
            "abstract": "Sequential recommendation models, models that learn from chronological user-item interactions, outperform traditional recommendation models in many settings. Despite the success of sequential recommendation models, their robustness has recently come into question. Two properties unique to the nature of sequential recommendation models may impair their robustness - the cascade effects induced during training and the model's tendency to rely too heavily on temporal information. To address these vulnerabilities, we propose Cascade-guided Adversarial training, a new adversarial training procedure that is specifically designed for sequential recommendation models. Our approach harnesses the intrinsic cascade effects present in sequential modeling to produce strategic adversarial perturbations to item embeddings during training. Experiments on training state-of-the-art sequential models on four public datasets from different domains show that our training approach produces superior model ranking accuracy and superior model robustness to real item replacement perturbations when compared to both standard model training and generic adversarial training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "71926704",
                    "name": "Shelby Heinecke"
                },
                {
                    "authorId": "1844299386",
                    "name": "Zhiwei Liu"
                },
                {
                    "authorId": "2109292621",
                    "name": "Yong-Guang Chen"
                },
                {
                    "authorId": "2154703853",
                    "name": "Yongfeng Zhang"
                },
                {
                    "authorId": "46507194",
                    "name": "Haiquan Wang"
                }
            ]
        },
        {
            "paperId": "38179848e2d6a3ad373b1793848816111428ac36",
            "title": "OpenAGI: When LLM Meets Domain Experts",
            "abstract": "Human intelligence excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive intelligent models, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research platform designed for multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152988336",
                    "name": "Yingqiang Ge"
                },
                {
                    "authorId": "2007245028",
                    "name": "Wenyue Hua"
                },
                {
                    "authorId": "2111810606",
                    "name": "Jianchao Ji"
                },
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "2145038716",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "3e82d12dbce6becdcae94e3ebb01dd569bdb734f",
            "title": "Causal Inference for Recommendation: Foundations, Methods and Applications",
            "abstract": "Recommender systems are important and powerful tools for various personalized services. Traditionally, these systems use data mining and machine learning techniques to make recommendations based on correlations found in the data. However, relying solely on correlation without considering the underlying causal mechanism may lead to various practical issues such as fairness, explainability, robustness, bias, echo chamber and controllability problems. Therefore, researchers in related area have begun incorporating causality into recommendation systems to address these issues. In this survey, we review the existing literature on causal inference in recommender systems. We discuss the fundamental concepts of both recommender systems and causal inference as well as their relationship, and review the existing work on causal methods for different problems in recommender systems. Finally, we discuss open problems and future directions in the field of causal inference for recommendations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "2111810606",
                    "name": "Jianchao Ji"
                },
                {
                    "authorId": "48515097",
                    "name": "Yunqi Li"
                },
                {
                    "authorId": "152988336",
                    "name": "Yingqiang Ge"
                },
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "2145038716",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "5b9c89f5b743916c95c88605f02ce78be7699941",
            "title": "ExplainableFold: Understanding AlphaFold Prediction with Explainable AI",
            "abstract": "This paper presents ExplainableFold (xFold), which is an Explainable AI framework for protein structure prediction. Despite the success of AI-based methods such as AlphaFold (\u03b1Fold) in this field, the underlying reasons for their predictions remain unclear due to the black-box nature of deep learning models. To address this, we propose a counterfactual learning framework inspired by biological principles to generate counterfactual explanations for protein structure prediction, enabling a dry-lab experimentation approach. Our experimental results demonstrate the ability of ExplainableFold to generate high-quality explanations for AlphaFold's predictions, providing near-experimental understanding of the effects of amino acids on 3D protein structure. This framework has the potential to facilitate a deeper understanding of protein structures. Source code and data of the ExplainableFold project are available at https://github.com/rutgerswiselab/ExplainableFold.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "2145038716",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "628219d6db682340eb40bc5122ed62dbccab3f34",
            "title": "Counterfactual Collaborative Reasoning",
            "abstract": "Causal reasoning and logical reasoning are two important types of reasoning abilities for human intelligence. However, their relationship has not been extensively explored under machine intelligence context. In this paper, we explore how the two reasoning abilities can be jointly modeled to enhance both accuracy and explainability of machine learning models. More specifically, by integrating two important types of reasoning ability--counterfactual reasoning and (neural) logical reasoning--we propose Counterfactual Collaborative Reasoning (CCR), which conducts counterfactual logic reasoning to improve the performance. In particular, we use recommender system as an example to show how CCR alleviate data scarcity, improve accuracy and enhance transparency. Technically, we leverage counterfactual reasoning to generate \"difficult\" counterfactual training examples for data augmentation, which--together with the original training examples--can enhance the model performance. Since the augmented data is model irrelevant, they can be used to enhance any model, enabling the wide applicability of the technique. Besides, most of the existing data augmentation methods focus on \"implicit data augmentation\" over users' implicit feedback, while our framework conducts \"explicit data augmentation\" over users explicit feedback based on counterfactual logic reasoning. Experiments on three real-world datasets show that CCR achieves better performance than non-augmented models and implicitly augmented models, and also improves model transparency by generating counterfactual explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111810606",
                    "name": "Jianchao Ji"
                },
                {
                    "authorId": "2109968285",
                    "name": "Zelong Li"
                },
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "2209210425",
                    "name": "Max Xiong"
                },
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "152988336",
                    "name": "Yingqiang Ge"
                },
                {
                    "authorId": "1728073",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2145038716",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "cd200572cb6ef882b8dd9fcf5632f703820b5bc4",
            "title": "User-Controllable Recommendation via Counterfactual Retrospective and Prospective Explanations",
            "abstract": "Modern recommender systems utilize users' historical behaviors to generate personalized recommendations. However, these systems often lack user controllability, leading to diminished user satisfaction and trust in the systems. Acknowledging the recent advancements in explainable recommender systems that enhance users' understanding of recommendation mechanisms, we propose leveraging these advancements to improve user controllability. In this paper, we present a user-controllable recommender system that seamlessly integrates explainability and controllability within a unified framework. By providing both retrospective and prospective explanations through counterfactual reasoning, users can customize their control over the system by interacting with these explanations. Furthermore, we introduce and assess two attributes of controllability in recommendation systems: the complexity of controllability and the accuracy of controllability. Experimental evaluations on MovieLens and Yelp datasets substantiate the effectiveness of our proposed framework. Additionally, our experiments demonstrate that offering users control options can potentially enhance recommendation accuracy in the future. Source code and data are available at \\url{https://github.com/chrisjtan/ucr}.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "152988336",
                    "name": "Yingqiang Ge"
                },
                {
                    "authorId": "2153095583",
                    "name": "Yangchun Zhu"
                },
                {
                    "authorId": "35846319",
                    "name": "Yinglong Xia"
                },
                {
                    "authorId": "33642939",
                    "name": "Jiebo Luo"
                },
                {
                    "authorId": "2111810606",
                    "name": "Jianchao Ji"
                },
                {
                    "authorId": "2145038716",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "d9ff3db7a9e37fe0363bb87aa47acd6b6b67977e",
            "title": "GenRec: Large Language Model for Generative Recommendation",
            "abstract": "In recent years, large language models (LLM) have emerged as powerful tools for diverse natural language processing tasks. However, their potential for recommender systems under the generative recommendation paradigm remains relatively unexplored. This paper presents an innovative approach to recommendation systems using large language models (LLMs) based on text data. In this paper, we present a novel LLM for generative recommendation (GenRec) that utilized the expressive power of LLM to directly generate the target item to recommend, rather than calculating ranking score for each candidate item one by one as in traditional discriminative recommendation. GenRec uses LLM's understanding ability to interpret context, learn user preferences, and generate relevant recommendation. Our proposed approach leverages the vast knowledge encoded in large language models to accomplish recommendation tasks. We first we formulate specialized prompts to enhance the ability of LLM to comprehend recommendation tasks. Subsequently, we use these prompts to fine-tune the LLaMA backbone LLM on a dataset of user-item interactions, represented by textual data, to capture user preferences and item characteristics. Our research underscores the potential of LLM-based generative recommendation in revolutionizing the domain of recommendation systems and offers a foundational framework for future explorations in this field. We conduct extensive experiments on benchmark datasets, and the experiments shows that our GenRec has significant better results on large dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111810606",
                    "name": "Jianchao Ji"
                },
                {
                    "authorId": "2109968285",
                    "name": "Zelong Li"
                },
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "2007245028",
                    "name": "Wenyue Hua"
                },
                {
                    "authorId": "152988336",
                    "name": "Yingqiang Ge"
                },
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "2145038716",
                    "name": "Yongfeng Zhang"
                }
            ]
        },
        {
            "paperId": "e0edec56a40719878ecff83c847a083b57641047",
            "title": "Efficient Non-Sampling Graph Neural Networks",
            "abstract": "A graph is a widely used and effective data structure in many applications; it describes the relationships among nodes or entities. Currently, most semi-supervised or unsupervised graph neural network models are trained based on a very basic operation called negative sampling. Usually, the purpose of the learning objective is to maximize the similarity between neighboring nodes while minimizing the similarity between nodes that are not close to each other. Negative sampling can reduce the time complexity by sampling a small fraction of the negative nodes instead of using all of the negative nodes when optimizing the objective. However, sampling of the negative nodes may fail to deliver stable model performance due to the uncertainty in the sampling procedure. To avoid such disadvantages, we provide an efficient Non-Sampling Graph Neural Network (NS-GNN) framework. The main idea is to use all the negative samples when optimizing the learning objective to avoid the sampling process. Of course, directly using all of the negative samples may cause a large increase in the model training time. To mitigate this problem, we rearrange the origin loss function into a linear form and take advantage of meticulous mathematical derivation to reduce the complexity of the loss function. Experiments on benchmark datasets show that our framework can provide better efficiency at the same level of prediction accuracy compared with existing negative sampling-based models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111810606",
                    "name": "Jianchao Ji"
                },
                {
                    "authorId": "2109968285",
                    "name": "Zelong Li"
                },
                {
                    "authorId": "2111044480",
                    "name": "Shuyuan Xu"
                },
                {
                    "authorId": "152988336",
                    "name": "Yingqiang Ge"
                },
                {
                    "authorId": "2110449137",
                    "name": "Juntao Tan"
                },
                {
                    "authorId": "2145038716",
                    "name": "Yongfeng Zhang"
                }
            ]
        }
    ]
}