{
    "authorId": "1768153749",
    "papers": [
        {
            "paperId": "01706dd038b959e92a93f3141bb98be8f1f048f0",
            "title": "Hyper-Decision Transformer for Efficient Online Policy Adaptation",
            "abstract": "Decision Transformers (DT) have demonstrated strong performances in offline reinforcement learning settings, but quickly adapting to unseen novel tasks remains challenging. To address this challenge, we propose a new framework, called Hyper-Decision Transformer (HDT), that can generalize to novel tasks from a handful of demonstrations in a data- and parameter-efficient manner. To achieve such a goal, we propose to augment the base DT with an adaptation module, whose parameters are initialized by a hyper-network. When encountering unseen tasks, the hyper-network takes a handful of demonstrations as inputs and initializes the adaptation module accordingly. This initialization enables HDT to efficiently adapt to novel tasks by only fine-tuning the adaptation module. We validate HDT's generalization capability on object manipulation tasks. We find that with a single expert demonstration and fine-tuning only 0.5% of DT parameters, HDT adapts faster to unseen tasks than fine-tuning the whole DT model. Finally, we explore a more challenging setting where expert actions are not available, and we show that HDT outperforms state-of-the-art baselines in terms of task success rates by a large margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "12016940",
                    "name": "Yuchen Lu"
                },
                {
                    "authorId": "2714199",
                    "name": "Yikang Shen"
                },
                {
                    "authorId": "2211163649",
                    "name": "Shun Zhang"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "2056157586",
                    "name": "Chuang Gan"
                }
            ]
        },
        {
            "paperId": "6c821635f65026c782ac37eceff97949a9f231de",
            "title": "Guardians as You Fall: Active Mode Transition for Safe Falling",
            "abstract": "Recent advancements in optimal control and reinforcement learning have enabled quadrupedal robots to perform various agile locomotion tasks over diverse terrains. During these agile motions, ensuring the stability and resiliency of the robot is a primary concern to prevent catastrophic falls and mitigate potential damages. Previous methods primarily focus on recovery policies after the robot falls. There is no active safe falling solution to the best of our knowledge. In this paper, we proposed Guardians as You Fall (GYF), a safe falling/tumbling and recovery framework that can actively tumble and recover to stable modes to reduce damage in highly dynamic scenarios. The key idea of GYF is to adaptively traverse different stable modes via active tumbling before the robot shifts to irrecoverable poses. Via comprehensive simulation and real-world experiments, we show that GYF significantly reduces the maximum acceleration and jerk of the robot base compared to the baselines. In particular, GYF reduces the maximum acceleration and jerk by 20%~73% in different scenarios in simulation and real-world experiments. GYF offers a new perspective on safe falling and recovery in locomotion tasks, potentially enabling much more aggressive explorations of existing agile locomotion skills.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257099226",
                    "name": "Yikai Wang"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "2249759531",
                    "name": "Guanya Shi"
                },
                {
                    "authorId": "2258097326",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "75a259f3a61bf27c214d61cd2ef26f3291d77add",
            "title": "A trajectory is worth three sentences: multimodal transformer for offline reinforcement learning",
            "abstract": "Transformers hold tremendous promise in solving of\ufb02ine reinforcement learning (RL) by formulating it as a sequence modeling problem inspired by language modeling (LM). Prior works using trans-formers model a sample (trajectory) of RL as one sequence analogous to a sequence of words (one sentence) in LM, despite the fact that each trajectory includes tokens from three diverse modalities: state, action, and reward, while a sentence contains words only. Rather than taking a modality-agnostic approach which uniformly models the tokens from different modalities as one sequence, we propose a multimodal sequence modeling approach in which a trajectory (one \u201csentence\u201d) of three modalities (state, action, reward) is disentangled into three unimodal ones (three \u201csentences\u201d). We investigate the correlation of different modalities during sequential decision-making and use the insights to design a multimodal transformer, named Decision Transducer (DTd). DTd outperforms prior art in of\ufb02ine RL on the conducted D4RL benchmarks and enjoys better sample ef\ufb01ciency and algorithm \ufb02exibility. Our code is made publicly here.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2125067155",
                    "name": "Yiqi Wang"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "51347064",
                    "name": "Laixi Shi"
                },
                {
                    "authorId": "1784472",
                    "name": "Yuejie Chi"
                }
            ]
        },
        {
            "paperId": "7ed237af793f43c442b3e8e1bc9ace906a276b2a",
            "title": "Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?",
            "abstract": "Recent advancements in Large Language Models (LLMs) have drawn increasing attention since the learned embeddings pretrained on large-scale datasets have shown powerful ability in various downstream applications. However, whether the learned knowledge by LLMs can be transferred to clinical cardiology remains unknown. In this work, we aim to bridge this gap by transferring the knowledge of LLMs to clinical Electrocardiography (ECG). We propose an approach for cardiovascular disease diagnosis and automatic ECG diagnosis report generation. We also introduce an additional loss function by Optimal Transport (OT) to align the distribution between ECG and language embedding. The learned embeddings are evaluated on two downstream tasks: (1) automatic ECG diagnosis report generation, and (2) zero-shot cardiovascular disease detection. Our approach is able to generate high-quality cardiac diagnosis reports and also achieves competitive zero-shot classification performance even compared with supervised baselines, which proves the feasibility of transferring knowledge from LLMs to the cardiac domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "2151860122",
                    "name": "Michael Rosenberg"
                },
                {
                    "authorId": "1418480916",
                    "name": "E. Liu"
                },
                {
                    "authorId": "2180027829",
                    "name": "Douglas Weber"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "82da02137bae421a3f7a89c3bf2ab662037f4dfa",
            "title": "Embodied Executable Policy Learning with Language-based Scene Summarization",
            "abstract": "Large Language models (LLMs) have shown remarkable success in assisting robot learning tasks, i.e., complex household planning.However, the performance of pretrained LLMs heavily relies on domain-specific templated text data, which may be infeasible in real-world robot learning tasks with image-based observations. Moreover, existing LLMs with text inputs lack the capability to evolve with non-expert interactions with environments.In this work, we introduce a novel learning paradigm that generates robots\u2019 executable actions in the form of text, derived solely from visual observations. Our proposed paradigm stands apart from previous works, which utilized either language instructions or a combination of language and visual data as inputs. We demonstrate that our proposed method can employ two fine-tuning strategies, including imitation learning and reinforcement learning approaches, to adapt to the target test tasks effectively.We conduct extensive experiments involving various model selections, environments, and tasks across 7 house layouts in the VirtualHome environment. Our experimental results demonstrate that our method surpasses existing baselines, confirming the effectiveness of this novel learning paradigm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "119119640",
                    "name": "William Jongwon Han"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "47783376",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "af07c2b3ec06b31568f9842f71185f2cfc7d9efc",
            "title": "SCCS: Semantics-Consistent Cross-domain Summarization via Optimal Transport Alignment",
            "abstract": "Multimedia summarization with multimodal output (MSMO) is a recently explored application in language grounding. It plays an essential role in real-world applications, i.e., automatically generating cover images and titles for news articles or providing introductions to online videos. However, existing meth-ods extract features from the whole video and article and use fusion methods to select the representative one, thus usually ignoring the critical structure and varying semantics with video/document. In this work, we propose a Semantics-Consistent Cross-domain Summa-rization (SCCS) model based on optimal transport alignment with visual and textual segmentation. Our method first decomposes both videos and articles into segments in order to capture the structural semantics, and then follows a cross-domain alignment objective with optimal transport distance, which leverages multimodal interaction to match and select the visual and textual summary. We evaluated our method on three MSMO datasets, and achieved performance improvement by 8% & 6% of textual and 6.6% &5.7% of video summarization, respectively, which demonstrated the effectiveness of our method in producing high-quality multimodal summaries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "145262461",
                    "name": "Trung Bui"
                },
                {
                    "authorId": "8056043",
                    "name": "Zhaowen Wang"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "41151701",
                    "name": "Hailin Jin"
                }
            ]
        },
        {
            "paperId": "b4eefd84c64e5caff20fc817fbcfa8975b5bc289",
            "title": "Creative Robot Tool Use with Large Language Models",
            "abstract": "Tool use is a hallmark of advanced intelligence, exemplified in both animal behavior and robotic capabilities. This paper investigates the feasibility of imbuing robots with the ability to creatively use tools in tasks that involve implicit physical constraints and long-term planning. Leveraging Large Language Models (LLMs), we develop RoboTool, a system that accepts natural language instructions and outputs executable code for controlling robots in both simulated and real-world environments. RoboTool incorporates four pivotal components: (i) an\"Analyzer\"that interprets natural language to discern key task-related concepts, (ii) a\"Planner\"that generates comprehensive strategies based on the language input and key concepts, (iii) a\"Calculator\"that computes parameters for each skill, and (iv) a\"Coder\"that translates these plans into executable Python code. Our results show that RoboTool can not only comprehend explicit or implicit physical constraints and environmental factors but also demonstrate creative tool use. Unlike traditional Task and Motion Planning (TAMP) methods that rely on explicit optimization, our LLM-based system offers a more flexible, efficient, and user-friendly solution for complex robotics tasks. Through extensive experiments, we validate that RoboTool is proficient in handling tasks that would otherwise be infeasible without the creative use of tools, thereby expanding the capabilities of robotic systems. Demos are available on our project page: https://creative-robotool.github.io/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "2119407102",
                    "name": "Peide Huang"
                },
                {
                    "authorId": "2255778992",
                    "name": "Wenhao Yu"
                },
                {
                    "authorId": "2261321236",
                    "name": "Shiqi Liu"
                },
                {
                    "authorId": "2158345669",
                    "name": "Xilun Zhang"
                },
                {
                    "authorId": "2253400416",
                    "name": "Yaru Niu"
                },
                {
                    "authorId": "2240715659",
                    "name": "Tingnan Zhang"
                },
                {
                    "authorId": "2261087052",
                    "name": "Fei Xia"
                },
                {
                    "authorId": "2257005884",
                    "name": "Jie Tan"
                },
                {
                    "authorId": "2261094763",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "dae6a9c393115ba36391d249918d743709e8aee8",
            "title": "What Went Wrong? Closing the Sim-to-Real Gap via Differentiable Causal Discovery",
            "abstract": "Training control policies in simulation is more appealing than on real robots directly, as it allows for exploring diverse states in an efficient manner. Yet, robot simulators inevitably exhibit disparities from the real-world \\rebut{dynamics}, yielding inaccuracies that manifest as the dynamical simulation-to-reality (sim-to-real) gap. Existing literature has proposed to close this gap by actively modifying specific simulator parameters to align the simulated data with real-world observations. However, the set of tunable parameters is usually manually selected to reduce the search space in a case-by-case manner, which is hard to scale up for complex systems and requires extensive domain knowledge. To address the scalability issue and automate the parameter-tuning process, we introduce COMPASS, which aligns the simulator with the real world by discovering the causal relationship between the environment parameters and the sim-to-real gap. Concretely, our method learns a differentiable mapping from the environment parameters to the differences between simulated and real-world robot-object trajectories. This mapping is governed by a simultaneously learned causal graph to help prune the search space of parameters, provide better interpretability, and improve generalization on unseen parameters. We perform experiments to achieve both sim-to-sim and sim-to-real transfer, and show that our method has significant improvements in trajectory alignment and task success rate over strong baselines in several challenging manipulation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119407102",
                    "name": "Peide Huang"
                },
                {
                    "authorId": "2158345669",
                    "name": "Xilun Zhang"
                },
                {
                    "authorId": "2113998517",
                    "name": "Ziang Cao"
                },
                {
                    "authorId": "2131159790",
                    "name": "Shiqi Liu"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "152425748",
                    "name": "Wenhao Ding"
                },
                {
                    "authorId": "26253744",
                    "name": "Jonathan M Francis"
                },
                {
                    "authorId": "12515120",
                    "name": "Bingqing Chen"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                }
            ]
        },
        {
            "paperId": "1dd9b35955cf8dddf1047044ca0a757249f817b8",
            "title": "Semantics-Consistent Cross-domain Summarization via Optimal Transport Alignment",
            "abstract": "Multimedia summarization with multimodal output (MSMO) is a recently explored application in language grounding. It plays an essential role in real-world applications, i.e., automatically generating cover images and titles for news articles or providing introductions to online videos. However, existing methods extract features from the whole video and article and use fusion methods to select the representative one, thus usually ignoring the critical structure and varying semantics. In this work, we propose a Semantics-Consistent Cross-domain Summarization (SCCS) model based on optimal transport alignment with visual and textual segmentation. In specific, our method first decomposes both video and article into segments in order to capture the structural semantics, respectively. Then SCCS follows a cross-domain alignment objective with optimal transport distance, which leverages multimodal interaction to match and select the visual and textual summary. We evaluated our method on three recent multimodal datasets and demonstrated the effectiveness of our method in producing high-quality multimodal summaries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "1870353",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "145262461",
                    "name": "Trung Bui"
                },
                {
                    "authorId": "8056043",
                    "name": "Zhaowen Wang"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "41151701",
                    "name": "Hailin Jin"
                }
            ]
        },
        {
            "paperId": "2ba0eb8aa87e5f3653a7735ae01aacefdb60ba11",
            "title": "Group Distributionally Robust Reinforcement Learning with Hierarchical Latent Variables",
            "abstract": "One key challenge for multi-task Reinforcement learning (RL) in practice is the absence of task indicators. Robust RL has been applied to deal with task ambiguity, but may result in over-conservative policies. To balance the worst-case (robustness) and average performance, we propose Group Distributionally Robust Markov Decision Process (GDR-MDP), a flexible hierarchical MDP formulation that encodes task groups via a latent mixture model. GDR-MDP identifies the optimal policy that maximizes the expected return under the worst-possible qualified belief over task groups within an ambiguity set. We rigorously show that GDR-MDP's hierarchical structure improves distributional robustness by adding regularization to the worst possible outcomes. We then develop deep RL algorithms for GDR-MDP for both value-based and policy-based RL methods. Extensive experiments on Box2D control tasks, MuJoCo benchmarks, and Google football platforms show that our algorithms outperform classic robust training algorithms across diverse environments in terms of robustness under belief uncertainties. Demos are available on our project page (\\url{https://sites.google.com/view/gdr-rl/home}).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1768153749",
                    "name": "Mengdi Xu"
                },
                {
                    "authorId": "2119407102",
                    "name": "Peide Huang"
                },
                {
                    "authorId": "11874816",
                    "name": "Yaru Niu"
                },
                {
                    "authorId": "2112128899",
                    "name": "Visak C. V. Kumar"
                },
                {
                    "authorId": "51459833",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "2192836350",
                    "name": "Chao Fang"
                },
                {
                    "authorId": "144015229",
                    "name": "Kuan-Hui Lee"
                },
                {
                    "authorId": "3307046",
                    "name": "Xuewei Qi"
                },
                {
                    "authorId": "49893525",
                    "name": "H. Lam"
                },
                {
                    "authorId": "71788673",
                    "name": "Bo Li"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                }
            ]
        }
    ]
}