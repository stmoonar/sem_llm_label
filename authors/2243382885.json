{
    "authorId": "2243382885",
    "papers": [
        {
            "paperId": "05883daa79bfc07c0ffbe069314e223050310f51",
            "title": "LinkGPT: Teaching Large Language Models To Predict Missing Links",
            "abstract": "Large Language Models (LLMs) have shown promising results on various language and vision tasks. Recently, there has been growing interest in applying LLMs to graph-based tasks, particularly on Text-Attributed Graphs (TAGs). However, most studies have focused on node classification, while the use of LLMs for link prediction (LP) remains understudied. In this work, we propose a new task on LLMs, where the objective is to leverage LLMs to predict missing links between nodes in a graph. This task evaluates an LLM's ability to reason over structured data and infer new facts based on learned patterns. This new task poses two key challenges: (1) How to effectively integrate pairwise structural information into the LLMs, which is known to be crucial for LP performance, and (2) how to solve the computational bottleneck when teaching LLMs to perform LP. To address these challenges, we propose LinkGPT, the first end-to-end trained LLM for LP tasks. To effectively enhance the LLM's ability to understand the underlying structure, we design a two-stage instruction tuning approach where the first stage fine-tunes the pairwise encoder, projector, and node projector, and the second stage further fine-tunes the LLMs to predict links. To address the efficiency challenges at inference time, we introduce a retrieval-reranking scheme. Experiments show that LinkGPT can achieve state-of-the-art performance on real-world graphs as well as superior generalization in zero-shot and few-shot learning, surpassing existing benchmarks. At inference time, it can achieve $10\\times$ speedup while maintaining high LP accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2305970801",
                    "name": "Zhongmou He"
                },
                {
                    "authorId": "2146272629",
                    "name": "Jing Zhu"
                },
                {
                    "authorId": "2282529091",
                    "name": "Shengyi Qian"
                },
                {
                    "authorId": "2243382885",
                    "name": "Joyce Chai"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                }
            ]
        },
        {
            "paperId": "265136f7a5438e252e59d8d807042b625e29111f",
            "title": "Groundhog Grounding Large Language Models to Holistic Segmentation",
            "abstract": "Most multimodal large language models (MLLMs) learn language-to-object grounding through causal language modeling where grounded objects are captured by bounding boxes as sequences of location tokens. This paradigm lacks pixel-level representations that are important for fine-grained visual understanding and diagnosis. In this work, we introduce Groundhog, an MLLM developed by grounding Large Language Models to holistic segmentation. GROUNDHOG incorporates a masked feature extractor and converts extracted features into visual entity tokens for the MLLM backbone, which then connects groundable phrases to unified grounding masks by retrieving and merging the entity masks. To train GROUNDHOG, we carefully curated M3G2, a grounded visual instruction tuning dataset with Multi-Modal Multi-Grained Grounding, by harvesting a collection of segmentation-grounded datasets with rich annotations. Our experimental results show that GROUNDHOG achieves superior performance on various language grounding tasks without task-specific fine-tuning, and significantly reduces object hallucination. GROUNDHOG also demonstrates better grounding towards complex forms of visual input and provides easy-to-understand diagnosis in failure cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46868553",
                    "name": "Yichi Zhang"
                },
                {
                    "authorId": "2151006930",
                    "name": "Ziqiao Ma"
                },
                {
                    "authorId": "46757485",
                    "name": "Xiaofeng Gao"
                },
                {
                    "authorId": "1831108414",
                    "name": "Suhaila Shakiah"
                },
                {
                    "authorId": "2288100488",
                    "name": "Qiaozi Gao"
                },
                {
                    "authorId": "2243382885",
                    "name": "Joyce Chai"
                }
            ]
        },
        {
            "paperId": "39d8486475173357619647061dda377f4c38853e",
            "title": "Multi-Object Hallucination in Vision-Language Models",
            "abstract": "Large vision language models (LVLMs) often suffer from object hallucination, producing objects not present in the given images. While current benchmarks for object hallucination primarily concentrate on the presence of a single object class rather than individual entities, this work systematically investigates multi-object hallucination, examining how models misperceive (e.g., invent nonexistent objects or become distracted) when tasked with focusing on multiple objects simultaneously. We introduce Recognition-based Object Probing Evaluation (ROPE), an automated evaluation protocol that considers the distribution of object classes within a single image during testing and uses visual referring prompts to eliminate ambiguity. With comprehensive empirical studies and analysis of potential factors leading to multi-object hallucination, we found that (1) LVLMs suffer more hallucinations when focusing on multiple objects compared to a single object. (2) The tested object class distribution affects hallucination behaviors, indicating that LVLMs may follow shortcuts and spurious correlations.(3) Hallucinatory behaviors are influenced by data-specific factors, salience and frequency, and model intrinsic behaviors. We hope to enable LVLMs to recognize and reason about multiple objects that often occur in realistic visual scenes, provide insights, and quantify our progress towards mitigating the issues.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2290026490",
                    "name": "Xuweiyi Chen"
                },
                {
                    "authorId": "2151006930",
                    "name": "Ziqiao Ma"
                },
                {
                    "authorId": "2310395975",
                    "name": "Xuejun Zhang"
                },
                {
                    "authorId": "2261187041",
                    "name": "Sihan Xu"
                },
                {
                    "authorId": "1390481263",
                    "name": "Shengyi Qian"
                },
                {
                    "authorId": "46477844",
                    "name": "Jianing Yang"
                },
                {
                    "authorId": "1786435",
                    "name": "D. Fouhey"
                },
                {
                    "authorId": "2243382885",
                    "name": "Joyce Chai"
                }
            ]
        },
        {
            "paperId": "c4f2683cdd3f3c0d485a214acae36df3a6863160",
            "title": "3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination",
            "abstract": "The integration of language and 3D perception is crucial for developing embodied agents and robots that comprehend and interact with the physical world. While large language models (LLMs) have demonstrated impressive language understanding and generation capabilities, their adaptation to 3D environments (3D-LLMs) remains in its early stages. A primary challenge is the absence of large-scale datasets that provide dense grounding between language and 3D scenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset comprising 40,087 household scenes paired with 6.2 million densely-grounded scene-language instructions. Our results show that instruction tuning with 3D-GRAND significantly enhances grounding capabilities and reduces hallucinations in 3D-LLMs. As part of our contributions, we propose a comprehensive benchmark 3D-POPE to systematically evaluate hallucination in 3D-LLMs, enabling fair comparisons among future models. Our experiments highlight a scaling effect between dataset size and 3D-LLM performance, emphasizing the critical role of large-scale 3D-text datasets in advancing embodied AI research. Notably, our results demonstrate early signals for effective sim-to-real transfer, indicating that models trained on large synthetic data can perform well on real-world 3D scans. Through 3D-GRAND and 3D-POPE, we aim to equip the embodied AI community with essential resources and insights, setting the stage for more reliable and better-grounded 3D-LLMs. Project website: https://3d-grand.github.io",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46477844",
                    "name": "Jianing Yang"
                },
                {
                    "authorId": "2290026490",
                    "name": "Xuweiyi Chen"
                },
                {
                    "authorId": "2243383240",
                    "name": "Nikhil Madaan"
                },
                {
                    "authorId": "2243403473",
                    "name": "Madhavan Iyengar"
                },
                {
                    "authorId": "1390481263",
                    "name": "Shengyi Qian"
                },
                {
                    "authorId": "1786435",
                    "name": "D. Fouhey"
                },
                {
                    "authorId": "2243382885",
                    "name": "Joyce Chai"
                }
            ]
        },
        {
            "paperId": "9db0247728950788a2b42097d81dc0e24eed6bb2",
            "title": "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent",
            "abstract": "3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs significantly improve the grounding capability, especially for complex language queries, making LLM-Grounder an effective approach for 3D vision-language tasks in robotics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46477844",
                    "name": "Jianing Yang"
                },
                {
                    "authorId": "2290026490",
                    "name": "Xuweiyi Chen"
                },
                {
                    "authorId": "1390481263",
                    "name": "Shengyi Qian"
                },
                {
                    "authorId": "2243383240",
                    "name": "Nikhil Madaan"
                },
                {
                    "authorId": "2243403473",
                    "name": "Madhavan Iyengar"
                },
                {
                    "authorId": "1786435",
                    "name": "D. Fouhey"
                },
                {
                    "authorId": "2243382885",
                    "name": "Joyce Chai"
                }
            ]
        }
    ]
}