{
    "authorId": "2150725025",
    "papers": [
        {
            "paperId": "e320fb19ac509f6a4b196d5a801ff69779c8f3c5",
            "title": "Self Supervised Vision for Climate Downscaling",
            "abstract": "Climate change is one of the most critical challenges that our planet is facing today. Rising global temperatures are already affecting Earth's weather and climate patterns with an increased frequency of unpredictable and extreme events. Future projections for climate change research are based on computer models like Earth System Models (ESMs). Climate simulations typically run on a coarser grid due to the high computational resources required, and then undergo a lighter downscaling process to obtain data on a finer grid. This work presents a self-supervised deep learning model that does not require high resolution ground truth data for downscaling. This is realized by leveraging salient distribution patterns and the hidden dependencies between weather variables for an individual data point at runtime. We propose three climate-specific components that well represent the patterns of underlying weather variables and learn intricate inter-variable dependencies. Extensive evaluation with 2x, 3x, and 4x scaling factors demonstrates that our model obtains 8% to 47% performance gain over existing baselines while greatly reducing the overall runtime. The improved performance and no dependence on high resolution ground truth data make our method a valuable tool for future climate research.",
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150725025",
                    "name": "Karandeep Singh"
                },
                {
                    "authorId": "2212076872",
                    "name": "Chaeyoon Jeong"
                },
                {
                    "authorId": "9434857",
                    "name": "Naufal Shidqi"
                },
                {
                    "authorId": "2109454735",
                    "name": "Sungwon Park"
                },
                {
                    "authorId": "2007390496",
                    "name": "A. Nellikkattil"
                },
                {
                    "authorId": "2273652954",
                    "name": "Elke Zeller"
                },
                {
                    "authorId": "2166262174",
                    "name": "Meeyoung Cha"
                }
            ]
        },
        {
            "paperId": "0f541b2cc5d5dd306b2460072dd2540f9d9b9924",
            "title": "Generating High-Resolution Regional Precipitation Using Conditional Diffusion Model",
            "abstract": "Climate downscaling is a crucial technique within climate research, serving to project low-resolution (LR) climate data to higher resolutions (HR). Previous research has demonstrated the effectiveness of deep learning for downscaling tasks. However, most deep learning models for climate downscaling may not perform optimally for high scaling factors (i.e., 4x, 8x) due to their limited ability to capture the intricate details required for generating HR climate data. Furthermore, climate data behaves differently from image data, necessitating a nuanced approach when employing deep generative models. In response to these challenges, this paper presents a deep generative model for downscaling climate data, specifically precipitation on a regional scale. We employ a denoising diffusion probabilistic model (DDPM) conditioned on multiple LR climate variables. The proposed model is evaluated using precipitation data from the Community Earth System Model (CESM) v1.2.2 simulation. Our results demonstrate significant improvements over existing baselines, underscoring the effectiveness of the conditional diffusion model in downscaling climate data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "9434857",
                    "name": "Naufal Shidqi"
                },
                {
                    "authorId": "2212076872",
                    "name": "Chaeyoon Jeong"
                },
                {
                    "authorId": "2109454735",
                    "name": "Sungwon Park"
                },
                {
                    "authorId": "2273652954",
                    "name": "Elke Zeller"
                },
                {
                    "authorId": "2007390496",
                    "name": "A. Nellikkattil"
                },
                {
                    "authorId": "2150725025",
                    "name": "Karandeep Singh"
                }
            ]
        },
        {
            "paperId": "5a6c8f22250482d94874d6de509625ce5d0a3445",
            "title": "Self-supervised learning for climate downscaling",
            "abstract": "Earth system models (ESM) are computer models that quantitatively simulate the Earth\u2019s climate system. These models are the basis of modern research on climate change and its effects on our planet. Advances in computational technologies and simulation methodologies have enabled ESM to produce simulation outputs at a finer level of detail, which is important for policy planning and research at the regional level. As ESM is a complex incorporation of different physical domains and environmental variables, computational costs for conducting simulations at a finer resolution are prohibitively expensive. In practice, the simulation at the coarser level is mapped onto the regional level by the process of \u2018\u2018downscaling\u2019\u2019. In this presents a self-supervised deep-learning solution for climate downscaling that does not require high-resolution ground truth data during the model training process. We introduce a self-supervised convolutional neural network (CNN) super-resolution model that trains on a single data instance at a time and can adapt to its underlying data patterns at runtime. Experimental results demonstrate that the proposed model consistently improves the climate downscaling performance over the widely used baselines by a large margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150725025",
                    "name": "Karandeep Singh"
                },
                {
                    "authorId": "2212076872",
                    "name": "Chaeyoon Jeong"
                },
                {
                    "authorId": "2109454735",
                    "name": "Sungwon Park"
                },
                {
                    "authorId": "2212405493",
                    "name": "Arjun N Babur"
                },
                {
                    "authorId": "51139019",
                    "name": "Elke Zeller"
                },
                {
                    "authorId": "1775511",
                    "name": "Meeyoung Cha"
                }
            ]
        },
        {
            "paperId": "612acb4f0d7ceef09c175e5e8a4824e4c9c58106",
            "title": "Assessing the net benefit of machine learning models in the presence of resource constraints",
            "abstract": "OBJECTIVE\nThe objective of this study is to provide a method to calculate model performance measures in the presence of resource constraints, with a focus on net benefit (NB).\n\n\nMATERIALS AND METHODS\nTo quantify a model's clinical utility, the Equator Network's TRIPOD guidelines recommend the calculation of the NB, which reflects whether the benefits conferred by intervening on true positives outweigh the harms conferred by intervening on false positives. We refer to the NB achievable in the presence of resource constraints as the realized net benefit (RNB), and provide formulae for calculating the RNB.\n\n\nRESULTS\nUsing 4 case studies, we demonstrate the degree to which an absolute constraint (eg, only 3 available intensive care unit [ICU] beds) diminishes the RNB of a hypothetical ICU admission model. We show how the introduction of a relative constraint (eg, surgical beds that can be converted to ICU beds for very high-risk patients) allows us to recoup some of the RNB but with a higher penalty for false positives.\n\n\nDISCUSSION\nRNB can be calculated in silico before the model's output is used to guide care. Accounting for the constraint changes the optimal strategy for ICU bed allocation.\n\n\nCONCLUSIONS\nThis study provides a method to account for resource constraints when planning model-based interventions, either to avoid implementations where constraints are expected to play a larger role or to design more creative solutions (eg, converted ICU beds) to overcome absolute constraints when possible.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2150725025",
                    "name": "Karandeep Singh"
                },
                {
                    "authorId": "1491329554",
                    "name": "N. Shah"
                },
                {
                    "authorId": "144016012",
                    "name": "A. Vickers"
                }
            ]
        },
        {
            "paperId": "70580745b7a4c851fa803b1e0275bdf103a12cd5",
            "title": "GraphFC: Customs Fraud Detection with Label Scarcity",
            "abstract": "Customs officials across the world encounter huge volumes of transactions. Associated with customs transactions is customs fraud-the intentional manipulation of goods declarations to avoid taxes and duties. Due to limited manpower, the customs offices can only manually inspect a small number of declarations, necessitating the automation of customs fraud detection by machine learning techniques. The limited availability of manually inspected ground truth data makes it essential for the ML approach to generalize well on unseen data. However, current customs fraud detection models are not well suited or designed for this setting. In this work, we propose GraphFC (Graph Neural networks for Customs Fraud), a model-agnostic, domain-specific, graph neural network based customs fraud detection model that is designed to work in a real-world setting with limited ground truth data. Extensive experimentation using real customs data from two countries demonstrates that GraphFC generalizes well over unseen data and outperforms various baselines and other models by a large margin.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150725025",
                    "name": "Karandeep Singh"
                },
                {
                    "authorId": "1896151979",
                    "name": "Yu-Che Tsai"
                },
                {
                    "authorId": "2169355",
                    "name": "Cheng-te Li"
                },
                {
                    "authorId": "2166262174",
                    "name": "Meeyoung Cha"
                },
                {
                    "authorId": "2107904896",
                    "name": "Shou-de Lin"
                }
            ]
        },
        {
            "paperId": "9fb25a75cbbd090271defa2858791f5514d84de3",
            "title": "Multi-Stage Machine Learning Model for Hierarchical Tie Valence Prediction",
            "abstract": "Individuals interacting in organizational settings involving varying levels of formal hierarchy naturally form a complex network of social ties having different tie valences (e.g., positive and negative connections). Social ties critically affect employees\u2019 satisfaction, behaviors, cognition, and outcomes\u2014yet identifying them solely through survey data is challenging because of the large size of some organizations or the often hidden nature of these ties and their valences. We present a novel deep learning model encompassing NLP and graph neural network techniques that identifies positive and negative ties in a hierarchical network. The proposed model uses human resource attributes as node information and web-logged work conversation data as link information. Our findings suggest that the presence of conversation data improves the tie valence classification by 8.91% compared to employing user attributes alone. This gain came from accurately distinguishing positive ties, particularly for male, non-minority, and older employee groups. We also show a substantial difference in conversation patterns for positive and negative ties with positive ties being associated with more messages exchanged on weekends, and lower use of words related to anger and sadness. These findings have broad implications for facilitating collaboration and managing conflict within organizational and other social networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150725025",
                    "name": "Karandeep Singh"
                },
                {
                    "authorId": "2116971826",
                    "name": "Seungeon Lee"
                },
                {
                    "authorId": "1856264",
                    "name": "G. Labianca"
                },
                {
                    "authorId": "36363706",
                    "name": "Jesse M. Fagan"
                },
                {
                    "authorId": "1775511",
                    "name": "Meeyoung Cha"
                }
            ]
        },
        {
            "paperId": "ef430bf3ae296efcef65789e3a5df809789c3ba4",
            "title": "Active Learning for Human-in-the-Loop Customs Inspection",
            "abstract": "We study the human-in-the-loop customs inspection scenario, where an AI-assisted algorithm supports customs officers by recommending a set of imported goods to be inspected. If the inspected items are fraudulent, the officers can levy extra duties. The updated decisions are used as additional training data for successive iterations. Inspecting only the likely fraudulent items may lead to an immediate gain in revenue, yet it does not bring new insights for learning dynamic trade patterns. In contrast, including uncertain items in the inspection helps gradually acquire new knowledge that will be used as supplementary training resources to update the system. Based on multiyear customs declaration logs obtained from three countries, we demonstrate that some degree of exploration is necessary to cope with domain shifts in trade data. The results show that a hybrid strategy of jointly selecting likely fraudulent and uncertain items will eventually outperform the exploitation-only strategy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115986606",
                    "name": "Sundong Kim"
                },
                {
                    "authorId": "144396718",
                    "name": "Tung Mai"
                },
                {
                    "authorId": "2049223300",
                    "name": "Thien Khanh"
                },
                {
                    "authorId": "1444699639",
                    "name": "Sungwon Han"
                },
                {
                    "authorId": "2109454735",
                    "name": "Sungwon Park"
                },
                {
                    "authorId": "2150725025",
                    "name": "Karandeep Singh"
                },
                {
                    "authorId": "1775511",
                    "name": "Meeyoung Cha"
                }
            ]
        }
    ]
}