{
    "authorId": "29963551",
    "papers": [
        {
            "paperId": "12c27a40fbcc9dfbf483185291e7c80114a2a9ae",
            "title": "MEMORYLLM: Towards Self-Updatable Large Language Models",
            "abstract": "Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model. We aim to build models containing a considerable portion of self-updatable parameters, enabling the model to integrate new knowledge effectively and efficiently. To this end, we introduce MEMORYLLM, a model that comprises a transformer and a fixed-size memory pool within the latent space of the transformer. MEMORYLLM can self-update with text knowledge and memorize the knowledge injected earlier. Our evaluations demonstrate the ability of MEMORYLLM to effectively incorporate new knowledge, as evidenced by its performance on model editing benchmarks. Meanwhile, the model exhibits long-term information retention capacity, which is validated through our custom-designed evaluations and long-context benchmarks. MEMORYLLM also shows operational integrity without any sign of performance degradation even after nearly a million memory updates. Our code and model are open-sourced at https://github.com/wangyu-ustc/MemoryLLM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256185766",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "29963551",
                    "name": "Xiusi Chen"
                },
                {
                    "authorId": "2254284383",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "2258962117",
                    "name": "Julian McAuley"
                }
            ]
        },
        {
            "paperId": "3dd5ad34012164c4ec9c571a12cc6a7561683dea",
            "title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
            "abstract": "In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one or two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this end, we comprehensively survey over 260 scientific LLMs, discuss their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality. Moreover, we investigate how LLMs have been deployed to benefit scientific discovery. Resources related to this survey are available at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293820617",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "29963551",
                    "name": "Xiusi Chen"
                },
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "2307033488",
                    "name": "Sheng Wang"
                },
                {
                    "authorId": "2307565578",
                    "name": "Shuiwang Ji"
                },
                {
                    "authorId": "2283212563",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "2257136881",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "d25c6e3fd588edb4eea39b09c90da0401bd16a85",
            "title": "TinyLLM: Learning a Small Student from Multiple Large Language Models",
            "abstract": "Transferring the reasoning capability from stronger large language models (LLMs) to smaller ones has been quite appealing, as smaller LLMs are more flexible to deploy with less expense. Among the existing solutions, knowledge distillation stands out due to its outstanding efficiency and generalization. However, existing methods suffer from several drawbacks, including limited knowledge diversity and the lack of rich contextual information. To solve the problems and facilitate the learning of compact language models, we propose TinyLLM, a new knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs. In particular, we encourage the student LLM to not only generate the correct answers but also understand the rationales behind these answers. Given that different LLMs possess diverse reasoning skills, we guide the student model to assimilate knowledge from various teacher LLMs. We further introduce an in-context example generator and a teacher-forcing Chain-of-Thought strategy to ensure that the rationales are accurate and grounded in contextually appropriate scenarios. Extensive experiments on six datasets across two reasoning tasks demonstrate the superiority of our method. Results show that TinyLLM can outperform large teacher LLMs significantly, despite a considerably smaller model size.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46879986",
                    "name": "Yijun Tian"
                },
                {
                    "authorId": "2283258326",
                    "name": "Yikun Han"
                },
                {
                    "authorId": "29963551",
                    "name": "Xiusi Chen"
                },
                {
                    "authorId": "2283212563",
                    "name": "Wei Wang"
                },
                {
                    "authorId": "144539424",
                    "name": "N. Chawla"
                }
            ]
        },
        {
            "paperId": "d7bc3fecc6372c9b3fe2d0581167f00caaf05f36",
            "title": "IterAlign: Iterative Constitutional Alignment of Large Language Models",
            "abstract": "With the rapid development of large language models (LLMs), aligning LLMs with human values and societal norms to ensure their reliability and safety has become crucial. Reinforcement learning with human feedback (RLHF) and Constitutional AI (CAI) have been proposed for LLM alignment. However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming. To overcome these drawbacks, we study constitution-based LLM alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM and automatically discovers new constitutions using a stronger LLM. These constitutions are then used to guide self-correction of the base LLM. Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target the alignment gaps in the current LLM. Empirical results on several safety benchmark datasets and multiple base LLMs show that IterAlign successfully improves truthfulness, helpfulness, harmlessness and honesty, improving the LLM alignment by up to 13.5% in harmlessness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29963551",
                    "name": "Xiusi Chen"
                },
                {
                    "authorId": "2293646170",
                    "name": "Hongzhi Wen"
                },
                {
                    "authorId": "7529854",
                    "name": "Sreyashi Nag"
                },
                {
                    "authorId": "2276186819",
                    "name": "Chen Luo"
                },
                {
                    "authorId": "2275598113",
                    "name": "Qingyu Yin"
                },
                {
                    "authorId": "2257590787",
                    "name": "Ruirui Li"
                },
                {
                    "authorId": "2257858428",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2283212563",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "39a9beda08f4bf33d800fa6bf2e9fdde12d0a118",
            "title": "MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering",
            "abstract": "Recent advances in few-shot question answering (QA) mostly rely on the power of pre-trained large language models (LLMs) and fine-tuning in specific settings. Although the pre-training stage has already equipped LLMs with powerful reasoning capabilities, LLMs still need to be fine-tuned to adapt to specific domains to achieve the best results. In this paper, we propose to select the most informative data for fine-tuning, thereby improving the efficiency of the fine-tuning process with comparative or even better accuracy on the open-domain QA task. We present MinPrompt, a minimal data augmentation framework for open-domain QA based on an approximate graph algorithm and unsupervised question generation. We transform the raw text into a graph structure to build connections between different factual sentences, then apply graph algorithms to identify the minimal set of sentences needed to cover the most information in the raw text. We then generate QA pairs based on the identified sentence subset and train the model on the selected sentences to obtain the final model. Empirical results on several benchmark datasets and theoretical analysis show that MinPrompt is able to achieve comparable or better results than baselines with a high degree of efficiency, bringing consistent improvements in F-1 scores.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29963551",
                    "name": "Xiusi Chen"
                },
                {
                    "authorId": "3165179",
                    "name": "Jyun-Yu Jiang"
                },
                {
                    "authorId": "1702500",
                    "name": "Wei-Cheng Chang"
                },
                {
                    "authorId": "2256992918",
                    "name": "Cho-Jui Hsieh"
                },
                {
                    "authorId": "2257105171",
                    "name": "Hsiang-Fu Yu"
                },
                {
                    "authorId": "2283212563",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "429a4535bd8645ec2dd4923eca9051c5825a43c6",
            "title": "Chain-of-Factors Paper-Reviewer Matching",
            "abstract": "With the rapid increase in paper submissions to academic conferences, the need for automated and accurate paper-reviewer matching is more critical than ever. Previous efforts in this area have considered various factors to assess the relevance of a reviewer's expertise to a paper, such as the semantic similarity, shared topics, and citation connections between the paper and the reviewer's previous works. However, most of these studies focus on only one factor, resulting in an incomplete evaluation of the paper-reviewer relevance. To address this issue, we propose a unified model for paper-reviewer matching that jointly considers semantic, topic, and citation factors. To be specific, during training, we instruction-tune a contextualized language model shared across all factors to capture their commonalities and characteristics; during inference, we chain the three factors to enable step-by-step, coarse-to-fine search for qualified reviewers given a submission. Experiments on four datasets (one of which is newly contributed by us) spanning various fields such as machine learning, computer vision, information retrieval, and data mining consistently demonstrate the effectiveness of our proposed Chain-of-Factors model in comparison with state-of-the-art paper-reviewer matching methods and scientific pre-trained language models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261413719",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "2203364310",
                    "name": "Yan-Jun Shen"
                },
                {
                    "authorId": "29963551",
                    "name": "Xiusi Chen"
                },
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "2257136881",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "8d46f6b8da505566a809e34c3e60d39413ae9342",
            "title": "Gotta: Generative Few-shot Question Answering by Prompt-based Cloze Data Augmentation",
            "abstract": "Few-shot question answering (QA) aims at precisely discovering answers to a set of questions from context passages while only a few training samples are available. Although existing studies have made some progress and can usually achieve proper results, they suffer from understanding deep semantics for reasoning out the questions. In this paper, we develop Gotta, a Generative prOmpT-based daTa Augmentation framework to mitigate the challenge above. Inspired by the human reasoning process, we propose to integrate the cloze task to enhance few-shot QA learning. Following the recent success of prompt-tuning, we present the cloze task in the same format as the main QA task, allowing the model to learn both tasks seamlessly together to fully take advantage of the power of prompt-tuning. Extensive experiments on widely used benchmarks demonstrate that Gotta consistently outperforms competitive baselines, validating the effectiveness of our proposed prompt-tuning-based cloze task, which not only fine-tunes language models but also learns to guide reasoning in QA tasks. Further analysis shows that the prompt-based loss incorporates the auxiliary task better than the multi-task loss, highlighting the strength of prompt-tuning on the few-shot QA task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29963551",
                    "name": "Xiusi Chen"
                },
                {
                    "authorId": "49891156",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "3591598",
                    "name": "Jinliang Deng"
                },
                {
                    "authorId": "3165179",
                    "name": "Jyun-Yu Jiang"
                },
                {
                    "authorId": "40397893",
                    "name": "Wei Wang"
                }
            ]
        },
        {
            "paperId": "c06b76673ccca18e3123421e62458da4c3a3edcc",
            "title": "Language Models As Semantic Indexers",
            "abstract": "Semantic identifier (ID) is an important concept in information retrieval that aims to preserve the semantics of objects such as documents and items inside their IDs. Previous studies typically adopt a two-stage pipeline to learn semantic IDs by first procuring embeddings using off-the-shelf text encoders and then deriving IDs based on the embeddings. However, each step introduces potential information loss, and there is usually an inherent mismatch between the distribution of embeddings within the latent space produced by text encoders and the anticipated distribution required for semantic indexing. It is non-trivial to design a method that can learn the document's semantic representations and its hierarchical structure simultaneously, given that semantic IDs are discrete and sequentially structured, and the semantic supervision is deficient. In this paper, we introduce LMIndexer, a self-supervised framework to learn semantic IDs with a generative language model. We tackle the challenge of sequential discrete ID by introducing a semantic indexer capable of generating neural sequential discrete representations with progressive training and contrastive learning. In response to the semantic supervision deficiency, we propose to train the model with a self-supervised document reconstruction objective. We show the high quality of the learned IDs and demonstrate their effectiveness on three tasks including recommendation, product search, and document retrieval on five datasets from various domains. Code is available at https://github.com/PeterGriffinJin/LMIndexer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "2029235362",
                    "name": "Hansi Zeng"
                },
                {
                    "authorId": "2257458191",
                    "name": "Guoyin Wang"
                },
                {
                    "authorId": "29963551",
                    "name": "Xiusi Chen"
                },
                {
                    "authorId": "2257346748",
                    "name": "Tianxin Wei"
                },
                {
                    "authorId": "2257590787",
                    "name": "Ruirui Li"
                },
                {
                    "authorId": "2274037416",
                    "name": "Zhengyang Wang"
                },
                {
                    "authorId": "2257858428",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2314920176",
                    "name": "Yang Li"
                },
                {
                    "authorId": "2257363913",
                    "name": "Hanqing Lu"
                },
                {
                    "authorId": "2257362558",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "2257136881",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "2258564842",
                    "name": "Xianfeng Tang"
                }
            ]
        },
        {
            "paperId": "e35598d25849c983c30310d49c9647385d4c8867",
            "title": "Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers",
            "abstract": "Instead of relying on human-annotated training samples to build a classifier, weakly supervised scientific paper classification aims to classify papers only using category descriptions (e.g., category names, category-indicative keywords). Existing studies on weakly supervised paper classification are less concerned with two challenges: (1) Papers should be classified into not only coarse-grained research topics but also fine-grained themes, and potentially into multiple themes, given a large and fine-grained label space; and (2) full text should be utilized to complement the paper title and abstract for classification. Moreover, instead of viewing the entire paper as a long linear sequence, one should exploit the structural information such as citation links across papers and the hierarchy of sections and paragraphs in each paper. To tackle these challenges, in this study, we propose FUTEX, a framework that uses the cross-paper network structure and the in-paper hierarchy structure to classify full-text scientific papers under weak supervision. A network-aware contrastive fine-tuning module and a hierarchy-aware aggregation module are designed to leverage the two types of structural signals, respectively. Experiments on two benchmark datasets demonstrate that FUTEX significantly outperforms competitive baselines and is on par with fully supervised classifiers that use 1,000 to 60,000 ground-truth training samples.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49891156",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "2057050247",
                    "name": "Bowen Jin"
                },
                {
                    "authorId": "29963551",
                    "name": "Xiusi Chen"
                },
                {
                    "authorId": "2203364310",
                    "name": "Yan-Jun Shen"
                },
                {
                    "authorId": "48379289",
                    "name": "Yunyi Zhang"
                },
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "6f2e895c96f058ceed29894985cbaca6dccf4e55",
            "title": "Scalable Graph Representation Learning via Locality-Sensitive Hashing",
            "abstract": "A massive amount of research on graph representation learning has been carried out to learn dense features as graph embedding for information networks, thereby capturing the semantics in complex networks and benefiting a variety of downstream tasks. Most of the existing studies focus on structural properties, such as distances and neighborhood proximity between nodes. However, real-world information networks are dominated by the low-degree nodes because they are not only sparse but also subject to the Power law form. Due to the sparsity, proximity-based methods are incapable of deriving satisfactory representations for these tail nodes. To address this challenge, we propose a novel approach, Content-Preserving Locality-Sensitive Hashing~(CP-LSH), by incorporating the content information for representation learning. Specifically, we aim at preserving LSH-based content similarity between nodes to leverage the knowledge from popular nodes to long-tail nodes. We also propose a novel hashing trick to reduce the redundant space consumption so that CP-LSH is capable of tackling industry-scale data. Extensive offline experiments have been conducted on three large-scale public datasets. We also deploy CP-LSH to real-world recommendation systems in one of the largest e-commerce platforms for online experiments. Experimental results demonstrate that CP-LSH outperforms competitive baseline methods in node classification and link prediction tasks. Besides, the results of online experiments also indicate that CP-LSH is practical and robust for real-world production systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "29963551",
                    "name": "Xiusi Chen"
                },
                {
                    "authorId": "3165179",
                    "name": "Jyun-Yu Jiang"
                },
                {
                    "authorId": "40397893",
                    "name": "Wei Wang"
                }
            ]
        }
    ]
}