{
    "authorId": "1681705",
    "papers": [
        {
            "paperId": "459ce0d315837a6a8524a27e746756b47327e47a",
            "title": "ISOP+: Machine Learning-Assisted Inverse Stack-Up Optimization for Advanced Package Design",
            "abstract": "The future of computing requires heterogeneous integration, including the recent adoption of chiplet methodology, where high-speed cross-chip interconnects and packaging are critical for the overall system performance. As an example of advanced packaging, a high-density interconnect (HDI) printed circuit board (PCB) has been widely used in complex electronics ranging from cell phones to computing servers. A modern HDI PCB may have over 20 layers, each with its unique material properties and geometrical dimensions, i.e., stack-up, to meet various design constraints and performance requirements. Stack-up design is usually done manually in the industry, where experienced designers may devote many hours adjusting the physical dimensions and materials in order to meet the desired specifications. This process, however, is time-consuming, tedious, and suboptimal, largely depending on the designer\u2019s expertise. In this article, we propose to automate the stack-up design with a new framework, ISOP+, using machine learning (ML) for inverse stack-up optimization for advanced package design with adaptive weight adjustment and multilevel optimization. Given a target design specification, ISOP+ automatically searches for ideal stack-up design parameters while optimizing performance. A novel ML-assisted hyperparameter optimization method is developed to make the search efficient and reliable. Experimental results demonstrate that ISOP+ is better in figure-of-merit (FoM) than conventional simulated annealing and Bayesian optimization algorithms, with all our design targets met with a shorter runtime. We also compare our fully automated ISOP+ with expert designers in the industry and achieve very promising results, with orders of magnitude reduction of turn-around time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3292684",
                    "name": "Hyunsu Chae"
                },
                {
                    "authorId": "1471646837",
                    "name": "Keren Zhu"
                },
                {
                    "authorId": "9415007",
                    "name": "B. Mutnury"
                },
                {
                    "authorId": "2072914078",
                    "name": "D. Wallace"
                },
                {
                    "authorId": "32158610",
                    "name": "D. Winterberg"
                },
                {
                    "authorId": "2191906707",
                    "name": "Daniel de Araujo"
                },
                {
                    "authorId": "143714096",
                    "name": "J. Reddy"
                },
                {
                    "authorId": "1682471",
                    "name": "Adam R. Klivans"
                },
                {
                    "authorId": "1681705",
                    "name": "D. Pan"
                }
            ]
        },
        {
            "paperId": "7a452babf23ef011ffd78705e2163acd056aa472",
            "title": "PiPSim: A Behavior-Level Modeling Tool for CNN Processing-in-Pixel Accelerators",
            "abstract": "Convolutional neural networks (CNNs) have been gaining popularity in recent years, and researchers have designed specialized architectures to speed up the inference process. However, despite the promising potential of processing near-/in- sensor architectures actively explored in the visual Internet of Things, there is still a need to develop a behavior-level simulator to model performance and facilitate early design exploration. This article proposes a stand-alone simulation platform for processing-in-pixel (PiP) systems, namely, PiPSim. It offers a flexible interface and a wide range of design options for customizing the efficiency and accuracy of PiP-based accelerators using a hierarchical structure. Its organization spans from the device level, e.g., memory technology, upward to the circuit level, e.g., compute-add on architecture, and then to the algorithm level, e.g., DNN workloads. PiPSim realizes instruction-accurate evaluation of circuit-level performance metrics as well as learning accuracy at run-time. Compared to SPICE simulation, PiPSim achieves over 25 $000\\times $ speed-up with less than a 2.5% error rate on average. Furthermore, PiPSim can optimize the design and estimate the tradeoff relationships among different performance metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1810701",
                    "name": "A. Roohi"
                },
                {
                    "authorId": "3434403",
                    "name": "Sepehr Tabrizchi"
                },
                {
                    "authorId": "1475748342",
                    "name": "Mehrdad Morsali"
                },
                {
                    "authorId": "1681705",
                    "name": "D. Pan"
                },
                {
                    "authorId": "1824271",
                    "name": "Shaahin Angizi"
                }
            ]
        },
        {
            "paperId": "004abdbefd8b89f866728802f9c659ab2bdd3b63",
            "title": "M3ICRO: Machine Learning-Enabled Compact Photonic Tensor Core based on PRogrammable Multi-Operand Multimode Interference",
            "abstract": "Photonic computing shows promise for transformative advancements in machine learning (ML) acceleration, offering ultrafast speed, massive parallelism, and high energy efficiency. However, current photonic tensor core (PTC) designs based on standard optical components hinder scalability and compute density due to their large spatial footprint. To address this, we propose an ultracompact PTC using customized programmable multi-operand multimode interference (MOMMI) devices, named M3ICRO. The programmable MOMMI leverages the intrinsic light propagation principle, providing a single-device programmable matrix unit beyond the conventional computing paradigm of one multiply-accumulate operation per device. To overcome the optimization difficulty of customized devices that often requires time-consuming simulation, we apply ML for optics to predict the device behavior and enable differentiable optimization flow. We thoroughly investigate the reconfigurability and matrix expressivity of our customized PTC and introduce a novel block unfolding method to fully exploit the computing capabilities of a complex-valued PTC for near-universal real-valued linear transformations. Extensive evaluations demonstrate that M3ICRO achieves a 3.5\u20138.9\u00d7 smaller footprint, 1.6\u20134.4\u00d7 higher speed, 9.9\u201338.5\u00d7 higher compute density, 3.7\u201312\u00d7 higher system throughput, and superior noise robustness compared to state-of-the-art coherent PTC designs. It also outperforms electronic digital A100 graphics processing unit by 34.8\u2013403\u00d7 higher throughput while maintaining close-to-digital task accuracy across various ML benchmarks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2098601",
                    "name": "Jiaqi Gu"
                },
                {
                    "authorId": "2115315923",
                    "name": "Hanqing Zhu"
                },
                {
                    "authorId": "46239910",
                    "name": "Chenghao Feng"
                },
                {
                    "authorId": "1471708315",
                    "name": "Zixuan Jiang"
                },
                {
                    "authorId": "2148472272",
                    "name": "Ray T. Chen"
                },
                {
                    "authorId": "1681705",
                    "name": "D. Pan"
                }
            ]
        },
        {
            "paperId": "1a468046a70b88c95528ab546d78374597c82531",
            "title": "Lightening-Transformer: A Dynamically-Operated Optically-Interconnected Photonic Transformer Accelerator",
            "abstract": "The wide adoption and significant computing resource cost of attention-based transformers, e.g., Vision Transformers and large language models, have driven the demand for efficient hardware accelerators. While electronic accelerators have been commonly used, there is a growing interest in exploring photonics as an alternative technology due to its high energy efficiency and ultra-fast processing speed. Photonic accelerators have demonstrated promising results for convolutional neural networks (CNNs) workloads, which predominantly rely on weight-static linear operations. However, they encounter challenges when it comes to efficiently supporting attention-based Transformer architectures, raising questions about the applicability of photonics to advanced machine-learning tasks. The primary hurdle lies in their inefficiency in handling the unique workloads inherent to Transformers, i.e., dynamic and full-range tensor multiplication. In this work, we propose Lightening-Transformer, the first light-empowered, high-performance, and energy-efficient photonic Transformer accelerator. To overcome the fundamental limitation of existing photonic tensor core designs, we introduce a novel dynamically-operated photonic tensor core, DPTC, consisting of a crossbar array of interference-based optical vector dot-product engines, supporting highly parallel, dynamic, and full-range matrix multiplication. Furthermore, we design a dedicated accelerator that integrates our novel photonic computing cores with photonic interconnects for inter-core data broadcast, fully unleashing the power of optics. The comprehensive evaluation demonstrates that Lightening-Transformer achieves >2.6x energy and > 12 x latency reductions compared to prior photonic accelerators and delivers the lowest energy cost and 2 to 3 orders of magnitude lower energy-delay product compared to the electronic Transformer accelerator, all while maintaining digital-comparable accuracy. Our work highlights the immense potential of photonics for efficient hardware accelerators, particularly for advanced machine-learning workloads, such as Transformer-backboned large language models (LLM). Our implementation is available at https://github.com/zhuhanqing/Lightening-Transformer.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2115315923",
                    "name": "Hanqing Zhu"
                },
                {
                    "authorId": "2098601",
                    "name": "Jiaqi Gu"
                },
                {
                    "authorId": "48017300",
                    "name": "Hanrui Wang"
                },
                {
                    "authorId": "1471708315",
                    "name": "Zixuan Jiang"
                },
                {
                    "authorId": "11089894",
                    "name": "Zhekai Zhang"
                },
                {
                    "authorId": "2180849957",
                    "name": "R. Tang"
                },
                {
                    "authorId": "46239910",
                    "name": "Chenghao Feng"
                },
                {
                    "authorId": "2157605306",
                    "name": "Song Han"
                },
                {
                    "authorId": "2148472272",
                    "name": "Ray T. Chen"
                },
                {
                    "authorId": "1681705",
                    "name": "D. Pan"
                }
            ]
        },
        {
            "paperId": "1d953eba62be149ca1cd7c243ff2782f546cf4fc",
            "title": "Integrated multi-operand optical neurons for scalable and hardware-efficient deep learning",
            "abstract": "Abstract Optical neural networks (ONNs) are promising hardware platforms for next-generation neuromorphic computing due to their high parallelism, low latency, and low energy consumption. However, previous integrated photonic tensor cores (PTCs) consume numerous single-operand optical modulators for signal and weight encoding, leading to large area costs and high propagation loss to implement large tensor operations. This work proposes a scalable and efficient optical dot-product engine based on customized multi-operand photonic devices, namely multi-operand optical neuron (MOON). We experimentally demonstrate the utility of a MOON using a multi-operand-Mach\u2013Zehnder-interferometer (MOMZI) in image recognition tasks. Specifically, our MOMZI-based ONN achieves a measured accuracy of 85.89\u202f% in the street view house number (SVHN) recognition dataset with 4-bit voltage control precision. Furthermore, our performance analysis reveals that a 128 \u00d7 128 MOMZI-based PTCs outperform their counterparts based on single-operand MZIs by one to two order-of-magnitudes in propagation loss, optical delay, and total device footprint, with comparable matrix expressivity.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "46239910",
                    "name": "Chenghao Feng"
                },
                {
                    "authorId": "2216587442",
                    "name": "Jiaqi Gu"
                },
                {
                    "authorId": "2115315923",
                    "name": "Hanqing Zhu"
                },
                {
                    "authorId": "2180849957",
                    "name": "R. Tang"
                },
                {
                    "authorId": "2137199956",
                    "name": "Shupeng Ning"
                },
                {
                    "authorId": "48544610",
                    "name": "M. Hlaing"
                },
                {
                    "authorId": "51176906",
                    "name": "J. Midkiff"
                },
                {
                    "authorId": "145353400",
                    "name": "Sourabh Jain"
                },
                {
                    "authorId": "1681705",
                    "name": "D. Pan"
                },
                {
                    "authorId": "2148472272",
                    "name": "Ray T. Chen"
                }
            ]
        },
        {
            "paperId": "2504bdb2b0efe61f0134110e0d8efe74baed8641",
            "title": "NormSoftmax: Normalizing the Input of Softmax to Accelerate and Stabilize Training",
            "abstract": "Softmax is a basic function that normalizes a vector to a probability distribution and is widely used in machine learning, most notably in cross-entropy loss function and dot product attention operations. However, the optimization of softmax-based models is sensitive to the input statistics change. We observe that the input of softmax changes significantly during the initial training stage, causing slow and unstable convergence when training the model from scratch. To remedy the optimization difficulty of softmax, we propose a simple yet effective substitution, named NormSoftmax, where the input vector is first normalized to unit variance and then fed to the standard softmax function. Similar to other existing normalization layers in machine learning models, NormSoftmax can stabilize and accelerate the training process, and also increase the robustness of the training procedure against hyperparameters. Experiments on Transformer-based models and convolutional neural networks validate that our proposed NormSoftmax is an effective plug-and-play module to stabilize and speed up the optimization of neural networks with cross-entropy loss or dot-product attention operations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1471708315",
                    "name": "Zixuan Jiang"
                },
                {
                    "authorId": "2098601",
                    "name": "Jiaqi Gu"
                },
                {
                    "authorId": "1681705",
                    "name": "D. Pan"
                }
            ]
        },
        {
            "paperId": "27a54f71033b9cc4a07a4a62eea42d91ef73ac86",
            "title": "Tutorial and Perspectives on MAGICAL: A Silicon-Proven Open-Source Analog IC Layout System",
            "abstract": "MAGICAL is an open-source system for analog and mixed-signal (AMS) circuit layout synthesis. Using custom place-and-route and constraint extraction algorithms, MAGICAL provides a fully-automated layout implementation flow. MAGICAL 1.0 has been proven in silicon with a 40-nm 1GS/s $\\Delta \\Sigma $ ADC. The source code has been released to enable broad usage. Recently, MAGICAL has also been extended to cover more circuit classes such as SAR-ADC. This tutorial/perspective paper describes the overall MAGICAL framework and algorithms. We also provide a tutorial on how to use and extend MAGICAL and discuss future research directions for AMS layout design automation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1471646837",
                    "name": "Keren Zhu"
                },
                {
                    "authorId": "2149050523",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "1471681696",
                    "name": "Mingjie Liu"
                },
                {
                    "authorId": "1681705",
                    "name": "D. Pan"
                }
            ]
        },
        {
            "paperId": "2d89ce31cac05f3652b5b27a253618151eed11f5",
            "title": "Hierarchical Analog and Mixed-Signal Circuit Placement Considering System Signal Flow",
            "abstract": "Placement is a critical step in layout automation for analog and mixed-signal (AMS)-integrated circuits (ICs). It determines the proximity of devices and influences the wiring topology, significantly impacting post-routing parasitics and coupling capacitance. Existing analog placement techniques mainly focus on geometric constraints in analog building blocks. However, there yet lacks an effective way to consider the system-level signal flow for sensitive AMS circuits. Leveraging prior knowledge from schematics, we propose considering the critical signal paths in automatic AMS placement. A multilevel analog layout automation flow is further developed to reduce manual efforts in synthesizing hierarchical AMS circuits. Experimental results demonstrate the efficiency and effectiveness of our proposed framework with a 22.8% reduction in routed wirelength compared to state-of-the-art AMS placer and a 10-dB improvement in the signal-to-noise-and-distortion ratio (SNDR) for an ADC.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1471646837",
                    "name": "Keren Zhu"
                },
                {
                    "authorId": "2149050523",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "1471681696",
                    "name": "Mingjie Liu"
                },
                {
                    "authorId": "1681705",
                    "name": "D. Pan"
                }
            ]
        },
        {
            "paperId": "590b4c7d0b5a8ea1ebd7100f94912809973df238",
            "title": "ISOP: Machine Learning-Assisted Inverse Stack-Up Optimization for Advanced Package Design",
            "abstract": "Future computing calls for heterogeneous integration, e.g., the recent adoption of the chiplet methodology. However, high-speed cross-chip interconnects and packaging shall be critical for the overall system performance. As an example of advanced packaging, a high-density interconnect (HDI) printed circuit board (PCB) has been widely used in complex electronics from cell phones to computing servers. A modern HDI PCB may have over 20 layers, each with its unique material properties and geometrical dimensions, i.e., stack-up, to meet various design constraints and performance optimizations. However, stack-up design is usually done manually in the industry, where experienced designers may devote many hours to adjusting the physical dimensions and materials to meet the desired specifications. This process, however, is time-consuming, tedious, and sub-optimal, largely depending on the designer's expertise. In this paper, we propose to automate the stack-up design with a new framework, ISOP, using machine learning for inverse stack-up optimization for advanced package design. Given a target design specification, ISOP automatically searches for ideal stack-up design parameters while optimizing performance. We develop a novel machine learning-assisted hyper-parameter optimization method to make the search efficient and reliable. Experimental results demonstrate that ISOP is better in figure-of-merit (FoM) than conventional simulated annealing and Bayesian optimization algorithms, with all our design targets met with a shorter runtime. We also compare our fully-automated ISOP with expert designers in the industry and achieve very promising results, with orders of magnitude reduction of turn-around time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3292684",
                    "name": "Hyunsu Chae"
                },
                {
                    "authorId": "9415007",
                    "name": "B. Mutnury"
                },
                {
                    "authorId": "1471646837",
                    "name": "Keren Zhu"
                },
                {
                    "authorId": "2072914078",
                    "name": "D. Wallace"
                },
                {
                    "authorId": "32158610",
                    "name": "D. Winterberg"
                },
                {
                    "authorId": "39389049",
                    "name": "D. D. Araujo"
                },
                {
                    "authorId": "143714096",
                    "name": "J. Reddy"
                },
                {
                    "authorId": "1682471",
                    "name": "Adam R. Klivans"
                },
                {
                    "authorId": "1681705",
                    "name": "D. Pan"
                }
            ]
        },
        {
            "paperId": "5d54fb362e725844e9f20b86477cfac880d8a3d4",
            "title": "An In-Memory-Computing Charge-Domain Ternary CNN Classifier",
            "abstract": "The article presents a charge-domain computing ternary neural network (TNN) classifier with a complete four-layer neural network (NN) on a chip. The proposed ternary network provides 1.5-b resolution (0/+1/\u22121) for weights and activations, leading to 3.9\u00d7 fewer operations (OPs) per inference than binary neural network (BNN) for the same Modified National Institute of Standards and Technology (MNIST) accuracy. The 1.5-b multiply-and-accumulate (MAC) is implemented by <inline-formula> <tex-math notation=\"LaTeX\">$V_{\\text {CM}}$ </tex-math></inline-formula>-based capacitor switching scheme, which inherently benefits from the reduced signal swing on the capacitive digital-to-analog converter (CDAC). Also, the <inline-formula> <tex-math notation=\"LaTeX\">$V_{\\text {CM}}$ </tex-math></inline-formula>-based MAC introduces sparsity during training, resulting in a lower switching rate. The prototype is fabricated in a 40-nm LP CMOS process with an active area of 0.98 mm2, operates at 549 frames/s (FPS), and consumes 96 <inline-formula> <tex-math notation=\"LaTeX\">$\\mu \\text{W}$ </tex-math></inline-formula>. With all OPs on the chip, it achieves 97.1% MNIST accuracy with 0.18 <inline-formula> <tex-math notation=\"LaTeX\">$\\mu \\text{J}$ </tex-math></inline-formula> per classification, which is the smallest to our knowledge for comparable MNIST classification accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "80480795",
                    "name": "Xiangxing Yang"
                },
                {
                    "authorId": "1471646837",
                    "name": "Keren Zhu"
                },
                {
                    "authorId": "2109653240",
                    "name": "Xiyuan Tang"
                },
                {
                    "authorId": "150237437",
                    "name": "Meizhi Wang"
                },
                {
                    "authorId": "1637900071",
                    "name": "Mingtao Zhan"
                },
                {
                    "authorId": "3400728",
                    "name": "N. Lu"
                },
                {
                    "authorId": "38943938",
                    "name": "J. Kulkarni"
                },
                {
                    "authorId": "1681705",
                    "name": "D. Pan"
                },
                {
                    "authorId": "2442306",
                    "name": "Yongpan Liu"
                },
                {
                    "authorId": "48000611",
                    "name": "Nan Sun"
                }
            ]
        }
    ]
}