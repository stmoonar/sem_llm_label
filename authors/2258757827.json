{
    "authorId": "2258757827",
    "papers": [
        {
            "paperId": "47d69cf2a9d8acaefab20b70d70a6df9e8011a0a",
            "title": "Efficient Link Prediction via GNN Layers Induced by Negative Sampling",
            "abstract": "Graph neural networks (GNNs) for link prediction can loosely be divided into two broad categories. First, \\emph{node-wise} architectures pre-compute individual embeddings for each node that are later combined by a simple decoder to make predictions. While extremely efficient at inference time (since node embeddings are only computed once and repeatedly reused), model expressiveness is limited such that isomorphic nodes contributing to candidate edges may not be distinguishable, compromising accuracy. In contrast, \\emph{edge-wise} methods rely on the formation of edge-specific subgraph embeddings to enrich the representation of pair-wise relationships, disambiguating isomorphic nodes to improve accuracy, but with the cost of increased model complexity. To better navigate this trade-off, we propose a novel GNN architecture whereby the \\emph{forward pass} explicitly depends on \\emph{both} positive (as is typical) and negative (unique to our approach) edges to inform more flexible, yet still cheap node-wise embeddings. This is achieved by recasting the embeddings themselves as minimizers of a forward-pass-specific energy function (distinct from the actual training loss) that favors separation of positive and negative samples. As demonstrated by extensive empirical evaluations, the resulting architecture retains the inference speed of node-wise models, while producing competitive accuracy with edge-wise alternatives.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2258757827",
                    "name": "Yuxin Wang"
                },
                {
                    "authorId": "2197411811",
                    "name": "Xiannian Hu"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "1790227",
                    "name": "Xuanjing Huang"
                },
                {
                    "authorId": "2256661980",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2256635267",
                    "name": "David Wipf"
                }
            ]
        },
        {
            "paperId": "bf195fed7592cc364abb7352ad0e41c175031305",
            "title": "Graph Structure Learning via Lottery Hypothesis at Scale",
            "abstract": "Graph Neural Networks (GNNs) are commonly applied to analyze real-world graph-structured data. However, GNNs are sensitive to the given graph structure, which cast importance on graph structure learning to find optimal graph structures and representations. Previous methods have been restricted from large graphs due to high computational complexity. Lottery ticket hypothesis suggests that there exists a subnetwork that has comparable or better performance with proto-networks, which has been transferred to suit for pruning GNNs recently. There are few studies that address lottery ticket hypothesis\u2019s performance on defense in graphs. In this paper, we propose a scalable graph structure learning method leveraging lottery (ticket) hypothesis : GSL-LH. Our experiments show that GSL-LH can outperform its backbone model without attack and show better robustness against attack, achieving state-of-the-art performances in regular-size graphs compared to other graph structure learning methods without feature augmentation. In large graphs, GSL-LH can have comparable results with state-of-the-art defense methods other than graph structure learning, while bringing some insights into explanation of robustness. 1 2 3",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258757827",
                    "name": "Yuxin Wang"
                },
                {
                    "authorId": "2197411811",
                    "name": "Xiannian Hu"
                },
                {
                    "authorId": "2301074845",
                    "name": "Jiaqing Xie"
                },
                {
                    "authorId": "2155273086",
                    "name": "Zhangyue Yin"
                },
                {
                    "authorId": "2118117212",
                    "name": "Yunhua Zhou"
                },
                {
                    "authorId": "2282972251",
                    "name": "Xipeng Qiu"
                },
                {
                    "authorId": "2284750473",
                    "name": "Xuanjing Huang"
                }
            ]
        },
        {
            "paperId": "54841b5f8495bb76a5e8210805d25ba25606b130",
            "title": "An Open-World Lottery Ticket for Out-of-Domain Intent Classification",
            "abstract": "Most existing methods of Out-of-Domain (OOD) intent classi\ufb01cation, which rely on extensive auxiliary OOD corpora or speci\ufb01c training paradigms, are underdeveloped in the underlying principle that the models should have differentiated con\ufb01dence in In-and Out-of-domain intent. In this work, we demonstrate that calibrated subnetworks can be un-covered by pruning the (poor-calibrated) over-parameterized model. Calibrated con\ufb01dence provided by the subnetwork can better distinguish In-and Out-of-domain. Furthermore, we theoretically bring new insights into why temperature scaling can differentiate In-and Out-of-Domain intent and empirically extend the Lottery Ticket Hypothesis to the open-world setting. Extensive experiments on three real-world datasets demonstrate our approach can establish consistent improvements compared with a suite of competitive baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118117212",
                    "name": "Yunhua Zhou"
                },
                {
                    "authorId": "2129354313",
                    "name": "Peiju Liu"
                },
                {
                    "authorId": "2258757827",
                    "name": "Yuxin Wang"
                },
                {
                    "authorId": "2323828625",
                    "name": "Xipeng Qiu"
                }
            ]
        }
    ]
}