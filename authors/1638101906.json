{
    "authorId": "1638101906",
    "papers": [
        {
            "paperId": "8c2cf43026879adf5d0faf0ab041fe61a63c73d5",
            "title": "Look before You Leap: Dual Logical Verification for Knowledge-based Visual Question Generation",
            "abstract": "Knowledge-based Visual Question Generation aims to generate visual questions with outside knowledge other than the image. Existing approaches are answer-aware, which incorporate answers into the question-generation process. However, these methods just focus on leveraging the semantics of inputs to propose questions, ignoring the logical coherence among generated questions (Q), images (V), answers (A), and corresponding acquired outside knowledge (K). It results in generating many non-expected questions with low quality, lacking insight and diversity, and some of them are even without any corresponding answer. To address this issue, we inject logical verification into the processes of knowledge acquisition and question generation, which is defined as LV\u02c62-Net. Through checking the logical structure among V, A, K, ground-truth and generated Q twice in the whole KB-VQG procedure, LV\u02c62-Net can propose diverse and insightful knowledge-based visual questions. And experimental results on two commonly used datasets demonstrate the superiority of LV\u02c62-Net. Our code will be released to the public soon.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2162443483",
                    "name": "Xumeng Liu"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2240858056",
                    "name": "Xubo Liu"
                },
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "50882212",
                    "name": "Shenglong Yu"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "221e5e14f14537bb65efec79f60aad5010834a66",
            "title": "AoM: Detecting Aspect-oriented Information for Multimodal Aspect-Based Sentiment Analysis",
            "abstract": "Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects from text-image pairs and recognize their sentiments. Existing methods make great efforts to align the whole image to corresponding aspects. However, different regions of the image may relate to different aspects in the same sentence, and coarsely establishing image-aspect alignment will introduce noise to aspect-based sentiment analysis (i.e., visual noise). Besides, the sentiment of a specific aspect can also be interfered by descriptions of other aspects (i.e., textual noise). Considering the aforementioned noises, this paper proposes an Aspect-oriented Method (AoM) to detect aspect-relevant semantic and sentiment information. Specifically, an aspect-aware attention module is designed to simultaneously select textual tokens and image blocks that are semantically related to the aspects. To accurately aggregate sentiment information, we explicitly introduce sentiment embedding into AoM, and use a graph convolutional network to model the vision-text and text-text interaction. Extensive experiments demonstrate the superiority of AoM to existing methods. The source code is publicly released at https://github.com/SilyRab/AoM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219327022",
                    "name": "Ru Zhou"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "2162443483",
                    "name": "Xumeng Liu"
                },
                {
                    "authorId": "50882212",
                    "name": "Shenglong Yu"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "f49eb198153c0aa1b33c88351fcaca37d023ca1f",
            "title": "Licon: A Diverse, Controllable and Challenging Linguistic Concept Learning Benchmark",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50882212",
                    "name": "Shenglong Yu"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "2273631986",
                    "name": "Zhengkun Zhang"
                },
                {
                    "authorId": "2219327022",
                    "name": "Ru Zhou"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "1a5a9d7fdfed0e5d3f5c1b00d6356361b8c57941",
            "title": "HyperPELT: Unified Parameter-Efficient Language Model Tuning for Both Language and Vision-and-Language Tasks",
            "abstract": "The workflow of pretraining and fine-tuning has emerged as a popular paradigm for solving various NLP and V&L (Vision-and-Language) downstream tasks. With the capacity of pretrained models growing rapidly, how to perform parameter-efficient fine-tuning has become fairly important for quick transfer learning and deployment. In this paper, we design a novel unified parameter-efficient transfer learning framework that works effectively on both pure language and V&L tasks. In particular, we use a shared hypernetwork that takes trainable hyper-embeddings as input, and outputs weights for fine-tuning different small modules in a pretrained language model, such as tuning the parameters inserted into multi-head attention blocks (i.e., prefix-tuning) and feed-forward blocks (i.e., adapter-tuning). We define a set of embeddings (e.g., layer, block, task and visual embeddings) as the key components to calculate hyper-embeddings, which thus can support both pure language and V&L tasks. Our proposed framework adds fewer trainable parameters in multi-task learning while achieving superior performances and transfer ability compared to state-of-the-art methods. Empirical results on the GLUE benchmark and multiple V&L tasks confirm the effectiveness of our framework on both textual and visual modalities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2148904527",
                    "name": "Zhengkun Zhang"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "1840054391",
                    "name": "Xiaojun Meng"
                },
                {
                    "authorId": "2136912252",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2125066358",
                    "name": "Yadao Wang"
                },
                {
                    "authorId": "2110310493",
                    "name": "Xin Jiang"
                },
                {
                    "authorId": "30738758",
                    "name": "Qun Liu"
                },
                {
                    "authorId": "2149231521",
                    "name": "Zhenglu Yang"
                }
            ]
        },
        {
            "paperId": "b14820cd3c051794c8db445dc21682cb6c198f8e",
            "title": "A Span-based Multimodal Variational Autoencoder for Semi-supervised Multimodal Named Entity Recognition",
            "abstract": "Multimodal named entity recognition (MNER) on social media is a challenging task which aims to extract named entities in free text and incorporate images to classify them into user-defined types. However, the annotation for named entities on social media demands a mount of human efforts. The existing semi-supervised named entity recognition methods focus on the text modal and are utilized to reduce labeling costs in traditional NER. However, the previous methods are not efficient for semi-supervised MNER. Because the MNER task is defined to combine the text information with image one and needs to consider the mismatch between the posted text and image. To fuse the text and image features for MNER effectively under semi-supervised setting, we propose a novel span-based multimodal variational autoencoder (SMVAE) model for semi-supervised MNER. The proposed method exploits modal-specific VAEs to model text and image latent features, and utilizes product-of-experts to acquire multimodal features. In our approach, the implicit relations between labels and multimodal features are modeled by multimodal VAE. Thus, the useful information of unlabeled data can be exploited in our method under semi-supervised setting. Experimental results on two benchmark datasets demonstrate that our approach not only outperforms baselines under supervised setting, but also improves MNER performance with less labeled data than existing semi-supervised methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "48379623",
                    "name": "Y. Zhang"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "2155786293",
                    "name": "Hongbin Wang"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "f3dda17911dd66c4e4a93fef8f24e13a3d0e1d30",
            "title": "Learning Disentangled Representation for Multimodal Cross-Domain Sentiment Analysis",
            "abstract": "Multimodal cross-domain sentiment analysis aims at transferring domain-invariant sentiment information across datasets to address the insufficiency of labeled data. Existing adaptation methods achieve well performance by remitting the discrepancies in characteristics of multiple modalities. However, the expressive styles of different datasets also contain domain-specific information, which hinders the adaptation performance. In this article, we propose a disentangled sentiment representation adversarial network (DiSRAN) to reduce the domain shift of expressive styles for multimodal cross-domain sentiment analysis. Specifically, we first align the multiple modalities and obtain the joint representation through a cross-modality attention layer. Then, we disentangle sentiment information from the multimodal joint representation that contains domain-specific expressive style by adversarial training. The obtained sentiment representation is domain-invariant, which can better facilitate the sentiment information transfer between different domains. Experimental results on two multimodal cross-domain sentiment analysis tasks demonstrate that the proposed method performs favorably against state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108544628",
                    "name": "Yuhao Zhang"
                },
                {
                    "authorId": "2204294131",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "3127329",
                    "name": "Xiangrui Cai"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "12c3a0f9e8a8a904544b8bb6c395d006d79aab5c",
            "title": "Re-Attention for Visual Question Answering",
            "abstract": "A simultaneous understanding of questions and images is crucial in Visual Question Answering (VQA). While the existing models have achieved satisfactory performance by associating questions with key objects in images, the answers also contain rich information that can be used to describe the visual contents in images. In this paper, we propose a re-attention framework to utilize the information in answers for the VQA task. The framework first learns the initial attention weights for the objects by calculating the similarity of each word-object pair in the feature space. Then, the visual attention map is reconstructed by re-attending the objects in images based on the answer. Through keeping the initial visual attention map and the reconstructed one to be consistent, the learned visual attention map can be corrected by the answer information. Besides, we introduce a gate mechanism to automatically control the contribution of re-attention to model training based on the entropy of the learned initial visual attention maps. We conduct experiments on three benchmark datasets, and the results demonstrate the proposed model performs favorably against state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "48379623",
                    "name": "Y. Zhang"
                },
                {
                    "authorId": "2242439004",
                    "name": "Jufeng Yang"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "3cc19d0e2df1af11a7a62da6533f70865a42ca11",
            "title": "MTAAL: Multi-Task Adversarial Active Learning for Medical Named Entity Recognition and Normalization",
            "abstract": "Automated medical named entity recognition and normalization are fundamental for constructing knowledge graphs and building QA systems. When it comes to medical text, the annotation demands a foundation of expertise and professionalism. Existing methods utilize active learning to reduce costs in corpus annotation, as well as the multi-task learning strategy to model the correlations between different tasks. However, existing models do not take task-specific features for different tasks and diversity of query samples into account. To address these limitations, this paper proposes a multi-task adversarial active learning model for medical named entity recognition and normalization. In our model, the adversarial learning keeps the effectiveness of multi-task learning module and active learning module. The task discriminator eliminates the influence of irregular task-specific features. And the diversity discriminator exploits the heterogeneity between samples to meet the diversity constraint. The empirical results on two medical benchmarks demonstrate the effectiveness of our model against the existing methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "3127329",
                    "name": "Xiangrui Cai"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "68ab14181e685e2e45ede5ef515cfe89162fb116",
            "title": "Multimodal Event-Aware Network for Sentiment Analysis in Tourism",
            "abstract": "Considering the application of a sentiment analysis in decision-making and personalized advertising, we adopt it in tourism. Specifically, we perform a sentiment analysis on the posted Weibos about the passengers\u2019 experience in civil aviation travel. Different travel events could influence passengers\u2019 sentiment, e.g., flight delay may cause negative sentiment. Inspired by this observation, we propose a novel multimodal event-aware network to analyze sentiment from Weibos that contain multiple modalities, i.e., text and images. We first extract features from each modality and, then, model the cross-modal associations to obtain more discriminative representations, based on which we simultaneously perceive the event and sentiment in a multitask framework. Extensive experiments demonstrate that the proposed method outperforms the existing state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118354856",
                    "name": "Lijuan Wang"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "51116041",
                    "name": "Xingxu Yao"
                },
                {
                    "authorId": "2119595262",
                    "name": "Yuxiang Zhang"
                },
                {
                    "authorId": "1755872",
                    "name": "Jufeng Yang"
                }
            ]
        },
        {
            "paperId": "b23a63bfecb42c5b790790f7d3e7b9ab03021b60",
            "title": "LD-MAN: Layout-Driven Multimodal Attention Network for Online News Sentiment Recognition",
            "abstract": "The prevailing use of both images and text to express opinions on the web leads to the need for multimodal sentiment recognition. Some commonly used social media data containing short text and few images, such as tweets and product reviews, have been well studied. However, it is still challenging to predict the readers\u2019 sentiment after reading online news articles, since news articles often have more complicated structures, e.g., longer text and more images. To address this problem, we propose a layout-driven multimodal attention network (LD-MAN) to recognize news sentiment in an end-to-end manner. Rather than modeling text and images individually, LD-MAN uses the layout of online news to align images with the corresponding text. Specifically, it exploits a set of distance-based coefficients to model the image locations and measure the contextual relationship between images and text. LD-MAN then learns the affective representations of the articles from the aligned text and images using a multimodal attention mechanism. Considering the lack of relevant datasets in this field, we collect two multimodal online news datasets, containing a total of 14,566 articles with 56,260 images and 251,202 words. Experimental results demonstrate that the proposed method performs favorably compared with state-of-the-art approaches. We will release all the codes, models and datasets to the community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "3127329",
                    "name": "Xiangrui Cai"
                },
                {
                    "authorId": "46643379",
                    "name": "L. Meng"
                },
                {
                    "authorId": "1755872",
                    "name": "Jufeng Yang"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        }
    ]
}