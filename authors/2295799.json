{
    "authorId": "2295799",
    "papers": [
        {
            "paperId": "e401616245c297d36f41073e53060c5f4325ee38",
            "title": "Pushing the Limits of Clingo's Incremental Grounding and Solving Capabilities in Practical Applications",
            "abstract": "Incremental techniques aim at making it possible to improve the performance of the grounding and solving processes by reusing the results of previous executions. Clingo supports both incremental grounding and incremental solving computations. In order to leverage incremental computations in clingo, the incremental fragments of ASP programs must satisfy certain safety-related conditions. In a number of problem domains and reasoning tasks, these conditions can be satisfied in a fairly straightforward way. However, we have observed that in certain practical applications, satisfying the conditions becomes more challenging, to the point that it is sometimes unclear how or even if it is possible to leverage incremental computations. In this paper, we report our findings, and ultimate success, with the use of incremental grounding and solving techniques in one of these challenging cases. We describe the domain, which is linked to a large practical application, discuss the challenges we faced in attempting to leverage incremental computations, and then describe the techniques that we developed, in particular at the level of methods for encoding the domain knowledge and of algorithms supporting the intended interleaving of grounding and solving. We believe that our findings may provide valuable information to practitioners facing similar challenges and ultimately increase the adoption of clingo\u2019s incremental capabilities for complex practical applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1729002",
                    "name": "M. Balduccini"
                },
                {
                    "authorId": "2285820",
                    "name": "M. Barborak"
                },
                {
                    "authorId": "2295799",
                    "name": "D. Ferrucci"
                }
            ]
        },
        {
            "paperId": "3ef92d3b3f8946b2101104060d10c653d5c13825",
            "title": "Action Languages and COVID-19: Lessons Learned",
            "abstract": "We recently conducted an exercise in which we evaluated the use of RAC and action languages to formalize policies related to covid19 In this paper, we summarize the most salient lessons we learned from this exercise We believe our findings are relevant not only to this specific domain, but also to policy formalization in general and possibly even to tasks beyond policy formalization \u00a9 2020 CEUR-WS All rights reserved",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1729002",
                    "name": "M. Balduccini"
                },
                {
                    "authorId": "2285820",
                    "name": "M. Barborak"
                },
                {
                    "authorId": "2295799",
                    "name": "D. Ferrucci"
                }
            ]
        },
        {
            "paperId": "6e399a6840c73013c389c2212bef5a87bdf2e996",
            "title": "Braid: Weaving Symbolic and Neural Knowledge into Coherent Logical Explanations",
            "abstract": "Traditional symbolic reasoning engines, while attractive for their precision and explicability, have a few major drawbacks: the use of brittle inference procedures that rely on exact matching (unification) of logical terms, an inability to deal with uncertainty, and the need for a precompiled rule-base of knowledge (the \u201cknowledge acquisition\u201d problem). To address these issues, we devise a novel logical reasoner called Braid, that supports probabilistic rules, and uses the notion of custom unification functions and dynamic rule generation to overcome the brittle matching and knowledge-gap problem prevalent in traditional reasoners. In this paper, we describe the reasoning algorithms used in Braid, and their implementation in a distributed task-based framework that builds proof/explanation graphs for an input query. We use a simple QA example from a children\u2019s story to motivate Braid\u2019s design and explain how the various components work together to produce a coherent logical explanation. Finally, we evaluate Braid on the ROC Story Cloze test and achieve close to state-of-the-art results while providing frame-based explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1973186",
                    "name": "Aditya Kalyanpur"
                },
                {
                    "authorId": "123516203",
                    "name": "Tom Breloff"
                },
                {
                    "authorId": "2295799",
                    "name": "D. Ferrucci"
                }
            ]
        },
        {
            "paperId": "a74b1bafe15ce4d212731887bf934fd9fcb1fa09",
            "title": "Braid: Weaving Symbolic and Statistical Knowledge into Coherent Logical Explanations",
            "abstract": "Traditional symbolic reasoning engines, while attractive for their precision and explicability, have a few major drawbacks: the use of brittle inference procedures that rely on exact matching/uni\ufb01cation of logical terms, an inability to deal with uncertainty, and the need for a pre-compiled rule-base of knowledge (the \u201cknowl-edge acquisition\u201d problem). These issues are particularly severe for the Natural Language Understanding (NLU) task, where we often use implicit background knowledge to understand and reason about text, resort to imperfect/fuzzy alignment of concepts and relations during reasoning, and constantly deal with ambiguity in representations. To address these issues, we devise a novel FOL-based reasoner, called Braid, that supports probabilistic rules, and uses the notion of custom uni\ufb01cation functions and dynamic rule generation to overcome the brittle matching and knowledge-gap problem prevalent in traditional reasoners. In this paper, we describe the reasoning algorithms used in Braid-BC (the backchaining component of Braid), and their implementation in a distributed task-based framework that builds proof/explanation graphs for an input query in a highly scalable manner. We use a simple QA example from a children\u2019s story to motivate Braid-BC\u2019s design and explain how the various components work together to produce a coherent logical explanation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1973186",
                    "name": "Aditya Kalyanpur"
                },
                {
                    "authorId": "123516203",
                    "name": "Tom Breloff"
                },
                {
                    "authorId": "2295799",
                    "name": "D. Ferrucci"
                },
                {
                    "authorId": "144071952",
                    "name": "Adam Lally"
                },
                {
                    "authorId": "2029236078",
                    "name": "John Jantos"
                }
            ]
        },
        {
            "paperId": "bc92eb614be9523b4f559f9a7a02b03cabf32529",
            "title": "SKATE: A Natural Language Interface for Encoding Structured Knowledge",
            "abstract": "In Natural Language (NL) applications, there is often a mismatch between what the NL interface is capable of interpreting and what a lay user knows how to express. This work describes a novel natural language interface that reduces this mismatch by refining natural language input through successive, automatically generated semi-structured templates. In this paper we describe how our approach, called SKATE, uses a neural semantic parser to parse NL input and suggest semi-structured templates, which are recursively filled to produce fully structured interpretations. We also show how SKATE integrates with a neural rule-generation model to interactively suggest and acquire commonsense knowledge. We provide a preliminary coverage analysis of SKATE for the task of story understanding, and then describe a current business use-case of the technology in a restricted domain: COVID-19 policy design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2959651",
                    "name": "C. McFate"
                },
                {
                    "authorId": "1973186",
                    "name": "Aditya Kalyanpur"
                },
                {
                    "authorId": "2295799",
                    "name": "D. Ferrucci"
                },
                {
                    "authorId": "2000179806",
                    "name": "Andrea Bradshaw"
                },
                {
                    "authorId": "2000179714",
                    "name": "Ariel Diertani"
                },
                {
                    "authorId": "2283931768",
                    "name": "David O. Melville"
                },
                {
                    "authorId": "32343030",
                    "name": "Lori Moon"
                }
            ]
        },
        {
            "paperId": "cbba07e754aaf2e364a4af42e18250e5debd8462",
            "title": "To Test Machine Comprehension, Start by Defining Comprehension",
            "abstract": "Many tasks aim to measure machine reading comprehension (MRC), often focusing on question types presumed to be difficult. Rarely, however, do task designers start by considering what systems should in fact comprehend. In this paper we make two key contributions. First, we argue that existing approaches do not adequately define comprehension; they are too unsystematic about what content is tested. Second, we present a detailed definition of comprehension\u2014a \u201cTemplate of Understanding\u201d\u2014for a widely useful class of texts, namely short narratives. We then conduct an experiment that strongly suggests existing systems are not up to the task of narrative understanding as we define it.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2076678",
                    "name": "Jesse Dunietz"
                },
                {
                    "authorId": "102524625",
                    "name": "Greg Burnham"
                },
                {
                    "authorId": "2528900",
                    "name": "Akash Bharadwaj"
                },
                {
                    "authorId": "1403252641",
                    "name": "Jennifer Chu-Carroll"
                },
                {
                    "authorId": "1702447",
                    "name": "Owen Rambow"
                },
                {
                    "authorId": "2295799",
                    "name": "D. Ferrucci"
                }
            ]
        },
        {
            "paperId": "37f7e7701ff4dfb7e7cdcf60c56c1693b1f6d1be",
            "title": "WatsonPaths: Scenario-Based Question Answering and Inference over Unstructured Information",
            "abstract": "We present WatsonPaths, a novel system that can answer scenario-based questions. These include medical questions that present a patient summary and ask for the most likely diagnosis or most appropriate treatment. WatsonPaths builds on the IBM Watson question answering system. WatsonPaths breaks down the input scenario into individual pieces of information, asks relevant subquestions of Watson to conclude new information, and represents these results in a graphical model. Probabilistic inference is performed over the graph to conclude the answer. On a set of medical test preparation questions, WatsonPaths shows a significant improvement in accuracy over multiple baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144071952",
                    "name": "Adam Lally"
                },
                {
                    "authorId": "48695762",
                    "name": "S. Bagchi"
                },
                {
                    "authorId": "2285820",
                    "name": "M. Barborak"
                },
                {
                    "authorId": "2065421106",
                    "name": "David W. Buchanan"
                },
                {
                    "authorId": "1684353",
                    "name": "Jennifer Chu-Carroll"
                },
                {
                    "authorId": "2295799",
                    "name": "D. Ferrucci"
                },
                {
                    "authorId": "143742133",
                    "name": "Michael R. Glass"
                },
                {
                    "authorId": "1973186",
                    "name": "Aditya Kalyanpur"
                },
                {
                    "authorId": "143801711",
                    "name": "E. Mueller"
                },
                {
                    "authorId": "145412011",
                    "name": "J. William Murdock"
                },
                {
                    "authorId": "145984521",
                    "name": "Siddharth Patwardhan"
                },
                {
                    "authorId": "30210546",
                    "name": "J. Prager"
                }
            ]
        },
        {
            "paperId": "765d0956e46846a33a1062749daede11ba71680f",
            "title": "Typing candidate answers using type coercion",
            "abstract": "Many questions explicitly indicate the type of answer required. One popular approach to answering those questions is to develop recognizers to identify instances of common answer types (e.g., countries, animals, and food) and consider only answers on those lists. Such a strategy is poorly suited to answering questions from the Jeopardy!\u2122 television quiz show. Jeopardy! questions have an extremely broad range of types of answers, and the most frequently occurring types cover only a small fraction of all answers. We present an alternative approach to dealing with answer types. We generate candidate answers without regard to type, and for each candidate, we employ a variety of sources and strategies to judge whether the candidate has the desired type. These sources and strategies provide a set of type coercion scores for each candidate answer. We use these scores to give preference to answers with more evidence of having the right type. Our question-answering system is significantly more accurate with type coercion than it is without type coercion; these components have a combined impact of nearly 5% on the accuracy of the IBM Watson\u2122 question-answering system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145412011",
                    "name": "J. William Murdock"
                },
                {
                    "authorId": "1973186",
                    "name": "Aditya Kalyanpur"
                },
                {
                    "authorId": "143778120",
                    "name": "Chris Welty"
                },
                {
                    "authorId": "48203512",
                    "name": "James Fan"
                },
                {
                    "authorId": "2295799",
                    "name": "D. Ferrucci"
                },
                {
                    "authorId": "2771137",
                    "name": "David Gondek"
                },
                {
                    "authorId": "47059333",
                    "name": "Lei Zhang"
                },
                {
                    "authorId": "35562751",
                    "name": "H. Kanayama"
                }
            ]
        },
        {
            "paperId": "b3c41d5b0ad862118e27c0b6f2317c9a7f50d175",
            "title": "Automatic knowledge extraction from documents",
            "abstract": "Access to a large amount of knowledge is critical for success at answering open-domain questions for DeepQA systems such as IBM Watson\u2122. Formal representation of knowledge has the advantage of being easy to reason with, but acquisition of structured knowledge in open domains from unstructured data is often difficult and expensive. Our central hypothesis is that shallow syntactic knowledge and its implied semantics can be easily acquired and can be used in many areas of a question-answering system. We take a two-stage approach to extract the syntactic knowledge and implied semantics. First, shallow knowledge from large collections of documents is automatically extracted. Second, additional semantics are inferred from aggregate statistics of the automatically extracted shallow knowledge. In this paper, we describe in detail what kind of shallow knowledge is extracted, how it is automatically done from a large corpus, and how additional semantics are inferred from aggregate statistics. We also briefly discuss the various ways extracted knowledge is used throughout the IBM DeepQA system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48203512",
                    "name": "James Fan"
                },
                {
                    "authorId": "1973186",
                    "name": "Aditya Kalyanpur"
                },
                {
                    "authorId": "2771137",
                    "name": "David Gondek"
                },
                {
                    "authorId": "2295799",
                    "name": "D. Ferrucci"
                }
            ]
        },
        {
            "paperId": "bde2d81eaf6086e664af87c04241d4ce4e1bd01b",
            "title": "Introduction to \"This is Watson\"",
            "abstract": "In 2007, IBM Research took on the grand challenge of building a computer system that could compete with champions at the game of Jeopardy!\u2122. In 2011, the open-domain question-answering (QA) system, dubbed Watson, beat the two highest ranked players in a nationally televised two-game Jeopardy! match. This paper provides a brief history of the events and ideas that positioned our team to take on the Jeopardy! challenge, build Watson, IBM Watson\u2122, and ultimately triumph. It describes both the nature of the QA challenge represented by Jeopardy! and our overarching technical approach. The main body of this paper provides a narrative of the DeepQA processing pipeline to introduce the articles in this special issue and put them in context of the overall system. Finally, this paper summarizes our main results, describing how the system, as a holistic combination of many diverse algorithmic techniques, performed at champion levels, and it briefly discusses the team's future research plans.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2295799",
                    "name": "D. Ferrucci"
                }
            ]
        }
    ]
}