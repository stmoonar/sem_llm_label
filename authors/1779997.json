{
    "authorId": "1779997",
    "papers": [
        {
            "paperId": "5881c7d349c303a1d0feafdcccfcaffbfeace1ea",
            "title": "HEALER: A Data Lake Architecture for Healthcare",
            "abstract": "With the growth of the Internet of Things and the rapid progress of social networks, everything appears to generate data. The ever-increasingnumberofconnecteddevicesisaccompaniedbyagrowthofthevolumeofdata, producedatanever-increasing rate, and this massive flow includes data types that are difficult to process using standard database techniques. One of the most critical scenarios is healthcare, whose activities need to store and manage a variety of data types \u2013 reports written in natural language, medical images, genomic data and waveforms of vital signs \u2013 which do not have a well-defined structure. In order to benefit from this large amount of complex data, Data Lakes have recently emerged as a solution to grant central storage and flexible analysis for all types of data. However, there is no Data Lake architecture that fits all the possible scenarios, since the architecture depends heavily on the application domain and, so far, there are no Data Lake architectures that support the specific needs of the healthcare domain. This work proposes HEALER: a Data Lake architecture that effectively performs data ingestion, data storage, and data access with the aim of providing a single central repository for efficient storage of different types of healthcare data. The architecture also enables the analysis and querying of the data, which can be loaded into the Data Lake regardless of their format and type. To verify the effectiveness of the architecture, a proof-of-concept of HEALER has been developed, that allows ingestion of various data, performs waveforms processing to make them more interpretable to researchers and analysts, grants access to the saved data and allows the analysis of natural language reports. Finally we studied the performance of the system in each of its main phases: ingestion, processing, data access and analysis. The results lead us to some important considerations to be taken into account when using and configuring the system components",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185847395",
                    "name": "Carlo Manco"
                },
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "2945086",
                    "name": "Enrico Barbierato"
                },
                {
                    "authorId": "1702227",
                    "name": "M. Gribaudo"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "96ff519428c517942072f07b6fb45e7d918b486d",
            "title": "Data Quality and Fairness: Rivals or Friends?",
            "abstract": "In the last decade, data-driven decision-making is considered one of the main drivers for organizational success. Within this approach, decisions are based on insights and patterns identified through data analysis. In this scenario, input data must be reliable to guarantee the accuracy of the results: they should be correct and complete but also unbiased, i.e., both Data Quality (DQ) and Fairness should be guaranteed. However, maximizing DQ and Fairness simultaneously is not trivial, since data quality improvement techniques can negatively affect Fairness and vice versa. Understanding and thoroughly analyzing this relationship between DQ and Fairness is therefore paramount, and is this paper\u2019s goal. The results of our experiments, based on a well-known biased dataset (the Adult Census Income) provided details about this trade-off and allowed us to draw some guidelines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "1723724",
                    "name": "C. Cappiello"
                },
                {
                    "authorId": "2239390808",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "2239381912",
                    "name": "Sergio Cuzzucoli"
                },
                {
                    "authorId": "2239390968",
                    "name": "Alessandro Dangelo"
                },
                {
                    "authorId": "2182555347",
                    "name": "Camilla Sancricca"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "03d87e347446a23a21bd0011d89d9069aae79076",
            "title": "Accounting for Bossy Users in Context-Aware Group Recommendations",
            "abstract": "Lots of activities, like watching a movie or going to the restaurant, are intrinsically group-based. To recommend such activities to groups, traditional single-user recommendation techniques are not appropriate and, as a consequence, over the years a number of group recommender systems have been developed. Recommending items to be enjoyed together by a group of people poses many ethical challenges: in fact, a system whose unique objective is to achieve the best recommendation accuracy might learn to disadvantage submissive users in favor of more aggressive ones. In this work we investigate the ethical challenges of context-aware group recommendations, in the general case of ephemeral groups (i.e., groups where the members might be together for the first time), using a method that can recommend also items that are new to the system. We show the goodness of our method on two real-world datasets. The first one is a very large dataset containing the personal and group choices regarding TV programs of 7,921 users w.r.t. sixteen contexts of viewing, while the second one gathers the musical preferences (both individual and in groups) of 280 real users w.r.t. two contexts of listening. Our extensive experiments show that our method always manages to obtain the highest recall while delivering ethical guarantees in line with the other fair group recommender systems tested.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25867417",
                    "name": "Davide Azzalini"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "2794140",
                    "name": "Emanuele Rabosio"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "b04aae9a1dac75f7e83f717d41bd5116e2c537ca",
            "title": "Functional Dependencies to Mitigate Data Bias",
            "abstract": "Technologies based on data are frequently adopted in many sensitive environments to build models that support important and life-changing decisions. As a result, for an application to be ethically reliable, it should be associated with tools to discover and mitigate bias in data, in order to avoid (possibly unintentional) unethical behaviors and the associated consequences. In this paper we propose a novel solution that, exploiting the notion of Functional Dependency and its variants - well-known data constraints - aims at enforcing fairness by discovering and solving discrimination in datasets. Our system first identifies the attributes of a dataset that encompass discrimination (e.g. gender, ethnicity or religion), generating a list of dependencies, then, based on this information, determines the smallest set of tuples that must be added or removed to mitigate such bias in the dataset. Experimental results on two real-world datasets demonstrated that our approach can greatly improve the ethical quality of data sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "5790290",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "b1119c6a7775c9c65bec45791e2668592039c565",
            "title": "SoCRATe: a Framework for Compensating Users Over Time with Limited Availability Recommendations",
            "abstract": "We present our preliminary ideas for developing SoCRATe, a framework and an online system dedicated to providing recommendations to users when items\u2019 availability is limited. SoCRATe is relevant to several real-world applications, among which movie and task recommendations. SoCRATe has several appealing features: it watches users as they consume recommendations and accounts for user feedback in refining recommendations in the next round, it implements loss compensation strategies to make up for sub-optimal recommendations, in terms of accuracy, when items have limited availability, and it decides when to re-generate recommendations on a need-based fashion. SoCRATe accommodates real users as well as simulated users to enable testing multiple recommendation choice models. To frame evaluation, SoCRATe introduces a new set of measures that capture recommendation accuracy over time as well as throughput and user satisfaction. All these features make SoCRATe unique and able to adapt recommendations to user preferences in a resource-limited setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25867417",
                    "name": "Davide Azzalini"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "5790290",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "b8c0a188b01f87c94a71bad13eb857b60f7b87e3",
            "title": "SiMBA: Systematic Clustering-Based Methodology to Support Built Environment Analysis",
            "abstract": "The general interest in sustainable development models has grown enormously over the last 50 years, and architecture and urban planning are certainly two areas in which research on the topic is most advanced. At the same time, the contribution of computer science for a systematic analysis of the territory, both from a morphological point of view and as regards performance, seems to have been underestimated in today\u2019s research. In this context, our research aims to joining the two - until now separate - worlds of computer science, and architecture and urban planning. In particular, in this work we present SIMBA: Systematic clusterIng-based Methodology to support Built environment Analysis. SIMBA aims to enhance a consolidated analysis methodology, the Integrated Modification Methodology (IMM), through the integration of advanced analysis methods for the extraction of relevant patterns from built environment data. Using the city of Milan as a case study, we will demonstrate the possibility for SIMBA to be generalised to the analysis of any built environment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "113091919",
                    "name": "C. A. Biraghi"
                },
                {
                    "authorId": "2064630478",
                    "name": "E. Lenzi"
                },
                {
                    "authorId": "145228093",
                    "name": "M. Matera"
                },
                {
                    "authorId": "100500852",
                    "name": "M. Tadi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "c0922becac3bfe298ff632f1999bf060775bd9ea",
            "title": "FAIR-DB: A system to discover unfairness in datasets",
            "abstract": "In our everyday lives, technologies based on data play an increasingly important role. With the widespread adoption of decision making systems also in very sensitive environments, fairness has become a very important topic of discussion within the data science community. In this context, it is crucial to ensure that the data on which we base these decisions, are fair, and do not reflect historical biases. In this demo, we propose FAIR-DB (FunctionAl dependencIes to discoveR Data Bias), a system that exploiting the notion of Functional Dependency, a particular type of constraint on the data, can discover unethical behaviours in a dataset. The proposed solution is implemented as a web-based application, that, given an input dataset, generates such dependencies, walks the user trough their analysis, and finally provides many insights about bias present in the data. Our tool uses a novel metric to evaluate the unfairness present in datasets, identifies the attributes that encompass discrimination (e.g. ethnicity, sex or religion), and provides very precise information about the groups treated unequally. We also provide a detailed description of the system architecture and present a demonstration scenario, based on a real-world dataset frequently used in the field of computer ethics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "5790290",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "c9c84ab6b1bb88cd3368092ea0feadf263c47d8a",
            "title": "E-FAIR-DB: Functional Dependencies to Discover Data Bias and Enhance Data Equity",
            "abstract": "Decisions based on algorithms and systems generated from data have become essential tools that pervade all aspects of our daily lives; for these advances to be reliable, the results should be accurate but should also respect all the facets of data equity [11]. In this context, the concepts of Fairness and Diversity have become relevant topics of discussion within the field of Data Science Ethics and, in general, in Data Science. Although data equity is desirable, reconciling this property with accurate decision-making is a critical tradeoff, because applying a repair procedure to restore equity might modify the original data in such a way that the final decision is inaccurate w.r.t. the ultimate objective of the analysis. In this work, we propose E-FAIR-DB, a novel solution that, exploiting the notion of Functional Dependency\u2014a type of data constraint\u2014aims at restoring data equity by discovering and solving discrimination in datasets. The proposed solution is implemented as a pipeline that, first, mines functional dependencies to detect and evaluate fairness and diversity in the input dataset, and then, based on these understandings and on the objective of the data analysis, mitigates data bias, minimizing the number of modifications. Our tool can identify, through the mined dependencies, the attributes of the database that encompass discrimination (e.g., gender, ethnicity, or religion); then, based on these dependencies, it determines the smallest amount of data that must be added and/or removed to mitigate such bias. We evaluate our proposal both through theoretical considerations and experiments on two real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "5790290",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "d902851ec94df388707d71003abd88045f881384",
            "title": "Editorial: Special Issue on Data Quality and Ethics",
            "abstract": "This editorial summarizes the content of the Special Issue on Data Quality and Ethics of the Journal of Data and Information Quality (JDIQ). The issue accepted submissions from June 1 to July 30, 2021.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "d9a956b36a465d18ff9edc13f7dfa1d40fb933c8",
            "title": "Personalized Context-Aware Recommender System for Travelers",
            "abstract": "Nowadays, traveling has become more convenient thanks to many recent technological advancements. However, the main problem is that, with non-customized offers, the risk for the travelers is to waste their time looking for the most appropriate one. Consequently, travelers need a system capable of understanding their contextual preferences for ranking travel offers accordingly. In this work, we propose The Hybrid Offer Ranker (THOR) as a possible solution: on the one hand, we employ various classification algorithms to learn the individuals\u2019 contextual preferences; on the other hand, to help new users the system has no information on (cold users), we employ unsupervised algorithms to identify clusters of users with similar preferences and build group preference models accordingly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2140106312",
                    "name": "Mahsa Shekari"
                },
                {
                    "authorId": "1885196164",
                    "name": "Alireza Javadian Sabet"
                },
                {
                    "authorId": null,
                    "name": "Chaofeng Guan"
                },
                {
                    "authorId": "49450068",
                    "name": "M. Rossi"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "e3b3cfd764bcb1f8c02c40ee241ba1fcff36724c",
            "title": "THOR: A Hybrid Recommender System for the Personalized Travel Experience",
            "abstract": "One of the travelers\u2019 main challenges is that they have to spend a great effort to find and choose the most desired travel offer(s) among a vast list of non-categorized and non-personalized items. Recommendation systems provide an effective way to solve the problem of information overload. In this work, we design and implement \u201cThe Hybrid Offer Ranker\u201d (THOR), a hybrid, personalized recommender system for the transportation domain. THOR assigns every traveler a unique contextual preference model built using solely their personal data, which makes the model sensitive to the user\u2019s choices. This model is used to rank travel offers presented to each user according to their personal preferences. We reduce the recommendation problem to one of binary classification that predicts the probability with which the traveler will buy each available travel offer. Travel offers are ranked according to the computed probabilities, hence to the user\u2019s personal preference model. Moreover, to tackle the cold start problem for new users, we apply clustering algorithms to identify groups of travelers with similar profiles and build a preference model for each group. To test the system\u2019s performance, we generate a dataset according to some carefully designed rules. The results of the experiments show that the THOR tool is capable of learning the contextual preferences of each traveler and ranks offers starting from those that have the higher probability of being selected.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2126507836",
                    "name": "Alireza Javadian Sabet"
                },
                {
                    "authorId": "2140106312",
                    "name": "Mahsa Shekari"
                },
                {
                    "authorId": null,
                    "name": "Chaofeng Guan"
                },
                {
                    "authorId": "49450068",
                    "name": "M. Rossi"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "08a7fa4cf4beeea652e5d5b6ed855a0dea9c9599",
            "title": "FAIR-DB: FunctionAl DependencIes to discoveR Data Bias",
            "abstract": "Computers and algorithms have become essential tools that per-vade all aspects of our daily lives; this technology is based on data and, for it to be reliable, we have to make sure that the data on which it is based on is fair and without bias. In this context, Fairness has become a relevant topic of discussion within the field of Data Science Ethics, and in general in Data Science. Today\u2019s applications should therefore be associated with tools to discover bias in data, in order to avoid (possibly unintentional) unethical behavior and consequences; as a result, technologies that accurately discover discrimination and bias in databases are of paramount importance. In this work we propose FAIR-DB (FunctionAl dependencIes to discoveR Data Bias), a novel solution to detect biases and discover discrimination in datasets, that exploits the notion of Functional Dependency, a particular type of constraint on the data. The proposed solution is implemented as a framework that focuses on the mining of such dependencies, also proposing some new metrics for evaluating the bias found in the input dataset. Our tool can identify the attributes of the database that encompass discrimination (e.g. gender, ethnicity or religion) and the ones that instead verify various fairness measures; moreover, based on special aspects of these metrics and the intrinsic nature of dependencies, the framework provides very precise information about the groups treated unequally, obtaining more insights regarding the bias present in dataset compared to other existing tools. Finally, our system also suggests possible future steps, by indicating the most appropriate (already existing) algorithms to correct the dataset on the basis of the computed results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "5790290",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "a98458fb47ca75647eb5ee7813c6850245d7cd3c",
            "title": "Extraction of Medical Concepts from Italian Natural Language Descriptions (Discussion Paper)",
            "abstract": "In this paper we present a Natural Language Processing (NLP) pipeline to automatically extract medical concepts from a free text written in a language other than English. To do so, we use common NLP techniques and the metathesaurus of Unified Medical Language System (UMLS). Specifically, our goal is to automatically extract ontological concepts representing which part of the human body is injured and what is the nature of the injury, given an Italian textual description of a work accident. We start by partitioning the text into tokens and assigning to each token its part-of-speech, and then use an appropriate tool to extract relevant concepts to be searched within UMLS. We tested our system on a public large repository containing textual descriptions of work accidents produced by INAIL. Experimental results confirm that our system is able to correctly extract relevant medical concepts from texts written in Italian.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48975370",
                    "name": "P. Agnello"
                },
                {
                    "authorId": "144940487",
                    "name": "S. Ansaldi"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "2084176008",
                    "name": "G. Gangemi"
                },
                {
                    "authorId": "150080368",
                    "name": "Davide Piantella"
                },
                {
                    "authorId": "2794140",
                    "name": "Emanuele Rabosio"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "c7f95999040771cb364bc30f5bd7b6488b3fde0b",
            "title": "RECKOn: a REal-world, Context-aware KnOwledge-based lab (Discussion Paper)",
            "abstract": "The RECKON project focuses on interconnection technologies and context-aware data-analytics techniques to improve safety in workplaces, with the ultimate objective of identifying and preventing dangerous situations before accidents occur. In RECKON, prevention is interpreted through the latest monitoring, diagnostics and prognostics techniques from a safety perspective, allowing to detect and use, even in real time, a large amount of data about the entire operational context. Using sensor networks, we are able to collect information that is used in two ways: (i) when a potentially dangerous situation is detected, the system raises an alarm to prevent an accident, and (ii) whenever an accident or a near-miss (i.e., a potential accident that was narrowly averted) occurs, the related useful information is stored in a case report automatically generated and later used to update the accident-prevention politics. This work briefly describes the operational framework of RECKON, along with its modules and their interaction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48975370",
                    "name": "P. Agnello"
                },
                {
                    "authorId": "144940487",
                    "name": "S. Ansaldi"
                },
                {
                    "authorId": "2064630478",
                    "name": "E. Lenzi"
                },
                {
                    "authorId": "2079163815",
                    "name": "Alessio Mongelluzzo"
                },
                {
                    "authorId": "150080368",
                    "name": "Davide Piantella"
                },
                {
                    "authorId": "1744102",
                    "name": "M. Roveri"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "2140087339",
                    "name": "Alessandra Scutti"
                },
                {
                    "authorId": "2140106312",
                    "name": "Mahsa Shekari"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "292039032dcd172a2346144daafef592758d1b80",
            "title": "Report on SEBD 2020",
            "abstract": "This paper reports on the 28th Italian Symposium on Advanced Database Systems (SEBD 2020), held online as a virtual conference from the 21st to the 24th of June 2020. The topics that were addressed in this edition of the conference were organized in the sessions: ontologies and data integration, anomaly detection and dependencies, text analysis and search, deep learning, noSQL data, trajectories and diffusion, health and medicine, context and ranking, social and knowledge graphs, multimedia content analysis, security issues, and data mining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1717075",
                    "name": "M. Agosti"
                },
                {
                    "authorId": "2413538",
                    "name": "Maurizio Atzori"
                },
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "b9dbe7610a2cd263e16984a608289d93b25dbfa6",
            "title": "Context Awareness in the Travel Companion of the Shift2Rail Initiative",
            "abstract": ". Providing personalized o\ufb00ers, and services in general, for the users of a system requires perceiving the context in which the users\u2019 preferences are rooted. In this work, we introduce the use of an already known model and methodology \u2013 based on the so-called Context Dimension Tree \u2013 along with a conceptual architecture to build a recommender system that o\ufb00ers personalized services for travelers. The research is performed in the frame of the Shift2Rail initiative as part of the Innovation Programme 4 of EU Horizon 2020.",
            "fieldsOfStudy": [
                "Sociology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1885196164",
                    "name": "Alireza Javadian Sabet"
                },
                {
                    "authorId": "49450068",
                    "name": "M. Rossi"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "27ea4b7b4f0e38d363aa7d3e8dc3c7c282bf0603",
            "title": "Ethical Dimensions for Data Quality",
            "abstract": "ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2019 Association for Computing Machinery. 1936-1955/2019/1-ART1 $15.00 https://doi.org/10.1145/3362121 ACM J. Data Inform. Quality, Vol. 1, No. 1, Article 1. Publication date: January 2019. 1:2 Donatella Firmani, Letizia Tanca, and Riccardo Torlone Transparency is the ability to interpret the information extraction process in order to verify which aspects of the data determine its results. In this context, transparency metrics can use the notions of (i) data provenance [19, 18], by measuring the amount of meta-data describing where the original data come from; (ii) explanation [15], by describing how a result has been obtained. Diversity is the degree to which different kinds of objects are represented in a dataset. Several metrics are proposed in [9]. Ensuring diversity at the beginning of the information extraction process may be useful for enforcing fairness at the end. The diversity dimension may conflict with established dimensions in the Trust cluster of [5], that prioritizes few high-reputation sources. Data Protection concerns the ways to secure data, algorithms and models against unauthorized access. Defining measures can be an elusive goal since, on the one hand, anonymized datasets that are secure in isolation can reveal sensible information when combined [1], and on the other hand, robust techniques such as \u03b5-differential privacy [10] can only describe the privacy impact of specific queries. Data protection is related to the well-established security dimension of [5]. 3 ETHICAL CHALLENGES IN THE INFORMATION EXTRACTION PROCESS We highlight some challenges of complying with the dimensions of the Ethics Cluster, throughout the three phases of the information extraction process mentioned in the Introduction. A. Source Selection. Data can typically come from multiple sources, and it is most desirable that each of these complies with the ethics dimensions described in the previous section. If sources do not comply with (some) dimension individually, we should consider that the really important requirement is that the data that are finally used for analysis or recommendations do. It is thus appropriate to consider ethics for multiple sources in combination, so that the bias towards a certain category in a single source can be eliminated by another source with opposite bias. While for the fairness, transparency and diversity dimensions this is clearly possible, for the privacy we can only act on the single data sources because adding more information can only lower the protection level, or, at most, leave it as it is. Ethics in source selection is tightly related to the transparency of the source, specifically for sources that are themselves aggregators. Information lineage is of paramount importance in this case and can be accomplished with the popular notion of provenance [18]; however, how to capture the most fine-grained type of provenance, namely data provenance, remains an open question [12]. A more general challenge is source meta-data extraction, especially for interpreting unstructured contents and thus their ethical implications. Finally, we note that also the data acquisition process plays a role, and developing inherently transparent and fair collection and extraction methods is an almost unstudied topic. B. Data Integration. Ensuring ethics in the selection step is not enough: even if the collected data satisfy the ethical requirements, not necessarily their integration does [1]. Data integration usually involves three main steps: (i) schema matching, i.e. the alignment of the schemata of the data sources (when present), (ii) identification of the items stored in different data sources that refer to the same entity (also called record linkage or entity resolution), and (iii) construction of an integrated database over the data sources, obtained by merging their contents (also called data fusion). Each step is prone to different ethical concerns, as discussed below. Schema Matching. Groups treated fairly in the sources can become overor under-represented as a consequence of the integration process, possibly causing, in the following steps, unfair decisions. Similar issues arise in connection with diversity. Entity Resolution. Integrating sources that, in isolation, protect identity (e.g. via anonymization) might generate a dataset that violates privacy: an instance of this is the so-called linkage attack [1]. We refer the reader to [21] for a survey of techniques and challenges of privacy-preserving entity resolution in the context of Big Data. ACM J. Data Inform. Quality, Vol. 1, No. 1, Article 1. Publication date: January 2019. Ethical Dimensions for DataQuality 1:3 Data Fusion. Data disclosure, i.e., violation of data protection, can happen also in the fusion step if privacy-preserving noise is accidentally removed by merging the data. Fusion can also affect fairness, when combining data coming from different sources leads to the exclusion of some groups. In all the above steps transparency is fundamental: we can check the fulfilment of the ethical dimensions only if we can (i) provide explanations of the intermediate results (ii) describe the provenance of the final data. Unfortunately, this can conflict with data protection since removing identity information can cause lack of transparency, which ultimately may lead to unfair outcomes. As source selection, also the integration process \u2013 especially the last two steps, where schema information is not present \u2013 can benefit from the existence of meta-data, allowing to infer contextual meanings for individual terms and phrases. Fair extraction of meta-data is an exciting topic, as stereotypes and prejudices can be often found into automatically derived word semantics. C. Knowledge Extraction. An information extraction process presents the user with data organized as to satisfy their information needs. Here we highlight some ethical challenges for a sample of the many possible information extractions operations. Search and Query. These are typical data selection tasks. Diversifying the results of information retrieval and recommendation systems has traditionally been used to minimize dissatisfaction of the average user [4]. However, since these search algorithms are employed also in critical tasks such as job candidate selection or for university admissions, diversity has also become a way to ensure the fairness of the selection process [9]. Interestingly, if integrated data are unfair and over-represent a certain category, diversity can lead to data exclusion of the same category. Aggregation. Many typical decision-support queries, such as GROUP BY queries, might yield biased result, e.g. trends appearing in different groups of data can disappear or even be reversedwhen these groups are combined, leading to incorrect insights. The work of [17] provides a framework for incorporating fairness in aggregated data based on independence tests, for specific aggregations. A future work is to detect bias in combined data with full-fledged query systems. Analytics. Data are typically analyzed by means of statistical, data mining and machine learning techniques, providing encouraging results in decision making, even in data management problems [14]. However, while we are able to understand statistics and data mining models, when using techniques such as deep learning we are still far from fully grasping how a model produces its output. Therefore, explaining systems has become an important new research area [16], related to the fairness and transparency of the training data as well as of the learning process. 4 RESEARCH DIRECTIONS In the spirit of the responsible data science initiatives towards a full-fledged data quality perspective on ethics (see, for instance, redasci.org and dataresponsibly.github.io), the key ingredient is shared responsibility. Like for any other engineering product, responsibility for data usage is shared by a contractor and a producer: only if the latter is able to provide a quality certification for the various ethical dimensions, the former can share the responsibility for improper usage. Similarly, producers should be aware of their responsibility when quality goes below the granted level. While such guarantees are available for many classical dimensions of quality, for instance timeliness, the same does not hold for most of the ethical dimensions. Privacy already has a well defined way for guaranteeing a privacy level by design: (i) in the knowledge extraction step, thanks to the notion of \u03b5-differential privacy [10], and (ii) in the integration step (see [21] for a survey). The so-called nutritional labels [13] mark a major step towards the idea of a quality certificate for fairness and diversity in the source selection and knowledge extraction steps, but how to preserve these properties throughout the process remains instead an open problem. Transparency is perhaps the hardest dimension to guarantee, and we believe that the well-known notion of provenance [12] ACM J. Data Inform. Quality, Vol. 1, No. 1, Article 1. Publication date: January 2019. 1:4 Donatella Firmani, Letizia Tanca, and Riccardo Torlone can provide a promising starting point. However, the rise of machine learning and deep learning techniques also for some data integration tasks [8] poses new and exciting challenges in tracking the way integration is achieved [22]. Summing up, recent literature provides a variety of methods for verifying/enforcing ethical dimensions. However, they typically apply to the very early (such as, collection) or very late steps (such as, analytics) of the information extraction process, but very few works study how to preserve ethics by design throughout the process. 5 RELATEDWORKS AND CONCLUDING REMARKS An early attempt to con",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "6aa0bc08a9d0257a5f04b8e42374c298ebeb862b",
            "title": "Tracking the Evolution of Financial Time Series Clusters",
            "abstract": "Nowadays, a huge amount of applications exist that natively adopt a data-streaming model to represent highly dynamic phenomena. A challenging application is constituted by data from the stock market, where the stock prices are naturally modeled as data streams that fluctuate very much and remain meaningful only for short amounts of time. In this paper we present a technique to track evolving clusters of financial time series, with the aim of constructing reliable models for this highly dynamic application. In our technique the clustering over a set of time series is iterated over time through sliding windows and, at each iteration, the differences between the current clustering and the previous one are studied to determine those changes that are \"significant\" with respect to the application. For example, in the financial domain, if a company that has belonged to the same cluster for a certain amount of time moves to another cluster, this may be a signal of a significant change in its economical or financial situation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25867417",
                    "name": "Davide Azzalini"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "2771866",
                    "name": "Mirjana Mazuran"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "ba521c6a5a45954813b09152b2855415b1cb1d1f",
            "title": "Data Processing: Reflections on Ethics",
            "abstract": "Ethics-related aspects are becoming prominent in data management, thus the current processes for searching, querying, or analyzing data should be designed is such a way as to take into account the social problems their outcomes could bring about. In this paper we provide reflections on the unavoidable ethical facets entailed by all the steps of the information life-cycle, including source selection, knowledge extraction, data integration and data analysis. Such reflections motivated us to organize the First International Workshop on Processing Information Ethically (PIE).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "d3555332477f46fbde20be6c8578a2300881af9f",
            "title": "Data Fusion with Source Authority and Multiple Truth",
            "abstract": "The abundance of data available on the Web makes more and more probable the case of finding that different sources contain (partially or completely) different values for the same item. Data Fusion is the relevant problem of discovering the true values of a data item when two entities representing it have been found and their values are different. Recent studies have shown that when, for finding the true value of an object, we rely only on majority voting, results may be wrong for up to 30% of the data items, since false values are spread very easily because data sources frequently copy from one another. Therefore, the problem must be solved by assessing the quality of the sources and giving more importance to the values coming from trusted sources. State-of-the-art Data Fusion systems define source trustworthiness on the basis of the accuracy of the provided values and on the dependence on other sources. In this paper we propose an improved algorithm for Data Fusion, that extends existing methods based on accuracy and correlation between sources by taking into account also source authority, defined on the basis of the knowledge of which sources copy from which ones. Our method has been designed to work well also in the multi-truth case, that is, when a data item can also have multiple true values. Preliminary experimental results on a multi-truth real-world dataset show that our algorithm outperforms previous state-of-the-art approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "150080368",
                    "name": "Davide Piantella"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "fcf5c111993b806dc3a575219e34ac99c5cb0ae6",
            "title": "Evolution of Financial Time Series Clusters",
            "abstract": "Nowadays, a huge amount of applications exist that natively adopt a data-streaming model to represent highly dynamic phenomena. A challenging application is constituted by data from the stock market, where the stock prices are naturally modeled as data streams that fluctuate very much and remain meaningful only for short amounts of time. In this paper we present a technique to track evolving clusters of financial time series, with the aim of constructing reliable models for this highly dynamic application. In our technique the clustering over a set of time series is iterated over time through sliding windows and, at each iteration, the differences between the current clustering and the previous one are studied to determine those changes that are \"significant\" with respect to the application. For example, in the financial domain, if a company that has belonged to the same cluster for a certain amount of time moves to another cluster, this may be a signal of a significant change in its economical or financial situation.",
            "fieldsOfStudy": [
                "Business",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25867417",
                    "name": "Davide Azzalini"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "2771866",
                    "name": "Mirjana Mazuran"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "0bc83a9bf8e87d32df7a2ad105e2d4bce5ca0c3d",
            "title": "Ethics-aware Data Governance (Vision Paper)",
            "abstract": "The number of datasets available to legal practitioners, policy makers, scientists, and many other categories of citizens is growing at an unprecedented rate. Ethics-aware data processing has become a pressing need, considering that data are often used within critical decision processes (e.g., staff evaluation, college admission, criminal sentencing). The goal of this paper is to propose a vision for the injection of ethical principles (fairness, non-discrimination, transparency, data protection, diversity, and human interpretability of results) into the data analysis lifecycle (source selection, data integration, and knowledge extraction) so as to make them first-class requirements. In our vision, a comprehensive checklist of ethical desiderata for data protection and processing needs to be developed, along with methods and techniques to ensure and verify that these ethically motivated requirements and related legal norms are fulfilled throughout the data selection and exploration processes. Ethical requirements can then be enforced at all the steps of knowledge extraction through a unified data modeling and analysis methodology relying on appropriate conceptual and technical tools.",
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "authors": [
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1741158",
                    "name": "P. Atzeni"
                },
                {
                    "authorId": "25867417",
                    "name": "Davide Azzalini"
                },
                {
                    "authorId": "1716642",
                    "name": "Ilaria Bartolini"
                },
                {
                    "authorId": "1766807",
                    "name": "L. Cabibbo"
                },
                {
                    "authorId": "2767120",
                    "name": "L. Calderoni"
                },
                {
                    "authorId": "1689798",
                    "name": "P. Ciaccia"
                },
                {
                    "authorId": "1791339",
                    "name": "Valter Crescenzi"
                },
                {
                    "authorId": "1807669",
                    "name": "Juan Carlos De Martin"
                },
                {
                    "authorId": "51209609",
                    "name": "Selina Fenoglietto"
                },
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "145042418",
                    "name": "S. Greco"
                },
                {
                    "authorId": "34675913",
                    "name": "F. Isgr\u00f2"
                },
                {
                    "authorId": "1747625",
                    "name": "D. Maio"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "145228093",
                    "name": "M. Matera"
                },
                {
                    "authorId": "1796590",
                    "name": "P. Merialdo"
                },
                {
                    "authorId": "1698803",
                    "name": "Cristian Molinaro"
                },
                {
                    "authorId": "1742653",
                    "name": "M. Patella"
                },
                {
                    "authorId": "145123169",
                    "name": "R. Prevete"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1471635988",
                    "name": "A. Santangelo"
                },
                {
                    "authorId": "1688359",
                    "name": "Andrea Tagarelli"
                },
                {
                    "authorId": "2527878",
                    "name": "G. Tamburrini"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "9485366631922e89b92d58be931e0fdd8a9b5bb7",
            "title": "Event Recognition Strategies applied in the Mercurio Project",
            "abstract": "Mercurio is a project currently investigated at Politecnico di Milano whose aim is to support the decision-making process of financial investors. Mercurio identifies relevant events both from financial news articles and financial indexes and uses sequential pattern mining to predict exceptional events given their past occurrences and relationships with other events. The process of event recognition, both from textual and numerical data sources, is crucial to successfully reach the goals. Investors constantly read financial news and analyze financial indexes, using their knowledge and experience to predict market events and make profitable investments. Mercurio [1, 2] aims at supporting this process by automatically extracting, from data freely available on the Web, events that influence and shake the market. Event recognition strategies are applied to both textual and numerical financial information. Textual data sources. Mercurio monitors Italian financial data sources such as Sole 24 Ore , Corriere della Sera , Radiocor , etc. These data are processed according to different strategies: Semantic recognition. Events are recognized through semantic rules that formalize the knowledge of our domain expert. Rules define relationships between sentence structures and events; they are designed to capture meanings that go beyond the sole natural language processing since they recognize \u201chidden\u201d information inside the news, e.g. financial newspapers, usually, publish interviews when requested by a company: why would a company want to be interviewed? It seems that interviews are often published for reassuring investors in times of crisis. Classification. Often, data information sources specify, for each article, one or more categories, possibly hierarchical, it belongs to, e.g. articles about balance, merge& acquisition, etc. These categories are mostly general-purpose 3 http://www.ilsole24ore.com/ 4 http://www.corriere.it/economia/ 5 http://www.radiocor.ilsole24ore.com/ and hard to use in our context in a significant way. Mercurio employs a domain-specific classification of articles, manually performed by our domain expert on a training set of articles, then automatically derived by the system for the remaining articles. Communication style. Over time, companies develop a certain communication style characterized by the amount of published news, the different sources dealing with it, the diversity of reported topics, etc.. It is interesting to discover events where a company breaks its expected communication trend, e.g. an out-of-the-blue article breaking a \u201clong\u201d communication silence. Article summarization. Long articles are hardly read by investors who, instead of reading the whole news, usually skim through its content or trust little more than title and introduction. Mercurio applies summarization techniques to provide investors with only the most relevant information. It ranks each sentence in the article according to its position in the text (at the beginning, end, etc.) and to its content (domain-dependent stop-words as well as significant expressions are kept into account) and constructs a summary that contains only the most informative sentences. Numerical data sources. Mercurio gathers stock prices from Yahoo! Finance 6 and employs technical analysis [3] techniques to determine significant events. Stock events. Mercurio identifies changes in stock using simple moving averages (sma) of different day lengths. For up/down trends and congestions, sma10, sma20 and sma40 are used, e.g., if the price is above sma10; sma10 is above sma20 and sma20 is above sma40 there is an ongoing uptrend. To find up/down jumps, Mercurio analyzes how sma3 and sma5 change in a time-window of a week. All stock changes are found a posteriori and then used for training. Candlestick patterns. Japanese candlestick charts 7 are used to represent stock prices at possibly different aggregation levels; specific candlestick patterns predict particular market movements. Mercurio uses these patterns in combination with stock events to increment the precision of the event recognition. Moreover, we are investigating the use of patterns at different levels of aggregation, e.g. a candlesticks representing days or weeks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25867417",
                    "name": "Davide Azzalini"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "2064113517",
                    "name": "Davide Greco"
                },
                {
                    "authorId": "2771866",
                    "name": "Mirjana Mazuran"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "97a3fd0ece9e2a79735a7e6437afb8887e29a363",
            "title": "Front matter",
            "abstract": "\"Universal design provides a blueprint for maximum inclusion of all people\" [7]. This statement is now nearly 20 years old. The present collection of papers from the 3rd International Conference on Universal Design (UD 2016) is testament to the fact that Universal Design is now benefitting from a climate that is more knowledgeable about, and possibly more accommodating of, individual differences between people. However, there are still many open issues, and much to be learnt from exchanging experiences between all stakeholders interested in Universal Design, be they policy makers, practitioners or researchers. This is due to the many changes in society, the environment and technology that have occurred in the last two decades. In this collection of papers from the conference, we cover many areas of theory and practice of Universal Design, with applications from the built environment and tangible products, to communication, services, and system design issues. There are also papers about advocating and teaching Universal Design, debates about policy, and about codes, regulations and standards. We hope the collection is a useful way for policy makers, practitioners and researchers interested in these different strands of work to learn and exchange ideas and best practices, and to break down the \"silos\" that inevitably emerge in any group attempting to address a topic of mutual interest from many perspectives. In gathering together these different strands, we also need to reflect on the current manifestations of Universal Design, and what implications there are for the years to come. We need to look back to where we came from, and to look forward to the future shaping of Universal Design. With the benefit of nearly 20 years since the publication of the Principles of Universal Design Principles of Universal Design Version 2.0 4/1/97. \u00a9 Copyright 1997 NC State University, The Center for Universal Design, an initiative of the College of Design. Compiled by advocates of universal design, listed in alphabetical order: Bettye Rose Connell, Mike Jones, Ron Mace, Jim Mueller, Abir Mullick, Elaine Ostroff, Jon Sanford, Ed Steinfeld, Molly Story, & Gregg Vanderheiden https://www.ncsu.edu/project/design-projects/sites/cud/content/principles/principles.html. , we can review some of the terminology and its development around the concept of Universal Design. From the outset, the term referred to a broad spectrum of applications. It covered the design of buildings, products and environments, and the need for these to be inherently accessible to older people, people without disabilities, and people with disabilities. More specifically, the term Universal Design was defined in 1997 as \"the design of products and environments to be usable by all people, to the greatest extent possible, without the need for adaptation or specialized design\" https://www.ncsu.edu/ncsu/design/cud/about_ud/about_ud.htm. . As with all concise definitions, it has been necessary to emphasise and add to this statement, for instance: \u2022 Universal Design is really about including all people and not a euphemism for \"design for those with a disability\". It is about products, services, and environments being usable, to the greatest extent possible, by everyone, regardless of their age, ability, cultural background or status in life [1]. \u2022 The phrase \"to the greatest extent possible\" was added to counter criticism that Universal Design was a utopian ideal, and to underscore that Universal Design is a practical as well as conceptual approach. \u2022 The focus of Universal Design is on mainstream products, services, and environments and not on adaptations or specialist products, services, and environments. \u2022 Universal Design emphasises the need to design from the outset for the widest possible range of users, rather than try to make modifications later on, whether during the design process or after release. \u2022 Products, services, and environments should also be aesthetically pleasing as well as non-discriminatory and non-stigmatising. Most of these elaborations are enshrined in the Principles of Universal Design. This is a set of seven principles that were developed to lay out guidance for the design of environments, products and communications, to evaluate existing designs, and to educate both designers and consumers about the characteristics of more usable products and environments. In addition, other terms have come into being, responding to the need to explain different aspects of the Universal Design spectrum. In 1998, at the \"Designing for the 21st Century Conference\" Ron Mace's presentation differentiated between the meanings and practices associated with the terms \"Barrier-Free Design\", \"Assistive Technology\" and \"Universal Design\" [2]. At the turn of the millennium, \"Design for All\" was the term adopted by the European Commission which focused on ensuring that environments, products, services and interfaces of the Information Society Technologies (ISTs) work for people of all ages and abilities in different situations and under various circumstances [6, 8]. It spelt out the \"adaptation or specialized design\" with a three-part strategy: \u2022 Design of IST products, services and applications which are demonstrably suitable for most of the potential users without any modifications. \u2022 Design of products which are easily adaptable to different users (e.g. by incorporating adaptable or customisable user interfaces). \u2022 Design of products which have standardised interfaces, capable of being accessed by specialised user interaction devices [6]. Alongside setting out this strategy, the definition of Design for All made a conscious effort to make the concept more widely acceptable by explaining how Design for All could benefit not just consumers of ISTs, but also producers, and give wider social and economic benefit. Amongst other things, adopting a Design for All approach would help deal proactively with the demographic trend of the aging population, and benefit businesses with increased sales of innovatively designed products that everyone could use. In short, Design for All advocated a policy of mutual benefit, where the \"for all\" descriptor included more than the user population. However, as with the term Universal Design, Design for All was misinterpreted, and accused firstly of the impractical aim of trying to accommodate everyone without exception. Secondly, there was a confusion that Design for All in its insistence on minimizing adaptation, was advocating a \"one design fits all\" stance, evidenced by the question posed at the time \"Could you imagine a pair of shoes being designed in such way that everybody would want to wear them?\" A preferred term to Design for All in the English-speaking European literature is \"Inclusive Design\". Coined by Roger Coleman in 1994 RICA (Research Institute for Consumer Affairs) (2016) Inclusive Design: manufacturing, design, and retail expert views available from rica.org.uk. , this was an elegant apposition to the marketing term \"exclusive design\" as well as being a way of bringing in the notion of social equity that is part of the ethos of Universal Design and enshrined in the first of the seven Principles of Universal Design. Interestingly, in 2005, the British Standards Institute described Inclusive Design in its \"Guide to Managing Inclusive Design\" as \"comprehensive, integrated design which encompasses all aspects of a product used by consumers of diverse age and capability in a wide range of contexts\" BS 7000-6:2005, Design management systems. Managing inclusive design. Guide. . The emphasis is firmly on products, as noted by the compilers of the Inclusive Design Toolkit Inclusive Design Toolkit, What is Inclusive Design, Section: Comparison with Universal Design, http://www.inclusivedesigntoolkit.com/betterdesign2/whatis/whatis.html#p3b. . However in the guide itself, the definition is widened to include services: \"the design of mainstream products and/or services that are accessible to, and usable by, as many people as reasonably possible without the need for special adaptation or specialised design\" BS 7000-6, 2005. Design Management Systems: Managing Inclusive Design, BSi, London, UK. http://shop.bsigroup.com/en/ProductDetail/?pid=000000000030142267. . With the term Inclusive Design, the notion that the design is of mutual benefit to all stakeholders, as in Design for All, was lost. It is perhaps telling that the Scandinavian countries with their tradition of collaborative and participatory design are the main supporters of the term Design for All as the most appropriate one to use [6]. It is a term no longer in such evident use by the European Commission. However, it is notable that in the proposal for what is being more commonly referred to as the European Accessibility Act, there is a somewhat awkward paraphrase \"Accessibility following a 'design for all' approach\" Section 1.2, European Commission (2015) COM(2015) 615 final2015/0278 (COD) Proposal for a Directive of the European Parliament and of the Council on the approximation of the laws, regulations and administrative provisions of the Member States as regards the accessibility requirements for products and services. which could be understood as an attempt to keep the \"mutual benefit\" notion, since the Directive is not calling for human rights non-discriminatory legislation directly, but for support of the Single Market. (ABSTRACT TRUNCATED)",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1422202304",
                    "name": "Dimitrios Gunopoulos"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "2146159299",
                    "name": "Jun Yang"
                }
            ]
        },
        {
            "paperId": "b4b6a58baac5d70eebb65302ddfb528a2a205b36",
            "title": "INDIANA the Database Explorer",
            "abstract": "We propose INDIANA, a system conceived to support a novel paradigm of database exploration. INDIANA guides users that are interested in gaining insights about a database by involving them in a \u201cconversation\u201d. During the conversation, the system proposes some features of the data that are \u201cinteresting\u201d from the statistical viewpoint. The user selects some of these as starting points, and INDIANA reacts by suggesting new features related to the ones selected by the user. A key requirement in this context is the ability to explore transactional databases, without the need to conduct complex pre-processing. To this end, we develop a number of novel algorithms to support interactive exploration of the database. We report an in-depth experimental evaluation to show that the proposed system guarantees a very good trade-off between accuracy and scalability, and a user study that supports the claim that the system is effective in real-world database-exploration tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2078737139",
                    "name": "Antonio Giuzio"
                },
                {
                    "authorId": "1785690",
                    "name": "G. Mecca"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1744102",
                    "name": "M. Roveri"
                },
                {
                    "authorId": "145530628",
                    "name": "Donatello Santoro"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "b71279bc4df8ee04a006a31cf6d9aab58265e08f",
            "title": "Process Conformance Checking by Relaxing Data Dependencies",
            "abstract": "Given the events modeled by a business process, it may happen in the presence of alternative execution paths that the data required by a certain event determines somehow what event is executed next. Then, the process can be modeled by using an approximate functional dependency between the data required by both events. We apply this approach in the context of conformance checking: given a business process model with a functional dependency (FD) that no longer corresponds to the observed reality, we propose corrections to the FD to make it exact or at least to improve its confidence and produce a more accurate model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14071006",
                    "name": "Montserrat Esta\u00f1ol"
                },
                {
                    "authorId": "2771866",
                    "name": "Mirjana Mazuran"
                },
                {
                    "authorId": "2072500893",
                    "name": "Xavier Oriol"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1709067",
                    "name": "Ernest Teniente"
                }
            ]
        },
        {
            "paperId": "c2c27a4b8b9d96bc92ab8a904c0c53aea0c38e33",
            "title": "Big Data Meets Digital Cultural Heritage",
            "abstract": "Information and Communication Technologies have radically changed the modern Cultural Heritage scenery: Simple traditional Information Systems supporting the management of cultural artifacts have left the place to complex systems that expose rich information extracted from heterogeneous data sources\u2014like Sensor Networks, Social Networks, Digital Libraries, Multimedia Collections, Web Data Service, and so on\u2014by means of sophisticated applications that enhance the users\u2019 experience. In this article, we describe SCRABS, a Smart Context-awaRe Browsing assistant for cultural EnvironmentS. SCRABS has been developed during the Cultural Heritage Information Systems national project and promoted by DATABENC, the Cultural Heritage Technological District of the Campania Region, in Italy. SCRABS has been designed on top of a Big Data technological stack as the result of a multidisciplinary project carried out by a heterogeneous team of computer scientists, archeologists, architects, and experts in humanities. We describe the main ideas that support the system, showing its use in some real application scenarios located in the Paestum Archeologica Sites.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40389697",
                    "name": "F. Amato"
                },
                {
                    "authorId": "1707400",
                    "name": "V. Moscato"
                },
                {
                    "authorId": "1695194",
                    "name": "A. Picariello"
                },
                {
                    "authorId": "1732822",
                    "name": "F. Colace"
                },
                {
                    "authorId": "143695813",
                    "name": "M. D. Santo"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "0946dff96c25eb8c50338143a2d12f0dd0bcef9f",
            "title": "Recommending New Items to Ephemeral Groups Using Contextual User Influence",
            "abstract": "Group recommender systems help groups of users in finding appropriate items to be enjoyed together. Lots of activities, like watching TV or going to the restaurant, are intrinsically group-based, thus making the group recommendation problem very relevant. In this paper we study ephemeral groups, i.e., groups where the members might be together for the first time. Recent approaches have tackled this issue introducing complex models to be learned offline, making them unable to deal with new items; on the contrary, we propose a group recommender able to manage new items too. In more detail, our technique determines the preference of a group for an item by combining the individual preferences of the group members on the basis of their contextual influence, where the contextual influence represents the ability of an individual, in a given situation, to direct the group's decision. We conducted an extensive experimental evaluation on a TV dataset containing a log of viewings performed by real groups, showing how our approach outperforms the comparable techniques from the literature.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "2794140",
                    "name": "Emanuele Rabosio"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "543f17e523779edaa7b9d57fe17f382bc3239389",
            "title": "Semi-automatic support for evolving functional dependencies",
            "abstract": "During the life of a database, systematic and frequent violations of a given constraint may suggest that the represented reality is changing and thus the constraint should evolve with it. In this paper we propose a method and a tool to (i) nd the functional dependencies that are violated by the current data, and (ii) support their evolution when it is necessary to update them. The method relies on the use of condence , as a measure that is associated with each dependency and allows us to understand \"how far\" the dependency is from correctly describing the current data; and of goodness, as a measure of balance between the data satisfying the antecedent of the dependency and those satisfying its consequent. Our method compares favorably with literature that approaches the same problem in a dierent way, and performs eectively and eciently as shown by our tests on both real and synthetic databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2771866",
                    "name": "Mirjana Mazuran"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "3366998",
                    "name": "Stefania Ugolini"
                }
            ]
        },
        {
            "paperId": "6eaf324ccfa48c016d191eb1c4bbf032eaeb48f8",
            "title": "Forecasting Out-of-the-Ordinary Financial Events",
            "abstract": "Being able to understand the financial market is very important for investors and, given the width and complexity of the topic, tools to support investor decisions are badly needed. In this paper we present Mercurio, a system that supports the decision-making process of financial investors through the automatic extraction and analysis of financial data coming from the Web. Mercurio formalizes the knowledge and reasoning of an expert in financial journalism and uses it to identify relevant events within financial newspapers. Moreover, it performs automatic analysis of financial indexes to identify relevant events related to the stock market. Then, sequential pattern mining is used to predict exceptional events on the basis of the knowledge of their past occurrences and relationships with other events, in order to to warn investors about them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "104594724",
                    "name": "M. Brambilla"
                },
                {
                    "authorId": "2064113517",
                    "name": "Davide Greco"
                },
                {
                    "authorId": "3421005",
                    "name": "S. Marchesini"
                },
                {
                    "authorId": "144471674",
                    "name": "Luca Marconi"
                },
                {
                    "authorId": "2771866",
                    "name": "Mirjana Mazuran"
                },
                {
                    "authorId": "10335574",
                    "name": "Martina Morlacchi Bonfanti"
                },
                {
                    "authorId": "40142330",
                    "name": "Alessandro Negrini"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "089b65988f1db85a8b9538808688b10b24f8b857",
            "title": "Exploratory Computing: What is there for the Database Researcher?",
            "abstract": "The need for effective tools helping users to make sense of very big datasets has received a lot of attention lately. In this paper we propose a paradigm for database exploration which is in turn inspired by the exploratory computing vision [2]. We may describe exploratory computing as the step-by-step \u201cconversation\u201d of a user and a system that \u201chelp each other\u201d to refine the data exploration process, ultimately gathering new knowledge that concretely fulfills the user needs. In this broad and innovative context, this paper proposes a model to concretely perform this kind of exploration over a database. The model is general enough to encompass most data models and query languages that have been proposed for data management in the last few years. At the same time, it is precise enough to provide a first formalization of the problem and reason about the research challenges posed to database researchers by this new paradigm of interaction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2834754",
                    "name": "Marcello Buoncristiano"
                },
                {
                    "authorId": "1785690",
                    "name": "G. Mecca"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1744102",
                    "name": "M. Roveri"
                },
                {
                    "authorId": "145530628",
                    "name": "Donatello Santoro"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "1cf9ec73f3e455a7638959d279cac8c72b2ad3e8",
            "title": "On the CALM Principle for BSP Computation",
            "abstract": "In recent times, considerable emphasis has been given to two apparently disjoint research topics: data-parallel and eventually consistent, distributed systems. In this paper we propose a study on an eventually consistent, dataparallel computational model, the keystone of which is provided by the recent finding that a class of programs exists that can be computed in an eventually consistent, coordination-free way: monotonic programs. This principle is called CALM and has been proven by Ameloot et al. for distributed, asynchronous settings. We advocate that CALM should be employed as a basic theoretical tool also for data-parallel systems, wherein computation usually proceeds synchronously in rounds and where communication is assumed to be reliable. We deem this problem relevant and interesting, especially for what concerns parallel workflow optimization, and make the case that CALM does not hold in general for dataparallel systems if the techniques developed by Ameloot et al. are directly used. In this paper we sketch how, using novel techniques, the satisfiability of the if direction of the CALM principle can still be obtained, although just for a subclass of monotonic queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192580",
                    "name": "Matteo Interlandi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "2bfede6994aa5e5df38c1e468c208fc96c2937e2",
            "title": "IQ4EC: Intensional answers as a support to exploratory computing",
            "abstract": "The advent of the Big Data challenge has stimulated research on methods and techniques to deal with the problem of managing data abundance. As a result, effective sense-making of semantically rich and big datasets has received a lot of attention, and new search approaches, such as Exploratory Computing (EC), have seen the light. In this paper we present IQ4EC, a system for data exploration inspired by EC, that supports users in the inspection of huge amounts of relational data through a step-by-step process, providing feedback based on approximate, intensional information expressed in terms of association rules. At each step of the process, the users can choose a portion of data to examine, and the system guides them to the next step by providing synthetic information and visualization of the resulting dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2771866",
                    "name": "Mirjana Mazuran"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "85e7fdcdbd9758489c6cf4a231a295b267ecb808",
            "title": "Database Challenges for Exploratory Computing",
            "abstract": "Helping users to make sense of very big datasets is nowadays considered an important research topic. However, the tools that are available for data analysis purposes typically address professional data scientists, who, besides a deep knowledge of the domain of interest, master one or more of the following disciplines: mathematics, statistics, computer science, computer engineering, and programming. On the contrary, in our vision it is vital to support also different kinds of users who, for various reasons, may want to analyze the data and obtain new insight from them. Examples of these data enthusiasts [4, 9] are journalists, investors, or politicians: non-technical users who can draw great advantage from exploring the data, achieving new and essential knowledge, instead of reading query results with tons of records. The term data exploration generally refers to a data user being able to find her way through large amounts of data in order to gather the necessary information. A more technical definition comes from the field of statistics, introduced by Tukey [12]: with exploratory data analysis the researcher explores the data in many possible ways, including the use of graphical tools like boxplots or histograms, gaining knowledge from the way data are displayed. Despite the emphasis on visualization, exploratory data analysis still assumes that the user understands at least the basics of statistics, while in this paper we propose a paradigm for database exploration which is in turn inspired by the exploratory computing vision [2]. We may describe exploratory computing as the step-by-step \u201cconversation\u201d of a user and a system that \u201chelp each other\u201d to refine the data exploration process, ultimately gathering new knowledge that concretely fullfils the user needs. The process is seen as a conversation since the system provides active support: it not only answers user\u2019s requests, but also suggests one or more possible actions that may help the user to focus the exploratory session. This activity may entail the use of a wide range of different techniques, including the use of statistics and data analysis, query suggestion, advanced visualization tools, etc. The closest analogy [2] is that of a human-tohuman dialogue, in which two people talk, and continuously make reference to their lives, priorities, knowledge and beliefs, leveraging them in order to provide the best possible contribution to the dialogue. In essence, through the conversation they are exploring themselves as well as the information that is conveyed through their words. This exploration process therefore means investigation, exploration-seeking, comparison-making, and learning altogether. It is most appropriate for big collections of semantically rich data, which typically hide precious knowledge behind their complexity. In this broad and innovative context, this paper intends to make a significant step further: it proposes a model to concretely perform this kind of exploration over a database. The model is general enough to encompass most data models and query languages that have been proposed for data management in the last few years. At the same time, it is precise enough to provide a first formalization of the problem and reason about the research challenges posed to database researchers by this new paradigm of interaction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2834754",
                    "name": "Marcello Buoncristiano"
                },
                {
                    "authorId": "1785690",
                    "name": "G. Mecca"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1744102",
                    "name": "M. Roveri"
                },
                {
                    "authorId": "145530628",
                    "name": "Donatello Santoro"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "57513d2145d69febc71c3ed7ae86e627bd9335d4",
            "title": "Green Move: A Platform for Highly Configurable, Heterogeneous Electric Vehicle Sharing",
            "abstract": "Vehicle sharing in urban areas has the potential to be the answer to some of the main issues that hinder the spreading of electric vehicles, in particular for what concerns the high upfront costs of the vehicles, combined with their still limited range, which can induce phenomena such as range anxiety. For its potential to be realized, vehicle sharing must be tailored to the multiform needs of its users by offering a wide range of support services that can be selected based on the user preferences. In this paper we present the platform for vehicle sharing developed in the Green Move project, which allows services to be dynamically loaded and unloaded on vehicles, and describe a pair of prototype applications to illustrate its benefits.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3138155",
                    "name": "Andrea G. Bianchessi"
                },
                {
                    "authorId": "3292643",
                    "name": "G. Cugola"
                },
                {
                    "authorId": "2173708",
                    "name": "S. Formentin"
                },
                {
                    "authorId": "1783938",
                    "name": "A. Morzenti"
                },
                {
                    "authorId": "1884747",
                    "name": "Carlo Ongini"
                },
                {
                    "authorId": "1769977",
                    "name": "Emanuele Panigati"
                },
                {
                    "authorId": "49450068",
                    "name": "M. Rossi"
                },
                {
                    "authorId": "2052313573",
                    "name": "S. Savaresi"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "2203611",
                    "name": "Edoardo G. Vannutelli Depoli"
                }
            ]
        },
        {
            "paperId": "66991a640a3734742f5c9884b51d03234d0b4c5e",
            "title": "Exploratory computing: a draft Manifesto",
            "abstract": "The advent of the Big Data challenge has stimulated research on methods and techniques to deal with the problem of managing data abundance. Many approaches have been developed, but for the most part, they attack one specific side of the problem: e.g. efficient querying, analysis techniques that summarize data or reduce its dimensionality, data visualization, etc. The approach proposed in this paper aims instead at taking a comprehensive view: first of all, it takes into account that human exploration is an iterative and multi-step process and therefore allows building upon a previous query on to the next, in a sort of \u201cdialogue\u201d between the user and the system. Second, it aims at supporting a variety of user experiences, like investigation, inspiration seeking, monitoring, comparison, decision-making, research, etc. Third, and probably most important, it adds to the notion of \u201cbig\u201d the notion of \u201crich\u201d: Exploratory Computing (EC) aims at dealing with datasets of semantically complex items, whose inspection may reach beyond the user's previous knowledge or expectations: an exploratory experience basically consists in creating, refining, modifying, comparing various datasets in order to \u201cmake sense\u201d of these meanings.",
            "fieldsOfStudy": [
                "Computer Science",
                "Political Science"
            ],
            "authors": [
                {
                    "authorId": "2292935",
                    "name": "N. D. Blas"
                },
                {
                    "authorId": "2771866",
                    "name": "Mirjana Mazuran"
                },
                {
                    "authorId": "1769550",
                    "name": "P. Paolini"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "7fe57b4b912dbd88364bd60e5a1ad3dfeb3e6cfc",
            "title": "On the CALM Principle for Bulk Synchronous Parallel Computation",
            "abstract": "In the recent years a lot of emphasis has been placed on two apparently disjoined fields: data-parallel and eventually consistent distributed systems. In this paper we propose a theoretical study over an eventually consistent data-parallel computational model. The keystone is provided by the recent finding that a class of programs exists which can be computed in an eventually consistent, coordination-free way: monotonic programs. This principle is called CALM and has been proven for distributed asynchronous settings. We make the case that, using the techniques developed by Ameloot et al., CALM does not hold in general for data-parallel systems, wherein computation usually proceeds synchronously in rounds and where communication is reliable. We then show that using novel techniques subsuming the one of Ameloot et al., the satisfiability of the CALM principle is directly related with the assumptions imposed on the behavior of the system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192580",
                    "name": "Matteo Interlandi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "954ddcdf114bcfca42cc8f85a72ee3b81c0a2a14",
            "title": "Exploratory computing: a challenge for visual interaction",
            "abstract": "The advent of the Big Data challenge has stimulated research on methods to deal with the problem of managing data abundance. Many approaches have been developed, but for the most part, they attack one specific side of the problem: e.g. efficient querying, analysis techniques that summarize data or reduce its dimensionality, data visualization, etc. The approach proposed in this poster aims instead at taking a comprehensive view: first of all, it supports human exploration as an iterative and multi-step process and therefore allows building upon a previous query on to the next, in a sort of \"dialogue\" between the user and the system. Second, it aims at supporting a variety of user experiences, like investigation, inspiration seeking, monitoring, comparison, decision-making, research, etc. Third, and probably most important, it adds to the notion of \"big\" the notion of \"rich\": Exploratory Computing (EC) aims at dealing with datasets of semantically complex items, whose inspection may reach beyond the user's previous knowledge or expectations: an exploratory experience basically consists in creating, refining, modifying, comparing various datasets in order to \"make sense\" of these meanings. A crucial challenge of EC lies at the user interface level (data visualization, feedback, relevance of the results, interaction possibilities): how to convey, in an effective manner, all the possible turn-takings of this \"dialogue\" between the user and the system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292935",
                    "name": "N. D. Blas"
                },
                {
                    "authorId": "2771866",
                    "name": "Mirjana Mazuran"
                },
                {
                    "authorId": "1769550",
                    "name": "P. Paolini"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "d0f1070013bbaf86412cf0388085d1fe46f194bd",
            "title": "A review of semantic languages for the conceptual modelling of the manufacturing domain",
            "abstract": "Manufacturing companies are pushed towards higher degrees of responsiveness and adaptability, reached through the flexibility and re-configurability of advanced manufacturing systems solutions, features that for the most part have been already achieved at the mechanical level. Unfortunately, at the information level, software modularity alone cannot fully support re-configurability in an automated way, since manual reprogramming is felt to be still needed. A possible solution to this problem may be the Open Automation paradigm, which uses Web Services to encapsulate software modules in a Service-Oriented Architecture, and where the automatic interactions among services may be supported by a semantically-enriched knowledge base, implemented as a domain ontology that offers reasoning and querying capabilities. The semantic languages proposed by the literature for this implementation are several. Each one is characterized by different reasoning capabilities and by other different characteristics. The paper aims at providing an overview of the semantic languages available in the literature. Each of them is evaluated in the light of objectives, requirements and specifications of the domain of advanced manufacturing systems. The methodology followed in the paper is a thorough literature review and analysis, with an interdisciplinary approach to the literature from the industrial engineering, information technology and industrial automation fields. The value of the current paper is a contribution to clarify what are the most promising semantic languages to support the creation of domain-specific conceptual modelling for advanced manufacturing systems. This is extremely useful to fill a gap in the literature and it is also relevant for industrial practices, as it represents a step forward in the development of an open automation paradigm for higher flexibility and re-configurability of manufacturing systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40269472",
                    "name": "Elisa Negri"
                },
                {
                    "authorId": "47318781",
                    "name": "L. Fumagalli"
                },
                {
                    "authorId": "3123346",
                    "name": "M. Garetti"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "e0dd1019b8fd26add1ce8a53ffaa2706eb3526bd",
            "title": "ADaPT: Automatic Data Personalization based on contextual preferences",
            "abstract": "This demo presents a framework for personalizing data access on the basis of the users' context and of the preferences they show while in that context. The system is composed of (i) a server application, which \u201ctailors\u201d a view over the available data on the basis of the user's contextual preferences, previously inferred from log data, and (ii) a client application running on the user's mobile device, which allows to query the data view and collects the activity log for later mining. At each change of context detected by the system the corresponding tailored view is loaded on the client device: accordingly, the most relevant data is available to the user even when the connection is unstable or lacking. The demo features a movie database, where users can browse data in different contexts and appreciate the personalization of the data views according to the inferred contextual preferences.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1680319",
                    "name": "A. Miele"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "2794140",
                    "name": "Emanuele Rabosio"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "e3ed4fe8d798d098293cfa9756012f9214b3a615",
            "title": "A datalog-based computational model for coordination-free, data-parallel systems",
            "abstract": "Abstract Cloud computing refers to maximizing efficiency by sharing computational and storage resources, while data-parallel systems exploit the resources available in the cloud to perform parallel transformations over large amounts of data. In the same line, considerable emphasis has been recently given to two apparently disjoint research topics: data-parallel, and eventually consistent, distributed systems. Declarative networking has been recently proposed to ease the task of programming in the cloud, by allowing the programmer to express only the desired result and leave the implementation details to the responsibility of the run-time system. In this context, we deem it appropriate to propose a study on a logic-programming-based computational model for eventually consistent, data-parallel systems, the keystone of which is provided by the recent finding that the class of programs that can be computed in an eventually consistent, coordination-free way is that of monotonic programs. This principle is called Consistency and Logical Monotonicity (CALM) and has been proven by Ameloot et al. for distributed, asynchronous settings. We advocate that CALM should be employed as a basic theoretical tool also for data-parallel systems, wherein computation usually proceeds synchronously in rounds and where communication is assumed to be reliable. We deem this problem relevant and interesting, especially for what concerns parallel dataflow optimizations. Nowadays, we are in fact witnessing an increasing concern about understanding which properties distinguish synchronous from asynchronous parallel processing, and when the latter can replace the former. It is general opinion that coordination-freedom can be seen as a major discriminant factor. In this work, we make the case that the current form of CALM does not hold in general for data-parallel systems, and show how, using novel techniques, the satisfiability of the CALM principle can still be obtained although just for the subclass of programs called connected monotonic queries. We complete the study with considerations on the relationships between our model and the one employed by Ameloot et al., showing that our techniques subsume the latter when the synchronization constraints imposed on the system are loosened.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192580",
                    "name": "Matteo Interlandi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "04c3b9eaaba5afbd6eed21add2949769dd87d562",
            "title": "Proceedings of the Fifth Workshop on Semantic Web Information Management, SWIM@SIGMOD Conference 2013, New York, NY, USA, June 23, 2013",
            "abstract": "The ceaseless expansion of the World Wide Web is making more and more complex for humans to efficiently find the needed information. The underlying idea of having a description of the data on the Web, organized in such a way as to be used by machines for automation, integration and reuse across various applications, has been exploited in several research fields. As in the previous editions, the International Workshop on \"Semantic Web Information Management\" (SWIM) aims at reviewing the most recent data-centered solutions for the Semantic Web. In particular, its ambition is to present and analyze the techniques for semantic information management, by taking advantage of the synergisms between the logical basis of the semantic web and the logical foundations of conceptual modeling. Indeed, the leitmotif of these researches is the proposal of models and methods conceived to represent and manage the so-called \"semantic data\", that is, data appropriately structured to be easily machine-processable on the Web, according to semantic models (e.g. RDF, RDF(S), OWL). The long-standing experience of the information modeling community can provide a priceless contribution to the substantial problems arising in semantic data management. The research issues can be summarized by the following problems: \n \n(1) How can we store efficiently and effectively large amounts of semantic data? \n \n(2) How can we query semantic data and reason on them in a feasible way? \n \n(3) How can we exploit such semantic data in real world scenarios? \n \nThis workshop covers the emerging area of Semantic Web gathering researchers to debate, propose, and elaborate the foundations for a data-modeling approach to these problems, by presenting running research and projects on these topics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34580886",
                    "name": "R. D. Virgilio"
                },
                {
                    "authorId": "1720285",
                    "name": "Fausto Giunchiglia"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "374b0382a975db8cc6fcebbd4abb7d1233e972da",
            "title": "The relational model is dead, SQL is dead, and I don't feel so good myself",
            "abstract": "We report the opinions expressed by well-known database researchers on the future of the relational model and SQL during a panel at the International Workshop on Non-Conventional Data Access (NoCoDa 2012), held in Florence, Italy in October 2012 in conjunction with the 31st International Conference on Conceptual Modeling. The panelists include: Paolo Atzeni (Universit\u00e0 Roma Tre, Italy), Umeshwar Dayal (HP Labs, USA), Christian S. Jensen (Aarhus University, Denmark), and Sudha Ram (University of Arizona, USA). Quotations from movies are used as a playful though effective way to convey the dramatic changes that database technology and research are currently undergoing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1741158",
                    "name": "P. Atzeni"
                },
                {
                    "authorId": "145917501",
                    "name": "Christian S. Jensen"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "144307032",
                    "name": "S. Ram"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "59c4e2dd3a4838366b6d5f23aa3938178450501b",
            "title": "Datalog in Time and Space, Synchronously",
            "abstract": "Motivated by recent developments of Datalog-based languages for highly distributed systems [9], in this paper we introduce a version of Datalog\u00ac specifically tailored for distributed programming [2] in synchronous settings, along with its operational and declarative semantics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192580",
                    "name": "Matteo Interlandi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "2724566",
                    "name": "S. Bergamaschi"
                }
            ]
        },
        {
            "paperId": "6435e300807081fd858f735a81667bd6d876946b",
            "title": "Pervasive Data Management in the Green Move System: a Progress Report",
            "abstract": "We discuss the use of context-aware and pervasive techniques applied to data gathering, shared services, and information distribution for a vehicle sharing system in the Green Move project.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1769977",
                    "name": "Emanuele Panigati"
                },
                {
                    "authorId": "36983079",
                    "name": "Angelo Rauseo"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "8ac06bb81cc8f4f4b69d989b1284486f9661c83b",
            "title": "The role and relevance of experimentation in informatics",
            "abstract": "Informatics is a relatively young field within science and engineering. Its research and development methodologies build on the scientific and design methodologies in the classical areas, often with new elements to it. We take an in-depth look at one of the less well-understood methodologies in informatics, namely experimentation. What does it mean to do experiments in informatics? Does it make sense to \u2018import\u2019 traditional principles of experimentation from classical disciplines into the field of computing and information processing? How should experiments be documented? These are some of the questions that are treated. The report argues for the key role of empirical research and experimentation in contemporary Informatics. Many IT systems, large and small, can only be designed sensibly with the help of experiments. We recommend that professionals and students alike are well-educated in the principles of sound experimentation in Informatics. We also recommend that experimentation protocols are used and standardized as part of the experimental method in Informatics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145765692",
                    "name": "C. And\u00fajar"
                },
                {
                    "authorId": "2226297",
                    "name": "V. Schiaffonati"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "144523037",
                    "name": "M. Tedre"
                },
                {
                    "authorId": "1794172",
                    "name": "K. V. Hee"
                },
                {
                    "authorId": "143817739",
                    "name": "J. Leeuwen"
                }
            ]
        },
        {
            "paperId": "01711b4ade178f5e0b3f1ed3679c1087c3fce876",
            "title": "A Framework for Mining and Querying Summarized XML Data through Tree-Based Association Rules",
            "abstract": "The massive amount of datasets expressed in different formats, such as relational, XML, and RDF, available in several real applications, may cause some difficulties to non-expert users trying to access these datasets without having sufficient knowledge on their content and structure. Moreover, the processes of query composition, especially in the absence of a schema, and interpretation of the obtained answers may be non-trivial. Data mining techniques, already widely applied to extract frequent correlations of values from both structured and semistructured datasets, provide several interesting solutions for knowledge elicitation. However, the mining process is often guided by the designer, who determines the portion of a dataset where useful patterns can be extracted based on his/her deep knowledge of the application scenario. In our opinion, a research challenge is to mine hidden information from huge datasets, and then use it order to gain useful knowledge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2771866",
                    "name": "Mirjana Mazuran"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "36983079",
                    "name": "Angelo Rauseo"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "872396c60febcbdbfdcb1659ff255c90d1a1d638",
            "title": "Context-aware Information Management in the Green Move System",
            "abstract": ". The Green Move project aims at realizing a zero-emission-vehicle (ZEV) sharing service that also includes pervasive information management. In this paper we discuss the use of context-aware techniques applied to data gathering, shared services and information distribution, and how they lead to the reduction of (noisy) information delivered to users and to the personalized, privacy-aware distribution of information among the various system\u2019s users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1769977",
                    "name": "Emanuele Panigati"
                },
                {
                    "authorId": "36983079",
                    "name": "Angelo Rauseo"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "880d6b2e9d18a3adc59446a169cfd3b5dbd3835e",
            "title": "Green Move: Towards next generation sustainable smartphone-based vehicle sharing",
            "abstract": "This paper presents the Green Move project, which is currently ongoing at Politecnico di Milano. The goal of Green Move is to create an innovative vehicle sharing system, based on electric vehicles, which can easily be accessed by users with little required infrastructure and few intermediaries. The paper provides an overview of the project, briefly describes the architecture of the system under development, and outlines the next steps in this research.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49517874",
                    "name": "G. Alli"
                },
                {
                    "authorId": "1704858",
                    "name": "L. Baresi"
                },
                {
                    "authorId": "3138155",
                    "name": "Andrea G. Bianchessi"
                },
                {
                    "authorId": "3292643",
                    "name": "G. Cugola"
                },
                {
                    "authorId": "2866882",
                    "name": "Alessandro Margara"
                },
                {
                    "authorId": "1783938",
                    "name": "A. Morzenti"
                },
                {
                    "authorId": "1884747",
                    "name": "Carlo Ongini"
                },
                {
                    "authorId": "1769977",
                    "name": "Emanuele Panigati"
                },
                {
                    "authorId": "49450068",
                    "name": "M. Rossi"
                },
                {
                    "authorId": "144733724",
                    "name": "S. Rotondi"
                },
                {
                    "authorId": "2052313573",
                    "name": "S. Savaresi"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "2517783",
                    "name": "A. Sivieri"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "2203611",
                    "name": "Edoardo G. Vannutelli Depoli"
                }
            ]
        },
        {
            "paperId": "9aab90df49c2adeb1e3dde3d77c61a91ee0603bd",
            "title": "NYAYA: A System Supporting the Uniform Management of Large Sets of Semantic Data",
            "abstract": "We present NYAYA, a flexible system for the management of large-scale semantic data which couples a general-purpose storage mechanism with efficient ontological query answering. NYAYA rapidly imports semantic data expressed in different formalisms into semantic data kiosks. Each kiosk exposes the native ontological constraints in a uniform fashion using data log\u00b1, a very general rule-based language for the representation of ontological constraints. A group of kiosks forms a semantic data market where the data in each kiosk can be uniformly accessed using conjunctive queries and where users can specify user-defined constraints over the data. NYAYA is easily extensible and robust to updates of both data and meta-data in the kiosk and can readily adapt to different logical organizations of the persistent storage. In the demonstration, we will show the capabilities of NYAYA over real-world case studies and demonstrate its efficiency over well-known benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34580886",
                    "name": "R. D. Virgilio"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "dd58f5c904f38c02fcae8d54752e6a4ba669df09",
            "title": "Aspects of Pervasive Information Management: An Account of the Green Move System",
            "abstract": "The Green Move project aims at realizing a zero-emission-vehicle (ZEV) sharing service that also includes pervasive information distribution. In this paper we discuss the use of context-aware techniques applied to data gathering, shared services, and information distribution, we also discuss how a context-aware approach applied to these tasks leads to the reduction of (noisy) information delivered to the users and to the personalized distribution of information. Privacy of data is also a main concern in the realization of the project, and a privacy-safe approach to information distribution and advertising is adopted. The project, grounded in many results on the use of context-awareness already published by the same authors, aims at building a real-life system based on them. Eventually, we briefly describe the rapid prototype produced and the approach employed so far for the realization of the full system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1769977",
                    "name": "Emanuele Panigati"
                },
                {
                    "authorId": "36983079",
                    "name": "Angelo Rauseo"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "e28ca1267444fcb83fb3fe1a823b08453e373c85",
            "title": "Data Mining for XML Query-Answering Support",
            "abstract": "Extracting information from semistructured documents is a very hard task, and is going to become more and more critical as the amount of digital information available on the Internet grows. Indeed, documents are often so large that the data set returned as answer to a query may be too big to convey interpretable knowledge. In this paper, we describe an approach based on Tree-Based Association Rules (TARs): mined rules, which provide approximate, intensional information on both the structure and the contents of Extensible Markup Language (XML) documents, and can be stored in XML format as well. This mined knowledge is later used to provide: 1) a concise idea-the gist-of both the structure and the content of the XML document and 2) quick, approximate answers to queries. In this paper, we focus on the second feature. A prototype system and experimental results demonstrate the effectiveness of the approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2771866",
                    "name": "Mirjana Mazuran"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "5a04553cfd7be53d498f47a204a5889f0acde9c8",
            "title": "Proceedings of the 4th International Workshop on Semantic Web Information Management",
            "abstract": "The ceaseless expansion of the World Wide Web is making more and more complex for humans to efficiently find the needed information. The underlying idea of having a description of the data on the Web, organized in such a way as to be used by machines for automation, integration and reuse across various applications, has been exploited in several research fields. \n \nAs in the previous editions, the International Workshop on \"Semantic Web Information Management\" (SWIM) aims at reviewing the most recent data-centered solutions for the Semantic Web. In particular, its ambition is to present and analyze the techniques for semantic information management, by taking advantage of the synergisms between the logical basis of the semantic web and the logical foundations of conceptual modeling. Indeed, the leitmotif of these researches is the proposal of models and methods conceived to represent and manage the so-called \"semantic data\", that is, data appropriately structured to be easily machine-processable on the Web, according to semantic models (e.g. RDF, RDF(S), OWL). The long-standing experience of the information modeling community can provide a priceless contribution to the substantial problems arising in semantic data management.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34580886",
                    "name": "R. D. Virgilio"
                },
                {
                    "authorId": "1720285",
                    "name": "Fausto Giunchiglia"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "63e9fe213f4387a99a8d9cab27d119259ab59c0f",
            "title": "Managing and Using Context Information within the PerLa Language",
            "abstract": "Self-adaptability in pervasive real-world applications can be achieved by adopting a context-aware middleware. In this paper, we propose a context-management extension to the PerLa language and middleware, which allows for: (i) gathering of data from the environment, (ii) feeding this data to the internal context model and, (iii) once a context is active, acting on the relevant resources of the pervasive system, according to the chosen contextual policy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "3016970",
                    "name": "R. Camplani"
                },
                {
                    "authorId": "47233778",
                    "name": "D. Vigan\u00f2"
                }
            ]
        },
        {
            "paperId": "7bbacda4042ac2fa5c0f8ad98b73a1f8aad34bc2",
            "title": "Towards autonomic pervasive systems: the PerLa context language",
            "abstract": "The property of context-awareness, inherent to a Pervasive System, requires a clear definition of context and of how the context parameter values must be extracted from the real world. Since often the same variables are common to the operational system and to the context it operates into, the usage of the same language to manage both the application and the context can lead to substantial savings in application development time and costs. In this paper we propose a context-management extension to the PerLa language and middleware that allows for declarative gathering of context data from the environment, feeding this data to the internal context model and, once a context is active, acting on the relevant resources of the pervasive system, according to the chosen contextual policy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "3016970",
                    "name": "R. Camplani"
                },
                {
                    "authorId": "47233778",
                    "name": "D. Vigan\u00f2"
                }
            ]
        },
        {
            "paperId": "9b8bf172605db11668bc3b880230cbf0c04da9ee",
            "title": "Keyword-based, context-aware selection of natural language query patterns",
            "abstract": "Pervasive access to distributed data sources by means of mobile devices is becoming a frequent realistic operational context in many application domains. In these scenarios data access may be thwarted by the scarce knowledge that users have of the application and of the underlying data schemas and complicated by limited query interfaces, due to the small size of the devices.\n A viable solution to this problem could be expressing the queries in natural language; however, in applications like medical emergencies, data management systems must obey requirements such as very fast and precise data access which make this solution infeasible.\n To reduce the time needed to get answers to user queries, the paper proposes a lightweight, context-aware approach based on the combination of keywords with natural language queries. The method employs ontologies and query patterns to support the users in formulating the most appropriate query for retrieving the desired data. Precision and query efficiency are further improved by focusing searches only to the data which are meaningful w.r.t. the current context, thus supporting the users' situation awareness.\n The approach has been integrated in the SAFE system, developed for mobile and Web, and has been applied in cardiology to support medical personnel in emergency interventions on patients affected by chronic cerebro-vascular diseases. Experimental results have shown that the proposed solution significantly reduces the time to get useful data w.r.t. traditional form-based approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1680369",
                    "name": "E. Zimeo"
                }
            ]
        },
        {
            "paperId": "b993679bdb3dc4795d2bf8371cf92c77c43281cd",
            "title": "Context through answer set programming",
            "abstract": "In a world of global networking, the variety and abundance of information generates the need for effectively and efficiently gathering, synthesizing, and querying the available data while removing information noise. The concept of context has been developed and refined since the first approaches to ubiquitous computing [3], the research area of everywhere computing systems, which has the objective to provide help and information to users in an almost imperceptible way. At first, the idea of context was limited to time and location; later it was extended also to the other external environmental factors, current trends and phenomena that may change or influence the information and services available to a user. In this work, we refer to context primarily in relation with the effects that this notion has over data.\n A system where context awareness is integrated with -- yet orthogonal to -- data management, allows the knowledge of the context in which the data are used to drive the process of focussing on currently useful information (represented as a view), keeping noise at bay: this activity is called context-aware data tailoring [1]. To do this, we model the context as a first-class citizen, by means of a tree-shaped structure called Context Dimension Tree (CDT). Formally defined in [1], the CDT is composed of two kinds of nodes: black nodes, which represent context dimensions, and white nodes, which represent context values. Context dimensions (black nodes) model the different perspectives from which the domain of interest can be seen with respect to the user, the system and their interactions. The values these dimensions can assume are represented as white nodes. A context is obtained as a set of dimension values, thus of white nodes. The hierarchical nature of the CDT grants different levels of abstraction to represent contexts.\n In this paper we propose Answer Set Programming (ASP) as a unified tool to address all the design-time and run-time tasks related to context management. Building on preliminary work presented in [2], at design time we encode contextual information via a disjunctive logic program using the approach of ASP: given a program representing a CDT, each admissible context is represented by a model (also called answer set) of the program. From a CDT many possible contexts can be built, obtained by guessing if a value is a context element or not. Structural constraints to the composition of contexts also apply, along with application-dependent constraints preventing meaningless combinations of white nodes. Candidate contexts, which are candidate models of the program, are checked against the constraints defined, and the -- possibly multiple -- admissible contexts correspond to the multiple models of the program in accordance with ASP fundamentals. Still at design time, the designer extends the program with suitable context-aware views over the extensional data, which establish the relationships between each context and the data to be focussed upon. At run time, whenever new context information is acquired from the system conditions and/or detected by sensors, we use ASP techniques to i) validate this information against the admissible contexts produced at design time from the CDT, and ii) evaluate user queries over the current context-dependent views.\n Overall, the main contribution of this work is the design of an ASP framework for supporting design-time and run-time data tailoring in context-aware systems. Notice that ASP allows us to retain the orthogonality of context modeling while adopting the same framework as for data representation. Moreover, the multiplicity of answers permits support for the imperfection of the gathered context information since, when multiple (possibly incompatible) contexts are detected, the system will be able to deliver all the answers to the user queries, each answer differently focussed according to one of the validated contexts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36983079",
                    "name": "Angelo Rauseo"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "c2c98360d036e3c1e54cdbecf78d8e511c2e94f7",
            "title": "Context Awareness in the Networked Enterprise: Methodological and Technological Issues",
            "abstract": ". The formidable amount of heterogeneous information, accessed by the networked enterprise through all the available channels, makes it difficult for users to find the right information at the right time and at the right level of detail. Contextual meta-data about the system and the users can be used to reduce this plethora of information, providing high-quality, focussed knowledge to users and applications at all decision-making points. We propose context-aware system design methodologies and techniques exemplified within the wine production scenario, where several classes of users access the networked-enterprise data sources, the sensors used for monitoring the productive cycle, and external sources of different nature .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "db920b9e6199d9aef26c594f45a33d7615c707fe",
            "title": "Problems and Opportunities in Context\u2212Based Personalization",
            "abstract": "In a world of global networking, the increasing amount of heterogeneous information, available through a variety of channels, has made it difficult for users to find the information they need in the current situation, at the right level of detail. This is true not only when accessing information from mobile devices, characterized by limited \u2013 although growing \u2013 resources and by high connection costs, but also when using powerful systems, since the amount of \u201cout-of-context\u201d answers to a given user request may be overwhelming. The knowledge of the context in which the data are going to be used can support the process of focussing on currently useful, personalized information. The activity needed for contextaware information personalization provides material for stimulating research, briefly illustrated in this paper.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "2418951",
                    "name": "C. Bolchini"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                }
            ]
        },
        {
            "paperId": "e5484fcea2d59c3ed5b90073f5101ef2c640b885",
            "title": "Semantic data markets: a flexible environment for knowledge management",
            "abstract": "We present Nyaya, a system for the management of Semantic-Web data which couples a general-purpose and extensible storage mechanism with efficient ontology reasoning and querying capabilities. Nyaya processes large Semantic-Web datasets, expressed in multiple formalisms, by transforming them into a collection of Semantic Data Kiosks. Nyaya uniformly exposes the native meta-data of each kiosk using the datalog+- language, a powerful rule-based modelling language for ontological databases. The kiosks form a Semantic Data Market where the data in each kiosk can be uniformly accessed using conjunctive queries and where users can specify user-defined constraints over the data. Nyaya is easily extensible and robust to updates of both data and meta-data in the kiosk and can readily adapt to different logical organization of the persistent storage. The approach has been experimented using well-known benchmarks, and compared to state-of-the-art research prototypes and commercial systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34580886",
                    "name": "R. D. Virgilio"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1718274",
                    "name": "Riccardo Torlone"
                }
            ]
        },
        {
            "paperId": "ef3f0448565b00b1b619d32f4424595b1eefa3db",
            "title": "Context Modeling and Context Awareness: steps forward in the Context-ADDICT project",
            "abstract": "We give an account of the researches on context-aware information tailoring which are going on within the PEDiGREE 1 group at Politecnico di Milano, starting from a foundational framework for the life-cycle of context-aware information systems, in which the system design and management activities consider context as an orthogonal, \ufb01rst-class citizen. The design-time and run-time activities involved in this life-cycle provide material for stimulating research, summarized in this paper.",
            "fieldsOfStudy": [
                "Computer Science",
                "Chemistry"
            ],
            "authors": [
                {
                    "authorId": "2418951",
                    "name": "C. Bolchini"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "3522032fec949ff0731f9b59749821aadd90958a",
            "title": "Context information for knowledge reshaping",
            "abstract": "More and more often, we face the necessity of extracting appropriately reshaped knowledge from an integrated representation of the information space. Be such a global representation a central database, a global view of several ones or an ontological representation of an information domain, we face the need to define personalised views for the knowledge stakeholders: single users, companies or applications. We propose exploiting the information usage context within a methodology for context-aware data design, where the notion of context is formally defined together with its role within the process of view building by information tailoring. This paper presents our context model, called the context dimension tree, which plays a fundamental role in tailoring the information space according to user information needs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2418951",
                    "name": "C. Bolchini"
                },
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "5e717a75777471847606e003d65edf3a575bf543",
            "title": "And what can context do for data?",
            "abstract": "Common to all aCtors in today\u2019s information world is the problem of lowering the \u201cinformation noise,\u201d both reducing the amount of data to be stored and accessed, and enhancing the \u201cprecision\u201d according to which the available data fit the application requirements. Thus, fitting data to the application needs is tantamount to fitting a dress to a person, and will be referred to as data tailoring. The context will be our scissors to tailor data, possibly assembled and integrated from many data sources. Since the 1980s, many organizations have evolved to comply with the market needs in terms of flexibility, effective customer relationship management, supply chain optimization and so on and so forth: the situation where a set of partners re-engineered their single organizations, generating a unique, extended enterprise, has frequently been observed. Together with the organizations, also their information systems evolved, embracing new technologies like XML and ontologies, used in ERP systems and Webservice based applications. In recent years many organizations introduced into their information systems also Knowledge Management features, to allow easy information sharing among the organizations\u2019 members; these new information sources and their content have to be managed together with other \u2013 we might say legacy \u2013 enterprise data. This growth of information, if not properly controlled, leads to a data overload that may cause confusion rather than knowledge, and dramatically reduce the benefits of a rich information system. However, distinguishing useful information from noise, i.e., from all the information not relevant to the specific application, is not a trivial task; the same piece of information can be considered differently, even by the same user, in different situations, or places \u2013 in a single word, in a different context. The notion of context, formerly emerged in various fields of research like psychology and philosophy, is acquiring great importance also in the computer science field. In a commonsense interpretation, the context is perceived as a set of variables that may be of interest for an agent and that influence its actions. The context has often a significant impact on the way humans (or machines) interpret their environment: a change in context causes a transformation in the actor\u2019s mental representation of the reality, even when the reality is not changed. The word itself, derived from the Latin cum (with or together) and texere (to weave), describes a context not just as a profile, but as an active process dealing with the way humans weave their experience within their whole environment, to give it meaning. In the last few years, sophisticated and general context models have been proposed to support context-aware applications. In the following we list the different meanings attributed to the word context: Presentation-oriented:  \u02f2 context is perceived as the capability of the system to adapt content presentation to different channels or to different devices. These context-models are often rigid, since they are designed for specific applications and rely on a well known set of presentation variables. Location-oriented:  \u02f2 with this family of context models, it is possible to handle and What can context do for data?",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2418951",
                    "name": "C. Bolchini"
                },
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "2121470",
                    "name": "R. Rossato"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "72dbb9d71d5e6676268f089dbaf5896eb23ce7ed",
            "title": "Mining tree-based association rules from XML documents",
            "abstract": "The increasing amount of very large XML datasets available to casual users is a most challenging problem for our community, and calls for an appropriate support to efficiently gather knowledge from these data. Data mining, already widely applied to extract frequent correlations of values from both structured and semi-structured datasets, is the appropriate tool for knowledge elicitation. In this work we describe an approach to extract Tree-based association rules from XML documents. Such rules provide approximate, intensional information on both the structure and the content of XML documents, and can be stored in XML format to be queried later on. The mined knowledge is used to provide: (i) quick, approximate answers to queries and (ii) information about structural regularities. A prototype system demonstrates the effectiveness of the approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2771866",
                    "name": "Mirjana Mazuran"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "ba75322f7030210760c09989441ba28615df8ed9",
            "title": "The SemanticWeb Languages",
            "abstract": "The SemanticWeb is basically an extension of the Web and of the Webenabling database and Internet technology, and, as a consequence, the Semantic Web methodologies, representation mechanisms and logics strongly rely on those developed in databases. This is the motivation for many attempts to, more or less loosely, merge the two worlds like, for instance, the various proposals to use relational technology for storing web data or the use of ontologies for data integration. This article comes second in this book, after an article on data management, in order to first complete the picture with the description of the languages that can be used to represent information on the Semantic Web, and then highlight a few fundamental differences which make the database and Semantic Web paradigms complementary but somehow difficult to integrate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1720285",
                    "name": "Fausto Giunchiglia"
                },
                {
                    "authorId": "2422137",
                    "name": "Feroz Farazi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "34580886",
                    "name": "R. D. Virgilio"
                }
            ]
        },
        {
            "paperId": "bbabc3d5d340aa165f0ddf4322d98b90e03f17c2",
            "title": "A methodology for preference-based personalization of contextual data",
            "abstract": "The widespread use of mobile appliances, with limitations in terms of storage, power, and connectivity capability, requires to minimize the amount of data to be loaded on user's devices, in order to quickly select only the information that is really relevant for the users in their current contexts: in such a scenario, specific methodologies and techniques focused on data reduction must be applied. We propose an extension to the data tailoring approach of Context-ADDICT, whose aim is to dynamically hook and integrate heterogeneous data to be stored on small, possibly mobile devices. The main goal of our extension is to personalize the context-dependent data obtained by means of the Context-ADDICT methodology, by allowing the user to express preferences that specify which data s/he is more interested in (and which not) in each specific context. This step allows us to impose a partial order among the data, and to load only the top (most preferred) portion of the data chunks. A running example is used to better illustrate the approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1680319",
                    "name": "A. Miele"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "f2882c7f89648bc55100fd00177829cfc19c3475",
            "title": "Semantic Web Information Management - A Model-Based Perspective",
            "abstract": "Databases have been designed to store large volumes of data and to provide efficient query interfaces. Semantic Web formats are geared towards capturing domain knowledge, interlinking annotations, and offering a high-level, machine-processable view of information. However, the gigantic amount of such useful information makes efficient management of it increasingly difficult, undermining the possibility of transforming it into useful knowledge. The research presented by De Virgilio, Giunchiglia and Tanca tries to bridge the two worlds in order to leverage the efficiency and scalability of database-oriented technologies to support an ontological high-level view of data and metadata. The contributions present and analyze techniques for semantic information management, by taking advantage of the synergies between the logical basis of the Semantic Web and the logical foundations of data management. The books leitmotif is to propose models and methods especially tailored to represent and manage data that is appropriately structured for easier machine processing on the Web. After two introductory chapters on data management and the Semantic Web in general, the remaining contributions are grouped into five parts on Semantic Web Data Storage, Reasoning in the Semantic Web, Semantic Web Data Querying, Semantic Web Applications, and Engineering Semantic Web Systems. The handbook-like presentation makes this volume an important reference on current work and a source of inspiration for future development, targeting academic and industrial researchers as well as graduate students in Semantic Web technologies or database design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34580886",
                    "name": "R. D. Virgilio"
                },
                {
                    "authorId": "1720285",
                    "name": "Fausto Giunchiglia"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "25ab084319579bc199acedec089366a4a73fe7fc",
            "title": "Multimedia knowledge management using ontologies",
            "abstract": "In this paper we describe a novel framework for managing multimedia data using ontologies. Differently from previous theories, we propose a uniform model for both low level multimedia data and semantic description. An ontological system for image retrieval is described and several experiments are carried out in order to demonstrate the effectiveness of our methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1707626",
                    "name": "Antonio Penta"
                },
                {
                    "authorId": "1695194",
                    "name": "A. Picariello"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "39555a4e1320f438586b3f4601415f843ded1bc5",
            "title": "Information Systems Integration and Evolution: Ontologies at Rescue",
            "abstract": "The life of a modern Information System is often characterized by (i) a push toward integration with other systems, and (ii) the evolution of its data management core in response to continuously changing application requirements. Most of the current proposals dealing with these issues from a database perspective rely on the formal notions of mapping and query rewriting. This paper presents the research agenda of ADAM (Advanced Data And Metadata Manager); by harvesting the recent theoretical advances in this area into a unified framework, ADAM seeks to deliver practical solutions to the problems of automatic schema mapping and assisted schema evolution. The evolution of an Information System (IS) reflects the changes occurring in the application reality that the IS is modelling: thus,ADAM exploits ontologies to capture such changes and provide traceability and automated documentation for such evolution. Initial results and immediate benefits of this approach",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1712630",
                    "name": "C. Zaniolo"
                }
            ]
        },
        {
            "paperId": "c11b8fc2c942ee227138754be07ece269b94aec2",
            "title": "Schema Evolution in Wikipedia - Toward a Web Information System Benchmark",
            "abstract": "Evolving the database that is at the core of an Information System represents a difficult maintenance problem that has only been studied in the framework of traditional information systems. However, the problem is likely to be even more severe in web information systems, where open-source software is often developed through the contributions and collaboration of many groups and individuals. Therefore, in this paper, we present an indepth analysis of the evolution history of the Wikipedia database and its schema; Wikipedia is the best-known example of a large family of web information systems built using the open-source software MediaWiki. Our study is based on: (i) a set of Schema Modification Operators that provide a simple conceptual representation for complex schema changes, and (ii) simple software tools to automate the analysis. This framework allowed us to dissect and analyze the 4.5 years of Wikipedia history, which was short in time, but intense in terms of growth and evolution. Beyond confirming the initial hunch about the severity of the problem, our analysis suggests the need for developing better methods and tools to support graceful schema evolution. Therefore, we briefly discuss documentation and automation support systems for database evolution, and suggest that the Wikipedia case study can provide the kernel of a benchmark for testing and improving such systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "9521597",
                    "name": "H. J. Moon"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1712630",
                    "name": "C. Zaniolo"
                }
            ]
        },
        {
            "paperId": "0e5cfbafc21f38906eb7359bd7c2df7327ceb9d6",
            "title": "Answering XML queries by means of data summaries",
            "abstract": "XML is a rather verbose representation of semistructured data, which may require huge amounts of storage space. We propose a summarized representation of XML data, based on the concept of instance pattern, which can both provide succinct information and be directly queried. The physical representation of instance patterns exploits itemsets or association rules to summarize the content of XML datasets. Instance patterns may be used for (possibly partially) answering queries, either when fast and approximate answers are required, or when the actual dataset is not available, for example, it is currently unreachable. Experiments on large XML documents show that instance patterns allow a significant reduction in storage space, while preserving almost entirely the completeness of the query result. Furthermore, they provide fast query answers and show good scalability on the size of the dataset, thus overcoming the document size limitation of most current XQuery engines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1750105",
                    "name": "Elena Baralis"
                },
                {
                    "authorId": "1712027",
                    "name": "P. Garza"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "4fdaa7480c2fda19ada52734e7c24e3b005d2d15",
            "title": "Towards a definition of an Image Ontology",
            "abstract": "Archiving, organizing, and searching multimedia data in an appropriate fashion is a task of increasing importance. The ontology theory may be appropriately extended in order to face this challenging issue. In this paper we propose an extension of an Image Database model based on NF2 with an Ontology model. We first describe NF2 and the main concepts of Image Ontologies, then we propose a general architecture for supporting creation and management of multimedia objects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1707626",
                    "name": "Antonio Penta"
                },
                {
                    "authorId": "1695194",
                    "name": "A. Picariello"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "a006a76ee9561cac052ff688c152bbe7f07b5b02",
            "title": "X-SOM: A Flexible Ontology Mapper",
            "abstract": "System interoperability is a well known issue, especially for heterogeneous information systems, where ontology- based representations may support automatic and user- transparent integration. In this paper we present X-SOM: an ontology mapping and integration tool. The contribution of our tool is a modular and extensible architecture that automatically combines several matching techniques by means of a neural network, performing also ontology debugging to avoid inconsistencies. Besides describing the tool components, we discuss the prototype implementation, which has been tested against the OAEI 2006 benchmark with promising results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "a784ba49549a6f4e26492245a03ed82fa7889c6a",
            "title": "Data and web management research at Politecnico di Milano",
            "abstract": "Research in data management at Politecnico di Milano has a long and solid tradition; forefront books on distributed databases, conceptual database design, logical databases, and active databases contributed to shape the foundations of this discipline in the last two decades. Historically, our work has addressed both all aspects of innovation in the technology of modern data management systems and the consequent support of design methods and tools.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "2418951",
                    "name": "C. Bolchini"
                },
                {
                    "authorId": "144864244",
                    "name": "Daniele Braga"
                },
                {
                    "authorId": "2112146631",
                    "name": "Marco Brambilla"
                },
                {
                    "authorId": "32452484",
                    "name": "A. Campi"
                },
                {
                    "authorId": "1742363",
                    "name": "S. Comai"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "1796949",
                    "name": "P. Lanzi"
                },
                {
                    "authorId": "1737880",
                    "name": "M. Masseroli"
                },
                {
                    "authorId": "145228093",
                    "name": "M. Matera"
                },
                {
                    "authorId": "145859803",
                    "name": "Mauro Negri"
                },
                {
                    "authorId": "2503789",
                    "name": "G. Pelagatti"
                },
                {
                    "authorId": "37898418",
                    "name": "G. Pozzi"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "b2ddb7eed599ccd1886b9ecf9ee51de6079a814f",
            "title": "Context-Based Data Tailoring for Mobile Users",
            "abstract": "Independent, heterogeneous, distributed, sometimes transient and mobile data sources produce an enormous amount of information that should be semantically integrated and filtered, or, as we say, tailored, based on the users\u2019 interests and context. Since both the user and the data sources may be mobile, and communication unreliable, caching the information on the user device is certainly useful. Thus, we propose to exploit knowledge about the user, the adopted device, and the environment altogether called context to the end of information tailoring. The key is context-aware data design where the notion of context must be formally defined, together with its role within the process of information tailoring. This paper presents a context model, called Context Dimension Tree, which plays a fundamental role in tailoring the information domain model within the framework of the Context-ADDICT project, currently under development at Politecnico di Milano. To conclude, we report on other context models, along with a brief comment on their relevant features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "b39b1ecc46bddb5e078f8bde873a3f5bda0ceb8a",
            "title": "A data-oriented survey of context models",
            "abstract": "Context-aware systems are pervading everyday life, therefore context modeling is becoming a relevant issue and an expanding research field. This survey has the goal to provide a comprehensive evaluation framework, allowing application designers to compare context models with respect to a given target application; in particular we stress the analysis of those features which are relevant for the problem of data tailoring. The contribution of this paper is twofold: a general analysis framework for context models and an up-to-date comparison of the most interesting, data-oriented approaches available in the literature.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2418951",
                    "name": "C. Bolchini"
                },
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "bdc742a9f9870498a7fe00476708329ada8c1317",
            "title": "Emergent semantics and cooperation in multi-knowledge environments: The ESTEEM architecture",
            "abstract": "In the present global society, information has to be exchangeable in open and dynamic environments, where interacting peers do not necessarily share a common understanding of the world at hand, and do not have a complete picture of the context where the interaction occurs. In this paper, we present the Esteem approach and the related peer architecture for emergent semantics in dynamic and multi-knowledge environments. In Esteem, semantic communities are built around declared interests in the form of manifesto ontologies, and their autonomous nature is preserved by allowing a shared semantics to naturally emerge from peer interactions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49168317",
                    "name": "C. Aiello"
                },
                {
                    "authorId": "1696934",
                    "name": "R. Baldoni"
                },
                {
                    "authorId": "1706823",
                    "name": "D. Bianchini"
                },
                {
                    "authorId": "2418951",
                    "name": "C. Bolchini"
                },
                {
                    "authorId": "1970469",
                    "name": "Silvia Bonomi"
                },
                {
                    "authorId": "143916599",
                    "name": "S. Castano"
                },
                {
                    "authorId": "1772402",
                    "name": "T. Catarci"
                },
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "1716333",
                    "name": "V. D. Antonellis"
                },
                {
                    "authorId": "1680664",
                    "name": "A. Ferrara"
                },
                {
                    "authorId": "1713742",
                    "name": "M. Melchiori"
                },
                {
                    "authorId": "2636213",
                    "name": "D. Milano"
                },
                {
                    "authorId": "1732265",
                    "name": "S. Montanelli"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "40291891",
                    "name": "Antonella Poggi"
                },
                {
                    "authorId": "1775963",
                    "name": "Leonardo Querzoni"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "2121470",
                    "name": "R. Rossato"
                },
                {
                    "authorId": "1735822",
                    "name": "Denise Salvi"
                },
                {
                    "authorId": "1789318",
                    "name": "M. Scannapieco"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "70470059",
                    "name": "S. T. Pergiovanni"
                }
            ]
        },
        {
            "paperId": "ea91f3af19f5125d232410ca9b35b5dd8cd80ede",
            "title": "CADD: A Tool for Context Modeling and Data Tailoring",
            "abstract": "Nowadays user mobility requires that both content and services be appropriately personalized, in order for the (mobile) user to be always - and anywhere - equipped with the adequate share of data. Thus, the knowledge about the user, the adopted device and the environment, altogether called context, has to be taken into account in order to minimize the amount of information imported on mobile devices. The Context-ADDICT (Context-Aware Data Design, Integration, Customization and Tailoring) project aims at the definition of a complete framework which, starting from a methodology for the early design phases, supports mobile users through the dynamic hooking and integration of new, available information sources, so that an appropriate context-based portion of data, called data chunk, is delivered to their mobile devices. Data tailoring is needed because of two main reasons: the first is to keep the amount of information manageable, in order for the user not to be confused by too much, possibly noisy, information; the second is the frequent case when the mobile device is a small one, like a palm computer or a cellular phone, and thus only the most significant information must be kept on board. Context is, thus, key metainformation whose role becomes essential within the process of view design. Two main design-time activities are supported by our system in order to provide context-aware data filtering: 1) context design, based on a context model called context dimension tree and 2) definition of the relationship between each context and relevant portions of the application domain data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2418951",
                    "name": "C. Bolchini"
                },
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "ed2e710618dc668153cd50bdb92080320fdf0cc1",
            "title": "X-SOM Results for OAEI 2007",
            "abstract": "This paper summarizes the results of the X-SOM tool in the OAEI 2007 campaign. X-SOM is an extensible ontology mapper that combines various matching algorithms by means of a feed-forward neural network. X-SOM exploits logical reasoning and local heuristics to improve the quality of mappings while guaranteeing their consistency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "0777d582b53a9f6ed99623d310a05414d1b2e675",
            "title": "Ontology-Based Information Tailoring",
            "abstract": "Current applications are often forced to filter the richness of datasources in order to reduce the information noise the user is subject to. We consider this aspect as a critical issue of applications, to be factorized at the data management level. The Context-ADDICT system, leveraging on ontology-based context and domain models, is able to personalize the data to be made available to the user by \"context-aware tailoring\". In this paper we present a formal approach to the definition of the relationship between context (represented by an appropriate context model) and application domain (modeled by a domain ontology). Once such relationship has been defined, we are able to work out the boundary of the portion of the domain relevant to a user in a certain context. We also sketch the implementation of a visual tool supporting the application designer in this modeling task",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "345ef1ed7dde3b9e12e295fc54f607dfa64d0d73",
            "title": "A survey of context models and a preliminary methodology for context driven data view definition",
            "abstract": "Very often we face the need of extracting appropriate data views from an integrated representation of the information space, and of defining, a posteriori, personalized views for the information stakeholders. The peer to peer scenario of the ESTEEM project introduces another interesting motivation behind the desire to define customized data views of a semantic community information, tailored on the peer\u2019s interests. This report presents our context model, called Context Dimension Tree, which plays a fundamental role in tailoring the information space according to the user\u2019s information needs, as well as an analysis of relevant features of context models, later used for classification and comparison.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2418951",
                    "name": "C. Bolchini"
                },
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "89eba12db00843ada2d338ddd672daf722afdf7e",
            "title": "Context Integration for Mobile Data Tailoring",
            "abstract": "Independent, heterogeneous, distributed, sometimes transient and mobile data sources produce an enormous amount of information that should be semantically integrated and filtered, or, as we say, tailored, based on the user\u2019s interests and context. Since both the user and the data sources can be mobile, and the communication might be unreliable, caching the information on the user device may become really useful. Therefore new challenges have to be faced such as: data filtering in a context-aware fashion, integration of not-known-in-advance data sources, automatic extraction of the semantics. We propose a novel system named Context-ADDICT (Context-Aware Data Design, Integration, Customization and Tailoring) able to deal with the described scenario. The system we are designing aims at tailoring the available information to the needs of the current user in the current context, in order to offer a more manageable amount of information; such information is to be cached on the user\u2019s device according to policies defined at design-time, to cope with data source transiency. This paper focuses on the information representation and tailoring problem and on the definition of the global architecture of the system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2418951",
                    "name": "C. Bolchini"
                },
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "37c2f347694c37bd79bdbdb38ba92940de6967c3",
            "title": "A context-aware methodology for very small data base design",
            "abstract": "The design of a Data Base to be resident on portable devices and embedded processors for professional systems requires considering both the device memory peculiarities and the mobility aspects, which are an essential feature of the embedded applications. Moreover, these devices are often part of a larger Information System, comprising fixed and mobile resources. We propose a complete methodology for designing Very Small Data Bases, from the identification of the device resident portions down to the choice of the physical data structure, optimizing the cost and power consumption of the Flash memory, which - in the greatest generality - constitutes the permanent storage of the device.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2418951",
                    "name": "C. Bolchini"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "8f13c717aa43b23d32a88bc2b676f2f348002b98",
            "title": "Requirements for Context-Dependent Mobile Access to Information Services",
            "abstract": "In this position paper we discuss information systems access in a mobile context-dependent user environment, characterized by multiple information sources, in particular w.r.t. the relationships between context and multiple sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "3656164",
                    "name": "A. Celentano"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "a188a58e621ccb23273d7bb66264a55a905fd7e8",
            "title": "XML document summarization: using XQuery for synopsis creation",
            "abstract": "This work presents a methodology to support approximate queries over massive and heterogeneous XML data sets, based on concise data statistics such as histograms or other statistical techniques. The basic idea for approximate answers is to store precomputed summaries of the XML data, also called synopses, and to query them instead of the original database, thus saving time and computational costs. In particular, We concentrate on a set of XQuery transformation rules for the construction of the synopses collection and for querying the synopsis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1742363",
                    "name": "S. Comai"
                },
                {
                    "authorId": "1691186",
                    "name": "S. Marrara"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "d6fef62b75cf4b1131af690564862dad167a4f69",
            "title": "PoLiDBMS: Design and Prototype Implementation of a DBMS for Portable Devices",
            "abstract": "Very Small DataBases (VSDB) is a methodology and a complete framework for database design and management in a complex environment where databases are distributed over different systems, from high-end servers to reduced-power portable devices. Within this framework the architecture of PoLiDBMS, a Portable Light Database Management System has been designed to be hosted on such portable devices, in order to efficiently manage the data stored in Flash EEPROM memory. A flexible and modular solution has been adopted with the aim of allowing the development of a system able to be customized in its features, depending on the needed functionality and the available processing power. The first prototype implementation provides all the elementary functionalities of a DBMS, supporting a reduced set of the SQL language that can be of interest in such a limited environment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2418951",
                    "name": "C. Bolchini"
                },
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "2137645",
                    "name": "M. Giorgetta"
                },
                {
                    "authorId": "33354551",
                    "name": "A. Giusti"
                },
                {
                    "authorId": "1680319",
                    "name": "A. Miele"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "2407820209de2e6920d48e006cb03ce84cb07fc8",
            "title": "Termination and Confluence by Rule Prioritization",
            "abstract": "An active database system is a DBMS endowed with active rules, i.e., stored procedures activated by the system when specific events occur. The processing of active rules is characterized by two important properties: termination and confluence. We say that the processing of a set of active rules terminates if, given any initial active database state, the execution of the rules does not continue indefinitely; it is confluent if, for any initial database state, the final state is not influenced by the order of execution of the rules. Finding sufficient conditions for these properties to hold is a nontrivial problem, and the lack of a structured theory for the design of a system of active rules makes the analysis of the two properties more difficult. In this work, we translate a set of rules from any of the existing systems into an internal format; then, we translate the active rules into logical clauses, taking into account the system's execution semantics, and transfer to the active process known simple results about termination and determinism available in the literature for deductive rules.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1742363",
                    "name": "S. Comai"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "cda528f9f02efd85eb680a523a035081313196df",
            "title": "The design of a web-geographical information system: an experience in the integration of territorial reclamation support services",
            "abstract": "The design of a Web-geographical information system strongly requires methodological and operational tools to deal with information distributed in multiple, autonomous and heterogeneous data sources, and a uniform data publishing methodology and policy over Internet web sites. In this chapter, we describe our experience for the activities of requirement analysis and conceptual design of the DEAFIN Web-geographical information system whose objective is to improve the quality and the comparability of information about available industrial vacant sites, coming from different regional data sources. Heterogeneity and web availability requirements have been taken into account in the system architecture design. The DEAFIN system is thus conceived as a federated web-based information system, capable of managing and providing access to all the regional relevant information in an integrated and complete fashion. Furthermore, since the data available by a given DEAFIN region partner can be both spatial data and alphanumeric data, for each regional component system in the DEAFIN system, a Web-GIS system is defined.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "2919717",
                    "name": "A. Belussi"
                },
                {
                    "authorId": "1716333",
                    "name": "V. D. Antonellis"
                },
                {
                    "authorId": "1702365",
                    "name": "M. Fugini"
                },
                {
                    "authorId": "37898418",
                    "name": "G. Pozzi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "47225189",
                    "name": "L. Tosi"
                }
            ]
        },
        {
            "paperId": "f76eae6536690fea10933447265b777e89a7a83a",
            "title": "Logical and physical design issues for smart card databases",
            "abstract": "The design of very small databases for smart cards and for portable embedded systems is deeply constrained by the peculiar features of the physical medium. We propose a joint approach to the logical and physical database design phases and evaluate several data structures with respect to the performance, power consumption, and endurance parameters of read/program operations on the Flash-EEPROM storage medium.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2418951",
                    "name": "C. Bolchini"
                },
                {
                    "authorId": "1723820",
                    "name": "F. Salice"
                },
                {
                    "authorId": "1785665",
                    "name": "F. Schreiber"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "02cbfb52089552a55d5a7ba0ec811d01de2c6e83",
            "title": "Editorial message: special track on web and E-business applications",
            "abstract": "The World WideWeb has become the standard computing platform for the development of new-generation information systems. A new tide of Web-based e-business applications (such as corporate portals, network-based supply chains and market places, etc.) is driving the need for a more open, flexible and distributed infrastructure, together with appropriate development methodologies and theoretical settings. Today's web applications involve skills from many different areas of computer science, including databases, AI and agent based applications, programming languages and algorithms, distributed computing, information retrieval, semantic modeling, etc. For this reason we proposed a track on Web and E-business applications based on the following main topics: data models for the World Wide Web, Web data management, languages for the World Wide Web and XML, E- business and Web services, transactions on the World Wide Web, security and integrity issues for the WWW, query systems for the World Wide Web, management and storage of Web information, information retrieval and search engines for the Web, Web semantics, data integration over the World Wide Web, data-intensive applications on the World Wide Web, Web architectures.We received 30 submissions, which were extensively reviewed for originality, significance, technical soundness and clarity of presentation. The submitted papers covered most of the proposed topics. The number of submissions distributed on each continent has been the following: 16 from Europe (53%), 9 from North America (30%), 3 from Asia (10%), 1 from Africa (3%) and 1 from Australia (3%). 12 papers corresponding to the 40% of the submitted papers have been selected for presentation at the conference, with the following distribution: 7 from Europe, 3 from America, 1 from Asia and 1 from Australia.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1742363",
                    "name": "S. Comai"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "2f3ce824c11b8afb0592fbc03697e192e30ccd45",
            "title": "Temporal aspects of semistructured data",
            "abstract": "In many applications information about the history of data and their dynamic aspects are just as important as static information. The increasing amount of information accessible through the Web has presented new challenges to academic and industrial research on databases. In this context, data are either structured, when coming from relational or object-oriented databases, or partially or completely unstructured, when they consist of simple collections of text or image files. In the context of semistructured data, models and query languages must be extended in order to consider dynamic aspects. We present a model based on labeled graphs for representing changes in semistructured data and a SQL-like query language for querying it.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1728772",
                    "name": "Barbara Oliboni"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "4512c7d9196e3943d2f283d6f87c8622bfa16db3",
            "title": "Modeling users' navigation history",
            "abstract": "A lot of recent research has focused on methods of modeling Web users and on efficient ways to initialize and manage user models. In this paper, we present a new user modeling technique relying on a temporal, graph-based data model for semistructured information. Our technique can be used to filter WWW information and to make the Web experience personalized for the individual user. Moreover, such graph based data model also offers appropriate query languages, which allow user-defined queries either over the whole web site or over the personalized user site views.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145522617",
                    "name": "E. Damiani"
                },
                {
                    "authorId": "1728772",
                    "name": "Barbara Oliboni"
                },
                {
                    "authorId": "2710276",
                    "name": "E. Quintarelli"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "0c0689266cc53249f7e1d96dfd1c74741b665b37",
            "title": "Flexible query techniques for well-formed XML documents",
            "abstract": "An increasing number of XML (eXtensible Markup Language) documents available on the World Wide Web have no document type definition (DTD); still, most current query languages for XML rely on the user knowing the DTD when composing queries on XML data. In this paper, a flexible query model for well-formed XML documents without DTDs is outlined.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145522617",
                    "name": "E. Damiani"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "6a51f57410d7f2d078fc67d756e917f29c9cdeb2",
            "title": "Complex queries in XML-GL",
            "abstract": "ABSTRACT In this paper we describe the advanced features of XML-GL, a graphical query language over XML data which enables the retrieval and restructuring of XML documents in a simple and intuitive way. After a summary of the basic features on the language, we focus on complex queries, whose expressive power subsumes some well-known relational operations (e.g. union, di erence, cartesian product) but goes well beyond from these relatively simple operators.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "1742363",
                    "name": "S. Comai"
                },
                {
                    "authorId": "145522617",
                    "name": "E. Damiani"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "f347935f2f6d236ffe279919cae6e105e2e5581e",
            "title": "Fuzzy XML queries via context-based choice of aggregations",
            "abstract": "A flexible query model is presented for semi-structured information stored in well-formed XML documents, modeled as XML fuzzy graphs by computing estimates of the importance of the information associated to XML elements and attributes. The notion of fuzzy graph closure with threshold is then used to obtain a fuzzy extension of the XML fuzzy graphs\u2019 topological structure. Weights associated to closure arcs are computed as a conjunction of the importance values of the underlying arcs in the original graph, via a context-based choice of conjunctions. Query results are subgraphs of the resulting fuzzy closure graph, presented as a ranked list according to their degree of matching to the user query.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145522617",
                    "name": "E. Damiani"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1695555",
                    "name": "F. Fontana"
                }
            ]
        },
        {
            "paperId": "e33bdb60e06d709f0ec3621f98b230e4a1a3b81a",
            "title": "Semantics-aware querying in the WWW: the WG-Log Web query system",
            "abstract": "We introduce a general model of a Web query system, i.e. a set of integrated tools for the efficient and effective retrieval of hypermedia information on the World Wide Web. Then we describe a prototype environment based on WG-Log, a graph-oriented query and data description language for Web sites, focusing on metadata processing and storage.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1742363",
                    "name": "S. Comai"
                },
                {
                    "authorId": "145522617",
                    "name": "E. Damiani"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "eeb9200682f91dcfc06e3c2ded40bb20a82851e8",
            "title": "Flexible Queries to Semistructured Datasources: The WG-log Approach",
            "abstract": "A line of research is presented aimed at specifying both logical and navigational aspects of semi-structured data sources such as Web sites through the unifying notion of schema. Gracefully supporting schemata that are huge or subject to change, the WG-Log language allows for a uniform representation of queries and views, the latter expressing customized access structures to site information. A survey of related work and some directions for future research involving fuzzy query techniques are also outlined.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1742363",
                    "name": "S. Comai"
                },
                {
                    "authorId": "145522617",
                    "name": "E. Damiani"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "156bf77b2e9a9e76f8a8d0ebf6984930e80e36b2",
            "title": "Using WG-Log to Represent Semistructured Data: The Example of OEM",
            "abstract": "In this paper we discuss the possibility to represent synthetically semistructured information via a loose notion of schema: we say that data are semistructured when, although some structure is present, it is not as strict, regular, or complete as the one required by the traditional database management systems. Our proposal is based on WG-Log, a graph based language for the representation of WWW site information. We show how information encoded in a typical semistructured information model, as OEM, can be represented and queried by means of the WG-Log language, and how the TSIMMIS and WG-Log Web Query System can be integrated to allow site content exploration and exploitation by means of WG-Log.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1728772",
                    "name": "Barbara Oliboni"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "2082522950",
                    "name": "D. Veronese"
                }
            ]
        },
        {
            "paperId": "208c094ca6456a8fc0e9f2fe450d2f81fcfd1736",
            "title": "Using WG-Log to interact with heterogeneous data sources: the example of OEM",
            "abstract": "In this paper we discuss the possibility to represent synthetically semistructured information via a loose notion of schema: we say that data are semistruc-tured when, although some structure is present, it is not as strict, regular, or complete as the one required by the traditional database management systems. Our proposal is based on WG-Log, a graph based language for the representation of WWW site information. We show how information encoded in a typical semistructured information model, as OEM, can be represented and queried by means of the WG-Log language, and how the TSIMMIS and WG-Log Web Query System can be integrated to allow site content exploration and exploitation by means of WG-Log.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145522617",
                    "name": "E. Damiani"
                },
                {
                    "authorId": "1728772",
                    "name": "Barbara Oliboni"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "2082522950",
                    "name": "D. Veronese"
                }
            ]
        },
        {
            "paperId": "dda54caa831706b6199eb81e096d380aed4a8825",
            "title": "Dealing with Deviations in DBMSs: An Approach to Revise Consistency Constraints",
            "abstract": "Information systems are used to support the execution of business processes. They are usually developed on the top of database management systems (DBMSs), that store all data used in the business process. Consistency constraints on the database schema reeect the policies and procedures adopted in the business process: they are deened and enforced to guarantee system correctness. During system operation, some constraints may result obsolete because of changes in the procedures the database is supporting or of incomplete information introduced during the design of the system itself. In both cases, we say that the business process is deviating from its model, represented by the consistency constraints. In this paper we present a semi-automatic approach for updating consistency constraints when they result obsolete: information on constraint violations occurred during database operation are collected, and are used to identify new acceptable constraints. The goal of the approach is to provide support during system operation, by prompting the database administrator with a set of possible constraints modiications, whereby she/he can choose the appropriate one.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1759139",
                    "name": "E. D. Nitto"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "5e4cd6d03e1bb5629f833b243bc08c779c85c87b",
            "title": "A structured approach for the definition of the semantics of active databases",
            "abstract": "Active DBMSs couple database technology with rule-based programming to achieve the capability of reaction to database (and possibly external) stimuli, called events. The reactive capabilities of active databases are useful for a wide spectrum of applications, including security, view materialization, integrity checking and enforcement, or heterogeneous database integration, which makes this technology very promising for the near future. An active database system consists of a (passive) database and a set of active rules; the most popular form of active rule is the so-called event-condition-action (ECA) rule, which specifies an action to be executed upon the occurrence of one or more events, provided that a condition holds. Several active database systems and prototypes have been designed and partially or completely implemented. Unfortunately, they have been designed in a totally independent way, without the support of a common theory dictating the semantics of ECA rules, and thus often show different behaviors for rules with a similar form. In this article we consider a number of different possible options in the behavior of an active DBMS, based on a broad analysis of some of the best known implemented systems and prototypes. We encode these options in a user-readable form, called Extended ECA. A rule from any existing system can be rewritten in this formalism making all the semantic choices apparent. Then an EECA rule can be automatically translated into an internal (less readable) format, based on a logical style, which is called core format: the execution semantics of core rules is specified as the fixpoint of a simple transformation involving core rules. As an important premise to this research, a semantics for database updates and transactions has also been established, with respect to a notion of state that comprises both data and events. The article also presents an extensive bibliography on the subject of active databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "c8863db34c00f290819fb4c63bc185de625f3ad2",
            "title": "G-Log: A Graph-Based Query Language",
            "abstract": "We introduce G-Log, a declarative query language based on graphs, which combines the expressive power of logic, the modeling power of complex objects with identity and the representation power of graphs. G-Log is a nondeterministic complete query language, and thus allows the expression of a large variety of queries. We compare G-Log to well-known deductive database languages, and find that it is the only nondeterministic and computationally complete language that does not suffer from the copy-elimination problem. G-Log may be used in a totally declarative way, as well as in a \"more procedural\" way. Thus, it provides an intuitive, flexible graph-based formalism for nonexpert database users. >",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1717880",
                    "name": "J. Paredaens"
                },
                {
                    "authorId": "2203607",
                    "name": "P. Peelman"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "0d7720b09915365c4b6f52fff7a318d61aa75ca6",
            "title": "Automatic generation of production rules for integrity maintenance",
            "abstract": "In this article we present an approach to integrity maintenance, consisting of automatically generating production rules for integrity enforcement. Constraints are expressed as particular formulas of Domain Relational Calculus; they are automatically translated into a set of repair actions, encoded as production rules of an active database system. Production rules may be redundant (they enforce the same constraint in different ways) and conflicting (because repairing one constraint may cause the violation of another constraint). Thus, it is necessary to develop techniques for analyzing the properties of the set of active rules and for ensuring that any computation of production rules after any incorrect transaction terminates and produces a consistent database state.\nAlong these guidelines, we describe a specific architecture for constraint definition and enforcement. The components of the architecture include a Rule Generator, for producing all possible repair actions, and a Rule Analyzer and Selector, for producing a collection of production rules such that their execution after an incorrect transaction always terminates in a consistent state (possibly by rolling back the transaction); moreover, the needs of applications are modeled, so that integrity-enforcing rules reach the final state that better represents the original intentions of the transaction's supplier. Specific input from the designer can also drive the process and integrate or modify the rules generated automatically by the method. Experimental results of a prototype implementation of the proposed architecture are also described.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "1690714",
                    "name": "S. Paraboschi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "eca7c376c38bfa4d7cd04cc2a3c375f9a58fe92a",
            "title": "The LOGRES prototype",
            "abstract": "Logres is a new-generation database system integrating features from deductive and object-oriented databases [1, 2, 3, 4, 5]. The data model of Logres supports structural and semantic complexity through a rich collection of concepts from object-oriented models. The rule language allows for the manipulation of complex objects, the generation of new objects, and the definition of passive and active constraints. The application of set of rules to database states is controlled by means of qualifiers, which dictate the side effects of rules; qualifiers are the unique procedural feature of Logres, otherwise a fully declarative language.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2074209151",
                    "name": "Filippo Cacace"
                },
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "1399376369",
                    "name": "S. Crespi-Reghizzi"
                },
                {
                    "authorId": "1704595",
                    "name": "P. Fraternali"
                },
                {
                    "authorId": "1690714",
                    "name": "S. Paraboschi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "4a5518bcac6d963d84b3c24ebc8f9c9e4f72abdd",
            "title": "Datalog: A self-contained tutorial (Part 2)",
            "abstract": "Datalog is a database query language based on the logic programming paradigm; it has been designed and intensively studied through the last five years. We present the syntax and semantics of Datalog and its use for querying a relational database. Then, we classify optimization methods for achieving efficient evaluations of Datalog queries, and present the most relevant methods. Finally, we discuss various enhancements of Datalog, currently under study, and indicate what is still needed in order to extend Datalog`s applicability to the solution of real-life problems The aim of this paper is to provide a self-contained tutorial, addressed also to those members of the database community who are not too familiar with logic programming concepts. The paper is divided into two parts. Part One contains Sections 1,2,3, and the bibliography. In Section 2 we presented the foundations of Datalog: in Section 3 we explained how Datalog is used as a query language over relational database. Part Two is thus organized: Section 4 presents a taxonomy of the various optimization methods: in Section 5, we introduce some representative evaluation methods and optimization techniques. In Section 6, we present several formal extensions to Datalog. Finally, in Section 7 we attempt an evaluationmore\u00a0\u00bb of what will be required to make Datalog more attractive and usable. For the list of references, the reader is referred to Part 1 (Issue No. 4, pp. 20-38, 1991). 6 figs.\u00ab\u00a0less",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "1684745",
                    "name": "G. Gottlob"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "7795b0f90e638b19704591367d3cf5371def4506",
            "title": "Designing and Prototyping Data-Intensive Applications in the Logres and Algres Programming Environment",
            "abstract": "The authors present an environment and a methodology for the design and rapid prototyping of data-intensive software applications, i.e., applications which perform substantial retrieval and update activity on persistent data. In the approach, the application is formally specified using Logres, a database language which combines object-oriented data modeling and rule-based programming. These specifications are translated into Algres, an extended relational algebra, thus yielding a rapid executable prototype. Algres programs embedded into a conventional programming language interface may be converted to conventional programs operating on a commercial relational system. This methodology helps automate the conversion from declarative requirements to imperative code, performing several tasks fully automatically and reducing the probability of human errors, while integrity constraints and application specifications are expressed in a declarative language, at a very high level of abstraction. >",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2074209151",
                    "name": "Filippo Cacace"
                },
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "1399376369",
                    "name": "S. Crespi-Reghizzi"
                }
            ]
        },
        {
            "paperId": "74cb9a8ca208b1e5e75610fed8943516e85dd4ce",
            "title": "Interoperability between a rule-based database language and an object-oriented database language",
            "abstract": "A general framework is given for achieving interoperability between two database languages based respectively on a rule-based and an object-oriented paradigm. Interoperability is obtained, offering at the language level two primitives which allow making 'visible' a portion of the schema and the corresponding data from one paradigm to another. These primitives are supported by the extensible knowledge base management system which is under development in the ESPRIT-II project STRETCH.<<ETX>>",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145723719",
                    "name": "R. Zicari"
                },
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "be949df6177b5c94e06e2f0e73141de76cec917e",
            "title": "Supporting interoperability between new database languages",
            "abstract": "A general framework is described for achieving interoperability between two database languages, Logres and SOL (Stretch object-oriented language), respectively based on a rule-based and an object-oriented paradigm. A discussion is presented of primitives for exporting and importing data and procedures, distinguishing the export modalities by 'copy' and by 'reference'. Several clauses add semantics to the import/export instructions for excluding, renaming, dereferencing, coercing, and unnesting data, for specifying the signatures, and for structuring the output parameters of programs.<<ETX>>",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1408300013",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "2072035533",
                    "name": "R. Zicari"
                }
            ]
        },
        {
            "paperId": "001682ae81273b230eafcf9b87a4de552be1787e",
            "title": "Expert design of local area networks",
            "abstract": "The Expert LAN Designer (ELAND) is described. It is a rule-based expert system for configuring distributed information systems operating on a LAN. A general overview of the system and the motivations for major design choices is presented, and the modules comprising ELAND's inference engine are explained. An example session with ELAND is provided.<<ETX>>",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "8ed3193c79ddf1aeea4870d34176efa0dc1b9bc2",
            "title": "Integrating object-oriented data modelling with a rule-based programming paradigm",
            "abstract": "LOGRES is a new project for the development of extended database systems which is based on the integration of the object-oriented data modelling paradigm and of the rule-based approach for the specification of queries and updates.\nThe data model supports generalization hierarchies and object sharing, the rule-based language extends Datalog to support generalized type constructors (sets, multisets, and sequences), rule-based integrity constraints are automatically produced by analyzing schema definitions. Modularization is a fundamental feature, as modules encapsulate queries and updates, when modules are applied to a LOGRES database, their side effects can be controlled.\nThe LOGRES project is a follow-up of the ALGRES project, and takes advantage of the ALGRES programming environment for the development of a fast prototype.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2074209151",
                    "name": "Filippo Cacace"
                },
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "1399376369",
                    "name": "S. Crespi-Reghizzi"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "145723719",
                    "name": "R. Zicari"
                }
            ]
        },
        {
            "paperId": "a602f2a1571cf2e7cbdadbc2540e3751b6304f4b",
            "title": "ALGRES: An Extended Relational Database System for the Specification and Prototyping of Complex Applications",
            "abstract": "This paper illustrates by means of examples the functionalities offered by ALGRES: An advanced relational programming environment for the formal specification and rapid prototyping of data-intensive applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2074209151",
                    "name": "Filippo Cacace"
                },
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "1399376369",
                    "name": "S. Crespi-Reghizzi"
                },
                {
                    "authorId": "1684745",
                    "name": "G. Gottlob"
                },
                {
                    "authorId": "1688383",
                    "name": "G. Lamperti"
                },
                {
                    "authorId": "1709285",
                    "name": "L. Lavazza"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                },
                {
                    "authorId": "145723719",
                    "name": "R. Zicari"
                }
            ]
        },
        {
            "paperId": "fa1570dc4e7853c2c6d0ff21a1ac8327e4ebe4b5",
            "title": "What you Always Wanted to Know About Datalog (And Never Dared to Ask)",
            "abstract": "Datalog, a database query language based on the logic programming paradigm, is described. The syntax and semantics of Datalog and its use for querying a relational database are presented. Optimization methods for achieving efficient evaluations of Datalog queries are classified, and the most relevant methods are presented. Various improvements of Datalog currently under study are discussed, and what is still needed in order to extend Datalog's applicability to the solution of real-life problems is indicated. >",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "1684745",
                    "name": "G. Gottlob"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        },
        {
            "paperId": "67a4146b05abcdc87c583d8a1e9c8ea70f5f23ac",
            "title": "Optimization of Systems of Algebraic Equations for Evaluating Datalog Queries",
            "abstract": "A Datalog program can be translated into a system of fixpoint equations of relational algebra; this paper studies how such a system can be solved and optimized for a particular query. The paper presents a structured approach to optimization, by identifying several optimization steps and by studying solution methods for each step.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144161686",
                    "name": "S. Ceri"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        }
    ]
}