{
    "authorId": "2115657798",
    "papers": [
        {
            "paperId": "b043a9640b789d5e90ef409730c26ac5ccb884db",
            "title": "You Can't Ignore Either: Unifying Structure and Feature Denoising for Robust Graph Learning",
            "abstract": "Recent research on the robustness of Graph Neural Networks (GNNs) under noises or attacks has attracted great attention due to its importance in real-world applications. Most previous methods explore a single noise source, recovering corrupt node embedding by reliable structures bias or developing structure learning with reliable node features. However, the noises and attacks may come from both structures and features in graphs, making the graph denoising a dilemma and challenging problem. In this paper, we develop a unified graph denoising (UGD) framework to unravel the deadlock between structure and feature denoising. Specifically, a high-order neighborhood proximity evaluation method is proposed to recognize noisy edges, considering features may be perturbed simultaneously. Moreover, we propose to refine noisy features with reconstruction based on a graph auto-encoder. An iterative updating algorithm is further designed to optimize the framework and acquire a clean graph, thus enabling robust graph learning for downstream tasks. Our UGD framework is self-supervised and can be easily implemented as a plug-and-play module. We carry out extensive experiments, which proves the effectiveness and advantages of our method. Code is avalaible at https://github.com/YoungTimmy/UGD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2306954810",
                    "name": "Tianmeng Yang"
                },
                {
                    "authorId": "2307088145",
                    "name": "Jiahao Meng"
                },
                {
                    "authorId": "2314827647",
                    "name": "Min Zhou"
                },
                {
                    "authorId": "2152661290",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "92385001",
                    "name": "Xiangtai Li"
                },
                {
                    "authorId": "2253401742",
                    "name": "Yunhai Tong"
                }
            ]
        },
        {
            "paperId": "3b3e51e32bb7be2cf49a934751548d2e15bba251",
            "title": "Mitigating Semantic Confusion from Hostile Neighborhood for Graph Active Learning",
            "abstract": "Graph Active Learning (GAL), which aims to find the most informative nodes in graphs for annotation to maximize the Graph Neural Networks (GNNs) performance, has attracted many research efforts but remains non-trivial challenges. One major challenge is that existing GAL strategies may introduce semantic confusion to the selected training set, particularly when graphs are noisy. Specifically, most existing methods assume all aggregating features to be helpful, ignoring the semantically negative effect between inter-class edges under the message-passing mechanism. In this work, we present Semantic-aware Active learning framework for Graphs (SAG) to mitigate the semantic confusion problem. Pairwise similarities and dissimilarities of nodes with semantic features are introduced to jointly evaluate the node influence. A new prototype-based criterion and query policy are also designed to maintain diversity and class balance of the selected nodes, respectively. Extensive experiments on the public benchmark graphs and a real-world financial dataset demonstrate that SAG significantly improves node classification performances and consistently outperforms previous methods. Moreover, comprehensive analysis and ablation study also verify the effectiveness of the proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40047504",
                    "name": "Tianmeng Yang"
                },
                {
                    "authorId": "2152175543",
                    "name": "Min Zhou"
                },
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2112752675",
                    "name": "Zhe-Min Lin"
                },
                {
                    "authorId": "2288532",
                    "name": "Lujia Pan"
                },
                {
                    "authorId": "2068228300",
                    "name": "Bin Cui"
                },
                {
                    "authorId": "2054671931",
                    "name": "Yu Tong"
                }
            ]
        },
        {
            "paperId": "896e5cee54d50d7a1f981823b4627948610d72a5",
            "title": "Estimating Treatment Effects from Irregular Time Series Observations with Hidden Confounders",
            "abstract": "Causal analysis for time series data, in particular estimating individualized treatment effect (ITE), is a key task in many real world applications, such as finance, retail, healthcare, etc. Real world time series, i.e., large-scale irregular or sparse and intermittent time series, raise significant challenges to existing work attempting to estimate treatment effects. Specifically, the existence of hidden confounders can lead to biased treatment estimates and complicate the causal inference process. In particular, anomaly hidden confounders which exceed the typical range can lead to high variance estimates. Moreover, in continuous time settings with irregular samples, it is challenging to directly handle the dynamics of causality. In this paper, we leverage recent advances in Lipschitz regularization and neural controlled differential equations (CDE) to develop an effective and scalable solution, namely LipCDE, to address the above challenges. LipCDE can directly model the dynamic causal relationships between historical data and outcomes with irregular samples by considering the boundary of hidden confounders given by Lipschitz constrained neural networks. Furthermore, we conduct extensive experiments on both synthetic and real world datasets to demonstrate the effectiveness and scalability of LipCDE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "120783624",
                    "name": "Defu Cao"
                },
                {
                    "authorId": "1390029897",
                    "name": "James Enouen"
                },
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "19214393",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "27737939",
                    "name": "Chuizheng Meng"
                },
                {
                    "authorId": "2223921871",
                    "name": "Hao Niu"
                },
                {
                    "authorId": "47909531",
                    "name": "Yan Liu"
                }
            ]
        },
        {
            "paperId": "530dbd3247bdec8ee9b2513ebbe6b6df5e4d6055",
            "title": "Stylized Knowledge-Grounded Dialogue Generation via Disentangled Template Rewriting",
            "abstract": "Current Knowledge-Grounded Dialogue Generation (KDG) models specialize in producing rational and factual responses. However, to establish long-term relationships with users, the KDG model needs the capability to generate responses in a desired style or attribute. Thus, we study a new problem: Stylized Knowledge-Grounded Dialogue Generation (SKDG). It presents two challenges: (1) How to train a SKDG model where no  triples are available. (2) How to cohere with context and preserve the knowledge when generating a stylized response. In this paper, we propose a novel disentangled template rewriting (DTR) method which generates responses via combing disentangled style templates (from monolingual stylized corpus) and content templates (from KDG corpus). The entire framework is end-to-end differentiable and learned without supervision. Extensive experiments on two benchmarks indicate that DTR achieves a significant improvement on all evaluation metrics compared with previous state-of-the-art stylized dialogue generation methods. Besides, DTR achieves comparable performance with the state-of-the-art KDG methods in standard KDG evaluation setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112549330",
                    "name": "Qingfeng Sun"
                },
                {
                    "authorId": "46747953",
                    "name": "Can Xu"
                },
                {
                    "authorId": "2144026961",
                    "name": "Huang Hu"
                },
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2150683091",
                    "name": "Jian Miao"
                },
                {
                    "authorId": "2442662",
                    "name": "Xiubo Geng"
                },
                {
                    "authorId": "2145262885",
                    "name": "Yining Chen"
                },
                {
                    "authorId": "2152480910",
                    "name": "Fei Xu"
                },
                {
                    "authorId": "2086994543",
                    "name": "Daxin Jiang"
                }
            ]
        },
        {
            "paperId": "86f190a4b54c678a88d49639613e82f41462fab1",
            "title": "Binary Classification with Positive Labeling Sources",
            "abstract": "To create a large amount of training labels for machine learning models effectively and efficiently, researchers have turned to Weak Supervision (WS), which uses programmatic labeling sources rather than manual annotation. Existing works of WS for binary classification typically assume the presence of labeling sources that are able to assign both positive and negative labels to data in roughly balanced proportions. However, for many tasks of interest where there is a minority positive class, negative examples could be too diverse for developers to generate indicative labeling sources. Thus, in this work, we study the application of WS on binary classification tasks with positive labeling sources only. We propose WEAPO, a simple yet competitive WS method for producing training labels without negative labeling sources. On 10 benchmark datasets, we show WEAPO achieves the highest averaged performance in terms of both the quality of synthesized labels and the performance of the final classifier supervised with these labels. We incorporated the implementation of WEAPO into WRENCH, an existing benchmarking platform.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47540245",
                    "name": "Jieyu Zhang"
                },
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2152661290",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "2158951748",
                    "name": "Yang Luo"
                },
                {
                    "authorId": "143711421",
                    "name": "Alexander J. Ratner"
                }
            ]
        },
        {
            "paperId": "a28191fe486ee49bbcd3ede75fe7001b0cf9e511",
            "title": "Enhancing Self-Attention with Knowledge-Assisted Attention Maps",
            "abstract": "Large-scale pre-trained language models have attracted extensive attentions in the research community and shown promising results on various tasks of natural language processing. However, the attention maps, which record the attention scores between tokens in self-attention mechanism, are sometimes ineffective as they are learned implicitly without the guidance of explicit semantic knowledge. Thus, we aim to infuse explicit external knowledge into pre-trained language models to further boost their performance. Existing works of knowledge infusion largely depend on multi-task learning frameworks, which are inefficient and require large-scale re-training when new knowledge is considered. In this paper, we propose a novel and generic solution, KAM-BERT, which directly incorporates knowledge-generated attention maps into the self-attention mechanism. It requires only a few extra parameters and supports efficient fine-tuning once new knowledge is added. KAM-BERT achieves consistent improvements on various academic datasets for natural language understanding. It also outperforms other state-of-the-art methods which conduct knowledge infusion into transformer-based architectures. Moreover, we apply our model to an industry-scale ad relevance application and show its advantages in the real-world scenario.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152770689",
                    "name": "Jiangang Bai"
                },
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2152993452",
                    "name": "Hong Sun"
                },
                {
                    "authorId": "2116926074",
                    "name": "Ruonan Wu"
                },
                {
                    "authorId": "40047504",
                    "name": "Tianmeng Yang"
                },
                {
                    "authorId": "48475903",
                    "name": "Pengfei Tang"
                },
                {
                    "authorId": "120783624",
                    "name": "Defu Cao"
                },
                {
                    "authorId": "2175481206",
                    "name": "Mingliang Zhang1"
                },
                {
                    "authorId": "2054671931",
                    "name": "Yu Tong"
                },
                {
                    "authorId": "2152661290",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "2113829510",
                    "name": "Jing Bai"
                },
                {
                    "authorId": "2124601065",
                    "name": "Ruofei Zhang"
                },
                {
                    "authorId": "2118180377",
                    "name": "Hao Sun"
                },
                {
                    "authorId": null,
                    "name": "Wei Shen"
                }
            ]
        },
        {
            "paperId": "a4bc9b74604eb94f225f8100bc919ab3e2c1233c",
            "title": "Heat-RL: Online Model Selection for Streaming Time-Series Anomaly Detection",
            "abstract": "Time-series",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "41159120",
                    "name": "Luoxing Xiong"
                },
                {
                    "authorId": "2129402731",
                    "name": "Mingliang Zhang"
                },
                {
                    "authorId": "2053313632",
                    "name": "Hui Xue"
                },
                {
                    "authorId": "2115813040",
                    "name": "Qi Chen"
                },
                {
                    "authorId": "2152661290",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "2054671931",
                    "name": "Yu Tong"
                },
                {
                    "authorId": "3261801",
                    "name": "Congrui Huang"
                },
                {
                    "authorId": "1388682473",
                    "name": "Bixiong Xu"
                }
            ]
        },
        {
            "paperId": "df6e42e06d77ebc28b50492b1fd26b39c1bd4277",
            "title": "Convolution-Enhanced Evolving Attention Networks",
            "abstract": "Attention-based neural networks, such as Transformers, have become ubiquitous in numerous applications, including computer vision, natural language processing, and time-series analysis. In all kinds of attention networks, the attention maps are crucial as they encode semantic dependencies between input tokens. However, most existing attention networks perform modeling or reasoning based on representations, wherein the attention maps of different layers are learned separately without explicit interactions. In this paper, we propose a novel and generic evolving attention mechanism, which directly models the evolution of inter-token relationships through a chain of residual convolutional modules. The major motivations are twofold. On the one hand, the attention maps in different layers share transferable knowledge, thus adding a residual connection can facilitate the information flow of inter-token relationships across layers. On the other hand, there is naturally an evolutionary trend among attention maps at different abstraction levels, so it is beneficial to exploit a dedicated convolution-based module to capture this process. Equipped with the proposed mechanism, the convolution-enhanced evolving attention networks achieve superior performance in various applications, including time-series representation, natural language understanding, machine translation, and image classification. Especially on time-series representation tasks, Evolving Attention-enhanced Dilated Convolutional (EA-DC-) Transformer outperforms state-of-the-art models significantly, achieving an average of 17% improvement compared to the best SOTA. To the best of our knowledge, this is the first work that explicitly models the layer-wise evolution of attention maps. Our implementation is available at https://github.com/pkuyym/EvolvingAttention.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2152661290",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "80389349",
                    "name": "Zhuowan Li"
                },
                {
                    "authorId": "152770689",
                    "name": "Jiangang Bai"
                },
                {
                    "authorId": "2129402731",
                    "name": "Mingliang Zhang"
                },
                {
                    "authorId": "92385001",
                    "name": "Xiangtai Li"
                },
                {
                    "authorId": "119883573",
                    "name": "J. Yu"
                },
                {
                    "authorId": "2109269339",
                    "name": "Ce Zhang"
                },
                {
                    "authorId": "2115218570",
                    "name": "Gao Huang"
                },
                {
                    "authorId": "2054671931",
                    "name": "Yu Tong"
                }
            ]
        },
        {
            "paperId": "0fe8b49369d70a2be473435a82b01544704b3c9f",
            "title": "Evolving Attention with Residual Convolutions",
            "abstract": "Transformer is a ubiquitous model for natural language processing and has attracted wide attentions in computer vision. The attention maps are indispensable for a transformer model to encode the dependencies among input tokens. However, they are learned independently in each layer and sometimes fail to capture precise patterns. In this paper, we propose a novel and generic mechanism based on evolving attention to improve the performance of transformers. On one hand, the attention maps in different layers share common knowledge, thus the ones in preceding layers can instruct the attention in succeeding layers through residual connections. On the other hand, low-level and high-level attentions vary in the level of abstraction, so we adopt convolutional layers to model the evolutionary process of attention maps. The proposed evolving attention mechanism achieves significant performance improvement over various state-of-the-art models for multiple tasks, including image classification, natural language understanding and machine translation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2152661290",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "152770689",
                    "name": "Jiangang Bai"
                },
                {
                    "authorId": "2129402731",
                    "name": "Mingliang Zhang"
                },
                {
                    "authorId": "2113829510",
                    "name": "Jing Bai"
                },
                {
                    "authorId": "119883573",
                    "name": "J. Yu"
                },
                {
                    "authorId": "1776014",
                    "name": "Ce Zhang"
                },
                {
                    "authorId": "143983679",
                    "name": "Gao Huang"
                },
                {
                    "authorId": "8230405",
                    "name": "Yunhai Tong"
                }
            ]
        },
        {
            "paperId": "20fdafb68d0e69d193527a9a1cbe64e7e69a3798",
            "title": "Learning Multi-granularity Consecutive User Intent Unit for Session-based Recommendation",
            "abstract": "Session-based recommendation aims to predict a user's next action based on previous actions in the current session. The major challenge is to capture authentic and complete user preferences in the entire session. Recent work utilizes graph structure to represent the entire session and adopts Graph Neural Network (GNN) to encode session information. This modeling choice has been proved to be effective and achieved remarkable results. However, most of the existing studies only consider each item within the session independently and do not capture session semantics from a high-level perspective. Such limitation often leads to severe information loss and increases the difficulty of capturing long-range dependencies within a session. Intuitively, compared with individual items, a session snippet, i.e., a group of locally consecutive items, is able to provide supplemental user intents which are hardly captured by existing methods. In this work, we propose to learn multi-granularity consecutive user intent unit to improve the recommendation performance. Specifically, we creatively propose Multi-granularity Intent Heterogeneous Session Graph (MIHSG) which captures the interactions between different granularity intent units and relieves the burden of long-dependency. Moreover, we propose the Intent Fusion Ranking (IFR) module to compose the recommendation results from various granularity user intents. Compared with current methods that only leverage intents from individual items, IFR benefits from different granularity user intents to generate more accurate and comprehensive session representation, thus eventually boosting recommendation performance. We conduct extensive experiments on five session-based recommendation datasets and the results demonstrate the effectiveness of our method. Compared to current state-of-the-art methods, we achieve as large as 10.21% gain on HR@20 and 15.53% gain on MRR@20.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5765645",
                    "name": "Jiayan Guo"
                },
                {
                    "authorId": "2152661290",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "19214393",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "2156008371",
                    "name": "Yuan Zhang"
                },
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2113829510",
                    "name": "Jing Bai"
                },
                {
                    "authorId": "34721188",
                    "name": "Yan Zhang"
                }
            ]
        }
    ]
}