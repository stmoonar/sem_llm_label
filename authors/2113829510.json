{
    "authorId": "2113829510",
    "papers": [
        {
            "paperId": "a359d7f5c2c1cb714e909247fcacad078676012f",
            "title": "Time-aware Graph Structure Learning via Sequence Prediction on Temporal Graphs",
            "abstract": "Temporal Graph Learning, which aims to model the time-evolving nature of graphs, has gained increasing attention and achieved remarkable performance recently. However, in reality, graph structures are often incomplete and noisy, which hinders temporal graph networks (TGNs) from learning informative representations. Graph contrastive learning uses data augmentation to generate plausible variations of existing data and learn robust representations. However, rule-based augmentation approaches may be suboptimal as they lack learnability and fail to leverage rich information from downstream tasks. To address these issues, we propose a Time-aware Graph Structure Learning (TGSL) approach via sequence prediction on temporal graphs, which learns better graph structures for downstream tasks through adding potential temporal edges. In particular, it predicts time-aware context embedding based on previously observed interactions and uses the Gumble-Top-K to select the closest candidate edges to this context embedding. Additionally, several candidate sampling strategies are proposed to ensure both efficiency and diversity. Furthermore, we jointly learn the graph structure and TGNs in an end-to-end manner and perform inference on the refined graph. Extensive experiments on temporal link prediction benchmarks demonstrate that TGSL yields significant gains for the popular TGNs such as TGAT and GraphMixer, and it outperforms other contrastive learning methods on temporal graphs. We release the code at https://github.com/ViktorAxelsen/TGSL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2215405494",
                    "name": "Haozhen Zhang"
                },
                {
                    "authorId": "2149224458",
                    "name": "Xueting Han"
                },
                {
                    "authorId": "144432144",
                    "name": "Xi Xiao"
                },
                {
                    "authorId": "2113829510",
                    "name": "Jing Bai"
                }
            ]
        },
        {
            "paperId": "bb0770be24e49d1c11a61ee4c0cec2730a7256cc",
            "title": "AdapterGNN: Efficient Delta Tuning Improves Generalization Ability in Graph Neural Networks",
            "abstract": "Fine-tuning pre-trained models has recently yielded remarkable performance gains in graph neural networks (GNNs). In addition to pre-training techniques, inspired by the latest work in the natural language \ufb01elds, more recent work has shifted towards applying effective \ufb01ne-tuning approaches, such as parameter-ef\ufb01cient tuning (delta tuning). However, given the substantial differences between GNNs and transformer-based models, applying such approaches directly to GNNs proved to be less effective. In this paper, we present a comprehensive comparison of delta tuning techniques for GNNs and propose a novel delta tuning method speci\ufb01cally designed for GNNs, called AdapterGNN. AdapterGNN preserves the knowledge of the large pre-trained model and leverages highly expressive adapters for GNNs, which can adapt to downstream tasks effectively with only a few parameters, while also improving the model\u2019s generalization ability on the downstream tasks. Extensive experiments show that AdapterGNN achieves higher evaluation performance (outperforming full \ufb01ne-tuning by 1.4% and 5.5% in the chemistry and biology domains respectively, with only 5% of its parameters tuned) and lower generalization gaps compared to full \ufb01ne-tuning. Moreover, we empirically show that a larger GNN model can have a worse generalization ability, which differs from the trend observed in large language models. We have also provided a theoretical justi\ufb01cation for delta tuning can improve the generalization ability of GNNs by applying generalization bounds.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39541577",
                    "name": "Sheng Li"
                },
                {
                    "authorId": "2149224458",
                    "name": "Xueting Han"
                },
                {
                    "authorId": "2113829510",
                    "name": "Jing Bai"
                }
            ]
        },
        {
            "paperId": "a28191fe486ee49bbcd3ede75fe7001b0cf9e511",
            "title": "Enhancing Self-Attention with Knowledge-Assisted Attention Maps",
            "abstract": "Large-scale pre-trained language models have attracted extensive attentions in the research community and shown promising results on various tasks of natural language processing. However, the attention maps, which record the attention scores between tokens in self-attention mechanism, are sometimes ineffective as they are learned implicitly without the guidance of explicit semantic knowledge. Thus, we aim to infuse explicit external knowledge into pre-trained language models to further boost their performance. Existing works of knowledge infusion largely depend on multi-task learning frameworks, which are inefficient and require large-scale re-training when new knowledge is considered. In this paper, we propose a novel and generic solution, KAM-BERT, which directly incorporates knowledge-generated attention maps into the self-attention mechanism. It requires only a few extra parameters and supports efficient fine-tuning once new knowledge is added. KAM-BERT achieves consistent improvements on various academic datasets for natural language understanding. It also outperforms other state-of-the-art methods which conduct knowledge infusion into transformer-based architectures. Moreover, we apply our model to an industry-scale ad relevance application and show its advantages in the real-world scenario.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152770689",
                    "name": "Jiangang Bai"
                },
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2152993452",
                    "name": "Hong Sun"
                },
                {
                    "authorId": "2116926074",
                    "name": "Ruonan Wu"
                },
                {
                    "authorId": "40047504",
                    "name": "Tianmeng Yang"
                },
                {
                    "authorId": "48475903",
                    "name": "Pengfei Tang"
                },
                {
                    "authorId": "120783624",
                    "name": "Defu Cao"
                },
                {
                    "authorId": "2175481206",
                    "name": "Mingliang Zhang1"
                },
                {
                    "authorId": "2054671931",
                    "name": "Yu Tong"
                },
                {
                    "authorId": "2152661290",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "2113829510",
                    "name": "Jing Bai"
                },
                {
                    "authorId": "2124601065",
                    "name": "Ruofei Zhang"
                },
                {
                    "authorId": "2118180377",
                    "name": "Hao Sun"
                },
                {
                    "authorId": null,
                    "name": "Wei Shen"
                }
            ]
        },
        {
            "paperId": "ea5ab8fb7a82469f18287f963fc5437ed2fe5d73",
            "title": "An Automated Question-Answering Framework Based on Evolution Algorithm",
            "abstract": "Building a deep learning model for a Question-Answering (QA) task requires a lot of human effort, it may need several months to carefully tune various model architectures and find a best one. It's even harder to find different excellent models for multiple datasets. Recent works show that the best model structure is related to the dataset used, and one single model cannot adapt to all tasks. In this paper, we propose an automated Question-Answering framework, which could automatically adjust network architecture for multiple datasets. Our framework is based on an innovative evolution algorithm, which is stable and suitable for multiple dataset scenario. The evolution algorithm for search combine prior knowledge into initial population and use a performance estimator to avoid inefficient mutation by predicting the performance of candidate model architecture. The prior knowledge used in initial population could improve the final result of the evolution algorithm. The performance estimator could quickly filter out models with bad performance in population as the number of trials increases, to speed up the convergence. Our framework achieves 78.9 EM and 86.1 F1 on SQuAD 1.1, 69.9 EM and 72.5 F1 on SQuAD 2.0. On NewsQA dataset, the found model achieves 47.0 EM and 62.9 F1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110171536",
                    "name": "Sinan Tan"
                },
                {
                    "authorId": "2053314215",
                    "name": "Hui Xue"
                },
                {
                    "authorId": "2054858849",
                    "name": "Qiyu Ren"
                },
                {
                    "authorId": "2145489220",
                    "name": "Huaping Liu"
                },
                {
                    "authorId": "2113829510",
                    "name": "Jing Bai"
                }
            ]
        },
        {
            "paperId": "0fe8b49369d70a2be473435a82b01544704b3c9f",
            "title": "Evolving Attention with Residual Convolutions",
            "abstract": "Transformer is a ubiquitous model for natural language processing and has attracted wide attentions in computer vision. The attention maps are indispensable for a transformer model to encode the dependencies among input tokens. However, they are learned independently in each layer and sometimes fail to capture precise patterns. In this paper, we propose a novel and generic mechanism based on evolving attention to improve the performance of transformers. On one hand, the attention maps in different layers share common knowledge, thus the ones in preceding layers can instruct the attention in succeeding layers through residual connections. On the other hand, low-level and high-level attentions vary in the level of abstraction, so we adopt convolutional layers to model the evolutionary process of attention maps. The proposed evolving attention mechanism achieves significant performance improvement over various state-of-the-art models for multiple tasks, including image classification, natural language understanding and machine translation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2152661290",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "152770689",
                    "name": "Jiangang Bai"
                },
                {
                    "authorId": "2129402731",
                    "name": "Mingliang Zhang"
                },
                {
                    "authorId": "2113829510",
                    "name": "Jing Bai"
                },
                {
                    "authorId": "119883573",
                    "name": "J. Yu"
                },
                {
                    "authorId": "1776014",
                    "name": "Ce Zhang"
                },
                {
                    "authorId": "143983679",
                    "name": "Gao Huang"
                },
                {
                    "authorId": "8230405",
                    "name": "Yunhai Tong"
                }
            ]
        },
        {
            "paperId": "20fdafb68d0e69d193527a9a1cbe64e7e69a3798",
            "title": "Learning Multi-granularity Consecutive User Intent Unit for Session-based Recommendation",
            "abstract": "Session-based recommendation aims to predict a user's next action based on previous actions in the current session. The major challenge is to capture authentic and complete user preferences in the entire session. Recent work utilizes graph structure to represent the entire session and adopts Graph Neural Network (GNN) to encode session information. This modeling choice has been proved to be effective and achieved remarkable results. However, most of the existing studies only consider each item within the session independently and do not capture session semantics from a high-level perspective. Such limitation often leads to severe information loss and increases the difficulty of capturing long-range dependencies within a session. Intuitively, compared with individual items, a session snippet, i.e., a group of locally consecutive items, is able to provide supplemental user intents which are hardly captured by existing methods. In this work, we propose to learn multi-granularity consecutive user intent unit to improve the recommendation performance. Specifically, we creatively propose Multi-granularity Intent Heterogeneous Session Graph (MIHSG) which captures the interactions between different granularity intent units and relieves the burden of long-dependency. Moreover, we propose the Intent Fusion Ranking (IFR) module to compose the recommendation results from various granularity user intents. Compared with current methods that only leverage intents from individual items, IFR benefits from different granularity user intents to generate more accurate and comprehensive session representation, thus eventually boosting recommendation performance. We conduct extensive experiments on five session-based recommendation datasets and the results demonstrate the effectiveness of our method. Compared to current state-of-the-art methods, we achieve as large as 10.21% gain on HR@20 and 15.53% gain on MRR@20.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5765645",
                    "name": "Jiayan Guo"
                },
                {
                    "authorId": "2152661290",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "19214393",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "2156008371",
                    "name": "Yuan Zhang"
                },
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2113829510",
                    "name": "Jing Bai"
                },
                {
                    "authorId": "34721188",
                    "name": "Yan Zhang"
                }
            ]
        },
        {
            "paperId": "5733233ea20498eba4afbe085038a067d4582f21",
            "title": "Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees",
            "abstract": "Pre-trained language models like BERT achieve superior performances in various NLP tasks without explicit consideration of syntactic information. Meanwhile, syntactic information has been proved to be crucial for the success of NLP applications. However, how to incorporate the syntax trees effectively and efficiently into pre-trained Transformers is still unsettled. In this paper, we address this problem by proposing a novel framework named Syntax-BERT. This framework works in a plug-and-play mode and is applicable to an arbitrary pre-trained checkpoint based on Transformer architecture. Experiments on various datasets of natural language understanding verify the effectiveness of syntax trees and achieve consistent improvement over multiple pre-trained models, including BERT, RoBERTa, and T5.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152770689",
                    "name": "Jiangang Bai"
                },
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2108952107",
                    "name": "Yiren Chen"
                },
                {
                    "authorId": "2152661290",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "2113829510",
                    "name": "Jing Bai"
                },
                {
                    "authorId": "119883573",
                    "name": "J. Yu"
                },
                {
                    "authorId": "8230405",
                    "name": "Yunhai Tong"
                }
            ]
        },
        {
            "paperId": "7ff74c7928a8bba4970303abbce380840bca1a52",
            "title": "Adaptive Transfer Learning on Graph Neural Networks",
            "abstract": "Graph neural networks (GNNs) is widely used to learn a powerful representation of graph-structured data. Recent work demonstrates that transferring knowledge from self-supervised tasks to downstream tasks could further improve graph representation. However, there is an inherent gap between self-supervised tasks and downstream tasks in terms of optimization objective and training data. Conventional pre-training methods may be not effective enough on knowledge transfer since they do not make any adaptation for downstream tasks. To solve such problems, we propose a new transfer learning paradigm on GNNs which could effectively leverage self-supervised tasks as auxiliary tasks to help the target task. Our methods would adaptively select and combine different auxiliary tasks with the target task in the fine-tuning stage. We design an adaptive auxiliary loss weighting model to learn the weights of auxiliary tasks by quantifying the consistency between auxiliary tasks and the target task. In addition, we learn the weighting model through meta-learning. Our methods can be applied to various transfer learning approaches, it performs well not only in multi-task learning but also in pre-training and fine-tuning. Comprehensive experiments on multiple downstream tasks demonstrate that the proposed methods can effectively combine auxiliary tasks with the target task and significantly improve the performance compared to state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149224458",
                    "name": "Xueting Han"
                },
                {
                    "authorId": "15766745",
                    "name": "Zhenhuan Huang"
                },
                {
                    "authorId": "49640821",
                    "name": "Bang An"
                },
                {
                    "authorId": "2113829510",
                    "name": "Jing Bai"
                }
            ]
        },
        {
            "paperId": "95a8b9dcd81fa30e583caccbac0d8f4eb41dd8a9",
            "title": "Attentive Knowledge-aware Graph Convolutional Networks with Collaborative Guidance for Personalized Recommendation",
            "abstract": "To alleviate data sparsity and cold-start problems of traditional recommender systems (RSs), incorporating knowledge graphs (KGs) to supplement auxiliary information has attracted considerable attention recently. However, simply integrating KGs in current KG-based RS models is not necessarily a guarantee to improve the recommendation performance, which may even weaken the holistic model capability. This is because the construction of these KGs is independent of the collection of historical user-item interactions; hence, information in these KGs may not always be helpful for recommendation to all users. In this paper, we propose attentive Knowledge-aware Graph convolutional networks with Collaborative Guidance for personalized Recommendation (CG-KGR). CG-KGR is a novel knowledge-aware recommendation model that enables ample and coherent learning of KGs and user-item interactions, via our proposed Collaborative Guidance Mechanism. Specifically, CG-KGR first encapsulates historical interactions to interactive information summarization. Then CG-KGR utilizes it as guidance to extract information out of KGs, which eventually provides more precise personalized recommendation. We conduct extensive experiments on four real-world datasets over two recommendation tasks, i.e., Top-K recommendation and Click-Through rate (CTR) prediction. The experimental results show that the CG-KGR model significantly outperforms recent state-of-the-art models by 1.4-27.0% in terms of Recall metric on Top-K recommendation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2681738",
                    "name": "Yankai Chen"
                },
                {
                    "authorId": "2152661290",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2113829510",
                    "name": "Jing Bai"
                },
                {
                    "authorId": "19214393",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "145310663",
                    "name": "Irwin King"
                }
            ]
        },
        {
            "paperId": "abc881f2c2c12716c4ccc462c74d2d99ca14c601",
            "title": "Graph Pointer Neural Networks",
            "abstract": "Graph Neural Networks (GNNs) have shown advantages in various graph-based applications. Most existing GNNs assume strong homophily of graph structure and apply permutation-invariant local aggregation of neighbors to learn a representation for each node. However, they fail to generalize to heterophilic graphs, where most neighboring nodes have different labels or features, and the relevant nodes are distant. Few recent studies attempt to address this problem by combining multiple hops of hidden representations of central nodes (i.e., multi-hop-based approaches) or sorting the neighboring nodes based on attention scores (i.e., ranking-based approaches). As a result, these approaches have some apparent limitations. On the one hand, multi-hop-based approaches do not explicitly distinguish relevant nodes from a large number of multi-hop neighborhoods, leading to a severe over-smoothing problem. On the other hand, ranking-based models do not joint-optimize node ranking with end tasks and result in sub-optimal solutions. In this work, we present Graph Pointer Neural Networks (GPNN) to tackle the challenges mentioned above. We leverage a pointer network to select the most relevant nodes from a large amount of multi-hop neighborhoods, which constructs an ordered sequence according to the relationship with the central node. 1D convolution is then applied to extract high-level features from the node sequence. The pointer-network-based ranker in GPNN is joint-optimized with other parts in an end-to-end manner. Extensive experiments are conducted on six public node classification datasets with heterophilic graphs. The results show that GPNN significantly improves the classification performance of state-of-the-art methods. In addition, analyses also reveal the privilege of the proposed GPNN in filtering out irrelevant neighbors and reducing over-smoothing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2122814619",
                    "name": "Tian-bao Yang"
                },
                {
                    "authorId": "2115657798",
                    "name": "Yujing Wang"
                },
                {
                    "authorId": "2068671746",
                    "name": "Z. Yue"
                },
                {
                    "authorId": "2152661290",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "8230405",
                    "name": "Yunhai Tong"
                },
                {
                    "authorId": "2113829510",
                    "name": "Jing Bai"
                }
            ]
        }
    ]
}