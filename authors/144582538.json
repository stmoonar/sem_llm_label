{
    "authorId": "144582538",
    "papers": [
        {
            "paperId": "09d434d02138ad4049be421bf2351de4aa295412",
            "title": "Alexa, play with robot: Introducing the First Alexa Prize SimBot Challenge on Embodied AI",
            "abstract": "The Alexa Prize program has empowered numerous university students to explore, experiment, and showcase their talents in building conversational agents through challenges like the SocialBot Grand Challenge and the TaskBot Challenge. As conversational agents increasingly appear in multimodal and embodied contexts, it is important to explore the affordances of conversational interaction augmented with computer vision and physical embodiment. This paper describes the SimBot Challenge, a new challenge in which university teams compete to build robot assistants that complete tasks in a simulated physical environment. This paper provides an overview of the SimBot Challenge, which included both online and offline challenge phases. We describe the infrastructure and support provided to the teams including Alexa Arena, the simulated environment, and the ML toolkit provided to teams to accelerate their building of vision and language models. We summarize the approaches the participating teams took to overcome research challenges and extract key lessons learned. Finally, we provide analysis of the performance of the competing SimBots during the competition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167861070",
                    "name": "Hangjie Shi"
                },
                {
                    "authorId": "2167616711",
                    "name": "Leslie Ball"
                },
                {
                    "authorId": "2028300167",
                    "name": "G. Thattai"
                },
                {
                    "authorId": "2187068732",
                    "name": "Desheng Zhang"
                },
                {
                    "authorId": "2144596247",
                    "name": "Lu Hu"
                },
                {
                    "authorId": "3193409",
                    "name": "Qiaozi Gao"
                },
                {
                    "authorId": "1831108414",
                    "name": "Suhaila Shakiah"
                },
                {
                    "authorId": "46757485",
                    "name": "Xiaofeng Gao"
                },
                {
                    "authorId": "2110665",
                    "name": "Aishwarya Padmakumar"
                },
                {
                    "authorId": "2119660672",
                    "name": "Bo Yang"
                },
                {
                    "authorId": "2210738933",
                    "name": "Cadence Chung"
                },
                {
                    "authorId": "2210732208",
                    "name": "Dinakar Guthy"
                },
                {
                    "authorId": "1732493",
                    "name": "G. Sukhatme"
                },
                {
                    "authorId": "2210732281",
                    "name": "Karthika Arumugam"
                },
                {
                    "authorId": "2210732608",
                    "name": "Matthew Wen"
                },
                {
                    "authorId": "2167638287",
                    "name": "Osman Ipek"
                },
                {
                    "authorId": "26882347",
                    "name": "P. Lange"
                },
                {
                    "authorId": "31606876",
                    "name": "Rohan Khanna"
                },
                {
                    "authorId": "31264280",
                    "name": "Shreyas Pansare"
                },
                {
                    "authorId": "144582538",
                    "name": "Vasu Sharma"
                },
                {
                    "authorId": "2256775738",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "1391844339",
                    "name": "C. Flagg"
                },
                {
                    "authorId": "34931641",
                    "name": "Daniel Pressel"
                },
                {
                    "authorId": "2167643644",
                    "name": "Lavina Vaz"
                },
                {
                    "authorId": "2115177932",
                    "name": "Luke Dai"
                },
                {
                    "authorId": "38774604",
                    "name": "Prasoon Goyal"
                },
                {
                    "authorId": "2167648845",
                    "name": "Sattvik Sahai"
                },
                {
                    "authorId": "2229954252",
                    "name": "Shaohua Liu"
                },
                {
                    "authorId": "2167771392",
                    "name": "Yao Lu"
                },
                {
                    "authorId": "1411423941",
                    "name": "Anna Gottardi"
                },
                {
                    "authorId": "2122825525",
                    "name": "Shui Hu"
                },
                {
                    "authorId": "2152801542",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "35834027",
                    "name": "Kate Bland"
                },
                {
                    "authorId": "151027894",
                    "name": "Heather Rocker"
                },
                {
                    "authorId": "2231135132",
                    "name": "James Jeun"
                },
                {
                    "authorId": "2210728984",
                    "name": "Yadunandana Rao"
                },
                {
                    "authorId": "2078507351",
                    "name": "Michael Johnston"
                },
                {
                    "authorId": "8621381",
                    "name": "Akshaya Iyengar"
                },
                {
                    "authorId": "33638380",
                    "name": "Arindam Mandal"
                },
                {
                    "authorId": "2104644641",
                    "name": "Premkumar Natarajan"
                },
                {
                    "authorId": "3306272",
                    "name": "R. Ghanadan"
                }
            ]
        },
        {
            "paperId": "5a9cb1b3dc4655218b3deeaf4a2417a9a8cd0891",
            "title": "DINOv2: Learning Robust Visual Features without Supervision",
            "abstract": "The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2093491",
                    "name": "M. Oquab"
                },
                {
                    "authorId": "2214523349",
                    "name": "Timoth'ee Darcet"
                },
                {
                    "authorId": "1752699898",
                    "name": "Th\u00e9o Moutakanni"
                },
                {
                    "authorId": "2047928985",
                    "name": "Huy Q. Vo"
                },
                {
                    "authorId": "23994377",
                    "name": "Marc Szafraniec"
                },
                {
                    "authorId": "2182694",
                    "name": "Vasil Khalidov"
                },
                {
                    "authorId": "2147013351",
                    "name": "Pierre Fernandez"
                },
                {
                    "authorId": "40864100",
                    "name": "Daniel Haziza"
                },
                {
                    "authorId": "1403239967",
                    "name": "Francisco Massa"
                },
                {
                    "authorId": "1388811741",
                    "name": "Alaaeldin El-Nouby"
                },
                {
                    "authorId": "38698856",
                    "name": "Mahmoud Assran"
                },
                {
                    "authorId": "2482072",
                    "name": "Nicolas Ballas"
                },
                {
                    "authorId": "2247475926",
                    "name": "Wojciech Galuba"
                },
                {
                    "authorId": "1410913697",
                    "name": "Russ Howes"
                },
                {
                    "authorId": "2319973",
                    "name": "Po-Yao (Bernie) Huang"
                },
                {
                    "authorId": "2530311",
                    "name": "Shang-Wen Li"
                },
                {
                    "authorId": "1806773",
                    "name": "Ishan Misra"
                },
                {
                    "authorId": "2066127975",
                    "name": "Michael G. Rabbat"
                },
                {
                    "authorId": "144582538",
                    "name": "Vasu Sharma"
                },
                {
                    "authorId": "2282478",
                    "name": "Gabriel Synnaeve"
                },
                {
                    "authorId": "2187601405",
                    "name": "Huijiao Xu"
                },
                {
                    "authorId": "1681054",
                    "name": "H. J\u00e9gou"
                },
                {
                    "authorId": "2599292",
                    "name": "J. Mairal"
                },
                {
                    "authorId": "1744868",
                    "name": "Patrick Labatut"
                },
                {
                    "authorId": "2319608",
                    "name": "Armand Joulin"
                },
                {
                    "authorId": "2329288",
                    "name": "Piotr Bojanowski"
                }
            ]
        },
        {
            "paperId": "9002a1c5422b3156efebdbb118101523e5184149",
            "title": "Alexa Arena: A User-Centric Interactive Platform for Embodied AI",
            "abstract": "We introduce Alexa Arena, a user-centric simulation platform for Embodied AI (EAI) research. Alexa Arena provides a variety of multi-room layouts and interactable objects, for the creation of human-robot interaction (HRI) missions. With user-friendly graphics and control mechanisms, Alexa Arena supports the development of gamified robotic tasks readily accessible to general human users, thus opening a new venue for high-efficiency HRI data collection and EAI system evaluation. Along with the platform, we introduce a dialog-enabled instruction-following benchmark and provide baseline results for it. We make Alexa Arena publicly available to facilitate research in building generalizable and assistive embodied agents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3193409",
                    "name": "Qiaozi Gao"
                },
                {
                    "authorId": "2028300167",
                    "name": "G. Thattai"
                },
                {
                    "authorId": "46757485",
                    "name": "Xiaofeng Gao"
                },
                {
                    "authorId": "1831108414",
                    "name": "Suhaila Shakiah"
                },
                {
                    "authorId": "31264280",
                    "name": "Shreyas Pansare"
                },
                {
                    "authorId": "144582538",
                    "name": "Vasu Sharma"
                },
                {
                    "authorId": "1732493",
                    "name": "G. Sukhatme"
                },
                {
                    "authorId": "2167861070",
                    "name": "Hangjie Shi"
                },
                {
                    "authorId": "2211083447",
                    "name": "Bofei Yang"
                },
                {
                    "authorId": "2210761679",
                    "name": "Desheng Zheng"
                },
                {
                    "authorId": "2210737878",
                    "name": "Lucy Hu"
                },
                {
                    "authorId": "2210732281",
                    "name": "Karthika Arumugam"
                },
                {
                    "authorId": "2122825525",
                    "name": "Shui Hu"
                },
                {
                    "authorId": "2210732608",
                    "name": "Matthew Wen"
                },
                {
                    "authorId": "2210732208",
                    "name": "Dinakar Guthy"
                },
                {
                    "authorId": "2210738933",
                    "name": "Cadence Chung"
                },
                {
                    "authorId": "31606876",
                    "name": "Rohan Khanna"
                },
                {
                    "authorId": "2167638287",
                    "name": "Osman Ipek"
                },
                {
                    "authorId": "2167616711",
                    "name": "Leslie Ball"
                },
                {
                    "authorId": "35834027",
                    "name": "Kate Bland"
                },
                {
                    "authorId": "151027894",
                    "name": "Heather Rocker"
                },
                {
                    "authorId": "2210728984",
                    "name": "Yadunandana Rao"
                },
                {
                    "authorId": "2078507351",
                    "name": "Michael Johnston"
                },
                {
                    "authorId": "3306272",
                    "name": "R. Ghanadan"
                },
                {
                    "authorId": "33638380",
                    "name": "Arindam Mandal"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "145603129",
                    "name": "P. Natarajan"
                }
            ]
        },
        {
            "paperId": "1cec3e33185a7c7e4f60cb85278320321facbc7a",
            "title": "PISA: PoIncar\u00e9 Saliency-Aware Interpolative Augmentation",
            "abstract": "Saliency-aware portion-wise mixup has proven to be an effective data augmentation technique for different modalities and tasks. However, it involves calculating the saliency over gradient vectors in the Euclidean space, representations that often possess complicated geometries and inherent hierarchical structure. We propose PISA, saliency-aware interpolative regularization operating in the hyperbolic space, to better capture the complex geometries of representations. To this end, we also formulate a saliency-aware mixup for speech signals. PISA outperforms existing state-of-the-art interpolative augmentation methods on 7 benchmark and low-resource datasets from the domains of speech signal processing and computer vision. PISA results in more stable training than existing data augmentation methods while being robust to adversarial attacks. It can be generalized across modalities, models and downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2069609589",
                    "name": "Vishwa Shah"
                },
                {
                    "authorId": "144144799",
                    "name": "Puneet Mathur"
                },
                {
                    "authorId": "144582538",
                    "name": "Vasu Sharma"
                },
                {
                    "authorId": "1699159",
                    "name": "Dinesh Manocha"
                }
            ]
        },
        {
            "paperId": "2009939f7f8adff27164debdb3c6f0455076b266",
            "title": "Tweet Based Reach Aware Temporal Attention Network for NFT Valuation",
            "abstract": "Non-Fungible Tokens (NFTs) are a relatively unexplored class of assets. Designing strategies to forecast NFT trends is an intricate task due to its extremely volatile nature. The market is largely driven by public sentiment and \"hype\", which in turn has a high correlation with conversations taking place on social media platforms like Twitter. Prior work done for modelling stock market data does not take into account the extent of impact certain highly influential tweets and their authors can have on the market. Building on these limitations and the nature of the NFT market, we propose a novel reach-aware temporal learning approach to make predictions for forecasting future trends in the NFT market. We perform experiments on a new dataset consisting of over 1.3 million tweets and 180 thousand NFT transactions spanning over 15 NFT collections curated by us. Our model (TA-NFT) outperforms other state-of-the-art methods by an average of 36%. Through extensive quantitative and ablative analysis, we demonstrate the ability of our approach as a practical method for predicting NFT trends.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51042088",
                    "name": "Ramit Sawhney"
                },
                {
                    "authorId": "71188587",
                    "name": "Megh Thakkar"
                },
                {
                    "authorId": "2161000447",
                    "name": "Ritesh Soun"
                },
                {
                    "authorId": "2157860264",
                    "name": "A. Neerkaje"
                },
                {
                    "authorId": "144582538",
                    "name": "Vasu Sharma"
                },
                {
                    "authorId": "2047009798",
                    "name": "Dipanwita Guhathakurta"
                },
                {
                    "authorId": "2275117",
                    "name": "S. Chava"
                }
            ]
        },
        {
            "paperId": "47d4151ba4ff2f34aa439af18bec9a30a4fc54e0",
            "title": "CH-MARL: A Multimodal Benchmark for Cooperative, Heterogeneous Multi-Agent Reinforcement Learning",
            "abstract": "We propose a multimodal (vision-and-language) benchmark for cooperative and heterogeneous multi-agent learning. We introduce a benchmark multimodal dataset with tasks involving collaboration between multiple simulated heterogeneous robots in a rich multi-room home environment. We provide an integrated learning framework, multimodal implementations of state-of-the-art multi-agent reinforcement learning techniques, and a consistent evaluation protocol. Our experiments investigate the impact of different modalities on multi-agent learning performance. We also introduce a simple message passing method between agents. The results suggest that multimodality introduces unique challenges for cooperative multi-agent learning and there is significant room for advancing multi-agent reinforcement learning methods in such settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144582538",
                    "name": "Vasu Sharma"
                },
                {
                    "authorId": "38774604",
                    "name": "Prasoon Goyal"
                },
                {
                    "authorId": "3002019",
                    "name": "Kaixiang Lin"
                },
                {
                    "authorId": "2028300167",
                    "name": "G. Thattai"
                },
                {
                    "authorId": "3193409",
                    "name": "Qiaozi Gao"
                },
                {
                    "authorId": "1732493",
                    "name": "G. Sukhatme"
                }
            ]
        },
        {
            "paperId": "de46871d33c5ceab19bcf05db31e6a16b3d5ba03",
            "title": "MAViL: Masked Audio-Video Learners",
            "abstract": "We present Masked Audio-Video Learners (MAViL) to train audio-visual representations. Our approach learns with three complementary forms of self-supervision: (1) reconstruction of masked audio and video input data, (2) intra- and inter-modal contrastive learning with masking, and (3) self-training by reconstructing joint audio-video contextualized features learned from the first two objectives. Pre-training with MAViL not only enables the model to perform well in audio-visual classification and retrieval tasks but also improves representations of each modality in isolation, without using information from the other modality for fine-tuning or inference. Empirically, MAViL sets a new state-of-the-art on AudioSet (53.1 mAP) and VGGSound (67.1% accuracy). For the first time, a self-supervised audio-visual model outperforms ones that use external supervision on these benchmarks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2319973",
                    "name": "Po-Yao (Bernie) Huang"
                },
                {
                    "authorId": "144582538",
                    "name": "Vasu Sharma"
                },
                {
                    "authorId": "2109084085",
                    "name": "Hu Xu"
                },
                {
                    "authorId": "52190116",
                    "name": "Chaitanya K. Ryali"
                },
                {
                    "authorId": "146884473",
                    "name": "Haoqi Fan"
                },
                {
                    "authorId": "3128506",
                    "name": "Yanghao Li"
                },
                {
                    "authorId": "2530311",
                    "name": "Shang-Wen Li"
                },
                {
                    "authorId": "134007132",
                    "name": "Gargi Ghosh"
                },
                {
                    "authorId": "153652147",
                    "name": "J. Malik"
                },
                {
                    "authorId": "2322150",
                    "name": "Christoph Feichtenhofer"
                }
            ]
        },
        {
            "paperId": "2c8fec98ce54da82fced829d56324bc7edfb88fb",
            "title": "Multimodal Behavioral Markers Exploring Suicidal Intent in Social Media Videos",
            "abstract": "Suicide is one of the leading causes of death in the modern world. In this digital age, individuals are increasingly using social media to express themselves and often use these platforms to express suicidal intent. Various studies have inspected suicidal intent behavioral markers in controlled environments but it is still unexplored if such markers will generalize to suicidal intent expressed on social media. In this work, we set out to study multimodal behavioral markers related to suicidal intent when expressed on social media videos. We explore verbal, acoustic and visual behavioral markers in the context of identifying individuals at higher risk of suicidal attempt. Our analysis reveals that frequent silences, slouched shoulders, rapid hand movements and profanity are predominant multimodal behavioral markers indicative of suicidal intent1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "144582538",
                    "name": "Vasu Sharma"
                },
                {
                    "authorId": "1380210469",
                    "name": "Vaibhav Vaibhav"
                },
                {
                    "authorId": "2065232527",
                    "name": "Mahmoud Alismail"
                },
                {
                    "authorId": "49933077",
                    "name": "Louis-Philippe Morency"
                }
            ]
        },
        {
            "paperId": "b7532ef7b67cb0661bf8df713b2d2329cb993353",
            "title": "Future of Wearable Devices Using IoT Synergy in AI",
            "abstract": "AI and IoT are the two terms which are predicted to be the future of technology; however they are being emerged in their new forms day by day. Various sensors are placed in the devices, which are then made light and wearable for humans, these devices keep a track of various aspects of human life.Wearables have given a new face to the healthcare of humans.The proposed research has been done over the wearable devices concerning health since they track steps, provide GPS facilities, find out calories burnt etc. but the breach which should be worked upon to make them much more useful is to set a system which not only calculates the amount of burnt calories burnt but also gives the ways to set yourself at your best health by providing you the guidance of your food intake to regulate the amount of the needful components of your health.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144582538",
                    "name": "Vasu Sharma"
                },
                {
                    "authorId": "2143459145",
                    "name": "Vineeta"
                },
                {
                    "authorId": "38126183",
                    "name": "S. Som"
                },
                {
                    "authorId": "2871903",
                    "name": "S. Khatri"
                }
            ]
        },
        {
            "paperId": "be74309535f4d4318332f39f3cb29d96e1c69f7f",
            "title": "Induced Attention Invariance: Defending VQA Models against Adversarial Attacks",
            "abstract": "Deep learning models are increasingly being deployed in a wide number of safety critical applications, which makes protecting them from adversarial attacks a topic of paramount importance. In this paper we study state of the art adversarial attacks and their effect on top performing Visual Question Answering (VQA) models. Since most modern VQA architectures rely heavily on the use of attention, we hypothesize that defending VQA models against adversarial attacks involves protecting the attention maps from distortion due to these attacks. Based on this hypothesis, we propose a new loss term which we name as \u2018Induced attention Invariance loss\u2019 (IAI) which is designed to reduce variations in the attention maps due to such distortions. We evaluate the advantages of this approach on a number of top performing VQA models and show that models trained with our proposed IAI loss are almost twice as robust as standard models against state of the art adversarial attacks. We further analyze the effect of adversarial attacks on attention maps and con\ufb01rm that the adversarial attacks typically distort the attention maps, leading them to focus on the incorrect image regions while also reducing their sharpness. In comparison, models trained with our proposed IAI loss show increased robustness to these distortions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144582538",
                    "name": "Vasu Sharma"
                },
                {
                    "authorId": "49761595",
                    "name": "Ankita Kalra"
                },
                {
                    "authorId": "69948163",
                    "name": "LP Morency"
                }
            ]
        }
    ]
}