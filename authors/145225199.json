{
    "authorId": "145225199",
    "papers": [
        {
            "paperId": "0346f60fbfe9dce881af0f3a8373fb785c8110af",
            "title": "FedWon: Triumphing Multi-domain Federated Learning Without Normalization",
            "abstract": "Federated learning (FL) enhances data privacy with collaborative in-situ training on decentralized clients. Nevertheless, FL encounters challenges due to non-independent and identically distributed (non-i.i.d) data, leading to potential performance degradation and hindered convergence. While prior studies predominantly addressed the issue of skewed label distribution, our research addresses a crucial yet frequently overlooked problem known as multi-domain FL. In this scenario, clients' data originate from diverse domains with distinct feature distributions, instead of label distributions. To address the multi-domain problem in FL, we propose a novel method called Federated learning Without normalizations (FedWon). FedWon draws inspiration from the observation that batch normalization (BN) faces challenges in effectively modeling the statistics of multiple domains, while existing normalization techniques possess their own limitations. In order to address these issues, FedWon eliminates the normalization layers in FL and reparameterizes convolution layers with scaled weight standardization. Through extensive experimentation on five datasets and five models, our comprehensive experimental results demonstrate that FedWon surpasses both FedAvg and the current state-of-the-art method (FedBN) across all experimental setups, achieving notable accuracy improvements of more than 10% in certain domains. Furthermore, FedWon is versatile for both cross-silo and cross-device FL, exhibiting robust domain generalization capability, showcasing strong performance even with a batch size as small as 1, thereby catering to resource-constrained devices. Additionally, FedWon can also effectively tackle the challenge of skewed label distribution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1900312976",
                    "name": "Weiming Zhuang"
                },
                {
                    "authorId": "145225199",
                    "name": "Lingjuan Lyu"
                }
            ]
        },
        {
            "paperId": "0e066d67cad7cdf036d8874d35a8348932cf7be7",
            "title": "DADFNet: Dual Attention and Dual Frequency-Guided Dehazing Network for Video-Empowered Intelligent Transportation",
            "abstract": "Visual surveillance technology is an indispensable functional component of advanced traffic management systems. It has been applied to perform traffic supervision tasks, such as object detection, tracking and recognition. However, adverse weather conditions, e.g., fog, haze and mist, pose severe challenges for video-based transportation surveillance. To eliminate the influences of adverse weather conditions, we propose a dual attention and dual frequency-guided dehazing network (termed DADFNet) for real-time visibility enhancement. It consists of a dual attention module (DAM) and a high-low frequency-guided sub-net (HLFN) to jointly consider the attention and frequency mapping to guide haze-free scene reconstruction. Extensive experiments on both synthetic and real-world images demonstrate the superiority of DADFNet over state-of-the-art methods in terms of visibility enhancement and improvement in detection accuracy. Furthermore, DADFNet only takes $6.3$ ms to process a 1,920 * 1,080 image on the 2080 Ti GPU, making it highly efficient for deployment in intelligent transportation systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143378699",
                    "name": "Yu Guo"
                },
                {
                    "authorId": "2124017717",
                    "name": "R. Liu"
                },
                {
                    "authorId": "26924116",
                    "name": "Jiangtian Nie"
                },
                {
                    "authorId": "145225199",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2943819",
                    "name": "Zehui Xiong"
                },
                {
                    "authorId": "145993626",
                    "name": "Jiawen Kang"
                },
                {
                    "authorId": "2186151237",
                    "name": "Han Yu"
                },
                {
                    "authorId": "1713586",
                    "name": "D. Niyato"
                }
            ]
        },
        {
            "paperId": "1432133bf51be0bcd42676ca916143e321e101f0",
            "title": "DEJA VU: Continual Model Generalization For Unseen Domains",
            "abstract": "In real-world applications, deep learning models often run in non-stationary environments where the target data distribution continually shifts over time. There have been numerous domain adaptation (DA) methods in both online and offline modes to improve cross-domain adaptation ability. However, these DA methods typically only provide good performance after a long period of adaptation, and perform poorly on new domains before and during adaptation - in what we call the\"Unfamiliar Period\", especially when domain shifts happen suddenly and significantly. On the other hand, domain generalization (DG) methods have been proposed to improve the model generalization ability on unadapted domains. However, existing DG works are ineffective for continually changing domains due to severe catastrophic forgetting of learned knowledge. To overcome these limitations of DA and DG in handling the Unfamiliar Period during continual domain shift, we propose RaTP, a framework that focuses on improving models' target domain generalization (TDG) capability, while also achieving effective target domain adaptation (TDA) capability right after training on certain domains and forgetting alleviation (FA) capability on past domains. RaTP includes a training-free data augmentation module to prepare data for TDG, a novel pseudo-labeling mechanism to provide reliable supervision for TDA, and a prototype contrastive alignment algorithm to align different domains for achieving TDG, TDA and FA. Extensive experiments on Digits, PACS, and DomainNet demonstrate that RaTP significantly outperforms state-of-the-art works from Continual DA, Source-Free DA, Test-Time/Online DA, Single DG, Multiple DG and Unified DA&DG in TDG, and achieves comparable TDA and FA capabilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2308913917",
                    "name": "Chenxi Liu"
                },
                {
                    "authorId": "2108631414",
                    "name": "Lixu Wang"
                },
                {
                    "authorId": "145225199",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2118831761",
                    "name": "Chen Sun"
                },
                {
                    "authorId": "144129720",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2152206866",
                    "name": "Qi Zhu"
                }
            ]
        },
        {
            "paperId": "15af33590302d09cf0b5a90e3f3fac26cc2bd6a6",
            "title": "MocoSFL: enabling cross-client collaborative self-supervised learning",
            "abstract": "Existing collaborative self-supervised learning (SSL) schemes are not suitable for cross-client applications because of their expensive computation and large local data requirements. To address these issues, we propose MocoSFL, a collaborative SSL framework based on Split Federated Learning (SFL) and Momentum Contrast (MoCo). In MocoSFL, the large backbone model is split into a small client-side model and a large server-side model, and only the small client-side model is processed locally on the client\u2019s local devices. MocoSFL has three key components: (i) vector concatenation which enables the use of small batch size and reduces computation and memory requirements by orders of magnitude; (ii) feature sharing that helps achieve high accuracy regardless of the quality and volume of local data; (iii) frequent synchronization that helps achieve better non-IID performance because of smaller local model divergence. For a 1,000-client case with non-IID data (each client only has data from 2 random classes of CIFAR-10), MocoSFL can achieve over 84% accuracy with ResNet-18 model. Next we present TAResSFL module that significantly improves the resistance to privacy threats and communication overhead with small sacrifice in accuracy for a MocoSFL system. On a Raspberry Pi 4B device, the MocoSFL-based scheme requires less than 1 MB of memory and less than 40 MB of communication, and consumes less than 5W power. The code is available at https:",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1492115539",
                    "name": "Jingtao Li"
                },
                {
                    "authorId": "145225199",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2952463",
                    "name": "Daisuke Iso"
                },
                {
                    "authorId": "143804422",
                    "name": "C. Chakrabarti"
                },
                {
                    "authorId": "145570895",
                    "name": "Michael Spranger"
                }
            ]
        },
        {
            "paperId": "242dbbef9e7f624525c57645f193e0b13a90ad44",
            "title": "When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions",
            "abstract": "The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations, challenges, and future directions. Through an exploration of the challenges faced by FL and FM individually and their interconnections, we aim to inspire future research directions that can further enhance both fields, driving advancements and propelling the development of privacy-preserving and scalable AI systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1900312976",
                    "name": "Weiming Zhuang"
                },
                {
                    "authorId": null,
                    "name": "Chen Chen"
                },
                {
                    "authorId": "145225199",
                    "name": "Lingjuan Lyu"
                }
            ]
        },
        {
            "paperId": "345c14dc6ba113f0705b537589da001a8a5c090e",
            "title": "Traffic Anomaly Prediction Based on Joint Static-Dynamic Spatio-Temporal Evolutionary Learning",
            "abstract": "Accurate traffic anomaly prediction offers an opportunity to save the wounded at the right location in time. However, the complex process of traffic anomaly is affected by both various static factors and dynamic interactions. The recent evolving representation learning provides a new possibility to understand this complicated process, but with challenges of imbalanced data distribution and heterogeneity of features. To tackle these problems, this paper proposes a spatio-temporal evolution model named SNIPERfor learning intricate feature interactions to predict traffic anomalies. Specifically, we design spatio-temporal encoders to transform spatio-temporal information into vector space indicating their natural relationship. Then, we propose a temporally dynamical evolving embedding method to pay more attention to rare traffic anomalies and develop an effective attention-based multiple graph convolutional network to formulate the spatially mutual influence from three different perspectives. The FC-LSTM is adopted to aggregate the heterogeneous features considering the spatio-temporal influences. Finally, a loss function is designed to overcome the \u2019over-smoothing\u2019 and solve the imbalanced data problem. Extensive experiments show that SNIPER averagely outperforms state-of-the-arts by 3.9%, 0.9%, 1.9% and 1.6% on Chicago datasets, and 2.4%, 0.6%, 2.6% and 1.3% on New York City datasets in metrics of AUC-PR, AUC-ROC, F1 score, and accuracy, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40134592",
                    "name": "Xiaoming Liu"
                },
                {
                    "authorId": "2156120236",
                    "name": "Zhanwei Zhang"
                },
                {
                    "authorId": "145225199",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2156119744",
                    "name": "Zhaohan Zhang"
                },
                {
                    "authorId": "2114810399",
                    "name": "Shuai Xiao"
                },
                {
                    "authorId": "2154740036",
                    "name": "Chao Shen"
                },
                {
                    "authorId": "2149668064",
                    "name": "P. Yu"
                }
            ]
        },
        {
            "paperId": "569f369ef0b2db7d6f6dae4f1b757064f1c0145f",
            "title": "Dimension-independent Certified Neural Network Watermarks via Mollifier Smoothing",
            "abstract": "Certi\ufb01ed Watermarks is the \ufb01rst to provide a watermark certi\ufb01cate against l 2 -norm watermark removal attacks, by leveraging the randomized smoothing techniques for certi\ufb01ed robustness to adversarial attacks. However, the randomized smoothing techniques suffer from hardness of certi\ufb01ed robustness in high-dimensional space against l p -norm attacks for large p ( p > 2 ). The certi\ufb01ed watermark method based on the randomized smoothing is no exception, i.e., fails to provide meaningful certi\ufb01cates in high-dimensional space against the l p -norm watermark removal attacks ( p > 2 ). By leveraging molli\ufb01er theory, this paper proposes a molli\ufb01er smoothing method with dimension-independent certi\ufb01ed radius of our proposed smooth classi\ufb01er, for conducting the certi\ufb01ed watermark problem against the l p - norm watermark removal attacks ( 1 \u2264 p \u2264 \u221e ) for high parameter dimension d . Based on partial differential equation (PDE) theory, an approximation of molli\ufb01er smoothing is developed to alleviate the inef\ufb01ciency of sampling and prediction in the randomized smoothing as well as numerical integration in the molli\ufb01er smoothing, while maintaining the certi\ufb01ed watermark against the l p - norm watermark removal attacks ( 1 \u2264 p \u2264 \u221e ).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48115953",
                    "name": "Jiaxiang Ren"
                },
                {
                    "authorId": "2145500063",
                    "name": "Yang Zhou"
                },
                {
                    "authorId": "103340106",
                    "name": "Jiayin Jin"
                },
                {
                    "authorId": "145225199",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2069088777",
                    "name": "Da Yan"
                }
            ]
        },
        {
            "paperId": "5c208a4565c9ce2935a518d06bf5f1e59de1eaed",
            "title": "Byzantine-Robust Learning on Heterogeneous Data via Gradient Splitting",
            "abstract": "Federated learning has exhibited vulnerabilities to Byzantine attacks, where the Byzantine attackers can send arbitrary gradients to a central server to destroy the convergence and performance of the global model. A wealth of robust AGgregation Rules (AGRs) have been proposed to defend against Byzantine attacks. However, Byzantine clients can still circumvent robust AGRs when data is non-Identically and Independently Distributed (non-IID). In this paper, we first reveal the root causes of performance degradation of current robust AGRs in non-IID settings: the curse of dimensionality and gradient heterogeneity. In order to address this issue, we propose GAS, a \\shorten approach that can successfully adapt existing robust AGRs to non-IID settings. We also provide a detailed convergence analysis when the existing robust AGRs are combined with GAS. Experiments on various real-world datasets verify the efficacy of our proposed GAS. The implementation code is provided in https://github.com/YuchenLiu-a/byzantine-gas.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116487134",
                    "name": "Yuchen Liu"
                },
                {
                    "authorId": null,
                    "name": "Chen Chen"
                },
                {
                    "authorId": "145225199",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2397264",
                    "name": "Fangzhao Wu"
                },
                {
                    "authorId": "2149344649",
                    "name": "Sai Wu"
                },
                {
                    "authorId": "1887510095",
                    "name": "Gang Chen"
                }
            ]
        },
        {
            "paperId": "6a72a8855b21bb02d955590e8dfd6e3b4516078f",
            "title": "Model Selection Based on Residual Dependence Measure",
            "abstract": "Choosing the most suitable model from a set of models is crucial. Based on the assumption that the noise and independent variables in the model are independent, the degree of fit of the model was determined by studying the correlation between the residuals of the fitting equation and independent variables. The dependence measure can measure the dependence relationship between random variables. Common dependence measures include the Canonical Correlation Coefficient (CCA) and Distance Correlation (DC). In this study, the feasibility of the standard correlation coefficient and distance correlation in a two-dimensional case is proven by numerical simulation experiments, and the robustness of different noises, noise intensity and model types are also demonstrated. On the Boston housing price dataset, ridge regression, lasso regression, Bayesian regression and other methods were used to obtain different fitting equations. By comparing the CCA and DC of the residuals and independent variables, the estimation equation of the ridge regression was found to be the best, which proved the feasibility of the method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145225199",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "5866121",
                    "name": "Yiming Ding"
                },
                {
                    "authorId": "50501478",
                    "name": "Yuan Wan"
                }
            ]
        },
        {
            "paperId": "6c7af165292825224c2851c38ce624104ab56706",
            "title": "International Workshop on Federated Learning for Distributed Data Mining",
            "abstract": "The past decade has witnessed wide applications of machine learning to various domains for decision-making, including crime detection, urban planning, drug discovery, and health monitoring, which benefited from surging data resources. As data collection in real-world applications is often done in different locations, being able to mine and discover knowledge from distributed data sources is an essential requirement for building powerful predictive models. However, directly uploading all data sources to an untrustworthy centralized data server for learning will lead to risks of privacy leakage. Federated Learning (FL) emerges as a decentralized learning framework that aggregates knowledge from distributed data without centralizing them, hence mitigating privacy risks. By hosting this workshop, we aim to attract a broad spectrum of audiences, including researchers and practitioners from academia and industry interested in the latest advances in FL. As an effort to advance the fundamental development of FL in data mining, this workshop will encourage ideas exchange on the trustworthiness, scalability, robustness, and broad applications of FL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110805917",
                    "name": "Junyuan Hong"
                },
                {
                    "authorId": "30451228",
                    "name": "Zhuangdi Zhu"
                },
                {
                    "authorId": "145225199",
                    "name": "Lingjuan Lyu"
                },
                {
                    "authorId": "2145500063",
                    "name": "Yang Zhou"
                },
                {
                    "authorId": "144223220",
                    "name": "Vishnu Naresh Boddeti"
                },
                {
                    "authorId": "2310573749",
                    "name": "Jiayu Zhou"
                }
            ]
        }
    ]
}