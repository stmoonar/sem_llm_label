{
    "authorId": "2087699735",
    "papers": [
        {
            "paperId": "d1e58774877071848dac227c08eb65843c190beb",
            "title": "CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process",
            "abstract": "Identifying the underlying time-delayed latent causal processes in sequential data is vital for grasping temporal dynamics and making downstream reasoning. While some recent methods can robustly identify these latent causal variables, they rely on strict assumptions about the invertible generation process from latent variables to observed data. However, these assumptions are often hard to satisfy in real-world applications containing information loss. For instance, the visual perception process translates a 3D space into 2D images, or the phenomenon of persistence of vision incorporates historical data into current perceptions. To address this challenge, we establish an identifiability theory that allows for the recovery of independent latent components even when they come from a nonlinear and non-invertible mix. Using this theory as a foundation, we propose a principled approach, CaRiNG, to learn the CAusal RepresentatIon of Non-invertible Generative temporal data with identifiability guarantees. Specifically, we utilize temporal context to recover lost latent information and apply the conditions in our theory to guide the training process. Through experiments conducted on synthetic datasets, we validate that our CaRiNG method reliably identifies the causal process, even when the generation process is non-invertible. Moreover, we demonstrate that our approach considerably improves temporal understanding and reasoning in practical applications.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2155315836",
                    "name": "Guan-Hong Chen"
                },
                {
                    "authorId": "2281910510",
                    "name": "Yifan Shen"
                },
                {
                    "authorId": "2144321586",
                    "name": "Zhenhao Chen"
                },
                {
                    "authorId": "2262495949",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "2116970666",
                    "name": "Yuewen Sun"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "2297195138",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                }
            ]
        },
        {
            "paperId": "0ef4993978f60b5065bac687746bc9857e2c99f7",
            "title": "Partial Identifiability for Domain Adaptation",
            "abstract": "Unsupervised domain adaptation is critical to many real-world applications where label information is unavailable in the target domain. In general, without further assumptions, the joint distribution of the features and the label is not identifiable in the target domain. To address this issue, we rely on the property of minimal changes of causal mechanisms across domains to minimize unnecessary influences of distribution shifts. To encode this property, we first formulate the data-generating process using a latent variable model with two partitioned latent subspaces: invariant components whose distributions stay the same across domains and sparse changing components that vary across domains. We further constrain the domain shift to have a restrictive influence on the changing components. Under mild conditions, we show that the latent variables are partially identifiable, from which it follows that the joint distribution of data and labels in the target domain is also identifiable. Given the theoretical insights, we propose a practical domain adaptation framework called iMSDA. Extensive experimental results reveal that iMSDA outperforms state-of-the-art domain adaptation algorithms on benchmark datasets, demonstrating the effectiveness of our framework.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2069275317",
                    "name": "Lingjing Kong"
                },
                {
                    "authorId": "25106675",
                    "name": "Shaoan Xie"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "8499589",
                    "name": "Yujia Zheng"
                },
                {
                    "authorId": "2155315836",
                    "name": "Guan-Hong Chen"
                },
                {
                    "authorId": "6616282",
                    "name": "P. Stojanov"
                },
                {
                    "authorId": "32662204",
                    "name": "Victor Akinwande"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                }
            ]
        },
        {
            "paperId": "1fae6cebac0e050d68ccd8718de780d69006644d",
            "title": "Salesforce CausalAI Library: A Fast and Scalable Framework for Causal Analysis of Time Series and Tabular Data",
            "abstract": "We introduce the Salesforce CausalAI Library, an open-source library for causal analysis using observational data. It supports causal discovery and causal inference for tabular and time series data, of discrete, continuous and heterogeneous types. This library includes algorithms that handle linear and non-linear causal relationships between variables, and uses multi-processing for speed-up. We also include a data generator capable of generating synthetic data with specified structural equation model for the aforementioned data formats and types, that helps users control the ground-truth causal process while investigating various algorithms. Finally, we provide a user interface (UI) that allows users to perform causal analysis on data without coding. The goal of this library is to provide a fast and flexible solution for a variety of problems in the domain of causality. This technical report describes the Salesforce CausalAI API along with its capabilities, the implementations of the supported algorithms, and experiments demonstrating their performance and speed. Our library is available at \\url{https://github.com/salesforce/causalai}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2309967",
                    "name": "Devansh Arpit"
                },
                {
                    "authorId": "2107314954",
                    "name": "M. Fern\u00e1ndez"
                },
                {
                    "authorId": "2215096385",
                    "name": "Chenghao Liu"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "50656252",
                    "name": "Wenjing Yang"
                },
                {
                    "authorId": "2183766446",
                    "name": "P. Josel"
                },
                {
                    "authorId": "71926704",
                    "name": "Shelby Heinecke"
                },
                {
                    "authorId": "2202701858",
                    "name": "Eric Hu"
                },
                {
                    "authorId": "46507194",
                    "name": "Haiquan Wang"
                },
                {
                    "authorId": "2202736952",
                    "name": "Stephen Hoi"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "2194598639",
                    "name": "Kun Zhang"
                },
                {
                    "authorId": "9200530",
                    "name": "Juan Carlos Niebles"
                }
            ]
        },
        {
            "paperId": "378ab7bf92fb24d7352a8a0772e69d6112518e48",
            "title": "On the Unlikelihood of D-Separation",
            "abstract": "Causal discovery aims to recover a causal graph from data generated by it; constraint based methods do so by searching for a d-separating conditioning set of nodes in the graph via an oracle. In this paper, we provide analytic evidence that on large graphs, d-separation is a rare phenomenon, even when guaranteed to exist, unless the graph is extremely sparse. We then provide an analytic average case analysis of the PC Algorithm for causal discovery, as well as a variant of the SGS Algorithm we call UniformSGS. We consider a set $V=\\{v_1,\\ldots,v_n\\}$ of nodes, and generate a random DAG $G=(V,E)$ where $(v_a, v_b) \\in E$ with i.i.d. probability $p_1$ if $ab$. We provide upper bounds on the probability that a subset of $V-\\{x,y\\}$ d-separates $x$ and $y$, conditional on $x$ and $y$ being d-separable; our upper bounds decay exponentially fast to $0$ as $|V| \\rightarrow \\infty$. For the PC Algorithm, while it is known that its worst-case guarantees fail on non-sparse graphs, we show that the same is true for the average case, and that the sparsity requirement is quite demanding: for good performance, the density must go to $0$ as $|V| \\rightarrow \\infty$ even in the average case. For UniformSGS, while it is known that the running time is exponential for existing edges, we show that in the average case, that is the expected running time for most non-existing edges as well.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3317442",
                    "name": "Itai Feigenbaum"
                },
                {
                    "authorId": "46507194",
                    "name": "Haiquan Wang"
                },
                {
                    "authorId": "71926704",
                    "name": "Shelby Heinecke"
                },
                {
                    "authorId": "9200530",
                    "name": "Juan Carlos Niebles"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "2309967",
                    "name": "Devansh Arpit"
                }
            ]
        },
        {
            "paperId": "4c7fbfdd777b67d70ae203ef9c8a6a64a7faf26a",
            "title": "REX: Rapid Exploration and eXploitation for AI Agents",
            "abstract": "In this paper, we propose an enhanced approach for Rapid Exploration and eXploitation for AI Agents called REX. Existing AutoGPT-style techniques have inherent limitations, such as a heavy reliance on precise descriptions for decision-making, and the lack of a systematic approach to leverage try-and-fail procedures akin to traditional Reinforcement Learning (RL). REX introduces an additional layer of rewards and integrates concepts similar to Upper Confidence Bound (UCB) scores, leading to more robust and efficient AI agent performance. This approach has the advantage of enabling the utilization of offline behaviors from logs and allowing seamless integration with existing foundation models while it does not require any model fine-tuning. Through comparative analysis with existing methods such as Chain-of-Thoughts(CoT) and Reasoning viA Planning(RAP), REX-based methods demonstrate comparable performance and, in certain cases, even surpass the results achieved by these existing techniques. Notably, REX-based methods exhibit remarkable reductions in execution time, enhancing their practical applicability across a diverse set of scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223748790",
                    "name": "Rithesh Murthy"
                },
                {
                    "authorId": "71926704",
                    "name": "Shelby Heinecke"
                },
                {
                    "authorId": "9200530",
                    "name": "Juan Carlos Niebles"
                },
                {
                    "authorId": "2223887365",
                    "name": "Zhiwei Liu"
                },
                {
                    "authorId": "2147380988",
                    "name": "Le Xue"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "22758695",
                    "name": "Yihao Feng"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "1752893100",
                    "name": "Akash Gokul"
                },
                {
                    "authorId": "2309967",
                    "name": "Devansh Arpit"
                },
                {
                    "authorId": "2115800155",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "2122258484",
                    "name": "P. M\u00f9i"
                },
                {
                    "authorId": "46507194",
                    "name": "Haiquan Wang"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "1702137",
                    "name": "S. Savarese"
                }
            ]
        },
        {
            "paperId": "81b10e64133e775dab53153cc82277d276efe1f7",
            "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
            "abstract": "Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "71926704",
                    "name": "Shelby Heinecke"
                },
                {
                    "authorId": "9200530",
                    "name": "Juan Carlos Niebles"
                },
                {
                    "authorId": "2223887365",
                    "name": "Zhiwei Liu"
                },
                {
                    "authorId": "22758695",
                    "name": "Yihao Feng"
                },
                {
                    "authorId": "2147380988",
                    "name": "Le Xue"
                },
                {
                    "authorId": "2223748790",
                    "name": "Rithesh Murthy"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "2108313930",
                    "name": "Jianguo Zhang"
                },
                {
                    "authorId": "2309967",
                    "name": "Devansh Arpit"
                },
                {
                    "authorId": "2115800155",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "2122258484",
                    "name": "P. M\u00f9i"
                },
                {
                    "authorId": "46507194",
                    "name": "Haiquan Wang"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "1702137",
                    "name": "S. Savarese"
                }
            ]
        },
        {
            "paperId": "99dadc987c90626e3c0b0adcf694b6a4b5bd3901",
            "title": "Temporally Disentangled Representation Learning under Unknown Nonstationarity",
            "abstract": "In unsupervised causal representation learning for sequential data with time-delayed latent causal influences, strong identifiability results for the disentanglement of causally-related latent variables have been established in stationary settings by leveraging temporal structure. However, in nonstationary setting, existing work only partially addressed the problem by either utilizing observed auxiliary variables (e.g., class labels and/or domain indexes) as side information or assuming simplified latent causal dynamics. Both constrain the method to a limited range of scenarios. In this study, we further explored the Markov Assumption under time-delayed causally related process in nonstationary setting and showed that under mild conditions, the independent latent components can be recovered from their nonlinear mixture up to a permutation and a component-wise transformation, without the observation of auxiliary variables. We then introduce NCTRL, a principled estimation framework, to reconstruct time-delayed latent causal variables and identify their relations from measured sequential data only. Empirical evaluations demonstrated the reliable identification of time-delayed latent causal influences, with our methodology substantially outperforming existing baselines that fail to exploit the nonstationarity adequately and then, consequently, cannot distinguish distribution shifts.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2262495949",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "2166103953",
                    "name": "Yewen Fan"
                },
                {
                    "authorId": "2262501910",
                    "name": "Xinshuai Dong"
                },
                {
                    "authorId": "2155315836",
                    "name": "Guan-Hong Chen"
                },
                {
                    "authorId": "9200530",
                    "name": "Juan Carlos Niebles"
                },
                {
                    "authorId": "2262446774",
                    "name": "Eric P. Xing"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                }
            ]
        },
        {
            "paperId": "ce212cb873a54e5716da53a66b10298ac013008a",
            "title": "BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents",
            "abstract": "The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs). An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions. Since the investigation of LAA is still very recent, limited explorations are available. Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones. Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action, \\textit{i.e.} BOLAA, where a controller manages the communication among multiple agents. We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs. Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both. We release our implementation code of LAAs to the public at \\url{https://github.com/salesforce/BOLAA}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223887365",
                    "name": "Zhiwei Liu"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "2108313930",
                    "name": "Jianguo Zhang"
                },
                {
                    "authorId": "2147380988",
                    "name": "Le Xue"
                },
                {
                    "authorId": "71926704",
                    "name": "Shelby Heinecke"
                },
                {
                    "authorId": "2223748790",
                    "name": "Rithesh Murthy"
                },
                {
                    "authorId": "22758695",
                    "name": "Yihao Feng"
                },
                {
                    "authorId": "5478513",
                    "name": "Zeyuan Chen"
                },
                {
                    "authorId": "9200530",
                    "name": "Juan Carlos Niebles"
                },
                {
                    "authorId": "2309967",
                    "name": "Devansh Arpit"
                },
                {
                    "authorId": "2115800155",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "2122258484",
                    "name": "P. M\u00f9i"
                },
                {
                    "authorId": "46507194",
                    "name": "Haiquan Wang"
                },
                {
                    "authorId": "2054594326",
                    "name": "Caiming Xiong"
                },
                {
                    "authorId": "1702137",
                    "name": "S. Savarese"
                }
            ]
        },
        {
            "paperId": "0d5103378a9f4f6e08bfcd364da207f93b31b8b7",
            "title": "Prompt Learning with Optimal Transport for Vision-Language Models",
            "abstract": "With the increasing attention to large vision-language models such as CLIP, there has been a significant amount of effort dedicated to building efficient prompts. Unlike conventional methods of only learning one single prompt, we propose to learn multiple comprehensive prompts to describe diverse characteristics of categories such as intrinsic attributes or extrinsic contexts. However, directly matching each prompt to the same visual feature is problematic, as it pushes the prompts to converge to one point. To solve this problem, we propose to apply optimal transport to match the vision and text modalities. Specifically, we first model images and the categories with visual and textual feature sets. Then, we apply a two-stage optimization strategy to learn the prompts. In the inner loop, we optimize the optimal transport distance to align visual features and prompts by the Sinkhorn algorithm, while in the outer loop, we learn the prompts by this distance from the supervised data. Extensive experiments are conducted on the few-shot recognition task and the improvement demonstrates the superiority of our method. The code is available at https://github.com/CHENGY12/PLOT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9230423",
                    "name": "Guangyi Chen"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "19214393",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "2144455457",
                    "name": "Xinyue Li"
                },
                {
                    "authorId": "39358728",
                    "name": "Yongming Rao"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                }
            ]
        },
        {
            "paperId": "4e421472493434f5563afde3d4734671ba4fa854",
            "title": "Temporally Disentangled Representation Learning",
            "abstract": "Recently in the field of unsupervised representation learning, strong identifiability results for disentanglement of causally-related latent variables have been established by exploiting certain side information, such as class labels, in addition to independence. However, most existing work is constrained by functional form assumptions such as independent sources or further with linear transitions, and distribution assumptions such as stationary, exponential family distribution. It is unknown whether the underlying latent variables and their causal relations are identifiable if they have arbitrary, nonparametric causal influences in between. In this work, we establish the identifiability theories of nonparametric latent causal processes from their nonlinear mixtures under fixed temporal causal influences and analyze how distribution changes can further benefit the disentanglement. We propose \\textbf{\\texttt{TDRL}}, a principled framework to recover time-delayed latent causal variables and identify their relations from measured sequential data under stationary environments and under different distribution shifts. Specifically, the framework can factorize unknown distribution shifts into transition distribution changes under fixed and time-varying latent causal relations, and under observation changes in observation. Through experiments, we show that time-delayed latent causal influences are reliably identified and that our approach considerably outperforms existing baselines that do not correctly exploit this modular representation of changes. Our code is available at: \\url{https://github.com/weirayao/tdrl}.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "9230423",
                    "name": "Guangyi Chen"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                }
            ]
        }
    ]
}