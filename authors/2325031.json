{
    "authorId": "2325031",
    "papers": [
        {
            "paperId": "22b1f605222ab4144db845aaa7e729b35f5f3dd1",
            "title": "Anytime-Lidar: Deadline-aware 3D Object Detection",
            "abstract": "In this work, we present a novel scheduling frame-work enabling anytime perception for deep neural network (DNN) based 3D object detection pipelines. We focus on computationally expensive region proposal network (RPN) and per-category multi-head detector components, which are common in 3D object detection pipelines, and make them deadline-aware. We propose a scheduling algorithm, which intelligently selects the subset of the components to make effective time and accuracy trade-off on the fly. We minimize accuracy loss of skipping some of the neural network sub-components by projecting previously detected objects onto the current scene through estimations. We apply our approach to a state-of-art 3D object detection network, PointPillars, and evaluate its performance on Jetson Xavier AGX using nuScenes dataset. Compared to the baselines, our approach significantly improve the network\u2019s accuracy under various deadline constraints.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1739404994",
                    "name": "Ahmet Soyyigit"
                },
                {
                    "authorId": "2325031",
                    "name": "Shuochao Yao"
                },
                {
                    "authorId": "1963388",
                    "name": "H. Yun"
                }
            ]
        },
        {
            "paperId": "85f69eadea7e4c30e6912dffc3498ec08337c832",
            "title": "Exploring Spherical Autoencoder for Spherical Video Content Processing",
            "abstract": "3D spherical content is increasingly presented in various applications (e.g., AR/MR/VR) for better users' immersiveness experience, yet today processing such spherical 3D content still mainly relies on the traditional 2D approaches after projection, leading to the distortion and/or loss of critical information. This study sets to explore methods to process spherical 3D content directly and more effectively. Using 360-degree videos as an example, we propose a novel approach called Spherical Autoencoder (SAE) for spherical video processing. Instead of projecting to a 2D space, SAE represents the 360-degree video content as a spherical object and employs encoding and decoding on the 360-degree video directly. Furthermore, to support the adoption of SAE on pervasive mobile devices that often have resource constraints, we further propose two optimizations on top of SAE.First, since the FoV (Field of View) prediction is widely studied and leveraged to transport only a portion of the content to the mobile device to save bandwidth and battery consumption, we design p-SAE, a SAE scheme with the partial view support that can utilize such FoV prediction. Second, since machine learning models are often compressed when running on mobile devices in order to reduce the processing load, which usually leads to degradation of output (e.g., video quality in SAE), we propose c-SAE by applying the compressive sensing theory into SAE to maintain the video quality when the model is compressed. Our extensive experiments show that directly incorporating and processing spherical signals is promising, and it outperforms the traditional approaches by a large margin. Both p-SAE and c-SAE show their effectiveness in delivering high quality videos (e.g., PSNR results) when used alone or combined together with model compression.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187398884",
                    "name": "Jin Zhou"
                },
                {
                    "authorId": "2157952367",
                    "name": "Na Li"
                },
                {
                    "authorId": null,
                    "name": "Yao Liu"
                },
                {
                    "authorId": "2325031",
                    "name": "Shuochao Yao"
                },
                {
                    "authorId": "2108194031",
                    "name": "Songqing Chen"
                }
            ]
        },
        {
            "paperId": "c8510031671052baf009197080469d513f67b26d",
            "title": "Self-Cueing Real-Time Attention Scheduling in Criticality-Aware Visual Machine Perception",
            "abstract": "This paper presents a self-cueing real-time frame-work for attention prioritization in AI-enabled visual perception systems that minimizes a notion of state uncertainty. By attention prioritization we refer to inspecting some parts of the scene before others in a criticality-aware fashion. By self-cueing, we refer to not needing external cueing sensors for prioritizing attention, thereby simplifying design. We show that attention prioritization saves resources, thus enabling more efficient and responsive real-time object tracking on resource-limited embedded platforms. The system consists of two components: First, an optical flow-based module decides on the regions to be viewed on a subframe level, as well as their criticality. Second, a novel batched proportional balancing (BPB) scheduling policy decides how to schedule these regions for inspection by a deep neural network (DNN), and how to parallelize execution on the GPU. We implement the system on an NVIDIA Jetson Xavier platform, and empirically demonstrate the superiority of the proposed architecture through an extensive evaluation using a real-word driving dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1390628682",
                    "name": "Shengzhong Liu"
                },
                {
                    "authorId": "9507379",
                    "name": "Xinzhe Fu"
                },
                {
                    "authorId": "2857477",
                    "name": "Maggie B. Wigness"
                },
                {
                    "authorId": "145776797",
                    "name": "P. David"
                },
                {
                    "authorId": "2325031",
                    "name": "Shuochao Yao"
                },
                {
                    "authorId": "2919287",
                    "name": "L. Sha"
                },
                {
                    "authorId": "1730531",
                    "name": "T. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "21738c379a7803394155bba24b8ee07e5fa5ce1c",
            "title": "Deep Compressive Offloading",
            "abstract": "Future mobile and embedded systems will be smarter and more user-friendly. They will perceive the physical environment, understand human context, and interact with end-users in a human-like fashion. Daily objects will be capable of leveraging sensor data to perform complex estimation and recognition tasks, such as recognizing visual inputs, understanding voice commands, tracking objects, and interpreting human actions. This raises important research questions on how to endow low-end embedded and mobile devices with the appearance of intelligence despite their resource limitations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2325031",
                    "name": "Shuochao Yao"
                },
                {
                    "authorId": "2124948781",
                    "name": "Jinyang Li"
                },
                {
                    "authorId": "153626198",
                    "name": "Dongxin Liu"
                },
                {
                    "authorId": "49981269",
                    "name": "Tianshi Wang"
                },
                {
                    "authorId": "1390628682",
                    "name": "Shengzhong Liu"
                },
                {
                    "authorId": "3395273",
                    "name": "Huajie Shao"
                },
                {
                    "authorId": "1730531",
                    "name": "T. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "49561857b5c515ed18b46b1f7147cc46df256d51",
            "title": "Computational Modeling of Hierarchically Polarized Groups by Structured Matrix Factorization",
            "abstract": "The paper extends earlier work on modeling hierarchically polarized groups on social media. An algorithm is described that 1) detects points of agreement and disagreement between groups, and 2) divides them hierarchically to represent nested patterns of agreement and disagreement given a structural guide. For example, two opposing parties might disagree on core issues. Moreover, within a party, despite agreement on fundamentals, disagreement might occur on further details. We call such scenarios hierarchically polarized groups. An (enhanced) unsupervised Non-negative Matrix Factorization (NMF) algorithm is described for computational modeling of hierarchically polarized groups. It is enhanced with a language model, and with a proof of orthogonality of factorized components. We evaluate it on both synthetic and real-world datasets, demonstrating ability to hierarchically decompose overlapping beliefs. In the case where polarization is flat, we compare it to prior art and show that it outperforms state of the art approaches for polarization detection and stance separation. An ablation study further illustrates the value of individual components, including new enhancements.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1630209136",
                    "name": "Dachun Sun"
                },
                {
                    "authorId": "1928291861",
                    "name": "Chaoqi Yang"
                },
                {
                    "authorId": "2124948781",
                    "name": "Jinyang Li"
                },
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "2325031",
                    "name": "Shuochao Yao"
                },
                {
                    "authorId": "3395273",
                    "name": "Huajie Shao"
                },
                {
                    "authorId": "153626198",
                    "name": "Dongxin Liu"
                },
                {
                    "authorId": "1390628682",
                    "name": "Shengzhong Liu"
                },
                {
                    "authorId": "49981269",
                    "name": "Tianshi Wang"
                },
                {
                    "authorId": "1730531",
                    "name": "T. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "6c44e93684410377035d11c59cb8e4a4f086e464",
            "title": "DyDiff-VAE: A Dynamic Variational Framework for Information Diffusion Prediction",
            "abstract": "This paper describes a novel diffusion model, DyDiff-VAE, for information diffusion prediction on social media. Given the initial content and a sequence of forwarding users, DyDiff-VAE aims to estimate the propagation likelihood for other potential users and predict the corresponding user rankings. Inferring user interests from diffusion data lies the foundation of diffusion prediction, because users often forward the information in which they are interested or the information from those who share similar interests. Their interests also evolve over time as the result of the dynamic social influence from neighbors and the time-sensitive information gained inside/outside the social media. Existing works fail to model users' intrinsic interests from the diffusion data and assume user interests remain static along the time. DyDiff-VAE advances the state of the art in two directions: (i) We propose a dynamic encoder to infer the evolution of user interests from observed diffusion data. (ii) We propose a dual attentive decoder to estimate the propagation likelihood by integrating information from both the initial cascade content and the forwarding user sequence. Extensive experiments on four real-world datasets from Twitter and Youtube demonstrate the advantages of the proposed model; we show that it achieves 43.3%relative gains over the best baseline on average. Moreover, it has the lowest run-time compared with recurrent neural network based models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "12318198",
                    "name": "Zijie Huang"
                },
                {
                    "authorId": "1390628682",
                    "name": "Shengzhong Liu"
                },
                {
                    "authorId": "3395273",
                    "name": "Huajie Shao"
                },
                {
                    "authorId": "153626198",
                    "name": "Dongxin Liu"
                },
                {
                    "authorId": "2124948781",
                    "name": "Jinyang Li"
                },
                {
                    "authorId": "49981269",
                    "name": "Tianshi Wang"
                },
                {
                    "authorId": "1630209136",
                    "name": "Dachun Sun"
                },
                {
                    "authorId": "2325031",
                    "name": "Shuochao Yao"
                },
                {
                    "authorId": "1730531",
                    "name": "T. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "800e273d2e122d81d576bec52d68b25283caaff0",
            "title": "Truth Discovery With Multi-Modal Data in Social Sensing",
            "abstract": "This article proposes unsupervised truth-finding algorithms that combine consideration of multi-modal content features with analysis of propagation patterns to evaluate the veracity of observations in social sensing applications. A key social sensing challenge is to develop effective algorithms for estimating both the reliability of sources and the veracity of their observations without prior knowledge. In contrast to prior solutions that use labeled examples to learn content features that are correlated with veracity, our approach is entirely unsupervised. Hence, given no prior training data, we jointly learn the importance of different content features together with the veracity of observations using propagation patterns as an indicator of perceived content reliability. A novel penalized expectation maximization (PEM) algorithm is proposed to improve the quality of estimation results for observations bolstered by multiple features. In addition, we develop a constrained expectation maximum likelihood with multiple features (CEM-MultiF) that introduces a novel constraint to boost the probability of correctness of some claims. Finally, we evaluate the performance of the proposed algorithms, called EM-Multi, CEM-Multi and PEM-MultiF, respectively, on real-world data sets collected from Twitter. The evaluation results demonstrate that the proposed algorithms outperform the existing fact-finding approaches, and offer tunable knobs for controlling robustness/performance trade-offs in the presence of malicious sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3395273",
                    "name": "Huajie Shao"
                },
                {
                    "authorId": "1630209136",
                    "name": "Dachun Sun"
                },
                {
                    "authorId": "2325031",
                    "name": "Shuochao Yao"
                },
                {
                    "authorId": "143843304",
                    "name": "Lu Su"
                },
                {
                    "authorId": "3623271",
                    "name": "Zhibo Wang"
                },
                {
                    "authorId": "153626198",
                    "name": "Dongxin Liu"
                },
                {
                    "authorId": "1390628682",
                    "name": "Shengzhong Liu"
                },
                {
                    "authorId": "1795727",
                    "name": "Lance M. Kaplan"
                },
                {
                    "authorId": "1730531",
                    "name": "T. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "b6e648d52aecad5479a71c0e12a1bf6c6e42b00f",
            "title": "CrossRoI: cross-camera region of interest optimization for efficient real time video analytics at scale",
            "abstract": "Video cameras are pervasively deployed in city scale for public good or community safety (i.e. traffic monitoring or suspected person tracking). However, analyzing large scale video feeds in real time is data intensive and poses severe challenges to today's network and computation systems. We present CrossRoI, a resource-efficient system that enables real time video analytics at scale via harnessing the videos content associations and redundancy across a fleet of cameras. CrossRoI exploits the intrinsic physical correlations of cross-camera viewing fields to drastically reduce the communication and computation costs. CrossRoI removes the repentant appearances of same objects in multiple cameras without harming comprehensive coverage of the scene. CrossRoI operates in two phases - an offline phase to establish cross-camera correlations, and an efficient online phase for real time video inference. Experiments on real-world video feeds show that CrossRoI achieves 42% ~ 65% reduction for network overhead and 25% ~ 34% reduction for response delay in real time video analytics applications with more than 99% query accuracy, when compared to baseline methods. If integrated with SotA frame filtering systems, the performance gains of CrossRoI reaches 50% ~ 80% (network overhead) and 33% ~ 61% (end-to-end delay).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1978772091",
                    "name": "Hongpeng Guo"
                },
                {
                    "authorId": "2325031",
                    "name": "Shuochao Yao"
                },
                {
                    "authorId": "2149232173",
                    "name": "Zhe Yang"
                },
                {
                    "authorId": "2116761530",
                    "name": "Qian Zhou"
                },
                {
                    "authorId": "1688353",
                    "name": "K. Nahrstedt"
                }
            ]
        },
        {
            "paperId": "d001c53c03ffcb8047ef8431c48ed67ef2a39777",
            "title": "Audio Keyword Reconstruction from On-Device Motion Sensor Signals via Neural Frequency Unfolding",
            "abstract": "In this paper, we present a novel deep neural network architecture that reconstructs the high-frequency audio of selected spoken human words from low-sampling-rate signals of (ego-)motion sensors, such as accelerometer and gyroscope data, recorded on everyday mobile devices. As the sampling rate of such motion sensors is much lower than the Nyquist rate of ordinary human voice (around 6kHz+), these motion sensor recordings suffer from a significant frequency aliasing effect. In order to recover the original high-frequency audio signal, our neural network introduces a novel layer, called the alias unfolding layer, specialized in expanding the bandwidth of an aliased signal by reversing the frequency folding process in the time-frequency domain. While perfect unfolding is known to be unrealizable, we leverage the sparsity of the original signal to arrive at a sufficiently accurate statistical approximation. Comprehensive experiments show that our neural network significantly outperforms the state of the art in audio reconstruction from motion sensor data, effectively reconstructing a pre-trained set of spoken keywords from low-frequency motion sensor signals (with a sampling rate of 100-400 Hz). The approach demonstrates the potential risk of information leakage from motion sensors in smart mobile devices.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49981269",
                    "name": "Tianshi Wang"
                },
                {
                    "authorId": "2325031",
                    "name": "Shuochao Yao"
                },
                {
                    "authorId": "1390628682",
                    "name": "Shengzhong Liu"
                },
                {
                    "authorId": "2124948781",
                    "name": "Jinyang Li"
                },
                {
                    "authorId": "153626198",
                    "name": "Dongxin Liu"
                },
                {
                    "authorId": "3395273",
                    "name": "Huajie Shao"
                },
                {
                    "authorId": "2144408471",
                    "name": "Ruijie Wang"
                },
                {
                    "authorId": "1730531",
                    "name": "T. Abdelzaher"
                }
            ]
        },
        {
            "paperId": "e3494cfc52917395051075f22c29d3a112a57842",
            "title": "Towards an Accurate Latency Model for Convolutional Neural Network Layers on GPUs",
            "abstract": "Convolutional Neural Networks (CNN) have shown great success in many sensing and recognition applications. However, the excessive resource demand remains a major barrier against their deployment on low-end devices. Optimizations, such as model compression, are thus a need for practical deployment. To fully exploit existing system resources, platform-aware optimizations emerged in recent years, where an execution-time model becomes a necessity. However, non-monotonicity over the network configuration space makes execution time modeling a challenging task. Data-driven approaches have the advantage of being portable over different platforms by treating the hardware and software stack as a black box but at the cost of extremely long profiling time. On the other hand, analytical models can be found in the architecture and system literature that do not need heavy profiling but require laborious analysis by domain experts. In this paper, we focus on building a general latency model for convolutional layers that account for the majority of the total execution time in CNN models. We identify two major non-linear modes in the relationship between latency and convolution parameters, and analyze the mechanism behind them. The resulting model has better interpretability and can reduce profiling workload. The evaluation results show that our model outperforms baselines on different platforms and CNN models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2124948781",
                    "name": "Jinyang Li"
                },
                {
                    "authorId": "2113552626",
                    "name": "Runyu Ma"
                },
                {
                    "authorId": "52205400",
                    "name": "Vikram Sharma Mailthody"
                },
                {
                    "authorId": "1415000647",
                    "name": "Colin Samplawski"
                },
                {
                    "authorId": "32671580",
                    "name": "Benjamin M. Marlin"
                },
                {
                    "authorId": "2108194031",
                    "name": "Songqing Chen"
                },
                {
                    "authorId": "2325031",
                    "name": "Shuochao Yao"
                },
                {
                    "authorId": "1730531",
                    "name": "T. Abdelzaher"
                }
            ]
        }
    ]
}