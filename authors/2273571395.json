{
    "authorId": "2273571395",
    "papers": [
        {
            "paperId": "2be343d63e5d1961a3ccb86870886fc0b202ec66",
            "title": "Serial Position Effects of Large Language Models",
            "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in zero-shot learning applications, generating responses to queries using only pre-training information without the need for additional fine-tuning. This represents a significant departure from traditional machine learning approaches. Previous research has indicated that LLMs may exhibit serial position effects, such as primacy and recency biases, which are well-documented cognitive biases in human psychology. Our extensive testing across various tasks and models confirms the widespread occurrence of these effects, although their intensity varies. We also discovered that while carefully designed prompts can somewhat mitigate these biases, their effectiveness is inconsistent. These findings underscore the significance of serial position effects during the inference process, particularly in scenarios where there are no ground truth labels, highlighting the need for greater focus on addressing these effects in LLM applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273571395",
                    "name": "Xiaobo Guo"
                },
                {
                    "authorId": "1918441",
                    "name": "Soroush Vosoughi"
                }
            ]
        },
        {
            "paperId": "49cddd8f5e37bc8c115fde4241d243afc418043f",
            "title": "JADS: A Framework for Self-supervised Joint Aspect Discovery and Summarization",
            "abstract": "To generate summaries that include multiple aspects or topics for text documents, most approaches use clustering or topic modeling to group relevant sentences and then generate a summary for each group. These approaches struggle to optimize the summarization and clustering algorithms jointly. On the other hand, aspect-based summarization requires known aspects. Our solution integrates topic discovery and summarization into a single step. Given text data, our Joint Aspect Discovery and Summarization algorithm (JADS) discovers aspects from the input and generates a summary of the topics, in one step. We propose a self-supervised framework that creates a labeled dataset by first mixing sentences from multiple documents (e.g., CNN/DailyMail articles) as the input and then uses the article summaries from the mixture as the labels. The JADS model outperforms the two-step baselines. With pretraining, the model achieves better performance and stability. Furthermore, embeddings derived from JADS exhibit superior clustering capabilities. Our proposed method achieves higher semantic alignment with ground truth and is factual.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273571395",
                    "name": "Xiaobo Guo"
                },
                {
                    "authorId": "2303653693",
                    "name": "Jay Desai"
                },
                {
                    "authorId": "1757518",
                    "name": "Srinivasan H. Sengamedu"
                }
            ]
        },
        {
            "paperId": "7348810c869647f1cac03bec29089c2b40c4bff1",
            "title": "Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in Disordered Texts",
            "abstract": "Aspect-based summarization has seen significant advancements, especially in structured text. Yet, summarizing disordered, large-scale texts, like those found in social media and customer feedback, remains a significant challenge. Current research largely targets predefined aspects within structured texts, neglecting the complexities of dynamic and disordered environments. Addressing this gap, we introduce Disordered-DABS, a novel benchmark for dynamic aspect-based summarization tailored to unstructured text. Developed by adapting existing datasets for cost-efficiency and scalability, our comprehensive experiments and detailed human evaluations reveal that Disordered-DABS poses unique challenges to contemporary summarization models, including state-of-the-art language models such as GPT-3.5.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273571395",
                    "name": "Xiaobo Guo"
                },
                {
                    "authorId": "1918441",
                    "name": "Soroush Vosoughi"
                }
            ]
        },
        {
            "paperId": "b3f2f689c210618920c3924a2b0ecdf5f6fce233",
            "title": "Enhanced Detection of Conversational Mental Manipulation Through Advanced Prompting Techniques",
            "abstract": "This study presents a comprehensive, long-term project to explore the effectiveness of various prompting techniques in detecting dialogical mental manipulation. We implement Chain-of-Thought prompting with Zero-Shot and Few-Shot settings on a binary mental manipulation detection task, building upon existing work conducted with Zero-Shot and Few- Shot prompting. Our primary objective is to decipher why certain prompting techniques display superior performance, so as to craft a novel framework tailored for detection of mental manipulation. Preliminary findings suggest that advanced prompting techniques may not be suitable for more complex models, if they are not trained through example-based learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2303404198",
                    "name": "Ivory Yang"
                },
                {
                    "authorId": "2273571395",
                    "name": "Xiaobo Guo"
                },
                {
                    "authorId": "2265458627",
                    "name": "Sean Xie"
                },
                {
                    "authorId": "1918441",
                    "name": "Soroush Vosoughi"
                }
            ]
        },
        {
            "paperId": "c58f2a7eaaad15de6da6186eb9532c9da3ac3c95",
            "title": "LOLAMEME: Logic, Language, Memory, Mechanistic Framework",
            "abstract": "The performance of Large Language Models has achieved superhuman breadth with unprecedented depth. At the same time, the language models are mostly black box models and the underlying mechanisms for performance have been evaluated using synthetic or mechanistic schemes. We extend current mechanistic schemes to incorporate Logic, memory, and nuances of Language such as latent structure. The proposed framework is called LOLAMEME and we provide two instantiations of LOLAMEME: LoLa and MeMe languages. We then consider two generative language model architectures: transformer-based GPT-2 and convolution-based Hyena. We propose the hybrid architecture T HEX and use LOLAMEME framework is used to compare three architectures. T HEX outperforms GPT-2 and Hyena on select tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2303653693",
                    "name": "Jay Desai"
                },
                {
                    "authorId": "2273571395",
                    "name": "Xiaobo Guo"
                },
                {
                    "authorId": "2287927370",
                    "name": "Srinivasan H. Sengamedu"
                }
            ]
        },
        {
            "paperId": "fe97864721b4c7fe04600bfbfade2706c0082ec7",
            "title": "MODABS: Multi-Objective Learning for Dynamic Aspect-Based Summarization",
            "abstract": "The rapid proliferation of online content necessitates effective summarization methods, among which dynamic aspect-based summarization stands out. Unlike its traditional counterpart, which assumes a fixed set of known aspects, this approach adapts to the varied aspects of the input text. We introduce a novel multi-objective learning framework employing a Longformer-Encoder-Decoder for this task. The framework optimizes aspect number prediction, minimizes disparity between generated and reference summaries for each aspect, and maximizes dissimilarity across aspect-specific summaries. Extensive experiments show our method significantly outperforms baselines on three diverse datasets, largely due to the effective alignment of generated and reference aspect counts without sacrificing single-aspect summarization quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273571395",
                    "name": "Xiaobo Guo"
                },
                {
                    "authorId": "1918441",
                    "name": "Soroush Vosoughi"
                }
            ]
        },
        {
            "paperId": "388a5f7e7def3b5af4c251a4ab435dc002498dfb",
            "title": "Length Does Matter: Summary Length can Bias Summarization Metrics",
            "abstract": "Establishing the characteristics of an effective summary is a complicated and often subjective endeavor. Consequently, the development of metrics for the summarization task has become a dynamic area of research within natural language processing. In this paper, we reveal that existing summarization metrics exhibit a bias toward the length of generated summaries. Our thorough experiments, conducted on a variety of datasets, metrics, and models, substantiate these findings. The results indicate that most metrics tend to favor longer summaries, even after accounting for other factors. To address this issue, we introduce a Bayesian normalization technique that effectively diminishes this bias. We demonstrate that our approach significantly improves the concordance between human annotators and the majority of metrics in terms of summary coherence 1 .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273571395",
                    "name": "Xiaobo Guo"
                },
                {
                    "authorId": "1918441",
                    "name": "Soroush Vosoughi"
                }
            ]
        },
        {
            "paperId": "5cad61bd6388aa540cf2215a3fc5d5a250a2984e",
            "title": "Capturing Topic Framing via Masked Language Modeling",
            "abstract": "Differential framing of issues can lead to divergent world views on important issues. This is especially true in domains where the information presented can reach a large audience, such as traditional and social media. Scalable and reliable measurement of such differential framing is an important first step in addressing them. In this work, based on the intuition that framing affects the tone and word choices in written language, we propose a framework for modeling the differential framing of issues through masked token prediction via large-scale fine-tuned language models (LMs). Specifically, we explore three key factors for our framework: 1) prompt generation methods for the masked token prediction; 2) methods for normalizing the output of fine-tuned LMs; 3) robustness to the choice of pre-trained LMs used for fine-tuning. Through experiments on a dataset of articles from traditional media outlets covering five diverse and politically polarized topics, we show that our framework can capture differential framing of these topics with high reliability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273571395",
                    "name": "Xiaobo Guo"
                },
                {
                    "authorId": "2227771",
                    "name": "Weicheng Ma"
                },
                {
                    "authorId": "1918441",
                    "name": "Soroush Vosoughi"
                }
            ]
        },
        {
            "paperId": "38a279ad33aa438b3eafb02d934049ab339853cf",
            "title": "Measuring Media Bias via Masked Language Modeling",
            "abstract": "Bias in news reporting can lead to tribalism and division on important issues. Scalable and reliable measurement of such biases is an important first step in addressing them. In this work, based on the intuition that media bias is captured by the tone and word choices in articles, we propose a framework for modeling the relative bias of media outlets through masked token prediction via large-scale pretrained masked language models fine-tuned on articles form news outlets. Through experiments on five diverse and politically polarized topics we show that our framework can capture media bias towards these topics with high reliability. Additionally, our experiments show that our framework is general, in that language models fine-tuned on one topic can be applied to other topics with little drop in performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273571395",
                    "name": "Xiaobo Guo"
                },
                {
                    "authorId": "2227771",
                    "name": "Weicheng Ma"
                },
                {
                    "authorId": "1918441",
                    "name": "Soroush Vosoughi"
                }
            ]
        },
        {
            "paperId": "da1121a8397d2a52ecbfd79b002b948af4b121e0",
            "title": "A Large-Scale Longitudinal Multimodal Dataset of State-Backed Information Operations on Twitter",
            "abstract": "This paper proposes a large-scale and comprehensive dataset of 28 sub-datasets of state-backed tweets and accounts affiliated with 14 different countries, spanning more than 3 years, and a corresponding \"negative\" dataset of background tweets from the same time period and on similar topics. To our knowledge, this is the first dataset that contains both state-sponsored propaganda tweets and carefully collected corresponding negative tweet datasets for so many countries spanning such a long period of time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273571395",
                    "name": "Xiaobo Guo"
                },
                {
                    "authorId": "1918441",
                    "name": "Soroush Vosoughi"
                }
            ]
        }
    ]
}