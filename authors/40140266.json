{
    "authorId": "40140266",
    "papers": [
        {
            "paperId": "9be636a98dd0345321b9120c37bc41ec6132ec5b",
            "title": "Ada2I: Enhancing Modality Balance for Multimodal Conversational Emotion Recognition",
            "abstract": "Multimodal Emotion Recognition in Conversations (ERC) is a typical multimodal learning task in exploiting various data modalities concurrently. Prior studies on effective multimodal ERC encounter challenges in addressing modality imbalances and optimizing learning across modalities. Dealing with these problems, we present a novel framework named Ada2I, which consists of two inseparable modules namely Adaptive Feature Weighting (AFW) and Adaptive Modality Weighting (AMW) for feature-level and modality-level balancing respectively via leveraging both Inter- and Intra-modal interactions. Additionally, we introduce a refined disparity ratio as part of our training optimization strategy, a simple yet effective measure to assess the overall discrepancy of the model's learning process when handling multiple modalities simultaneously. Experimental results validate the effectiveness of Ada2I with state-of-the-art performance compared to baselines on three benchmark datasets, particularly in addressing modality imbalances.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220630783",
                    "name": "Cam-Van Thi Nguyen"
                },
                {
                    "authorId": "2265722930",
                    "name": "The-Son Le"
                },
                {
                    "authorId": "2265646724",
                    "name": "Anh-Tuan Mai"
                },
                {
                    "authorId": "40140266",
                    "name": "Duc-Trong Le"
                }
            ]
        },
        {
            "paperId": "c621504ed6141b56da42a339f10a157639e9ae3f",
            "title": "Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition",
            "abstract": "Emotion recognition in conversation (ERC) is a crucial task in natural language processing and affective computing. This paper proposes MultiDAG+CL, a novel approach for Multimodal Emotion Recognition in Conversation (ERC) that employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual features within a unified framework. The model is enhanced by Curriculum Learning (CL) to address challenges related to emotional shifts and data imbalance. Curriculum learning facilitates the learning process by gradually presenting training samples in a meaningful order, thereby improving the model\u2019s performance in handling emotional variations and data imbalance. Experimental results on the IEMOCAP and MELD datasets demonstrate that the MultiDAG+CL models outperform baseline models. We release the code for and experiments: https://github.com/vanntc711/MultiDAG-CL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220630783",
                    "name": "Cam-Van Thi Nguyen"
                },
                {
                    "authorId": "2287875442",
                    "name": "Cao-Bach Nguyen"
                },
                {
                    "authorId": "2265579183",
                    "name": "Quang-Thuy Ha"
                },
                {
                    "authorId": "40140266",
                    "name": "Duc-Trong Le"
                }
            ]
        },
        {
            "paperId": "fe89175dc05835a59c1ad8dd675eee4a0d0c1124",
            "title": "Towards Efficient Pareto-optimal Utility-Fairness between Groups in Repeated Rankings",
            "abstract": "In this paper, we tackle the problem of computing a sequence of rankings with the guarantee of the Pareto-optimal balance between (1) maximizing the utility of the consumers and (2) minimizing unfairness between producers of the items. Such a multi-objective optimization problem is typically solved using a combination of a scalarization method and linear programming on bi-stochastic matrices, representing the distribution of possible rankings of items. However, the above-mentioned approach relies on Birkhoff-von Neumann (BvN) decomposition, of which the computational complexity is $\\mathcal{O}(n^5)$ with $n$ being the number of items, making it impractical for large-scale systems. To address this drawback, we introduce a novel approach to the above problem by using the Expohedron - a permutahedron whose points represent all achievable exposures of items. On the Expohedron, we profile the Pareto curve which captures the trade-off between group fairness and user utility by identifying a finite number of Pareto optimal solutions. We further propose an efficient method by relaxing our optimization problem on the Expohedron's circumscribed $n$-sphere, which significantly improve the running time. Moreover, the approximate Pareto curve is asymptotically close to the real Pareto optimal curve as the number of substantial solutions increases. Our methods are applicable with different ranking merits that are non-decreasing functions of item relevance. The effectiveness of our methods are validated through experiments on both synthetic and real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284985657",
                    "name": "Phuong Dinh Mai"
                },
                {
                    "authorId": "40140266",
                    "name": "Duc-Trong Le"
                },
                {
                    "authorId": "2284985721",
                    "name": "Tuan-Anh Hoang"
                },
                {
                    "authorId": "2284983126",
                    "name": "Dung D. Le"
                }
            ]
        },
        {
            "paperId": "1cb60a180fabc80dfae2e817a9175b4a5c2da71f",
            "title": "Self-MI: Efficient Multimodal Fusion via Self-Supervised Multi-Task Learning with Auxiliary Mutual Information Maximization",
            "abstract": "Multimodal representation learning poses significant challenges in capturing informative and distinct features from multiple modalities. Existing methods often struggle to exploit the unique characteristics of each modality due to unified multimodal annotations. In this study, we propose Self-MI in the self-supervised learning fashion, which also leverage Contrastive Predictive Coding (CPC) as an auxiliary technique to maximize the Mutual Information (MI) between unimodal input pairs and the multimodal fusion result with unimodal inputs. Moreover, we design a label generation module, $ULG_{MI}$ for short, that enables us to create meaningful and informative labels for each modality in a self-supervised manner. By maximizing the Mutual Information, we encourage better alignment between the multimodal fusion and the individual modalities, facilitating improved multimodal fusion. Extensive experiments on three benchmark datasets including CMU-MOSI, CMU-MOSEI, and SIMS, demonstrate the effectiveness of Self-MI in enhancing the multimodal fusion task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220630783",
                    "name": "Cam-Van Thi Nguyen"
                },
                {
                    "authorId": "2265634562",
                    "name": "Ngoc-Hoa Thi Nguyen"
                },
                {
                    "authorId": "40140266",
                    "name": "Duc-Trong Le"
                },
                {
                    "authorId": "2265579183",
                    "name": "Quang-Thuy Ha"
                }
            ]
        },
        {
            "paperId": "46a536da745f2ecb1ebe7435e0a9a4beffd244ed",
            "title": "HHMC: A Heterogeneous x Homogeneous Graph-Based Network for Multimodal Cross-Selling Recommendation",
            "abstract": "Currently, research models that effectively predict cross-selling products while utilizing multimodal data sources are limited, and similarly, models focusing on multimodal recommendation do not adequately address cross-selling. To address this gap, our study introduces the model HHMC: A Heterogeneous x Homogeneous Graph-based Network for Multimodal Cross-Selling Recommendation. This innovative approach leverages historical order's data and diverse multimodal data to recommend cross-selling products. The architecture of model HHMC is thoughtfully designed to explore potential relationships between user-item and item-item interactions while also improving the efficiency of item feature representation through the enrichment of multimodal data sources. Due to the scarcity of published datasets for the cross-selling recommendation problem, we utilized the well-known Instacart dataset to define and explore empirical directions for addressing this challenge. Experimental results demonstrate that HHMC surpasses widely used deep learning-based techniques, highlighting its potential to effectively address multimodal cross-selling recommendation problems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110493359",
                    "name": "Huy-Son Nguyen"
                },
                {
                    "authorId": "2265674396",
                    "name": "Tuan-Nghia Bui"
                },
                {
                    "authorId": "2265741927",
                    "name": "Long-Hai Nguyen"
                },
                {
                    "authorId": "30495322",
                    "name": "Duy-Cat Can"
                },
                {
                    "authorId": "2220630783",
                    "name": "Cam-Van Thi Nguyen"
                },
                {
                    "authorId": "40140266",
                    "name": "Duc-Trong Le"
                },
                {
                    "authorId": "2261405744",
                    "name": "Hoang-Quynh Le"
                }
            ]
        },
        {
            "paperId": "859ccb98f1ca79b1ddb1ff6d3f7a7766762a26df",
            "title": "Multimodal Learning with Incompleteness towards Multimodal Sentiment Analysis and Emotion Recognition Task",
            "abstract": "Multimodal machine learning tasks have gained significant popularity and demonstrated promising results in mul-timodal data analysis. However, existing multimodal approaches primarily focus on scenarios where all modalities have complete data, neglecting the challenges posed by missing data. In real-world settings, certain modalities may lack parallel sequences due to noise during data collection and preprocessing, leading to random missingness in each modality. In this paper, we propose an integrated model MM-Align+ that leverages information from three input modalities: audio, visual, and text, to address two important multimodal tasks: sentiment analysis and emotion recognition under missing data with varying rates. We conduct extensive experiments on three public datasets, namely MOSEI, MOSI, and MELD. The preliminary results demonstrate the promising performance of the MM-Align+ model and suggest potential for further improvements and novel insights.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220630783",
                    "name": "Cam-Van Thi Nguyen"
                },
                {
                    "authorId": "2266031461",
                    "name": "Huy-Sang Nguyen"
                },
                {
                    "authorId": "40140266",
                    "name": "Duc-Trong Le"
                },
                {
                    "authorId": "2265579183",
                    "name": "Quang-Thuy Ha"
                }
            ]
        },
        {
            "paperId": "c666cb640fb64b4382f5007c1c968204f7ccad69",
            "title": "Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction",
            "abstract": "Emotion recognition is a crucial task for human conversation understanding. It becomes more challenging with the notion of multimodal data, e.g., language, voice, and facial expressions. As a typical solution, the global- and the local context information are exploited to predict the emotional label for every single sentence, i.e., utterance, in the dialogue. Specifically, the global representation could be captured via modeling of cross-modal interactions at the conversation level. The local one is often inferred using the temporal information of speakers or emotional shifts, which neglects vital factors at the utterance level. Additionally, most existing approaches take fused features of multiple modalities in an unified input without leveraging modality-specific representations. Motivating from these problems, we propose the Relational Temporal Graph Neural Network with Auxiliary Cross-Modality Interaction (CORECT), an novel neural network framework that effectively captures conversation-level cross-modality interactions and utterance-level temporal dependencies with the modality-specific manner for conversation understanding. Extensive experiments demonstrate the effectiveness of CORECT via its state-of-the-art results on the IEMOCAP and CMU-MOSEI datasets for the multimodal ERC task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220630783",
                    "name": "Cam-Van Thi Nguyen"
                },
                {
                    "authorId": "2265646724",
                    "name": "Anh-Tuan Mai"
                },
                {
                    "authorId": "2265722930",
                    "name": "The-Son Le"
                },
                {
                    "authorId": "67032833",
                    "name": "Hai-Dang Kieu"
                },
                {
                    "authorId": "40140266",
                    "name": "Duc-Trong Le"
                }
            ]
        },
        {
            "paperId": "fddd487aed177267bbdd025e9a93b3fa06896b6d",
            "title": "GAT-FP: Addressing Imperfect Multimodal Learning using Graph Attention Networks and Feature Propagation",
            "abstract": "Multimodal learning tries to increase generalization performance by leveraging information from several data modalities. However, effectively integrating such multi-modality data remains a complex endeavor, particularly when dealing with incomplete data. In real-world scenarios, modalities are often not entirely absent but rather incomplete due to various external or internal factors. For instance, audio data can be corrupted by noise, and text data may suffer from inaccuracies stemming from automatic speech recognition errors. To address these challenges, we propose a novel framework for incomplete multimodal learning in conversational contexts, named GAT-FP. Our GAT-FP model incorporates two key graph neural network-based modules, namely, \u201cFeature Propagation\u201d and \u201cGraph Attention Network\u201d. These modules are designed to estimate missing features and discern the significance of interactions among incomplete feature nodes within the graph structure. To validate the effectiveness of our approach, we conduct extensive experiments on two well-established benchmark multimodal conversational datasets: IEMOCAP and MELD. The experimental results demonstrate that our GAT-FP model surpasses existing state-of-the-art methods in the realm of incomplete multimodal learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2270592275",
                    "name": "Nguyen Van Doan"
                },
                {
                    "authorId": "2271603134",
                    "name": "Trong Duc Nguyen"
                },
                {
                    "authorId": "2271573129",
                    "name": "Dat Tran Nguyen"
                },
                {
                    "authorId": "2220630783",
                    "name": "Cam-Van Thi Nguyen"
                },
                {
                    "authorId": "67032833",
                    "name": "Hai-Dang Kieu"
                },
                {
                    "authorId": "40140266",
                    "name": "Duc-Trong Le"
                }
            ]
        },
        {
            "paperId": "b84442d13fbb312013c5e6527212c0d0f47b3fd8",
            "title": "Memory-Based Method using Prototype Augmentation for Continual Relation Extraction",
            "abstract": "Continual relation extraction is a field that applies continual learning in relation extraction, which aims to train the model with new relations without losing the accurate classification of old ones. There are two significant challenges in continual Learning: catastrophic forgetting and knowledge transfer. Some previous work has shown that the memory-based method, which stores a few training examples in old tasks and retrains them while training new tasks, can solve these problems of continual learning, and it is well-performed in natural language processing tasks. However, memory-based methods tend to be overfitting and low-performed on imbalanced datasets. A combination of consistent representation learning and the prototype augmentation mechanism is proposed to solve this problem. We also reduce the real data storage by using the virtual data points, instead of using all data are the real ones. These virtual data points are generated by the prototypes of relations obtained through the data augmentation process. Experiments on FewRel and TACRED datasets reveal that our method performs better than the latest baselines in catastrophic forgetting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2202018899",
                    "name": "Quynh-Trang Pham Thi"
                },
                {
                    "authorId": "2201715770",
                    "name": "Anh-Cuong Pham"
                },
                {
                    "authorId": "2201791519",
                    "name": "Ngoc-Huyen Ngo"
                },
                {
                    "authorId": "40140266",
                    "name": "Duc-Trong Le"
                }
            ]
        },
        {
            "paperId": "2bc4b07ec5a382223ed40f87d1e49e4d41e26199",
            "title": "Modeling Multi-Intent Basket Sequences for Next-Basket Recommendation",
            "abstract": "Recommendation systems have a preponderance in assisting customers to save time by suggesting relevant options. With this convenience, a customer may purchase multiple items in a browsing session, referred to as an item basket. The notion of basket manifests his underlying preference of multiple implicit intentions, which becomes more sophisticated once considering the basket sequence of his chronological intersession list. With the objective of modeling basket sequences, most of previous methods hypothesize a homogeneous intention in each basket. The exploitation on multi-intent basket sequences for the recommendation task becomes an emerging demand. In this work, we present a novel framework named MIBS to model multi-intent basket sequences to recommend next basket of relevant items. Given a user's basket sequence, each basket is encoded via aggregating the item-item correlation matrix with a latent intent parameter matrix to generate the respective basket representation. This representation is later fed into a LSTM layer to infer the sequential encoding, which is also combined with the correlation matrix and the multi-intent matrix to produce item scores. The top-K items with the highest scores are employed to form the next-basket suggestion. Comprehensive experiments on three publicly-available datasets demonstrate the superiority of MIBS compared against state-of-the-art baselines for the next-basket recommendation task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2148254755",
                    "name": "Quoc-Viet Pham Hoang"
                },
                {
                    "authorId": "40140266",
                    "name": "Duc-Trong Le"
                }
            ]
        }
    ]
}