{
    "authorId": "1683688",
    "papers": [
        {
            "paperId": "245a1f574c253159689e4205e0b2eb37023057e5",
            "title": "Matching Roles from Temporal Data: Why Joe Biden is not only President, but also Commander-in-Chief",
            "abstract": "We present role matching, a novel, fine-grained integrity constraint on temporal fact data, i.e., (subject, predicate, object, timestamp)-quadruples. A role is a combination of subject and predicate and can be associated with different objects as the real world evolves and the data changes over time. A role matching states that the associated object of two or more roles should always match across time. Once discovered, role matchings can serve as integrity constraints to improve data quality, for instance of structured data in Wikipedia[3]. If violated, role matchings can alert data owners or editors and thus allow them to correct the error. Finding all role matchings is challenging due both to the inherent quadratic complexity of the matching problem and the need to identify true matches based on the possibly short history of the facts observed so far. To address the first challenge, we introduce several blocking methods both for clean and dirty input data. For the second challenge, the matching stage, we show how the entity resolution method Ditto[27] can be adapted to achieve satisfactory performance for the role matching task. We evaluate our method on datasets from Wikipedia infoboxes, showing that our blocking approaches can achieve 95% recall, while maintaining a reduction ratio of more than 99.99%, even in the presence of dirty data. In the matching stage, we achieve a macro F1-score of 89% on our datasets, using automatically generated labels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49625591",
                    "name": "Leon Bornemann"
                },
                {
                    "authorId": "7870886",
                    "name": "Tobias Bleifu\u00df"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "2315516",
                    "name": "F. Nargesian"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "83fd7e0dc08f288b69a63b2cd80893e9902d33cb",
            "title": "Pollock: A Data Loading Benchmark",
            "abstract": "Any system at play in a data-driven project has a fundamental requirement: the ability to load data. The de-facto standard format to distribute and consume raw data is csv. Yet, the plain text and flexible nature of this format make such files often difficult to parse and correctly load their content, requiring cumbersome data preparation steps. We propose a benchmark to assess the robustness of systems in loading data from non-standard csv formats and with structural inconsistencies. First, we formalize a model to describe the issues that affect real-world files and use it to derive a systematic \"pollution\" process to generate dialects for any given grammar. Our benchmark leverages the pollution framework for the csv format. To guide pollution, we have surveyed thousands of real-world, publicly available csv files, recording the problems we encountered. We demonstrate the applicability of our benchmark by testing and scoring 16 different systems: popular csv parsing frameworks, relational database tools, spreadsheet systems, and a data visualization tool.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1387885664",
                    "name": "Gerardo Vitagliano"
                },
                {
                    "authorId": "31842197",
                    "name": "Mazhar Hameed"
                },
                {
                    "authorId": "2112536899",
                    "name": "Lan Jiang"
                },
                {
                    "authorId": "2169809228",
                    "name": "Lucas Reisener"
                },
                {
                    "authorId": "48144872",
                    "name": "Eugene Wu"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                }
            ]
        },
        {
            "paperId": "90de1b5eeeff15d4b163208b89ac3433ecbdd9e1",
            "title": "BCNF* - From Normalized- to Star-Schemas and Back Again",
            "abstract": "Data warehouses are the core of many data analysis processes. They contain various database schemas, which are designed and created through schema transformation and integration. These processes are complex and require technical knowledge, which makes them costly and prevents business teams to start new analyses independently. BCNF* is a web application that enables users to safely explore valid schema transformations and generate transformation scripts automatically. It can be used for any schema transformation, but is optimized for semi-automatic data warehouse creation through means like a dedicated star schema mode.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2219336733",
                    "name": "Marie Fischer"
                },
                {
                    "authorId": "2218908995",
                    "name": "Paul Roessler"
                },
                {
                    "authorId": "2219556026",
                    "name": "Paul Sieben"
                },
                {
                    "authorId": "2219555764",
                    "name": "Janina Adamcic"
                },
                {
                    "authorId": "2219552061",
                    "name": "Christoph Kirchherr"
                },
                {
                    "authorId": "2219556776",
                    "name": "Tobias Straeubig"
                },
                {
                    "authorId": "2163157046",
                    "name": "Youri Kaminsky"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                }
            ]
        },
        {
            "paperId": "959dd4446857e78795dc36c73395999b4c4ff130",
            "title": "BrewER: Entity Resolution On-Demand",
            "abstract": "The task of entity resolution (ER) aims to detect multiple records describing the same real-world entity in datasets and to consolidate them into a single consistent record. ER plays a fundamental role in guaranteeing good data quality, e.g., as input for data science pipelines. Yet, the traditional approach to ER requires cleaning the entire data before being able to run consistent queries on it; hence, users struggle to tackle common scenarios with limited time or resources (e.g., when the data changes frequently or the user is only interested in a portion of the dataset for the task).\n We previously introduced BrewER, a framework to evaluate SQL SP queries on dirty data while progressively returning results as if they were issued on cleaned data, according to a priority defined by the user. In this demonstration, we show how BrewER can be exploited to ease the burden of ER, allowing data scientists to save a significant amount of resources for their tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2008182705",
                    "name": "Luca Zecchini"
                },
                {
                    "authorId": "2301232",
                    "name": "Giovanni Simonini"
                },
                {
                    "authorId": "2239502655",
                    "name": "Sonia Bergamaschi"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                }
            ]
        },
        {
            "paperId": "e8f00227b9a50929b1534ce4adf2cb7023b4b581",
            "title": "Detecting Stale Data in Wikipedia Infoboxes",
            "abstract": "Today\u2019s fast-paced society is increasingly reliant on correct and up-to-date data. Wikipedia is the world\u2019s most popular source of knowledge, and its infoboxes contain concise semi-structured data with important facts about a page\u2019s topic. However, these data are not always up-to-date: we do not expect Wikipedia editors to update items at the moment their true values change. Also, many pages might not be well maintained and users might forget to update the data, e.g., when they are on holiday. To detect stale data in Wikipedia infoboxes, we combine correlation-based and rule-based approaches trained on different temporal granularities, based on all infobox changes over 15 years of English Wikipedia. We are able to predict 8 . 19% of all changes with a precision of 89 . 69% over a whole year, thus meeting our target precision of",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2214588524",
                    "name": "Malte Barth"
                },
                {
                    "authorId": "2114705333",
                    "name": "Tibor Bleidt"
                },
                {
                    "authorId": "2114710496",
                    "name": "Martin B\u00fc\u00dfemeyer"
                },
                {
                    "authorId": "2114863886",
                    "name": "Fabian Heseding"
                },
                {
                    "authorId": "2148228603",
                    "name": "Niklas K\u00f6hnecke"
                },
                {
                    "authorId": "7870886",
                    "name": "Tobias Bleifu\u00df"
                },
                {
                    "authorId": "49625591",
                    "name": "Leon Bornemann"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "ecc14a332649b50af8d61a40935bb0e4eb46e1c0",
            "title": "Entity Resolution On-Demand for Querying Dirty Datasets",
            "abstract": "Entity Resolution (ER) is the process of identifying and merging records that refer to the same real-world entity. ER is usually applied as an expensive cleaning step on the entire data before consuming it, yet the relevance of cleaned entities ultimately depends on the user\u2019s specific application, which may only require a small portion of the entities. We introduce BrewER , a framework designed to evaluate SQL SP queries on unclean data while progressively providing results as if they were obtained from cleaned data. BrewER aims at cleaning a single entity at a time, adhering to an ORDERBY predicate, thus it inherently supports top-k queries and stop-and-resume execution. This approach can save a significant amount of resources for various applications. BrewER has been implemented as an open-source Python library and can be seamlessly employed with existing ER tools and algorithms. We thoroughly demonstrated its efficiency through its evaluation on four real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2301232",
                    "name": "Giovanni Simonini"
                },
                {
                    "authorId": "2008182705",
                    "name": "Luca Zecchini"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "2239502655",
                    "name": "Sonia Bergamaschi"
                }
            ]
        },
        {
            "paperId": "fe72efc638270a46ee1c87a45476d78689f58604",
            "title": "Discovering Similarity Inclusion Dependencies",
            "abstract": "Inclusion dependencies (INDs) are a well-known type of data dependency, specifying that the values of one column are contained in those of another column. INDs can be used for various purposes, such as foreign-key candidate selection or join partner discovery. The traditional notion of INDs is based on clean data, where the dependencies hold without exceptions. Unfortunately, data often contain errors, preventing otherwise valid INDs from being discovered. A typical response to this problem is to relax the dependency definition using a similarity measure to account for minor data errors, such as typos or different formatting. While this relaxation is known for functional dependencies, for inclusion dependencies no such relaxation has been defined. We formally introduce similarity inclusion dependencies, which relax the inclusion by demanding the existence only of sufficiently similar values. Similarity inclusion dependencies can fulfill traditional IND use cases, such as foreign-key candidate discovery, even in the presence of dirty data. We present Sawfish, the first algorithm to discover all similarity inclusion dependencies in a given dataset efficiently. Our algorithm combines approaches for the discovery of traditional INDs and string similarity joins with a novel sliding-window approach and lazy candidate validation. Our experimental evaluation shows that Sawfish can outperform a baseline by a factor of up to 6.5.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2163157046",
                    "name": "Youri Kaminsky"
                },
                {
                    "authorId": "83116770",
                    "name": "Eduardo H. M. Pena"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                }
            ]
        },
        {
            "paperId": "1159e88a35233ccbb77b8fae2fcc3d73a397adcc",
            "title": "Mining Change Rules",
            "abstract": "Changes in data happen frequently, and discovering how the changes interrelate can reveal information about the data and the transactions on them. In this paper, we define change rules as recurring patterns in database changes. Change rules embody valuable metadata and reveal semantic as well as functional relationships between versions of data. We can use change rules to discover formerly unknown relationships, anticipate data changes and explore anomalies if changes do not occur as expected. Wepropose the CR-Miner algorithm, which dispenses the manual formulation of rules to uncover this hidden knowledge in a generic and domain-independent way. Given a dataset together with its past versions, we efficiently discover change rules and rank them according to their potential for a manual review. The experimental results confirm that our method finds change rules efficiently in big data: On a subset of Wikipedia infoboxes encompassing data from four years and different categories, we discover 4456 change rules. Rules between changes from 48706 tables of open-government data observed over the period of one year can be discovered within 33minutes, and rules between about 2.5million Wikipedia infoboxes from 153 templates within 77minutes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2045980201",
                    "name": "Daniel Lindner"
                },
                {
                    "authorId": "1391457470",
                    "name": "Franziska Schumann"
                },
                {
                    "authorId": "2090084616",
                    "name": "Nicolas Alder"
                },
                {
                    "authorId": "7870886",
                    "name": "Tobias Bleifu\u00df"
                },
                {
                    "authorId": "49625591",
                    "name": "Leon Bornemann"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                }
            ]
        },
        {
            "paperId": "3123dde649f5247d60f059c5f098c724fe251728",
            "title": "The Effects of Data Quality on ML-Model Performance",
            "abstract": "Modern artificial intelligence (AI) applications require large quantities of training and test data. This need creates critical challenges not only concerning the availability of such data, but also regarding its quality. For example, incomplete, erroneous or inappropriate training data can lead to unreliable models that produce ultimately poor decisions. Trustworthy AI applications require high-quality training and test data along many dimensions, such as accuracy, completeness, consistency, and uniformity. We explore empirically the correlation between six of the traditional data quality dimensions and the performance of fifteen widely used ML algorithms covering the tasks of classification, regression, and clustering, with the goal of explaining ML results in terms of data quality. Our experiments distinguish three scenarios based on the AI pipeline steps that were fed with polluted data: polluted training data, test data, or both. We conclude the paper with an extensive discussion of our observations and recommendations, alongside open questions and future directions to be explored.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2179708130",
                    "name": "Lukas Budach"
                },
                {
                    "authorId": "2008167565",
                    "name": "Moritz Feuerpfeil"
                },
                {
                    "authorId": "2150119881",
                    "name": "Nina Ihde"
                },
                {
                    "authorId": "2179708327",
                    "name": "Andrea Nathansen"
                },
                {
                    "authorId": "50404914",
                    "name": "N. Noack"
                },
                {
                    "authorId": "2179709843",
                    "name": "Hendrik Patzlaff"
                },
                {
                    "authorId": "3417798",
                    "name": "Hazar Harmouch"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                }
            ]
        },
        {
            "paperId": "3b694dfa9df03823db0d765847a16f7cd21a8795",
            "title": "Data Errors: Symptoms, Causes and Origins",
            "abstract": "The \ufb02ood of data that has enabled breakthroughs in medicine, commerce, transportation, science and society also threatens to overwhelm our storage capacities and our privacy. Due to the volume of data and growth of regulations governing its maintenance and use, it is essential to develop automatic disposal techniques to manage this \ufb02ood. We present a vision for automating data disposal \u2013 disposal by design \u2013 which takes into account processing constraints, regulatory constraints as well as storage constraints, and give three concrete examples which address aspects of this vision. Two of the examples address current needs in e-commerce, while the third suggests how to use machine learning to \ufb01nd summaries of relational data. We then discuss the research challenges that remain to provide a holistic solution to disposal by design. Abstract Most modern data systems have been designed with two goals in mind \u2013 fast ingestion and low-latency query processing. The \ufb01rst goal has led to the development of a plethora of write-optimized data stores that employ the out-of-place paradigm. Due to their write-optimized design, out-of-place data systems perform deletes logically via invalidation, and retain the invalid data for arbitrarily long. However, due to the recent enactment of new data privacy regulations, the requirement of timely deletion of user data has become central. The right to be forgotten (in EU\u2019s GDPR), right to delete (in California\u2019s CCPA and CPRA), or deletion right (in Virginia\u2019s VCDPA) mandates that service providers persistently delete a user\u2019s data within a pre-set time duration. Logical deletion in out-of-place data systems, however, does not offer guarantees for timely and persistent deletion, and attempting to enforce it using existing tools leads to poor performance and increased operational costs. In this paper, we present a new framework for building deletion-compliant data systems from a holistic perspective. We analyze the new regulations and the requirements derived from the new policies, and we propose changes in the application and the system layer of data management. We outline the new types of deletion requests that need to be supported, the query language modi\ufb01cations needed to be able to request for timely persistent data deletion, and the system-level changes needed to realize timely and persistent deletes. The proposed framework for deletion compliance lays the groundwork for a new class of data systems that can offer system-level guarantees for user data privacy. We present recent results spanning all layers of the framework: the requirements and the application layer target any database system, while the system layer discussion is geared towards out-of-place systems. Finally, we conclude with a discussion on next steps and open challenges on building deletion-compliant data systems. Abstract With the increasing need for Machine-learning-as-a-service (MaaS) online systems, effectively maintaining and reusing machine learning models in light of changes to the underlying data has become a big concern. In particular, it is extremely challenging to refresh existing models after the removal of training samples, which is called \u201cmachine unlearning\u201d. Addressing this challenge not only requires an ef\ufb01cient solution, but must comply with emerging privacy issues, e.g. GDPR, which implies that the removed samples must be fully erased from the models so that they cannot be leaked to an adversary. We review two provenance-based solutions, PrIU and DeltaGrad, and show how they can guard against \u201cmodel inversion attacks\", which reconstruct the removed training samples from the updated models after the unlearning process. Since PrIU and DeltaGrad support a limited class of models, we envision a system that can unlearn general models in an ef\ufb01cient and secure manner and outline possible technical challenges for building this system. Abstract In an increasingly digital world, compliance with data regulations play an important role. More and more individuals are rapidly getting concerned with the way their data is being stored and processed by organizations. Therefore, it is crucial that data processing be subjected to regulatory obligations at its core. Yet, achieving compliance with data regulations requires the entire data processing pipeline to be revisited to embrace data policies as \ufb01rst-class citizens. In this paper, we present our work on novel systems and methods for federated data processing, where the processing of geo-distributed data is subjected to data transfer regulations. We showcase our work on compliant geo-distributed data processing and present research challenges and opportunities for a federated data processing system to make compliance truly its \ufb01rst-class citizens. Abstract Both societal and regulatory pressure (GDPR) increasingly challenge organizations and engineering teams to balance privacy and innovation. Striking this balance can be costly in terms of effort, data utility and computation costs. Moreover, current approaches in scalable data systems often treat privacy as an access problem, which is at odds with important legal and design principles. A plethora of privacy preserving- and enhancing- technologies are available, yet their adoption in production data systems still faces challenges. In particular, their focus is often on narrow use cases such as external data sharing, on mostly existing data sets, rendering them unusable in real-time data architectures. In this paper we argue engineering teams should \u201cshift left\u201c with their data privacy efforts, to the point of data collection. We show how privacy challenges in production architectures can be addressed without compromising speed, data quality or privacy. We provide a detailed yet practical explanation of an architectural set-up that allows users to launch privacy streams in seconds.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2180399",
                    "name": "Sebastian Schelter"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "134207254",
                    "name": "Jorge Quian\u00e9"
                },
                {
                    "authorId": "1733290",
                    "name": "V. Markl"
                }
            ]
        }
    ]
}