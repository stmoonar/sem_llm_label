{
    "authorId": "3032058",
    "papers": [
        {
            "paperId": "5ca4de4a0643374b6d9a68a52b93e67abf29cfe7",
            "title": "BadSampler: Harnessing the Power of Catastrophic Forgetting to Poison Byzantine-robust Federated Learning",
            "abstract": "Federated Learning (FL) is susceptible to poisoning attacks, wherein compromised clients manipulate the global model by modifying local datasets or sending manipulated model updates. Experienced defenders can readily detect and mitigate the poisoning effects of malicious behaviors using Byzantine-robust aggregation rules. However, the exploration of poisoning attacks in scenarios where such behaviors are absent remains largely unexplored for Byzantine-robust FL. This paper addresses the challenging problem of poisoning Byzantine-robust FL by introducing catastrophic forgetting. To fill this gap, we first formally define generalization error and establish its connection to catastrophic forgetting, paving the way for the development of a clean-label data poisoning attack named BadSampler. This attack leverages only clean-label data (i.e., without poisoned data) to poison Byzantine-robust FL and requires the adversary to selectively sample training data with high loss to feed model training and maximize the model's generalization error. We formulate the attack as an optimization problem and present two elegant adversarial sampling strategies, Top-$\\kappa$ sampling, and meta-sampling, to approximately solve it. Additionally, our formal error upper bound and time complexity analysis demonstrate that our design can preserve attack utility with high efficiency. Extensive evaluations on two real-world datasets illustrate the effectiveness and performance of our proposed attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307161736",
                    "name": "Yi Liu"
                },
                {
                    "authorId": "2239064971",
                    "name": "Cong Wang"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                }
            ]
        },
        {
            "paperId": "668c2fc744f5b428dca6426319f45efc889c8e42",
            "title": "Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts",
            "abstract": "Large Vision Language Models (VLMs) extend and enhance the perceptual abilities of Large Language Models (LLMs). Despite offering new possibilities for LLM applications, these advancements raise significant security and ethical concerns, particularly regarding the generation of harmful content. While LLMs have undergone extensive security evaluations with the aid of red teaming frameworks, VLMs currently lack a well-developed one. To fill this gap, we introduce Arondight, a standardized red team framework tailored specifically for VLMs. Arondight is dedicated to resolving issues related to the absence of visual modality and inadequate diversity encountered when transitioning existing red teaming methodologies from LLMs to VLMs. Our framework features an automated multi-modal jailbreak attack, wherein visual jailbreak prompts are produced by a red team VLM, and textual prompts are generated by a red team LLM guided by a reinforcement learning agent. To enhance the comprehensiveness of VLM security evaluation, we integrate entropy bonuses and novelty reward metrics. These elements incentivize the RL agent to guide the red team LLM in creating a wider array of diverse and previously unseen test cases. Our evaluation of ten cutting-edge VLMs exposes significant security vulnerabilities, particularly in generating toxic images and aligning multi-modal prompts. In particular, our Arondight achieves an average attack success rate of 84.5\\% on GPT-4 in all fourteen prohibited scenarios defined by OpenAI in terms of generating toxic text. For a clearer comparison, we also categorize existing VLMs based on their safety levels and provide corresponding reinforcement recommendations. Our multimodal prompt dataset and red team code will be released after ethics committee approval. CONTENT WARNING: THIS PAPER CONTAINS HARMFUL MODEL RESPONSES.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307161736",
                    "name": "Yi Liu"
                },
                {
                    "authorId": "152460589",
                    "name": "Chengjun Cai"
                },
                {
                    "authorId": "2312444403",
                    "name": "Xiaoli Zhang"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2259852815",
                    "name": "Cong Wang"
                }
            ]
        },
        {
            "paperId": "e87ffcd193b564a1d2c4050d59599f54ac4525f0",
            "title": "Practical Multi-Source Multi-Client Searchable Encryption With Forward Privacy: Refined Security Notion and New Constructions",
            "abstract": "Multi-source multi-client (M/M) searchable encryption has drawn increasing attention as data sharing becomes prevalent in the digital economics era. It allows data from multiple sources to be securely outsourced to third parties and queried by authorized clients. In response to these demands, various schemes sprung up in the last few years. However, empirical results show that they suffer from performance limitations. Specifically, they either require per-interaction in per-query between data sources and clients or time-consuming public-key encryption. To address these issues, we propose a searchable encryption scheme that allows authorized clients to efficiently search encrypted data from multiple sources. Compared to previous schemes, our design reduces the interaction overhead of authorization and query with the aid of a set-constrained pseudo-random function. Given practical considerations in the M/M setting, we further refine the forward privacy (FP) as \u201cFP with client\u201d and \u201cFP with server\u201d for data addition. To achieve these new security notions, we construct a new M/M scheme only with efficient symmetric cryptographic tools. We perform a formal security analysis of the proposed schemes and implement them to compare with prior arts. The theoretical and experimental results confirm that our designs are practical with lower communication and computation overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2061654390",
                    "name": "Lin Mei"
                },
                {
                    "authorId": "2576956",
                    "name": "Chungen Xu"
                },
                {
                    "authorId": "2116557967",
                    "name": "Lei Xu"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "1416519437",
                    "name": "Joseph K. Liu"
                }
            ]
        },
        {
            "paperId": "38cf10e88aeb690411a1f0d02e678e52176db992",
            "title": "Leakage-Abuse Attacks Against Forward and Backward Private Searchable Symmetric Encryption",
            "abstract": "Dynamic searchable symmetric encryption (DSSE) enables a server to efficiently search and update over encrypted files. To minimize the leakage during updates, a security notion named forward and backward privacy is expected for newly proposed DSSE schemes. Those schemes are generally constructed in a way to break the linkability across search and update queries to a given keyword. However, it remains underexplored whether forward and backward private DSSE is resilient against practical leakage-abuse attacks (LAAs), where an attacker attempts to recover query keywords from the leakage passively collected during queries. In this paper, we aim to be the first to answer this question firmly through two non-trivial efforts. First, we revisit the spectrum of forward and backward private DSSE schemes over the past few years, and unveil some inherent constructional limitations in most schemes. Those limitations allow attackers to exploit query equality and establish a guaranteed linkage among different (refreshed) query tokens surjective to a candidate keyword. Second, we refine volumetric leakage profiles of updates and queries by associating each with a specific operation. By further exploiting update volume and query response volume, we demonstrate that all forward and backward private DSSE schemes can leak the same volumetric information (e.g., insertion volume, deletion volume) as those without such security guarantees. To testify our findings, we realize two generic LAAs, i.e., frequency matching attack and volumetric inference attack, and we evaluate them over various experimental settings in the dynamic context. Finally, we call for new efficient schemes to protect query equality and volumetric information across search and update queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2239054093",
                    "name": "Lei Xu"
                },
                {
                    "authorId": "1964303141",
                    "name": "Leqian Zheng"
                },
                {
                    "authorId": "2239066594",
                    "name": "Chengzhi Xu"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2239064971",
                    "name": "Cong Wang"
                }
            ]
        },
        {
            "paperId": "3beedc05ca956bd39dc84173f3f5b4660535e814",
            "title": "Demystifying Uneven Vulnerability of Link Stealing Attacks against Graph Neural Networks",
            "abstract": "While graph neural networks (GNNs) dominate the state-of-the-art for exploring graphs in real-world applications, they have been shown to be vulnerable to a growing number of privacy attacks. For instance, link stealing is a well-known membership inference attack (MIA) on edges that infers the presence of an edge in a GNN\u2019s training graph. Recent studies on independent and identically distributed data (e.g., images) have empirically demonstrated that individuals from different groups suffer from different levels of privacy risks to MIAs, i.e., uneven vulnerability. However, theoretical evidence for such uneven vulnerability is missing. In this paper, we first present theoretical evidence of the uneven vulnerability of GNNs to link stealing attacks, which lays the foundation for demystifying such uneven risks among different groups of edges. We further demonstrate a group-based attack paradigm to expose the practical privacy harm to GNN users derived from the uneven vulnerability of edges. Finally, we empirically validate the existence of obvious uneven vulnerability on ten real-world datasets (e.g., about 25% AUC difference between different groups in the Credit graph). Compared with existing methods, the outperformance of our group-based attack paradigm confirms that cus-tomising different strategies for different groups results in more effective privacy attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2117010331",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2112166824",
                    "name": "Xiangwen Yang"
                },
                {
                    "authorId": "2837434",
                    "name": "Minhui Xue"
                },
                {
                    "authorId": "2153326034",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                }
            ]
        },
        {
            "paperId": "5d896fb2f0da16060f22ed43e582464605237f28",
            "title": "Training-free Lexical Backdoor Attacks on Language Models",
            "abstract": "Large-scale language models have achieved tremendous success across various natural language processing (NLP) applications. Nevertheless, language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors. Most existing backdoor attacks, such as data poisoning, require further (re)training or fine-tuning language models to learn the intended backdoor patterns. The additional training process however diminishes the stealthiness of the attacks, as training a language model usually requires long optimization time, a massive amount of data, and considerable modifications to the model parameters. In this work, we propose Training-Free Lexical Backdoor Attack (TFLexAttack) as the first training-free backdoor attack on language models. Our attack is achieved by injecting lexical triggers into the tokenizer of a language model via manipulating its embedding dictionary using carefully designed rules. These rules are explainable to human developers which inspires attacks from a wider range of hackers. The sparse manipulation of the dictionary also habilitates the stealthiness of our attack. We conduct extensive experiments on three dominant NLP tasks based on nine language models to demonstrate the effectiveness and universality of our attack. The code of this work is available at https://github.com/Jinxhy/TFLexAttack.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108733121",
                    "name": "Yujin Huang"
                },
                {
                    "authorId": "2080123731",
                    "name": "Terry Yue Zhuo"
                },
                {
                    "authorId": "3101288",
                    "name": "Qiongkai Xu"
                },
                {
                    "authorId": "145030939",
                    "name": "Han Hu"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2143953186",
                    "name": "Chunyang Chen"
                }
            ]
        },
        {
            "paperId": "e4c4b4bf831e66b7bb920220fdb4e03c404793df",
            "title": "Securely Outsourcing Neural Network Inference to the Cloud With Lightweight Techniques",
            "abstract": "Neural network (NN) inference services enrich many applications, like image classification, object recognition, facial verification, and more. These NN inference services are increasingly becoming an essential offering from cloud computing providers, where end-users\u2019 data are offloaded to the cloud for inference under a customized model. However, current cloud-based inference services operate on clear inputs and NN models, raising paramount privacy concerns. Individual user data may contain private information that should always remain confidential. Meanwhile, the NN model is deemed proprietary to the model owner as model training requires substantial resources. In this article, we present, tailor, and evaluate <monospace>Sonic</monospace>, a lightweight secure NN inference service delegated in the cloud. <monospace>Sonic</monospace> leverages the cloud computing paradigm to fully outsource the secure inference, freeing end devices and model owners from being actively online for assistance. <monospace>Sonic</monospace> guards both user input and model privacy along the whole service flow. We design a series of secure and efficient NN layer functions purely using lightweight cryptographic primitives. Extensive evaluations demonstrate that <monospace>Sonic</monospace> achieves up to <inline-formula><tex-math notation=\"LaTeX\">$60\\times$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>60</mml:mn><mml:mo>\u00d7</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"zheng-ieq1-3141391.gif\"/></alternatives></inline-formula> bandwidth saving in online inference compared to prior art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110661801",
                    "name": "Xiaoning Liu"
                },
                {
                    "authorId": "144227659",
                    "name": "Yifeng Zheng"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "143824407",
                    "name": "X. Yi"
                }
            ]
        },
        {
            "paperId": "edf2a9a2ee3c82756288366ce9d6592823399d53",
            "title": "Unraveling Privacy Risks of Individual Fairness in Graph Neural Networks",
            "abstract": "Graph neural networks (GNNs) have gained significant attraction due to their expansive real-world applications. To build trustworthy GNNs, two aspects - fairness and privacy - have emerged as critical considerations. Previous studies have separately examined the fairness and privacy aspects of GNNs, revealing their tradeoff with GNN performance. Yet, the inter-play between these two aspects remains unexplored. In this paper, we pioneer the exploration of the interaction between the privacy risks of edge leakage and the individual fairness of a GNN. Our theoretical analysis unravels that edge privacy risks unfortunately escalate when the nodes' individual fairness improves. Such an issue hinders the accomplishment of privacy and fairness of GNNs at the same time. To balance fairness and privacy, we carefully introduce fairness-aware loss reweighting based on in-fluence function and privacy-aware graph structure perturbation modules within a fine-tuning mechanism. Experimental results underscore the effectiveness of our approach in achieving GNN fairness with limited performance compromise and controlled privacy risks. This work contributes to the comprehensively developing trustworthy GNNs by simultaneously addressing both fairness and privacy aspects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2153326034",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "106bf505885362082e0286595b9ae09c6e512959",
            "title": "Towards Efficient Cryptographic Data Validation Service in Edge Computing",
            "abstract": "[J1C2 Presentation Abstract at IEEE SERVICES 2021 for IEEE Transactions on Services Computing DOI 10.1109/TSC.2021.3111208]",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116557967",
                    "name": "Lei Xu"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "8525964",
                    "name": "Zhengxiang Zhou"
                },
                {
                    "authorId": "2116640008",
                    "name": "Cong Wang"
                },
                {
                    "authorId": "2576956",
                    "name": "Chungen Xu"
                }
            ]
        },
        {
            "paperId": "1ff32b615497085a999f17192a09914628ebf77c",
            "title": "The Right to be Forgotten in Federated Learning: An Efficient Realization with Rapid Retraining",
            "abstract": "In Machine Learning, the emergence of the right to be forgotten gave birth to a paradigm named machine unlearning, which enables data holders to proactively erase their data from a trained model. Existing machine unlearning techniques focus on centralized training, where access to all holders\u2019 training data is a must for the server to conduct the unlearning process. It remains largely underexplored about how to achieve unlearning when full access to all training data becomes unavailable. One noteworthy example is Federated Learning (FL), where each participating data holder trains locally, without sharing their training data to the central server. In this paper, we investigate the problem of machine unlearning in FL systems. We start with a formal definition of the unlearning problem in FL and propose a rapid retraining approach to fully erase data samples from a trained FL model. The resulting design allows data holders to jointly conduct the unlearning process efficiently while keeping their training data locally. Our formal convergence and complexity analysis demonstrate that our design can preserve model utility with high efficiency. Extensive evaluations on four real-world datasets illustrate the effectiveness and performance of our proposed realization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153629202",
                    "name": "Yi Liu"
                },
                {
                    "authorId": "2109329605",
                    "name": "Lei Xu"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2116639836",
                    "name": "Cong Wang"
                },
                {
                    "authorId": null,
                    "name": "Bo Li"
                }
            ]
        }
    ]
}