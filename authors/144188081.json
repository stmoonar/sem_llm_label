{
    "authorId": "144188081",
    "papers": [
        {
            "paperId": "5156b3be33abbc3267b2d41e35537bf33c149420",
            "title": "Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?",
            "abstract": "Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the generation of coherent sentences resembling human writing on a large scale, resulting in the creation of so-called deepfake texts. However, this progress poses security and privacy concerns, necessitating effective solutions for distinguishing deepfake texts from human-written ones. Although prior works studied humans\u2019 ability to detect deepfake texts, none has examined whether \u201ccollaboration\u201d among humans improves the detection of deepfake texts. In this study, to address this gap of understanding on deepfake texts, we conducted experiments with two groups: (1) nonexpert individuals from the AMT platform and (2) writing experts from the Upwork platform. The results demonstrate that collaboration among humans can potentially improve the detection of deepfake texts for both groups, increasing detection accuracies by 6.36% for non-experts and 12.76% for experts, respectively, compared to individuals\u2019 detection accuracies. We further analyze the explanations that humans used for detecting a piece of text as deepfake text, and find that the strongest indicator of deepfake texts is their lack of coherence and consistency. Our study provides useful insights for future tools and framework designs to facilitate the collaborative human detection of deepfake texts. The experiment datasets and AMT implementations are available at: https://github.com/huashen218/llm-deepfake-human-study.git",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150035131",
                    "name": "Adaku Uchendu"
                },
                {
                    "authorId": "2159644449",
                    "name": "Jooyoung Lee"
                },
                {
                    "authorId": "145028030",
                    "name": "Hua Shen"
                },
                {
                    "authorId": "145535348",
                    "name": "Thai Le"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                },
                {
                    "authorId": "2158951945",
                    "name": "Dongwon Lee"
                }
            ]
        },
        {
            "paperId": "75ddc4fb91332f95222d74449d96b9f7c8f976c7",
            "title": "Nationality Bias in Text Generation",
            "abstract": "Little attention is placed on analyzing nationality bias in language models, especially when nationality is highly used as a factor in increasing the performance of social NLP models. This paper examines how a text generation model, GPT-2, accentuates pre-existing societal biases about country-based demonyms. We generate stories using GPT-2 for various nationalities and use sensitivity analysis to explore how the number of internet users and the country\u2019s economic status impacts the sentiment of the stories. To reduce the propagation of biases through large language models (LLM), we explore the debiasing method of adversarial triggering. Our results show that GPT-2 demonstrates significant bias against countries with lower internet users, and adversarial triggering effectively reduces the same.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053812167",
                    "name": "Pranav Narayanan Venkit"
                },
                {
                    "authorId": "2142840013",
                    "name": "Sanjana Gautam"
                },
                {
                    "authorId": "2204579729",
                    "name": "Ruchi Panchanadikar"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                },
                {
                    "authorId": "31950200",
                    "name": "Shomir Wilson"
                }
            ]
        },
        {
            "paperId": "98827195324a3a00633042980d87db3fed2ab1d4",
            "title": "What Types of Questions Require Conversation to Answer? A Case Study of AskReddit Questions",
            "abstract": "The proliferation of automated conversational systems such as chatbots, spoken-dialogue systems, and smart speakers, has significantly impacted modern digital life. However, these systems are primarily designed to provide answers to well-defined questions rather than to support users in exploring complex, ill-defined questions. In this paper, we aim to push the boundaries of conversational systems by examining the types of nebulous, open-ended questions that can best be answered through conversation. We first sampled 500 questions from one million open-ended requests posted on AskReddit, and then recruited online crowd workers to answer eight inquiries about these questions. We also performed open coding to categorize the questions into 27 different domains. We found that the issues people believe require conversation to resolve satisfactorily are highly social and personal. Our work provides insights into how future research could be geared to align with users\u2019 needs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110443181",
                    "name": "Shih-Hong Huang"
                },
                {
                    "authorId": "1414028847",
                    "name": "Huang Chieh-Yang"
                },
                {
                    "authorId": "152206490",
                    "name": "Yande Lin"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                }
            ]
        },
        {
            "paperId": "e32d41c4b7c3c77177719e847195fd586e7ce64e",
            "title": "The Second Workshop on Intelligent and Interactive Writing Assistants",
            "abstract": "In recent years, writing assistants have become increasingly sophisticated and ubiquitous, fueled by advances in artificial intelligence, particularly large language models. As new use cases and models emerge, we expect the adoption rate to accelerate. In this interdisciplinary workshop, we, as a diverse group of researchers and practitioners interested in intelligent and interactive writing assistants, will create a taxonomy of writing assistants and discuss their desirable features and potential consequences. We invite writers, educators, researchers, industry practitioners, students, and anyone interested in creating, using, and testing future writing assistant technologies to join the conversation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10741225",
                    "name": "Minsuk Chang"
                },
                {
                    "authorId": "152836325",
                    "name": "John Joon Young Chung"
                },
                {
                    "authorId": "2134414724",
                    "name": "Katy Ilonka Gero"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                },
                {
                    "authorId": "48493368",
                    "name": "Dongyeop Kang"
                },
                {
                    "authorId": "49316195",
                    "name": "Mina Lee"
                },
                {
                    "authorId": "2831377",
                    "name": "Vipul Raheja"
                },
                {
                    "authorId": "94398251",
                    "name": "Thiemo Wambsganss"
                }
            ]
        },
        {
            "paperId": "e7003e2a313558e62d9aec3db4154d4efdd46016",
            "title": "ConvXAI : Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing",
            "abstract": "Despite a surge collection of XAI methods, users still struggle to obtain required AI explanations. Previous research suggests chatbots as dynamic solutions, but the effective design of conversational XAI agents for practical human needs remains under-explored. This paper focuses on Conversational XAI for AI-assisted scientific writing tasks. Drawing from human linguistic theories and formative studies, we identify four design rationales: \u201cmultifaceted\u201d, \u201ccontrollability\u201d, \u201cmix-initiative\u201d, \u201ccontext-aware drill-down\u201d. We incorporate them into an interactive prototype, ConvXAI 1, which facilitates heterogeneous AI explanations for scientific writing through dialogue. In two studies with 21 users, ConvXAI outperforms a GUI-based baseline on improving human-perceived understanding and writing improvement. The paper further discusses the practical human usage patterns in interacting with ConvXAI for scientific co-writing2.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145028030",
                    "name": "Hua Shen"
                },
                {
                    "authorId": "1414028847",
                    "name": "Huang Chieh-Yang"
                },
                {
                    "authorId": "35232494",
                    "name": "Tongshuang Sherry Wu"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                }
            ]
        },
        {
            "paperId": "fd8ebaa67ee959b9e794af179feaa2c26a65e86f",
            "title": "Conveying the Predicted Future to Users: A Case Study of Story Plot Prediction",
            "abstract": "Creative writing is hard: Novelists struggle with writer's block daily. While automatic story generation has advanced recently, it is treated as a\"toy task\"for advancing artificial intelligence rather than helping people. In this paper, we create a system that produces a short description that narrates a predicted plot using existing story generation approaches. Our goal is to assist writers in crafting a consistent and compelling story arc. We conducted experiments on Amazon Mechanical Turk (AMT) to examine the quality of the generated story plots in terms of consistency and storiability. The results show that short descriptions produced by our frame-enhanced GPT-2 (FGPT-2) were rated as the most consistent and storiable among all models; FGPT-2's outputs even beat some random story snippets written by humans. Next, we conducted a preliminary user study using a story continuation task where AMT workers were given access to machine-generated story plots and asked to write a follow-up story. FGPT-2 could positively affect the writing process, though people favor other baselines more. Our study shed some light on the possibilities of future creative writing support systems beyond the scope of completing sentences. Our code is available at: https://github.com/appleternity/Story-Plot-Generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1414028847",
                    "name": "Huang Chieh-Yang"
                },
                {
                    "authorId": "2208701417",
                    "name": "Saniya Naphade"
                },
                {
                    "authorId": "113116084",
                    "name": "Kavya Laalasa Karanam"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                }
            ]
        },
        {
            "paperId": "2e2147ca34e726f568fb28089424d2d97cb8b7f5",
            "title": "Learning to Rank Visual Stories From Human Ranking Data",
            "abstract": "Visual storytelling (VIST) is a typical vision and language task that has seen extensive development in the natural language generation research domain. However, it remains unclear whether conventional automatic evaluation metrics for text generation are applicable on VIST. In this paper, we present the VHED (VIST Human Evaluation Data) dataset, which first re-purposes human evaluation results for automatic evaluation; hence we develop Vrank (VIST Ranker), a novel reference-free VIST metric for story evaluation. We first show that the results from commonly adopted automatic metrics for text generation have little correlation with those obtained from human evaluation, which motivates us to directly utilize human evaluation results to learn the automatic evaluation model. In the experiments, we evaluate the generated texts to predict story ranks using our model as well as other reference-based and reference-free metrics. Results show that Vrank prediction is significantly more aligned to human evaluation than other metrics with almost 30% higher accuracy when ranking story pairs. Moreover, we demonstrate that only Vrank shows human-like behavior in its strong ability to find better stories when the quality gap between two stories is high. Finally, we show the superiority of Vrank by its generalizability to pure textual stories, and conclude that this reuse of human evaluation results puts Vrank in a strong position for continued future advances.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48162772",
                    "name": "Chi-Yang Hsu"
                },
                {
                    "authorId": "9628638",
                    "name": "Yun-Wei Chu"
                },
                {
                    "authorId": "2165225005",
                    "name": "Vincent Chen"
                },
                {
                    "authorId": "2051973162",
                    "name": "Kuan-Chieh Lo"
                },
                {
                    "authorId": "2131059444",
                    "name": "Chacha Chen"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                },
                {
                    "authorId": "1746959",
                    "name": "Lun-Wei Ku"
                }
            ]
        },
        {
            "paperId": "65cd4c7b2d4899772a0e6d6895a12fdcfe7fa885",
            "title": "Empathy-Centric Design At Scale",
            "abstract": "EmpathiCH aims at bringing together and blend different expertise to develop new research agenda in the context of \u201cEmpathy-Centric Design at Scale\u201d. The main research question is to investigate how new technologies can contribute to the elicitation of empathy across and within multiple stakeholders at scale; and how empathy can be used to design solutions to societal problems that are not only effective but also balanced, inclusive, and aware of their effect on society. Through presentations, participatory sessions, and a living experiment\u2014where data about the peoples\u2019 interactions is collected throughout the event\u2014we aim to make this workshop the ideal venue to foster collaboration, build networks, and shape the future direction of \u201cEmpathy-Centric Design at Scale\u201d.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064018408",
                    "name": "A. Mauri"
                },
                {
                    "authorId": "2465856",
                    "name": "Yen-Chia Hsu"
                },
                {
                    "authorId": "40350773",
                    "name": "Marco Brambilla"
                },
                {
                    "authorId": "1401835413",
                    "name": "A. O'Kane"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                },
                {
                    "authorId": "2061293498",
                    "name": "Himanshu Verma"
                }
            ]
        },
        {
            "paperId": "69b9c2c561913373e4890cc363a176e36f3ab8c5",
            "title": "Let's Talk! Striking Up Conversations via Conversational Visual Question Generation",
            "abstract": "An engaging and provocative question can open up a great conversation. In this work, we explore a novel scenario: a conversation agent views a set of the user\u2019s photos (for ex- ample, from social media platforms) and asks an engaging question to initiate a conversation with the user. The existing vision-to-question models mostly generate tedious and obvious questions, which might not be ideals conversa- tion starters. This paper introduces a two-phase framework that \ufb01rst generates a visual story for the photo set and then uses the story to produce an interesting question. The human evaluation shows that our framework generates more response-provoking questions for starting conversations than other vision-to-question baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40304784",
                    "name": "Shih-Han Chan"
                },
                {
                    "authorId": "2109191163",
                    "name": "Tsai-Lun Yang"
                },
                {
                    "authorId": "9628638",
                    "name": "Yun-Wei Chu"
                },
                {
                    "authorId": "48162772",
                    "name": "Chi-Yang Hsu"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                },
                {
                    "authorId": "34272463",
                    "name": "Yu-Shian Chiu"
                },
                {
                    "authorId": "1746959",
                    "name": "Lun-Wei Ku"
                }
            ]
        },
        {
            "paperId": "9f3540baf1f1e5bf3dd4efdd776d6e2ffc3f55e9",
            "title": "Are Shortest Rationales the Best Explanations for Human Understanding?",
            "abstract": "Existing self-explaining models typically favor extracting the shortest possible rationales \u2014 snippets of an input text \u201cresponsible for\u201d corresponding output \u2014 to explain the model prediction, with the assumption that shorter rationales are more intuitive to humans. However, this assumption has yet to be validated. Is the shortest rationale indeed the most human-understandable? To answer this question, we design a self-explaining model, LimitedInk, which allows users to extract rationales at any target length. Compared to existing baselines, LimitedInk achieves compatible end-task performance and human-annotated rationale agreement, making it a suitable representation of the recent class of self-explaining models. We use LimitedInk to conduct a user study on the impact of rationale length, where we ask human judges to predict the sentiment label of documents based only on LimitedInk-generated rationales with different lengths. We show rationales that are too short do not help humans predict labels better than randomly masked text, suggesting the need for more careful design of the best human rationales.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145028030",
                    "name": "Hua Shen"
                },
                {
                    "authorId": "35232494",
                    "name": "Tongshuang Sherry Wu"
                },
                {
                    "authorId": "3138660",
                    "name": "Wenbo Guo"
                },
                {
                    "authorId": "144188081",
                    "name": "Ting-Hao 'Kenneth' Huang"
                }
            ]
        }
    ]
}