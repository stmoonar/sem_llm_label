{
    "authorId": "2031845",
    "papers": [
        {
            "paperId": "1a45269d9b5f7168c7d9daf3afde37ffd5677b5f",
            "title": "Emerging Trends in Intelligent Vehicles: The IEEE TIV Perspective",
            "abstract": "This article is focused on bibliographic analysis and collaboration pattern analysis of the textpapers published in the IEEE Transactions on Intelligent Vehicles (TIV) from January 2019 to January 2023. We have identified the authors, institutions, and countries/regions that are the most productive and have the highest impact. It is found that research on intelligent vehicles is dominated by researchers and institutions from China. Researchers from the US are the second largest contributor to the transaction, whilst those from Germany rank the third. It is also found that, Fei-Yue Wang, Mohan Manubhai Trivedi, and J. Christian Gerdes are the three most productive and influential authors at IEEE TIV, and the Institute of Automation, Chinese Academy of Sciences, National University of Defense Technology, and the University of California, are three most productive and influential institutions. In addition, three networks are generated (i.e., the co-authorship, co-keyword, and author-keyword) to mine collaboration patterns among authors and keywords. Collaboration relationship between researchers and hot research topics are also investigated. Furthermore, an open-source tool is developed to automatically collect metadata, perform bibliographic analysis, and mine collaboration patterns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1561587870",
                    "name": "Hui Zhang"
                },
                {
                    "authorId": "2031845",
                    "name": "Junbo Guo"
                },
                {
                    "authorId": "3489301",
                    "name": "Guiyang Luo"
                },
                {
                    "authorId": "118220206",
                    "name": "Lingxi Li"
                },
                {
                    "authorId": "3005478",
                    "name": "X. Na"
                },
                {
                    "authorId": "144129720",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2134580871",
                    "name": "Siyu Teng"
                },
                {
                    "authorId": "2210931213",
                    "name": "Siji Ma"
                },
                {
                    "authorId": "2110410718",
                    "name": "Yidong Li"
                }
            ]
        },
        {
            "paperId": "5699d5050cc5495ad3bf4d299c0285f8239224f1",
            "title": "Improving Chinese Spelling Check by Character Pronunciation Prediction: The Effects of Adaptivity and Granularity",
            "abstract": "Chinese spelling check (CSC) is a fundamental NLP task that detects and corrects spelling errors in Chinese texts. As most of these spelling errors are caused by phonetic similarity, effectively modeling the pronunciation of Chinese characters is a key factor for CSC. In this paper, we consider introducing an auxiliary task of Chinese pronunciation prediction (CPP) to improve CSC, and, for the first time, systematically discuss the adaptivity and granularity of this auxiliary task. We propose SCOPE which builds upon a shared encoder two parallel decoders, one for the primary CSC task and the other for a fine-grained auxiliary CPP task, with a novel adaptive weighting scheme to balance the two tasks. In addition, we design a delicate iterative correction strategy for further improvements during inference. Empirical evaluation shows that SCOPE achieves new state-of-the-art on three CSC benchmarks, demonstrating the effectiveness and superiority of the auxiliary CPP task. Comprehensive ablation studies further verify the positive effects of adaptivity and granularity of the task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3392041",
                    "name": "Jiahao Li"
                },
                {
                    "authorId": "2145348814",
                    "name": "Quang Wang"
                },
                {
                    "authorId": "1855978",
                    "name": "Zhendong Mao"
                },
                {
                    "authorId": "2031845",
                    "name": "Junbo Guo"
                },
                {
                    "authorId": "2108575864",
                    "name": "Yanyan Yang"
                },
                {
                    "authorId": "2145050375",
                    "name": "Yongdong Zhang"
                }
            ]
        },
        {
            "paperId": "6211b2f5cf757dfb253b0342c21a71d9dd40c714",
            "title": "Look Back Again: Dual Parallel Attention Network for Accurate and Robust Scene Text Recognition",
            "abstract": "Nowadays, it is a trend that using a parallel-decoupled encoder-decoder (PDED) framework in scene text recognition for its flexibility and efficiency. However, due to the inconsistent information content between queries and keys in the parallel positional attention module (PPAM) used in this kind of framework(queries: position information, keys: context and position information), visual misalignment tends to appear when confronting hard samples(e.g., blurred texts, irregular texts, or low-quality images). To tackle this issue, in this paper, we propose a dual parallel attention network (DPAN), in which a newly designed parallel context attention module (PCAM) is cascaded with the original PPAM, using linguistic contextual information to compensate for the information inconsistency between queries and keys. Specifically, in PCAM, we take the visual features from PPAM as inputs and present a bidirectional language model to enhance them with linguistic contexts to produce queries. In this way, we make the information content of the queries and keys consistent in PCAM, which helps to generate more precise visual glimpses to improve the entire PDED framework's accuracy and robustness. Experimental results verify the effectiveness of the proposed PCAM, showing the necessity of keeping the information consistency between queries and keys in the attention mechanism. On six benchmarks, including regular text and irregular text, the performance of DPAN surpasses the existing leading methods by large margins, achieving new state-of-the-art performance. The code is available on \\urlhttps://github.com/Jackandrome/DPAN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "108102793",
                    "name": "Zilong Fu"
                },
                {
                    "authorId": "143994657",
                    "name": "Hongtao Xie"
                },
                {
                    "authorId": "2071128650",
                    "name": "Guoqing Jin"
                },
                {
                    "authorId": "2031845",
                    "name": "Junbo Guo"
                }
            ]
        },
        {
            "paperId": "f0265b84baeb037a955037afe1d6c5d7eb456cb5",
            "title": "HUMA'21: 2nd International Workshop on Human-centric Multimedia Analysis",
            "abstract": "The Second International Workshop on Human-centric Multimedia Analysis is focused on human-centric analysis using multimedia information. The human-centric multimedia analysis is one of the fundamental and challenging problems of multimedia understanding. It involves various human-centric analysis tasks like face recognition, human pose estimation, person re-identification, human action recognition, person tracking, human-computer interaction, etc. Nowadays, various multimedia sensing devices and large-scale computing infrastructures are generating a wide variety of multi-modality data at a rapid velocity, which supplies rich knowledge to tackle these challenges for human-centric analysis. Researchers and engineers have strived to push the limits of human-centric multimedia analysis in a wide variety of applications, such as smart city, retailing, intelligent manufacturing, and public services. To this end, our workshop aims to provide a platform to promote exchanges and integration for the fields of human analysis and multimedia.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117994423",
                    "name": "Wu Liu"
                },
                {
                    "authorId": "2623871",
                    "name": "Xinchen Liu"
                },
                {
                    "authorId": "2346105",
                    "name": "Jingkuan Song"
                },
                {
                    "authorId": "39901030",
                    "name": "Dingwen Zhang"
                },
                {
                    "authorId": "123175679",
                    "name": "Wenbing Huang"
                },
                {
                    "authorId": "2031845",
                    "name": "Junbo Guo"
                },
                {
                    "authorId": "2118968338",
                    "name": "John Smith"
                }
            ]
        },
        {
            "paperId": "7e82015c386726f4b8f6f686b6e6bb7d1e7564bb",
            "title": "Integrating Semantic and Structural Information with Graph Convolutional Network for Controversy Detection",
            "abstract": "Identifying controversial posts on social media is a fundamental task for mining public sentiment, assessing the influence of events, and alleviating the polarized views. However, existing methods fail to 1) effectively incorporate the semantic information from content-related posts; 2) preserve the structural information for reply relationship modeling; 3) properly handle posts from topics dissimilar to those in the training set. To overcome the first two limitations, we propose Topic-Post-Comment Graph Convolutional Network (TPC-GCN), which integrates the information from the graph structure and content of topics, posts, and comments for post-level controversy detection. As to the third limitation, we extend our model to Disentangled TPC-GCN (DTPC-GCN), to disentangle topic-related and topic-unrelated features and then fuse dynamically. Extensive experiments on two real-world datasets demonstrate that our models outperform existing methods. Analysis of the results and cases proves that our models can integrate both semantic and structural information with significant generalizability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39223973",
                    "name": "L. Zhong"
                },
                {
                    "authorId": "144089410",
                    "name": "Juan Cao"
                },
                {
                    "authorId": "46630548",
                    "name": "Qiang Sheng"
                },
                {
                    "authorId": "2031845",
                    "name": "Junbo Guo"
                },
                {
                    "authorId": "2000280978",
                    "name": "Ziang Wang"
                }
            ]
        },
        {
            "paperId": "3fc5d77e3238a3a9d17698b35fb425cc227263be",
            "title": "Not All Words Are Equal: Video-specific Information Loss for Video Captioning",
            "abstract": "An ideal description for a given video should fix its gaze on salient and representative content, which is capable of distinguishing this video from others. However, the distribution of different words is unbalanced in video captioning datasets, where distinctive words for describing video-specific salient objects are far less than common words such as 'a' 'the' and 'person'. The dataset bias often results in recognition error or detail deficiency of salient but unusual objects. To address this issue, we propose a novel learning strategy called Information Loss, which focuses on the relationship between the video-specific visual content and corresponding representative words. Moreover, a framework with hierarchical visual representations and an optimized hierarchical attention mechanism is established to capture the most salient spatial-temporal visual information, which fully exploits the potential strength of the proposed learning strategy. Extensive experiments demonstrate that the ingenious guidance strategy together with the optimized architecture outperforms state-of-the-art video captioning methods on MSVD with CIDEr score 87.5, and achieves superior CIDEr score 47.7 on MSR-VTT. We also show that our Information Loss is generic which improves various models by significant margins.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51288954",
                    "name": "Jiarong Dong"
                },
                {
                    "authorId": "2054042955",
                    "name": "Ke Gao"
                },
                {
                    "authorId": "2108954002",
                    "name": "Xiaokai Chen"
                },
                {
                    "authorId": "2031845",
                    "name": "Junbo Guo"
                },
                {
                    "authorId": "144089410",
                    "name": "Juan Cao"
                },
                {
                    "authorId": "1699819",
                    "name": "Yongdong Zhang"
                }
            ]
        },
        {
            "paperId": "704b14df5429416f9178c54b570972aadf392aea",
            "title": "Near-infrared Image Guided Neural Networks for Color Image Denoising",
            "abstract": "Noisy color image and guided near-infrared (NIR) image can be jointly employed to eliminate noise and enhance details. Existing methods mostly rely on explicit designed filters and hand-crafted objective function optimization. These methods usually introduce erroneous structures from guidance signal. Besides, they are time-consuming and not suitable for real time applications. In this paper, we come up with a learning based method. The noisy color image and NIR image are fused, then fed into a fully convolutional neural network. The network learns a directly map from degraded image to restored sharp image. Our architecture can effectively eliminate image noise and transfer detail structure from guided image. Our trained network accepts any resolution of input image and runs in constant time. We evaluate the presented approach on both synthetic and real images. Results show that our approach outperforms the state-of-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108010158",
                    "name": "Xuehui Wang"
                },
                {
                    "authorId": "152130996",
                    "name": "Feng Dai"
                },
                {
                    "authorId": "3193532",
                    "name": "Yike Ma"
                },
                {
                    "authorId": "2031845",
                    "name": "Junbo Guo"
                },
                {
                    "authorId": "2111000077",
                    "name": "Qiang Zhao"
                },
                {
                    "authorId": "1699819",
                    "name": "Yongdong Zhang"
                }
            ]
        },
        {
            "paperId": "fd91524a58f25dd25fd76b5521c2fb0da10fb6a9",
            "title": "Exploiting Multi-domain Visual Information for Fake News Detection",
            "abstract": "The increasing popularity of social media promotes the proliferation of fake news. With the development of multimedia technology, fake news attempts to utilize multimedia content with images or videos to attract and mislead readers for rapid dissemination, which makes visual content an important part of fake news. Fake-news images, images attached to fake news posts, include not only fake images that are maliciously tampered but also real images that are wrongly used to represent irrelevant events. Hence, how to fully exploit the inherent characteristics of fake-news images is an important but challenging problem for fake news detection. In the real world, fake-news images may have significantly different characteristics from real-news images at both physical and semantic levels, which can be clearly reflected in the frequency and pixel domain, respectively. Therefore, we propose a novel framework Multi-domain Visual Neural Network (MVNN) to fuse the visual information of frequency and pixel domains for detecting fake news. Specifically, we design a CNN-based network to automatically capture the complex patterns of fake-news images in the frequency domain; and utilize a multi-branch CNN-RNN model to extract visual features from different semantic levels in the pixel domain. An attention mechanism is utilized to fuse the feature representations of frequency and pixel domains dynamically. Extensive experiments conducted on a real world dataset demonstrate that MVNN outperforms existing methods with at least 9.2% in accuracy, and can help improve the performance of multi-modal fake news detection by over 5.2%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2063066946",
                    "name": "Peng Qi"
                },
                {
                    "authorId": "144089410",
                    "name": "Juan Cao"
                },
                {
                    "authorId": "2006294831",
                    "name": "Tianyun Yang"
                },
                {
                    "authorId": "2031845",
                    "name": "Junbo Guo"
                },
                {
                    "authorId": "30821609",
                    "name": "Jintao Li"
                }
            ]
        },
        {
            "paperId": "13402ca147038e8424a4ac18ecfff968083308d1",
            "title": "Style Separation and Synthesis via Generative Adversarial Networks",
            "abstract": "Style synthesis attracts great interests recently, while few works focus on its dual problem \"style separation\". In this paper, we propose the Style Separation and Synthesis Generative Adversarial Network (S3-GAN) to simultaneously implement style separation and style synthesis on object photographs of specific categories. Based on the assumption that the object photographs lie on a manifold, and the contents and styles are independent, we employ S3-GAN to build mappings between the manifold and a latent vector space for separating and synthesizing the contents and styles. The S3-GAN consists of an encoder network, a generator network, and an adversarial network. The encoder network performs style separation by mapping an object photograph to a latent vector. Two halves of the latent vector represent the content and style, respectively. The generator network performs style synthesis by taking a concatenated vector as input. The concatenated vector contains the style half vector of the style target image and the content half vector of the content target image. Once obtaining the images from the generator network, an adversarial network is imposed to generate more photo-realistic images. Experiments on CelebA and UT Zappos 50K datasets demonstrate that the S3-GAN has the capacity of style separation and synthesis simultaneously, and could capture various styles in a single model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49775222",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "144044848",
                    "name": "Sheng Tang"
                },
                {
                    "authorId": "2116610600",
                    "name": "Yu Li"
                },
                {
                    "authorId": "2031845",
                    "name": "Junbo Guo"
                },
                {
                    "authorId": "1699819",
                    "name": "Yongdong Zhang"
                },
                {
                    "authorId": "30821609",
                    "name": "Jintao Li"
                },
                {
                    "authorId": "143653681",
                    "name": "Shuicheng Yan"
                }
            ]
        },
        {
            "paperId": "3a16f4f1b63df43280f30b8cdd370553429b4381",
            "title": "Rumor Detection with Hierarchical Social Attention Network",
            "abstract": "Microblogs have become one of the most popular platforms for news sharing. However, due to its openness and lack of supervision, rumors could also be easily posted and propagated on social networks, which could cause huge panic and threat during its propagation. In this paper, we detect rumors by leveraging hierarchical representations at different levels and the social contexts. Specifically, we propose a novel hierarchical neural network combined with social information (HSA-BLSTM). We first build a hierarchical bidirectional long short-term memory model for representation learning. Then, the social contexts are incorporated into the network via attention mechanism, such that important semantic information is introduced to the framework for more robust rumor detection. Experimental results on two real world datasets demonstrate that the proposed method outperforms several state-of-the-arts in both rumor detection and early detection scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110877865",
                    "name": "Han Guo"
                },
                {
                    "authorId": "144089410",
                    "name": "Juan Cao"
                },
                {
                    "authorId": "2108008944",
                    "name": "Yazi Zhang"
                },
                {
                    "authorId": "2031845",
                    "name": "Junbo Guo"
                },
                {
                    "authorId": "30821609",
                    "name": "Jintao Li"
                }
            ]
        }
    ]
}