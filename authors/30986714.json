{
    "authorId": "30986714",
    "papers": [
        {
            "paperId": "1783e71045bcc6e9c76679d7ec2c0198efe0043d",
            "title": "Can Adversarial Examples Be Parsed to Reveal Victim Model Information?",
            "abstract": "Numerous adversarial attack methods have been developed to generate imperceptible image perturbations that can cause erroneous predictions of state-of-the-art machine learning (ML) models, in particular, deep neural networks (DNNs). Despite intense research on adversarial attacks, little effort was made to uncover 'arcana' carried in adversarial attacks. In this work, we ask whether it is possible to infer data-agnostic victim model (VM) information (i.e., characteristics of the ML model or DNN used to generate adversarial attacks) from data-specific adversarial instances. We call this 'model parsing of adversarial attacks' - a task to uncover 'arcana' in terms of the concealed VM information in attacks. We approach model parsing via supervised learning, which correctly assigns classes of VM's model attributes (in terms of architecture type, kernel size, activation function, and weight sparsity) to an attack instance generated from this VM. We collect a dataset of adversarial attacks across 7 attack types generated from 135 victim models (configured by 5 architecture types, 3 kernel size setups, 3 activation function types, and 3 weight sparsity ratios). We show that a simple, supervised model parsing network (MPN) is able to infer VM attributes from unseen adversarial attacks if their attack settings are consistent with the training setting (i.e., in-distribution generalization assessment). We also provide extensive experiments to justify the feasibility of VM parsing from adversarial attacks, and the influence of training and evaluation factors in the parsing performance (e.g., generalization challenge raised in out-of-distribution evaluation). We further demonstrate how the proposed MPN can be used to uncover the source VM attributes from transfer attacks, and shed light on a potential connection between model parsing and attack transferability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100630765",
                    "name": "Yuguang Yao"
                },
                {
                    "authorId": "2168533640",
                    "name": "Jiancheng Liu"
                },
                {
                    "authorId": "2114981469",
                    "name": "Yifan Gong"
                },
                {
                    "authorId": "2161013139",
                    "name": "Xiaoming Liu"
                },
                {
                    "authorId": "2136922252",
                    "name": "Yanzhi Wang"
                },
                {
                    "authorId": "145282404",
                    "name": "X. Lin"
                },
                {
                    "authorId": "30986714",
                    "name": "Sijia Liu"
                }
            ]
        },
        {
            "paperId": "3343c26a70fc4a69b80bfef5b7ba7eca40a4481c",
            "title": "AutoSeqRec: Autoencoder for Efficient Sequential Recommendation",
            "abstract": "Sequential recommendation demonstrates the capability to recommend items by modeling the sequential behavior of users. Traditional methods typically treat users as sequences of items, overlooking the collaborative relationships among them. Graph-based methods incorporate collaborative information by utilizing the user-item interaction graph. However, these methods sometimes face challenges in terms of time complexity and computational efficiency. To address these limitations, this paper presents AutoSeqRec, an incremental recommendation model specifically designed for sequential recommendation tasks. AutoSeqRec is based on autoencoders and consists of an encoder and three decoders within the autoencoder architecture. These components consider both the user-item interaction matrix and the rows and columns of the item transition matrix. The reconstruction of the user-item interaction matrix captures user long-term preferences through collaborative filtering. In addition, the rows and columns of the item transition matrix represent the item out-degree and in-degree hopping behavior, which allows for modeling the user's short-term interests. When making incremental recommendations, only the input matrices need to be updated, without the need to update parameters, which makes AutoSeqRec very efficient. Comprehensive evaluations demonstrate that AutoSeqRec outperforms existing methods in terms of accuracy, while showcasing its robustness and efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30986714",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "2108421421",
                    "name": "Jiahao Liu"
                },
                {
                    "authorId": "1759997",
                    "name": "Hansu Gu"
                },
                {
                    "authorId": "2108481070",
                    "name": "Dongsheng Li"
                },
                {
                    "authorId": "1711569",
                    "name": "T. Lu"
                },
                {
                    "authorId": "1896008570",
                    "name": "Peng Zhang"
                },
                {
                    "authorId": "39341617",
                    "name": "Ning Gu"
                }
            ]
        },
        {
            "paperId": "377e5e0e5f4253442ad1e476c076a3928b031811",
            "title": "Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks",
            "abstract": "Due to the significant computational challenge of training large-scale graph neural networks (GNNs), various sparse learning techniques have been exploited to reduce memory and storage costs. Examples include \\textit{graph sparsification} that samples a subgraph to reduce the amount of data aggregation and \\textit{model sparsification} that prunes the neural network to reduce the number of trainable weights. Despite the empirical successes in reducing the training cost while maintaining the test accuracy, the theoretical generalization analysis of sparse learning for GNNs remains elusive. To the best of our knowledge, this paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error. It proves analytically that both sampling important nodes and pruning neurons with the lowest-magnitude can reduce the sample complexity and improve convergence without compromising the test accuracy. Although the analysis is centered on two-layer GNNs with structural constraints on data, the insights are applicable to more general setups and justified by both synthetic and practical citation datasets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2108432203",
                    "name": "Shuai Zhang"
                },
                {
                    "authorId": "39872583",
                    "name": "M. Wang"
                },
                {
                    "authorId": "153191489",
                    "name": "Pin-Yu Chen"
                },
                {
                    "authorId": "30986714",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "1606015788",
                    "name": "Songtao Lu"
                },
                {
                    "authorId": "2156102035",
                    "name": "Miaoyuan Liu"
                }
            ]
        },
        {
            "paperId": "44b04ff4944031def4f6cfc80f4baca2195f1b4f",
            "title": "Text-Visual Prompting for Efficient 2D Temporal Video Grounding",
            "abstract": "In this paper, we study the problem of temporal video grounding (TVG), which aims to predict the starting/ending time points of moments described by a text sentence within a long untrimmed video. Benefiting from fine-grained 3D visual features, the TVG techniques have achieved remarkable progress in recent years. However, the high complexity of 3D convolutional neural networks (CNNs) makes extracting dense 3D visual features time-consuming, which calls for intensive memory and computing resources. Towards efficient TVG, we propose a novel text-visual prompting (TVP) framework, which incorporates optimized perturbation patterns (that we call 'prompts') into both visual inputs and textual features of a TVG model. In sharp contrast to 3D CNNs, we show that TVP allows us to effectively co-train vision encoder and language encoder in a 2D TVG model and improves the performance of crossmodal feature fusion using only low-complexity sparse 2D visual features. Further, we propose a Temporal-Distance IoU (TDIoU) loss for efficient learning of TVG. Experiments on two benchmark datasets, Charades-STA and Activityblet Captions datasets, empirically show that the proposed TVP significantly boosts the performance of 2D TVG (e.g., 9.79% improvement on Charades-STA and 30.77% improvement on ActivityNet Captions) and achieves $5\\times$ inference acceleration over TVG using 3D visual features. Codes are available at Open.Intel.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108441128",
                    "name": "Yimeng Zhang"
                },
                {
                    "authorId": "89507637",
                    "name": "Xin Chen"
                },
                {
                    "authorId": "2051552791",
                    "name": "Jinghan Jia"
                },
                {
                    "authorId": "30986714",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "2162150143",
                    "name": "Ke Ding"
                }
            ]
        },
        {
            "paperId": "641a89edf2d60424a5a83627d7c694bc2de4e129",
            "title": "Fairness Improves Learning from Noisily Labeled Long-Tailed Data",
            "abstract": "Both long-tailed and noisily labeled data frequently appear in real-world applications and impose significant challenges for learning. Most prior works treat either problem in an isolated way and do not explicitly consider the coupling effects of the two. Our empirical observation reveals that such solutions fail to consistently improve the learning when the dataset is long-tailed with label noise. Moreover, with the presence of label noise, existing methods do not observe universal improvements across different sub-populations; in other words, some sub-populations enjoyed the benefits of improved accuracy at the cost of hurting others. Based on these observations, we introduce the Fairness Regularizer (FR), inspired by regularizing the performance gap between any two sub-populations. We show that the introduced fairness regularizer improves the performances of sub-populations on the tail and the overall learning performance. Extensive experiments demonstrate the effectiveness of the proposed solution when complemented with certain existing popular robust or class-balanced methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "103410241",
                    "name": "Jiaheng Wei"
                },
                {
                    "authorId": "49658780",
                    "name": "Zhaowei Zhu"
                },
                {
                    "authorId": "47537639",
                    "name": "Gang Niu"
                },
                {
                    "authorId": "121698214",
                    "name": "Tongliang Liu"
                },
                {
                    "authorId": "30986714",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "67154907",
                    "name": "Masashi Sugiyama"
                },
                {
                    "authorId": "2152799147",
                    "name": "Yang Liu"
                }
            ]
        },
        {
            "paperId": "7c5aa120a582bd192b2be4952953040b41d3d503",
            "title": "Certified Robustness for Large Language Models with Self-Denoising",
            "abstract": "Although large language models (LLMs) have achieved great success in vast real-world applications, their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments. In these contexts, it is crucial to ensure that every prediction made by large language models is stable, i.e., LLM predictions should be consistent given minor differences in the input. This largely falls into the study of certified robust LLMs, i.e., all predictions of LLM are certified to be correct in a local region around the input. Randomized smoothing has demonstrated great potential in certifying the robustness and prediction stability of LLMs. However, randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. As a result, its direct application to LLMs remains challenging and often results in a small certification radius. To address this issue, we take advantage of the multitasking nature of LLMs and propose to denoise the corrupted inputs with LLMs in a self-denoising manner. Different from previous works like denoised smoothing, which requires training a separate model to robustify LLM, our method enjoys far better efficiency and flexibility. Our experiment results show that our method outperforms the existing certification methods under both certified robustness and empirical robustness. The codes are available at https://github.com/UCSB-NLP-Chang/SelfDenoise.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144352362",
                    "name": "Zhen Zhang"
                },
                {
                    "authorId": "46266569",
                    "name": "Guanhua Zhang"
                },
                {
                    "authorId": "1955614986",
                    "name": "Bairu Hou"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2117897052",
                    "name": "Qing Li"
                },
                {
                    "authorId": "30986714",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "37873860",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2191955409",
                    "name": "Shiyu Chang"
                }
            ]
        },
        {
            "paperId": "a96762ae0ac80206f33657d2941beae41c09b16b",
            "title": "MERCY: Multiple Response Ranking Concurrently in Realistic Open-Domain Conversational Systems",
            "abstract": "Automatic Evaluation (AE) and Response Selection (RS) models assign quality scores to various candidate responses and rank them in conversational setups. Prior response ranking research compares various models\u2019 performance on synthetically generated test sets. In this work, we investigate the performance of model-based reference-free AE and RS models on our constructed response ranking datasets that mirror real-case scenarios of ranking candidates during inference time. Metrics\u2019 unsatisfying performance can be interpreted as their low generalizability over more pragmatic conversational domains such as human-chatbot dialogs. To alleviate this issue we propose a novel RS model called MERCY that simulates human behavior in selecting the best candidate by taking into account distinct candidates concurrently and learns to rank them. In addition, MERCY leverages natural language feedback as another component to help the ranking task by explaining why each candidate response is relevant/irrelevant to the dialog context. These feedbacks are generated by prompting large language models in a few-shot setup. Our experiments show the better performance of MERCY over baselines for the response ranking task in our curated realistic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3022427",
                    "name": "Sarik Ghazarian"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "30986714",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "2239906544",
                    "name": "Violet Peng"
                },
                {
                    "authorId": "2240100316",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                }
            ]
        },
        {
            "paperId": "b85a020b4fc7554b01e64be82f64a35fef887c45",
            "title": "Towards Understanding How Self-training Tolerates Data Backdoor Poisoning",
            "abstract": "Recent studies on backdoor attacks in model training have shown that polluting a small portion of training data is sufficient to produce incorrect manipulated predictions on poisoned test-time data while maintaining high clean accuracy in downstream tasks. The stealthiness of backdoor attacks has imposed tremendous defense challenges in today's machine learning paradigm. In this paper, we explore the potential of self-training via additional unlabeled data for mitigating backdoor attacks. We begin by making a pilot study to show that vanilla self-training is not effective in backdoor mitigation. Spurred by that, we propose to defend the backdoor attacks by leveraging strong but proper data augmentations in the self-training pseudo-labeling stage. We find that the new self-training regime help in defending against backdoor attacks to a great extent. Its effectiveness is demonstrated through experiments for different backdoor triggers on CIFAR-10 and a combination of CIFAR-10 with an additional unlabeled 500K TinyImages dataset. Finally, we explore the direction of combining self-supervised representation learning with self-training for further improvement in backdoor defense.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39484102",
                    "name": "Soumyadeep Pal"
                },
                {
                    "authorId": "153537425",
                    "name": "Ren Wang"
                },
                {
                    "authorId": "100630765",
                    "name": "Yuguang Yao"
                },
                {
                    "authorId": "30986714",
                    "name": "Sijia Liu"
                }
            ]
        },
        {
            "paperId": "0908532a6d86bfaba8cacc42e1585f22128a0c38",
            "title": "ClawSAT: Towards Both Robust and Accurate Code Models",
            "abstract": "We integrate contrastive learning (CL) with adversarial learning to co-optimize the robustness and accuracy of code models. Different from existing works, we show that code obfuscation, a standard code transformation operation, provides novel means to generate complementary \u2018views\u2019 of a code that enable us to achieve both robust and accurate code models. To the best of our knowledge, this is the first systematic study to explore and exploit the robustness and accuracy benefits of (multi-view) code obfuscations in code models. Specifically, we first adopt adversarial codes as robustness-promoting views in CL at the self-supervised pre-training phase. This yields improved robustness and transferability for downstream tasks. Next, at the supervised fine-tuning stage, we show that adversarial training with a proper temporally-staggered schedule of adversarial code generation can further improve robustness and accuracy of the pre-trained code model. Built on the above two modules, we develop ClawSAT, a novel self-supervised learning (SSL) framework for code by integrating CL with adversarial views (Claw) with staggered adversarial training (SAT). On evaluating three downstream tasks across Python and Java, we show that ClawSAT consistently yields the best robustness and accuracy (e.g. 11% in robustness and 6% in accuracy on the code summarization task in Python). We additionally demonstrate the effectiveness of adversarial learning in Claw by analyzing the characteristics of the loss landscape and interpretability of the pre-trained models. Codes are available at https://github.com/OPTML-Group/Claw-SAT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051552791",
                    "name": "Jinghan Jia"
                },
                {
                    "authorId": "2437563",
                    "name": "Shashank Srikant"
                },
                {
                    "authorId": "2056766911",
                    "name": "Tamara Mitrovska"
                },
                {
                    "authorId": "144158271",
                    "name": "Chuang Gan"
                },
                {
                    "authorId": "2122374354",
                    "name": "Shiyu Chang"
                },
                {
                    "authorId": "30986714",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "1398192342",
                    "name": "Una-May O\u2019Reilly"
                }
            ]
        },
        {
            "paperId": "1a1df10346dc4b9da55d219064024f79e87de944",
            "title": "Fairness Reprogramming",
            "abstract": "Despite a surge of recent advances in promoting machine Learning (ML) fairness, the existing mainstream approaches mostly require retraining or finetuning the entire weights of the neural network to meet the fairness criteria. However, this is often infeasible in practice for those large-scale trained models due to large computational and storage costs, low data efficiency, and model privacy issues. In this paper, we propose a new generic fairness learning paradigm, called FairReprogram, which incorporates the model reprogramming technique. Specifically, FairReprogram considers the case where models can not be changed and appends to the input a set of perturbations, called the fairness trigger, which is tuned towards the fairness criteria under a min-max formulation. We further introduce an information-theoretic framework that explains why and under what conditions fairness goals can be achieved using the fairness trigger. We show both theoretically and empirically that the fairness trigger can effectively obscure demographic biases in the output prediction of fixed ML models by providing false demographic information that hinders the model from utilizing the correct demographic information to make the prediction. Extensive experiments on both NLP and CV datasets demonstrate that our method can achieve better fairness improvements than retraining-based methods with far less data dependency under two widely-used fairness criteria. Codes are available at https://github.com/UCSB-NLP-Chang/Fairness-Reprogramming.git.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46266569",
                    "name": "Guanhua Zhang"
                },
                {
                    "authorId": "2155369380",
                    "name": "Yihua Zhang"
                },
                {
                    "authorId": "37873860",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2117897052",
                    "name": "Qing Li"
                },
                {
                    "authorId": "30986714",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "2122374354",
                    "name": "Shiyu Chang"
                }
            ]
        }
    ]
}