{
    "authorId": "2321455461",
    "papers": [
        {
            "paperId": "be7122f7b2db3bce3137519b1f81e79fa57c9eaa",
            "title": "Eureka: Evaluating and Understanding Large Foundation Models",
            "abstract": "Rigorous and reproducible evaluation is critical for assessing the state of the art and for guiding scientific advances in Artificial Intelligence. Evaluation is challenging in practice due to several reasons, including benchmark saturation, lack of transparency in methods used for measurement, development challenges in extracting measurements for generative tasks, and, more generally, the extensive number of capabilities required for a well-rounded comparison across models. We make three contributions to alleviate the above challenges. First, we present Eureka, an open-source framework for standardizing evaluations of large foundation models beyond single-score reporting and rankings. Second, we introduce Eureka-Bench as an extensible collection of benchmarks testing capabilities that (i) are still challenging for state-of-the-art models and (ii) represent fundamental but overlooked language and multimodal capabilities. The inherent space for improvement in non-saturated benchmarks enables us to discover meaningful differences between models at a capability level. Third, using Eureka, we conduct an analysis of 12 state-of-the-art models, providing in-depth insights into failure understanding and model comparison, which can be leveraged to plan targeted improvements. In contrast to recent trends in reports and leaderboards showing absolute rankings and claims for one model or another to be the best, our analysis shows that there is no such best model. Different models have different strengths, but there are models that appear more often than others as best performers for some capabilities. Despite the recent improvements, current models still struggle with several fundamental capabilities including detailed image understanding, benefiting from multimodal input when available rather than fully relying on language, factuality and grounding for information retrieval, and over refusals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2321484904",
                    "name": "Jingya Chen"
                },
                {
                    "authorId": "2250480908",
                    "name": "Neel Joshi"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2321455461",
                    "name": "Eduardo Salinas"
                },
                {
                    "authorId": "143729959",
                    "name": "Vibhav Vineet"
                },
                {
                    "authorId": "2321455359",
                    "name": "James Woffinden-Luey"
                },
                {
                    "authorId": "2670023",
                    "name": "Safoora Yousefi"
                }
            ]
        }
    ]
}