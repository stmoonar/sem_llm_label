{
    "authorId": "1831395",
    "papers": [
        {
            "paperId": "de262b7ccab4a9b85c801f301918f4b61df3d6af",
            "title": "Accountability in Algorithmic Systems: From Principles to Practice",
            "abstract": "Growing concerns over the societal implications of artificial intelligence has motivated an interdisciplinary push towards mechanisms and tools that hold algorithmic systems accountable. Although there have been considerable strides around defining what it means to hold AI systems accountable, operationalizing those principles have created a barrage of challenges. Researchers, practitioners, and regulators have all raised concerns about the completeness of accountability methods and observed spikes in anxiousness about the potential risk of these tools being manipulated as rubber stamps of approval while harms continue to slip through the cracks. This interactive panel gathers researchers and practitioners with expertise in HCI, Responsible AI, Machine Learning, and Public Policy to critically discuss issues regarding accountability in algorithmic systems to reflect on potential opportunities for re-imagining scalable directions for accountability within these systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40074624",
                    "name": "Daricia Wilkinson"
                },
                {
                    "authorId": "1810680217",
                    "name": "Kate Crawford"
                },
                {
                    "authorId": "1831395",
                    "name": "Hanna M. Wallach"
                },
                {
                    "authorId": "2214749399",
                    "name": "Deborah Raji"
                },
                {
                    "authorId": "1913516",
                    "name": "B. Rakova"
                },
                {
                    "authorId": "2214838126",
                    "name": "Ranjit Singh"
                },
                {
                    "authorId": "39331349",
                    "name": "Angelika Strohmayer"
                },
                {
                    "authorId": "2214843",
                    "name": "Ethan Zuckerman"
                }
            ]
        },
        {
            "paperId": "b43e2d429f6a2f52336c9749651f34d354062418",
            "title": "Understanding Machine Learning Practitioners' Data Documentation Perceptions, Needs, Challenges, and Desiderata",
            "abstract": "Data is central to the development and evaluation of machine learning (ML) models. However, the use of problematic or inappropriate datasets can result in harms when the resulting models are deployed. To encourage responsible AI practice through more deliberate reflection on datasets and transparency around the processes by which they are created, researchers and practitioners have begun to advocate for increased data documentation and have proposed several data documentation frameworks. However, there is little research on whether these data documentation frameworks meet the needs of ML practitioners, who both create and consume datasets. To address this gap, we set out to understand ML practitioners' data documentation perceptions, needs, challenges, and desiderata, with the ultimate goal of deriving design requirements that can inform future data documentation frameworks. We conducted a series of semi-structured interviews with 14 ML practitioners at a single large, international technology company. We had them answer a list of questions taken from datasheets for datasets~\\citegebru2018datasheets. Our findings show that current approaches to data documentation are largely ad hoc and myopic in nature. Participants expressed needs for data documentation frameworks to be adaptable to their contexts, integrated into their existing tools and workflows, and automated wherever possible. Despite the fact that data documentation frameworks are often motivated from the perspective of responsible AI, participants did not make the connection between the questions that they were asked to answer and their responsible AI implications. In addition, participants often had difficulties prioritizing the needs of dataset consumers and providing information that someone unfamiliar with their datasets might need to know. Based on these findings, we derive seven design requirements for future data documentation frameworks such as more actionable guidance on how the characteristics of datasets might result in harms and how these harms might be mitigated, more explicit prompts for reflection, automated adaptation to different contexts, and integration into ML practitioners' existing tools and workflows.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144884428",
                    "name": "A. Heger"
                },
                {
                    "authorId": "2062953227",
                    "name": "Elizabeth B. Marquis"
                },
                {
                    "authorId": "3109339",
                    "name": "Mihaela Vorvoreanu"
                },
                {
                    "authorId": "1831395",
                    "name": "Hanna M. Wallach"
                },
                {
                    "authorId": "147171110",
                    "name": "J. W. Vaughan"
                }
            ]
        },
        {
            "paperId": "eb344bd1e99ab4d204e1d82f1dd9faa6087b79da",
            "title": "REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research",
            "abstract": "Transparency around limitations can improve the scientific rigor of research, help ensure appropriate interpretation of research findings, and make research claims more credible. Despite these benefits, the machine learning (ML) research community lacks well-developed norms around disclosing and discussing limitations. To address this gap, we conduct an iterative design process with 30 ML and ML-adjacent researchers to develop and test REAL ML, a set of guided activities to help ML researchers recognize, explore, and articulate the limitations of their research. Using a three-stage interview and survey study, we identify ML researchers\u2019 perceptions of limitations, as well as the challenges they face when recognizing, exploring, and articulating limitations. We develop REAL ML to address some of these practical challenges, and highlight additional cultural challenges that will require broader shifts in community norms to address. We hope our study and REAL ML help move the ML research community toward more active and appropriate engagement with limitations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109849714",
                    "name": "Jessie J. Smith"
                },
                {
                    "authorId": "1719124",
                    "name": "Saleema Amershi"
                },
                {
                    "authorId": "2881033",
                    "name": "Solon Barocas"
                },
                {
                    "authorId": "1831395",
                    "name": "Hanna M. Wallach"
                },
                {
                    "authorId": "147171110",
                    "name": "J. W. Vaughan"
                }
            ]
        },
        {
            "paperId": "15455ad33ae60c10a276de85fee304de6a978e18",
            "title": "Assessing the Fairness of AI Systems: AI Practitioners' Processes, Challenges, and Needs for Support",
            "abstract": "Various tools and practices have been developed to support practitioners in identifying, assessing, and mitigating fairness-related harms caused by AI systems. However, prior research has highlighted gaps between the intended design of these tools and practices and their use within particular contexts, including gaps caused by the role that organizational factors play in shaping fairness work. In this paper, we investigate these gaps for one such practice: disaggregated evaluations of AI systems, intended to uncover performance disparities between demographic groups. By conducting semi-structured interviews and structured workshops with thirty-three AI practitioners from ten teams at three technology companies, we identify practitioners' processes, challenges, and needs for support when designing disaggregated evaluations. We find that practitioners face challenges when choosing performance metrics, identifying the most relevant direct stakeholders and demographic groups on which to focus, and collecting datasets with which to conduct disaggregated evaluations. More generally, we identify impacts on fairness work stemming from a lack of engagement with direct stakeholders or domain experts, business imperatives that prioritize customers over marginalized groups, and the drive to deploy AI systems at scale.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38113700",
                    "name": "Michael A. Madaio"
                },
                {
                    "authorId": "2137967742",
                    "name": "Lisa Egede"
                },
                {
                    "authorId": "2985239",
                    "name": "Hariharan Subramonyam"
                },
                {
                    "authorId": "4006636",
                    "name": "Jennifer Wortman Vaughan"
                },
                {
                    "authorId": "1831395",
                    "name": "Hanna M. Wallach"
                }
            ]
        },
        {
            "paperId": "3e65f572322e192fe36ae52a8a7f025b0685dfc6",
            "title": "Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets",
            "abstract": "Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system\u2019s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens\u2014originating from the social sciences\u2014to inventory a range of pitfalls that threaten these benchmarks\u2019 validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3422038",
                    "name": "Su Lin Blodgett"
                },
                {
                    "authorId": "2057983441",
                    "name": "Gilsinia Lopez"
                },
                {
                    "authorId": "2064011617",
                    "name": "Alexandra Olteanu"
                },
                {
                    "authorId": "1562202621",
                    "name": "Robert Sim"
                },
                {
                    "authorId": "1831395",
                    "name": "Hanna M. Wallach"
                }
            ]
        },
        {
            "paperId": "6b203d09193bf8cac934f9ab6e98a9184738990a",
            "title": "Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs",
            "abstract": "Disaggregated evaluations of AI systems, in which system performance is assessed and reported separately for different groups of people, are conceptually simple. However, their design involves a variety of choices. Some of these choices influence the results that will be obtained, and thus the conclusions that can be drawn; others influence the impacts---both beneficial and harmful---that a disaggregated evaluation will have on people, including the people whose data is used to conduct the evaluation. We argue that a deeper understanding of these choices will enable researchers and practitioners to design careful and conclusive disaggregated evaluations. We also argue that better documentation of these choices, along with the underlying considerations and tradeoffs that have been made, will help others when interpreting an evaluation's results and conclusions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2881033",
                    "name": "Solon Barocas"
                },
                {
                    "authorId": "2582404",
                    "name": "Anhong Guo"
                },
                {
                    "authorId": "1783184",
                    "name": "Ece Kamar"
                },
                {
                    "authorId": "2052356058",
                    "name": "J. Krones"
                },
                {
                    "authorId": "144844426",
                    "name": "M. Morris"
                },
                {
                    "authorId": "4006636",
                    "name": "Jennifer Wortman Vaughan"
                },
                {
                    "authorId": "2052356071",
                    "name": "Duncan Wadsworth"
                },
                {
                    "authorId": "1831395",
                    "name": "Hanna M. Wallach"
                }
            ]
        },
        {
            "paperId": "6f6963859ddc3a54db2a9ec85f6596c865a906eb",
            "title": "Doubly Non-Central Beta Matrix Factorization for DNA Methylation Data",
            "abstract": "We present a new non-negative matrix factorization model for $(0,1)$ bounded-support data based on the doubly non-central beta (DNCB) distribution, a generalization of the beta distribution. The expressiveness of the DNCB distribution is particularly useful for modeling DNA methylation datasets, which are typically highly dispersed and multi-modal; however, the model structure is sufficiently general that it can be adapted to many other domains where latent representations of $(0,1)$ bounded-support data are of interest. Although the DNCB distribution lacks a closed-form conjugate prior, several augmentations let us derive an efficient posterior inference algorithm composed entirely of analytic updates. Our model improves out-of-sample predictive performance on both real and synthetic DNA methylation datasets over state-of-the-art methods in bioinformatics. In addition, our model yields meaningful latent representations that accord with existing biological knowledge.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "50545056",
                    "name": "Aaron Schein"
                },
                {
                    "authorId": "2111845668",
                    "name": "Anjali Nagulpally"
                },
                {
                    "authorId": "1831395",
                    "name": "Hanna M. Wallach"
                },
                {
                    "authorId": "145486286",
                    "name": "Patrick Flaherty"
                }
            ]
        },
        {
            "paperId": "a991ea321525e23ce8ab7b4915c87a9919e842c9",
            "title": "A Human-Centered Interpretability Framework Based on Weight of Evidence",
            "abstract": "In this paper, we take a human-centered approach to interpretable machine learning. First, drawing inspiration from the study of explanation in philosophy, cognitive science, and the social sciences, we propose a list of design principles for machine-generated explanations that are meaningful to humans. Using the concept of weight of evidence from information theory, we develop a method for producing explanations that adhere to these principles. We show that this method can be adapted to handle high-dimensional, multi-class settings, yielding a \ufb02exible meta-algorithm for generating explanations. We demonstrate that these explanations can be estimated accurately from \ufb01nite samples and are robust to small perturbations of the inputs. We also evaluate our method through a qualitative user study with machine learning practitioners, where we observe that the resulting explanations are usable despite some participants struggling with background concepts like prior class probabilities. Finally, we conclude by surfacing design implications for interpretability tools.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1390096054",
                    "name": "David Alvarez-Melis"
                },
                {
                    "authorId": "48712213",
                    "name": "Harmanpreet Kaur"
                },
                {
                    "authorId": "2065041692",
                    "name": "Hal Daum'e"
                },
                {
                    "authorId": "1831395",
                    "name": "Hanna M. Wallach"
                },
                {
                    "authorId": "4006636",
                    "name": "Jennifer Wortman Vaughan"
                }
            ]
        },
        {
            "paperId": "ab3c7124f1b1c5518e180f48238dd9cee55d35a9",
            "title": "From Human Explanation to Model Interpretability: A Framework Based on Weight of Evidence",
            "abstract": "We take inspiration from the study of human explanation to inform the design and evaluation of interpretability methods in machine learning. First, we survey the literature on human explanation in philosophy, cognitive science, and the social sciences, and propose a list of design principles for machine-generated explanations that are meaningful to humans. Using the concept of weight of evidence from information theory, we develop a method for generating explanations that adhere to these principles. We show that this method can be adapted to handle high-dimensional, multi-class settings, yielding a flexible framework for generating explanations. We demonstrate that these explanations can be estimated accurately from finite samples and are robust to small perturbations of the inputs. We also evaluate our method through a qualitative user study with machine learning practitioners, where we observe that the resulting explanations are usable despite some participants struggling with background concepts like prior class probabilities. Finally, we conclude by surfacing design implications for interpretability tools in general.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2127471133",
                    "name": "David Alvarez-Melis"
                },
                {
                    "authorId": "48712213",
                    "name": "Harmanpreet Kaur"
                },
                {
                    "authorId": "2065041692",
                    "name": "Hal Daum'e"
                },
                {
                    "authorId": "1831395",
                    "name": "Hanna M. Wallach"
                },
                {
                    "authorId": "4006636",
                    "name": "Jennifer Wortman Vaughan"
                }
            ]
        },
        {
            "paperId": "bc89a6fbf43cf911f71e5428d0b4a70fa5a40be9",
            "title": "A Human-Centered Agenda for Intelligible Machine Learning",
            "abstract": "To build machine learning systems that are reliable, trustworthy, and fair, we must be able to provide relevant stakeholders with an understanding of how these systems work. Yet what makes a system \u201cintelligible\u201d is difficult to pin down. Intelligibility is a fundamentally human-centered concept that lacks a one-size-fits-all solution. Although many intelligibility techniques have been proposed in the machine learning literature, there are many more open questions about how best to provide stakeholders with the information they need to achieve their desired goals. In this chapter, we begin with an overview of the intelligible machine learning landscape and give several examples of the diverse ways in which needs for intelligibility can arise. We provide an overview of the techniques for achieving intelligibility that have been proposed in the machine learning literature. We discuss the importance of taking a human-centered strategy when designing intelligibility techniques or when verifying that these techniques achieve their intended goals. We also argue that the notion of intelligibility should be expanded beyond machine learning models to other components of machine learning systems, such as datasets and performance metrics. Finally, we emphasize the necessity of tight integration between the machine learning and human\u2013computer interaction communities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4006636",
                    "name": "Jennifer Wortman Vaughan"
                },
                {
                    "authorId": "1831395",
                    "name": "Hanna M. Wallach"
                }
            ]
        }
    ]
}