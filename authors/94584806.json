{
    "authorId": "94584806",
    "papers": [
        {
            "paperId": "22b8b6fa5a982a06cea1dbf446804fc4f69e1f9b",
            "title": "ImSimCSE: Improving Contrastive Learning for Sentence Embeddings from Two Perspectives",
            "abstract": "This paper improves contrastive learning for sentence embeddings from two perspectives: handling dropout noise and addressing feature corruption. Specifically, for the first perspective, we identify that the dropout noise from negative pairs affects the model's performance. Therefore, we propose a simple yet effective method to deal with such type of noise. Secondly, we pinpoint the rank bottleneck of current solutions to feature corruption and propose a dimension-wise contrastive learning objective to address this issue. Both proposed methods are generic and can be applied to any contrastive learning based models for sentence embeddings. Experimental results on standard benchmarks demonstrate that combining both proposed methods leads to a gain of 1.8 points compared to the strong baseline SimCSE configured with BERT base. Furthermore, applying the proposed method to DiffCSE, another strong contrastive learning based baseline, results in a gain of 1.4 points.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "81946114",
                    "name": "Jiahao Xu"
                },
                {
                    "authorId": "152348954",
                    "name": "Wei Shao"
                },
                {
                    "authorId": "94584806",
                    "name": "Lihu Chen"
                },
                {
                    "authorId": "2978364",
                    "name": "Lemao Liu"
                }
            ]
        },
        {
            "paperId": "627b6f7687e122b5578f095221f66583850f0ea5",
            "title": "GLADIS: A General and Large Acronym Disambiguation Benchmark",
            "abstract": "Acronym Disambiguation (AD) is crucial for natural language understanding on various sources, including biomedical reports, scientific papers, and search engine queries. However, existing acronym disambiguationbenchmarks and tools are limited to specific domains, and the size of prior benchmarks is rather small. To accelerate the research on acronym disambiguation, we construct a new benchmark with three components: (1) a much larger acronym dictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus with 160 million sentences;(3) three datasets that cover thegeneral, scientific, and biomedical domains.We then pre-train a language model, {emph{AcroBERT}, on our constructed corpus for general acronym disambiguation, and show the challenges and values of our new benchmark.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "94584806",
                    "name": "Lihu Chen"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                },
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "e634efdd6b5d6a3d1b33870ae7a208724afdd62f",
            "title": "Knowledge Base Completion for Long-Tail Entities",
            "abstract": "Despite their impressive scale, knowledge bases (KBs), such as Wikidata, still contain significant gaps. Language models (LMs) have been proposed as a source for filling these gaps. However, prior works have focused on prominent entities with rich coverage by LMs, neglecting the crucial case of long-tail entities. In this paper, we present a novel method for LM-based-KB completion that is specifically geared for facts about long-tail entities. The method leverages two different LMs in two stages: for candidate retrieval and for candidate verification and disambiguation. To evaluate our method and various baselines, we introduce a novel dataset, called MALT, rooted in Wikidata. Our method outperforms all baselines in F1, with major gains especially in recall.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "94584806",
                    "name": "Lihu Chen"
                },
                {
                    "authorId": "2499758",
                    "name": "Simon Razniewski"
                },
                {
                    "authorId": "1751591",
                    "name": "G. Weikum"
                }
            ]
        },
        {
            "paperId": "e4645f7d08b5bc878bd38c3db19c3b1a9978bd43",
            "title": "Imputing Out-of-Vocabulary Embeddings with LOVE Makes LanguageModels Robust with Little Cost",
            "abstract": "State-of-the-art NLP systems represent inputs with word embeddings, but these are brittle when faced with Out-of-Vocabulary (OOV) words.To address this issue, we follow the principle of mimick-like models to generate vectors for unseen words, by learning the behavior of pre-trained embeddings using only the surface form of words.We present a simple contrastive learning framework, LOVE, which extends the word representation of an existing pre-trained language model (such as BERT) and makes it robust to OOV with few additional parameters.Extensive evaluations demonstrate that our lightweight model achieves similar or even better performances than prior competitors, both on original datasets and on corrupted variants. Moreover, it can be used in a plug-and-play fashion with FastText and BERT, where it significantly improves their robustness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "94584806",
                    "name": "Lihu Chen"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                },
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "9bd97867c7b7afa4ffe1255621299ad01f04f073",
            "title": "mSHINE: A Multiple-Meta-Paths Simultaneous Learning Framework for Heterogeneous Information Network Embedding",
            "abstract": "Heterogeneous information networks(HINs) become popular in recent years for its strong capability of modelling objects with abundant information using explicit network structure. Network embedding has been proved as an effective method to convert information networks into lower-dimensional space, whereas the core information can be well preserved. However, traditional network embedding algorithms are sub-optimal in capturing rich while potentially incompatible semantics provided by HINs. To address this issue, a novel meta-path-based HIN representation learning framework named mSHINE is designed to simultaneously learn multiple node representations for different meta-paths. More specifically, one representation learning module inspired by the RNN structure is developed and multiple node representations can be learned simultaneously, where each representation is associated with one respective meta-path. By measuring the relevance between nodes with the designed objective function, the learned module can be applied in downstream link prediction tasks. A set of criteria for selecting initial meta-paths is proposed as the other module in mSHINE which is important to reduce the optimal meta-path selection cost when no prior knowledge of suitable meta-paths is available. To corroborate the effectiveness of mSHINE, extensive experimental studies including node classification and link prediction are conducted on five real-world datasets. The results demonstrate that mSHINE outperforms other state-of-the-art HIN embedding methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108262789",
                    "name": "Xinyi Zhang"
                },
                {
                    "authorId": "94584806",
                    "name": "Lihu Chen"
                }
            ]
        },
        {
            "paperId": "74c344c029adebe03d7278474b34e15c09162eca",
            "title": "A Lightweight Neural Model for Biomedical Entity Linking",
            "abstract": "Biomedical entity linking aims to map biomedical mentions,\nsuch as diseases and drugs, to standard entities in a given\nknowledge base. The specific challenge in this context is\nthat the same biomedical entity can have a wide range of\nnames, including synonyms, morphological variations, and\nnames with different word orderings. Recently, BERT-based\nmethods have advanced the state-of-the-art by allowing for\nrich representations of word sequences. However, they often have hundreds of millions of parameters and require\nheavy computing resources, which limits their applications\nin resource-limited scenarios. Here, we propose a lightweight\nneural method for biomedical entity linking, which needs just\na fraction of the parameters of a BERT model and much less\ncomputing resources. Our method uses a simple alignment\nlayer with attention mechanisms to capture the variations\nbetween mention and entity names. Yet, we show that our\nmodel is competitive with previous work on standard evaluation benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "94584806",
                    "name": "Lihu Chen"
                },
                {
                    "authorId": "3025780",
                    "name": "G. Varoquaux"
                },
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "67de0bc5d1dfb458efc9aa192c879bf00ee667b3",
            "title": "Capsule Graph Neural Network",
            "abstract": "The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representations may not suf\ufb01ce to preserve the node/graph properties",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47687002",
                    "name": "Z. Xinyi"
                },
                {
                    "authorId": "94584806",
                    "name": "Lihu Chen"
                }
            ]
        }
    ]
}