{
    "authorId": "2261748535",
    "papers": [
        {
            "paperId": "32de19309b9916eebc6f32ce67d6cfb3a1a8f0e2",
            "title": "TVR-Ranking: A Dataset for Ranked Video Moment Retrieval with Imprecise Queries",
            "abstract": "In this paper, we propose the task of \\textit{Ranked Video Moment Retrieval} (RVMR) to locate a ranked list of matching moments from a collection of videos, through queries in natural language. Although a few related tasks have been proposed and studied by CV, NLP, and IR communities, RVMR is the task that best reflects the practical setting of moment search. To facilitate research in RVMR, we develop the TVR-Ranking dataset, based on the raw videos and existing moment annotations provided in the TVR dataset. Our key contribution is the manual annotation of relevance levels for 94,442 query-moment pairs. We then develop the $NDCG@K, IoU\\geq \\mu$ evaluation metric for this new task and conduct experiments to evaluate three baseline models. Our experiments show that the new RVMR task brings new challenges to existing models and we believe this new dataset contributes to the research on multi-modality search. The dataset is available at \\url{https://github.com/Ranking-VMR/TVR-Ranking}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261748535",
                    "name": "Renjie Liang"
                },
                {
                    "authorId": "2156060734",
                    "name": "Li Li"
                },
                {
                    "authorId": "2279759625",
                    "name": "Chongzhi Zhang"
                },
                {
                    "authorId": "2310611744",
                    "name": "Jing Wang"
                },
                {
                    "authorId": "2310543867",
                    "name": "Xizhou Zhu"
                },
                {
                    "authorId": "1735962",
                    "name": "Aixin Sun"
                }
            ]
        },
        {
            "paperId": "06673e37f8d8a515d6fe26511dd1fbae69a9237d",
            "title": "Partial Annotation-based Video Moment Retrieval via Iterative Learning",
            "abstract": "Given a descriptive language query, Video Moment Retrieval (VMR) aims to seek the corresponding semantic-consistent moment clip in the video, which is represented as a pair of the start and end timestamps. Although current methods have achieved satisfying performance, training these models heavily relies on the fully-annotated VMR datasets. Nonetheless, precise video temporal annotations are extremely labor-intensive and ambiguous due to the diverse preferences of different annotators. Although there are several works trying to explore weakly supervised VMR tasks with scattered annotated frames as labels, there is still much room to improve in terms of accuracy. Therefore, we design a new setting of VMR where users can easily point to small segments of non-controversy video moments and our proposed method can automatically fill in the remaining parts based on the video and query semantics. To support this, we propose a new framework named Video Moment Retrieval via Iterative Learning (VMRIL). It treats the partial temporal region as the seed, then expands the pseudo label by iterative training. In order to restrict the expansion with reasonable boundaries, we utilize a pretrained video action localization model to provide coarse guidance of potential video segments. Compared with other VMR methods, our VMRIL achieves a trade-off between satisfying performance and annotation efficiency. Experimental results show that our proposed method can achieve the SOTA performance in the weakly supervised VMR setting, and are even comparable with some fully-supervised VMR methods but with much less annotation cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261791648",
                    "name": "Wei Ji"
                },
                {
                    "authorId": "2261748535",
                    "name": "Renjie Liang"
                },
                {
                    "authorId": "2261751914",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "2261750082",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "c84a35eb7da90b7ac54a9cbcfe04631423e825cd",
            "title": "Biased-Predicate Annotation Identification via Unbiased Visual Predicate Representation",
            "abstract": "Panoptic Scene Graph Generation (PSG) translates visual scenes to structured linguistic descriptions, i.e., mapping visual instances to subjects/objects, and their relationships to predicates. However, the annotators' preferences and semantic overlaps between predicates inevitably lead to the semantic mappings of multiple predicates to one relationship, i.e., biased-predicate annotations. As a result, with the contradictory mapping between visual and linguistics, PSG models are struggled to construct clear decision planes among predicates, so as to cause existing poor performances. Obviously, it is essential for the PSG task to tackle this multi-modal contradiction. Therefore, we propose a novel method that utilizes unbiased visual predicate representations for Biased-Annotation Identification (BAI) as a fundamental step for PSG/SGG tasks. Our BAI includes three main steps: predicate representation extraction, predicate representation debiasing, and biased-annotation identification. With flexible biased annotation processing methods, our BAI can act as a fundamental step of dataset debiasing. Experimental results demonstrate that our proposed BAI has achieved state-of-the-art performance, which promotes the performance of benchmark models to various degrees with ingenious biased annotation processing methods. Furthermore, our BAI shows great generalization and effectiveness on multiple datasets. Our codes are released at https://github.com/lili0415/BAI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156060734",
                    "name": "Li Li"
                },
                {
                    "authorId": "2261805541",
                    "name": "Chenwei Wang"
                },
                {
                    "authorId": "2142095326",
                    "name": "Youxuan Qin"
                },
                {
                    "authorId": "2261791648",
                    "name": "Wei Ji"
                },
                {
                    "authorId": "2261748535",
                    "name": "Renjie Liang"
                }
            ]
        }
    ]
}