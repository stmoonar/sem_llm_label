{
    "authorId": "2146020415",
    "papers": [
        {
            "paperId": "ff394eb00579968974e41049b26cbd2411cd5798",
            "title": "Towards Few-shot Entity Recognition in Document Images: A Graph Neural Network Approach Robust to Image Manipulation",
            "abstract": "Recent advances of incorporating layout information, typically bounding box coordinates, into pre-trained language models have achieved significant performance in entity recognition from document images. Using coordinates can easily model the position of each token, but they are sensitive to manipulations in document images (e.g., shifting, rotation or scaling) which are common in real scenarios. Such limitation becomes even worse when the training data is limited in few-shot settings. In this paper, we propose a novel framework, LAGER, which leverages the topological adjacency relationship among the tokens through learning their relative layout information with graph neural networks. Specifically, we consider the tokens in the documents as nodes and formulate the edges based on the topological heuristics. Such adjacency graphs are invariant to affine transformations, making it robust to the common image manipulations. We incorporate these graphs into the pre-trained language model by adding graph neural network layers on top of the language model embeddings. Extensive experiments on two benchmark datasets show that LAGER significantly outperforms strong baselines under different few-shot settings and also demonstrate better robustness to manipulations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2068122286",
                    "name": "Prashant Krishnan"
                },
                {
                    "authorId": "1762478",
                    "name": "Zilong Wang"
                },
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "23e176e940cfb2362687a2c3dc1e81f1ec0eb55a",
            "title": "When Does A Spectral Graph Neural Network Fail in Node Classification?",
            "abstract": "Spectral Graph Neural Networks (GNNs) with various graph filters have received extensive affirmation due to their promising performance in graph learning problems. However, it is known that GNNs do not always perform well. Although graph filters provide theoretical foundations for model explanations, it is unclear when a spectral GNN will fail. In this paper, focusing on node classification problems, we conduct a theoretical analysis of spectral GNNs performance by investigating their prediction error. With the aid of graph indicators including homophily degree and response efficiency we proposed, we establish a comprehensive understanding of complex relationships between graph structure, node labels, and graph filters. We indicate that graph filters with low response efficiency on label difference are prone to fail. To enhance GNNs performance, we provide a provably better strategy for filter design from our theoretical analysis - using data-driven filter banks, and propose simple models for empirical validation. Experimental results show consistency with our theoretical results and support our strategy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111320521",
                    "name": "Zhi-Xing Chen"
                },
                {
                    "authorId": "1488667108",
                    "name": "Tengfei Ma"
                },
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                }
            ]
        },
        {
            "paperId": "271f40f4fad4d112e436565e668b79ede690d755",
            "title": "Inductive Relation Prediction Using Analogy Subgraph Embeddings",
            "abstract": "Prevailing methods for relation prediction in heterogeneous graphs including knowledge graphs aim at learning the latent representations (i.e., embeddings) of observed nodes and relations, and are thus limited to the transductive setting where the relation types must be known during training. In this paper, we propose ANalogy SubGraph Embedding Learning (GraphANGEL), a novel relation prediction framework that predicts relations between each node pair by checking whether the subgraphs containing the pair are similar to other subgraphs containing the considered relation. Each graph pattern explicitly represents a specific logical rule, which contributes to an inductive bias that facilitates generalization to unseen relation types and leads to more explainable predictive models. Our model consistently outperforms existing models in terms of heterogeneous graph based recommendation as well as knowledge graph completion. We also empirically demonstrate the capability of our model in generalizing to new relation types while producing explainable heat maps of attention scores across the discovered logics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "16278568",
                    "name": "Jiarui Jin"
                },
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                },
                {
                    "authorId": "2156098229",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                }
            ]
        },
        {
            "paperId": "6e7e2b075363015e046dcb1fcddae9f5c30c68f5",
            "title": "Learning Enhanced Representations for Tabular Data via Neighborhood Propagation",
            "abstract": "Prediction over tabular data is an essential and fundamental problem in many important downstream tasks. However, existing methods either take a data instance of the table independently as input or do not fully utilize the multi-rows features and labels to directly change and enhance the target data representations. In this paper, we propose to 1) construct a hypergraph from relevant data instance retrieval to model the cross-row and cross-column patterns of those instances, and 2) perform message Propagation to Enhance the target data instance representation for Tabular prediction tasks. Specifically, our specially-designed message propagation step benefits from 1) fusion of label and features during propagation, and 2) locality-aware high-order feature interactions. Experiments on two important tabular data prediction tasks validate the superiority of the proposed PET model against other baselines. Additionally, we demonstrate the effectiveness of the model components and the feature enhancement ability of PET via various ablation studies and visualizations. The code is included in https://github.com/KounianhuaDu/PET.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1780721965",
                    "name": "Kounianhua Du"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2046150680",
                    "name": "Ruiwen Zhou"
                },
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "2163050574",
                    "name": "Xilong Zhao"
                },
                {
                    "authorId": "16278568",
                    "name": "Jiarui Jin"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                }
            ]
        },
        {
            "paperId": "a74ef2ac05e167bbef9c5fa4f59f31ee04c00454",
            "title": "Refined Edge Usage of Graph Neural Networks for Edge Prediction",
            "abstract": "Graph Neural Networks (GNNs), originally proposed for node classification, have also motivated many recent works on edge prediction (a.k.a., link prediction). However, existing methods lack elaborate design regarding the distinctions between two tasks that have been frequently overlooked: (i) edges only constitute the topology in the node classification task but can be used as both the topology and the supervisions (i.e., labels) in the edge prediction task; (ii) the node classification makes prediction over each individual node, while the edge prediction is determinated by each pair of nodes. To this end, we propose a novel edge prediction paradigm named Edge-aware Message PassIng neuRal nEtworks (EMPIRE). Concretely, we first introduce an edge splitting technique to specify use of each edge where each edge is solely used as either the topology or the supervision (named as topology edge or supervision edge). We then develop a new message passing mechanism that generates the messages to source nodes (through topology edges) being aware of target nodes (through supervision edges). In order to emphasize the differences between pairs connected by supervision edges and pairs unconnected, we further weight the messages to highlight the relative ones that can reflect the differences. In addition, we design a novel negative node-pair sampling trick that efficiently samples 'hard' negative instances in the supervision instances, and can significantly improve the performance. Experimental results verify that the proposed method can significantly outperform existing state-of-the-art models regarding the edge prediction task on multiple homogeneous and heterogeneous graph datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "16278568",
                    "name": "Jiarui Jin"
                },
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "2118943843",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "2156098229",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                }
            ]
        },
        {
            "paperId": "1b65f01c6fdb6c836e82ce716264d5391663228f",
            "title": "Bag of Tricks for Node Classification with Graph Neural Networks",
            "abstract": "Over the past few years, graph neural networks (GNN) and label propagation-based methods have made significant progress in addressing node classification tasks on graphs. However, in addition to their reliance on elaborate architectures and algorithms, there are several key technical details that are frequently overlooked, and yet nonetheless can play a vital role in achieving satisfactory performance. In this paper, we first summarize a series of existing tricks-of-the-trade, and then propose several new ones related to label usage, loss function formulation, and model design that can significantly improve various GNN architectures. We empirically evaluate their impact on final node classification accuracy by conducting ablation studies and demonstrate consistently-improved performance, often to an extent that outweighs the gains from more dramatic changes in the underlying GNN architecture. Notably, many of the top-ranked models on the Open Graph Benchmark (OGB) leaderboard and KDDCUP 2021 Large-Scale Challenge MAG240M-LSC benefit from these techniques we initiated.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "16278568",
                    "name": "Jiarui Jin"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "1811427",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                }
            ]
        },
        {
            "paperId": "2f64df4e61994b127e36f8114da3a070535941f4",
            "title": "Why Propagate Alone? Parallel Use of Labels and Features on Graphs",
            "abstract": "Graph neural networks (GNNs) and label propagation represent two interrelated modeling strategies designed to exploit graph structure in tasks such as node property prediction. The former is typically based on stacked message-passing layers that share neighborhood information to transform node features into predictive embeddings. In contrast, the latter involves spreading label information to unlabeled nodes via a parameter-free diffusion process, but operates independently of the node features. Given then that the material difference is merely whether features or labels are smoothed across the graph, it is natural to consider combinations of the two for improving performance. In this regard, it has recently been proposed to use a randomly-selected portion of the training labels as GNN inputs, concatenated with the original node features for making predictions on the remaining labels. This so-called label trick accommodates the parallel use of features and labels, and is foundational to many of the top-ranking submissions on the Open Graph Benchmark (OGB) leaderboard. And yet despite its wide-spread adoption, thus far there has been little attempt to carefully unpack exactly what statistical properties the label trick introduces into the training pipeline, intended or otherwise. To this end, we prove that under certain simplifying assumptions, the stochastic label trick can be reduced to an interpretable, deterministic training objective composed of two factors. The first is a data-fitting term that naturally resolves potential label leakage issues, while the second serves as a regularization factor conditioned on graph structure that adapts to graph size and connectivity. Later, we leverage this perspective to motivate a broader range of label trick use cases, and provide experiments to verify the efficacy of these extensions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "16278568",
                    "name": "Jiarui Jin"
                },
                {
                    "authorId": "2108309275",
                    "name": "Weinan Zhang"
                },
                {
                    "authorId": "2144182425",
                    "name": "Yongyi Yang"
                },
                {
                    "authorId": "1391200710",
                    "name": "Jiuhai Chen"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "2119021541",
                    "name": "Yong Yu"
                },
                {
                    "authorId": "2148906289",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "2109583192",
                    "name": "Zengfeng Huang"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                }
            ]
        },
        {
            "paperId": "35f209f3644be4b8d40a1ae91b0c9f2c82eb9f3f",
            "title": "Graph Neural Networks Inspired by Classical Iterative Algorithms",
            "abstract": "Despite the recent success of graph neural networks (GNN), common architectures often exhibit significant limitations, including sensitivity to oversmoothing, long-range dependencies, and spurious edges, e.g., as can occur as a result of graph heterophily or adversarial attacks. To at least partially address these issues within a simple transparent framework, we consider a new family of GNN layers designed to mimic and integrate the update rules of two classical iterative algorithms, namely, proximal gradient descent and iterative reweighted least squares (IRLS). The former defines an extensible base GNN architecture that is immune to oversmoothing while nonetheless capturing long-range dependencies by allowing arbitrary propagation steps. In contrast, the latter produces a novel attention mechanism that is explicitly anchored to an underlying end-to-end energy function, contributing stability with respect to edge uncertainty. When combined we obtain an extremely simple yet robust model that we evaluate across disparate scenarios including standardized benchmarks, adversarially-perturbated graphs, graphs with heterophily, and graphs involving long-range dependencies. In doing so, we compare against SOTA GNN approaches that have been explicitly designed for the respective task, achieving competitive or superior node classification accuracy. Our code is available at https://github.com/FFTYYY/TWIRLS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144182425",
                    "name": "Yongyi Yang"
                },
                {
                    "authorId": "2167249750",
                    "name": "T. Liu"
                },
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "9695889",
                    "name": "Jinjing Zhou"
                },
                {
                    "authorId": "47594426",
                    "name": "Quan Gan"
                },
                {
                    "authorId": "12457830",
                    "name": "Zhewei Wei"
                },
                {
                    "authorId": "1852415",
                    "name": "Zheng Zhang"
                },
                {
                    "authorId": "3251920",
                    "name": "Zengfeng Huang"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                }
            ]
        },
        {
            "paperId": "37cdcca448f16d3d96e704374a89358d6485c080",
            "title": "Visualizing Graph Neural Networks With CorGIE: Corresponding a Graph to Its Embedding",
            "abstract": "Graph neural networks (GNNs) are a class of powerful machine learning tools that model node relations for making predictions of nodes or links. GNN developers rely on quantitative metrics of the predictions to evaluate a GNN, but similar to many other neural networks, it is difficult for them to understand if the GNN truly learns characteristics of a graph as expected. We propose an approach to corresponding an input graph to its node embedding (aka latent space), a common component of GNNs that is later used for prediction. We abstract the data and tasks, and develop an interactive multi-view interface called CorGIE to instantiate the abstraction. As the key function in CorGIE, we propose the K-hop graph layout to show topological neighbors in hops and their clustering structure. To evaluate the functionality and usability of CorGIE, we present how to use CorGIE in two usage scenarios, and conduct a case study with five GNN experts. Availability: Open-source code at https://github.com/zipengliu/corgie-ui/, supplemental materials & video at https://osf.io/tr3sb/.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2117941689",
                    "name": "Zipeng Liu"
                },
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "3322644",
                    "name": "J. Bernard"
                },
                {
                    "authorId": "1732016",
                    "name": "T. Munzner"
                }
            ]
        },
        {
            "paperId": "387015b173fc7579714baaf85dbfc25e826e64a7",
            "title": "Convergent Boosted Smoothing for Modeling Graph Data with Tabular Node Features",
            "abstract": "For supervised learning with tabular data, decision tree ensembles produced via boosting techniques generally dominate real-world applications involving iid training/test sets. However for graph data where the iid assumption is violated due to structured relations between samples, it remains unclear how to best incorporate this structure within existing boosting pipelines. To this end, we propose a generalized framework for iterating boosting with graph propagation steps that share node/sample information across edges connecting related samples. Unlike previous e\ufb00orts to integrate graph-based models with boosting, our approach is anchored in a principled meta loss function such that provable convergence can be guaranteed under relatively mild assumptions. Across a variety of non-iid graph datasets with tabular node features, our method achieves comparable or superior performance than both tabular and graph neural network models, as well as existing hybrid strategies that combine the two. Beyond producing better predictive performance than recently proposed graph models, our proposed techniques are easy to implement, computationally more e\ufb03cient, and enjoy stronger theoretical guarantees (which make our results more reproducible).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1391200710",
                    "name": "Jiuhai Chen"
                },
                {
                    "authorId": "153430733",
                    "name": "Jonas W. Mueller"
                },
                {
                    "authorId": "40043851",
                    "name": "V. Ioannidis"
                },
                {
                    "authorId": "2121390172",
                    "name": "Soji Adeshina"
                },
                {
                    "authorId": "2146020415",
                    "name": "Yangkun Wang"
                },
                {
                    "authorId": "1962083",
                    "name": "T. Goldstein"
                },
                {
                    "authorId": "2242717",
                    "name": "D. Wipf"
                }
            ]
        }
    ]
}