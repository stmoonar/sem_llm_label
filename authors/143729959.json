{
    "authorId": "143729959",
    "papers": [
        {
            "paperId": "3c292c9426612e2f2e5616543b8df8888bbc90d3",
            "title": "GeoMeter: Probing Depth and Height Perception of Large Visual-Language Models",
            "abstract": "Geometric understanding is crucial for navigating and interacting with our environment. While large Vision Language Models (VLMs) demonstrate impressive capabilities, deploying them in real-world scenarios necessitates a comparable geometric understanding in visual perception. In this work, we focus on the geometric comprehension of these models; specifically targeting the depths and heights of objects within a scene. Our observations reveal that, although VLMs excel in basic geometric properties perception such as shape and size, they encounter significant challenges in reasoning about the depth and height of objects. To address this, we introduce GeoMeter, a suite of benchmark datasets encompassing Synthetic 2D, Synthetic 3D, and Real-World scenarios to rigorously evaluate these aspects. We benchmark 17 state-of-the-art VLMs using these datasets and find that they consistently struggle with both depth and height perception. Our key insights include detailed analyses of the shortcomings in depth and height reasoning capabilities of VLMs and the inherent bias present in these models. This study aims to pave the way for the development of VLMs with enhanced geometric understanding, crucial for real-world applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2126484148",
                    "name": "Shehreen Azad"
                },
                {
                    "authorId": "2316559856",
                    "name": "Yash Jain"
                },
                {
                    "authorId": "2316627388",
                    "name": "Rishit Garg"
                },
                {
                    "authorId": "2256373851",
                    "name": "Y. S. Rawat"
                },
                {
                    "authorId": "143729959",
                    "name": "Vibhav Vineet"
                }
            ]
        },
        {
            "paperId": "a711ca27d12c0755e628ccd07ce95927d49c0d30",
            "title": "Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models",
            "abstract": "Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning -- a fundamental component of human cognition -- remains under-explored. We develop novel benchmarks that cover diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting. We conduct a comprehensive evaluation of competitive language and vision-language models. Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided. Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance. We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307984256",
                    "name": "Jiayu Wang"
                },
                {
                    "authorId": "2321601631",
                    "name": "Yifei Ming"
                },
                {
                    "authorId": "113515522",
                    "name": "Zhenmei Shi"
                },
                {
                    "authorId": "143729959",
                    "name": "Vibhav Vineet"
                },
                {
                    "authorId": "2308010128",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2250480908",
                    "name": "Neel Joshi"
                }
            ]
        },
        {
            "paperId": "a95e16c6bbf912a2452fc974d3f6a50482726877",
            "title": "Exposing the Achilles' Heel: Evaluating LLMs Ability to Handle Mistakes in Mathematical Reasoning",
            "abstract": "Large Language Models (LLMs) have been applied to Math Word Problems (MWPs) with transformative impacts, revolutionizing how these complex problems are approached and solved in various domains including educational settings. However, the evaluation of these models often prioritizes final accuracy, overlooking the crucial aspect of reasoning capabilities. This work addresses this gap by focusing on the ability of LLMs to detect and correct reasoning mistakes. We introduce a novel dataset MWP-MISTAKE, incorporating MWPs with both correct and incorrect reasoning steps generated through rule-based methods and smaller language models. Our comprehensive benchmarking reveals significant insights into the strengths and weaknesses of state-of-the-art models, such as GPT-4o, GPT-4, GPT-3.5Turbo, and others. We highlight GPT-$o's superior performance in mistake detection and rectification and the persistent challenges faced by smaller models. Additionally, we identify issues related to data contamination and memorization, impacting the reliability of LLMs in real-world applications. Our findings emphasize the importance of rigorous evaluation of reasoning processes and propose future directions to enhance the generalization and robustness of LLMs in mathematical problem-solving.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2308075211",
                    "name": "Joykirat Singh"
                },
                {
                    "authorId": "51464520",
                    "name": "A. Nambi"
                },
                {
                    "authorId": "143729959",
                    "name": "Vibhav Vineet"
                }
            ]
        },
        {
            "paperId": "be7122f7b2db3bce3137519b1f81e79fa57c9eaa",
            "title": "Eureka: Evaluating and Understanding Large Foundation Models",
            "abstract": "Rigorous and reproducible evaluation is critical for assessing the state of the art and for guiding scientific advances in Artificial Intelligence. Evaluation is challenging in practice due to several reasons, including benchmark saturation, lack of transparency in methods used for measurement, development challenges in extracting measurements for generative tasks, and, more generally, the extensive number of capabilities required for a well-rounded comparison across models. We make three contributions to alleviate the above challenges. First, we present Eureka, an open-source framework for standardizing evaluations of large foundation models beyond single-score reporting and rankings. Second, we introduce Eureka-Bench as an extensible collection of benchmarks testing capabilities that (i) are still challenging for state-of-the-art models and (ii) represent fundamental but overlooked language and multimodal capabilities. The inherent space for improvement in non-saturated benchmarks enables us to discover meaningful differences between models at a capability level. Third, using Eureka, we conduct an analysis of 12 state-of-the-art models, providing in-depth insights into failure understanding and model comparison, which can be leveraged to plan targeted improvements. In contrast to recent trends in reports and leaderboards showing absolute rankings and claims for one model or another to be the best, our analysis shows that there is no such best model. Different models have different strengths, but there are models that appear more often than others as best performers for some capabilities. Despite the recent improvements, current models still struggle with several fundamental capabilities including detailed image understanding, benefiting from multimodal input when available rather than fully relying on language, factuality and grounding for information retrieval, and over refusals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2321484904",
                    "name": "Jingya Chen"
                },
                {
                    "authorId": "2250480908",
                    "name": "Neel Joshi"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2321455461",
                    "name": "Eduardo Salinas"
                },
                {
                    "authorId": "143729959",
                    "name": "Vibhav Vineet"
                },
                {
                    "authorId": "2321455359",
                    "name": "James Woffinden-Luey"
                },
                {
                    "authorId": "2670023",
                    "name": "Safoora Yousefi"
                }
            ]
        },
        {
            "paperId": "f12b2e22ed2530e2b87054e5735208449616bf3c",
            "title": "Navigating Hallucinations for Reasoning of Unintentional Activities",
            "abstract": "In this work we present a novel task of understanding unintentional human activities in videos. We formalize this problem as a reasoning task under zero-shot scenario, where given a video of an unintentional activity we want to know why it transitioned from intentional to unintentional. We first evaluate the effectiveness of current state-of-the-art Large Multimodal Models on this reasoning task and observe that they suffer from hallucination. We further propose a novel prompting technique,termed as Dream of Thoughts (DoT), which allows the model to navigate through hallucinated thoughts to achieve better reasoning. To evaluate the performance on this task, we also introduce three different specialized metrics designed to quantify the models reasoning capability. We perform our experiments on two different datasets, OOPs and UCF-Crimes, and our findings show that DOT prompting technique is able to outperform standard prompting, while minimizing hallucinations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2288285098",
                    "name": "Shresth Grover"
                },
                {
                    "authorId": "143729959",
                    "name": "Vibhav Vineet"
                },
                {
                    "authorId": "2256373851",
                    "name": "Y. S. Rawat"
                }
            ]
        },
        {
            "paperId": "11546d3695bc2da7fc1df229ff72297907edf61a",
            "title": "Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation",
            "abstract": "We propose a new paradigm to automatically generate training data with accurate labels at scale using the text-to-image synthesis frameworks (e.g., DALL-E, Stable Diffusion, etc.). The proposed approach1 decouples training data generation into foreground object generation, and contextually coherent background generation. To generate foreground objects, we employ a straightforward textual template, incorporating the object class name as input prompts. This is fed into a text-to-image synthesis framework, producing various foreground images set against isolated backgrounds. A foreground-background segmentation algorithm is then used to generate foreground object masks. To generate context images, we begin by creating language descriptions of the context. This is achieved by applying an image captioning method to a small set of images representing the desired context. These textual descriptions are then transformed into a diverse array of context images via a text-to-image synthesis framework. Subsequently, we composite these with the foreground object masks produced in the initial step, utilizing a cut-and-paste method, to formulate the training data. We demonstrate the advantages of our approach on five object detection and segmentation datasets, including Pascal VOC and COCO. We found that detectors trained solely on synthetic data produced by our method achieve performance comparable to those trained on real data (Fig. 1). Moreover, a combination of real and synthetic data yields even much better results. Further analysis indicates that the synthetic data distribution complements the real data distribution effectively. Additionally, we emphasize the compositional nature of our data generation approach in out-of-distribution and zero-shot data generation scenarios. We open-source our code at https://github.com/gyhandy/Text2Image-for-Detection",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "22579225",
                    "name": "Yunhao Ge"
                },
                {
                    "authorId": "2110519123",
                    "name": "Jiashu Xu"
                },
                {
                    "authorId": "10146912",
                    "name": "Brian Nlong Zhao"
                },
                {
                    "authorId": "2250480908",
                    "name": "Neel Joshi"
                },
                {
                    "authorId": "2239100051",
                    "name": "Laurent Itti"
                },
                {
                    "authorId": "143729959",
                    "name": "Vibhav Vineet"
                }
            ]
        },
        {
            "paperId": "366676a10f920aa616112960994d7b160ad509b2",
            "title": "A Large-Scale Analysis on Self-Supervised Video Representation Learning",
            "abstract": "Self-supervised learning is an effective way for label-free model pre-training, especially in the video domain where labeling is expensive. Existing self-supervised works in the video domain use varying experimental setups to demonstrate their effectiveness and comparison across approaches becomes challenging with no standard benchmark. In this work, we first provide a benchmark that enables a comparison of existing approaches on the same ground. Next, we study five different aspects of self-supervised learning important for videos; 1) dataset size, 2) complexity, 3) data distribution, 4) data noise, and, 5)feature analysis. To facilitate this study, we focus on seven different methods along with seven different network architectures and perform an extensive set of experiments on 5 different datasets with an evaluation of two different downstream tasks. We present several interesting insights from this study which span across different properties of pretraining and target datasets, pretext-tasks, and model architectures among others. We further put some of these insights to the real test and propose an approach that requires a limited amount of training data and outperforms existing state-of-the-art approaches which use 10x pretraining data. We believe this work will pave the way for researchers to a better understanding of self-supervised pretext tasks in video representation learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12364057",
                    "name": "Akash Kumar"
                },
                {
                    "authorId": "2219879442",
                    "name": "Ashlesha Kumar"
                },
                {
                    "authorId": "143729959",
                    "name": "Vibhav Vineet"
                },
                {
                    "authorId": "2116440",
                    "name": "Y. Rawat"
                }
            ]
        },
        {
            "paperId": "3a79545719fb193a6b4042ef7d1d87cfd267be06",
            "title": "Controllable Text-to-Image Generation with GPT-4",
            "abstract": "Current text-to-image generation models often struggle to follow textual instructions, especially the ones requiring spatial reasoning. On the other hand, Large Language Models (LLMs), such as GPT-4, have shown remarkable precision in generating code snippets for sketching out text inputs graphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide the diffusion-based text-to-image pipelines with programmatic sketches generated by GPT-4, enhancing their abilities for instruction following. Control-GPT works by querying GPT-4 to write TikZ code, and the generated sketches are used as references alongside the text instructions for diffusion models (e.g., ControlNet) to generate photo-realistic images. One major challenge to training our pipeline is the lack of a dataset containing aligned text, images, and sketches. We address the issue by converting instance masks in existing datasets into polygons to mimic the sketches used at test time. As a result, Control-GPT greatly boosts the controllability of image generation. It establishes a new state-of-art on the spatial arrangement and object positioning generation and enhances users' control of object positions, sizes, etc., nearly doubling the accuracy of prior models. Our work, as a first attempt, shows the potential for employing LLMs to enhance the performance in computer vision tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218634341",
                    "name": "Tianjun Zhang"
                },
                {
                    "authorId": "2153910714",
                    "name": "Yi Zhang"
                },
                {
                    "authorId": "143729959",
                    "name": "Vibhav Vineet"
                },
                {
                    "authorId": "39678486",
                    "name": "Neel Joshi"
                },
                {
                    "authorId": "2153689937",
                    "name": "Xin Wang"
                }
            ]
        },
        {
            "paperId": "5eace371f1d7b668781b12aca1592f94876c10e9",
            "title": "Robustness Analysis on Foundational Segmentation Models",
            "abstract": "Due to the increase in computational resources and accessibility of data, an increase in large, deep learning models trained on copious amounts of multi-modal data using self-supervised or semi-supervised learning have emerged. These \"foundation\" models are often adapted to a variety of downstream tasks like classification, object detection, and segmentation with little-to-no training on the target dataset. In this work, we perform a robustness analysis of Visual Foundation Models (VFMs) for segmentation tasks and focus on robustness against real-world distribution shift inspired perturbations. We benchmark seven state-of-the-art segmentation architectures using 2 different perturbed datasets, MS COCO-P and ADE20K-P, with 17 different perturbations with 5 severity levels each. Our findings reveal several key insights: (1) VFMs exhibit vulnerabilities to compression-induced corruptions, (2) despite not outpacing all of unimodal models in robustness, multimodal models show competitive resilience in zero-shot scenarios, and (3) VFMs demonstrate enhanced robustness for certain object categories. These observations suggest that our robustness evaluation framework sets new requirements for foundational models, encouraging further advancements to bolster their adaptability and performance. The code and dataset is available at: https://tinyurl.com/fm-robust.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1470726743",
                    "name": "Madeline Chantry Schiappa"
                },
                {
                    "authorId": "1852255",
                    "name": "V. Sachidanand"
                },
                {
                    "authorId": "22579225",
                    "name": "Yunhao Ge"
                },
                {
                    "authorId": "3336488",
                    "name": "O. Mik\u0161\u00edk"
                },
                {
                    "authorId": "2116440",
                    "name": "Y. Rawat"
                },
                {
                    "authorId": "143729959",
                    "name": "Vibhav Vineet"
                }
            ]
        },
        {
            "paperId": "6c2b21c69866e44f6ac9fd9e6ec693a0f2311435",
            "title": "Efficiently Robustify Pre-Trained Models",
            "abstract": "A recent trend in deep learning algorithms has been towards training large scale models, having high parameter count and trained on big dataset. However, robustness of such large scale models towards real-world settings is still a less-explored topic. In this work, we first benchmark the performance of these models under different perturbations and datasets thereby representing real-world shifts, and highlight their degrading performance under these shifts. We then discuss on how complete model fine-tuning based existing robustification schemes might not be a scalable option given very large scale networks and can also lead them to forget some of the desired characterstics. Finally, we propose a simple and cost-effective method to solve this problem, inspired by knowledge transfer literature. It involves robustifying smaller models, at a lower computation cost, and then use them as teachers to tune a fraction of these large scale networks, reducing the overall computational overhead. We evaluate our proposed method under various vision perturbations including ImageNet-C,R,S,A datasets and also for transfer learning, zero-shot evaluation setups on different datasets. Benchmark results show that our method is able to induce robustness to these large scale models efficiently, requiring significantly lower time and also preserves the transfer learning, zero-shot properties of the original model which none of the existing methods are able to achieve.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1959778232",
                    "name": "Nishant Jain"
                },
                {
                    "authorId": "2238786587",
                    "name": "Iit Roorkee"
                },
                {
                    "authorId": "145560551",
                    "name": "Harkirat Singh Behl"
                },
                {
                    "authorId": "143729959",
                    "name": "Vibhav Vineet"
                }
            ]
        }
    ]
}