{
    "authorId": "2143856455",
    "papers": [
        {
            "paperId": "14d2e8eb3958af9bb05011116a95d1a35aa8fb1d",
            "title": "Dynamic Edge Caching via Online Meta-RL",
            "abstract": "The content request patterns perceived by edge devices are becoming highly dynamic, especially for emerging short video platforms compared to traditional video platforms. This calls for caching policies that can continuously adapt to dynamic environments, challenging previously popular reinforcement learning (RL)-based policies. A straightforward solution, i.e., repeatedly restarting and training RL agents, would fail to converge timely while meeting the observed adaptation process. Offering transferable knowledge is considered a possible method to speed up the adaptation process. Unfortunately, it fails to outperform the RL-based approach as an alternative solution in these scenarios. To alleviate this drawback, we 1) design a sequential-pair meta-learning for edge caching that captures the meta-knowledge of dynamic changes from sequential-pair-wise intervals, which are segmentations from the whole dynamic episode, and 2) develop an online meta-RL-based solution called Online Meta Actor-Critic (OMAC), which updates the meta-knowledge in an online manner. To evaluate the proposed framework, we conduct trace-driven experiments to demonstrate the effectiveness of our design: it improves the average cache hit rate by up to 37.4% (normalized) compared with other baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1840978357",
                    "name": "Yinan Mao"
                },
                {
                    "authorId": "2149195194",
                    "name": "Shiji Zhou"
                },
                {
                    "authorId": "2143856455",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "2135451624",
                    "name": "Zhi Wang"
                },
                {
                    "authorId": "145583986",
                    "name": "Wenwu Zhu"
                }
            ]
        },
        {
            "paperId": "8c21a2886b261bf33509f3e79c1ab3d748b74628",
            "title": "MMMLP: Multi-modal Multilayer Perceptron for\u00a0Sequential\u00a0Recommendations",
            "abstract": "Sequential recommendation aims to offer potentially interesting products to users by capturing their historical sequence of interacted items. Although it has facilitated extensive physical scenarios, sequential recommendation for multi-modal sequences has long been neglected. Multi-modal data that depicts a user\u2019s historical interactions exists ubiquitously, such as product pictures, textual descriptions, and interacted item sequences, providing semantic information from multiple perspectives that comprehensively describe a user\u2019s preferences. However, existing sequential recommendation methods either fail to directly handle multi-modality or suffer from high computational complexity. To address this, we propose a novel Multi-Modal Multi-Layer Perceptron (MMMLP) for maintaining multi-modal sequences for sequential recommendation. MMMLP is a purely MLP-based architecture that consists of three modules - the Feature Mixer Layer, Fusion Mixer Layer, and Prediction Layer - and has an edge on both efficacy and efficiency. Extensive experiments show that MMMLP achieves state-of-the-art performance with linear complexity. We also conduct ablating analysis to verify the contribution of each component. Furthermore, compatible experiments are devised, and the results show that the multi-modal representation learned by our proposed model generally benefits other recommendation models, emphasizing our model\u2019s ability to handle multi-modal information. We have made our code available online to ease reproducibility1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2215479697",
                    "name": "Jiahao Liang"
                },
                {
                    "authorId": "2116710405",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2187870890",
                    "name": "Muyang Li"
                },
                {
                    "authorId": "2187882131",
                    "name": "Zijian Zhang"
                },
                {
                    "authorId": "2211473272",
                    "name": "Wanyu Wang"
                },
                {
                    "authorId": "2143856455",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                }
            ]
        },
        {
            "paperId": "f2c12f705aea19ab5e129f72ae9030375f06602f",
            "title": "Fairly Adaptive Negative Sampling for Recommendations",
            "abstract": "Pairwise learning strategies are prevalent for optimizing recommendation models on implicit feedback data, which usually learns user preference by discriminating between positive (i.e., clicked by a user) and negative items (i.e., obtained by negative sampling). However, the size of different item groups (specified by item attribute) is usually unevenly distributed. We empirically find that the commonly used uniform negative sampling strategy for pairwise algorithms (e.g., BPR) can inherit such data bias and oversample the majority item group as negative instances, severely countering group fairness on the item side. In this paper, we propose a Fairly adaptive Negative sampling approach (FairNeg), which improves item group fairness via adaptively adjusting the group-level negative sampling distribution in the training process. In particular, it first perceives the model\u2019s unfairness status at each step and then adjusts the group-wise sampling distribution with an adaptive momentum update strategy for better facilitating fairness optimization. Moreover, a negative sampling distribution Mixup mechanism is proposed, which gracefully incorporates existing importance-aware sampling techniques intended for mining informative negative samples, thus allowing for achieving multiple optimization purposes. Extensive experiments on four public datasets show our proposed method\u2019s superiority in group fairness enhancement and fairness-utility tradeoff.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2157320978",
                    "name": "Xiao Chen"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2108110907",
                    "name": "Jingfan Chen"
                },
                {
                    "authorId": "2143856455",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "2206162255",
                    "name": "Zhaoxiang Zhang"
                },
                {
                    "authorId": "2117897052",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "7476093b6831ab406d5d1acf9d10c86bcfa4334c",
            "title": "Evaluating and Mitigating Inherent Linguistic Bias of African American English through Inference",
            "abstract": "Recent studies show that NLP models trained on standard English texts tend to produce biased outcomes against underrepresented English varieties. In this work, we conduct a pioneering study of the English variety use of African American English (AAE) in NLI task. First, we propose CodeSwitch, a greedy unidirectional morphosyntactically-informed rule-based translation method for data augmentation. Next, we use CodeSwitch to present a preliminary study to determine if demographic language features do in fact influence models to produce false predictions. Then, we conduct experiments on two popular datasets and propose two simple, yet effective and generalizable debiasing methods. Our findings show that NLI models (e.g. BERT) trained under our proposed frameworks outperform traditional large language models while maintaining or even improving the prediction performance. In addition, we intend to release CodeSwitch, in hopes of promoting dialectal language diversity in training data to both reduce the discriminatory societal impacts and improve model robustness of downstream NLP tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1380259269",
                    "name": "Jamell Dacon"
                },
                {
                    "authorId": "2143856455",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "a2d3bc6a57ab742b40e0e7c3dba5a0e454aab5e6",
            "title": "Enhancing Individual Fairness through Propensity Score Matching",
            "abstract": "The central idea of individual fairness is based on an auspicious yet intuitive assertion: similar individuals should be treated similarly. Nevertheless, the fulfillment of individual fairness is hindered by three major obstacles. First, one needs to determine individuals who should receive similar treatment. Second, seamlessly formulating the notion of individual fairness in an ML learning process is another challenge. Third, effectively evaluating the notion of similar treatment in probabilistic classifiers is another challenge. To overcome these challenges, we propose a novel framework called FairMatch. Our proposed framework offers a new approach to pairing similar and dissimilar individuals using a causal analysis method called propensity score matching. Moreover, we formulate individual fairness as a representation learning problem where we incorporate similar and dissimilar pairs in a triplet-based loss function. Eventually, we devise a novel metric to evaluate individual fairness that captures the notion of similar treatment in probabilistic classifiers in a better way. Experimental results on four real-world datasets verify the superiority of FairMatch to existing solutions where we demonstrate it can deliver fairer decisions without scarifying the predictive performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1596725015",
                    "name": "Hamid Karimi"
                },
                {
                    "authorId": "2204917553",
                    "name": "Muhammad Fawad Akbar Khan"
                },
                {
                    "authorId": "2143856455",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "12524628",
                    "name": "Tyler Derr"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                }
            ]
        },
        {
            "paperId": "f7d18e6f4ce97ef614e5731edede20d0c29635b5",
            "title": "Trustworthy Machine Learning: Fairness and Robustness",
            "abstract": "In recent years, machine learning (ML) technologies have experienced swift developments and attracted extensive attention from both academia and industry. The applications of ML are extended to multiple domains, from computer vision, text processing, to recommendations, etc. However, recent studies have uncovered the untrustworthy side of ML applications. For example, ML algorithms could show human-like discrimination against certain individuals or groups, or make unreliable decisions in safety-critical scenarios, which implies the absence of fairness and robustness, respectively. Consequently, building trustworthy machine learning systems has become an urgent need. My research strives to help meet this demand. In particular, my research focuses on designing trustworthy ML models and spans across three main areas: (1) fairness in ML, where we aim to detect, eliminate bias and ensure fairness in various ML applications; (2) robustness in ML, where we seek to ensure the robustness of certain ML applications towards adversarial attacks; (3) specific applications of ML, where my research involves the development of ML-based natural language processing (NLP) models and recommendation systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143856455",
                    "name": "Haochen Liu"
                }
            ]
        },
        {
            "paperId": "07f4920a4a120a4a8644804904d0f5cc215b8ff8",
            "title": "The Authors Matter: Understanding and Mitigating Implicit Bias in Deep Text Classification",
            "abstract": "It is evident that deep text classification models trained on human data could be biased. In particular, they produce biased outcomes for texts that explicitly include identity terms of certain demographic groups. We refer to this type of bias as explicit bias, which has been extensively studied. However, deep text classification models can also produce biased outcomes for texts written by authors of certain demographic groups. We refer to such bias as implicit bias of which we still have a rather limited understanding. In this paper, we first demonstrate that implicit bias exists in different text classification tasks for different demographic groups. Then, we build a learning-based interpretation method to deepen our knowledge of implicit bias. Specifically, we verify that classifiers learn to make predictions based on language features that are related to the demographic attributes of the authors. Next, we propose a framework Debiased-TC to train deep text classifiers to make predictions on the right features and consequently mitigate implicit bias. We conduct extensive experiments on three real-world datasets. The results show that the text classification models trained under our proposed framework outperform traditional models significantly in terms of fairness, and also slightly in terms of classification performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143856455",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "144767914",
                    "name": "Wei Jin"
                },
                {
                    "authorId": "1596725015",
                    "name": "Hamid Karimi"
                },
                {
                    "authorId": "2117940912",
                    "name": "Zitao Liu"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "6dc8b0018bbc183c8d15747e5f6cc3fb14678a9f",
            "title": "Toward Annotator Group Bias in Crowdsourcing",
            "abstract": "Crowdsourcing has emerged as a popular approach for collecting annotated data to train supervised machine learning models. However, annotator bias can lead to defective annotations. Though there are a few works investigating individual annotator bias, the group effects in annotators are largely overlooked. In this work, we reveal that annotators within the same demographic group tend to show consistent group bias in annotation tasks and thus we conduct an initial study on annotator group bias. We first empirically verify the existence of annotator group bias in various real-world crowdsourcing datasets. Then, we develop a novel probabilistic graphical framework GroupAnno to capture annotator group bias with an extended Expectation Maximization (EM) algorithm. We conduct experiments on both synthetic and real-world datasets. Experimental results demonstrate the effectiveness of our model in modeling annotator group bias in label aggregation and model learning over competitive baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143856455",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "80601470",
                    "name": "J. Thekinen"
                },
                {
                    "authorId": "2315117721",
                    "name": "Sinem Mollaoglu"
                },
                {
                    "authorId": "97705488",
                    "name": "Da Tang"
                },
                {
                    "authorId": "2109869278",
                    "name": "Ji Yang"
                },
                {
                    "authorId": "73416451",
                    "name": "Youlong Cheng"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "8b365890c0224f17fffb90bf33da46fccacd9331",
            "title": "Trustworthy AI: A Computational Perspective",
            "abstract": "In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone\u2019s daily life and profoundly altering the course of human society. The intention behind developing AI was and is to benefit humans by reducing labor, increasing everyday conveniences, and promoting social good. However, recent research and AI applications indicate that AI can cause unintentional harm to humans by, for example, making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against a group or groups. Consequently, trustworthy AI has recently garnered increased attention regarding the need to avoid the adverse effects that AI could bring to people, so people can fully trust and live in harmony with AI technologies. A tremendous amount of research on trustworthy AI has been conducted and witnessed in recent years. In this survey, we present a comprehensive appraisal of trustworthy AI from a computational perspective to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex subject, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii) Nondiscrimination & Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability & Auditability, and (vi) Environmental Well-being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143856455",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "2108941389",
                    "name": "Yiqi Wang"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                },
                {
                    "authorId": "1527096073",
                    "name": "Yaxin Li"
                },
                {
                    "authorId": "39720946",
                    "name": "Shaili Jain"
                },
                {
                    "authorId": "1739705",
                    "name": "Anil K. Jain"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                }
            ]
        },
        {
            "paperId": "d166e32057d38ccddf6e35062ceaa42cd5dd30f6",
            "title": "AutoLoss: Automated Loss Function Search in Recommendations",
            "abstract": "Designing an effective loss function plays a crucial role in training deep recommender systems. Most existing works often leverage a predefined and fixed loss function that could lead to suboptimal recommendation quality and training efficiency. Some recent efforts rely on exhaustively or manually searched weights to fuse a group of candidate loss functions, which is exceptionally costly in computation and time. They also neglect the various convergence behaviors of different data examples. In this work, we propose an AutoLoss framework that can automatically and adaptively search for the appropriate loss function from a set of candidates. To be specific, we develop a novel controller network, which can dynamically adjust the loss probabilities in a differentiable manner. Unlike existing algorithms, the proposed controller can adaptively generate the loss probabilities for different data examples according to their varied convergence behaviors. Such design improves the model's generalizability and transferability between deep recommender systems and datasets. We evaluate the proposed framework on two benchmark datasets. The results show that AutoLoss outperforms representative baselines. Further experiments have been conducted to deepen our understandings of AutoLoss, including its transferability, components and training efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2733057",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2143856455",
                    "name": "Haochen Liu"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2146672392",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "2128077726",
                    "name": "Chong Wang"
                }
            ]
        }
    ]
}