{
    "authorId": "2257099189",
    "papers": [
        {
            "paperId": "0cee098244c9978032702862a43a09f468f691a4",
            "title": "Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding",
            "abstract": "To mitigate the high inference latency stemming from autoregressive decoding in Large Language Models (LLMs), Speculative Decoding has emerged as a novel decoding paradigm for LLM inference. In each decoding step, this method first drafts several future tokens efficiently and then verifies them in parallel. Unlike autoregressive decoding, Speculative Decoding facilitates the simultaneous decoding of multiple tokens per step, thereby accelerating inference. This paper presents a comprehensive overview and analysis of this promising decoding paradigm. We begin by providing a formal definition and formulation of Speculative Decoding. Then, we organize in-depth discussions on its key facets, such as drafter selection and verification strategies. Furthermore, we present a comparative analysis of leading methods under third-party testing environments. We aim for this work to serve as a catalyst for further research on Speculative Decoding, ultimately contributing to more efficient LLM inference.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2093662286",
                    "name": "Heming Xia"
                },
                {
                    "authorId": "2257389788",
                    "name": "Zhe Yang"
                },
                {
                    "authorId": "2047143813",
                    "name": "Qingxiu Dong"
                },
                {
                    "authorId": "2263478880",
                    "name": "Peiyi Wang"
                },
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2279540460",
                    "name": "Tao Ge"
                },
                {
                    "authorId": "2261522028",
                    "name": "Tianyu Liu"
                },
                {
                    "authorId": "2284641622",
                    "name": "Wenjie Li"
                },
                {
                    "authorId": "3335836",
                    "name": "Zhifang Sui"
                }
            ]
        },
        {
            "paperId": "18f9175f7a11bc3eb5c6ddc4ebd4fecede742e17",
            "title": "Discriminative Probing and Tuning for Text-to-Image Generation",
            "abstract": "Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a bonus of the discriminative adapter, a self-correction mechanism can leverage discriminative gradients to better align generated images to text prompts during inference. Comprehensive evaluations across three benchmark datasets, including both in-distribution and out-of-distribution scenarios, demonstrate our method's superior generation performance. Meanwhile, it achieves state-of-the-art discriminative performance on the two discriminative tasks compared to other generative models. The code is available at https://dpt-t2i.github.io/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1990265392",
                    "name": "Leigang Qu"
                },
                {
                    "authorId": "49336556",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2257264007",
                    "name": "Hanwang Zhang"
                },
                {
                    "authorId": "2270021997",
                    "name": "Liqiang Nie"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "21a1102fae1a031e3556b251c0cfd1e3fc31f713",
            "title": "Data-efficient Fine-tuning for LLM-based Recommendation",
            "abstract": "Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs' adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLM-based recommendation, aimed at identifying representative samples tailored for LLMs' few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data. To tackle these issues, we introduce two objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and 2) high efficiency underlines the low costs of the data pruning process. To pursue the two objectives, we propose a novel data pruning method based on two scores, i.e., influence score and effort score, to efficiently identify the influential samples. Particularly, the influence score is introduced to accurately estimate the influence of sample removal on the overall performance. To achieve low costs of the data pruning process, we use a small-sized surrogate model to replace LLMs to obtain the influence score. Considering the potential gap between the surrogate model and LLMs, we further propose an effort score to prioritize some hard samples specifically for LLMs. Empirical results on three real-world datasets validate the effectiveness of our proposed method. In particular, the proposed method uses only 2% samples to surpass the full data fine-tuning, reducing time costs by 97%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192203811",
                    "name": "Xinyu Lin"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2281902534",
                    "name": "Shuo Yang"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2279771878",
                    "name": "Yinwei Wei"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "382d3b37f53fd6118ab979cb56f7f3d13eb0951d",
            "title": "GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative Decoding",
            "abstract": "Speculative decoding is a relatively new decoding framework that leverages small and efficient draft models to reduce the latency of LLMs. In this study, we introduce GliDe and CaPE, two low-hassle modifications to vanilla speculative decoding to further improve the decoding speed of a frozen LLM. Specifically, GliDe is a modified draft model architecture that reuses the cached keys and values from the target LLM, while CaPE is a proposal expansion method that uses the draft model's confidence scores to help select additional candidate tokens for verification. Extensive experiments on different benchmarks demonstrate that our proposed GliDe draft model significantly reduces the expected decoding latency. Additional evaluation using walltime reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend the improvement to 2.61x with CaPE. We will release our code, data, and the trained draft models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2694098",
                    "name": "Cunxiao Du"
                },
                {
                    "authorId": "2256945290",
                    "name": "Jing Jiang"
                },
                {
                    "authorId": "2282563755",
                    "name": "Yuanchen Xu"
                },
                {
                    "authorId": "2282543653",
                    "name": "Jiawei Wu"
                },
                {
                    "authorId": "2282544477",
                    "name": "Sicheng Yu"
                },
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2227938491",
                    "name": "Shenggui Li"
                },
                {
                    "authorId": "2282919997",
                    "name": "Kai Xu"
                },
                {
                    "authorId": "2270021997",
                    "name": "Liqiang Nie"
                },
                {
                    "authorId": "2282538629",
                    "name": "Zhaopeng Tu"
                },
                {
                    "authorId": "2282532621",
                    "name": "Yang You"
                }
            ]
        },
        {
            "paperId": "4a353d97234317520193b42b4d402637ea32c6b7",
            "title": "Distillation Enhanced Generative Retrieval",
            "abstract": "Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generative language models, distinct from traditional sparse or dense retrieval methods. In this work, we identify a viable direction to further enhance generative retrieval via distillation and propose a feasible framework, named DGR. DGR utilizes sophisticated ranking models, such as the cross-encoder, in a teacher role to supply a passage rank list, which captures the varying relevance degrees of passages instead of binary hard labels; subsequently, DGR employs a specially designed distilled RankNet loss to optimize the generative retrieval model, considering the passage rank order provided by the teacher model as labels. This framework only requires an additional distillation step to enhance current generative retrieval systems and does not add any burden to the inference stage. We conduct experiments on four public datasets, and the results indicate that DGR achieves state-of-the-art performance among the generative retrieval methods. Additionally, DGR demonstrates exceptional robustness and generalizability with various teacher models and distillation losses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2284702952",
                    "name": "Zhen Zhang"
                },
                {
                    "authorId": "49336556",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2270021997",
                    "name": "Liqiang Nie"
                },
                {
                    "authorId": "2284641622",
                    "name": "Wenjie Li"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "61cf2eb3dc4467e96b20eef52152a8aa486da32d",
            "title": "A Survey of Generative Search and Recommendation in the Era of Large Language Models",
            "abstract": "With the information explosion on the Web, search and recommendation are foundational infrastructures to satisfying users' information needs. As the two sides of the same coin, both revolve around the same core research problem, matching queries with documents or users with items. In the recent few decades, search and recommendation have experienced synchronous technological paradigm shifts, including machine learning-based and deep learning-based paradigms. Recently, the superintelligent generative large language models have sparked a new paradigm in search and recommendation, i.e., generative search (retrieval) and recommendation, which aims to address the matching problem in a generative manner. In this paper, we provide a comprehensive survey of the emerging paradigm in information systems and summarize the developments in generative search and recommendation from a unified perspective. Rather than simply categorizing existing works, we abstract a unified framework for the generative paradigm and break down the existing works into different stages within this framework to highlight the strengths and weaknesses. And then, we distinguish generative search and recommendation with their unique challenges, identify open problems and future directions, and envision the next information-seeking paradigm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2192203811",
                    "name": "Xinyu Lin"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2263589454",
                    "name": "Liang Pang"
                },
                {
                    "authorId": "2284641622",
                    "name": "Wenjie Li"
                },
                {
                    "authorId": "2270021997",
                    "name": "Liqiang Nie"
                },
                {
                    "authorId": "2239071206",
                    "name": "Xiangnan He"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "6aa5177b38969f69a5c1b17b2846e6e90bd684e8",
            "title": "Learnable Item Tokenization for Generative Recommendation",
            "abstract": "Utilizing powerful Large Language Models (LLMs) for generative recommendation has attracted much attention. Nevertheless, a crucial challenge is transforming recommendation data into the language space of LLMs through effective item tokenization. Current approaches, such as ID, textual, and codebook-based identifiers, exhibit shortcomings in encoding semantic information, incorporating collaborative signals, or handling code assignment bias. To address these limitations, we propose LETTER (a LEarnable Tokenizer for generaTivE Recommendation), which integrates hierarchical semantics, collaborative signals, and code assignment diversity to satisfy the essential requirements of identifiers. LETTER incorporates Residual Quantized VAE for semantic regularization, a contrastive alignment loss for collaborative regularization, and a diversity loss to mitigate code assignment bias. We instantiate LETTER on two models and propose a ranking-guided generation loss to augment their ranking ability theoretically. Experiments on three datasets validate the superiority of LETTER, advancing the state-of-the-art in the field of LLM-based generative recommendation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2301153513",
                    "name": "Honghui Bao"
                },
                {
                    "authorId": "2192203811",
                    "name": "Xinyu Lin"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2241348826",
                    "name": "See-kiong Ng"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "79172e64a8b7abb8523b0fcf44802b9e1142aede",
            "title": "Efficient Inference for Large Language Model-based Generative Recommendation",
            "abstract": "Large Language Model (LLM)-based generative recommendation has achieved notable success, yet its practical deployment is costly particularly due to excessive inference latency caused by autoregressive decoding. For lossless LLM decoding acceleration, Speculative Decoding (SD) has emerged as a promising solution. However, applying SD to generative recommendation presents unique challenges due to the requirement of generating top-K items (i.e., K distinct token sequences) as a recommendation list by beam search. This leads to more stringent verification in SD, where all the top-K sequences from the target LLM must be successfully drafted by the draft model at each decoding step. To alleviate this, we consider 1) boosting top-K sequence alignment between the draft model and the target LLM, and 2) relaxing the verification strategy to reduce trivial LLM calls. To this end, we propose an alignment framework named AtSpeed, which presents the AtSpeed-S optimization objective for top-K alignment under the strict top-K verification. Moreover, we introduce a relaxed sampling verification strategy that allows high-probability non-top-K drafted sequences to be accepted, significantly reducing LLM calls. Correspondingly, we propose AtSpeed-R for top-K alignment under this relaxed sampling verification. Empirical results on two real-world datasets demonstrate that AtSpeed significantly accelerates LLM-based generative recommendation, e.g., near 2x speedup under strict top-K verification and up to 2.5 speedup under relaxed sampling verification. The codes and datasets will be released in the near future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2192203811",
                    "name": "Xinyu Lin"
                },
                {
                    "authorId": "2324959545",
                    "name": "Chaoqun Yang"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2324793557",
                    "name": "Cunxiao Du"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": null,
                    "name": "See-Kiong Ng"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "802b57e802e32413556ef85144b475ab5185669f",
            "title": "Unified Text-to-Image Generation and Retrieval",
            "abstract": "How humans can efficiently and effectively acquire images has always been a perennial question. A typical solution is text-to-image retrieval from an existing database given the text query; however, the limited database typically lacks creativity. By contrast, recent breakthroughs in text-to-image generation have made it possible to produce fancy and diverse visual content, but it faces challenges in synthesizing knowledge-intensive images. In this work, we rethink the relationship between text-to-image generation and retrieval and propose a unified framework in the context of Multimodal Large Language Models (MLLMs). Specifically, we first explore the intrinsic discriminative abilities of MLLMs and introduce a generative retrieval method to perform retrieval in a training-free manner. Subsequently, we unify generation and retrieval in an autoregressive generation way and propose an autonomous decision module to choose the best-matched one between generated and retrieved images as the response to the text query. Additionally, we construct a benchmark called TIGeR-Bench, including creative and knowledge-intensive domains, to standardize the evaluation of unified text-to-image generation and retrieval. Extensive experimental results on TIGeR-Bench and two retrieval benchmarks, i.e., Flickr30K and MS-COCO, demonstrate the superiority and effectiveness of our proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1990265392",
                    "name": "Leigang Qu"
                },
                {
                    "authorId": "2307185232",
                    "name": "Haochuan Li"
                },
                {
                    "authorId": "2306004318",
                    "name": "Tan Wang"
                },
                {
                    "authorId": "49336556",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2270021997",
                    "name": "Liqiang Nie"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "a1a369959203dc05c2eec1a2f82b3fc1dc34dded",
            "title": "Revolutionizing Text-to-Image Retrieval as Autoregressive Token-to-Voken Generation",
            "abstract": "Text-to-image retrieval is a fundamental task in multimedia processing, aiming to retrieve semantically relevant cross-modal content. Traditional studies have typically approached this task as a discriminative problem, matching the text and image via the cross-attention mechanism (one-tower framework) or in a common embedding space (two-tower framework). Recently, generative cross-modal retrieval has emerged as a new research line, which assigns images with unique string identifiers and generates the target identifier as the retrieval target. Despite its great potential, existing generative approaches are limited due to the following issues: insufficient visual information in identifiers, misalignment with high-level semantics, and learning gap towards the retrieval target. To address the above issues, we propose an autoregressive voken generation method, named AVG. AVG tokenizes images into vokens, i.e., visual tokens, and innovatively formulates the text-to-image retrieval task as a token-to-voken generation problem. AVG discretizes an image into a sequence of vokens as the identifier of the image, while maintaining the alignment with both the visual information and high-level semantics of the image. Additionally, to bridge the learning gap between generative training and the retrieval target, we incorporate discriminative training to modify the learning direction during token-to-voken training. Extensive experiments demonstrate that AVG achieves superior results in both effectiveness and efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2312897913",
                    "name": "Hongru Cai"
                },
                {
                    "authorId": "49336556",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "1990265392",
                    "name": "Leigang Qu"
                },
                {
                    "authorId": "2279771878",
                    "name": "Yinwei Wei"
                },
                {
                    "authorId": "2284641622",
                    "name": "Wenjie Li"
                },
                {
                    "authorId": "2270021997",
                    "name": "Liqiang Nie"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        }
    ]
}