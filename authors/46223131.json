{
    "authorId": "46223131",
    "papers": [
        {
            "paperId": "918fb17504fe62438e40c3340669ea53c202be04",
            "title": "DR-RAG: Applying Dynamic Document Relevance to Retrieval-Augmented Generation for Question-Answering",
            "abstract": "Retrieval-Augmented Generation (RAG) has recently demonstrated the performance of Large Language Models (LLMs) in the knowledge-intensive tasks such as Question-Answering (QA). RAG expands the query context by incorporating external knowledge bases to enhance the response accuracy. However, it would be inefficient to access LLMs multiple times for each query and unreliable to retrieve all the relevant documents by a single query. We have found that even though there is low relevance between some critical documents and query, it is possible to retrieve the remaining documents by combining parts of the documents with the query. To mine the relevance, a two-stage retrieval framework called Dynamic-Relevant Retrieval-Augmented Generation (DR-RAG) is proposed to improve document retrieval recall and the accuracy of answers while maintaining efficiency. Additionally, a compact classifier is applied to two different selection strategies to determine the contribution of the retrieved documents to answering the query and retrieve the relatively relevant documents. Meanwhile, DR-RAG call the LLMs only once, which significantly improves the efficiency of the experiment. The experimental results on multi-hop QA datasets show that DR-RAG can significantly improve the accuracy of the answers and achieve new progress in QA systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2305682244",
                    "name": "Zijian Hei"
                },
                {
                    "authorId": "2305742427",
                    "name": "Weiling Liu"
                },
                {
                    "authorId": "46223131",
                    "name": "Wenjie Ou"
                },
                {
                    "authorId": "2305681770",
                    "name": "Juyi Qiao"
                },
                {
                    "authorId": "2305663892",
                    "name": "Junming Jiao"
                },
                {
                    "authorId": "2305665203",
                    "name": "Guowen Song"
                },
                {
                    "authorId": "2306947084",
                    "name": "Ting Tian"
                },
                {
                    "authorId": "2306954967",
                    "name": "Yi Lin"
                }
            ]
        },
        {
            "paperId": "a3ffe5939a3cbd1f8424175e6725c2e577b0f518",
            "title": "Effective Unsupervised Constrained Text Generation based on Perturbed Masking",
            "abstract": "Unsupervised constrained text generation aims to generate text under a given set of constraints without any supervised data. Current state-of-the-art methods stochastically sample edit positions and actions, which may cause unnecessary search steps. In this paper, we propose PMCTG to improve effectiveness by searching for the best edit position and action in each step. Specifically, PMCTG extends perturbed masking technique to effectively search for the most incongruent token to edit. Then it introduces four multi-aspect scoring functions to select edit action to further reduce search difficulty. Since PMCTG does not require supervised data, it could be applied to different generation tasks. We show that under the unsupervised setting, PMCTG achieves new state-of-the-art results in two representative tasks, namely keywords- to-sentence generation and paraphrasing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "46223131",
                    "name": "Wenjie Ou"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "642e04ece55c5b532ff7e5408d8723c7d9c835db",
            "title": "MIGA: A Unified Multi-task Generation Framework for Conversational Text-to-SQL",
            "abstract": "Conversational text-to-SQL is designed to translate multi-turn natural language questions into their corresponding SQL queries. Most advanced conversational text-to-SQL methods are incompatible with generative pre-trained language models (PLMs), such as T5. In this paper, we present a two-stage unified MultI-task Generation frAmework (MIGA) that leverages PLMs\u2019 ability to tackle conversational text-to-SQL. In the pre-training stage, MIGA first decomposes the main task into several related sub-tasks and then unifies them into the same sequence-to-sequence (Seq2Seq) paradigm with task-specific natural language prompts to boost the main task from multi-task training. Later in the fine-tuning stage, we propose four SQL perturbations to alleviate the error propagation problem. MIGA tends to achieve state-of-the-art performance on two benchmarks (SparC and CoSQL). We also provide extensive analyses and discussions to shed light on some new perspectives for conversational text-to-SQL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "46223131",
                    "name": "Wenjie Ou"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "8fc04ba2024f99b0a3bcde765cd3e4c0fbb2f074",
            "title": "Improve Cross-Lingual Text-To-Speech Synthesis on Monolingual Corpora with Pitch Contour Information",
            "abstract": "Cross-lingual text-to-speech (TTS) synthesis on monolingual corpora is still a challenging task, especially when many kinds of languages are involved. In this paper, we improve the cross-lingual TTS model on monolingual corpora with pitch contour information. We propose a method to obtain pitch contour sequences for different languages without manual annotation, and extend the Tacotron-based TTS model with the proposed Pitch Contour Extraction (PCE) module. Our experimental results show that the proposed approach can effectively improve the naturalness and consistency of synthesized mixed-lingual utterances.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065267796",
                    "name": "Hao Zhan"
                },
                {
                    "authorId": "1391191130",
                    "name": "Haitong Zhang"
                },
                {
                    "authorId": "46223131",
                    "name": "Wenjie Ou"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "911f056c39220634b9d7a1e8ff7f2aea24998061",
            "title": "Systems at SDU-2021 Task 1: Transformers for Sentence Level Sequence Label",
            "abstract": "This paper describes the system proposed for addressing the research problem posed in Task1 of scienti\ufb01c document understanding (SDU@AAAI-2021): Acronym Identi\ufb01cation. We proposed an end-to-end model that takes the text as input and corresponding to each word gives the label of word to be acronyms (short-forms) or their meanings (long-forms). We take experiment on several totally different ideas, including features engineering, transformer model, multi-task learning, Span and CRF. Our result shows that feature-based method can handle this task well, and transformer-based models are particularly effective in this task. Moreover, different model frameworks complement each other. We achieved the best f1 score of 0.931 on test dataset and were ranked second.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146315741",
                    "name": "Feng Li"
                },
                {
                    "authorId": "2067325517",
                    "name": "Zhensheng Mai"
                },
                {
                    "authorId": "2057329020",
                    "name": "Wuhe Zou"
                },
                {
                    "authorId": "46223131",
                    "name": "Wenjie Ou"
                },
                {
                    "authorId": "47719677",
                    "name": "Xiaolei Qin"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                },
                {
                    "authorId": "2108134355",
                    "name": "Weidong Zhang"
                }
            ]
        },
        {
            "paperId": "1feffd371f7066d299e8ca4f0501483600a31014",
            "title": "A Clarifying Question Selection System from NTES_ALONG in Convai3 Challenge",
            "abstract": "This paper presents the participation of NetEase Game AI Lab team for the ClariQ challenge at Search-oriented Conversational AI (SCAI) EMNLP workshop in 2020. The challenge asks for a complete conversational information retrieval system that can understanding and generating clarification questions. We propose a clarifying question selection system which consists of response understanding, candidate question recalling and clarifying question ranking. We fine-tune a RoBERTa model to understand user's responses and use an enhanced BM25 model to recall the candidate questions. In clarifying question ranking stage, we reconstruct the training dataset and propose two models based on ELECTRA. Finally we ensemble the models by summing up their output probabilities and choose the question with the highest probability as the clarification question. Experiments show that our ensemble ranking model outperforms in the document relevance task and achieves the best recall@[20,30] metrics in question relevance task. And in multi-turn conversation evaluation in stage2, our system achieve the top score of all document relevance metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46223131",
                    "name": "Wenjie Ou"
                },
                {
                    "authorId": "8708059",
                    "name": "Yue Lin"
                }
            ]
        }
    ]
}