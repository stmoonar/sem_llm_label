{
    "authorId": "2260340390",
    "papers": [
        {
            "paperId": "06b3fadf49b2cf9927f8f56539c9cb6e99ea6a24",
            "title": "Attribute or Abstain: Large Language Models as Long Document Assistants",
            "abstract": "LLMs can help humans working with long documents, but are known to hallucinate. Attribution can increase trust in LLM responses: The LLM provides evidence that supports its response, which enhances verifiability. Existing approaches to attribution have only been evaluated in RAG settings, where the initial retrieval confounds LLM performance. This is crucially different from the long document setting, where retrieval is not needed, but could help. Thus, a long document specific evaluation of attribution is missing. To fill this gap, we present LAB, a benchmark of 6 diverse long document tasks with attribution, and experiment with different approaches to attribution on 4 LLMs of different sizes, both prompted and fine-tuned. We find that citation, i.e. response generation and evidence extraction in one step, mostly performs best. We investigate whether the ``Lost in the Middle'' phenomenon exists for attribution, but do not find this. We also find that evidence quality can predict response quality on datasets with simple responses, but not so for complex responses, as models struggle with providing evidence for complex claims. We release code and data for further investigation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "122228470",
                    "name": "Jan Buchmann"
                },
                {
                    "authorId": "2310833946",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                }
            ]
        },
        {
            "paperId": "0793b0ff0bebf68a2b2fa5296fae6c850e25c2dc",
            "title": "HDT: Hierarchical Document Transformer",
            "abstract": "In this paper, we propose the Hierarchical Document Transformer (HDT), a novel sparse Transformer architecture tailored for structured hierarchical documents. Such documents are extremely important in numerous domains, including science, law or medicine. However, most existing solutions are inefficient and fail to make use of the structure inherent to documents. HDT exploits document structure by introducing auxiliary anchor tokens and redesigning the attention mechanism into a sparse multi-level hierarchy. This approach facilitates information exchange between tokens at different levels while maintaining sparsity, thereby enhancing computational and memory efficiency while exploiting the document structure as an inductive bias. We address the technical challenge of implementing HDT's sample-dependent hierarchical attention pattern by developing a novel sparse attention kernel that considers the hierarchical structure of documents. As demonstrated by our experiments, utilizing structural information present in documents leads to faster convergence, higher sample efficiency and better performance on downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2310778640",
                    "name": "Haoyu He"
                },
                {
                    "authorId": "2310700076",
                    "name": "Markus Flicke"
                },
                {
                    "authorId": "122228470",
                    "name": "Jan Buchmann"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2310699712",
                    "name": "Andreas Geiger"
                }
            ]
        },
        {
            "paperId": "0bc0168f2c92861f6711cd5a2e54c027c9c024d5",
            "title": "Learning from Emotions, Demographic Information and Implicit User Feedback in Task-Oriented Document-Grounded Dialogues",
            "abstract": "The success of task-oriented and document-grounded dialogue systems depends on users accepting and enjoying using them. To achieve this, recently published work in the field of Human-Computer Interaction suggests that the combination of considering demographic information, user emotions and learning from the implicit feedback in their utterances, is particularly important. However, these findings have not yet been transferred to the field of Natural Language Processing, where these data are primarily studied separately. Accordingly, no sufficiently annotated dataset is available. To address this gap, we introduce FEDI, the first English dialogue dataset for task-oriented document-grounded dialogues annotated with demographic information, user emotions and implicit feedback. Our experiments with FLAN-T5, GPT-2 and LLaMA-2 show that these data have the potential to improve task completion and the factual consistency of the generated responses and user acceptance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145795628",
                    "name": "Dominic Petrak"
                },
                {
                    "authorId": "151502102",
                    "name": "Thy Thy Tran"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                }
            ]
        },
        {
            "paperId": "278b813f0a0e18566d41493a933efba7c2da892c",
            "title": "Diagnostic Reasoning in Natural Language: Computational Model and Application",
            "abstract": "Diagnostic reasoning is a key component of expert work in many domains. It is a hard, time-consuming activity that requires expertise, and AI research has investigated the ways automated systems can support this process. Yet, due to the complexity of natural language, the applications of AI for diagnostic reasoning to language-related tasks are lacking. To close this gap, we investigate diagnostic abductive reasoning (DAR) in the context of language-grounded tasks (NL-DAR). We propose a novel modeling framework for NL-DAR based on Pearl's structural causal models and instantiate it in a comprehensive study of scientific paper assessment in the biomedical domain. We use the resulting dataset to investigate the human decision-making process in NL-DAR and determine the potential of LLMs to support structured decision-making over text. Our framework, open resources and tools lay the groundwork for the empirical study of collaborative diagnostic reasoning in the age of LLMs, in the scholarly domain and beyond.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1740602147",
                    "name": "Nils Dycke"
                },
                {
                    "authorId": "2170347392",
                    "name": "Matej Zevcevi'c"
                },
                {
                    "authorId": "145775250",
                    "name": "Ilia Kuznetsov"
                },
                {
                    "authorId": "2320310696",
                    "name": "Beatrix Suess"
                },
                {
                    "authorId": "2320303871",
                    "name": "Kristian Kersting"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                }
            ]
        },
        {
            "paperId": "3414be052766667fe375221804e48f4a9c815ba5",
            "title": "Multimodal Large Language Models to Support Real-World Fact-Checking",
            "abstract": "Multimodal large language models (MLLMs) carry the potential to support humans in processing vast amounts of information. While MLLMs are already being used as a fact-checking tool, their abilities and limitations in this regard are understudied. Here is aim to bridge this gap. In particular, we propose a framework for systematically assessing the capacity of current multimodal models to facilitate real-world fact-checking. Our methodology is evidence-free, leveraging only these models' intrinsic knowledge and reasoning capabilities. By designing prompts that extract models' predictions, explanations, and confidence levels, we delve into research questions concerning model accuracy, robustness, and reasons for failure. We empirically find that (1) GPT-4V exhibits superior performance in identifying malicious and misleading multimodal claims, with the ability to explain the unreasonable aspects and underlying motives, and (2) existing open-source models exhibit strong biases and are highly sensitive to the prompt. Our study offers insights into combating false multimodal information and building secure, trustworthy multimodal models. To the best of our knowledge, we are the first to evaluate MLLMs for real-world fact-checking.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266466915",
                    "name": "Jiahui Geng"
                },
                {
                    "authorId": "51208524",
                    "name": "Yova Kementchedjhieva"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                }
            ]
        },
        {
            "paperId": "3c95662116a1a06ad9688460121643e7a52177ec",
            "title": "Reviewer No. 2: Old and New Problems in Peer Review (Dagstuhl Seminar 24052)",
            "abstract": "This report documents the program and the outcomes of Dagstuhl Seminar 24052 \u201cReviewer No. 2: Old and New Problems in Peer Review\u201d. This seminar provided a point of reflection on decades of personal experience of the participants in organizing different kinds of peer-reviewed venues in Natural Language Processing (NLP) and beyond, enabling an in-depth discussion of what has been tried, what seems to work and what doesn\u2019t. The outcomes of the seminar include a white paper co-authored by most of the seminar participants",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2301020698",
                    "name": "Anna Rogers"
                },
                {
                    "authorId": "2266853981",
                    "name": "Nihar B. Shah"
                },
                {
                    "authorId": "2301091804",
                    "name": "Jingyan Wang"
                }
            ]
        },
        {
            "paperId": "4098bc3bfaa4c8f5e92f5e55670234e29d80a63c",
            "title": "Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions",
            "abstract": "Classification is a core NLP task architecture with many potential applications. While large language models (LLMs) have brought substantial advancements in text generation, their potential for enhancing classification tasks remains underexplored. To address this gap, we propose a framework for thoroughly investigating fine-tuning LLMs for classification, including both generation- and encoding-based approaches. We instantiate this framework in edit intent classification (EIC), a challenging and underexplored classification task. Our extensive experiments and systematic comparisons with various training approaches and a representative selection of LLMs yield new insights into their application for EIC. We investigate the generalizability of these findings on five further classification tasks. To demonstrate the proposed methods and address the data shortage for empirical edit analysis, we use our best-performing EIC model to create Re3-Sci2.0, a new large-scale dataset of 1,780 scientific document revisions with over 94k labeled edits. The quality of the dataset is assessed through human evaluation. The new dataset enables an in-depth empirical study of human editing behavior in academic writing. We make our experimental framework, models and data publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2304431752",
                    "name": "Qian Ruan"
                },
                {
                    "authorId": "145775250",
                    "name": "Ilia Kuznetsov"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                }
            ]
        },
        {
            "paperId": "476356bf86b393fb16321bccec54c23a902ac137",
            "title": "The Nature of NLP: Analyzing Contributions in NLP Papers",
            "abstract": "Natural Language Processing (NLP) is a dynamic, interdisciplinary field that integrates intellectual traditions from computer science, linguistics, social science, and more. Despite its established presence, the definition of what constitutes NLP research remains debated. In this work, we quantitatively investigate what constitutes NLP by examining research papers. For this purpose, we propose a taxonomy and introduce NLPContributions, a dataset of nearly $2k$ research paper abstracts, expertly annotated to identify scientific contributions and classify their types according to this taxonomy. We also propose a novel task to automatically identify these elements, for which we train a strong baseline on our dataset. We present experimental results from this task and apply our model to $\\sim$$29k$ NLP research papers to analyze their contributions, aiding in the understanding of the nature of NLP research. Our findings reveal a rising involvement of machine learning in NLP since the early nineties, alongside a declining focus on adding knowledge about language or people; again, in post-2020, there has been a resurgence of focus on language and people. We hope this work will spark discussions on our community norms and inspire efforts to consciously shape the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35156694",
                    "name": "Aniket Pramanick"
                },
                {
                    "authorId": "2282504397",
                    "name": "Yufang Hou"
                },
                {
                    "authorId": "2323503899",
                    "name": "Saif M. Mohammad"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                }
            ]
        },
        {
            "paperId": "49afbec1f114f46d2c6609cec57b631b81929e80",
            "title": "DOCE: Finding the Sweet Spot for Execution-Based Code Generation",
            "abstract": "Recently, a diverse set of decoding and reranking procedures have been shown effective for LLM-based code generation. However, a comprehensive framework that links and experimentally compares these methods is missing. We address this by proposing Decoding Objectives for Code Execution, a comprehensive framework that includes candidate generation, $n$-best reranking, minimum Bayes risk (MBR) decoding, and self-debugging as the core components. We then study the contributions of these components through execution-based evaluation metrics. Our findings highlight the importance of execution-based methods and the difference gap between execution-based and execution-free methods. Furthermore, we assess the impact of filtering based on trial unit tests, a simple and effective strategy that has been often overlooked in prior works. We also propose self-debugging on multiple candidates, obtaining state-of-the-art performance on reranking for code generation. We expect our framework to provide a solid guideline for future research on code generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1993902967",
                    "name": "Haau-Sing Li"
                },
                {
                    "authorId": "2282214698",
                    "name": "Patrick Fernandes"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2254383708",
                    "name": "Andr'e F. T. Martins"
                }
            ]
        },
        {
            "paperId": "568244f718a2546fdbe3ff471f89224e3055daa9",
            "title": "GeoHard: Towards Measuring Class-wise Hardness through Modelling Class Semantics",
            "abstract": "Recent advances in measuring hardness-wise properties of data guide language models in sample selection within low-resource scenarios. However, class-specific properties are overlooked for task setup and learning. How will these properties influence model learning and is it generalizable across datasets? To answer this question, this work formally initiates the concept of $\\textit{class-wise hardness}$. Experiments across eight natural language understanding (NLU) datasets demonstrate a consistent hardness distribution across learning paradigms, models, and human judgment. Subsequent experiments unveil a notable challenge in measuring such class-wise hardness with instance-level metrics in previous works. To address this, we propose $\\textit{GeoHard}$ for class-wise hardness measurement by modeling class geometry in the semantic embedding space. $\\textit{GeoHard}$ surpasses instance-level metrics by over 59 percent on $\\textit{Pearson}$'s correlation on measuring class-wise hardness. Our analysis theoretically and empirically underscores the generality of $\\textit{GeoHard}$ as a fresh perspective on data diagnosis. Additionally, we showcase how understanding class-wise hardness can practically aid in improving task learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266465656",
                    "name": "Fengyu Cai"
                },
                {
                    "authorId": "1500662261",
                    "name": "Xinran Zhao"
                },
                {
                    "authorId": "2279761345",
                    "name": "Hongming Zhang"
                },
                {
                    "authorId": "2260340390",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2311446031",
                    "name": "Heinz Koeppl"
                }
            ]
        }
    ]
}