{
    "authorId": "2187856386",
    "papers": [
        {
            "paperId": "a8d187fcd203caa2509e6c679015fb62bce428a7",
            "title": "Sequential Recommendation for Optimizing Both Immediate Feedback and Long-term Retention",
            "abstract": "In the landscape of Recommender System (RS) applications, reinforcement learning (RL) has recently emerged as a powerful tool, primarily due to its proficiency in optimizing long-term rewards. Nevertheless, it suffers from instability in the learning process, stemming from the intricate interactions among bootstrapping, off-policy training, and function approximation. Moreover, in multi-reward recommendation scenarios, designing a proper reward setting that reconciles the inner dynamics of various tasks is quite intricate. In response to these challenges, we introduce DT4IER, an advanced decision transformer-based recommendation model that is engineered to not only elevate the effectiveness of recommendations but also to achieve a harmonious balance between immediate user engagement and long-term retention. The DT4IER applies an innovative multi-reward design that adeptly balances short and long-term rewards with user-specific attributes, which serve to enhance the contextual richness of the reward sequence ensuring a more informed and personalized recommendation process. To enhance its predictive capabilities, DT4IER incorporates a high-dimensional encoder, skillfully designed to identify and leverage the intricate interrelations across diverse tasks. Furthermore, we integrate a contrastive learning approach within the action embedding predictions, a strategy that significantly boosts the model's overall performance. Experiments on three real-world datasets demonstrate the effectiveness of DT4IER against state-of-the-art Sequential Recommender Systems (SRSs) and Multi-Task Learning (MTL) models in terms of both prediction accuracy and effectiveness in specific tasks. The source code is accessible online to facilitate replication",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2204700561",
                    "name": "Ziru Liu"
                },
                {
                    "authorId": "2244772979",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "2187882131",
                    "name": "Zijian Zhang"
                },
                {
                    "authorId": "2244625023",
                    "name": "Qingpeng Cai"
                },
                {
                    "authorId": "2244774353",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2187856386",
                    "name": "Kesen Zhao"
                },
                {
                    "authorId": "2258334183",
                    "name": "Lantao Hu"
                },
                {
                    "authorId": "2244625137",
                    "name": "Peng Jiang"
                },
                {
                    "authorId": "2244624390",
                    "name": "Kun Gai"
                }
            ]
        },
        {
            "paperId": "2807de52310ca3750ccc8dd2636d5ddc6d14a187",
            "title": "KuaiSim: A Comprehensive Simulator for Recommender Systems",
            "abstract": "Reinforcement Learning (RL)-based recommender systems (RSs) have garnered considerable attention due to their ability to learn optimal recommendation policies and maximize long-term user rewards. However, deploying RL models directly in online environments and generating authentic data through A/B tests can pose challenges and require substantial resources. Simulators offer an alternative approach by providing training and evaluation environments for RS models, reducing reliance on real-world data. Existing simulators have shown promising results but also have limitations such as simplified user feedback, lacking consistency with real-world data, the challenge of simulator evaluation, and difficulties in migration and expansion across RSs. To address these challenges, we propose KuaiSim, a comprehensive user environment that provides user feedback with multi-behavior and cross-session responses. The resulting simulator can support three levels of recommendation problems: the request level list-wise recommendation task, the whole-session level sequential recommendation task, and the cross-session level retention optimization task. For each task, KuaiSim also provides evaluation protocols and baseline recommendation algorithms that further serve as benchmarks for future research. We also restructure existing competitive simulators on the KuaiRand Dataset and compare them against KuaiSim to future assess their performance and behavioral differences. Furthermore, to showcase KuaiSim's flexibility in accommodating different datasets, we demonstrate its versatility and robustness when deploying it on the ML-1m dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187856386",
                    "name": "Kesen Zhao"
                },
                {
                    "authorId": "2244772979",
                    "name": "Shuchang Liu"
                },
                {
                    "authorId": "2244625023",
                    "name": "Qingpeng Cai"
                },
                {
                    "authorId": "2244774353",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2204700561",
                    "name": "Ziru Liu"
                },
                {
                    "authorId": "2153430224",
                    "name": "Dong Zheng"
                },
                {
                    "authorId": "2244625137",
                    "name": "Peng Jiang"
                },
                {
                    "authorId": "2244624390",
                    "name": "Kun Gai"
                }
            ]
        },
        {
            "paperId": "a66bb19c36db1a2f400b4c8f90b5da464b6c9e80",
            "title": "User Retention-oriented Recommendation with Decision Transformer",
            "abstract": "Improving user retention with reinforcement learning (RL) has attracted increasing attention due to its significant importance in boosting user engagement. However, training the RL policy from scratch without hurting users\u2019 experience is unavoidable due to the requirement of trial-and-error searches. Furthermore, the offline methods, which aim to optimize the policy without online interactions, suffer from the notorious stability problem in value estimation or unbounded variance in counterfactual policy evaluation. To this end, we propose optimizing user retention with Decision Transformer (DT), which avoids the offline difficulty by translating the RL as an autoregressive problem. However, deploying the DT in recommendation is a non-trivial problem because of the following challenges: (1) deficiency in modeling the numerical reward value; (2) data discrepancy between the policy learning and recommendation generation; (3) unreliable offline performance evaluation. In this work, we, therefore, contribute a series of strategies for tackling the exposed issues. We first articulate an efficient reward prompt by weighted aggregation of meta embeddings for informative reward embedding. Then, we endow a weighted contrastive learning method to solve the discrepancy between training and inference. Furthermore, we design two robust offline metrics to measure user retention. Finally, the significant improvement in the benchmark datasets demonstrates the superiority of the proposed method. The implementation code is available at https://github.com/kesenzhao/DT4Rec.git.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187856386",
                    "name": "Kesen Zhao"
                },
                {
                    "authorId": "8718022",
                    "name": "Lixin Zou"
                },
                {
                    "authorId": "2116711669",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2109132520",
                    "name": "Maolin Wang"
                },
                {
                    "authorId": "50559722",
                    "name": "Dawei Yin"
                }
            ]
        },
        {
            "paperId": "3bf91e6998b6d02a728a96ebf47e69a5635dc8c9",
            "title": "MAE4Rec: Storage-saving Transformer for Sequential Recommendations",
            "abstract": "Sequential recommender systems (SRS) aim to infer the users' preferences from their interaction history and predict items that will be of interest to the users. The majority of SRS models typically incorporate all historical interactions for next-item recommendations. Despite their success, feeding all interactions into the model without filtering may lead to severe practical issues: (i) redundant interactions hinder the SRS model from capturing the users' intentions; (ii) the computational cost is huge, as the computational complexity is proportional to the length of the interaction sequence; (iii) more memory space is necessitated to store all interaction records from all users. To this end, we propose a novel storage-saving SRS framework, MAE4Rec, based on a unidirectional self-attentive mechanism and masked autoencoder. Specifically, in order to lower the storage consumption, MAE4Rec first masks and discards a large percentage of historical interactions, and then infers the next interacted item solely based on the latent representation of unmarked ones. Experiments on two real-world datasets demonstrate that the proposed model achieves competitive performance against state-of-the-art SRS models with more than 40% compression of storage.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187856386",
                    "name": "Kesen Zhao"
                },
                {
                    "authorId": "2116710405",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2187882131",
                    "name": "Zijian Zhang"
                },
                {
                    "authorId": "2187870890",
                    "name": "Muyang Li"
                }
            ]
        }
    ]
}