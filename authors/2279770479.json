{
    "authorId": "2279770479",
    "papers": [
        {
            "paperId": "0a2cdb13a15d95a5ab05aa5ec921518aa3655e93",
            "title": "All Against Some: Efficient Integration of Large Language Models for Message Passing in Graph Neural Networks",
            "abstract": "Graph Neural Networks (GNNs) have attracted immense attention in the past decade due to their numerous real-world applications built around graph-structured data. On the other hand, Large Language Models (LLMs) with extensive pretrained knowledge and powerful semantic comprehension abilities have recently shown a remarkable ability to benefit applications using vision and text data. In this paper, we investigate how LLMs can be leveraged in a computationally efficient fashion to benefit rich graph-structured data, a modality relatively unexplored in LLM literature. Prior works in this area exploit LLMs to augment every node features in an ad-hoc fashion (not scalable for large graphs), use natural language to describe the complex structural information of graphs, or perform computationally expensive finetuning of LLMs in conjunction with GNNs. We propose E-LLaGNN (Efficient LLMs augmented GNNs), a framework with an on-demand LLM service that enriches message passing procedure of graph learning by enhancing a limited fraction of nodes from the graph. More specifically, E-LLaGNN relies on sampling high-quality neighborhoods using LLMs, followed by on-demand neighborhood feature enhancement using diverse prompts from our prompt catalog, and finally information aggregation using message passing from conventional GNN architectures. We explore several heuristics-based active node selection strategies to limit the computational and memory footprint of LLMs when handling millions of nodes. Through extensive experiments&ablation on popular graph benchmarks of varying scales (Cora, PubMed, ArXiv,&Products), we illustrate the effectiveness of our E-LLaGNN framework and reveal many interesting capabilities such as improved gradient flow in deep GNNs, LLM-free inference ability etc.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2253397454",
                    "name": "A. Jaiswal"
                },
                {
                    "authorId": "2262444503",
                    "name": "Nurendra Choudhary"
                },
                {
                    "authorId": "2299329980",
                    "name": "Ravinarayana Adkathimar"
                },
                {
                    "authorId": "1390172802",
                    "name": "M. P. Alagappan"
                },
                {
                    "authorId": "46566733",
                    "name": "G. Hiranandani"
                },
                {
                    "authorId": "2279770479",
                    "name": "Ying Ding"
                },
                {
                    "authorId": "2254949434",
                    "name": "Zhangyang Wang"
                },
                {
                    "authorId": "2057479333",
                    "name": "E-Wen Huang"
                },
                {
                    "authorId": "2691095",
                    "name": "Karthik Subbian"
                }
            ]
        },
        {
            "paperId": "3aa33174a2a10e1cd3470f67376ccd76557d0fb6",
            "title": "Turning A Curse into A Blessing: Data-Aware Memory-Efficient Training of Graph Neural Networks by Dynamic Exiting",
            "abstract": "Training Graph Neural Networks (GNNs) efficiently remains a challenge due to the high memory demands, especially during recursive neighborhood aggregation. Traditional sampling-based GNN training methods often overlook the data's inherent structure, such as the power-law distribution observed in most real-world graphs, which results in inefficient memory usage and processing. We introduce a novel framework, M emory-A ware D ynamic E xiting GNN (MADE-GNN )), which capitalizes on the power-law nature of graph data to enhance training efficiency. MADE-GNN is designed to be data-aware, dynamically adjusting the depth of feature aggregation based on the connectivity of each node. Specifically, it routes well-connected \"head'' nodes through extensive aggregation while allowing sparsely connected \"tail'' nodes to exit early, thus reducing memory consumption without sacrificing model performance. This approach not only addresses the challenge of memory-intensive GNN training but also turns the power-law distribution from a traditional \"curse'' into a strategic \"blessing''. By enabling partial weight sharing between the early-exit mechanism and the full model, MADE-GNN effectively improves the representation of cold-start nodes, leveraging the structural information from head nodes to enhance generalization across the network. Our extensive evaluations across multiple public benchmarks, including industrial-level graphs, show that MADE-GNN outperforms existing GNN training methods in both memory efficiency and performance, offering significant improvements particularly for tail nodes. This demonstrates MADE-GNN's potential as a versatile solution for GNN applications facing similar scalability and distribution challenges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2279757365",
                    "name": "Yan Han"
                },
                {
                    "authorId": "2301302449",
                    "name": "Kaiqi Chen"
                },
                {
                    "authorId": "2119028796",
                    "name": "Shan Li"
                },
                {
                    "authorId": "2112408681",
                    "name": "Ji Yan"
                },
                {
                    "authorId": "2301214526",
                    "name": "Baoxu Shi"
                },
                {
                    "authorId": "2301408375",
                    "name": "Lei Zhang"
                },
                {
                    "authorId": "2157326100",
                    "name": "Fei Chen"
                },
                {
                    "authorId": "2275528909",
                    "name": "Jaewon Yang"
                },
                {
                    "authorId": "2301409271",
                    "name": "Yunpeng Xu"
                },
                {
                    "authorId": "2301355292",
                    "name": "Xiaoqiang Luo"
                },
                {
                    "authorId": "2152881560",
                    "name": "Qi He"
                },
                {
                    "authorId": "2279770479",
                    "name": "Ying Ding"
                },
                {
                    "authorId": "2248972119",
                    "name": "Zhangyang Wang"
                }
            ]
        },
        {
            "paperId": "7019cdff756a566a030830c40d1948994fa9088f",
            "title": "Vision HGNN: An Image is More than a Graph of Nodes",
            "abstract": "The realm of graph-based modeling has proven its adaptability across diverse real-world data types. However, its applicability to general computer vision tasks had been limited until the introduction of the Vision Graph Neural Network (ViG). ViG divides input images into patches, conceptualized as nodes, constructing a graph through connections to nearest neighbors. Nonetheless, this method of graph construction confines itself to simple pairwise relationships, leading to surplus edges and unwarranted memory and computation expenses. In this paper, we enhance ViG by transcending conventional \"pairwise\" linkages and harnessing the power of the hypergraph to encapsulate image information. Our objective is to encompass more intricate inter-patch associations. In both training and inference phases, we adeptly establish and update the hypergraph structure using the Fuzzy C-Means method, ensuring minimal computational burden. This augmentation yields the Vision HyperGraph Neural Network (ViHGNN). The model\u2019s efficacy is empirically substantiated through its state-of-the-art performance on both image classification and object detection tasks, courtesy of the hypergraph structure learning module that uncovers higher-order relationships. Our code is available at: https://github.com/VITA-Group/ViHGNN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2279757365",
                    "name": "Yan Han"
                },
                {
                    "authorId": "2118952622",
                    "name": "Peihao Wang"
                },
                {
                    "authorId": "2253458053",
                    "name": "Souvik Kundu"
                },
                {
                    "authorId": "2279770479",
                    "name": "Ying Ding"
                },
                {
                    "authorId": "2279750101",
                    "name": "Zhangyang Wang"
                }
            ]
        }
    ]
}