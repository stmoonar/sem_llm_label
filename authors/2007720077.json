{
    "authorId": "2007720077",
    "papers": [
        {
            "paperId": "48f404538e9c0b1af27b23518098b9ab78a01e41",
            "title": "Training Neural Networks for Execution on Approximate Hardware",
            "abstract": "Approximate computing methods have shown great potential for deep learning. Due to the reduced hardware costs, these methods are especially suitable for inference tasks on battery-operated devices that are constrained by their power budget. However, approximate computing hasn't reached its full potential due to the lack of work on training methods. In this work, we discuss training methods for approximate hardware. We demonstrate how training needs to be specialized for approximate hardware, and propose methods to speed up the training process by up to 18X.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10742390",
                    "name": "Tianmu Li"
                },
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                }
            ]
        },
        {
            "paperId": "d858a618ae5c5b73466a59e6754ed1835e42d556",
            "title": "REX-SC: Range-Extended Stochastic Computing Accumulation for Neural Network Acceleration",
            "abstract": "Deep learning has grown in capability and size in recent years, prompting research on alternative computing methods to cope with the increased compute cost. Stochastic computing (SC) promises higher compute efficiency with its compact compute units, but accuracy issues have prevented wide adoption, and accuracy-improving techniques have sacrificed runtime or training performance. In this work, we propose extended range SC\u2014Range-Extended SC Accumulation to deal with the accuracy issues of SC. By modifying the functionality of OR-based SC accumulation, we increase SC computation accuracy without sacrificing the performance benefits. Our approach achieves a <inline-formula> <tex-math notation=\"LaTeX\">$2\\times $ </tex-math></inline-formula> reduction in stream length for the same accuracy compared to SC with OR-based accumulation and an up to <inline-formula> <tex-math notation=\"LaTeX\">$3.6\\times $ </tex-math></inline-formula> improvement in energy compared to SC with binary addition. With proper modeling, our approach improves training performance for SC-based neural networks and makes training SC models practical for large datasets like ImageNet.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10742390",
                    "name": "Tianmu Li"
                },
                {
                    "authorId": "102401101",
                    "name": "W. Romaszkan"
                },
                {
                    "authorId": "1742134",
                    "name": "S. Pamarti"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                }
            ]
        },
        {
            "paperId": "ef356de51a3dc84b0f1753a79e6156d18edc4f86",
            "title": "ReFOCUS: Reusing Light for Efficient Fourier Optics-Based Photonic Neural Network Accelerator",
            "abstract": "In recent years, there has been a significant focus on achieving low-latency and high-throughput convolutional neural network (CNN) inference. Integrated photonics offers the potential to substantially expedite neural networks due to its inherent low-latency properties. Recently, on-chip Fourier optics-based neural network accelerators have been demonstrated and achieved superior energy efficiency for CNN acceleration. By incorporating Fourier optics, computationally intensive convolution operations can be performed instantaneously through on-chip lenses at a significantly lower cost compared to other on-chip photonic neural network accelerators. This is thanks to the complexity reduction offered by the convolution theorem and the passive Fourier transforms computed by on-chip lenses. However, conversion overhead between optical and digital domains and memory access energy still hinder overall efficiency.We introduce ReFOCUS, a Joint Transform Correlator (JTC) based on-chip neural network accelerator that efficiently reuses light through optical buffers. By incorporating optical delay lines, wavelength-division multiplexing, dataflow, and memory hierarchy optimization, ReFOCUS minimizes both conversion overhead and memory access energy. As a result, ReFOCUS achieves 2\u00d7 throughput, 2.2\u00d7 energy efficiency, and 1.36\u00d7 area efficiency compared to state-of-the-art photonic neural network accelerators.CCS CONCEPTS\u2022 Computer systems organization \u2192 Architectures; \u2022 Hardware \u2192 Emerging technologies; Emerging optical and photonic technologies; \u2022 Computing methodologies \u2192 Artificial intelligence; Machine learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "1944367741",
                    "name": "Hangbo Yang"
                },
                {
                    "authorId": "2273700411",
                    "name": "C. W. Wong"
                },
                {
                    "authorId": "2249165298",
                    "name": "V. Sorger"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                }
            ]
        },
        {
            "paperId": "6c3b4dc6e54e99f629954b10bc2c6194f15c500c",
            "title": "Batch processing and data streaming Fourier-based convolutional neural network accelerator",
            "abstract": "Decision-making through artificial neural networks with minimal latency is critical for numerous applications such as navigation, tracking, and real-time machine action systems. This requires machine learning hardware to process multidimensional data at high throughput. Unfortunately, handling convolution operations, the primary computational tool for data classification tasks, obeys challenging runtime complexity scaling laws. However, homomorphically implementing the convolution theorem in a Fourier optics display light processor can achieve a non-iterative O(1) runtime complexity for data inputs beyond 1,000 \u00d7 1,000 large matrices. Following this approach, here we demonstrate data streaming multi-kernel image batching using a Fourier Convolutional Neural Network (FCNN) accelerator. We show image batch processing of large-scale matrices as 2 million dot product multiplications performed by a digital light processing module in the Fourier domain. Furthermore, we further parallelize this optical FCNN system by exploiting multiple spatially parallel diffraction orders, achieving a 98x throughput improvement over state-of-the-art FCNN accelerators. A comprehensive discussion of the practical challenges associated with working at the edge of system capabilities highlights the problem of crosstalk and resolution scaling laws in the Fourier domain. Accelerating convolution by exploiting massive parallelism in display technology brings non-Van Neumann-based machine learning acceleration.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "15368955",
                    "name": "Zibo Hu"
                },
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "2147107765",
                    "name": "Russell L. T. Schwartz"
                },
                {
                    "authorId": "1405208974",
                    "name": "Maria Solyanik-Gorgone"
                },
                {
                    "authorId": "50533784",
                    "name": "M. Miscuglio"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                },
                {
                    "authorId": "1995822",
                    "name": "V. Sorger"
                }
            ]
        },
        {
            "paperId": "6dd5ee1f89c6d60252edf4f5f92f9b8489f27677",
            "title": "Bit-serial Weight Pools: Compression and Arbitrary Precision Execution of Neural Networks on Resource Constrained Processors",
            "abstract": "Applications of neural networks on edge systems have proliferated in recent years but the ever-increasing model size makes neural networks not able to deploy on resource-constrained microcontrollers efficiently. We propose bit-serial weight pools, an end-to-end framework that includes network compression and acceleration of arbitrary sub-byte precision. The framework can achieve up to 8x compression compared to 8-bit networks by sharing a pool of weights across the entire network. We further propose a bit-serial lookup based software implementation that allows runtime-bitwidth tradeoff and is able to achieve more than 2.8x speedup and 7.5x storage compression compared to 8-bit weight pool networks, with less than 1% accuracy drop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                }
            ]
        },
        {
            "paperId": "8ee3e1613ad534661d8510bb3f0136e9276e31c1",
            "title": "SASCHA\u2014Sparsity-Aware Stochastic Computing Hardware Architecture for Neural Network Acceleration",
            "abstract": "Stochastic computing (SC) has recently emerged as a promising method for efficient machine learning acceleration. Its high compute density, affinity with dense linear algebra primitives, and approximation properties have an uncanny level of synergy with the deep neural network computational requirements. However, there is a conspicuous lack of works trying to integrate SC hardware with sparsity awareness, which has brought significant performance improvements to conventional architectures. In this work, we identify why common sparsity-exploiting techniques are not easily applicable to SC accelerators and propose a new architecture\u2014SASCHA\u2014sparsity-aware SC hardware architecture for the neural network acceleration that addresses those issues. SASCHA encompasses a set of techniques that make utilizing sparsity in inference practical for different types of SC computation. At 90% weight sparsity, SASCHA can be up to <inline-formula> <tex-math notation=\"LaTeX\">$6.5\\times $ </tex-math></inline-formula> faster and <inline-formula> <tex-math notation=\"LaTeX\">$5.5\\times $ </tex-math></inline-formula> more energy-efficient than comparable dense SC accelerators with a similar area without sacrificing the dense network throughput. SASCHA also outperforms sparse fixed-point accelerators by up to <inline-formula> <tex-math notation=\"LaTeX\">$4\\times $ </tex-math></inline-formula> in terms of latency. To the best of our knowledge, SASCHA is the first SC accelerator architecture oriented around sparsity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "102401101",
                    "name": "W. Romaszkan"
                },
                {
                    "authorId": "10742390",
                    "name": "Tianmu Li"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                }
            ]
        },
        {
            "paperId": "b636e99ea9a2f277b28ee4a3174266e9a19a7208",
            "title": "PhotoFourier: A Photonic Joint Transform Correlator-Based Neural Network Accelerator",
            "abstract": "The last few years have seen a lot of work to address the challenge of low-latency and high-throughput convolutional neural network inference. Integrated photonics has the potential to dramatically accelerate neural networks because of its low-latency nature. Combined with the concept of Joint Transform Correlator (JTC), the computationally expensive convolution functions can be computed instantaneously (time of flight of light) with almost no cost. This \u2018free\u2019 convolution computation provides the theoretical basis of the proposed PhotoFourier JTC-based CNN accelerator. PhotoFourier addresses a myriad of challenges posed by on-chip photonic computing in the Fourier domain including 1D lenses and high-cost optoelectronic conversions. The proposed PhotoFourier accelerator achieves more than 28\u00d7 better energy-delay product compared to state-of-art photonic neural network accelerators.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "1944367741",
                    "name": "Hangbo Yang"
                },
                {
                    "authorId": "2109650903",
                    "name": "C. Wong"
                },
                {
                    "authorId": "1995822",
                    "name": "V. Sorger"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                }
            ]
        },
        {
            "paperId": "085e86ba82fa36d3e0919d8502406e0f347bc148",
            "title": "SWIS - Shared Weight bIt Sparsity for Efficient Neural Network Acceleration",
            "abstract": "Quantization is spearheading the increase in performance and efficiency of neural network computing systems making headway into commodity hardware. We present SWIS - Shared Weight bIt Sparsity, a quantization framework for efficient neural network inference acceleration delivering improved performance and storage compression through an offline weight decomposition and scheduling algorithm. SWIS can achieve up to 54.3% (19.8%) point accuracy improvement compared to weight truncation when quantizing MobileNet-v2 to 4 (2) bits post-training (with retraining) showing the strength of leveraging shared bit-sparsity in weights. SWIS accelerator gives up to 6x speedup and 1.9x energy improvement overstate of the art bit-serial architectures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "102401101",
                    "name": "W. Romaszkan"
                },
                {
                    "authorId": "2051800694",
                    "name": "A. Graening"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                }
            ]
        },
        {
            "paperId": "26abbf5e20ddbd072bbad0aeb2e01b5c693a1ad8",
            "title": "High\u2010Throughput Multichannel Parallelized Diffraction Convolutional Neural Network Accelerator",
            "abstract": "Convolutional neural networks are paramount in image and signal processing, and are responsible for the majority of image recognition power consumption today, concentrated mainly in convolution computations. With convolution operations being computationally intensive, next\u2010generation hardware accelerators need to offer parallelization and high efficiency. Diffractive optics offers the promise of low\u2010latency, highly parallel convolution operations. However, thus far parallelism is only partially harvested, thereby significantly underdelivering in comparison to its throughput potential. Here, a parallelized operation high\u2010throughput Fourier optic convolutional accelerator is demonstrated. For the first time, simultaneous processing of multiple kernels in Fourier domain enabled by optical diffraction orders is achieved alongside input parallelism. The proposed approach can offer \u2248100\u00d7 speedup over the previous generation optical diffraction\u2010based processor and 10\u00d7 speedup over other state\u2010of\u2010the\u2010art optical Fourier classifiers.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "15368955",
                    "name": "Zibo Hu"
                },
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "2147107765",
                    "name": "Russell L. T. Schwartz"
                },
                {
                    "authorId": "1405208974",
                    "name": "Maria Solyanik-Gorgone"
                },
                {
                    "authorId": "50533784",
                    "name": "M. Miscuglio"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                },
                {
                    "authorId": "1995822",
                    "name": "V. Sorger"
                }
            ]
        },
        {
            "paperId": "4fa547b7ccfaac5f3aebd7b8957d1ad699a892c2",
            "title": "Massively-parallel Amplitude-Only Fourier Optical Convolutional Neural Network",
            "abstract": "Here we introduce a novel amplitude-only Fourier-optical processor paradigm and demonstrate a prototype system capable of processing large-scale ~(2,000x1,000) matrices in a single time-step and 100 microsecond-short latency, for accelerating machine-learning applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50533784",
                    "name": "M. Miscuglio"
                },
                {
                    "authorId": "15368955",
                    "name": "Zibo Hu"
                },
                {
                    "authorId": "1945230198",
                    "name": "Shurui Li"
                },
                {
                    "authorId": "51174084",
                    "name": "J. George"
                },
                {
                    "authorId": "116123609",
                    "name": "R. Capanna"
                },
                {
                    "authorId": "1766287",
                    "name": "H. Dalir"
                },
                {
                    "authorId": "46179055",
                    "name": "P. Bardet"
                },
                {
                    "authorId": "2007720077",
                    "name": "Puneet Gupta"
                },
                {
                    "authorId": "1995822",
                    "name": "V. Sorger"
                }
            ]
        }
    ]
}