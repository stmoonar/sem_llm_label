{
    "authorId": "2080053811",
    "papers": [
        {
            "paperId": "f28e5d83d12eadee6aa30e215bddadf1ad1ea38a",
            "title": "AnaDE1.0: A Novel Data Set for Benchmarking Analogy Detection and Extraction",
            "abstract": "Textual analogies that make comparisons between two concepts are often used for explaining complex ideas, creative writing, and scientific discovery. In this paper, we propose and study a new task, called Analogy Detection and Extraction (AnaDE), which includes three synergistic sub-tasks: 1) detecting documents containing analogies, 2) extracting text segments that make up the analogy, and 3) identifying the (source and target) concepts being compared. To facilitate the study of this new task, we create a benchmark dataset by scraping Metamia.com and investigate the performances of state-of-the-art models on all sub-tasks to establish the first-generation benchmark results for this new task. We find that the Longformer model achieves the best performance on all the three sub-tasks demonstrating its effectiveness for handling long texts. Moreover, smaller models fine-tuned on our dataset perform better than non-finetuned ChatGPT, suggesting high task difficulty. Overall, the models achieve a high performance on documents detection suggesting that it could be used to develop applications like analogy search engines. Further, there is a large room for improvement on the segment and concept extraction tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2080053811",
                    "name": "B. Bhavya"
                },
                {
                    "authorId": "2291365388",
                    "name": "Shradha Sehgal"
                },
                {
                    "authorId": "2291439634",
                    "name": "Jinjun Xiong"
                },
                {
                    "authorId": "2291361881",
                    "name": "ChengXiang Zhai"
                }
            ]
        },
        {
            "paperId": "ad6a1ef3f75bcfda9f7a3567d45f0b5ac108e431",
            "title": "CAM: A Large Language Model-based Creative Analogy Mining Framework",
            "abstract": "Analogies inspire creative solutions to problems, and facilitate the creative expression of ideas and the explanation of complex concepts. They have widespread applications in scientific innovation, creative writing, and education. The ability to discover creative analogies that are not explicitly mentioned but can be inferred from the web is highly desirable to power all such applications dynamically and augment human creativity. Recently, Large Pre-trained Language Models (PLMs), trained on massive Web data, have shown great promise in generating mostly known analogies that are explicitly mentioned on the Web. However, it is unclear how they could be leveraged for mining creative analogies not explicitly mentioned on the Web. We address this challenge and propose Creative Analogy Mining (CAM), a novel framework for mining creative analogies, which consists of the following three main steps: 1) Generate analogies using PLMs with effectively designed prompts, 2) Evaluate their quality using scoring functions, and 3) Refine the low-quality analogies by another round of prompt-based generation. We propose both unsupervised and supervised instantiations of the framework so that it can be used even without any annotated data. Based on human evaluation using Amazon Mechanical Turk, we find that our unsupervised framework can mine 13.7% highly-creative and 56.37% somewhat-creative analogies. Moreover, our supervised scores are generally better than the unsupervised ones and correlate moderately with human evaluators, indicating that they would be even more effective at mining creative analogies. These findings also shed light on the creativity of PLMs 1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2080053811",
                    "name": "B. Bhavya"
                },
                {
                    "authorId": "145042856",
                    "name": "Jinjun Xiong"
                },
                {
                    "authorId": "143869012",
                    "name": "Chengxiang Zhai"
                }
            ]
        },
        {
            "paperId": "fb7b538363240bc0be33633d51559795db656c75",
            "title": "Exploring Large Language Models for Low-Resource IT Information Extraction",
            "abstract": "Information Extraction (IE) in IT is an important foundational task that is needed for many AIOps applications. A major challenge of IE in IT is that we often do not have sufficient labelled data for training machine learning algorithms since acquiring labels is labor-intensive and costly. In this paper, we propose to leverage Large Language Models (LLMs) to address this challenge of low resources and study two data augmentation strategies, i.e., using LLMs to generate pseudo labels and generate synthetic data. We use multiple IE tasks and datasets, including a new Semantic Troubleshooting-Segment Extraction task and Named Entity Recognition, to evaluate the benefits of LLMs. Our experiment results suggest that data augmentation using LLMs, specifically, using SeqMix model that combines active labeling with synthetic data samples generated in the embedding-vector space, is a promising approach for IT domain IE. Our study also shows that although data augmentation and direct labeling with the state-of-the-art, ChatGPT model achieves a high performance on general domain IE, there is a need to adapt it for IE from IT text data. Moreover, our initial exploration of two label weighting and selection strategies (confidence and consistency-based) suggests that they could be used to improve data augmentation with ChatGPT for IT domain IE. Finally, we also suggest directions for future research on the new STSE task, including developing better evaluation metrics. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2080053811",
                    "name": "B. Bhavya"
                },
                {
                    "authorId": "40521719",
                    "name": "Paulina Toro Isaza"
                },
                {
                    "authorId": "2280964160",
                    "name": "Yu Deng"
                },
                {
                    "authorId": "2283157983",
                    "name": "Michael Nidd"
                },
                {
                    "authorId": "2283159181",
                    "name": "Amar Prakash Azad"
                },
                {
                    "authorId": "2280906688",
                    "name": "Larisa Shwartz"
                },
                {
                    "authorId": "2253607011",
                    "name": "ChengXiang Zhai"
                }
            ]
        },
        {
            "paperId": "e8795958aad9c8514d5d7d28b022d42ca4a3a243",
            "title": "Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT",
            "abstract": "We propose a novel application of prompting Pre-trained Language Models (PLMs) to generate analogies and study how to design effective prompts for two task settings: generating a source concept analogous to a given target concept (aka Analogous Concept Generation or ACG), and generating an explanation of the similarity between a given pair of target concept and source concept (aka Analogous Explanation Generation or AEG). We found that it is feasible to prompt InstructGPT to generate meaningful analogies and the best prompts tend to be precise imperative statements especially with a low temperature setting. We also systematically analyzed the sensitivity of the InstructGPT model to prompt design, temperature, and injected spelling errors, and found that the model is particularly sensitive to certain variations (e.g., questions vs. imperative statements). Further, we conducted human evaluation on 1.4k of the generated analogies and found that the quality of generations varies substantially by model size. The largest InstructGPT model can achieve human-level performance at generating meaningful analogies for a given target while there is still room for improvement on the AEG task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2080053811",
                    "name": "B. Bhavya"
                },
                {
                    "authorId": "145042856",
                    "name": "Jinjun Xiong"
                },
                {
                    "authorId": "143869012",
                    "name": "Chengxiang Zhai"
                }
            ]
        },
        {
            "paperId": "81e1e9375a939e1a4c6d0185d38bb6cbda394f96",
            "title": "Scaling Up Data Science Course Projects: A Case Study",
            "abstract": "Large-scale, online Data Science (DS) courses and degree programs are becoming increasingly common due to the global rise in popularity and demand for data scientists. Although project-based learning is integral to gaining hands-on experience in DS education, providing fair, timely, and high-quality feedback on varied projects for a large number of diverse students is challenging. To address those challenges in scaling up the assessment of DS group projects, we integrated multiple techniques, such as rapid feedback, peer grading, graders as meta-reviewers, etc. We present a case study of deploying those strategies for group projects in a large online DS course titled Text Information Systems offered in Fall, 2020. We synthesize our findings from analyzing student and grader survey responses, and share useful lessons and future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2080053811",
                    "name": "B. Bhavya"
                },
                {
                    "authorId": "46851930",
                    "name": "Jinfeng Xiao"
                },
                {
                    "authorId": "1736467",
                    "name": "ChengXiang Zhai"
                }
            ]
        }
    ]
}