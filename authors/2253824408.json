{
    "authorId": "2253824408",
    "papers": [
        {
            "paperId": "2a487ca3f9a1431f2a21a70d97e12dd955985d88",
            "title": "Deep Learning for Multivariate Time Series Imputation: A Survey",
            "abstract": "The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis. Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks. In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods. First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations. We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks. Finally, the open issues for future research on multivariate time series imputation are pointed out. All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be found in the GitHub repository~\\url{https://github.com/WenjieDu/Awesome\\_Imputation}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283071353",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2049476022",
                    "name": "Wenjie Du"
                },
                {
                    "authorId": "2190953133",
                    "name": "Wei Cao"
                },
                {
                    "authorId": "2282953676",
                    "name": "Keli Zhang"
                },
                {
                    "authorId": "2283086874",
                    "name": "Wenjia Wang"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "7357961c1f068f6b56e5513c4887ef00d28113ba",
            "title": "TwinS: Revisiting Non-Stationarity in Multivariate Time Series Forecasting",
            "abstract": "Recently, multivariate time series forecasting tasks have garnered increasing attention due to their significant practical applications, leading to the emergence of various deep forecasting models. However, real-world time series exhibit pronounced non-stationary distribution characteristics. These characteristics are not solely limited to time-varying statistical properties highlighted by non-stationary Transformer but also encompass three key aspects: nested periodicity, absence of periodic distributions, and hysteresis among time variables. In this paper, we begin by validating this theory through wavelet analysis and propose the Transformer-based TwinS model, which consists of three modules to address the non-stationary periodic distributions: Wavelet Convolution, Period-Aware Attention, and Channel-Temporal Mixed MLP. Specifically, The Wavelet Convolution models nested periods by scaling the convolution kernel size like wavelet transform. The Period-Aware Attention guides attention computation by generating period relevance scores through a convolutional sub-network. The Channel-Temporal Mixed MLP captures the overall relationships between time series through channel-time mixing learning. TwinS achieves SOTA performance compared to mainstream TS models, with a maximum improvement in MSE of 25.8\\% over PatchTST.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284730172",
                    "name": "Jiaxi Hu"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2297769202",
                    "name": "Sijie Ruan"
                },
                {
                    "authorId": "2293451547",
                    "name": "Li Liu"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                }
            ]
        },
        {
            "paperId": "74049a723c1c52b4f2dc01b28bef137771cb1148",
            "title": "TSI-Bench: Benchmarking Time Series Imputation",
            "abstract": "Effective imputation is a crucial preprocessing step for time series analysis. Despite the development of numerous deep learning algorithms for time series imputation, the community lacks standardized and comprehensive benchmark platforms to effectively evaluate imputation performance across different settings. Moreover, although many deep learning forecasting algorithms have demonstrated excellent performance, whether their modeling achievements can be transferred to time series imputation tasks remains unexplored. To bridge these gaps, we develop TSI-Bench, the first (to our knowledge) comprehensive benchmark suite for time series imputation utilizing deep learning techniques. The TSI-Bench pipeline standardizes experimental settings to enable fair evaluation of imputation algorithms and identification of meaningful insights into the influence of domain-appropriate missingness ratios and patterns on model performance. Furthermore, TSI-Bench innovatively provides a systematic paradigm to tailor time series forecasting algorithms for imputation purposes. Our extensive study across 34,804 experiments, 28 algorithms, and 8 datasets with diverse missingness scenarios demonstrates TSI-Bench's effectiveness in diverse downstream tasks and potential to unlock future directions in time series imputation research and analysis. The source code and experiment logs are available at https://github.com/WenjieDu/AwesomeImputation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2303468840",
                    "name": "Wenjie Du"
                },
                {
                    "authorId": "2283071353",
                    "name": "Jun Wang"
                },
                {
                    "authorId": "2151171181",
                    "name": "Linglong Qian"
                },
                {
                    "authorId": "2303519564",
                    "name": "Yiyuan Yang"
                },
                {
                    "authorId": "1752776622",
                    "name": "Fanxing Liu"
                },
                {
                    "authorId": "2300283769",
                    "name": "Zepu Wang"
                },
                {
                    "authorId": "2276610490",
                    "name": "Zina Ibrahim"
                },
                {
                    "authorId": "2287498313",
                    "name": "Haoxin Liu"
                },
                {
                    "authorId": "2257259600",
                    "name": "Zhiyuan Zhao"
                },
                {
                    "authorId": "2294921996",
                    "name": "Yingjie Zhou"
                },
                {
                    "authorId": "2283086874",
                    "name": "Wenjia Wang"
                },
                {
                    "authorId": "2307085073",
                    "name": "Kaize Ding"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2266467183",
                    "name": "B. A. Prakash"
                },
                {
                    "authorId": "2306250667",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "ade46150fbb93b4e473f2fafbe39dfbb3346ee94",
            "title": "A Survey on Diffusion Models for Time Series and Spatio-Temporal Data",
            "abstract": "The study of time series is crucial for understanding trends and anomalies over time, enabling predictive insights across various sectors. Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic perspective on complex system interactions. Recently, diffusion models have seen widespread application in time series and spatio-temporal data mining. Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but they also extend to other downstream tasks. In this survey, we comprehensively and thoroughly review the use of diffusion models in time series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain. In detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series and spatio-temporal data separately. Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models, serving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation. Conditioned models, on the other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks. Our survey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and transportation, providing a foundational understanding of how these models analyze and generate data. Through this structured overview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and spatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring innovative solutions within the diffusion model framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2303519564",
                    "name": "Yiyuan Yang"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2262445381",
                    "name": "Haomin Wen"
                },
                {
                    "authorId": "2152737103",
                    "name": "Chaoli Zhang"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2253908414",
                    "name": "Lintao Ma"
                },
                {
                    "authorId": "2259354682",
                    "name": "Yi Wang"
                },
                {
                    "authorId": "2275745358",
                    "name": "Cheng-Ming Liu"
                },
                {
                    "authorId": "2298971233",
                    "name": "Bin Yang"
                },
                {
                    "authorId": "2298925809",
                    "name": "Zenglin Xu"
                },
                {
                    "authorId": "2298896985",
                    "name": "Jiang Bian"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "b66eaa2e5b65ca4cbfbffd1a592cc04a6bd81e91",
            "title": "Cluster-Wide Task Slowdown Detection in Cloud System",
            "abstract": "Slow task detection is a critical problem in cloud operation and maintenance since it is highly related to user experience and can bring substantial liquidated damages. Most anomaly detection methods detect it from a single-task aspect. However, considering millions of concurrent tasks in large-scale cloud computing clusters, it becomes impractical and inefficient. Moreover, single-task slowdowns are very common and do not necessarily indicate a malfunction of a cluster due to its violent fluctuation nature in a virtual environment. Thus, we shift our attention to cluster-wide task slowdowns by utilizing the duration time distribution of tasks across a cluster, so that the computation complexity is not relevant to the number of tasks. The task duration time distribution often exhibits compound periodicity and local exceptional fluctuations over time. Though transformer-based methods are one of the most powerful methods to capture these time series normal variation patterns, we empirically find and theoretically explain the flaw of the standard attention mechanism in reconstructing subperiods with low amplitude when dealing with compound periodicity. To tackle these challenges, we propose SORN (i.e., Skimming Off subperiods in descending amplitude order and Reconstructing Non-slowing fluctuation), which consists of a Skimming Attention mechanism to reconstruct the compound periodicity and a Neural Optimal Transport module to distinguish cluster-wide slowdowns from other exceptional fluctuations. Furthermore, since anomalies in the training set are inevitable in a practical scenario, we propose a picky loss function, which adaptively assigns higher weights to reliable time slots in the training set. Extensive experiments demonstrate that SORN outperforms state-of-the-art methods on multiple real-world industrial datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2247977481",
                    "name": "Feiyi Chen"
                },
                {
                    "authorId": "2257101815",
                    "name": "Yingying Zhang"
                },
                {
                    "authorId": "2262197641",
                    "name": "Lunting Fan"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2262443854",
                    "name": "Guansong Pang"
                },
                {
                    "authorId": "2242692407",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2239865956",
                    "name": "Shuiguang Deng"
                }
            ]
        },
        {
            "paperId": "c30abb2ad76fcbfd4e6dc8881850b591d3434a3e",
            "title": "Foundation Models for Time Series Analysis: A Tutorial and Survey",
            "abstract": "Time series analysis stands as a focal point within the data mining community, serving as a cornerstone for extracting valuable insights crucial to a myriad of real-world applications. Recent advances in Foundation Models (FMs) have fundamentally reshaped the paradigm of model design for time series analysis, boosting various downstream tasks in practice. These innovative approaches often leverage pre-trained or fine-tuned FMs to harness generalized knowledge tailored for time series analysis. This survey aims to furnish a comprehensive and up-to-date overview of FMs for time series analysis. While prior surveys have predominantly focused on either application or pipeline aspects of FMs in time series analysis, they have often lacked an in-depth understanding of the underlying mechanisms that elucidate why and how FMs benefit time series analysis. To address this gap, our survey adopts a methodology-centric classification, delineating various pivotal elements of time-series FMs, including model architectures, pre-training techniques, adaptation methods, and data modalities. Overall, this survey serves to consolidate the latest advancements in FMs pertinent to time series analysis, accentuating their theoretical underpinnings, recent strides in development, and avenues for future exploration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2262445381",
                    "name": "Haomin Wen"
                },
                {
                    "authorId": "2300149940",
                    "name": "Yuqi Nie"
                },
                {
                    "authorId": "2214140574",
                    "name": "Yushan Jiang"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2292142150",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "d622d8b2d5adc4b638a73b105686840e264dfb8f",
            "title": "Position: What Can Large Language Models Tell Us about Time Series Analysis",
            "abstract": "Time series analysis is essential for comprehending the complexities inherent in various realworld systems and applications. Although large language models (LLMs) have recently made significant strides, the development of artificial general intelligence (AGI) equipped with time series analysis capabilities remains in its nascent phase. Most existing time series models heavily rely on domain knowledge and extensive model tuning, predominantly focusing on prediction tasks. In this paper, we argue that current LLMs have the potential to revolutionize time series analysis, thereby promoting efficient decision-making and advancing towards a more universal form of time series analytical intelligence. Such advancement could unlock a wide range of possibilities, including time series modality switching and question answering. We encourage researchers and practitioners to recognize the potential of LLMs in advancing time series analysis and emphasize the need for trust in these related efforts. Furthermore, we detail the seamless integration of time series analysis with existing LLM technologies and outline promising avenues for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2304517120",
                    "name": "Yifan Zhang"
                },
                {
                    "authorId": "2301489384",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2304920924",
                    "name": "Kexin Zhang"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2298971233",
                    "name": "Bin Yang"
                },
                {
                    "authorId": "2304515949",
                    "name": "Jindong Wang"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2303259263",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "db9accd4e74a2c48887d572350b75ba663c679f1",
            "title": "Unlocking the Power of LSTM for Long Term Time Series Forecasting",
            "abstract": "Traditional recurrent neural network architectures, such as long short-term memory neural networks (LSTM), have historically held a prominent role in time series forecasting (TSF) tasks. While the recently introduced sLSTM for Natural Language Processing (NLP) introduces exponential gating and memory mixing that are beneficial for long term sequential learning, its potential short memory issue is a barrier to applying sLSTM directly in TSF. To address this, we propose a simple yet efficient algorithm named P-sLSTM, which is built upon sLSTM by incorporating patching and channel independence. These modifications substantially enhance sLSTM's performance in TSF, achieving state-of-the-art results. Furthermore, we provide theoretical justifications for our design, and conduct extensive comparative and analytical experiments to fully validate the efficiency and superior performance of our model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307607703",
                    "name": "Yaxuan Kong"
                },
                {
                    "authorId": "2300283769",
                    "name": "Zepu Wang"
                },
                {
                    "authorId": "2300149940",
                    "name": "Yuqi Nie"
                },
                {
                    "authorId": "2316586016",
                    "name": "Tian Zhou"
                },
                {
                    "authorId": "2279077978",
                    "name": "Stefan Zohren"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2216789869",
                    "name": "Peng Sun"
                },
                {
                    "authorId": "2307076087",
                    "name": "Qingsong Wen"
                }
            ]
        },
        {
            "paperId": "de7f235452bec5304290f6faa54b9ff36a27e592",
            "title": "Time-SSM: Simplifying and Unifying State Space Models for Time Series Forecasting",
            "abstract": "State Space Models (SSMs) have emerged as a potent tool in sequence modeling tasks in recent years. These models approximate continuous systems using a set of basis functions and discretize them to handle input data, making them well-suited for modeling time series data collected at specific frequencies from continuous systems. Despite its potential, the application of SSMs in time series forecasting remains underexplored, with most existing models treating SSMs as a black box for capturing temporal or channel dependencies. To address this gap, this paper proposes a novel theoretical framework termed Dynamic Spectral Operator, offering more intuitive and general guidance on applying SSMs to time series data. Building upon our theory, we introduce Time-SSM, a novel SSM-based foundation model with only one-seventh of the parameters compared to Mamba. Various experiments validate both our theoretical framework and the superior performance of Time-SSM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284730172",
                    "name": "Jiaxi Hu"
                },
                {
                    "authorId": "2303407868",
                    "name": "Disen Lan"
                },
                {
                    "authorId": "2303418753",
                    "name": "Ziyu Zhou"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                }
            ]
        },
        {
            "paperId": "f3ca1515d610ec8811d3b4f9a209eae099523048",
            "title": "Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective",
            "abstract": "In long-term time series forecasting (LTSF) tasks, an increasing number of models have acknowledged that discrete time series originate from continuous dynamic systems and have attempted to model their dynamical structures. Recognizing the chaotic nature of real-world data, our model, \\textbf{\\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes non-parametric Phase Space Reconstruction embedding and the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets with only one-twelfth of the parameters compared to PatchTST.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2284730172",
                    "name": "Jiaxi Hu"
                },
                {
                    "authorId": "2284735864",
                    "name": "Yuehong Hu"
                },
                {
                    "authorId": "2262453292",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                }
            ]
        }
    ]
}