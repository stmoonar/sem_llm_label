{
    "authorId": "35183492",
    "papers": [
        {
            "paperId": "753f76976c3218d4268fa2c9ec4141a144105d2a",
            "title": "Approaches to Corpus Creation for Low-Resource Language Technology: the Case of Southern Kurdish and Laki",
            "abstract": "One of the major challenges that under-represented and endangered language communities face in language technology is the lack or paucity of language data. This is also the case of the Southern varieties of the Kurdish and Laki languages for which very limited resources are available with insubstantial progress in tools. To tackle this, we provide a few approaches that rely on the content of local news websites, a local radio station that broadcasts content in Southern Kurdish and fieldwork for Laki. In this paper, we describe some of the challenges of such under-represented languages, particularly in writing and standardization, and also, in retrieving sources of data and retro-digitizing handwritten content to create a corpus for Southern Kurdish and Laki. In addition, we study the task of language identification in light of the other variants of Kurdish and Zaza-Gorani languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35183492",
                    "name": "Sina Ahmadi"
                },
                {
                    "authorId": "114874509",
                    "name": "Zahra Azin"
                },
                {
                    "authorId": "2099186118",
                    "name": "Sara Belelli"
                },
                {
                    "authorId": "49513989",
                    "name": "Antonios Anastasopoulos"
                }
            ]
        },
        {
            "paperId": "7f29d01468002a9eabe0eed79793a8c95ba2867d",
            "title": "CODET: A Benchmark for Contrastive Dialectal Evaluation of Machine Translation",
            "abstract": "Neural machine translation (NMT) systems exhibit limited robustness in handling source-side linguistic variations. Their performance tends to degrade when faced with even slight deviations in language usage, such as different domains or variations introduced by second-language speakers. It is intuitive to extend this observation to encompass dialectal variations as well, but the work allowing the community to evaluate MT systems on this dimension is limited. To alleviate this issue, we compile and release CODET, a contrastive dialectal benchmark encompassing 891 different variations from twelve different languages. We also quantitatively demonstrate the challenges large MT models face in effectively translating dialectal variants. All the data and code have been released.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2007788331",
                    "name": "Md Mahfuz Ibn Alam"
                },
                {
                    "authorId": "35183492",
                    "name": "Sina Ahmadi"
                },
                {
                    "authorId": "49513989",
                    "name": "Antonios Anastasopoulos"
                }
            ]
        },
        {
            "paperId": "9243f5197fe44b7a92c154dab21460c3b994fc9a",
            "title": "Transfer Learning for Low-Resource Sentiment Analysis",
            "abstract": "Sentiment analysis is the process of identifying and extracting subjective information from text. Despite the advances to employ cross-lingual approaches in an automatic way, the implementation and evaluation of sentiment analysis systems require language-specific data to consider various sociocultural and linguistic peculiarities. In this paper, the collection and annotation of a dataset are described for sentiment analysis of Central Kurdish. We explore a few classical machine learning and neural network-based techniques for this task. Additionally, we employ an approach in transfer learning to leverage pretrained models for data augmentation. We demonstrate that data augmentation achieves a high F$_1$ score and accuracy despite the difficulty of the task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2214034176",
                    "name": "Razhan Hameed"
                },
                {
                    "authorId": "35183492",
                    "name": "Sina Ahmadi"
                },
                {
                    "authorId": "3146660",
                    "name": "Fatemeh Daneshfar"
                }
            ]
        },
        {
            "paperId": "a39aca15cd61b72820a3a30d894bf9d254bfafd0",
            "title": "Revisiting and Amending Central Kurdish Data on UniMorph 4.0",
            "abstract": "UniMorph\u2013the Universal Morphology project is a collaborative initiative to create and maintain morphological data and organize numerous related tasks for various language processing communities. The morphological data is provided by linguists for over 160 languages in the latest version of UniMorph 4.0. This paper sheds light on the Central Kurdish data on UniMorph 4.0 by analyzing the existing data, its fallacies, and systematic morphological errors. It also presents an approach to creating more reliable morphological data by considering various specific phenomena in Central Kurdish that have not been addressed previously, such as Izafe and several enclitics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35183492",
                    "name": "Sina Ahmadi"
                },
                {
                    "authorId": "2050165274",
                    "name": "A. Mahmudi"
                }
            ]
        },
        {
            "paperId": "a6a29fd7f9aaa8eb446fd857181ef65f19cf602c",
            "title": "PALI: A Language Identification Benchmark for Perso-Arabic Scripts",
            "abstract": "The Perso-Arabic scripts are a family of scripts that are widely adopted and used by various linguistic communities around the globe. Identifying various languages using such scripts is crucial to language technologies and challenging in low-resource setups. As such, this paper sheds light on the challenges of detecting languages using Perso-Arabic scripts, especially in bilingual communities where \u201cunconventional\u201d writing is practiced. To address this, we use a set of supervised techniques to classify sentences into their languages. Building on these, we also propose a hierarchical model that targets clusters of languages that are more often confused by the classifiers. Our experiment results indicate the effectiveness of our solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35183492",
                    "name": "Sina Ahmadi"
                },
                {
                    "authorId": "121465663",
                    "name": "M. Agarwal"
                },
                {
                    "authorId": "49513989",
                    "name": "Antonios Anastasopoulos"
                }
            ]
        },
        {
            "paperId": "f5384bfa8806a4274dcfc72cdbc774746de7965b",
            "title": "Script Normalization for Unconventional Writing of Under-Resourced Languages in Bilingual Communities",
            "abstract": "The wide accessibility of social media has provided linguistically under-represented communities with an extraordinary opportunity to create content in their native languages. This, however, comes with certain challenges in script normalization, particularly where the speakers of a language in a bilingual community rely on another script or orthography to write their native language. This paper addresses the problem of script normalization for several such languages that are mainly written in a Perso-Arabic script. Using synthetic data with various levels of noise and a transformer-based model, we demonstrate that the problem can be effectively remediated. We conduct a small-scale evaluation of real data as well. Our experiments indicate that script normalization is also beneficial to improve the performance of downstream tasks such as machine translation and language identification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35183492",
                    "name": "Sina Ahmadi"
                },
                {
                    "authorId": "49513989",
                    "name": "Antonios Anastasopoulos"
                }
            ]
        },
        {
            "paperId": "03f487649f57177a33a61c79ad2dd74d8022f247",
            "title": "Cross-Lingual Link Discovery for Under-Resourced Languages",
            "abstract": "In this paper, we provide an overview of current technologies for cross-lingual link discovery, and we discuss challenges, experiences and prospects of their application to under-resourced languages. We rst introduce the goals of cross-lingual linking and associated technologies, and in particular, the role that the Linked Data paradigm (Bizer et al., 2011) applied to language data can play in this context. We de ne under-resourced languages with a speci c focus on languages actively used on the internet, i.e., languages with a digitally versatile speaker community, but limited support in terms of language technology. We argue that languages for which considerable amounts of textual data and (at least) a bilingual word list are available, techniques for cross-lingual linking can be readily applied, and that these enable the implementation of downstream applications for under-resourced languages via the localisation and adaptation of existing technologies and resources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35041118",
                    "name": "M. Rosner"
                },
                {
                    "authorId": "35183492",
                    "name": "Sina Ahmadi"
                },
                {
                    "authorId": "3202565",
                    "name": "E. Apostol"
                },
                {
                    "authorId": "1409496551",
                    "name": "Julia Bosque-Gil"
                },
                {
                    "authorId": "1723161",
                    "name": "C. Chiarcos"
                },
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "2598237",
                    "name": "K. Gkirtzou"
                },
                {
                    "authorId": "143708147",
                    "name": "J. Gracia"
                },
                {
                    "authorId": "2640975",
                    "name": "Dagmar Gromann"
                },
                {
                    "authorId": "2601200",
                    "name": "Chaya Liebeskind"
                },
                {
                    "authorId": "118562165",
                    "name": "G. Ole\u0161kevi\u010dien\u0117"
                },
                {
                    "authorId": "1753940",
                    "name": "Gilles S\u00e9rasset"
                },
                {
                    "authorId": "39812162",
                    "name": "Ciprian-Octavian Truic\u0103"
                }
            ]
        },
        {
            "paperId": "656665216d18440e9908e4be8684983bff192981",
            "title": "Toward an Integrative Approach for Making Sense Distinctions",
            "abstract": "Word senses are the fundamental unit of description in lexicography, yet it is rarely the case that different dictionaries reach any agreement on the number and definition of senses in a language. With the recent rise in natural language processing and other computational approaches there is an increasing demand for quantitatively validated sense catalogues of words, yet no consensus methodology exists. In this paper, we look at four main approaches to making sense distinctions: formal, cognitive, distributional, and intercultural and examine the strengths and weaknesses of each approach. We then consider how these may be combined into a single sound methodology. We illustrate this by examining two English words, \u201cwing\u201d and \u201cfish,\u201d using existing resources for each of these four approaches and illustrate the weaknesses of each. We then look at the impact of such an integrated method and provide some future perspectives on the research that is necessary to reach a principled method for making sense distinctions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1689974",
                    "name": "John P. McCrae"
                },
                {
                    "authorId": "1515519789",
                    "name": "Theodorus Fransen"
                },
                {
                    "authorId": "35183492",
                    "name": "Sina Ahmadi"
                },
                {
                    "authorId": "3338131",
                    "name": "P. Buitelaar"
                },
                {
                    "authorId": "120873790",
                    "name": "Koustava Goswami"
                }
            ]
        },
        {
            "paperId": "fd2317bc0b6c6f79c0e5b5dd07c771c11e12b512",
            "title": "CoFiF Plus: A French Financial Narrative Summarisation Corpus",
            "abstract": "Natural Language Processing is increasingly being applied in the finance and business industry to analyse the text of many different types of financial documents. Given the increasing growth of firms around the world, the volume of financial disclosures and financial texts in different languages and forms is increasing sharply and therefore the study of language technology methods that automatically summarise content has grown rapidly into a major research area. Corpora for financial narrative summarisation exists in English, but there is a significant lack of financial text resources in the French language. To remedy this, we present CoFiF Plus, the first French financial narrative summarisation dataset providing a comprehensive set of financial text written in French. The dataset has been extracted from French financial reports published in PDF file format. It is composed of 1,703 reports from the most capitalised companies in France (Euronext Paris) covering a time frame from 1995 to 2021. This paper describes the collection, annotation and validation of the financial reports and their summaries. It also describes the dataset and gives the results of some baseline summarisers. Our datasets will be openly available upon the acceptance of the paper.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Nadhem Zmandar"
                },
                {
                    "authorId": "3467058",
                    "name": "Tobias Daudert"
                },
                {
                    "authorId": "35183492",
                    "name": "Sina Ahmadi"
                },
                {
                    "authorId": "1401874424",
                    "name": "Mahmoud El-Haj"
                },
                {
                    "authorId": "1929390",
                    "name": "Paul Rayson"
                }
            ]
        },
        {
            "paperId": "fe08126650f2fefd1e1a4846301f17ddf8a46791",
            "title": "Monolingual alignment of word senses and definitions in lexicographical resources",
            "abstract": "The focus of this thesis is broadly on the alignment of lexicographical data, particularly dictionaries. In order to tackle some of the challenges in this field, two main tasks of word sense alignment and translation inference are addressed. The first task aims to find an optimal alignment given the sense definitions of a headword in two different monolingual dictionaries. This is a challenging task, especially due to differences in sense granularity, coverage and description in two resources. After describing the characteristics of various lexical semantic resources, we introduce a benchmark containing 17 datasets of 15 languages where monolingual word senses and definitions are manually annotated across different resources by experts. In the creation of the benchmark, lexicographers' knowledge is incorporated through the annotations where a semantic relation, namely exact, narrower, broader, related or none, is selected for each sense pair. This benchmark can be used for evaluation purposes of word-sense alignment systems. The performance of a few alignment techniques based on textual and non-textual semantic similarity detection and semantic relation induction is evaluated using the benchmark. Finally, we extend this work to translation inference where translation pairs are induced to generate bilingual lexicons in an unsupervised way using various approaches based on graph analysis. This task is of particular interest for the creation of lexicographical resources for less-resourced and under-represented languages and also, assists in increasing coverage of the existing resources. From a practical point of view, the techniques and methods that are developed in this thesis are implemented within a tool that can facilitate the alignment task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35183492",
                    "name": "Sina Ahmadi"
                }
            ]
        }
    ]
}