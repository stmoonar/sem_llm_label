{
    "authorId": "2274067913",
    "papers": [
        {
            "paperId": "2902a67f9aebb115fc2b6cdf611910e72e896bdd",
            "title": "GOODAT: Towards Test-time Graph Out-of-Distribution Detection",
            "abstract": "Graph neural networks (GNNs) have found widespread application in modeling graph data across diverse domains. While GNNs excel in scenarios where the testing data shares the distribution of their training counterparts (in distribution, ID), they often exhibit incorrect predictions when confronted with samples from an unfamiliar distribution (out-of-distribution, OOD). To identify and reject OOD samples with GNNs, recent studies have explored graph OOD detection, often focusing on training a specific model or modifying the data on top of a well-trained GNN. Despite their effectiveness, these methods come with heavy training resources and costs, as they need to optimize the GNN-based models on training data. Moreover, their reliance on modifying the original GNNs and accessing training data further restricts their universality. To this end, this paper introduces a method to detect Graph Out-of-Distribution At Test-time (namely GOODAT), a data-centric, unsupervised, and plug-and-play solution that operates independently of training data and modifications of GNN architecture. With a lightweight graph masker, GOODAT can learn informative subgraphs from test samples, enabling the capture of distinct graph patterns between OOD and ID samples. To optimize the graph masker, we meticulously design three unsupervised objective functions based on the graph information bottleneck principle, motivating the masker to capture compact yet informative subgraphs for OOD detection. Comprehensive evaluations confirm that our GOODAT method outperforms state-of-the-art benchmarks across a variety of real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2278588129",
                    "name": "Luzhi Wang"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2242962967",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "49336556",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2277958230",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "4a339e9e6b8879f785e9af607517c17b31a06154",
            "title": "Gradient Transformation: Towards Efficient and Model-Agnostic Unlearning for Dynamic Graph Neural Networks",
            "abstract": "Graph unlearning has emerged as an essential tool for safeguarding user privacy and mitigating the negative impacts of undesirable data. Meanwhile, the advent of dynamic graph neural networks (DGNNs) marks a significant advancement due to their superior capability in learning from dynamic graphs, which encapsulate spatial-temporal variations in diverse real-world applications (e.g., traffic forecasting). With the increasing prevalence of DGNNs, it becomes imperative to investigate the implementation of dynamic graph unlearning. However, current graph unlearning methodologies are designed for GNNs operating on static graphs and exhibit limitations including their serving in a pre-processing manner and impractical resource demands. Furthermore, the adaptation of these methods to DGNNs presents non-trivial challenges, owing to the distinctive nature of dynamic graphs. To this end, we propose an effective, efficient, model-agnostic, and post-processing method to implement DGNN unlearning. Specifically, we first define the unlearning requests and formulate dynamic graph unlearning in the context of continuous-time dynamic graphs. After conducting a role analysis on the unlearning data, the remaining data, and the target DGNN model, we propose a method called Gradient Transformation and a loss function to map the unlearning request to the desired parameter update. Evaluations on six real-world datasets and state-of-the-art DGNN backbones demonstrate its effectiveness (e.g., limited performance drop even obvious improvement) and efficiency (e.g., at most 7.23$\\times$ speed-up) outperformance, and potential advantages in handling future unlearning requests (e.g., at most 32.59$\\times$ speed-up).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2112166824",
                    "name": "Xiangwen Yang"
                },
                {
                    "authorId": "2274650211",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2302859288",
                    "name": "Chengqi Zhang"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "6471973e16a2c314deddb8867d6d9fdb01f2af6a",
            "title": "Online GNN Evaluation Under Test-time Graph Distribution Shifts",
            "abstract": "Evaluating the performance of a well-trained GNN model on real-world graphs is a pivotal step for reliable GNN online deployment and serving. Due to a lack of test node labels and unknown potential training-test graph data distribution shifts, conventional model evaluation encounters limitations in calculating performance metrics (e.g., test error) and measuring graph data-level discrepancies, particularly when the training graph used for developing GNNs remains unobserved during test time. In this paper, we study a new research problem, online GNN evaluation, which aims to provide valuable insights into the well-trained GNNs's ability to effectively generalize to real-world unlabeled graphs under the test-time graph distribution shifts. Concretely, we develop an effective learning behavior discrepancy score, dubbed LeBeD, to estimate the test-time generalization errors of well-trained GNN models. Through a novel GNN re-training strategy with a parameter-free optimality criterion, the proposed LeBeD comprehensively integrates learning behavior discrepancies from both node prediction and structure reconstruction perspectives. This enables the effective evaluation of the well-trained GNNs' ability to capture test node semantics and structural representations, making it an expressive metric for estimating the generalization error in online GNN evaluation. Extensive experiments on real-world test graphs under diverse graph distribution shifts could verify the effectiveness of the proposed method, revealing its strong correlation with ground-truth test errors on various well-trained GNN models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261358112",
                    "name": "Xin Zheng"
                },
                {
                    "authorId": "2292142150",
                    "name": "Dongjin Song"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                },
                {
                    "authorId": "2291965209",
                    "name": "Bo Du"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "4fca71ebaeb89a5918bfd944d0645eec50eb647b",
            "title": "On the Interaction between Node Fairness and Edge Privacy in Graph Neural Networks",
            "abstract": "Due to the emergence of graph neural networks (GNNs) and their widespread implementation in real-world scenarios, the fairness and privacy of GNNs have attracted considerable interest since they are two essential social concerns in the era of building trustworthy GNNs. Existing studies have respectively explored the fairness and privacy of GNNs and exhibited that both fairness and privacy are at the cost of GNN performance. However, the interaction between them is yet to be explored and understood. In this paper, we investigate the interaction between the fairness of a GNN and its privacy for the \ufb01rst time. We empirically identify that edge privacy risks increase when the individual fairness of nodes is improved. Next, we present the intuition behind such a trade-off and employ the in\ufb02uence function and Pearson correlation to measure it theoretically. To take the performance, fairness, and privacy of GNNs into account simultaneously, we propose implementing fairness-aware re-weighting and privacy-aware graph structure perturbation modules in a retraining mechanism. Experimental results demonstrate that our method is effective in implementing GNN fairness with limited performance cost and restricted privacy risks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2274650211",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2300198381",
                    "name": "Quoc Viet Hung Nguyen"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "80568b425b09e53debcfd23929ba68595874c8de",
            "title": "GNNEvaluator: Evaluating GNN Performance On Unseen Graphs Without Labels",
            "abstract": "Evaluating the performance of graph neural networks (GNNs) is an essential task for practical GNN model deployment and serving, as deployed GNNs face significant performance uncertainty when inferring on unseen and unlabeled test graphs, due to mismatched training-test graph distributions. In this paper, we study a new problem, GNN model evaluation, that aims to assess the performance of a specific GNN model trained on labeled and observed graphs, by precisely estimating its performance (e.g., node classification accuracy) on unseen graphs without labels. Concretely, we propose a two-stage GNN model evaluation framework, including (1) DiscGraph set construction and (2) GNNEvaluator training and inference. The DiscGraph set captures wide-range and diverse graph data distribution discrepancies through a discrepancy measurement function, which exploits the outputs of GNNs related to latent node embeddings and node class predictions. Under the effective training supervision from the DiscGraph set, GNNEvaluator learns to precisely estimate node classification accuracy of the to-be-evaluated GNN model and makes an accurate inference for evaluating GNN model performance. Extensive experiments on real-world unseen and unlabeled test graphs demonstrate the effectiveness of our proposed method for GNN model evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261358112",
                    "name": "Xin Zheng"
                },
                {
                    "authorId": "2261360957",
                    "name": "Miao Zhang"
                },
                {
                    "authorId": "2143946417",
                    "name": "C. Chen"
                },
                {
                    "authorId": "5443435",
                    "name": "Soheila Molaei"
                },
                {
                    "authorId": "2261453327",
                    "name": "Chuan Zhou"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "93b5f8111957a5b2ba36e1b4377d49b0309f3ae3",
            "title": "GraphGuard: Detecting and Counteracting Training Data Misuse in Graph Neural Networks",
            "abstract": "The emergence of Graph Neural Networks (GNNs) in graph data analysis and their deployment on Machine Learning as a Service platforms have raised critical concerns about data misuse during model training. This situation is further exacerbated due to the lack of transparency in local training processes, potentially leading to the unauthorized accumulation of large volumes of graph data, thereby infringing on the intellectual property rights of data owners. Existing methodologies often address either data misuse detection or mitigation, and are primarily designed for local GNN models rather than cloud-based MLaaS platforms. These limitations call for an effective and comprehensive solution that detects and mitigates data misuse without requiring exact training data while respecting the proprietary nature of such data. This paper introduces a pioneering approach called GraphGuard, to tackle these challenges. We propose a training-data-free method that not only detects graph data misuse but also mitigates its impact via targeted unlearning, all without relying on the original training data. Our innovative misuse detection technique employs membership inference with radioactive data, enhancing the distinguishability between member and non-member data distributions. For mitigation, we utilize synthetic graphs that emulate the characteristics previously learned by the target model, enabling effective unlearning even in the absence of exact graph data. We conduct comprehensive experiments utilizing four real-world graph datasets to demonstrate the efficacy of GraphGuard in both detection and unlearning. We show that GraphGuard attains a near-perfect detection rate of approximately 100% across these datasets with various GNN models. In addition, it performs unlearning by eliminating the impact of the unlearned graph with a marginal decrease in accuracy (less than 5%).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2112166824",
                    "name": "Xiangwen Yang"
                },
                {
                    "authorId": "2274079627",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2273925304",
                    "name": "Minhui Xue"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2274650211",
                    "name": "Xingliang Yuan"
                }
            ]
        },
        {
            "paperId": "c8c865abf67950ab6a8c7ea042861fb05ae97d27",
            "title": "Securing Graph Neural Networks in MLaaS: A Comprehensive Realization of Query-based Integrity Verification",
            "abstract": "The deployment of Graph Neural Networks (GNNs) within Machine Learning as a Service (MLaaS) has opened up new attack surfaces and an escalation in security concerns regarding model-centric attacks. These attacks can directly manipulate the GNN model parameters during serving, causing incorrect predictions and posing substantial threats to essential GNN applications. Traditional integrity verification methods falter in this context due to the limitations imposed by MLaaS and the distinct characteristics of GNN models.In this research, we introduce a groundbreaking approach to protect GNN models in MLaaS from model-centric attacks. Our approach includes a comprehensive verification schema for GNN\u2019s integrity, taking into account both transductive and inductive GNNs, and accommodating varying pre-deployment knowledge of the models. We propose a query-based verification technique, fortified with innovative node fingerprint generation algorithms. To deal with advanced attackers who know our mechanisms in advance, we introduce randomized fingerprint nodes within our design. The experimental evaluation demonstrates that our method can detect five representative adversarial model-centric attacks, displaying 2 to 4 times greater efficiency compared to baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2274650211",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2274079627",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2274083090",
                    "name": "Qi Li"
                },
                {
                    "authorId": "2273925304",
                    "name": "Minhui Xue"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                }
            ]
        }
    ]
}