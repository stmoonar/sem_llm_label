{
    "authorId": "1713097",
    "papers": [
        {
            "paperId": "b9f20ca9aa392714876bcede05fe976e1d584d68",
            "title": "Loss Attitude Aware Energy Management for Signal Detection",
            "abstract": "This work considers a Bayesian signal processing problem where increasing the power of the probing signal may cause risks or undesired consequences. We employ a market based approach to solve energy management problems for signal detection while balancing multiple objectives. In particular, the optimal amount of resource consumption is determined so as to maximize a profit-loss based expected utility function. Next, we study the human behavior of resource consumption while taking individuals' behavioral disparity into account. Unlike rational decision makers who consume the amount of resource to maximize the expected utility function, human decision makers act to maximize their subjective utilities. We employ prospect theory to model humans' loss aversion towards a risky event. The amount of resource consumption that maximizes the humans' subjective utility is derived to characterize the actual behavior of humans. It is shown that loss attitudes may lead the human to behave quite differently from a rational decision maker.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144176883",
                    "name": "Baocheng Geng"
                },
                {
                    "authorId": "2058865328",
                    "name": "Chen Quan"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "1925309",
                    "name": "P. Varshney"
                }
            ]
        },
        {
            "paperId": "cc5e7bf0432543d842badab51b002fcee913c406",
            "title": "AdverSparse: An Adversarial Attack Framework for Deep Spatial-Temporal Graph Neural Networks",
            "abstract": "Spatial-temporal graph have been widely observed in various domains such as neuroscience, climate research, and transportation engineering. The state-of-the-art models of spatialtemporal graphs rely on Graph Neural Networks (GNNs) to obtain explicit representations for such networks and to discover hidden spatial dependencies in them. These models have demonstrated superior performance in various tasks. In this paper, we propose a sparse adversarial attack framework AdverSparse to illustrate that when only a few key connections are removed in such graphs, hidden spatial dependencies learned by such spatial-temporal models are significantly impacted, leading to various issues such as increasing prediction errors. We formulate the adversarial attack as an optimization problem and solve it by the Alternating Direction Method of Multipliers (ADMM). Experiments show that AdverSparse can find and remove key connections in these graphs, leading to malfunctioning models, even in models capable of learning hidden spatial dependencies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46277026",
                    "name": "Jiayu Li"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "28044622",
                    "name": "Shengmin Jin"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "2281410",
                    "name": "R. Zafarani"
                }
            ]
        },
        {
            "paperId": "3c40774aedede4605ad4e5ce88484a441e9136f8",
            "title": "Compact Multi-level Sparse Neural Networks with Input Independent Dynamic Rerouting",
            "abstract": "Deep neural networks (DNNs) have shown to provide superb performance in many real life applications, but their large computation cost and storage requirement have prevented them from being deployed to many edge and internet-of-things (IoT) devices. Sparse deep neural networks, whose majority weight parameters are zeros, can substantially reduce the computation complexity and memory consumption of the models. In real-use scenarios, devices may suffer from large fluctuations of the available computation and memory resources under different environment, and the quality of service (QoS) is difficult to maintain due to the long tail inferences with large latency. Facing the real-life challenges, we propose to train a sparse model that supports multiple sparse levels. That is, a hierarchical structure of weights are satisfied such that the locations and the values of the non-zero parameters of the more-sparse sub-model are a subset of the less-sparse sub-model. In this way, one can dynamically select the appropriate sparsity level during inference, while the storage cost is capped by the least sparse sub-model. We have verified our methodologies on a variety of DNN models and tasks, including the ResNet-50, PointNet++, GNMT, and graph attention networks. We obtain sparse sub-models with an average of 13.38% weights and 14.97% FLOPs, while the accuracies are as good as their dense counterparts. More-sparse sub-models with 5.38% weights and 4.47% of FLOPs, which are subsets of the less-sparse ones, can be obtained with only 3.25% relative accuracy loss. In addition, our proposed hierarchical model structure supports the mechanism to inference the first part of the model with less sparsity, and dynamically reroute to the more-sparse level if the real-time latency constraint is estimated to be violated. Preliminary analysis shows that we can improve the QoS by one or two nines depending on the task and the computation-memory resources of the inference engine.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39449475",
                    "name": "Minghai Qin"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "2075373569",
                    "name": "Fei Sun"
                },
                {
                    "authorId": "123331823",
                    "name": "Yen-kuang Chen"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                },
                {
                    "authorId": "2154871317",
                    "name": "Yuan Xie"
                }
            ]
        },
        {
            "paperId": "55f2d654de4cc3822549c1f6335e5316b99d7765",
            "title": "StructADMM: Achieving Ultrahigh Efficiency in Structured Pruning for DNNs",
            "abstract": "Weight pruning methods of deep neural networks (DNNs) have been demonstrated to achieve a good model pruning rate without loss of accuracy, thereby alleviating the significant computation/storage requirements of large-scale DNNs. Structured weight pruning methods have been proposed to overcome the limitation of irregular network structure and demonstrated actual GPU acceleration. However, in prior work, the pruning rate (degree of sparsity) and GPU acceleration are limited (to less than 50%) when accuracy needs to be maintained. In this work, we overcome these limitations by proposing a unified, systematic framework of structured weight pruning for DNNs. It is a framework that can be used to induce different types of structured sparsity, such as filterwise, channelwise, and shapewise sparsity, as well as nonstructured sparsity. The proposed framework incorporates stochastic gradient descent (SGD; or ADAM) with alternating direction method of multipliers (ADMM) and can be understood as a dynamic regularization method in which the regularization target is analytically updated in each iteration. Leveraging special characteristics of ADMM, we further propose a progressive, multistep weight pruning framework and a network purification and unused path removal procedure, in order to achieve higher pruning rate without accuracy loss. Without loss of accuracy on the AlexNet model, we achieve <inline-formula> <tex-math notation=\"LaTeX\">$2.58\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$3.65\\times $ </tex-math></inline-formula> average measured speedup on two GPUs, clearly outperforming the prior work. The average speedups reach <inline-formula> <tex-math notation=\"LaTeX\">$3.15\\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=\"LaTeX\">$8.52\\times $ </tex-math></inline-formula> when allowing a moderate accuracy loss of 2%. In this case, the model compression for convolutional layers is <inline-formula> <tex-math notation=\"LaTeX\">$15.0\\times $ </tex-math></inline-formula>, corresponding to <inline-formula> <tex-math notation=\"LaTeX\">$11.93\\times $ </tex-math></inline-formula> measured CPU speedup. As another example, for the ResNet-18 model on the CIFAR-10 data set, we achieve an unprecedented <inline-formula> <tex-math notation=\"LaTeX\">$54.2\\times $ </tex-math></inline-formula> structured pruning rate on CONV layers. This is <inline-formula> <tex-math notation=\"LaTeX\">$32\\times $ </tex-math></inline-formula> higher pruning rate compared with recent work and can further translate into <inline-formula> <tex-math notation=\"LaTeX\">$7.6\\times $ </tex-math></inline-formula> inference time speedup on the Adreno 640 mobile GPU compared with the original, unpruned DNN model. We share our codes and models at the link <uri>http://bit.ly/2M0V7DO</uri>.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "35539001",
                    "name": "Shaokai Ye"
                },
                {
                    "authorId": "1992710",
                    "name": "Xiaoyu Feng"
                },
                {
                    "authorId": "151480882",
                    "name": "Xiaolong Ma"
                },
                {
                    "authorId": "145054784",
                    "name": "Kaiqi Zhang"
                },
                {
                    "authorId": "48459506",
                    "name": "Z. Li"
                },
                {
                    "authorId": "2115854503",
                    "name": "Jian Tang"
                },
                {
                    "authorId": "143743061",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "1662772707",
                    "name": "Xue Lin"
                },
                {
                    "authorId": "2442306",
                    "name": "Yongpan Liu"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                }
            ]
        },
        {
            "paperId": "d30d0b3266e80c49c6cc2f279d29d06f99ab4ca9",
            "title": "A Unified DNN Weight Pruning Framework Using Reweighted Optimization Methods",
            "abstract": "To address the large model size and intensive computation requirement of deep neural networks (DNNs), weight pruning techniques have been proposed and generally fall into two categories, i.e., static regularization-based pruning and dynamic regularization-based pruning. However, the former method currently suffers either complex workloads or accuracy degradation, while the latter one takes a long time to tune the parameters to achieve the desired pruning rate without accuracy loss. In this paper, we propose a unified DNN weight pruning framework with dynamically updated regularization terms bounded by the designated constraint. Our proposed method increases the compression rate, reduces the training time and reduces the number of hyper-parameters compared with state-of-the-art ADMM-based hard constraint method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "151480882",
                    "name": "Xiaolong Ma"
                },
                {
                    "authorId": "2949135",
                    "name": "Zheng Zhan"
                },
                {
                    "authorId": "3147635",
                    "name": "Shangli Zhou"
                },
                {
                    "authorId": "2881873",
                    "name": "Caiwen Ding"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                }
            ]
        },
        {
            "paperId": "52a8da61c0ef2cc1fc7dd3d920c20d153e6fdd73",
            "title": "A Unified DNN Weight Compression Framework Using Reweighted Optimization Methods",
            "abstract": "To address the large model size and intensive computation requirement of deep neural networks (DNNs), weight pruning techniques have been proposed and generally fall into two categories, i.e., static regularization-based pruning and dynamic regularization-based pruning. However, the former method currently suffers either complex workloads or accuracy degradation, while the latter one takes a long time to tune the parameters to achieve the desired pruning rate without accuracy loss. In this paper, we propose a unified DNN weight pruning framework with dynamically updated regularization terms bounded by the designated constraint, which can generate both non-structured sparsity and different kinds of structured sparsity. We also extend our method to an integrated framework for the combination of different DNN compression tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "151480882",
                    "name": "Xiaolong Ma"
                },
                {
                    "authorId": "2949135",
                    "name": "Zheng Zhan"
                },
                {
                    "authorId": "3147635",
                    "name": "Shangli Zhou"
                },
                {
                    "authorId": "39449475",
                    "name": "Minghai Qin"
                },
                {
                    "authorId": "2075373569",
                    "name": "Fei Sun"
                },
                {
                    "authorId": "123331823",
                    "name": "Yen-kuang Chen"
                },
                {
                    "authorId": "2881873",
                    "name": "Caiwen Ding"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                }
            ]
        },
        {
            "paperId": "6c38fa72ee53d6ff6ec8a68875222f795863756c",
            "title": "On the Optimal Interdiction of Transportation Networks",
            "abstract": "We consider the optimal interdiction problem in transportation networks as a game in which an attacker acts as the player who goes first and, subject to budget constraints, fails nodes (partially or fully) at time zero so as to maximize the total travel time of the mass. A centralized network operator then acts as the player who goes second and, subject to the system's dynamics, routes the mass so as to minimize its total travel time. We prove that the attacker's best action is to find the most consequential nodes and employ his resources to fail them fully, so that the optimal attack is both sparse and binary. We then propose an algorithm to numerically solve the optimal interdiction problem, and demonstrate the utility of our approach through illustrative examples.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                }
            ]
        },
        {
            "paperId": "01030a3390dc1a19c9768e038bbe7e40ed4efdbd",
            "title": "Progressive DNN Compression: A Key to Achieve Ultra-High Weight Pruning and Quantization Rates using ADMM",
            "abstract": "Weight pruning and weight quantization are two important categories of DNN model compression. Prior work on these techniques are mainly based on heuristics. A recent work developed a systematic frame-work of DNN weight pruning using the advanced optimization technique ADMM (Alternating Direction Methods of Multipliers), achieving one of state-of-art in weight pruning results. In this work, we first extend such one-shot ADMM-based framework to guarantee solution feasibility and provide fast convergence rate, and generalize to weight quantization as well. We have further developed a multi-step, progressive DNN weight pruning and quantization framework, with dual benefits of (i) achieving further weight pruning/quantization thanks to the special property of ADMM regularization, and (ii) reducing the search space within each step. Extensive experimental results demonstrate the superior performance compared with prior work. Some highlights: (i) we achieve 246x,36x, and 8x weight pruning on LeNet-5, AlexNet, and ResNet-50 models, respectively, with (almost) zero accuracy loss; (ii) even a significant 61x weight pruning in AlexNet (ImageNet) results in only minor degradation in actual accuracy compared with prior work; (iii) we are among the first to derive notable weight pruning results for ResNet and MobileNet models; (iv) we derive the first lossless, fully binarized (for all layers) LeNet-5 for MNIST and VGG-16 for CIFAR-10; and (v) we derive the first fully binarized (for all layers) ResNet for ImageNet with reasonable accuracy loss.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35539001",
                    "name": "Shaokai Ye"
                },
                {
                    "authorId": "1992710",
                    "name": "Xiaoyu Feng"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "151480882",
                    "name": "Xiaolong Ma"
                },
                {
                    "authorId": "152386403",
                    "name": "Sheng Lin"
                },
                {
                    "authorId": "48459506",
                    "name": "Z. Li"
                },
                {
                    "authorId": "46321210",
                    "name": "Kaidi Xu"
                },
                {
                    "authorId": "35420329",
                    "name": "Wujie Wen"
                },
                {
                    "authorId": "143743061",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "2115854503",
                    "name": "Jian Tang"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "145282404",
                    "name": "X. Lin"
                },
                {
                    "authorId": "2442306",
                    "name": "Yongpan Liu"
                },
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                }
            ]
        },
        {
            "paperId": "09290b938c9d4bbc7de94bdc3f5acba5cee2d49c",
            "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness",
            "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework. We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110100442",
                    "name": "Jingkang Wang"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "143743061",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "153191489",
                    "name": "Pin-Yu Chen"
                },
                {
                    "authorId": "2292419368",
                    "name": "Jiacen Xu"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "48218911",
                    "name": "B. Li"
                }
            ]
        },
        {
            "paperId": "58e8d4dca8c91e760bd0d55c2f444a2cc3d62977",
            "title": "Generation of Low Distortion Adversarial Attacks via Convex Programming",
            "abstract": "As deep neural networks (DNNs) achieve extraordinary performance in a wide range of tasks, testing their robustness under adversarial attacks becomes paramount. Adversarial attacks, also known as adversarial examples, are used to measure the robustness of DNNs and are generated by incorporating imperceptible perturbations into the input data with the intention of altering a DNN's classification. In prior work in this area, most of the proposed optimization based methods employ gradient descent to find adversarial examples. In this paper, we present an innovative method which generates adversarial examples via convex programming. Our experiment results demonstrate that we can generate adversarial examples with lower distortion and higher transferability than the C&W attack, which is the current state-of-the-art adversarial attack method for DNNs. We achieve 100% attack success rate on both the original undefended models and the adversarially-trained models. Our distortions of the L_inf attack are respectively 31% and 18% lower than the C&W attack for the best case and average case on the CIFAR-10 data set.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "143743061",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                }
            ]
        }
    ]
}