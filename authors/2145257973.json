{
    "authorId": "2145257973",
    "papers": [
        {
            "paperId": "58ae4f0504c40db26c2fc773cc3943651742c20a",
            "title": "DREAM: Combating Concept Drift with Explanatory Detection and Adaptation in Malware Classification",
            "abstract": "Deep learning-based malware classifiers face significant challenges due to concept drift. The rapid evolution of malware, especially with new families, can depress classification accuracy to near-random levels. Previous research has primarily focused on detecting drift samples, relying on expert-led analysis and labeling for model retraining. However, these methods often lack a comprehensive understanding of malware concepts and provide limited guidance for effective drift adaptation, leading to unstable detection performance and high human labeling costs. To address these limitations, we introduce DREAM, a novel system designed to surpass the capabilities of existing drift detectors and to establish an explanatory drift adaptation process. DREAM enhances drift detection through model sensitivity and data autonomy. The detector, trained in a semi-supervised approach, proactively captures malware behavior concepts through classifier feedback. During testing, it utilizes samples generated by the detector itself, eliminating reliance on extensive training data. For drift adaptation, DREAM enlarges human intervention, enabling revisions of malware labels and concept explanations embedded within the detector's latent space. To ensure a comprehensive response to concept drift, it facilitates a coordinated update process for both the classifier and the detector. Our evaluation shows that DREAM can effectively improve the drift detection accuracy and reduce the expert analysis effort in adaptation across different malware datasets and classifiers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2217446811",
                    "name": "Yiling He"
                },
                {
                    "authorId": "2300175757",
                    "name": "Junchi Lei"
                },
                {
                    "authorId": "2300431708",
                    "name": "Zhan Qin"
                },
                {
                    "authorId": "2145257973",
                    "name": "Kui Ren"
                }
            ]
        },
        {
            "paperId": "679b1c81bf676d0389e1d4317bee630f250d3399",
            "title": "A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions",
            "abstract": "Large language models (LLMs), such as GPT series models, have received substantial attention due to their impressive capabilities for generating and understanding human-level language. More recently, LLMs have emerged as an innovative and powerful adjunct in the medical field, transforming traditional practices and heralding a new era of enhanced healthcare services. This survey provides a comprehensive overview of Medical Large Language Models (Med-LLMs), outlining their evolution from general to the medical-specific domain (i.e, Technology and Application), as well as their transformative impact on healthcare (e.g., Trustworthiness and Safety). Concretely, starting from the fundamental history and technology of LLMs, we first delve into the progressive adaptation and refinements of general LLM models in the medical domain, especially emphasizing the advanced algorithms that boost the LLMs' performance in handling complicated medical environments, including clinical reasoning, knowledge graph, retrieval-augmented generation, human alignment, and multi-modal learning. Secondly, we explore the extensive applications of Med-LLMs across domains such as clinical decision support, report generation, and medical education, illustrating their potential to streamline healthcare services and augment patient outcomes. Finally, recognizing the imperative and responsible innovation, we discuss the challenges of ensuring fairness, accountability, privacy, and robustness in Med-LLMs applications. Finally, we conduct a concise discussion for anticipating possible future trajectories of Med-LLMs, identifying avenues for the prudent expansion of Med-LLMs. By consolidating above-mentioned insights, this review seeks to provide a comprehensive investigation of the potential strengths and limitations of Med-LLMs for professionals and researchers, ensuring a responsible landscape in the healthcare setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267013360",
                    "name": "Lei Liu"
                },
                {
                    "authorId": "2215685191",
                    "name": "Xiaoyan Yang"
                },
                {
                    "authorId": "2300175757",
                    "name": "Junchi Lei"
                },
                {
                    "authorId": "2305229149",
                    "name": "Xiaoyang Liu"
                },
                {
                    "authorId": "2180240771",
                    "name": "Yue Shen"
                },
                {
                    "authorId": "2275032826",
                    "name": "Zhiqiang Zhang"
                },
                {
                    "authorId": "2304955019",
                    "name": "Peng Wei"
                },
                {
                    "authorId": "2269769748",
                    "name": "Jinjie Gu"
                },
                {
                    "authorId": "2303254578",
                    "name": "Zhixuan Chu"
                },
                {
                    "authorId": "2279958447",
                    "name": "Zhan Qin"
                },
                {
                    "authorId": "2145257973",
                    "name": "Kui Ren"
                }
            ]
        },
        {
            "paperId": "84215777e48948b806436ae8b57b736b94e26b9e",
            "title": "Explanation as a Watermark: Towards Harmless and Multi-bit Model Ownership Verification via Watermarking Feature Attribution",
            "abstract": "Ownership verification is currently the most critical and widely adopted post-hoc method to safeguard model copyright. In general, model owners exploit it to identify whether a given suspicious third-party model is stolen from them by examining whether it has particular properties `inherited' from their released models. Currently, backdoor-based model watermarks are the primary and cutting-edge methods to implant such properties in the released models. However, backdoor-based methods have two fatal drawbacks, including harmfulness and ambiguity. The former indicates that they introduce maliciously controllable misclassification behaviors ($i.e.$, backdoor) to the watermarked released models. The latter denotes that malicious users can easily pass the verification by finding other misclassified samples, leading to ownership ambiguity. In this paper, we argue that both limitations stem from the `zero-bit' nature of existing watermarking schemes, where they exploit the status ($i.e.$, misclassified) of predictions for verification. Motivated by this understanding, we design a new watermarking paradigm, $i.e.$, Explanation as a Watermark (EaaW), that implants verification behaviors into the explanation of feature attribution instead of model predictions. Specifically, EaaW embeds a `multi-bit' watermark into the feature attribution explanation of specific trigger samples without changing the original prediction. We correspondingly design the watermark embedding and extraction algorithms inspired by explainable artificial intelligence. In particular, our approach can be used for different tasks ($e.g.$, image classification and text generation). Extensive experiments verify the effectiveness and harmlessness of our EaaW and its resistance to potential attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2300286306",
                    "name": "Shuo Shao"
                },
                {
                    "authorId": "2300335251",
                    "name": "Yiming Li"
                },
                {
                    "authorId": "2028819641",
                    "name": "Hongwei Yao"
                },
                {
                    "authorId": "2217446811",
                    "name": "Yiling He"
                },
                {
                    "authorId": "2300431708",
                    "name": "Zhan Qin"
                },
                {
                    "authorId": "2145257973",
                    "name": "Kui Ren"
                }
            ]
        },
        {
            "paperId": "96aa0327c9faeecf8d520286a1b1999b541becb1",
            "title": "LLM-Guided Multi-View Hypergraph Learning for Human-Centric Explainable Recommendation",
            "abstract": "As personalized recommendation systems become vital in the age of information overload, traditional methods relying solely on historical user interactions often fail to fully capture the multifaceted nature of human interests. To enable more human-centric modeling of user preferences, this work proposes a novel explainable recommendation framework, i.e., LLMHG, synergizing the reasoning capabilities of large language models (LLMs) and the structural advantages of hypergraph neural networks. By effectively profiling and interpreting the nuances of individual user interests, our framework pioneers enhancements to recommendation systems with increased explainability. We validate that explicitly accounting for the intricacies of human preferences allows our human-centric and explainable LLMHG approach to consistently outperform conventional models across diverse real-world datasets. The proposed plug-and-play enhancement framework delivers immediate gains in recommendation performance while offering a pathway to apply advanced LLMs for better capturing the complexity of human interests across machine learning applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237992280",
                    "name": "Zhixuan Chu"
                },
                {
                    "authorId": "2205710030",
                    "name": "Yan Wang"
                },
                {
                    "authorId": "2276424293",
                    "name": "Qing Cui"
                },
                {
                    "authorId": "2238177474",
                    "name": "Longfei Li"
                },
                {
                    "authorId": "2279760195",
                    "name": "Wenqing Chen"
                },
                {
                    "authorId": "2262276636",
                    "name": "Sheng Li"
                },
                {
                    "authorId": "2279958447",
                    "name": "Zhan Qin"
                },
                {
                    "authorId": "2145257973",
                    "name": "Kui Ren"
                }
            ]
        },
        {
            "paperId": "a658fd83058ee4727ae41317ec5ced0dd7c2a13c",
            "title": "A Causal Explainable Guardrails for Large Language Models",
            "abstract": "Large Language Models (LLMs) have shown impressive performance in natural language tasks, but their outputs can exhibit undesirable attributes or biases. Existing methods for steering LLMs toward desired attributes often assume unbiased representations and rely solely on steering prompts. However, the representations learned from pre-training can introduce semantic biases that influence the steering process, leading to suboptimal results. We propose LLMGuardrail, a novel framework that incorporates causal analysis and adversarial learning to obtain unbiased steering representations in LLMs. LLMGuardrail systematically identifies and blocks the confounding effects of biases, enabling the extraction of unbiased steering representations. Additionally, it includes an explainable component that provides insights into the alignment between the generated output and the desired direction. Experiments demonstrate LLMGuardrail's effectiveness in steering LLMs toward desired attributes while mitigating biases. Our work contributes to the development of safe and reliable LLMs that align with desired attributes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237992280",
                    "name": "Zhixuan Chu"
                },
                {
                    "authorId": "2205710030",
                    "name": "Yan Wang"
                },
                {
                    "authorId": "2238177474",
                    "name": "Longfei Li"
                },
                {
                    "authorId": "2288040582",
                    "name": "Zhibo Wang"
                },
                {
                    "authorId": "2279958447",
                    "name": "Zhan Qin"
                },
                {
                    "authorId": "2145257973",
                    "name": "Kui Ren"
                }
            ]
        },
        {
            "paperId": "c17e776a8288375cfc1f14a70a35dae5eaaee10d",
            "title": "TAPI: Towards Target-Specific and Adversarial Prompt Injection against Code LLMs",
            "abstract": "Recently, code-oriented large language models (Code LLMs) have been widely and successfully used to simplify and facilitate code programming. With these tools, developers can easily generate desired complete functional codes based on incomplete code and natural language prompts. However, a few pioneering works revealed that these Code LLMs are also vulnerable, e.g., against backdoor and adversarial attacks. The former could induce LLMs to respond to triggers to insert malicious code snippets by poisoning the training data or model parameters, while the latter can craft malicious adversarial input codes to reduce the quality of generated codes. However, both attack methods have underlying limitations: backdoor attacks rely on controlling the model training process, while adversarial attacks struggle with fulfilling specific malicious purposes. To inherit the advantages of both backdoor and adversarial attacks, this paper proposes a new attack paradigm, i.e., target-specific and adversarial prompt injection (TAPI), against Code LLMs. TAPI generates unreadable comments containing information about malicious instructions and hides them as triggers in the external source code. When users exploit Code LLMs to complete codes containing the trigger, the models will generate attacker-specified malicious code snippets at specific locations. We evaluate our TAPI attack on four representative LLMs under three representative malicious objectives and seven cases. The results show that our method is highly threatening (achieving an attack success rate of up to 98.3%) and stealthy (saving an average of 53.1% of tokens in the trigger design). In particular, we successfully attack some famous deployed code completion integrated applications, including CodeGeex and Github Copilot. This further confirms the realistic threat of our attack.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2289317561",
                    "name": "Yuchen Yang"
                },
                {
                    "authorId": "2028819641",
                    "name": "Hongwei Yao"
                },
                {
                    "authorId": "2311518604",
                    "name": "Bingrun Yang"
                },
                {
                    "authorId": "2217446811",
                    "name": "Yiling He"
                },
                {
                    "authorId": "2300335251",
                    "name": "Yiming Li"
                },
                {
                    "authorId": "2311284368",
                    "name": "Tianwei Zhang"
                },
                {
                    "authorId": "2272854003",
                    "name": "Zhan Qin"
                },
                {
                    "authorId": "2145257973",
                    "name": "Kui Ren"
                }
            ]
        },
        {
            "paperId": "d9094f06aab2cc4b74805fe1615af8f4ea72614d",
            "title": "Sora Detector: A Unified Hallucination Detection for Large Text-to-Video Models",
            "abstract": "The rapid advancement in text-to-video (T2V) generative models has enabled the synthesis of high-fidelity video content guided by textual descriptions. Despite this significant progress, these models are often susceptible to hallucination, generating contents that contradict the input text, which poses a challenge to their reliability and practical deployment. To address this critical issue, we introduce the SoraDetector, a novel unified framework designed to detect hallucinations across diverse large T2V models, including the cutting-edge Sora model. Our framework is built upon a comprehensive analysis of hallucination phenomena, categorizing them based on their manifestation in the video content. Leveraging the state-of-the-art keyframe extraction techniques and multimodal large language models, SoraDetector first evaluates the consistency between extracted video content summary and textual prompts, then constructs static and dynamic knowledge graphs (KGs) from frames to detect hallucination both in single frames and across frames. Sora Detector provides a robust and quantifiable measure of consistency, static and dynamic hallucination. In addition, we have developed the Sora Detector Agent to automate the hallucination detection process and generate a complete video quality report for each input video. Lastly, we present a novel meta-evaluation benchmark, T2VHaluBench, meticulously crafted to facilitate the evaluation of advancements in T2V hallucination detection. Through extensive experiments on videos generated by Sora and other large T2V models, we demonstrate the efficacy of our approach in accurately detecting hallucinations. The code and dataset can be accessed via GitHub.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237992280",
                    "name": "Zhixuan Chu"
                },
                {
                    "authorId": "2279806204",
                    "name": "Lei Zhang"
                },
                {
                    "authorId": "2300306317",
                    "name": "Yichen Sun"
                },
                {
                    "authorId": "2149919635",
                    "name": "Siqiao Xue"
                },
                {
                    "authorId": "2288040582",
                    "name": "Zhibo Wang"
                },
                {
                    "authorId": "2279958447",
                    "name": "Zhan Qin"
                },
                {
                    "authorId": "2145257973",
                    "name": "Kui Ren"
                }
            ]
        },
        {
            "paperId": "f8036a0dbeb67c3523a59173a8c3feda12702d66",
            "title": "Towards Reliable and Efficient Backdoor Trigger Inversion via Decoupling Benign Features",
            "abstract": "Recent studies revealed that using third-party models may lead to backdoor threats, where adversaries can maliciously manipulate model predictions based on back-doors implanted during model training. Arguably, backdoor trigger inversion (BTI), which generates trigger patterns of given benign samples for a backdoored model, is the most critical module for backdoor defenses used in these scenarios. With BTI, defenders can remove backdoors by fine-tuning based on generated poisoned samples with ground-truth labels or deactivate backdoors by removing trigger patterns during the inference process. However, we find that existing BTI methods suffer from relatively poor performance, i.e. , their generated triggers are significantly different from the ones used by the adversaries even in the feature space. We argue that it is mostly because existing methods require to \u2018extract\u2019 backdoor features at first, while this task is very difficult since defenders have no information ( e.g. , trigger pattern or target label) about poisoned samples. In this paper, we explore BTI from another perspective where we decouple benign features instead of decoupling backdoor features directly. Specifically, our method consists of two main steps, including (1) decoupling benign features and (2) trigger inversion by minimizing the differences between benign samples and their generated poisoned version in decoupled benign features while maximizing the differences in remaining backdoor features. In particular, our method is more efficient since it doesn\u2019t need to \u2018scan\u2019 all classes to speculate the target label, as required by",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2313687149",
                    "name": "Xiong Xu"
                },
                {
                    "authorId": "1974382802",
                    "name": "Kunzhe Huang"
                },
                {
                    "authorId": "2300335251",
                    "name": "Yiming Li"
                },
                {
                    "authorId": "2272854003",
                    "name": "Zhan Qin"
                },
                {
                    "authorId": "2145257973",
                    "name": "Kui Ren"
                }
            ]
        },
        {
            "paperId": "f9a19f537b263621106b21f2ceb656a0da0e6fed",
            "title": "Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models",
            "abstract": "The rapid advancement of Text-to-Image(T2I) generative models has enabled the synthesis of high-quality images guided by textual descriptions. Despite this significant progress, these models are often susceptible in generating contents that contradict the input text, which poses a challenge to their reliability and practical deployment. To address this problem, we introduce a novel diffusion-based framework to significantly enhance the alignment of generated images with their corresponding descriptions, addressing the inconsistency between visual output and textual input. Our framework is built upon a comprehensive analysis of inconsistency phenomena, categorizing them based on their manifestation in the image. Leveraging a state-of-the-art large language module, we first extract objects and construct a knowledge graph to predict the locations of these objects in potentially generated images. We then integrate a state-of-the-art controllable image generation model with a visual text generation module to generate an image that is consistent with the original prompt, guided by the predicted object locations. Through extensive experiments on an advanced multimodal hallucination benchmark, we demonstrate the efficacy of our approach in accurately generating the images without the inconsistency with the original prompt. The code can be accessed via https://github.com/TruthAI-Lab/PCIG.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2300306317",
                    "name": "Yichen Sun"
                },
                {
                    "authorId": "2303254578",
                    "name": "Zhixuan Chu"
                },
                {
                    "authorId": "2279958447",
                    "name": "Zhan Qin"
                },
                {
                    "authorId": "2145257973",
                    "name": "Kui Ren"
                }
            ]
        },
        {
            "paperId": "279d315a102b8784e90d52880f91248220acf9e8",
            "title": "Certified Minimax Unlearning with Generalization Rates and Deletion Capacity",
            "abstract": "We study the problem of $(\\epsilon,\\delta)$-certified machine unlearning for minimax models. Most of the existing works focus on unlearning from standard statistical learning models that have a single variable and their unlearning steps hinge on the direct Hessian-based conventional Newton update. We develop a new $(\\epsilon,\\delta)$-certified machine unlearning algorithm for minimax models. It proposes a minimax unlearning step consisting of a total-Hessian-based complete Newton update and the Gaussian mechanism borrowed from differential privacy. To obtain the unlearning certification, our method injects calibrated Gaussian noises by carefully analyzing the\"sensitivity\"of the minimax unlearning step (i.e., the closeness between the minimax unlearning variables and the retraining-from-scratch variables). We derive the generalization rates in terms of population strong and weak primal-dual risk for three different cases of loss functions, i.e., (strongly-)convex-(strongly-)concave losses. We also provide the deletion capacity to guarantee that a desired population risk can be maintained as long as the number of deleted samples does not exceed the derived amount. With training samples $n$ and model dimension $d$, it yields the order $\\mathcal O(n/d^{1/4})$, which shows a strict gap over the baseline method of differentially private minimax learning that has $\\mathcal O(n/d^{1/2})$. In addition, our rates of generalization and deletion capacity match the state-of-the-art rates derived previously for standard statistical learning models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268538599",
                    "name": "Jiaqi Liu"
                },
                {
                    "authorId": "2187312292",
                    "name": "Jian Lou"
                },
                {
                    "authorId": "2260857611",
                    "name": "Zhan Qin"
                },
                {
                    "authorId": "2145257973",
                    "name": "Kui Ren"
                }
            ]
        }
    ]
}