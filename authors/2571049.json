{
    "authorId": "2571049",
    "papers": [
        {
            "paperId": "828be35293d29e1c2e926bf711f1fc862e5680e9",
            "title": "Understanding Information Storage and Transfer in Multi-modal Large Language Models",
            "abstract": "Understanding the mechanisms of information storage and transfer in Transformer-based models is important for driving model understanding progress. Recent work has studied these mechanisms for Large Language Models (LLMs), revealing insights on how information is stored in a model's parameters and how information flows to and from these parameters in response to specific prompts. However, these studies have not yet been extended to Multi-modal Large Language Models (MLLMs). Given their expanding capabilities and real-world use, we start by studying one aspect of these models -- how MLLMs process information in a factual visual question answering task. We use a constraint-based formulation which views a visual question as having a set of visual or textual constraints that the model's generated answer must satisfy to be correct (e.g. What movie directed by the director in this photo has won a Golden Globe?). Under this setting, we contribute i) a method that extends causal information tracing from pure language to the multi-modal setting, and ii) VQA-Constraints, a test-bed of 9.7K visual questions annotated with constraints. We use these tools to study two open-source MLLMs, LLaVa and multi-modal Phi-2. Our key findings show that these MLLMs rely on MLP and self-attention blocks in much earlier layers for information storage, compared to LLMs whose mid-layer MLPs are more important. We also show that a consistent small subset of visual tokens output by the vision encoder are responsible for transferring information from the image to these causal blocks. We validate these mechanisms by introducing MultEdit, a model-editing algorithm that can correct errors and insert new long-tailed information into MLLMs by targeting these causal blocks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114710333",
                    "name": "Samyadeep Basu"
                },
                {
                    "authorId": "2052886960",
                    "name": "Martin Grayson"
                },
                {
                    "authorId": "121927341",
                    "name": "C. Morrison"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "34389431",
                    "name": "S. Feizi"
                },
                {
                    "authorId": "2286878667",
                    "name": "Daniela Massiceti"
                }
            ]
        },
        {
            "paperId": "9775bb6870d0c80881487be57de0a0f31cea08be",
            "title": "Introducing v0.5 of the AI Safety Benchmark from MLCommons",
            "abstract": "This paper introduces v0.5 of the AI Safety Benchmark, which has been created by the MLCommons AI Safety Working Group. The AI Safety Benchmark has been designed to assess the safety risks of AI systems that use chat-tuned language models. We introduce a principled approach to specifying and constructing the benchmark, which for v0.5 covers only a single use case (an adult chatting to a general-purpose assistant in English), and a limited set of personas (i.e., typical users, malicious users, and vulnerable users). We created a new taxonomy of 13 hazard categories, of which 7 have tests in the v0.5 benchmark. We plan to release version 1.0 of the AI Safety Benchmark by the end of 2024. The v1.0 benchmark will provide meaningful insights into the safety of AI systems. However, the v0.5 benchmark should not be used to assess the safety of AI systems. We have sought to fully document the limitations, flaws, and challenges of v0.5. This release of v0.5 of the AI Safety Benchmark includes (1) a principled approach to specifying and constructing the benchmark, which comprises use cases, types of systems under test (SUTs), language and context, personas, tests, and test items; (2) a taxonomy of 13 hazard categories with definitions and subcategories; (3) tests for seven of the hazard categories, each comprising a unique set of test items, i.e., prompts. There are 43,090 test items in total, which we created with templates; (4) a grading system for AI systems against the benchmark; (5) an openly available platform, and downloadable tool, called ModelBench that can be used to evaluate the safety of AI systems on the benchmark; (6) an example evaluation report which benchmarks the performance of over a dozen openly available chat-tuned language models; (7) a test specification for the benchmark.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2737827",
                    "name": "Bertie Vidgen"
                },
                {
                    "authorId": "2297190740",
                    "name": "Adarsh Agrawal"
                },
                {
                    "authorId": "2297807964",
                    "name": "Ahmed M. Ahmed"
                },
                {
                    "authorId": "2257002651",
                    "name": "Victor Akinwande"
                },
                {
                    "authorId": "2023118917",
                    "name": "Namir Al-nuaimi"
                },
                {
                    "authorId": "2124768",
                    "name": "Najla Alfaraj"
                },
                {
                    "authorId": "102661476",
                    "name": "Elie Alhajjar"
                },
                {
                    "authorId": "2257256357",
                    "name": "Lora Aroyo"
                },
                {
                    "authorId": "2297187181",
                    "name": "Trupti Bavalatti"
                },
                {
                    "authorId": "1517663067",
                    "name": "Borhane Blili-Hamelin"
                },
                {
                    "authorId": "1742448",
                    "name": "K. Bollacker"
                },
                {
                    "authorId": "2297187179",
                    "name": "Rishi Bomassani"
                },
                {
                    "authorId": "2498618",
                    "name": "Marisa Ferrara Boston"
                },
                {
                    "authorId": "2297188715",
                    "name": "Sim'eon Campos"
                },
                {
                    "authorId": "2297188195",
                    "name": "Kal Chakra"
                },
                {
                    "authorId": "2163546329",
                    "name": "Canyu Chen"
                },
                {
                    "authorId": "2091029496",
                    "name": "Cody Coleman"
                },
                {
                    "authorId": "2297187212",
                    "name": "Zacharie Delpierre Coudert"
                },
                {
                    "authorId": "113320522",
                    "name": "Leon Derczynski"
                },
                {
                    "authorId": "2267726934",
                    "name": "Debojyoti Dutta"
                },
                {
                    "authorId": "2297188410",
                    "name": "Ian Eisenberg"
                },
                {
                    "authorId": "46449010",
                    "name": "J. Ezick"
                },
                {
                    "authorId": "2238786311",
                    "name": "Heather Frase"
                },
                {
                    "authorId": "2223748737",
                    "name": "Brian Fuller"
                },
                {
                    "authorId": "2297187207",
                    "name": "Ram Gandikota"
                },
                {
                    "authorId": "2199260209",
                    "name": "Agasthya Gangavarapu"
                },
                {
                    "authorId": "1972481155",
                    "name": "Ananya Gangavarapu"
                },
                {
                    "authorId": "9250608",
                    "name": "J. Gealy"
                },
                {
                    "authorId": "2213553962",
                    "name": "Rajat Ghosh"
                },
                {
                    "authorId": "2297187194",
                    "name": "James Goel"
                },
                {
                    "authorId": "1386345955",
                    "name": "Usman Gohar"
                },
                {
                    "authorId": "2297188541",
                    "name": "Sujata Goswami"
                },
                {
                    "authorId": "1741886127",
                    "name": "Scott A. Hale"
                },
                {
                    "authorId": "2297187154",
                    "name": "Wiebke Hutiri"
                },
                {
                    "authorId": "151472158",
                    "name": "Joseph Marvin Imperial"
                },
                {
                    "authorId": "2230701705",
                    "name": "Surgan Jandial"
                },
                {
                    "authorId": "2294772116",
                    "name": "Nicholas C. Judd"
                },
                {
                    "authorId": "1389940060",
                    "name": "Felix Juefei-Xu"
                },
                {
                    "authorId": "1703493",
                    "name": "Foutse Khomh"
                },
                {
                    "authorId": "2208575928",
                    "name": "B. Kailkhura"
                },
                {
                    "authorId": "90729626",
                    "name": "Hannah Rose Kirk"
                },
                {
                    "authorId": "2260341611",
                    "name": "Kevin Klyman"
                },
                {
                    "authorId": "2297186980",
                    "name": "Chris Knotz"
                },
                {
                    "authorId": "52176720",
                    "name": "Michael Kuchnik"
                },
                {
                    "authorId": "2109680564",
                    "name": "Shachi H. Kumar"
                },
                {
                    "authorId": "2297188456",
                    "name": "Chris Lengerich"
                },
                {
                    "authorId": "2296831558",
                    "name": "Bo Li"
                },
                {
                    "authorId": "2284224390",
                    "name": "Zeyi Liao"
                },
                {
                    "authorId": "2297188657",
                    "name": "E. Long"
                },
                {
                    "authorId": "2297188789",
                    "name": "Victor Lu"
                },
                {
                    "authorId": "2054708905",
                    "name": "Yifan Mai"
                },
                {
                    "authorId": "46213894",
                    "name": "P. Mammen"
                },
                {
                    "authorId": "2297187982",
                    "name": "Kelvin Manyeki"
                },
                {
                    "authorId": "2297187225",
                    "name": "Sean McGregor"
                },
                {
                    "authorId": "2130578944",
                    "name": "Virendra Mehta"
                },
                {
                    "authorId": "2297825575",
                    "name": "Shafee Mohammed"
                },
                {
                    "authorId": "2297187017",
                    "name": "Emanuel Moss"
                },
                {
                    "authorId": "1896095",
                    "name": "L. Nachman"
                },
                {
                    "authorId": "2297187185",
                    "name": "Dinesh Jinenhally Naganna"
                },
                {
                    "authorId": "2076564",
                    "name": "Amin Nikanjam"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "1594025351",
                    "name": "Luis Oala"
                },
                {
                    "authorId": "2297187978",
                    "name": "Iftach Orr"
                },
                {
                    "authorId": "2265756309",
                    "name": "Alicia Parrish"
                },
                {
                    "authorId": "3259057",
                    "name": "\u00c7igdem Patlak"
                },
                {
                    "authorId": "2424199",
                    "name": "William Pietri"
                },
                {
                    "authorId": "1405364889",
                    "name": "Forough Poursabzi-Sangdeh"
                },
                {
                    "authorId": "6072807",
                    "name": "Eleonora Presani"
                },
                {
                    "authorId": "2296442894",
                    "name": "Fabrizio Puletti"
                },
                {
                    "authorId": "2043232919",
                    "name": "Paul R\u00f6ttger"
                },
                {
                    "authorId": "38531701",
                    "name": "Saurav Sahay"
                },
                {
                    "authorId": "2297431493",
                    "name": "Tim Santos"
                },
                {
                    "authorId": "1742339548",
                    "name": "Nino Scherrer"
                },
                {
                    "authorId": "47540766",
                    "name": "Alice Schoenauer Sebag"
                },
                {
                    "authorId": "40896023",
                    "name": "P. Schramowski"
                },
                {
                    "authorId": "2297187191",
                    "name": "Abolfazl Shahbazi"
                },
                {
                    "authorId": "2297338785",
                    "name": "Vin Sharma"
                },
                {
                    "authorId": "2144058688",
                    "name": "Xudong Shen"
                },
                {
                    "authorId": "2297188681",
                    "name": "Vamsi Sistla"
                },
                {
                    "authorId": "2297445874",
                    "name": "Leonard Tang"
                },
                {
                    "authorId": "2273657478",
                    "name": "Davide Testuggine"
                },
                {
                    "authorId": "51153332",
                    "name": "Vithursan Thangarasa"
                },
                {
                    "authorId": "29830026",
                    "name": "E. A. Watkins"
                },
                {
                    "authorId": "2297190630",
                    "name": "Rebecca Weiss"
                },
                {
                    "authorId": "2062396538",
                    "name": "Christoper A. Welty"
                },
                {
                    "authorId": "2297188261",
                    "name": "Tyler Wilbers"
                },
                {
                    "authorId": "2297757196",
                    "name": "Adina Williams"
                },
                {
                    "authorId": "2293742452",
                    "name": "Carole-Jean Wu"
                },
                {
                    "authorId": "2297189809",
                    "name": "Poonam Yadav"
                },
                {
                    "authorId": "2145170944",
                    "name": "Xianjun Yang"
                },
                {
                    "authorId": "2297444637",
                    "name": "Yi Zeng"
                },
                {
                    "authorId": "2297247783",
                    "name": "Wenhui Zhang"
                },
                {
                    "authorId": "2297188641",
                    "name": "Fedor Zhdanov"
                },
                {
                    "authorId": "2297319602",
                    "name": "Jiacheng Zhu"
                },
                {
                    "authorId": "2260342171",
                    "name": "Percy Liang"
                },
                {
                    "authorId": "2065823421",
                    "name": "Peter Mattson"
                },
                {
                    "authorId": "1717534",
                    "name": "J. Vanschoren"
                }
            ]
        },
        {
            "paperId": "a58112381efffe6f1b43f8b663a5d30a2ff27eef",
            "title": "Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models",
            "abstract": "While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over. In this work, we address this concern for tabular data. Specifically, we introduce a variety of different techniques to assess whether a language model has seen a tabular dataset during training. This investigation reveals that LLMs have memorized many popular tabular datasets verbatim. We then compare the few-shot learning performance of LLMs on datasets that were seen during training to the performance on datasets released after training. We find that LLMs perform better on datasets seen during training, indicating that memorization leads to overfitting. At the same time, LLMs show non-trivial performance on novel datasets and are surprisingly robust to data transformations. We then investigate the in-context statistical learning abilities of LLMs. While LLMs are significantly better than random at solving statistical classification problems, the sample efficiency of few-shot learning lags behind traditional statistical learning algorithms, especially as the dimension of the problem increases. This suggests that much of the observed few-shot performance on novel real-world datasets is due to the LLM's world knowledge. Overall, our results highlight the importance of testing whether an LLM has seen an evaluation dataset during pre-training. We release the https://github.com/interpretml/LLM-Tabular-Memorization-Checker Python package to test LLMs for memorization of tabular datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802528351",
                    "name": "Sebastian Bordt"
                },
                {
                    "authorId": "40900039",
                    "name": "Harsha Nori"
                },
                {
                    "authorId": "2295731554",
                    "name": "Vanessa Rodrigues"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "2282539419",
                    "name": "Rich Caruana"
                }
            ]
        },
        {
            "paperId": "be7122f7b2db3bce3137519b1f81e79fa57c9eaa",
            "title": "Eureka: Evaluating and Understanding Large Foundation Models",
            "abstract": "Rigorous and reproducible evaluation is critical for assessing the state of the art and for guiding scientific advances in Artificial Intelligence. Evaluation is challenging in practice due to several reasons, including benchmark saturation, lack of transparency in methods used for measurement, development challenges in extracting measurements for generative tasks, and, more generally, the extensive number of capabilities required for a well-rounded comparison across models. We make three contributions to alleviate the above challenges. First, we present Eureka, an open-source framework for standardizing evaluations of large foundation models beyond single-score reporting and rankings. Second, we introduce Eureka-Bench as an extensible collection of benchmarks testing capabilities that (i) are still challenging for state-of-the-art models and (ii) represent fundamental but overlooked language and multimodal capabilities. The inherent space for improvement in non-saturated benchmarks enables us to discover meaningful differences between models at a capability level. Third, using Eureka, we conduct an analysis of 12 state-of-the-art models, providing in-depth insights into failure understanding and model comparison, which can be leveraged to plan targeted improvements. In contrast to recent trends in reports and leaderboards showing absolute rankings and claims for one model or another to be the best, our analysis shows that there is no such best model. Different models have different strengths, but there are models that appear more often than others as best performers for some capabilities. Despite the recent improvements, current models still struggle with several fundamental capabilities including detailed image understanding, benefiting from multimodal input when available rather than fully relying on language, factuality and grounding for information retrieval, and over refusals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2321484904",
                    "name": "Jingya Chen"
                },
                {
                    "authorId": "2250480908",
                    "name": "Neel Joshi"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2321455461",
                    "name": "Eduardo Salinas"
                },
                {
                    "authorId": "143729959",
                    "name": "Vibhav Vineet"
                },
                {
                    "authorId": "2321455359",
                    "name": "James Woffinden-Luey"
                },
                {
                    "authorId": "2670023",
                    "name": "Safoora Yousefi"
                }
            ]
        },
        {
            "paperId": "0d943aa547690c40aff35b4e0b329bf04aedc59d",
            "title": "Diversity of Thought Improves Reasoning Abilities of LLMs",
            "abstract": "Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps, or ensembling various generations through modifying decoding steps boosts performance. However, these methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we discuss how one can create and leverage variations of the input prompt as a means of diversity of thought. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that are apt for the problem. We then ensemble the diverse prompts in our method DIVSE (DIVerse reasoning path Self-Ensemble) across multiple inference calls, or use diverse approaches within a single inference call; we call the latter IDIV-SE (In-call DIVerse reasoning path Self-Ensemble). Apart from our approaches outperforming prior work, DIV-SE(in particular) advances state-of-the-art performance on the challenging planning and graph coloring benchmarks. Our results improve the Pareto frontier of the accuracy-cost trade-off.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2205397794",
                    "name": "Ranjita Naik"
                },
                {
                    "authorId": "2247670741",
                    "name": "Varun Chandrasekaran"
                },
                {
                    "authorId": "2282542500",
                    "name": "Mert Yuksekgonul"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                }
            ]
        },
        {
            "paperId": "38a2c434ad10f26e2b1f970d944d862da74f9e71",
            "title": "Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning",
            "abstract": "Spurious correlations that degrade model generalization or lead the model to be right for the wrong reasons are one of the main robustness concerns for real-world deployments. However, mitigating these correlations during pre-training for large-scale models can be costly and impractical, particularly for those without access to high-performance computing resources. This paper proposes a novel approach to address spurious correlations during fine-tuning for a given domain of interest. With a focus on multi-modal models (e.g., CLIP), the proposed method leverages different modalities in these models to detect and explicitly set apart spurious attributes from the affected class, achieved through a multi-modal contrastive loss function that expresses spurious relationships through language. Our experimental results and in-depth visualizations on CLIP show that such an intervention can effectively i) improve the model's accuracy when spurious attributes are not present, and ii) directs the model's activation maps towards the actual class rather than the spurious attribute when present. In particular, on the Waterbirds dataset, our algorithm achieved a worst-group accuracy 23% higher than ERM on CLIP with a ResNet-50 backbone, and 32% higher on CLIP with a ViT backbone, while maintaining the same average accuracy as ERM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116468252",
                    "name": "Yu Yang"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2389094",
                    "name": "Baharan Mirzasoleiman"
                }
            ]
        },
        {
            "paperId": "7b18bdf21b74b7d97bace715f78d05bd447ee30f",
            "title": "Social Biases through the Text-to-Image Generation Lens",
            "abstract": "Text-to-Image (T2I) generation is enabling new applications that support creators, designers, and general end users of productivity software by generating illustrative content with high photorealism starting from a given descriptive text as a prompt. Such models are however trained on massive amounts of web data, which surfaces the peril of potential harmful biases that may leak in the generation process itself. In this paper, we take a multi-dimensional approach to studying and quantifying common social biases as reflected in the generated images, by focusing on how occupations, personality traits, and everyday situations are depicted across representations of (perceived) gender, age, race, and geographical location. Through an extensive set of both automated and human evaluation experiments we present findings for two popular T2I models: DALLE-v2 and Stable Diffusion. Our results reveal that there exist severe occupational biases of neutral prompts majorly excluding groups of people from results for both models. Such biases can get mitigated by increasing the amount of specification in the prompt itself, although the prompting mitigation will not address discrepancies in image quality or other usages of the model or its representations in other scenarios. Further, we observe personality traits being associated with only a limited set of people at the intersection of race, gender, and age. Finally, an analysis of geographical location representations on everyday situations (e.g., park, food, weddings) shows that for most situations, images generated through default location-neutral prompts are closer and more similar to images generated for locations of United States and Germany.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2205397794",
                    "name": "Ranjita Naik"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                }
            ]
        },
        {
            "paperId": "e9505feb8098935caa43a4bb069abe180389343f",
            "title": "KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval",
            "abstract": "We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., 'a list of ice cream shops in San Diego'). In the past, such queries were considered to be tasks that could only be solved via web-search or knowledge bases. More recently, large language models (LLMs) have demonstrated initial emergent abilities in this task. However, many current retrieval benchmarks are either saturated or do not measure constraint satisfaction. Motivated by rising concerns around factual incorrectness and hallucinations of LLMs, we present KITAB, a new dataset for measuring constraint satisfaction abilities of language models. KITAB consists of book-related data across more than 600 authors and 13,000 queries, and also offers an associated dynamic data collection and constraint verification approach for acquiring similar test data for other authors. Our extended experiments on GPT4 and GPT3.5 characterize and decouple common failure modes across dimensions such as information popularity, constraint types, and context availability. Results show that in the absence of context, models exhibit severe limitations as measured by irrelevant information, factual errors, and incompleteness, many of which exacerbate as information popularity decreases. While context availability mitigates irrelevant information, it is not helpful for satisfying constraints, identifying fundamental barriers to constraint satisfaction. We open source our contributions to foster further research on improving constraint satisfaction abilities of future models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114891202",
                    "name": "Marah Abdin"
                },
                {
                    "authorId": "2261390673",
                    "name": "Suriya Gunasekar"
                },
                {
                    "authorId": "2247670741",
                    "name": "Varun Chandrasekaran"
                },
                {
                    "authorId": "2261393413",
                    "name": "Jerry Li"
                },
                {
                    "authorId": "2186981598",
                    "name": "Mert Yuksekgonul"
                },
                {
                    "authorId": "1387122758",
                    "name": "Rahee Peshawaria"
                },
                {
                    "authorId": "2205397794",
                    "name": "Ranjita Naik"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                }
            ]
        },
        {
            "paperId": "f6007270b8aba3a69eac7c98375c99b7ca26c9b0",
            "title": "Diversity of Thought Improves Reasoning Abilities of Large Language Models",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2205397794",
                    "name": "Ranjita Naik"
                },
                {
                    "authorId": "2247670741",
                    "name": "Varun Chandrasekaran"
                },
                {
                    "authorId": "2186981598",
                    "name": "Mert Yuksekgonul"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                }
            ]
        },
        {
            "paperId": "fdc75b7b51ca98e555a4a8f0bc261e76bac6fb70",
            "title": "Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models",
            "abstract": "We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as constraint satisfaction problems and use this framework to investigate how the LLM interacts internally with factual constraints. We find a strong positive relationship between the LLM's attention to constraint tokens and the factual accuracy of generations. We curate a suite of 10 datasets containing over 40,000 prompts to study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing attention patterns, that can predict factual errors and fine-grained constraint satisfaction, and allow early error identification. The approach and findings take another step towards using the mechanistic understanding of LLMs to enhance their reliability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2186981598",
                    "name": "Mert Yuksekgonul"
                },
                {
                    "authorId": "2247670741",
                    "name": "Varun Chandrasekaran"
                },
                {
                    "authorId": "2247370218",
                    "name": "Erik Jones"
                },
                {
                    "authorId": "3317356",
                    "name": "Suriya Gunasekar"
                },
                {
                    "authorId": "2205397794",
                    "name": "Ranjita Naik"
                },
                {
                    "authorId": "2247662718",
                    "name": "Hamid Palangi"
                },
                {
                    "authorId": "2247662058",
                    "name": "Ece Kamar"
                },
                {
                    "authorId": "2571049",
                    "name": "Besmira Nushi"
                }
            ]
        }
    ]
}