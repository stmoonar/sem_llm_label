{
    "authorId": "2037491",
    "papers": [
        {
            "paperId": "42bf360fccb6ab719de7f6f0ac4ac8d9912f98e7",
            "title": "IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce",
            "abstract": "Enhancing Language Models' (LMs) ability to understand purchase intentions in E-commerce scenarios is crucial for their effective assistance in various downstream tasks. However, previous approaches that distill intentions from LMs often fail to generate meaningful and human-centric intentions applicable in real-world E-commerce contexts. This raises concerns about the true comprehension and utilization of purchase intentions by LMs. In this paper, we present IntentionQA, a double-task multiple-choice question answering benchmark to evaluate LMs' comprehension of purchase intentions in E-commerce. Specifically, LMs are tasked to infer intentions based on purchased products and utilize them to predict additional purchases. IntentionQA consists of 4,360 carefully curated problems across three difficulty levels, constructed using an automated pipeline to ensure scalability on large E-commerce platforms. Human evaluations demonstrate the high quality and low false-negative rate of our benchmark. Extensive experiments across 19 language models show that they still struggle with certain scenarios, such as understanding products and intentions accurately, jointly reasoning with products and intentions, and more, in which they fall far behind human performances. Our code and data are publicly available at https://github.com/HKUST-KnowComp/IntentionQA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2037491",
                    "name": "Wenxuan Ding"
                },
                {
                    "authorId": "1587728690",
                    "name": "Weiqi Wang"
                },
                {
                    "authorId": "2306785813",
                    "name": "Sze Heng Douglas Kwok"
                },
                {
                    "authorId": "2306782213",
                    "name": "Minghao Liu"
                },
                {
                    "authorId": "2044202073",
                    "name": "Tianqing Fang"
                },
                {
                    "authorId": "145677395",
                    "name": "Jiaxin Bai"
                },
                {
                    "authorId": "2306845961",
                    "name": "Junxian He"
                },
                {
                    "authorId": "2241325169",
                    "name": "Yangqiu Song"
                }
            ]
        },
        {
            "paperId": "5aec6865043cb7c7f281699ae95652e0ff680f09",
            "title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning",
            "abstract": "The sequential process of conceptualization and instantiation is essential to generalizable commonsense reasoning as it allows the application of existing knowledge to unfamiliar scenarios. However, existing works tend to undervalue the step of instantiation and heavily rely on pre-built concept taxonomies and human annotations to collect both types of knowledge, resulting in a lack of instantiated knowledge to complete reasoning, high cost, and limited scalability. To tackle these challenges, we introduce CANDLE, a distillation framework that iteratively performs contextualized conceptualization and instantiation over commonsense knowledge bases by instructing large language models to generate both types of knowledge with critic filtering. By applying CANDLE to ATOMIC, we construct a comprehensive knowledge base comprising six million conceptualizations and instantiated commonsense knowledge triples. Both types of knowledge are firmly rooted in the original ATOMIC dataset, and intrinsic evaluations demonstrate their exceptional quality and diversity. Empirical results indicate that distilling CANDLE on student models provides benefits across four downstream tasks. Our code, data, and models are publicly available at https://github.com/HKUST-KnowComp/CANDLE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1587728690",
                    "name": "Weiqi Wang"
                },
                {
                    "authorId": "2044202073",
                    "name": "Tianqing Fang"
                },
                {
                    "authorId": "2279661258",
                    "name": "Chunyang Li"
                },
                {
                    "authorId": "2257452757",
                    "name": "Haochen Shi"
                },
                {
                    "authorId": "2037491",
                    "name": "Wenxuan Ding"
                },
                {
                    "authorId": "2257133167",
                    "name": "Baixuan Xu"
                },
                {
                    "authorId": "2187830802",
                    "name": "Zhaowei Wang"
                },
                {
                    "authorId": "145677395",
                    "name": "Jiaxin Bai"
                },
                {
                    "authorId": "2260287837",
                    "name": "Xin Liu"
                },
                {
                    "authorId": "2109077713",
                    "name": "Cheng Jiayang"
                },
                {
                    "authorId": "2216598559",
                    "name": "Chunkit Chan"
                },
                {
                    "authorId": "2241325169",
                    "name": "Yangqiu Song"
                }
            ]
        },
        {
            "paperId": "72099c9eae74859e5389b3806732ed1b7f2ce69b",
            "title": "MIND: Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding",
            "abstract": "Improving user experience and providing personalized search results in E-commerce platforms heavily rely on understanding purchase intention. However, existing methods for acquiring large-scale intentions bank on distilling large language models with human annotation for verification. Such an approach tends to generate product-centric intentions, overlook valuable visual information from product images, and incurs high costs for scalability. To address these issues, we introduce MIND, a multimodal framework that allows Large Vision-Language Models (LVLMs) to infer purchase intentions from multimodal product metadata and prioritize human-centric ones. Using Amazon Review data, we apply MIND and create a multimodal intention knowledge base, which contains 1,264,441 million intentions derived from 126,142 co-buy shopping records across 107,215 products. Extensive human evaluations demonstrate the high plausibility and typicality of our obtained intentions and validate the effectiveness of our distillation framework and filtering mechanism. Additional experiments reveal that our obtained intentions significantly enhance large language models in two intention comprehension tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257133167",
                    "name": "Baixuan Xu"
                },
                {
                    "authorId": "1587728690",
                    "name": "Weiqi Wang"
                },
                {
                    "authorId": "2257452757",
                    "name": "Haochen Shi"
                },
                {
                    "authorId": "2037491",
                    "name": "Wenxuan Ding"
                },
                {
                    "authorId": "2307004841",
                    "name": "Huihao Jing"
                },
                {
                    "authorId": "2044202073",
                    "name": "Tianqing Fang"
                },
                {
                    "authorId": "145677395",
                    "name": "Jiaxin Bai"
                },
                {
                    "authorId": "2306951868",
                    "name": "Long Chen"
                },
                {
                    "authorId": "2241325169",
                    "name": "Yangqiu Song"
                }
            ]
        },
        {
            "paperId": "d70a1f45665fcef448ce4daedeac34f5f4befaa2",
            "title": "On the Role of Entity and Event Level Conceptualization in Generalizable Reasoning: A Survey of Tasks, Methods, Applications, and Future Directions",
            "abstract": "Entity- and event-level conceptualization, as fundamental elements of human cognition, plays a pivotal role in generalizable reasoning. This process involves abstracting specific instances into higher-level concepts and forming abstract knowledge that can be applied in unfamiliar or novel situations, which can enhance models' inferential capabilities and support the effective transfer of knowledge across various domains. Despite its significance, there is currently a lack of a systematic overview that comprehensively examines existing works in the definition, execution, and application of conceptualization to enhance reasoning tasks. In this paper, we address this gap by presenting the first comprehensive survey of 150+ papers, categorizing various definitions, resources, methods, and downstream applications related to conceptualization into a unified taxonomy, with a focus on the entity and event levels. Furthermore, we shed light on potential future directions in this field and hope to garner more attention from the community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1587728690",
                    "name": "Weiqi Wang"
                },
                {
                    "authorId": "2044202073",
                    "name": "Tianqing Fang"
                },
                {
                    "authorId": "2257452757",
                    "name": "Haochen Shi"
                },
                {
                    "authorId": "2257133167",
                    "name": "Baixuan Xu"
                },
                {
                    "authorId": "2037491",
                    "name": "Wenxuan Ding"
                },
                {
                    "authorId": "2307020772",
                    "name": "Liyu Zhang"
                },
                {
                    "authorId": "2262397115",
                    "name": "Wei Fan"
                },
                {
                    "authorId": "145677395",
                    "name": "Jiaxin Bai"
                },
                {
                    "authorId": "2260286743",
                    "name": "Haoran Li"
                },
                {
                    "authorId": "2260287837",
                    "name": "Xin Liu"
                },
                {
                    "authorId": "2241325169",
                    "name": "Yangqiu Song"
                }
            ]
        },
        {
            "paperId": "26f5a7259bad9ff05f481aeb57d7431d499fb100",
            "title": "CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering",
            "abstract": "The task of zero-shot commonsense question answering evaluates models on their capacity to reason about general scenarios beyond those presented in specific datasets. Existing approaches for tackling this task leverage external knowledge from CommonSense Knowledge Bases (CSKBs) by pretraining the model on synthetic QA pairs constructed from CSKBs. In these approaches, negative examples (distractors) are formulated by randomly sampling from CSKBs using fairly primitive keyword constraints. However, two bottlenecks limit these approaches: the inherent incompleteness of CSKBs limits the semantic coverage of synthetic QA pairs, and the lack of human annotations makes the sampled negative examples potentially uninformative and contradictory. To tackle these limitations above, we propose Conceptualization-Augmented Reasoner (CAR), a zero-shot commonsense question-answering framework that fully leverages the power of conceptualization. Specifically, CAR abstracts a commonsense knowledge triple to many higher-level instances, which increases the coverage of CSKB and expands the ground-truth answer space, reducing the likelihood of selecting false-negative distractors. Extensive experiments demonstrate that CAR more robustly generalizes to answering questions about zero-shot commonsense scenarios than existing methods, including large language models, such as GPT3.5 and ChatGPT. Our codes, data, and model checkpoints are available at https://github.com/HKUST-KnowComp/CAR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1587728690",
                    "name": "Weiqi Wang"
                },
                {
                    "authorId": "2044202073",
                    "name": "Tianqing Fang"
                },
                {
                    "authorId": "2037491",
                    "name": "Wenxuan Ding"
                },
                {
                    "authorId": "7576048",
                    "name": "Baixuan Xu"
                },
                {
                    "authorId": "89121677",
                    "name": "Xin Liu"
                },
                {
                    "authorId": "1809614",
                    "name": "Yangqiu Song"
                },
                {
                    "authorId": "2691021",
                    "name": "Antoine Bosselut"
                }
            ]
        },
        {
            "paperId": "33d944de189d6edf3a510ea195803a381c5a3bab",
            "title": "Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with Large Language Models",
            "abstract": "Large language models (LLMs) are widely adopted in knowledge-intensive tasks and have achieved impressive performance thanks to their knowledge abilities. While LLMs have demonstrated outstanding performance on atomic or linear (multi-hop) QA tasks, whether they can reason in knowledge-rich scenarios with interweaving constraints remains an underexplored problem. In this work, we propose geometric reasoning over structured knowledge, where pieces of knowledge are connected in a graph structure and models need to fill in the missing information of this graph. Such geometric knowledge reasoning would require the ability to handle structured knowledge, reason with uncertainty, verify facts, and backtrack when an error occurs. We specifically propose KNOWLEDGE CROSSWORDS, a multi-blank QA dataset where each problem consists of a natural language question representing the geometric constraints of an incomplete entity network, where LLMs are tasked with working out the missing entities while meeting all factual constraints. KNOWLEDGE CROSSWORDS contains 2,101 individual problems, covering a wide array of knowledge domains and further divided into three difficulty levels. We conduct extensive experiments to evaluate existing LLM prompting approaches on the KNOWLEDGE CROSSWORDS benchmark. We additionally propose two new approaches, STAGED PROMPTING and VERIFY-ALL, to augment LLMs\u2019 ability to backtrack and verify structured constraints. Our results demonstrate that while baseline approaches perform well on easier problems but struggle with questions on the hard side, our proposed VERIFY-ALL outperforms other methods by a large margin and is more robust with hard problems. Further analysis reveals that LLMs\u2019 ability of geometric reasoning over structured knowledge is still far from robust or perfect, susceptible to confounders such as the order of options, certain structural patterns, assumption of existence of correct answer, and more. Code and data are publicly available at https://github.com/Wenwen-D/KnowledgeCrosswords.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2037491",
                    "name": "Wenxuan Ding"
                },
                {
                    "authorId": "2114887261",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "2169159066",
                    "name": "Yuhan Liu"
                },
                {
                    "authorId": "2093186816",
                    "name": "Zhaoxuan Tan"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "7fbe04f6dc6b51f60225ae3b3700ee93d50daa34",
            "title": "QADYNAMICS: Training Dynamics-Driven Synthetic QA Diagnostic for Zero-Shot Commonsense Question Answering",
            "abstract": "Zero-shot commonsense Question-Answering (QA) requires models to reason about general situations beyond specific benchmarks. State-of-the-art approaches fine-tune language models on QA pairs constructed from CommonSense Knowledge Bases (CSKBs) to equip the models with more commonsense knowledge in a QA context. However, current QA synthesis protocols may introduce noise from the CSKBs and generate ungrammatical questions and false negative options, which impede the model's ability to generalize. To address these issues, we propose QADYNAMICS, a training dynamics-driven framework for QA diagnostics and refinement. Our approach analyzes the training dynamics of each QA pair at both the question level and option level, discarding machine-detectable artifacts by removing uninformative QA pairs and mislabeled or false-negative options. Extensive experiments demonstrate the effectiveness of our approach, which outperforms all baselines while using only 33% of the synthetic data, even including LLMs such as ChatGPT. Moreover, expert evaluations confirm that our framework significantly improves the quality of QA synthesis. Our codes and model checkpoints are available at https://github.com/HKUST-KnowComp/QaDynamics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257452757",
                    "name": "Haochen Shi"
                },
                {
                    "authorId": "1587728690",
                    "name": "Weiqi Wang"
                },
                {
                    "authorId": "2044202073",
                    "name": "Tianqing Fang"
                },
                {
                    "authorId": "2257133167",
                    "name": "Baixuan Xu"
                },
                {
                    "authorId": "2037491",
                    "name": "Wenxuan Ding"
                },
                {
                    "authorId": "2260287837",
                    "name": "Xin Liu"
                },
                {
                    "authorId": "2241325169",
                    "name": "Yangqiu Song"
                }
            ]
        }
    ]
}