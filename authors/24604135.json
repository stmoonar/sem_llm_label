{
    "authorId": "24604135",
    "papers": [
        {
            "paperId": "4a04964bab3c41ab9c0796dbb6052e6e17a12fb0",
            "title": "When Automatic Filtering Comes to the Rescue: Pre-Computing Company Competitor Pairs in Owler",
            "abstract": "Competitor data constitutes information significantly valuable for many business applications. Meltwater provides users with access to a large Company Information System (CIS), Owler, which contains competitor pairs and other useful information about companies. Meltwater has been seeking a practical solution to discover more competitor pairs in Owler. The first attempt, a fully-manual workflow (called MW_Manual) for finding more competitor pairs in Owler consisted of two manual steps: a filtering step that excludes obvious non-competitor company pairs, and a further inspection process that inspects each left company pair after the filtering step. MW_Manual was cost prohibitive because the results of the filtering step contained too many non-competitor pairs. Inspecting such non-competitor pairs caused an overhead to the overall workload. To reduce the manual workload, especially the required human effort in the manual inspection process, Meltwater has transformed MW_Manual into a semi-automatic workflow (called MW_CPFilter) by replacing the manual filtering with an automatic yet more precise process that adopts a system called CPFilter. This paper presents CPFilter, a system used in the filtering process of MW_CPFilter. CPFilter automatically pre-computes likely competitor pairs from existing competitor pairs in Owler. CPFilter combines (i) the generation of new competitor candidate pairs by inference from existing competitors and other company-specific knowledge, with (ii) the validation of each candidate competitor pair of two companies by checking whether or not empirical evidence that indicates the competitor relationships of these two companies can be found. CPFilter has three key advantages compared with the manual filtering process and previous works: (i) it resulted in a high workload reduction rate of 0.81, (ii) it is domain-independent so that it can be applied to different sectors in Owler, and (iii) its results are explainable so that humans can easily understand its results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2157956850",
                    "name": "Jinsong Guo"
                },
                {
                    "authorId": "2301684",
                    "name": "Aditya Jami"
                },
                {
                    "authorId": "3372037",
                    "name": "Markus Kr\u00f6ll"
                },
                {
                    "authorId": "2478624",
                    "name": "Lukas Schweizer"
                },
                {
                    "authorId": "144327984",
                    "name": "Sergey Paramonov"
                },
                {
                    "authorId": "2220349289",
                    "name": "Eric Aichinger"
                },
                {
                    "authorId": "2152237728",
                    "name": "Stefano Sferrazza"
                },
                {
                    "authorId": "100735545",
                    "name": "Matt D. Scaccia"
                },
                {
                    "authorId": "2900854",
                    "name": "St\u00e9phane Reissfelder"
                },
                {
                    "authorId": "2069664429",
                    "name": "Eda Cicek"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "1684745",
                    "name": "G. Gottlob"
                }
            ]
        },
        {
            "paperId": "a5c92e7a966bfdc954e2e9447c04ff0edb836e10",
            "title": "RED: Redundancy-Driven Data Extraction from Result Pages?",
            "abstract": "Data-driven websites are mostly accessed through search interfaces. Such sites follow a common publishing pattern that, surprisingly, has not been fully exploited for unsupervised data extraction yet: the result of a search is presented as a paginated list of result records. Each result record contains the main attributes about one single object, and links to a page dedicated to the details of that object. We present red, an automatic approach and a prototype system for extracting data records from sites following this publishing pattern. red leverages the inherent redundancy between result records and corresponding detail pages to design an effective, yet fully-unsupervised and domain-independent method. It is able to extract from result pages all the attributes of the objects that appear both in the result records and in the corresponding detail pages. With respect to previous unsupervised methods, our method does not require any a priori domain-dependent knowledge (e.g, an ontology), can achieve a significantly higher accuracy while automatically selecting only object attributes, a task which is out of the scope of traditional fully unsupervised approaches. With respect to previous supervised or semi-supervised methods, red can reach similar accuracy in many domains (e.g., job postings) without requiring supervision for each domain, let alone each website.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2157956850",
                    "name": "Jinsong Guo"
                },
                {
                    "authorId": "1791339",
                    "name": "Valter Crescenzi"
                },
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "1684745",
                    "name": "G. Gottlob"
                }
            ]
        },
        {
            "paperId": "fcdb6286c82022a79750784dd5595065c3bdd435",
            "title": "PeaCE-Ful Web Event Extraction and Processing as Bitemporal Mutable Events",
            "abstract": "The web is the largest bulletin board of the world. Events of all types, from flight arrivals to business meetings, are announced on this board. Tracking and reacting to such event announcements, however, is a tedious manual task, only slightly alleviated by email or similar notifications. Announcements are published with human readers in mind, and updates or delayed announcements are frequent. These characteristics have hampered attempts at automatic tracking. PeaCE provides the first integrated framework for event processing on top of web event ads, consisting of event extraction, complex event processing, and action execution in response to these events. Given a schema of the events to be tracked, the framework populates this schema by extracting events from announcement sources. This extraction is performed by little programs called wrappers that produce the events including updates and retractions. PeaCE then queries these events to detect complex events, often combining announcements from multiple sources. To deal with updates and delayed announcements, PeaCE\u2019s schemas are bitemporal, to distinguish between occurrence and detection time. This allows complex event specifications to track updates and to react upon differences in occurrence and detection time. In case of new, changing, or deleted events, PeaCE allows one to execute actions, such as tweeting or sending out email notifications. Actions are typically specified as web interactions, for example, to fill and submit a form with attributes of the triggering event. Our evaluation shows that PeaCE\u2019s processing is dominated by the time needed for accessing the web to extract events and perform actions, allotting to 97.4%. Thus, PeaCE requires only 2.6% overhead, and therefore, the complex event processor scales well even with moderate resources. We further show that simple and reasonable restrictions on complex event specifications and the timing of constituent events suffice to guarantee that PeaCE only requires a constant buffer to process arbitrarily many event announcements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "114232744",
                    "name": "M. Huemer"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                },
                {
                    "authorId": "1723099",
                    "name": "M. Schrefl"
                }
            ]
        },
        {
            "paperId": "7aa8ac9fe4c94d8636a5997c8115c3c6498e9dc6",
            "title": "Linking Historical Data on the Web",
            "abstract": "Linked Data today available on the Web mostly represent snapshots at particular points in time. The temporal aspect of data is mostly taken into account only by adding and removing triples to keep datasets up-to-date, thus neglecting the importance to keep track of the evolution of data over time. To overcome this limitation, we introduce the LinkHisData framework to automatize the creation and publication of linked historical data extracted from the Deep Web.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705291",
                    "name": "Valeria Fionda"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                }
            ]
        },
        {
            "paperId": "ee6196f097453aa897b0712e3a31c315b8ae9894",
            "title": "DIADEM: Thousands of Websites to a Single Database",
            "abstract": "The web is overflowing with implicitly structured data, spread over hundreds of thousands of sites, hidden deep behind search forms, or siloed in marketplaces, only accessible as HTML. Automatic extraction of structured data at the scale of thousands of websites has long proven elusive, despite its central role in the \"web of data\". \n \nThrough an extensive evaluation spanning over 10000 web sites from multiple application domains, we show that automatic, yet accurate full-site extraction is no longer a distant dream. diadem is the first automatic full-site extraction system that is able to extract structured data from different domains at very high accuracy. It combines automated exploration of websites, identification of relevant data, and induction of exhaustive wrappers. Automating these components is the first challenge. diadem overcomes this challenge by combining phenomenological and ontological knowledge. Integrating these components is the second challenge. diadem overcomes this challenge through a self-adaptive network of relational transducers that produces effective wrappers for a wide variety of websites. \n \nOur extensive and publicly available evaluation shows that, for more than 90% of sites from three domains, diadem obtains an effective wrapper that extracts all relevant data with 97% average precision. diadem also tolerates noisy entity recognisers, and its components individually outperform comparable approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "1684745",
                    "name": "G. Gottlob"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "2146371097",
                    "name": "Xiaonan Guo"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                },
                {
                    "authorId": "2119128164",
                    "name": "Cheng Wang"
                }
            ]
        },
        {
            "paperId": "2ff8e72f2868a4c1abf5e3cab95dc8fd7a659c97",
            "title": "Effective web scraping with OXPath",
            "abstract": "Even in the third decade of the Web, scraping web sites remains a challenging task: Most scraping programs are still developed as ad-hoc solutions using a complex stack of languages and tools. Where comprehensive extraction solutions exist, they are expensive, heavyweight, and proprietary. OXPath is a minimalistic wrapping language that is nevertheless expressive and versatile enough for a wide range of scraping tasks. In this presentation, we want to introduce you to a new paradigm of scraping: declarative navigation--instead of complex scripting or heavyweight, limited visual tools, OXPath turns scraping into a simple two step process: pick the relevant nodes through an XPath expression and then specify which action to apply to those nodes. OXPath takes care of browser synchronisation, page and state management, making scraping as easy as node selection with XPath. To achieve this, OXPath does not require a complex or heavyweight infrastructure. OXPath is an open source project and has seen first adoption in a wide variety of scraping tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                }
            ]
        },
        {
            "paperId": "1b5a4122a22f6b46b2f727a867d70353b822b961",
            "title": "AMBER: Automatic Supervision for Multi-Attribute Extraction",
            "abstract": "The extraction of multi-attribute objects from the deep web is the bridge between the unstructured web and structured data. Existing approaches either induce wrappers from a set of human-annotated pages or leverage repeated structures on the page without supervision. What the former lack in automation, the latter lack in accuracy. Thus accurate, automatic multi-attribute object extraction has remained an open challenge. \nAMBER overcomes both limitations through mutual supervision between the repeated structure and automatically produced annotations. Previous approaches based on automatic annotations have suffered from low quality due to the inherent noise in the annotations and have attempted to compensate by exploring multiple candidate wrappers. In contrast, AMBER compensates for this noise by integrating repeated structure analysis with annotation-based induction: The repeated structure limits the search space for wrapper induction, and conversely, annotations allow the repeated structure analysis to distinguish noise from relevant data. Both, low recall and low precision in the annotations are mitigated to achieve almost human quality (more than 98 percent) multi-attribute object extraction. \nTo achieve this accuracy, AMBER needs to be trained once for an entire domain. AMBER bootstraps its training from a small, possibly noisy set of attribute instances and a few unannotated sites of the domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "1684745",
                    "name": "G. Gottlob"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                },
                {
                    "authorId": "2119128164",
                    "name": "Cheng Wang"
                }
            ]
        },
        {
            "paperId": "27edee8ee9c9a6b1a901206817074f65f064009c",
            "title": "JASP: A Framework for Integrating Answer Set Programming with Java",
            "abstract": "Answer Set Programming (ASP) is a fully-declarative logic programming paradigm, which has been proposed in the area of knowledge representation and non-monotonic reasoning. Nowadays, the formal properties of ASP are well-understood, efficient ASP systems are available, and, recently, ASP has been employed in a few industrial applications. However, ASP technology is not mature for a successful exploitation in industry yet; mainly because ASP technologies are not integrated in the well-assessed development processes and platforms which are tailored for imperative/object-oriented programming languages. In this paper we present a new programming framework blending ASP with Java. The framework is based on JASP, an hybrid language that transparently supports a bilateral interaction between ASP and Java. JASP specifications are compliant with the JPA standard to perfectly fit extensively-adopted enterprise application technologies. The framework also encompasses an implementation of JASP as a plug-in for the Eclipse platform, called JDLV, which includes a compiler from JASP to Java. Moreover, we show a real-world application developed with JASP and JDLV, which highlights the effectiveness of our ASP-Java integration framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1892002",
                    "name": "Onofrio Febbraro"
                },
                {
                    "authorId": "144491891",
                    "name": "N. Leone"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "17858580",
                    "name": "F. Ricca"
                }
            ]
        },
        {
            "paperId": "31417b645a68a259b68161683096c941229f96e2",
            "title": "OPAL: a passe-partout for web forms",
            "abstract": "Web forms are the interfaces of the deep web. Though modern web browsers provide facilities to assist in form filling, this assistance is limited to prior form fillings or keyword matching. Automatic form understanding enables a broad range of applications, including crawlers, meta-search engines, and usability and accessibility support for enhanced web browsing. In this demonstration, we use a novel form understanding approach, OPAL, to assist in form filling even for complex, previously unknown forms. OPAL associates form labels to fields by analyzing structural properties in the HTML encoding and visual features of the page rendering. OPAL interprets this labeling and classifies the fields according to a given domain ontology. The combination of these two properties, allows OPAL to deal effectively with many forms outside of the grasp of existing form filling techniques. In the UK real estate domain, OPAL achieves >99% accuracy in form understanding.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146371097",
                    "name": "Xiaonan Guo"
                },
                {
                    "authorId": "2539815",
                    "name": "Jochen Kranzdorf"
                },
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                }
            ]
        },
        {
            "paperId": "335b0cc78f36e90228e88349f725f0fad16e5e8c",
            "title": "OPAL: automated form understanding for the deep web",
            "abstract": "Forms are our gates to the web. They enable us to access the deep content of web sites. Automatic form understanding unlocks this content for applications ranging from crawlers to meta-search engines and is essential for improving usability and accessibility of the web. Form understanding has received surprisingly little attention other than as component in specific applications such as crawlers. No comprehensive approach to form understanding exists and previous works disagree even in the definition of the problem. In this paper, we present OPAL, the first comprehensive approach to form understanding. We identify form labeling and form interpretation as the two main tasks involved in form understanding. On both problems OPAL pushes the state of the art: For form labeling, it combines signals from the text, structure, and visual rendering of a web page, yielding robust characterisations of common design patterns. In extensive experiments on the ICQ and TEL-8 benchmarks and a set of 200 modern web forms OPAL outperforms previous approaches by a significant margin. For form interpretation, we introduce a template language to describe frequent form patterns. These two parts of OPAL combined yield form understanding with near perfect accuracy (> 98%).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "1684745",
                    "name": "G. Gottlob"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "2146371097",
                    "name": "Xiaonan Guo"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                }
            ]
        },
        {
            "paperId": "ab614ec55cd66ea92a8529b4823829b03388a938",
            "title": "DIADEM: domain-centric, intelligent, automated data extraction methodology",
            "abstract": "Search engines are the sinews of the web. These sinews have become strained, however: Where the web's function once was a mix of library and yellow pages, it has become the central marketplace for information of almost any kind. We search more and more for objects with specific characteristics, a car with a certain mileage, an affordable apartment close to a good school, or the latest accessory for our phones. Search engines all too often fail to provide reasonable answers, making us sift through dozens of websites with thousands of offers--never to be sure a better offer isn't just around the corner. What search engines are missing is understanding of the objects and their attributes published on websites. Automatically identifying and extracting these objects is akin to alchemy: transforming unstructured web information into highly structured data with near perfect accuracy. With DIADEM we present a formula for this transformation, but at a price: DIADEM identifies and extracts data from a website with high accuracy. The price is that for this task we need to provide DIADEM with extensive knowledge about the ontology and phenomenology of the domain, i.e., about entities (and relations) and about the representation of these entities in the textual, structural, and visual language of a website of this domain. In this demonstration, we demonstrate with a first prototype of DIADEM that, in contrast to alchemists, DIADEM has developed a viable formula.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "1684745",
                    "name": "G. Gottlob"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "3347054",
                    "name": "Omer Gunes"
                },
                {
                    "authorId": "2146371097",
                    "name": "Xiaonan Guo"
                },
                {
                    "authorId": "48322444",
                    "name": "A. Kravchenko"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                },
                {
                    "authorId": "34565417",
                    "name": "A. Sellers"
                },
                {
                    "authorId": "2119128164",
                    "name": "Cheng Wang"
                }
            ]
        },
        {
            "paperId": "b209b90a2978db1391f10c41c28856226a015266",
            "title": "Visual oXPath: robust wrapping by example",
            "abstract": "Good examples are hard to find, particularly in wrapper induction: Picking even one wrong example can spell disaster by yielding overgeneralized or overspecialized wrappers. Such wrappers extract data with low precision or recall, unless adjusted by human experts at significant cost. Visual OXPath is an open-source, visual wrapper induction system that requires minimal examples and eases wrapper refinement: Often it derives the intended wrapper from a single example through sophisticated heuristics that determine the best set of similar examples. To ease wrapper refinement, it offers a list of wrappers ranked by example similarity and robustness. Visual OXPath offers extensive visual feedback for this refinement which can be performed without any knowledge of the underlying wrapper language. Where further refinement by a human wrapper is needed, Visual OXPath profits from being based on OXPath, a declarative wrapper language that extends XPath with a thin layer of features necessary for extraction and page navigation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2539815",
                    "name": "Jochen Kranzdorf"
                },
                {
                    "authorId": "34565417",
                    "name": "A. Sellers"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                },
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                }
            ]
        },
        {
            "paperId": "b43b277c70fd4ea37e4df9b3320a797e7ee7cb7a",
            "title": "Automatically learning gazetteers from the deep web",
            "abstract": "Wrapper induction faces a dilemma: To reach web scale, it requires automatically generated examples, but to produce accurate results, these examples must have the quality of human annotations. We resolve this conflict with AMBER, a system for fully automated data extraction from result pages. In contrast to previous approaches, AMBER employs domain specific gazetteers to discern basic domain attributes on a page, and leverages repeated occurrences of similar attributes to group related attributes into records rather than relying on the noisy structure of the DOM. With this approach AMBER is able to identify records and their attributes with almost perfect accuracy (>98%) on a large sample of websites. To make such an approach feasible at scale, AMBER automatically learns domain gazetteers from a small seed set. In this demonstration, we show how AMBER uses the repeated structure of records on deep web result pages to learn such gazetteers. This is only possible with a highly accurate extraction system. Depending on its parametrization, this learning process runs either fully automatically or with human interaction. We show how AMBER bootstraps a gazetteer for UK locations in 4 iterations: From a small seed sample we achieve 94.4% accuracy in recognizing UK locations in the $4th$ iteration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                },
                {
                    "authorId": "2119128164",
                    "name": "Cheng Wang"
                }
            ]
        },
        {
            "paperId": "e52b6cc213e26e011a343e351d9a42b2912e2ec3",
            "title": "Think before you Act! Minimising Action Execution in Wrappers",
            "abstract": "Web wrappers access databases hidden in the deep web by first interacting with web sites by, e.g., filling forms or clicking buttons, to extract the relevant data from the thus unearthed result pages. Though the (semi-)automatic induction and maintenance of such wrappers has been extensively studied, the efficient execution and optimization of wrappers has seen far less attention. We demonstrate that static and adaptive optimisation techniques, as used for query languages, significantly improve the wrapper execution performance. At the same time, we highlight difference between wrapper optimisation and common query optimisation for databases: (1) The runtime of wrappers is entirely dominated by page loads, while other operations (such as querying DOMs) have almost no impact, requiring a new cost model to guide the optimisation. (2) While adaptive query planning is otherwise often considered inessential, wrappers need to be optimised during runtime, since crucial information on the structure of the visited pages becomes only accessible at runtime. We introduce two basic, but highly effective optimisation techniques, one static, one adaptive, and show that they can easily cut wrapper evaluation time by one order of magnitude. We demonstrate our approach with wrappers specified in OXPath.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                },
                {
                    "authorId": "34565417",
                    "name": "A. Sellers"
                },
                {
                    "authorId": "1932064",
                    "name": "Antonino Rullo"
                }
            ]
        },
        {
            "paperId": "034bdaea6ad924792d874dd237c62b164801cbf3",
            "title": "Real understanding of real estate forms",
            "abstract": "Finding an apartment is a lengthy and tedious process. Once decided, one can never be sure not to have missed an even better offer which would have been just one click away. Form understanding is key to automatically access and process all the relevant---and nowadays readily available---data.\n We introduce opal (ontology-based web pattern analysis with logic), a novel, purely logical approach to web form understanding: opal labels, structures, and groups form fields according to a domain-specific ontology linked through phenomenological rules to a logical representation of a DOM. The phenomenological rules describe how ontological concepts appear on the web; the ontology formalizes and structures common patterns of web pages observed in a domain. A unique feature of opal is that all domain-independent assumptions about web forms are represented in rules, whereas domain-specific assumptions are represented in the ontology. This yields a coherent logical framework, robust in face of changing web trends.\n We apply opal to a significant, randomly selected sample of UK real estate sites, showing that straightforward rules suffice to achieve high precision form understanding.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "1684745",
                    "name": "G. Gottlob"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "2146371097",
                    "name": "Xiaonan Guo"
                },
                {
                    "authorId": "35203250",
                    "name": "G. Orsi"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                }
            ]
        },
        {
            "paperId": "0b1e17feb5a82c505469937375a721be50d3a412",
            "title": "Team-building with answer set programming in the Gioia-Tauro seaport",
            "abstract": "Abstract The seaport of Gioia Tauro is the largest transshipment terminal of the Mediterranean coast. A crucial management task for the companies operating in the seaport is team-building: the problem of properly allocating the available personnel for serving the incoming ships. Teams have to be carefully arranged in order to meet several constraints, such as allocation of employees with appropriate skills, fair distribution of the working load, and turnover of the heavy/dangerous roles. This makes team-building a hard and expensive task requiring several hours of manual preparation per day. In this paper we present a system based on Answer Set Programming for the automatic generation of the teams of employees in the seaport of Gioia Tauro. The system is currently exploited in the Gioia Tauro seaport by ICO BLG, a company specialized in automobile logistics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17858580",
                    "name": "F. Ricca"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "1775574",
                    "name": "Mario Alviano"
                },
                {
                    "authorId": "2061026012",
                    "name": "M. Manna"
                },
                {
                    "authorId": "144515265",
                    "name": "V. Lio"
                },
                {
                    "authorId": "6077755",
                    "name": "S. Iiritano"
                },
                {
                    "authorId": "144491891",
                    "name": "N. Leone"
                }
            ]
        },
        {
            "paperId": "67ea8a15e0dbaae897611f6694e0d1835717ef38",
            "title": "OXPath",
            "abstract": "The evolution of the web has outpaced itself: The growing wealth of information and the increasing sophistication of interfaces necessitate automated processing. Web automation and extraction technologies have been overwhelmed by this very growth. To address this trend, we identify four key requirements of web extraction: (1) Interact with sophisticated web application interfaces, (2) Precisely capture the relevant data for most web extraction tasks, (3) Scale with the number of visited pages, and (4) Readily embed into existing web technologies. We introduce OXPath, an extension of XPath for interacting with web applications and for extracting information thus revealed. It addresses all the above requirements. OXPath's page-at-a-time evaluation guarantees memory use independent of the number of visited pages, yet remains polynomial in time. We validate experimentally the theoretical complexity and demonstrate that its evaluation is dominated by the page rendering of the underlying browser. Our experiments show that OXPath outperforms existing commercial and academic data extraction tools by a wide margin. OX-Path is available under an open source license.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "1684745",
                    "name": "G. Gottlob"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                },
                {
                    "authorId": "34565417",
                    "name": "A. Sellers"
                }
            ]
        },
        {
            "paperId": "950195f451bef41080a00e8898fbeee57ed76c5c",
            "title": "Exploring the web with OXPath",
            "abstract": "OXPath is a careful extension of XPath that facilitates data extraction from the deep web. It is designed to facilitate the large-scale extraction of data from sophisticated modern web interfaces with client-side scripting and asynchronous server communication. Its main characteristics are (1) a minimal extension of XPath to allow page navigation and action execution, (2) a set-theoretic formal semantics for full OXPath, (3) and a sophisticated memory management that minimizes page buffering. In this poster, we briefly review the main features of the language and discuss ongoing and future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "2323045768",
                    "name": "Georg Gottlob"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                },
                {
                    "authorId": "34565417",
                    "name": "A. Sellers"
                }
            ]
        },
        {
            "paperId": "9a9208242200afdbc51b8591eaebd00f479761ca",
            "title": "Taking the OXPath down the deep web",
            "abstract": "Although deep web analysis has been studied extensively, there is no succinct formalism to describe user interactions with AJAX-enabled web applications.\n Toward this end, we introduce OXPath as a superset of XPath 1.0. Beyond XPath, OXPath is able (1) to fill web forms and trigger DOM events, (2) to access dynamically computed CSS attributes, (3) to navigate between visible form fields, and (4) to mark relevant information for extraction. This way, OXPath expressions can closely simulate the human interaction relevant for navigation rather than rely exclusively on the HTML structure. Thus, they are quite resilient against technical changes.\n We demonstrate the expressiveness and practical efficacy of OXPath to tackle a group flight planning problem. We use the OXPath implementation and visual interface to access the popular, highly-scripted travel site Kayak. We show, how to formulate OXPath expressions to extract all booking information with just a few lines of code.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34565417",
                    "name": "A. Sellers"
                },
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "1684745",
                    "name": "G. Gottlob"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                }
            ]
        },
        {
            "paperId": "fda15a2290ea4355a0807c3786c8796109238a89",
            "title": "OXPath: little language, little memory, great value",
            "abstract": "Data about everything is readily available on the web-but often only accessible through elaborate user interactions. For automated decision support, extracting that data is essential, but infeasible with existing heavy-weight data extraction systems. In this demonstration, we present OXPath, a novel approach to web extraction, with a system that supports informed job selection and integrates information from several different web sites. By carefully extending XPath, OXPath exploits its familiarity and provides a light-weight interface, which is easy to use and embed. We highlight how OXPath guarantees optimal page buffering, storing only a constant number of pages for non-recursive queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34565417",
                    "name": "A. Sellers"
                },
                {
                    "authorId": "2874808",
                    "name": "Tim Furche"
                },
                {
                    "authorId": "1684745",
                    "name": "G. Gottlob"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "1766300",
                    "name": "C. Schallhart"
                }
            ]
        },
        {
            "paperId": "d1e869962bf05687d6fe68dac95ea6e8e82b2ff1",
            "title": "A Logic-Based System for e-Tourism",
            "abstract": "In this paper we present a successful application of logic programming for e-tourism: the iTravel system. The system exploits two technologies that are based on the state-of-the-art computational logic system DLV: (i) a system for ontology representation and reasoning, called OntoDLV; and, (ii) H$\\dotlessi$LeX a semantic information-extraction tool. The core of iTravel is an ontology which models the domain of tourism offers. The ontology is automatically populated by extracting the information contained in the tourism leaflets produced by tour operators. A set of specifically devised logic programs is used to reason on the information contained in the ontology for selecting the holiday packages that best fit the customer needs. An intuitive web-based user interface eases the task of interacting with the system for both the customers and the operators of a travel agency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17858580",
                    "name": "F. Ricca"
                },
                {
                    "authorId": "2642662",
                    "name": "Antonella Dimasi"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "2272345",
                    "name": "S. M. Ielpa"
                },
                {
                    "authorId": "6077755",
                    "name": "S. Iiritano"
                },
                {
                    "authorId": "1876664",
                    "name": "M. Manna"
                },
                {
                    "authorId": "144491891",
                    "name": "N. Leone"
                }
            ]
        },
        {
            "paperId": "7b6ab936c10b44f61a2723708c7bed831cbbd7a6",
            "title": "OntoDLV: An ASP-based System for Enterprise Ontologies",
            "abstract": "Enterprise/Corporate ontologies are widely adopted to conceptualize business enterprise information. In this area, the semantic peculiarities of Answer Set Programming (ASP), like the Closed World Assumption (CWA) and the Unique Name Assumption (UNA), are more appropriate than the Ontology Web Language (OWL) assumptions, also because such ontologies frequently stem from relational databases, where both CWA and UNA are adopted. This article presents OntoDLV, a system based on ASP for the specification and reasoning on enterprise ontologies. OntoDLV implements a powerful ontology representation language, called OntoDLP, extending (disjunctive) ASP with all the main ontology features including classes, inheritance, relations and axioms. OntoDLP is strongly typed, and includes also complex type constructors, like lists and sets. Importantly, OntoDLV supports a powerful interoperability mechanism with OWL, allowing the user to retrieve information from OWL ontologies, and build rule-based reasoning on top of OWL ontologies. The system is already used in a number of real-world applications including agent-based systems, information extraction, and text classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17858580",
                    "name": "F. Ricca"
                },
                {
                    "authorId": "2124888",
                    "name": "L. Gallucci"
                },
                {
                    "authorId": "2746504",
                    "name": "Roman Schindlauer"
                },
                {
                    "authorId": "1404583773",
                    "name": "Tina Dell'Armi"
                },
                {
                    "authorId": "24604135",
                    "name": "G. Grasso"
                },
                {
                    "authorId": "144491891",
                    "name": "N. Leone"
                }
            ]
        }
    ]
}