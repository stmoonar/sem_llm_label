{
    "authorId": "1705260",
    "papers": [
        {
            "paperId": "028d75496e51943f52c7b2177344a3c089c18058",
            "title": "Fine-grained Hallucination Detection and Editing for Language Models",
            "abstract": "Large language models (LMs) are prone to generate factual errors, which are often called hallucinations. In this paper, we introduce a comprehensive taxonomy of hallucinations and argue that hallucinations manifest in diverse forms, each requiring varying degrees of careful assessments to verify factuality. We propose a novel task of automatic fine-grained hallucination detection and construct a new evaluation benchmark, FavaBench, that includes about one thousand fine-grained human judgments on three LM outputs across various domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B) exhibit diverse types of hallucinations in the majority of their outputs in information-seeking scenarios. We train FAVA, a retrieval-augmented LM by carefully creating synthetic data to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that FAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination detection, and edits suggested by FAVA improve the factuality of LM-generated text.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2279677197",
                    "name": "Abhika Mishra"
                },
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                },
                {
                    "authorId": "2257032956",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                }
            ]
        },
        {
            "paperId": "341da3f8af6edd31edd8f5a3d9452957aeaaa744",
            "title": "Tur[k]ingBench: A Challenge Benchmark for Web Agents",
            "abstract": "Can advanced multi-modal models effectively tackle complex web-based tasks? Such tasks are often found on crowdsourcing platforms, where crowdworkers engage in challenging micro-tasks within web-based environments. Building on this idea, we present TurkingBench, a benchmark consisting of tasks presented as web pages with textual instructions and multi-modal contexts. Unlike previous approaches that rely on artificially synthesized web pages, our benchmark uses natural HTML pages originally designed for crowdsourcing workers to perform various annotation tasks. Each task's HTML instructions are instantiated with different values derived from crowdsourcing tasks, creating diverse instances. This benchmark includes 32.2K instances spread across 158 tasks. To support the evaluation of TurkingBench, we have developed a framework that links chatbot responses to actions on web pages (e.g., modifying a text box, selecting a radio button). We assess the performance of cutting-edge private and open-source models, including language-only and vision-language models (such as GPT4 and InternVL), on this benchmark. Our results show that while these models outperform random chance, there is still significant room for improvement. We hope that this benchmark will drive progress in the evaluation and development of web-based agents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2292213227",
                    "name": "Kevin Xu"
                },
                {
                    "authorId": "2156538832",
                    "name": "Yeganeh Kordi"
                },
                {
                    "authorId": "2187060946",
                    "name": "Kate Sanders"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2292197723",
                    "name": "Adam Byerly"
                },
                {
                    "authorId": "2167508843",
                    "name": "Jingyu (Jack) Zhang"
                },
                {
                    "authorId": "2292194313",
                    "name": "Benjamin Van Durme"
                },
                {
                    "authorId": "1783281",
                    "name": "Daniel Khashabi"
                }
            ]
        },
        {
            "paperId": "65b5c132a90b66d6b21f0672abbe9eba5f9c63cb",
            "title": "Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback",
            "abstract": "Learning from preference feedback has emerged as an essential step for improving the generation quality and performance of modern language models (LMs). Despite its widespread use, the way preference-based learning is applied varies wildly, with differing data, learning algorithms, and evaluations used, making disentangling the impact of each aspect difficult. In this work, we identify four core aspects of preference-based learning: preference data, learning algorithm, reward model, and policy training prompts, systematically investigate the impact of these components on downstream model performance, and suggest a recipe for strong learning for preference feedback. Our findings indicate that all aspects are important for performance, with better preference data leading to the largest improvements, followed by the choice of learning algorithm, the use of improved reward models, and finally the use of additional unlabeled prompts for policy training. Notably, PPO outperforms DPO by up to 2.5% in math and 1.2% in general domains. High-quality preference data leads to improvements of up to 8% in instruction following and truthfulness. Despite significant gains of up to 5% in mathematical evaluation when scaling up reward models, we surprisingly observe marginal improvements in other categories. We publicly release the code used for training (https://github.com/hamishivi/EasyLM) and evaluating (https://github.com/allenai/open-instruct) our models, along with the models and datasets themselves (https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056776606",
                    "name": "Hamish Ivison"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2144174497",
                    "name": "Jiacheng Liu"
                },
                {
                    "authorId": "7806955",
                    "name": "Zeqiu Wu"
                },
                {
                    "authorId": "22330666",
                    "name": "Valentina Pyatkin"
                },
                {
                    "authorId": "2267244197",
                    "name": "Nathan Lambert"
                },
                {
                    "authorId": "2292425227",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2253903625",
                    "name": "Yejin Choi"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                }
            ]
        },
        {
            "paperId": "944b983ee059503c53afef772052d065d662527c",
            "title": "Set the Clock: Temporal Alignment of Pretrained Language Models",
            "abstract": "Language models (LMs) are trained on web text originating from many points in time and, in general, without any explicit temporal grounding. This work investigates the temporal chaos of pretrained LMs and explores various methods to align their internal knowledge to a target time, which we call\"temporal alignment.\"To do this, we first automatically construct a dataset containing 20K time-sensitive questions and their answers for each year from 2000 to 2023. Based on this dataset, we empirically show that pretrained LMs (e.g., LLaMa2), despite having a recent pretraining cutoff (e.g., 2022), mostly answer questions using earlier knowledge (e.g., in 2019). We then develop several methods, from prompting to finetuning, to align LMs to use their most recent knowledge when answering questions, and investigate various factors in this alignment. Our experiments demonstrate that aligning LLaMa2 to the year 2022 can enhance its performance by up to 62% according to that year's answers. This improvement occurs even without explicitly mentioning time information, indicating the possibility of aligning models' internal sense of time after pretraining. Finally, we find that alignment to a historical time is also possible, with up to 2.8$\\times$ the performance of the unaligned LM in 2010 if finetuning models to that year. These findings hint at the sophistication of LMs' internal knowledge organization and the necessity of tuning them properly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280917335",
                    "name": "Bowen Zhao"
                },
                {
                    "authorId": "2210989402",
                    "name": "Zander Brumbaugh"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                },
                {
                    "authorId": "2268796196",
                    "name": "Noah A. Smith"
                }
            ]
        },
        {
            "paperId": "ac45bbf9940512d9d686cf8cd3a95969bc313570",
            "title": "OLMo: Accelerating the Science of Language Models",
            "abstract": "Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, we have built OLMo, a competitive, truly Open Language Model, to enable the scientific study of language models. Unlike most prior efforts that have only released model weights and inference code, we release OLMo alongside open training data and training and evaluation code. We hope this release will empower the open research community and inspire a new wave of innovation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3458736",
                    "name": "Dirk Groeneveld"
                },
                {
                    "authorId": "2260133345",
                    "name": "Iz Beltagy"
                },
                {
                    "authorId": "2158819969",
                    "name": "Pete Walsh"
                },
                {
                    "authorId": "2166136235",
                    "name": "Akshita Bhagia"
                },
                {
                    "authorId": "2282136328",
                    "name": "Rodney Kinney"
                },
                {
                    "authorId": "3385516",
                    "name": "Oyvind Tafjord"
                },
                {
                    "authorId": "47286118",
                    "name": "A. Jha"
                },
                {
                    "authorId": "2056776606",
                    "name": "Hamish Ivison"
                },
                {
                    "authorId": "2124977543",
                    "name": "Ian Magnusson"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2259924223",
                    "name": "Shane Arora"
                },
                {
                    "authorId": "2282136757",
                    "name": "David Atkinson"
                },
                {
                    "authorId": "2202417686",
                    "name": "Russell Authur"
                },
                {
                    "authorId": "37619618",
                    "name": "Khyathi Raghavi Chandu"
                },
                {
                    "authorId": "2527954",
                    "name": "Arman Cohan"
                },
                {
                    "authorId": "2282136556",
                    "name": "Jennifer Dumas"
                },
                {
                    "authorId": "51131518",
                    "name": "Yanai Elazar"
                },
                {
                    "authorId": "2261456046",
                    "name": "Yuling Gu"
                },
                {
                    "authorId": "2689239",
                    "name": "Jack Hessel"
                },
                {
                    "authorId": "2236429",
                    "name": "Tushar Khot"
                },
                {
                    "authorId": "2282138468",
                    "name": "William Merrill"
                },
                {
                    "authorId": "2146964035",
                    "name": "Jacob Daniel Morrison"
                },
                {
                    "authorId": "2037383772",
                    "name": "Niklas Muennighoff"
                },
                {
                    "authorId": "23175870",
                    "name": "Aakanksha Naik"
                },
                {
                    "authorId": "2282136595",
                    "name": "Crystal Nam"
                },
                {
                    "authorId": "2267244582",
                    "name": "Matthew E. Peters"
                },
                {
                    "authorId": "22330666",
                    "name": "Valentina Pyatkin"
                },
                {
                    "authorId": "3023068",
                    "name": "Abhilasha Ravichander"
                },
                {
                    "authorId": "2264248042",
                    "name": "Dustin Schwenk"
                },
                {
                    "authorId": "2282190660",
                    "name": "Saurabh Shah"
                },
                {
                    "authorId": "2282155558",
                    "name": "Will Smith"
                },
                {
                    "authorId": "2268272",
                    "name": "Emma Strubell"
                },
                {
                    "authorId": "34202134",
                    "name": "Nishant Subramani"
                },
                {
                    "authorId": "52193502",
                    "name": "Mitchell Wortsman"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                },
                {
                    "authorId": "2267244197",
                    "name": "Nathan Lambert"
                },
                {
                    "authorId": "46666605",
                    "name": "Kyle Richardson"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "34176020",
                    "name": "Jesse Dodge"
                },
                {
                    "authorId": "46258841",
                    "name": "Kyle Lo"
                },
                {
                    "authorId": "3328733",
                    "name": "Luca Soldaini"
                },
                {
                    "authorId": "2264002618",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                }
            ]
        },
        {
            "paperId": "e98392f4906cf8fc912697b8fdb808e5cef0aa71",
            "title": "Can Language Models Act as Knowledge Bases at Scale?",
            "abstract": "Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating responses to complex queries through large-scale pre-training. However, the efficacy of these models in memorizing and reasoning among large-scale structured knowledge, especially world knowledge that explicitly covers abundant factual information remains questionable. Addressing this gap, our research investigates whether LLMs can effectively store, recall, and reason with knowledge on a large scale comparable to latest knowledge bases (KBs) such as Wikidata. Specifically, we focus on three crucial aspects to study the viability: (1) the efficiency of LLMs with different sizes in memorizing the exact knowledge in the large-scale KB; (2) the flexibility of recalling the memorized knowledge in response to natural language queries; (3) the capability to infer new knowledge through reasoning. Our findings indicate that while LLMs hold promise as large-scale KBs capable of retrieving and responding with flexibility, enhancements in their reasoning capabilities are necessary to fully realize their potential.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285680921",
                    "name": "Qiyuan He"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2285204830",
                    "name": "Wenya Wang"
                }
            ]
        },
        {
            "paperId": "edd705ebe3546272b7fe952e2ed6088200adad76",
            "title": "Retrieval Head Mechanistically Explains Long-Context Factuality",
            "abstract": "Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads. We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5\\%) of the attention heads are retrieval. (3) intrinsic: retrieval heads already exist in models pretrained with short context. When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval. (4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed. The rest of the retrieval heads are activated in different contexts. (5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability. We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context. Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads. These observations collectively explain which internal part of the model seeks information from the input tokens. We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298066086",
                    "name": "Wenhao Wu"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2298042633",
                    "name": "Guangxuan Xiao"
                },
                {
                    "authorId": "2258803434",
                    "name": "Hao Peng"
                },
                {
                    "authorId": "46956602",
                    "name": "Yao Fu"
                }
            ]
        },
        {
            "paperId": "f6d7482bb5baf33f22b862ad4997f5c8cd13db21",
            "title": "Tuning Language Models by Proxy",
            "abstract": "Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the same end as direct tuning, but by accessing only its predictions over the output vocabulary, not its parameters. Our method tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the larger untuned model in the direction of tuning, while retaining the benefits of larger-scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when evaluated across knowledge, reasoning, and safety benchmarks. We then demonstrate the generality of proxy-tuning by applying it to domain adaptation on code, and task-specific finetuning on question-answering and math problems. Finally, we show how to proxy-tune a truly black-box LM, GPT-3.5, for temporal adaptation, increasing its knowledge about recent events. Our work demonstrates the promise of using small tuned LMs to efficiently customize large, potentially proprietary LMs through decoding-time guidance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261363520",
                    "name": "Alisa Liu"
                },
                {
                    "authorId": "2257023881",
                    "name": "Xiaochuang Han"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2258958466",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2259707400",
                    "name": "Yejin Choi"
                },
                {
                    "authorId": "2261399966",
                    "name": "Noah A. Smith"
                }
            ]
        },
        {
            "paperId": "1f02ba1c6fae779ec3d003340e72eaf82351cfb9",
            "title": "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering",
            "abstract": "Despite thousands of researchers, engineers, and artists actively working on improving text-to-image generation models, systems often fail to produce images that accurately align with the text inputs. We introduce TIFA (Text-to-Image Faithfulness evaluation with question Answering), an automatic evaluation metric that measures the faithfulness of a generated image to its text input via visual question answering (VQA). Specifically, given a text input, we automatically generate several question-answer pairs using a language model. We calculate image faithfulness by checking whether existing VQA models can answer these questions using the generated image. TIFA is a reference-free metric that allows for fine-grained and interpretable evaluations of generated images. TIFA also has better correlations with human judgments than existing metrics. Based on this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diverse text inputs and 25K questions across 12 categories (object, counting, etc.). We present a comprehensive evaluation of existing text-to-image models using TIFA v1.0 and highlight the limitations and challenges of current models. For instance, we find that current text-to-image models, despite doing well on color and material, still struggle in counting, spatial relations, and composing multiple objects. We hope our benchmark will help carefully measure the research progress in text-to-image synthesis and provide valuable insights for further research. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112209725",
                    "name": "Yushi Hu"
                },
                {
                    "authorId": "67215934",
                    "name": "Benlin Liu"
                },
                {
                    "authorId": "11348687",
                    "name": "Jungo Kasai"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "144339506",
                    "name": "Mari Ostendorf"
                },
                {
                    "authorId": "145237361",
                    "name": "Ranjay Krishna"
                },
                {
                    "authorId": "144365875",
                    "name": "Noah A. Smith"
                }
            ]
        },
        {
            "paperId": "37680e5cb6030e01f1a44a5abe2257972196ae26",
            "title": "Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2",
            "abstract": "Since the release of T\\\"ULU [Wang et al., 2023b], open resources for instruction tuning have developed quickly, from better base models to new finetuning techniques. We test and incorporate a number of these advances into T\\\"ULU, resulting in T\\\"ULU 2, a suite of improved T\\\"ULU models for advancing the understanding and best practices of adapting pretrained language models to downstream tasks and user preferences. Concretely, we release: (1) T\\\"ULU-V2-mix, an improved collection of high-quality instruction datasets; (2) T\\\"ULU 2, LLAMA-2 models finetuned on the V2 mixture; (3) T\\\"ULU 2+DPO, T\\\"ULU 2 models trained with direct preference optimization (DPO), including the largest DPO-trained model to date (T\\\"ULU 2+DPO 70B); (4) CODE T\\\"ULU 2, CODE LLAMA models finetuned on our V2 mix that outperform CODE LLAMA and its instruction-tuned variant, CODE LLAMA-Instruct. Our evaluation from multiple perspectives shows that the T\\\"ULU 2 suite achieves state-of-the-art performance among open models and matches or exceeds the performance of GPT-3.5-turbo-0301 on several benchmarks. We release all the checkpoints, data, training and evaluation code to facilitate future open efforts on adapting large language models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056776606",
                    "name": "Hamish Ivison"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "22330666",
                    "name": "Valentina Pyatkin"
                },
                {
                    "authorId": "2267244197",
                    "name": "Nathan Lambert"
                },
                {
                    "authorId": "2267244582",
                    "name": "Matthew E. Peters"
                },
                {
                    "authorId": "2697425",
                    "name": "Pradeep Dasigi"
                },
                {
                    "authorId": "2000091730",
                    "name": "Joel Jang"
                },
                {
                    "authorId": "30051202",
                    "name": "David Wadden"
                },
                {
                    "authorId": "2264002618",
                    "name": "Noah A. Smith"
                },
                {
                    "authorId": "2260133345",
                    "name": "Iz Beltagy"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                }
            ]
        }
    ]
}