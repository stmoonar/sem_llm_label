{
    "authorId": "2157128693",
    "papers": [
        {
            "paperId": "4576a20af20e58d461ef5179bb321be00250999c",
            "title": "Personal Attribute Prediction from Conversations",
            "abstract": "Personal knowledge bases (PKBs) are critical to many applications, such as Web-based chatbots and personalized recommendation. Conversations containing rich personal knowledge can be regarded as a main source to populate the PKB. Given a user, a user attribute, and user utterances from a conversational system, we aim to predict the personal attribute value for the user, which is helpful for the enrichment of PKBs. However, there are three issues existing in previous studies: (1) manually labeled utterances are required for model training; (2) personal attribute knowledge embedded in both utterances and external resources is underutilized; (3) the performance on predicting some difficult personal attributes is unsatisfactory. In this paper, we propose a framework DSCGN based on the pre-trained language model with a noise-robust loss function to predict personal attributes from conversations without requiring any labeled utterances. We yield two categories of supervision, i.e., document-level supervision via a distant supervision strategy and contextualized word-level supervision via a label guessing method, by mining the personal attribute knowledge embedded in both unlabeled utterances and external resources to fine-tune the language model. Extensive experiments over two real-world data sets (i.e., a profession data set and a hobby data set) show our framework obtains the best performance compared with all the twelve baselines in terms of nDCG and MRR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "119924024",
                    "name": "Yinan Liu"
                },
                {
                    "authorId": "2157128693",
                    "name": "Hu Chen"
                },
                {
                    "authorId": "144084234",
                    "name": "Wei Shen"
                }
            ]
        },
        {
            "paperId": "0216df07704f7084f097fd5b5467dc50e0f29f4b",
            "title": "Recurrent Support Vector Machines For Slot Tagging In Spoken Language Understanding",
            "abstract": "We propose recurrent support vector machine ( RSVM ) for slot tagging. This model is a combination of the recurrent neural network ( RNN ) and the structured support vector machine. RNN extracts features from the input sequence. The structured support vector machine uses a sequence-level discriminative objective function. The proposed model therefore combines the sequence representation capability of an RNN with the sequence-level discriminative objective. We have observed new state-of-the-art results on two benchmark datasets and one private dataset. RSVM obtained statistical signi\ufb01cant 4% and 2% relative average F1 score improvement on ATIS dataset and Chunking dataset, respectively. Out of eight domains in Cortana live log dataset, RSVM achieved F1 score improvement on seven domains. Experiments also show that RSVM significantly speeds up the model training by skipping the weight updating for non-support vector training samples, compared against training using RNN with CRF or minimum cross-entropy objectives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48081355",
                    "name": "Yangyang Shi"
                },
                {
                    "authorId": "39922478",
                    "name": "K. Yao"
                },
                {
                    "authorId": "2157128693",
                    "name": "Hu Chen"
                },
                {
                    "authorId": "144580027",
                    "name": "Dong Yu"
                },
                {
                    "authorId": "2216108938",
                    "name": "Yi-Cheng Pan"
                },
                {
                    "authorId": "144091892",
                    "name": "M. Hwang"
                }
            ]
        },
        {
            "paperId": "12193ef445b7b001977e27f1256625cceb8fc1a5",
            "title": "Semi-supervised slot tagging in spoken language understanding using recurrent transductive support vector machines",
            "abstract": "In this paper, we propose a recurrent transductive support vector machine (rtsvm) for semi-supervised slot tagging. Taking advantage of the superior sequence representation capability of recurrent neural networks (rnns) and the semi-supervised learning capability of transductive support vector machines (tsvms), the rtsvm is stacking a tsvm on top of a rnn. The performance of the traditional tsvm is sensitive to the regularization weight for unlabeled data in semi-supervised learning. In practice, a suitable unlabeled data regularization weight is difficult to determine. To make the rtsvm semi-supervised learning robust, we propose a confident subset regularization method enforcing that the new decision boundaries learned from unlabeled data would not separate the confident clusters learned from labeled data. The experiments based on two datasets show that without using unlabeled data, the supervised version of rtsvm achieves significant F1 score improvement over previous best methods. By taking the unlabeled data into account, the semi-supervised rtsvm gets significant improvement over its supervised opponent.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48081355",
                    "name": "Yangyang Shi"
                },
                {
                    "authorId": "39922478",
                    "name": "K. Yao"
                },
                {
                    "authorId": "2157128693",
                    "name": "Hu Chen"
                },
                {
                    "authorId": "2216108938",
                    "name": "Yi-Cheng Pan"
                },
                {
                    "authorId": "144091892",
                    "name": "M. Hwang"
                }
            ]
        },
        {
            "paperId": "4ca3f5aa4baac110afd6308f14a1ce0625b9c97c",
            "title": "A factorization network based method for multi-lingual domain classification",
            "abstract": "In many spoken language understanding systems (SLUS), domain classification is the most crucial component, as system responses based on wrong domains often yield very unpleasant user experiences. In multi-lingual domain classification, the training data for some poor-resource languages often comes from machine translation. Some of the higher order n-gram features are distorted during machine translation. Feature co-occurrence becomes reliable feature in multi-lingual domain classification. In this paper, in order to effectively model feature co-occurrences, we propose Factorization Networks that are combinations of Factorization Machines (FMs) with Neural Networks (NNs). FNs extend the linear connections from the input feature layer to the hidden layer in NNs to factorization connections that represent the weights of feature co-occurrences using factorized method. In addition to FNs, we also propose a hybrid model that integrates FNs, NNs and Maximum Entropy (ME) models together. The component models in the hybrid model share the same input features. Based on two data sets (ATIS data set and Microsoft Cortana Chinese data ), the proposed models shows promising results. Especially for large Microsoft Cortana Chinese data which is translated from well annotated English data, FNs using unigram, class and query length features achieve more than 20% relative error reduction over linear (SVMs).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48081355",
                    "name": "Yangyang Shi"
                },
                {
                    "authorId": "2216108938",
                    "name": "Yi-Cheng Pan"
                },
                {
                    "authorId": "144091892",
                    "name": "M. Hwang"
                },
                {
                    "authorId": "39922478",
                    "name": "K. Yao"
                },
                {
                    "authorId": "2157128693",
                    "name": "Hu Chen"
                },
                {
                    "authorId": "3103561",
                    "name": "Yuanhang Zou"
                },
                {
                    "authorId": "1780690",
                    "name": "Baolin Peng"
                }
            ]
        },
        {
            "paperId": "d017b141c90c8cff4af16b37bd2286cc66a60335",
            "title": "Contextual spoken language understanding using recurrent neural networks",
            "abstract": "We present a contextual spoken language understanding (contextual SLU) method using Recurrent Neural Networks (RNNs). Previous work has shown that context information, specifically the previously estimated domain assignment, is helpful for domain identification. We further show that other context information such as the previously estimated intent and slot labels are useful for both intent classification and slot filling tasks in SLU. We propose a step-n-gram model to extract sentence-level features from RNNs, which extract sequential features. The step-n-gram model is used together with a stack of Convolution Networks for training domain/intent classification. Our method therefore exploits possible correlations among domain/intent classification and slot filling and incorporates context information from the past predictions of domain/intent and slots. The proposed method obtains new state-of-the-art results on ATIS and improved performances over baseline techniques such as conditional random fields (CRFs) on a large context-sensitive SLU dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48081355",
                    "name": "Yangyang Shi"
                },
                {
                    "authorId": "39922478",
                    "name": "K. Yao"
                },
                {
                    "authorId": "2157128693",
                    "name": "Hu Chen"
                },
                {
                    "authorId": "2216108938",
                    "name": "Yi-Cheng Pan"
                },
                {
                    "authorId": "144091892",
                    "name": "M. Hwang"
                },
                {
                    "authorId": "1780690",
                    "name": "Baolin Peng"
                }
            ]
        }
    ]
}