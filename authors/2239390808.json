{
    "authorId": "2239390808",
    "papers": [
        {
            "paperId": "1a17c004b168f8bfd385f52da629937509bc4fda",
            "title": "Loss Compensation in Multi-Session Recommendation Under Limited Availability",
            "abstract": "In many recommendation applications, items may have limited availability thereby causing conflict among users interested in the same items. Over time, this results in unequal user treat-ment: few users are recommended the limited items and receive preferential treatment, while the rest is left with sub-optimal recommendations, ultimately leading them to leave. In this paper, we formalize the novel problem of compensating users in multi-session recommendations under limited item availability. Our aim is to generate recommendations that not only optimize accuracy, but also compensate users over time for the loss of accuracy incurred in previous iterations. We design compensation strategies that serve users and items in different orders and accommodate various recommendation adoption models. Our algorithms are integrated into SoCRATe (System for Compensating Recommendations with Availability and Time), a framework that enables us to study loss compensation over time. Our experiments on real data demonstrate that to best compensate users for the incurred loss, traditional recommenders need to be revisited to account for item availability. Our experiments on synthetic data explore different parameters of our solution and show that it is much faster than an optimal (brute-force) compensation strategy, while achieving comparable results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25867417",
                    "name": "Davide Azzalini"
                },
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "2239390808",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                },
                {
                    "authorId": "2292416780",
                    "name": "Davide Martinenghi"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "21cd449c22c858cf5e0478f2ed0597c0992e5886",
            "title": "Mitigating Unfairness in Machine Learning: A Taxonomy and an Evaluation Pipeline",
            "abstract": "Big data poses challenges in maintaining ethical standards for reliable outcomes in machine learning. Data that inaccurately represent populations may result in biased algorithmic models, whose application leads to unfair decisions in delicate fields such as medicine and industry. To address this issue, many fairness mitigation techniques have been introduced, but the proliferation of overlapping methods complicates decision-making for data scientists. This paper proposes a taxonomy to organize these techniques and a pipeline for their evaluation, supporting practitioners in selecting the most suitable ones. The taxonomy classifies and describes techniques qualitatively, while the pipeline offers a quantitative framework for evaluation and comparison. The proposed approach supports data scientists in addressing biased models and data effectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2239390808",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                },
                {
                    "authorId": "2258428",
                    "name": "Mattia Salnitri"
                }
            ]
        },
        {
            "paperId": "a4432680b8c0dba65f0f44de91156075625405a6",
            "title": "Exploring Fairness Interpretability with FairnessFriend: A Chatbot Solution",
            "abstract": "In the contemporary world, artificial intelligence and machine learning algorithms are an important driver for decision-making, by leveraging real-world data for future predictions. Despite clearly improving efficiency, the lack of transparency in their predictions raises concerns about the degree of fairness of machine learning models, well highlighted by recent instances of algorithmic unfairness, from automated decisions on criminal recidivism to disease prediction. Increased user awareness of algorithmic fairness is met with a deficiency in systems guiding data analysts and practitioners in comprehending the implications of their outputs. To tackle the challenge of fairness interpretability, we propose FairnessFriend, a chatbot solution that combines data science with a human-computer interaction perspective. Given a dataset and a trained machine learning model with established fairness metrics, our system facilitates users in understanding these metrics and their significance in the context of the training data. FairnessFriend provides meanings for various statistical fairness metrics, and presents the resulting metrics values with detailed explanations, offering specific insights into their implications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2239390808",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "2182555493",
                    "name": "Tommaso Dolci"
                }
            ]
        },
        {
            "paperId": "899aa4a3f21f4b300f3fb890fbb04efa56f1520b",
            "title": "Understanding Fairness Requirements for ML-based Software",
            "abstract": "Today's technologies are becoming more and more pervasive and advanced software systems can replace human beings in many different tasks. This is especially true in the case of automated decision-making systems based on machine learning (ML). Important ethical implications arise when such decision systems are used in sensitive contexts (e.g., justice or loans). The elicitation of these implications, that is, of the ethical requirements behind ML-based systems is a new challenge we must address to avoid societal risks. This is particularly urgent for fairness since this notion lacks a precise and commonly accepted definition, thus hampering its assessment. This paper aims to give a comprehensive definition of fairness, present a unified taxonomy of alternative interpretations, define a new decision tree that can guide the choice of the correct interpretation, and carry out a preliminary assessment with experiments in a real-world context.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2242736209",
                    "name": "Luciano Baresi"
                },
                {
                    "authorId": "2239390808",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "2248812920",
                    "name": "Carlo Ghezzi"
                }
            ]
        },
        {
            "paperId": "96ff519428c517942072f07b6fb45e7d918b486d",
            "title": "Data Quality and Fairness: Rivals or Friends?",
            "abstract": "In the last decade, data-driven decision-making is considered one of the main drivers for organizational success. Within this approach, decisions are based on insights and patterns identified through data analysis. In this scenario, input data must be reliable to guarantee the accuracy of the results: they should be correct and complete but also unbiased, i.e., both Data Quality (DQ) and Fairness should be guaranteed. However, maximizing DQ and Fairness simultaneously is not trivial, since data quality improvement techniques can negatively affect Fairness and vice versa. Understanding and thoroughly analyzing this relationship between DQ and Fairness is therefore paramount, and is this paper\u2019s goal. The results of our experiments, based on a well-known biased dataset (the Adult Census Income) provided details about this trade-off and allowed us to draw some guidelines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25919018",
                    "name": "Fabio Azzalini"
                },
                {
                    "authorId": "1723724",
                    "name": "C. Cappiello"
                },
                {
                    "authorId": "2239390808",
                    "name": "Chiara Criscuolo"
                },
                {
                    "authorId": "2239381912",
                    "name": "Sergio Cuzzucoli"
                },
                {
                    "authorId": "2239390968",
                    "name": "Alessandro Dangelo"
                },
                {
                    "authorId": "2182555347",
                    "name": "Camilla Sancricca"
                },
                {
                    "authorId": "1779997",
                    "name": "L. Tanca"
                }
            ]
        }
    ]
}