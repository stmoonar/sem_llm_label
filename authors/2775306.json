{
    "authorId": "2775306",
    "papers": [
        {
            "paperId": "790bfef4bf4dec0f86680431256652e647296166",
            "title": "Purity-Preserving Kernel Tensor Low-Rank Learning for Robust Subspace Clustering",
            "abstract": "In recent years, multi-kernel learning (MKL) methods have been widely used in performing nonlinear data subspace clustering tasks, benefiting from the fact that they do not require the selection and tuning of predefined kernels. However, the effect of raw noise on the data structure in the feature space has been neglected in most MKL studies so far. In this paper, we propose a robust subspace clustering method called purity kernel tensor low-rank learning (KTLL), which effectively isolates noise transfer from the original data space to the high-dimensional feature space. Specifically, we construct the kernel pool obtained by MKL as a primitive third-order kernel tensor, separate the corrupted information in the feature space, and use the separated pure kernel tensor to learn the optimal affinity matrix. The tensor learning of the kernel pool can effectively mine the higher-order correlations among different kernel matrices, thus improving the clustering performance of KTLL.We have conducted extensive experiments to compare KTLL with state-of-the-art MKL and deep subspace clustering algorithms, and our results demonstrate the superiority of KTLL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108113219",
                    "name": "Xiaoqian Zhang"
                },
                {
                    "authorId": "2225643470",
                    "name": "Shuai Zhao"
                },
                {
                    "authorId": "2152451091",
                    "name": "Jing Wang"
                },
                {
                    "authorId": "2110765963",
                    "name": "Li Guo"
                },
                {
                    "authorId": "144129720",
                    "name": "Xiao Wang"
                },
                {
                    "authorId": "2775306",
                    "name": "Huaijiang Sun"
                }
            ]
        },
        {
            "paperId": "22c613a7b13d3fe8fb9d3d354955d3336864011b",
            "title": "Test-time Personalizable Forecasting of 3D Human Poses",
            "abstract": "Current motion forecasting approaches typically train a deep end-to-end model from the source domain data, and then apply it directly to target subjects. Despite promising results, they remain non-optimal, due to privacy considerations, the test person and his/her natural properties (e.g., behavioral trait) are typically unseen in training. In this case, the source pre-trained model has a low ability to adapt to these out-of-source characteristics, resulting in an unreliable prediction. To tackle this issue, we propose a novel helper-predictor test-time personalization approach (H/P-TTP), which allows for a generalizable representation of out-of-source subjects to gain more realistic predictions. Concretely, the helper is preceded by explicit and implicit augmenters, where the former yields noisy sequences to improve robustness, while the latter is to generate novel-domain data with an adversarial learning paradigm. Then, the domain-generalizable learning is achieved where the helper can extract cross-subject invariant-knowledge to update the predictor. At test time, given a new person, the predictor is able to be further optimized to empower personalized capabilities to the specific properties. Extensive experiments show that with H/P-TTP, the existing models are significantly improved for various unseen subjects. The project page is available at https://sites.google.com/view/hp-ttp.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66290737",
                    "name": "Qiongjie Cui"
                },
                {
                    "authorId": "2775306",
                    "name": "Huaijiang Sun"
                },
                {
                    "authorId": "2152962838",
                    "name": "Jian-Zhou Lu"
                },
                {
                    "authorId": "2180311317",
                    "name": "Weiqing Li"
                },
                {
                    "authorId": "2180121880",
                    "name": "Bin Li"
                },
                {
                    "authorId": "2274104587",
                    "name": "Hongwei Yi"
                },
                {
                    "authorId": "2275529051",
                    "name": "Haofan Wang"
                }
            ]
        },
        {
            "paperId": "390a927e0c70bdc3e3a327dc8f18a0a90b9f82b9",
            "title": "Meta-Auxiliary Learning for Adaptive Human Pose Prediction",
            "abstract": "Predicting high-fidelity future human poses, from a historically observed sequence, is crucial for intelligent robots to interact with humans. Deep end-to-end learning approaches, which typically train a generic pre-trained model on external datasets and then directly apply it to all test samples, emerge as the dominant solution to solve this issue. Despite encouraging progress, they remain non-optimal, as the unique properties (e.g., motion style, rhythm) of a specific sequence cannot be adapted. More generally, once encountering out-of-distributions, the predicted poses tend to be unreliable. Motivated by this observation, we propose a novel test-time adaptation framework that leverages two self-supervised auxiliary tasks to help the primary forecasting network adapt to the test sequence. In the testing phase, our model can adjust the model parameters by several gradient updates to improve the generation quality. However, due to catastrophic forgetting, both auxiliary tasks typically have a low ability to automatically present the desired positive incentives for the final prediction performance. For this reason, we also propose a meta-auxiliary learning scheme for better adaptation. Extensive experiments show that the proposed approach achieves higher accuracy and more realistic visualization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "66290737",
                    "name": "Qiongjie Cui"
                },
                {
                    "authorId": "2775306",
                    "name": "Huaijiang Sun"
                },
                {
                    "authorId": "145313246",
                    "name": "Jianfeng Lu"
                },
                {
                    "authorId": "2180121880",
                    "name": "Bin Li"
                },
                {
                    "authorId": "2180311317",
                    "name": "Weiqing Li"
                }
            ]
        },
        {
            "paperId": "3c56d501c4c380a0fb96e44c9a78df5e54092630",
            "title": "Res-LGAN for human motion style transfer with more features preserved",
            "abstract": "\u00a0Human motion style transfer is a technique that aims to apply a desired style to neutral motions, which is an essential aspect of motion generation and retargeting. With the advancement of deep learning networks, significant progress has been made in this field. However, one of the main challenges is preserving the essential features of the original motions, such as velocities and trace, during the style transfer process. To overcome this challenge, we have proposed a novel method called Residual LSTM Generative Adversarial Networks (Res-LGAN) for motion style transfer. The Res-LGAN models consist of a transfer network and a refinement network, which work together to generate smooth and natural stylized motions while preserving key features of the original motions. Additionally, we have introduced a reconstruction loss term to ensure the stylized motions closely retain the features of the original motions. Our experiments demonstrate that the proposed Res-LGAN model outperforms existing state-of-the-art models by generating high-quality stylized motions while preserving the original motion features. To the best of our knowledge, Res-LGAN is the leading method for preserving original content features during motion style transferring.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2000385873",
                    "name": "Yue Kong"
                },
                {
                    "authorId": "2775306",
                    "name": "Huaijiang Sun"
                },
                {
                    "authorId": "66290737",
                    "name": "Qiongjie Cui"
                },
                {
                    "authorId": "2122296021",
                    "name": "Jian Pan"
                },
                {
                    "authorId": "2110481810",
                    "name": "Yanmeng Li"
                }
            ]
        },
        {
            "paperId": "8eff8c7784c4f9f43377013f14b68aac6c3f1558",
            "title": "Enhanced Fine-Grained Motion Diffusion for Text-Driven Human Motion Synthesis",
            "abstract": "The emergence of text-driven motion synthesis technique provides animators with great potential to create efficiently. However, in most cases, textual expressions only contain general and qualitative motion descriptions, while lack fine depiction and sufficient intensity, leading to the synthesized motions that either (a) semantically compliant but uncontrollable over specific pose details, or (b) even deviates from the provided descriptions, bringing animators with undesired cases. In this paper, we propose DiffKFC, a conditional diffusion model for text-driven motion synthesis with KeyFrames Collaborated, enabling realistic generation with collaborative and efficient dual-level control: coarse guidance at semantic level, with only few keyframes for direct and fine-grained depiction down to body posture level. Unlike existing inference-editing diffusion models that incorporate conditions without training, our conditional diffusion model is explicitly trained and can fully exploit correlations among texts, keyframes and the diffused target frames. To preserve the control capability of discrete and sparse keyframes, we customize dilated mask attention modules where only partial valid tokens participate in local-to-global attention, indicated by the dilated keyframe mask. Additionally, we develop a simple yet effective smoothness prior, which steers the generated frames towards seamless keyframe transitions at inference. Extensive experiments show that our model not only achieves state-of-the-art performance in terms of semantic fidelity, but more importantly, is able to satisfy animator requirements through fine-grained guidance without tedious labor.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187584535",
                    "name": "Dong Wei"
                },
                {
                    "authorId": "2145038916",
                    "name": "Xiaoning Sun"
                },
                {
                    "authorId": "2775306",
                    "name": "Huaijiang Sun"
                },
                {
                    "authorId": "2122826829",
                    "name": "Sheng-liang Hu"
                },
                {
                    "authorId": "2180121880",
                    "name": "Bin Li"
                },
                {
                    "authorId": "2180311317",
                    "name": "Weiqing Li"
                },
                {
                    "authorId": "2152962838",
                    "name": "Jian-Zhou Lu"
                }
            ]
        },
        {
            "paperId": "dd4a51f7305751df661a94f9be8f462ad4a72ecb",
            "title": "DeFeeNet: Consecutive 3D Human Motion Prediction with Deviation Feedback",
            "abstract": "Let us rethink the real-world scenarios that require human motion prediction techniques, such as human-robot collaboration. Current works simplify the task of predicting human motions into a one-off process of forecasting a short future sequence (usually no longer than 1 second) based on a historical observed one. However, such simplification may fail to meet practical needs due to the neglect of the fact that motion prediction in real applications is not an isolated \u201cobserve then predict\u201d unit, but a consecutive process composed of many rounds of such unit, semi-overlapped along the entire sequence. As time goes on, the predicted part of previous round has its corresponding ground truth observable in the new round, but their deviation in-between is neither exploited nor able to be captured by existing isolated learning fashion. In this paper, we propose DeFeeNet, a simple yet effective network that can be added on existing one-off prediction models to realize deviation perception and feedback when applied to consecutive motion prediction task. At each prediction round, the deviation generated by previous unit is first encoded by our DeFeeNet, and then incorporated into the existing predictor to enable a deviation-aware prediction manner, which, for the first time, allows for information transmit across adjacent prediction units. We design two versions of DeFeeNet as MLP-based and GRU-based, respectively. On Human3.6M and more complicated BABEL, experimental results indicate that our proposed network improves consecutive human motion prediction performance regardless of the basic model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145038916",
                    "name": "Xiaoning Sun"
                },
                {
                    "authorId": "2775306",
                    "name": "Huaijiang Sun"
                },
                {
                    "authorId": "2180121880",
                    "name": "Bin Li"
                },
                {
                    "authorId": "2090458756",
                    "name": "Dong Wei"
                },
                {
                    "authorId": "2180311317",
                    "name": "Weiqing Li"
                },
                {
                    "authorId": "145313246",
                    "name": "Jianfeng Lu"
                }
            ]
        },
        {
            "paperId": "11ebc69f40d44a49f65fb4f53891bff2cfddad01",
            "title": "Overlooked Poses Actually Make Sense: Distilling Privileged Knowledge for Human Motion Prediction",
            "abstract": "Previous works on human motion prediction follow the pattern of building a mapping relation between the sequence observed and the one to be predicted. However, due to the inherent complexity of multivariate time series data, it still remains a challenge to find the extrapolation relation between motion sequences. In this paper, we present a new prediction pattern, which introduces previously overlooked human poses, to implement the prediction task from the view of interpolation. These poses exist after the predicted sequence, and form the privileged sequence. To be specific, we first propose an InTerPolation learning Network (ITP-Network) that encodes both the observed sequence and the privileged sequence to interpolate the in-between predicted sequence, wherein the embedded Privileged-sequence-Encoder (Priv-Encoder) learns the privileged knowledge (PK) simultaneously. Then, we propose a Final Prediction Network (FP-Network) for which the privileged sequence is not observable, but is equipped with a novel PK-Simulator that distills PK learned from the previous network. This simulator takes as input the observed sequence, but approximates the behavior of Priv-Encoder, enabling FP-Network to imitate the interpolation process. Extensive experimental results demonstrate that our prediction pattern achieves state-of-the-art performance on benchmarked H3.6M, CMU-Mocap and 3DPW datasets in both short-term and long-term predictions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145038916",
                    "name": "Xiaoning Sun"
                },
                {
                    "authorId": "66290737",
                    "name": "Qiongjie Cui"
                },
                {
                    "authorId": "2775306",
                    "name": "Huaijiang Sun"
                },
                {
                    "authorId": "2180121880",
                    "name": "Bin Li"
                },
                {
                    "authorId": "2180311317",
                    "name": "Weiqing Li"
                },
                {
                    "authorId": "2152960206",
                    "name": "Jianfeng Lu"
                }
            ]
        },
        {
            "paperId": "34f622243804ba15e29f1a22a8898eb4cf33772d",
            "title": "Human Joint Kinematics Diffusion-Refinement for Stochastic Motion Prediction",
            "abstract": "Stochastic human motion prediction aims to forecast multiple plausible future motions given a single pose sequence from the past. Most previous works focus on designing elaborate losses to improve the accuracy, while the diversity is typically characterized by randomly sampling a set of latent variables from the latent prior, which is then decoded into possible motions. This joint training of sampling and decoding, however, suffers from posterior collapse as the learned latent variables tend to be ignored by a strong decoder, leading to limited diversity. Alternatively, inspired by the diffusion process in nonequilibrium thermodynamics, we propose MotionDiff, a diffusion probabilistic model to treat the kinematics of human joints as heated particles, which will diffuse from original states to a noise distribution. This process not only offers a natural way to obtain the \"whitened'' latents without any trainable parameters, but also introduces a new noise in each diffusion step, both of which facilitate more diverse motions. Human motion prediction is then regarded as the reverse diffusion process that converts the noise distribution into realistic future motions conditioned on the observed sequence. Specifically, MotionDiff consists of two parts: a spatial-temporal transformer-based diffusion network to generate diverse yet plausible motions, and a flexible refinement network to further enable geometric losses and align with the ground truth. Experimental results on two datasets demonstrate that our model yields the competitive performance in terms of both diversity and accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187584535",
                    "name": "Dong Wei"
                },
                {
                    "authorId": "2775306",
                    "name": "Huaijiang Sun"
                },
                {
                    "authorId": "2180121880",
                    "name": "Bin Li"
                },
                {
                    "authorId": "145313246",
                    "name": "Jianfeng Lu"
                },
                {
                    "authorId": "2180311317",
                    "name": "Weiqing Li"
                },
                {
                    "authorId": "2145038916",
                    "name": "Xiaoning Sun"
                },
                {
                    "authorId": "2122826829",
                    "name": "Sheng-liang Hu"
                }
            ]
        },
        {
            "paperId": "53f8e84930efc6c9710ebb0090dace46a62502ed",
            "title": "Orthogonal Low-Rank Projection Learning for Robust Image Feature Extraction",
            "abstract": "Projecting the original data into a low-dimensional target space for feature extraction is a common method. Recently, presentation-based approaches have been widely concerned and many feature extraction algorithms based on this have been proposed. However, in the process of acquiring real data, the pollution of complex noise cannot always be avoided, which will greatly increase the difficulty of feature extraction and even lead to failed feature extraction results. Thus, a robust image feature extraction model based on Orthogonal Low-Rank Projection Learning (OLRPL) is proposed, in which the introduction of orthogonal matrix can encourage the preservation of the main components of the sample. Particularly, the row sparsity constraint introduced on the projection matrix can encourage the features to be more compact, discriminative and interpretable. In particular, the Weighted Truncated Schatten p-norm (WTSN) is proposed to better solve the optimization problem of low-rank constraints. At the same time, the correntropy is applied in OLRPL to suppress the complex noise in the data and thus improve the robustness of the model. Finally, we specially design a robust classification loss function so that our model can be fitted the supervised scene effectively. Experiments on five general databases have proved that OLRPL has better effectiveness and robustness than existing advanced methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108113219",
                    "name": "Xiaoqian Zhang"
                },
                {
                    "authorId": "151478693",
                    "name": "Zhengtao Tan"
                },
                {
                    "authorId": "2775306",
                    "name": "Huaijiang Sun"
                },
                {
                    "authorId": "2145053072",
                    "name": "Zungang Wang"
                },
                {
                    "authorId": "2058195288",
                    "name": "Mingwei Qin"
                }
            ]
        },
        {
            "paperId": "e4773b3d0a35132a8a51180d03f4867208315b0f",
            "title": "Local Self-Expression Subspace Learning Network for Motion Capture Data",
            "abstract": "Deep subspace learning is an important branch of self-supervised learning and has been a hot research topic in recent years, but current methods do not fully consider the individualities of temporal data and related tasks. In this paper, by transforming the individualities of motion capture data and segmentation task as the supervision, we propose the local self-expression subspace learning network. Specifically, considering the temporality of motion data, we use the temporal convolution module to extract temporal features. To implement the local validity of self-expression in temporal tasks, we design the local self-expression layer which only maintains the representation relations with temporally adjacent motion frames. To simulate the interpolatability of motion data in the feature space, we impose a group sparseness constraint on the local self-expression layer to impel the representations only using selected keyframes. Besides, based on the subspace assumption, we propose the subspace projection loss, which is induced from distances of each frame projected to the fitted subspaces, to penalize the potential clustering errors. The superior performances of the proposed model on the segmentation task of synthetic data and three tasks of real motion capture data demonstrate the feature learning ability of our model.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40387793",
                    "name": "Guiyu Xia"
                },
                {
                    "authorId": "2142117541",
                    "name": "Peng Xue"
                },
                {
                    "authorId": "2775306",
                    "name": "Huaijiang Sun"
                },
                {
                    "authorId": "2108667109",
                    "name": "Yubao Sun"
                },
                {
                    "authorId": "2140414147",
                    "name": "Du Zhang"
                },
                {
                    "authorId": "48873988",
                    "name": "Qingshan Liu"
                }
            ]
        }
    ]
}