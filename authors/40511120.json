{
    "authorId": "40511120",
    "papers": [
        {
            "paperId": "509d4c6778baf9d42abd86e56be69488804ddeda",
            "title": "Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey)",
            "abstract": "With the ongoing rapid adoption of Artificial Intelligence (AI)-based systems in high-stakes domains, ensuring the trustworthiness, safety, and observability of these systems has become crucial. It is essential to evaluate and monitor AI systems not only for accuracy and quality-related metrics but also for robustness, bias, security, interpretability, and other responsible AI dimensions. We focus on large language models (LLMs) and other generative AI models, which present additional challenges such as hallucinations, harmful and manipulative content, and copyright infringement. In this survey article accompanying our KDD 2024 tutorial, we highlight a wide range of harms associated with generative AI systems, and survey state of the art approaches (along with open challenges) to address these harms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "2128305",
                    "name": "M. Sameki"
                },
                {
                    "authorId": "40511120",
                    "name": "Ankur Taly"
                }
            ]
        },
        {
            "paperId": "f904551ef0622587a10434cfb6734710fd267dc0",
            "title": "Which Pretrain Samples to Rehearse when Finetuning Pretrained Models?",
            "abstract": "Fine-tuning pretrained foundational models on specific tasks is now the de facto approach for text and vision tasks. A known pitfall of this approach is the forgetting of pretraining knowledge that happens during finetuning. Rehearsing samples randomly from the pretrain dataset is a common approach to alleviate such forgetting. However, we find that random mixing unintentionally includes samples which are not (yet) forgotten or unlearnable by the model. We propose a novel sampling scheme, mix-cd, that identifies and prioritizes samples that actually face forgetting, which we call collateral damage. Since directly identifying collateral damage samples is computationally expensive, we propose a procedure to estimate the distribution of such samples by tracking the statistics of finetuned samples. Our approach is lightweight, easy to implement, and can be seamlessly integrated into existing models, offering an effective means to retain pretrain performance without additional computational costs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2279749940",
                    "name": "Andrew Bai"
                },
                {
                    "authorId": "2283936030",
                    "name": "Chih-Kuan Yeh"
                },
                {
                    "authorId": "2279751532",
                    "name": "Cho-Jui Hsieh"
                },
                {
                    "authorId": "40511120",
                    "name": "Ankur Taly"
                }
            ]
        },
        {
            "paperId": "c11dad59cbca5cc4875391ebf5360f945aec933a",
            "title": "Identifying and Mitigating the Security Risks of Generative AI",
            "abstract": "Every major technical invention resurfaces the dual-use dilemma -- the new technology has the potential to be used for good as well as for harm. Generative AI (GenAI) techniques, such as large language models (LLMs) and diffusion models, have shown remarkable capabilities (e.g., in-context learning, code-completion, and text-to-image generation and editing). However, GenAI can be used just as well by attackers to generate new attacks and increase the velocity and efficacy of existing attacks. This paper reports the findings of a workshop held at Google (co-organized by Stanford University and the University of Wisconsin-Madison) on the dual-use dilemma posed by GenAI. This paper is not meant to be comprehensive, but is rather an attempt to synthesize some of the interesting findings from the workshop. We discuss short-term and long-term goals for the community on this topic. We hope this paper provides both a launching point for a discussion on this important topic as well as interesting problems that the research community can work to address.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2052981589",
                    "name": "Clark W. Barrett"
                },
                {
                    "authorId": "2226720780",
                    "name": "Bradley L Boyd"
                },
                {
                    "authorId": "2235068798",
                    "name": "Ellie Burzstein"
                },
                {
                    "authorId": "2483738",
                    "name": "Nicholas Carlini"
                },
                {
                    "authorId": "2235255848",
                    "name": "Brad Chen"
                },
                {
                    "authorId": "2116551496",
                    "name": "Jihye Choi"
                },
                {
                    "authorId": "2599906",
                    "name": "A. Chowdhury"
                },
                {
                    "authorId": "8200989",
                    "name": "Mihai Christodorescu"
                },
                {
                    "authorId": "33374965",
                    "name": "Anupam Datta"
                },
                {
                    "authorId": "34389431",
                    "name": "S. Feizi"
                },
                {
                    "authorId": "145271194",
                    "name": "Kathleen Fisher"
                },
                {
                    "authorId": "2117567142",
                    "name": "Tatsunori Hashimoto"
                },
                {
                    "authorId": "3422872",
                    "name": "Dan Hendrycks"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                },
                {
                    "authorId": "35342489",
                    "name": "Daniel Kang"
                },
                {
                    "authorId": "1682949",
                    "name": "F. Kerschbaum"
                },
                {
                    "authorId": "49688913",
                    "name": "E. Mitchell"
                },
                {
                    "authorId": "2115536787",
                    "name": "John C. Mitchell"
                },
                {
                    "authorId": "2803967",
                    "name": "Zulfikar Ramzan"
                },
                {
                    "authorId": "1890695",
                    "name": "K. Shams"
                },
                {
                    "authorId": "143711382",
                    "name": "D. Song"
                },
                {
                    "authorId": "40511120",
                    "name": "Ankur Taly"
                },
                {
                    "authorId": "2143919864",
                    "name": "Diyi Yang"
                }
            ]
        },
        {
            "paperId": "423a8907bf0adb7ccd5f38dd8e7848d0dfc6d842",
            "title": "First is Better Than Last for Language Data Influence",
            "abstract": "The ability to identify influential training examples enables us to debug training data and explain model behavior. Existing techniques to do so are based on the flow of training data influence through the model parameters. For large models in NLP applications, it is often computationally infeasible to study this flow through all model parameters, therefore techniques usually pick the last layer of weights. However, we observe that since the activation connected to the last layer of weights contains\"shared logic\", the data influenced calculated via the last layer weights prone to a ``cancellation effect'', where the data influence of different examples have large magnitude that contradicts each other. The cancellation effect lowers the discriminative power of the influence score, and deleting influential examples according to this measure often does not change the model's behavior by much. To mitigate this, we propose a technique called TracIn-WE that modifies a method called TracIn to operate on the word embedding layer instead of the last layer, where the cancellation effect is less severe. One potential concern is that influence based on the word embedding layer may not encode sufficient high level information. However, we find that gradients (unlike embeddings) do not suffer from this, possibly because they chain through higher layers. We show that TracIn-WE significantly outperforms other data influence methods applied on the last layer significantly on the case deletion evaluation on three language classification tasks for different models. In addition, TracIn-WE can produce scores not just at the level of the overall training input, but also at the level of words within the training input, a further aid in debugging.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3438923",
                    "name": "Chih-Kuan Yeh"
                },
                {
                    "authorId": "40511120",
                    "name": "Ankur Taly"
                },
                {
                    "authorId": "30740726",
                    "name": "Mukund Sundararajan"
                },
                {
                    "authorId": "2155134",
                    "name": "Frederick Liu"
                },
                {
                    "authorId": "145969795",
                    "name": "Pradeep Ravikumar"
                }
            ]
        },
        {
            "paperId": "4e7921a326075f3fa1d6fa08c892b9e01730cc77",
            "title": "First is Better Than Last for Training Data Influence",
            "abstract": "classi\ufb01cation task for 40 \u00d7 6 \u00d7 10 \u00d7 2 \u00d7 10 times, where the \ufb01ne-tuning takes around 10 \u2212 20 GPU-minute on a V100 for Bert-Small, and 40 , 6 , 10 , 2 , 10 stands for number of test points, number of methods, removal numbers, proponents/ opponents",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3438923",
                    "name": "Chih-Kuan Yeh"
                },
                {
                    "authorId": "40511120",
                    "name": "Ankur Taly"
                },
                {
                    "authorId": "30740726",
                    "name": "Mukund Sundararajan"
                },
                {
                    "authorId": "2155134",
                    "name": "Frederick Liu"
                },
                {
                    "authorId": "145969795",
                    "name": "Pradeep Ravikumar"
                }
            ]
        },
        {
            "paperId": "68e64acad9ff92952f4fe1fb9edd1b70d6bf842b",
            "title": "Interpretable Mixture of Experts for Structured Data",
            "abstract": "With the growth of machine learning for structured data, the need for reliable model explanations is essential, especially in high-stakes applications. We introduce a novel framework, Interpretable Mixture of Experts (IME) 2 , that provides interpretability for structured data while preserving accuracy. IME consists of an assignment module and a mixture of interpretable experts such as linear models where each sample is assigned to a single interpretable expert. This results in an inherently-interpretable architecture where the explanations produced by IME are the exact descriptions of how the prediction is computed. In addition to constituting a standalone inherently-interpretable architecture, an additional IME capability is that it can be integrated with existing Deep Neural Networks (DNNs) to offer inter-pretability to a subset of samples while maintaining the accuracy of the DNNs. Experiments on various structured datasets demonstrate that IME is more accurate than a single interpretable model and performs comparably to existing state-of-the-art deep learning models in terms of accuracy while providing faithful explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30151156",
                    "name": "Aya Abdelsalam Ismail"
                },
                {
                    "authorId": "2676352",
                    "name": "Sercan \u00d6. Arik"
                },
                {
                    "authorId": "2144029",
                    "name": "Jinsung Yoon"
                },
                {
                    "authorId": "40511120",
                    "name": "Ankur Taly"
                },
                {
                    "authorId": "34389431",
                    "name": "S. Feizi"
                },
                {
                    "authorId": "1945962",
                    "name": "Tomas Pfister"
                }
            ]
        },
        {
            "paperId": "ba244bcdba17129dbb5955159fdf793db505c2f8",
            "title": "Interpretable Mixture of Experts",
            "abstract": "The need for reliable model explanations is prominent for many machine learning applications, particularly for tabular and time-series data as their use cases often involve high-stakes decision making. Towards this goal, we introduce a novel interpretable modeling framework, Interpretable Mixture of Experts (IME), that yields high accuracy, comparable to `black-box' Deep Neural Networks (DNNs) in many cases, along with useful interpretability capabilities. IME consists of an assignment module and a mixture of experts, with each sample being assigned to a single expert for prediction. We introduce multiple options for IME based on the assignment and experts being interpretable. When the experts are chosen to be interpretable such as linear models, IME yields an inherently-interpretable architecture where the explanations produced by IME are the exact descriptions of how the prediction is computed. In addition to constituting a standalone inherently-interpretable architecture, IME has the premise of being integrated with existing DNNs to offer interpretability to a subset of samples while maintaining the accuracy of the DNNs. Through extensive experiments on 15 tabular and time-series datasets, IME is demonstrated to be more accurate than single interpretable models and perform comparably with existing state-of-the-art DNNs in accuracy. On most datasets, IME even outperforms DNNs, while providing faithful explanations. Lastly, IME's explanations are compared to commonly-used post-hoc explanations methods through a user study -- participants are able to better predict the model behavior when given IME explanations, while finding IME's explanations more faithful and trustworthy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30151156",
                    "name": "Aya Abdelsalam Ismail"
                },
                {
                    "authorId": "2676352",
                    "name": "Sercan \u00d6. Arik"
                },
                {
                    "authorId": "2144029",
                    "name": "Jinsung Yoon"
                },
                {
                    "authorId": "40511120",
                    "name": "Ankur Taly"
                },
                {
                    "authorId": "34389431",
                    "name": "S. Feizi"
                },
                {
                    "authorId": "1945962",
                    "name": "Tomas Pfister"
                }
            ]
        },
        {
            "paperId": "bacfb8d5f6f75da1f5fde6f92e38d0de9fb13d04",
            "title": "Explainable AI in Industry: Practical Challenges and Lessons Learned",
            "abstract": "Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with the proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI have become far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability [11]. Model explainability is considered a prerequisite for building trust and adoption of AI systems in high stakes domains such as lending and healthcare [1] which require reliability, safety, and fairness. It is also critical to automated transportation, and other industrial applications with significant socio-economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling. As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale [14, 15, 25]. The challenges for the research community include: (i) defining model explainability, (ii) formulating explainability tasks for understanding model behavior and developing solutions for these tasks, and finally (iii) designing measures for evaluating the performance of models in explainability tasks. In this tutorial, we will first motivate the need for model interpretability and explainability in AI [6] from societal, legal, enterprise, end-user, and model developer perspectives, and present techniques & tools for providing explainability as part of AI/ML systems [13]. Then, we will focus on the real-world application of explainability techniques in industry, wherein we present practical challenges & implications for using explainability techniques effectively and lessons learned from deploying explainable models for several web-scale machine learning and data mining applications. We will present case studies across different companies, spanning application domains such as search and recommendation systems, hiring, lending, sales, and fraud detection. Finally, based on our experiences in industry, we will identify open problems and research directions for the WWW community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144315468",
                    "name": "Krishna Gade"
                },
                {
                    "authorId": "6748971",
                    "name": "S. Geyik"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "1750278",
                    "name": "Varun Mithal"
                },
                {
                    "authorId": "40511120",
                    "name": "Ankur Taly"
                }
            ]
        },
        {
            "paperId": "d49b739cd43c7cb228a7ee7d0b34b0ca6fd75d47",
            "title": "Explainable AI in industry: practical challenges and lessons learned: implications tutorial",
            "abstract": "Artificial Intelligence is increasingly playing an integral role in determining our day-to-day experiences. Moreover, with the proliferation of AI based solutions in areas such as hiring, lending, criminal justice, healthcare, and education, the resulting personal and professional implications of AI have become far-reaching. The dominant role played by AI models in these domains has led to a growing concern regarding potential bias in these models, and a demand for model transparency and interpretability [2, 4]. Model explainability is considered a prerequisite for building trust and adoption of AI systems in high stakes domains such as lending and healthcare [1] requiring reliability, safety, and fairness. It is also critical to automated transportation, and other industrial applications with significant socio-economic implications such as predictive maintenance, exploration of natural resources, and climate change modeling. As a consequence, AI researchers and practitioners have focused their attention on explainable AI to help them better trust and understand models at scale [5, 6, 8]. In fact, the field of explainability in AI/ML is at an inflexion point. There is a tremendous need from the societal, regulatory, commercial, end-user, and model developer perspectives. Consequently, practical and scalable explainability approaches are rapidly becoming available. The challenges for the research community include: (i) achieving consensus on the right notion of model explainability, (ii) identifying and formalizing explainability tasks from the perspectives of various stakeholders, and (iii) designing measures for evaluating explainability techniques. In this tutorial, we will first motivate the need for model interpretability and explainability in AI [3] from various perspectives. We will then provide a brief overview of several explainability techniques and tools. The rest of the tutorial will focus on the real-world application of explainability techniques in industry. We will present case studies spanning several domains such as: \u2022 Search and Recommendation systems: Understanding of search and recommendations systems, as well as how retrieval and ranking decisions happen in real-time [7]. Example applications include explanation of decisions made by an AI system towards job recommendations, ranking of potential candidates for job posters, and content recommendations. \u2022 Sales: Understanding of sales predictions in terms of customer up-sell/churn. \u2022 Fraud Detection: Examining and explaining AI systems that determine whether a content or event is fraudulent. \u2022 Lending: How to understand/interpret lending decisions made by an AI system. We will focus on the sociotechnical dimensions, practical challenges, and lessons learned during development and deployment of these systems, which would be beneficial for researchers and practitioners interested in explainable AI. Finally, we will discuss open challenges and research directions for the community.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "144315468",
                    "name": "Krishna Gade"
                },
                {
                    "authorId": "6748971",
                    "name": "S. Geyik"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "1750278",
                    "name": "Varun Mithal"
                },
                {
                    "authorId": "40511120",
                    "name": "Ankur Taly"
                }
            ]
        },
        {
            "paperId": "0321d2342574062fa234ec89e4e02515ce4519f4",
            "title": "Finding Invariants in Deep Neural Networks",
            "abstract": "We present techniques for automatically inferring invariant properties of feed-forward neural networks. Our insight is that feed forward networks should be able to learn a decision logic that is captured in the activation patterns of its neurons. We propose to extract such decision patterns that can be considered as invariants of the network with respect to a certain output behavior. We present techniques to extract input invariants as convex predicates on the input space, and layer invariants that represent features captured in the hidden layers. We apply the techniques on the networks for the MNIST and ACASXU applications. Our experiments highlight the use of invariants in a variety of applications, such as explainability, providing robustness guarantees, detecting adversaries, simplifying proofs and network distillation.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "39921534",
                    "name": "D. Gopinath"
                },
                {
                    "authorId": "40511120",
                    "name": "Ankur Taly"
                },
                {
                    "authorId": "8118488",
                    "name": "Hayes Converse"
                },
                {
                    "authorId": "1723381",
                    "name": "C. P\u0103s\u0103reanu"
                }
            ]
        }
    ]
}