{
    "authorId": "2138053020",
    "papers": [
        {
            "paperId": "1acd0fbda499ffc00abf325a94bb479187708844",
            "title": "Impoverished Language Technology: The Lack of (Social) Class in NLP",
            "abstract": "Since Labov\u2019s foundational 1964 work on the social stratification of language, linguistics has dedicated concerted efforts towards understanding the relationships between socio-demographic factors and language production and perception. Despite the large body of evidence identifying significant relationships between socio-demographic factors and language production, relatively few of these factors have been investigated in the context of NLP technology. While age and gender are well covered, Labov\u2019s initial target, socio-economic class, is largely absent. We survey the existing Natural Language Processing (NLP) literature and find that only 20 papers even mention socio-economic status. However, the majority of those papers do not engage with class beyond collecting information of annotator-demographics. Given this research lacuna, we provide a definition of class that can be operationalised by NLP researchers, and argue for including socio-economic class in future language technologies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3451318",
                    "name": "A. C. Curry"
                },
                {
                    "authorId": "2138053020",
                    "name": "Zeerak Talat"
                },
                {
                    "authorId": "2267334203",
                    "name": "Dirk Hovy"
                }
            ]
        },
        {
            "paperId": "4f452e6a453f4ccd8f6c1960a8291a0ad8e4c2ee",
            "title": "Exploring the Limitations of Detecting Machine-Generated Text",
            "abstract": "Recent improvements in the quality of the generations by large language models have spurred research into identifying machine-generated text. Systems proposed for the task often achieve high performance. However, humans and machines can produce text in different styles and in different domains, and it remains unclear whether machine generated-text detection models favour particular styles or domains. In this paper, we critically examine the classification performance for detecting machine-generated text by evaluating on texts with varying writing styles. We find that classifiers are highly sensitive to stylistic changes and differences in text complexity, and in some cases degrade entirely to random classifiers. We further find that detection systems are particularly susceptible to misclassify easy-to-read texts while they have high performance for complex texts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1724523030",
                    "name": "Jad Doughman"
                },
                {
                    "authorId": "49843300",
                    "name": "Osama Mohammed Afzal"
                },
                {
                    "authorId": "2261493094",
                    "name": "Hawau Olamide Toyin"
                },
                {
                    "authorId": "38510157",
                    "name": "Shady Shehata"
                },
                {
                    "authorId": "2026545715",
                    "name": "Preslav Nakov"
                },
                {
                    "authorId": "2138053020",
                    "name": "Zeerak Talat"
                }
            ]
        },
        {
            "paperId": "9c970ab6a3f8a5c3a5b3fcff19952edaaa79d24a",
            "title": "Understanding \"Democratization\" in NLP and ML Research",
            "abstract": "Recent improvements in natural language processing (NLP) and machine learning (ML) and increased mainstream adoption have led to researchers frequently discussing the\"democratization\"of artificial intelligence. In this paper, we seek to clarify how democratization is understood in NLP and ML publications, through large-scale mixed-methods analyses of papers using the keyword\"democra*\"published in NLP and adjacent venues. We find that democratization is most frequently used to convey (ease of) access to or use of technologies, without meaningfully engaging with theories of democratization, while research using other invocations of\"democra*\"tends to be grounded in theories of deliberation and debate. Based on our findings, we call for researchers to enrich their use of the term democratization with appropriate theory, towards democratic technologies beyond superficial access.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1677386832",
                    "name": "Arjun Subramonian"
                },
                {
                    "authorId": "2047602319",
                    "name": "Vagrant Gautam"
                },
                {
                    "authorId": "2279917171",
                    "name": "Dietrich Klakow"
                },
                {
                    "authorId": "2138053020",
                    "name": "Zeerak Talat"
                }
            ]
        },
        {
            "paperId": "aafdcae57973ecd3f562648986c61bbf5f7fbf41",
            "title": "Subjective Isms? On the Danger of Conflating Hate and Offence in Abusive Language Detection",
            "abstract": "Natural language processing research has begun to embrace the notion of annotator subjectivity, motivated by variations in labelling. This approach understands each annotator\u2019s view as valid, which can be highly suitable for tasks that embed subjectivity, e.g., sentiment analysis. However, this construction may be inappropriate for tasks such as hate speech detection, as it affords equal validity to all positions on e.g., sexism or racism. We argue that the conflation of hate and offence can invalidate findings on hate speech, and call for future work to be situated in theory, disentangling hate from its orthogonal concept, offence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3451318",
                    "name": "A. C. Curry"
                },
                {
                    "authorId": "17038002",
                    "name": "Gavin Abercrombie"
                },
                {
                    "authorId": "2138053020",
                    "name": "Zeerak Talat"
                }
            ]
        },
        {
            "paperId": "d22e336171c7ac92167f9b03b0e660af922069b4",
            "title": "Zero-shot Sentiment Analysis in Low-Resource Languages Using a Multilingual Sentiment Lexicon",
            "abstract": "Improving multilingual language models capabilities in low-resource languages is generally difficult due to the scarcity of large-scale data in those languages. In this paper, we relax the reliance on texts in low-resource languages by using multilingual lexicons in pretraining to enhance multilingual capabilities. Specifically, we focus on zero-shot sentiment analysis tasks across 34 languages, including 6 high/medium-resource languages, 25 low-resource languages, and 3 code-switching datasets. We demonstrate that pretraining using multilingual lexicons, without using any sentence-level sentiment data, achieves superior zero-shot performance compared to models fine-tuned on English sentiment datasets, and large language models like GPT\u20133.5, BLOOMZ, and XGLM. These findings are observable for unseen low-resource languages to code-mixed scenarios involving high-resource languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2789148",
                    "name": "Fajri Koto"
                },
                {
                    "authorId": "2282538967",
                    "name": "Tilman Beck"
                },
                {
                    "authorId": "2138053020",
                    "name": "Zeerak Talat"
                },
                {
                    "authorId": "69033154",
                    "name": "Iryna Gurevych"
                },
                {
                    "authorId": "2256987316",
                    "name": "Timothy Baldwin"
                }
            ]
        },
        {
            "paperId": "d341da8368e950c0efc44d74ba3c33c9c5f1314e",
            "title": "The Perspectivist Paradigm Shift: Assumptions and Challenges of Capturing Human Labels",
            "abstract": "Longstanding data labeling practices in machine learning involve collecting and aggregating labels from multiple annotators. But what should we do when annotators disagree? Though annotator disagreement has long been seen as a problem to minimize, new perspectivist approaches challenge this assumption by treating disagreement as a valuable source of information. In this position paper, we examine practices and assumptions surrounding the causes of disagreement\u2013some challenged by perspectivist approaches, and some that remain to be addressed\u2013as well as practical and normative challenges for work operating under these assumptions. We conclude with recommendations for the data labeling pipeline and avenues for future research engaging with subjectivity and disagreement.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1988380666",
                    "name": "Eve Fleisig"
                },
                {
                    "authorId": "3422038",
                    "name": "Su Lin Blodgett"
                },
                {
                    "authorId": "2265492379",
                    "name": "Dan Klein"
                },
                {
                    "authorId": "2138053020",
                    "name": "Zeerak Talat"
                }
            ]
        },
        {
            "paperId": "fede0a0c61c613c90c49f473c488e2529d63680e",
            "title": "Classist Tools: Social Class Correlates with Performance in NLP",
            "abstract": "Since the foundational work of William Labov on the social stratification of language (Labov, 1964), linguistics has made concentrated efforts to explore the links between sociodemographic characteristics and language production and perception. But while there is strong evidence for socio-demographic characteristics in language, they are infrequently used in Natural Language Processing (NLP). Age and gender are somewhat well represented, but Labov's original target, socioeconomic status, is noticeably absent. And yet it matters. We show empirically that NLP disadvantages less-privileged socioeconomic groups. We annotate a corpus of 95K utterances from movies with social class, ethnicity and geographical language variety and measure the performance of NLP systems on three tasks: language modelling, automatic speech recognition, and grammar error correction. We find significant performance disparities that can be attributed to socioeconomic status as well as ethnicity and geographical differences. With NLP technologies becoming ever more ubiquitous and quotidian, they must accommodate all language varieties to avoid disadvantaging already marginalised groups. We argue for the inclusion of socioeconomic class in future language technologies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3451318",
                    "name": "A. C. Curry"
                },
                {
                    "authorId": "1481857041",
                    "name": "Giuseppe Attanasio"
                },
                {
                    "authorId": "2138053020",
                    "name": "Zeerak Talat"
                },
                {
                    "authorId": "2267334203",
                    "name": "Dirk Hovy"
                }
            ]
        },
        {
            "paperId": "488993aefb9247afa39859d740952833f06b70b0",
            "title": "Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing",
            "abstract": "Dual use, the intentional, harmful reuse of technology and scientific artefacts, is a problem yet to be well-defined within the context of Natural Language Processing (NLP). However, as NLP technologies continue to advance and become increasingly widespread in society, their inner workings have become increasingly opaque. Therefore, understanding dual use concerns and potential ways of limiting them is critical to minimising the potential harms of research and development. In this paper, we conduct a survey of NLP researchers and practitioners to understand the depth and their perspective of the problem as well as to assess existing available support. Based on the results of our survey, we offer a definition of dual use that is tailored to the needs of the NLP community. The survey revealed that a majority of researchers are concerned about the potential dual use of their research but only take limited action toward it. In light of the survey results, we discuss the current state and potential means for mitigating dual use in NLP and propose a checklist that can be integrated into existing conference ethics-frameworks, e.g., the ACL ethics checklist.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "23319388",
                    "name": "Lucie-Aim\u00e9e Kaffee"
                },
                {
                    "authorId": "1943255906",
                    "name": "Arnav Arora"
                },
                {
                    "authorId": "2138053020",
                    "name": "Zeerak Talat"
                },
                {
                    "authorId": "1736067",
                    "name": "Isabelle Augenstein"
                }
            ]
        },
        {
            "paperId": "9379d519b8ddfa194ef6f575127451e5016e1803",
            "title": "Mirages: On Anthropomorphism in Dialogue Systems",
            "abstract": "Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism may be inevitable due to the choice of medium, conscious and unconscious design choices can guide users to personify such systems to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have investigated the factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be explored. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, including reinforcing gender stereotypes and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17038002",
                    "name": "Gavin Abercrombie"
                },
                {
                    "authorId": "3451318",
                    "name": "A. C. Curry"
                },
                {
                    "authorId": "72115354",
                    "name": "Tanvi Dinkar"
                },
                {
                    "authorId": "2138053020",
                    "name": "Zeerak Talat"
                }
            ]
        },
        {
            "paperId": "a4d543cc77a0a643a271988f5b4b6699f421c85c",
            "title": "Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms",
            "abstract": "Bias evaluation benchmarks and dataset and model documentation have emerged as central processes for assessing the biases and harms of artificial intelligence (AI) systems. However, these auditing processes have been criticized for their failure to integrate the knowledge of marginalized communities and consider the power dynamics between auditors and the communities. Consequently, modes of bias evaluation have been proposed that engage impacted communities in identifying and assessing the harms of AI systems (e.g., bias bounties). Even so, asking what marginalized communities want from such auditing processes has been neglected. In this paper, we ask queer communities for their positions on, and desires from, auditing processes. To this end, we organized a participatory workshop to critique and redesign bias bounties from queer perspectives. We found that when given space, the scope of feedback from workshop participants goes far beyond what bias bounties afford, with participants questioning the ownership, incentives, and efficacy of bounties. We conclude by advocating for community ownership of bounties and complementing bounties with participatory processes (e.g., co-creation).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2224018196",
                    "name": "Organizers of QueerInAI"
                },
                {
                    "authorId": "2291653855",
                    "name": "Nathaniel Dennler"
                },
                {
                    "authorId": "51494507",
                    "name": "Anaelia Ovalle"
                },
                {
                    "authorId": "2165747190",
                    "name": "Ashwin Singh"
                },
                {
                    "authorId": "3328733",
                    "name": "Luca Soldaini"
                },
                {
                    "authorId": "1677386832",
                    "name": "Arjun Subramonian"
                },
                {
                    "authorId": "41016373",
                    "name": "Huy Tu"
                },
                {
                    "authorId": "27377925",
                    "name": "William Agnew"
                },
                {
                    "authorId": "2217841900",
                    "name": "Avijit Ghosh"
                },
                {
                    "authorId": "32254917",
                    "name": "Kyra Yee"
                },
                {
                    "authorId": "1723419523",
                    "name": "Irene Font Peradejordi"
                },
                {
                    "authorId": "2138053020",
                    "name": "Zeerak Talat"
                },
                {
                    "authorId": "2211662920",
                    "name": "Mayra Russo"
                },
                {
                    "authorId": "2122360278",
                    "name": "Jessica de Jesus de Pinho Pinhal"
                }
            ]
        }
    ]
}