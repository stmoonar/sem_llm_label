{
    "authorId": "2143785210",
    "papers": [
        {
            "paperId": "a3ffe5939a3cbd1f8424175e6725c2e577b0f518",
            "title": "Effective Unsupervised Constrained Text Generation based on Perturbed Masking",
            "abstract": "Unsupervised constrained text generation aims to generate text under a given set of constraints without any supervised data. Current state-of-the-art methods stochastically sample edit positions and actions, which may cause unnecessary search steps. In this paper, we propose PMCTG to improve effectiveness by searching for the best edit position and action in each step. Specifically, PMCTG extends perturbed masking technique to effectively search for the most incongruent token to edit. Then it introduces four multi-aspect scoring functions to select edit action to further reduce search difficulty. Since PMCTG does not require supervised data, it could be applied to different generation tasks. We show that under the unsupervised setting, PMCTG achieves new state-of-the-art results in two representative tasks, namely keywords- to-sentence generation and paraphrasing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "46223131",
                    "name": "Wenjie Ou"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "87f046b2f12174acbbe2ed68a101297464c5dc5f",
            "title": "NSV-TTS: Non-Speech Vocalization Modeling And Transfer In Emotional Text-To-Speech",
            "abstract": "This paper addresses the problem of non-speech vocalization (NSV) modeling and transfer in emotional TTS. We propose an emotion TTS system (NSV-TTS) to model NSV and emotional speech. The model utilizes self-supervised learning to extract unsupervised linguistic units (ULUs) for NSV labeling and zero-shot NSV transfer. Furthermore, we propose token mixing and random masking to boost the performance. We evaluate the proposed method on various NSV types and emotion classes. The experimental results reveal that the proposed method performs well in the zero-shot NSV transfer task. Lastly, we conduct ablation studies to investigate the proposed method further.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1391191130",
                    "name": "Haitong Zhang"
                },
                {
                    "authorId": "2151419992",
                    "name": "Xinyuan Yu"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "0f49918ce8b0aaf8e6bc70c675978ac5e0ee6cfe",
            "title": "Improve Few-Shot Voice Cloning Using Multi-Modal Learning",
            "abstract": "Recently, few-shot voice cloning has achieved a significant improvement. However, most models for few-shot voice cloning are single-modal, and multi-modal few-shot voice cloning has been understudied. In this paper, we propose to use multi-modal learning to improve the few-shot voice cloning performance. Inspired by the recent works on un-supervised speech representation, the proposed multi-modal system is built by extending Tacotron2 with an unsupervised speech representation module. We evaluate our proposed system in two few-shot voice cloning scenarios, namely few-shot text-to-speech (TTS) and voice conversion (VC). Experimental results demonstrate that the proposed multi-modal learning can significantly improve the few-shot voice cloning performance over their counterpart single-modal systems.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1391191130",
                    "name": "Haitong Zhang"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "5a1250adb415d439e8b8e69485794bf196492584",
            "title": "Data Augmentation for Long-Tailed and Imbalanced Polyphone Disambiguation in Mandarin",
            "abstract": "Polyphone disambiguation is an important module in Mandarin Chinese text-to-speech (TTS). Recently, neural-network-based (NN-based) models have achieved a great improvement on poly-phone disambiguation. However, a long-tailed and imbalanced distribution is usually observed in the training data of polyphone disambiguation, resulting in an unsatisfying performance on the low-frequent polyphone in the imbalanced pinyin set, and the least-frequent polyphonic characters and polyphones. In this paper, we proposed a simple data-augmentation method based on the pre-trained mask language model BERT to mitigate the long-tailed and imbalanced distribution problem. We incorporate a weighted sampling technique in the data augmentation method to balance the data distribution, and a useful filtering strategy to remove some noisy augmented data. Experimental results show that the proposed data-augmentation method can improve the prediction accuracy, especially for those low-frequent polyphone in the imbalanced pinyin set, and the least-frequent polyphonic characters and polyphones.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145957511",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "1391191130",
                    "name": "Haitong Zhang"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "642e04ece55c5b532ff7e5408d8723c7d9c835db",
            "title": "MIGA: A Unified Multi-task Generation Framework for Conversational Text-to-SQL",
            "abstract": "Conversational text-to-SQL is designed to translate multi-turn natural language questions into their corresponding SQL queries. Most advanced conversational text-to-SQL methods are incompatible with generative pre-trained language models (PLMs), such as T5. In this paper, we present a two-stage unified MultI-task Generation frAmework (MIGA) that leverages PLMs\u2019 ability to tackle conversational text-to-SQL. In the pre-training stage, MIGA first decomposes the main task into several related sub-tasks and then unifies them into the same sequence-to-sequence (Seq2Seq) paradigm with task-specific natural language prompts to boost the main task from multi-task training. Later in the fine-tuning stage, we propose four SQL perturbations to alleviate the error propagation problem. MIGA tends to achieve state-of-the-art performance on two benchmarks (SparC and CoSQL). We also provide extensive analyses and discussions to shed light on some new perspectives for conversational text-to-SQL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110659921",
                    "name": "Yingwen Fu"
                },
                {
                    "authorId": "46223131",
                    "name": "Wenjie Ou"
                },
                {
                    "authorId": "144007938",
                    "name": "Zhou Yu"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "94c76558eceebac0dea519ae5947168034817734",
            "title": "DGC-Vector: A New Speaker Embedding for Zero-Shot Voice Conversion",
            "abstract": "Recently, more and more zero-shot voice conversion algorithms have been proposed. As a fundamental part of zero-shot voice conversion, speaker embeddings are the key to improving the converted speech\u2019s speaker similarity. In this paper, we study the impact of speaker embeddings on zero-shot voice conversion performance. To better represent the characteristics of the target speaker and improve the speaker similarity in zero-shot voice conversion, we propose a novel speaker representation method in this paper. Our method combines the advantages of D-vector, global style token (GST) based speaker representation and auxiliary supervision. Objective and subjective evaluations show that the proposed method achieves a decent performance on zero-shot voice conversion and significantly improves speaker similarity over D-vector and GST-based speaker embedding.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2152647795",
                    "name": "Ruitong Xiao"
                },
                {
                    "authorId": "1391191130",
                    "name": "Haitong Zhang"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "4634c0d4e179131dcd5b411c6248f1e352ccc115",
            "title": "Exploring Timbre Disentanglement in Non-Autoregressive Cross-Lingual Text-to-Speech",
            "abstract": "In this paper, we study the disentanglement of speaker and language representations in non-autoregressive cross-lingual TTS models from various aspects. We propose a phoneme length regulator that solves the length mismatch problem between IPA input sequence and monolingual alignment results. Using the phoneme length regulator, we present a FastPitch-based cross-lingual model with IPA symbols as input representations. Our experiments show that language-independent input representations (e.g. IPA symbols), an increasing number of training speakers, and explicit modeling of speech variance information all encourage non-autoregressive cross-lingual TTS model to disentangle speaker and language representations. The subjective evaluation shows that our proposed model can achieve decent naturalness and speaker similarity in cross-language voice cloning.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2134590693",
                    "name": "Haoyue Zhan"
                },
                {
                    "authorId": "2151419992",
                    "name": "Xinyuan Yu"
                },
                {
                    "authorId": "1391191130",
                    "name": "Haitong Zhang"
                },
                {
                    "authorId": "2145954768",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "83bf3fdb99de68ef4f9b5eabb5c99c3904010fe6",
            "title": "Revisiting IPA-based Cross-lingual Text-to-speech",
            "abstract": "International Phonetic Alphabet (IPA) has been widely used in cross-lingual text-to-speech (TTS) to achieve cross-lingual voice cloning (CL VC). However, IPA itself has been understudied in cross-lingual TTS. In this paper, we report some empirical findings of building a cross-lingual TTS model using IPA as inputs. Experiments show that the way to process the IPA and suprasegmental sequence has a negligible impact on the CL VC performance. Furthermore, we find that using a dataset including one speaker per language to build an IPA-based TTS system would fail CL VC since the language-unique IPA and tone/stress symbols could leak the speaker information. In addition, we experiment with different combinations of speakers in the training dataset to further investigate the effect of the number of speakers on the CL VC performance.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1391191130",
                    "name": "Haitong Zhang"
                },
                {
                    "authorId": "2134590693",
                    "name": "Haoyue Zhan"
                },
                {
                    "authorId": "2145954768",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2151419992",
                    "name": "Xinyuan Yu"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "8fc04ba2024f99b0a3bcde765cd3e4c0fbb2f074",
            "title": "Improve Cross-Lingual Text-To-Speech Synthesis on Monolingual Corpora with Pitch Contour Information",
            "abstract": "Cross-lingual text-to-speech (TTS) synthesis on monolingual corpora is still a challenging task, especially when many kinds of languages are involved. In this paper, we improve the cross-lingual TTS model on monolingual corpora with pitch contour information. We propose a method to obtain pitch contour sequences for different languages without manual annotation, and extend the Tacotron-based TTS model with the proposed Pitch Contour Extraction (PCE) module. Our experimental results show that the proposed approach can effectively improve the naturalness and consistency of synthesized mixed-lingual utterances.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065267796",
                    "name": "Hao Zhan"
                },
                {
                    "authorId": "1391191130",
                    "name": "Haitong Zhang"
                },
                {
                    "authorId": "46223131",
                    "name": "Wenjie Ou"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                }
            ]
        },
        {
            "paperId": "911f056c39220634b9d7a1e8ff7f2aea24998061",
            "title": "Systems at SDU-2021 Task 1: Transformers for Sentence Level Sequence Label",
            "abstract": "This paper describes the system proposed for addressing the research problem posed in Task1 of scienti\ufb01c document understanding (SDU@AAAI-2021): Acronym Identi\ufb01cation. We proposed an end-to-end model that takes the text as input and corresponding to each word gives the label of word to be acronyms (short-forms) or their meanings (long-forms). We take experiment on several totally different ideas, including features engineering, transformer model, multi-task learning, Span and CRF. Our result shows that feature-based method can handle this task well, and transformer-based models are particularly effective in this task. Moreover, different model frameworks complement each other. We achieved the best f1 score of 0.931 on test dataset and were ranked second.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146315741",
                    "name": "Feng Li"
                },
                {
                    "authorId": "2067325517",
                    "name": "Zhensheng Mai"
                },
                {
                    "authorId": "2057329020",
                    "name": "Wuhe Zou"
                },
                {
                    "authorId": "46223131",
                    "name": "Wenjie Ou"
                },
                {
                    "authorId": "47719677",
                    "name": "Xiaolei Qin"
                },
                {
                    "authorId": "2143785210",
                    "name": "Yue Lin"
                },
                {
                    "authorId": "2108134355",
                    "name": "Weidong Zhang"
                }
            ]
        }
    ]
}