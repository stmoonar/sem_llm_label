{
    "authorId": "151483422",
    "papers": [
        {
            "paperId": "22fb55c33dbdf6354ecd6b5e174f3382132bf6a8",
            "title": "EFWI: Multi-parameter Benchmark Datasets for Elastic Full Waveform Inversion of Geophysical Properties",
            "abstract": "Elastic geophysical properties (such as P- and S-wave velocities) are of great importance to various subsurface applications like CO$_2$ sequestration and energy exploration (e.g., hydrogen and geothermal). Elastic full waveform inversion (FWI) is widely applied for characterizing reservoir properties. In this paper, we introduce $\\mathbf{\\mathbb{E}^{FWI}}$, a comprehensive benchmark dataset that is specifically designed for elastic FWI. $\\mathbf{\\mathbb{E}^{FWI}}$ encompasses 8 distinct datasets that cover diverse subsurface geologic structures (flat, curve, faults, etc). The benchmark results produced by three different deep learning methods are provided. In contrast to our previously presented dataset (pressure recordings) for acoustic FWI (referred to as OpenFWI), the seismic dataset in $\\mathbf{\\mathbb{E}^{FWI}}$ has both vertical and horizontal components. Moreover, the velocity maps in $\\mathbf{\\mathbb{E}^{FWI}}$ incorporate both P- and S-wave velocities. While the multicomponent data and the added S-wave velocity make the data more realistic, more challenges are introduced regarding the convergence and computational cost of the inversion. We conduct comprehensive numerical experiments to explore the relationship between P-wave and S-wave velocities in seismic data. The relation between P- and S-wave velocities provides crucial insights into the subsurface properties such as lithology, porosity, fluid content, etc. We anticipate that $\\mathbf{\\mathbb{E}^{FWI}}$ will facilitate future research on multiparameter inversions and stimulate endeavors in several critical research topics of carbon-zero and new energy exploration. All datasets, codes and relevant information can be accessed through our website at https://efwi-lanl.github.io/",
            "fieldsOfStudy": [
                "Physics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100684917",
                    "name": "Shihang Feng"
                },
                {
                    "authorId": "2113289242",
                    "name": "Hanchen Wang"
                },
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "144314826",
                    "name": "Yinan Feng"
                },
                {
                    "authorId": "2108218510",
                    "name": "Yanhua Liu"
                },
                {
                    "authorId": "2195449047",
                    "name": "Min Zhu"
                },
                {
                    "authorId": "144155035",
                    "name": "Peng Jin"
                },
                {
                    "authorId": "2109306087",
                    "name": "Yinpeng Chen"
                },
                {
                    "authorId": "2108138012",
                    "name": "Youzuo Lin"
                }
            ]
        },
        {
            "paperId": "628e9824c363775ace9d1fc91f1f8d9494234302",
            "title": "Evaluating Stability in Massive Social Networks: Efficient Streaming Algorithms for Structural Balance",
            "abstract": "Structural balance theory studies stability in networks. Given a $n$-vertex complete graph $G=(V,E)$ whose edges are labeled positive or negative, the graph is considered \\emph{balanced} if every triangle either consists of three positive edges (three mutual ``friends''), or one positive edge and two negative edges (two ``friends'' with a common ``enemy''). From a computational perspective, structural balance turns out to be a special case of correlation clustering with the number of clusters at most two. The two main algorithmic problems of interest are: $(i)$ detecting whether a given graph is balanced, or $(ii)$ finding a partition that approximates the \\emph{frustration index}, i.e., the minimum number of edge flips that turn the graph balanced. We study these problems in the streaming model where edges are given one by one and focus on \\emph{memory efficiency}. We provide randomized single-pass algorithms for: $(i)$ determining whether an input graph is balanced with $O(\\log{n})$ memory, and $(ii)$ finding a partition that induces a $(1 + \\varepsilon)$-approximation to the frustration index with $O(n \\cdot \\text{polylog}(n))$ memory. We further provide several new lower bounds, complementing different aspects of our algorithms such as the need for randomization or approximation. To obtain our main results, we develop a method using pseudorandom generators (PRGs) to sample edges between independently-chosen \\emph{vertices} in graph streaming. Furthermore, our algorithm that approximates the frustration index improves the running time of the state-of-the-art correlation clustering with two clusters (Giotis-Guruswami algorithm [SODA 2006]) from $n^{O(1/\\varepsilon^2)}$ to $O(n^2\\log^3{n}/\\varepsilon^2 + n\\log n \\cdot (1/\\varepsilon)^{O(1/\\varepsilon^4)})$ time for $(1+\\varepsilon)$-approximation. These results may be of independent interest.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388003587",
                    "name": "Vikrant Ashvinkumar"
                },
                {
                    "authorId": "35067898",
                    "name": "Sepehr Assadi"
                },
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "2110618947",
                    "name": "Jie Gao"
                },
                {
                    "authorId": "2146562893",
                    "name": "Chen Wang"
                }
            ]
        },
        {
            "paperId": "6847b9658f287f430098199cd81bf26308da13f9",
            "title": "Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey",
            "abstract": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to make large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to better summarize and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059988575",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "50879401",
                    "name": "Xujiang Zhao"
                },
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "2182238045",
                    "name": "Can Zheng"
                },
                {
                    "authorId": "2120473483",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "2123930262",
                    "name": "Tanmoy Chowdhury"
                },
                {
                    "authorId": "2110425042",
                    "name": "Yun-Qing Li"
                },
                {
                    "authorId": "2112821580",
                    "name": "Hejie Cui"
                },
                {
                    "authorId": "2048981220",
                    "name": "Xuchao Zhang"
                },
                {
                    "authorId": "2211987764",
                    "name": "Tian-yu Zhao"
                },
                {
                    "authorId": "2218486790",
                    "name": "Amit Panalkar"
                },
                {
                    "authorId": "145859270",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": null,
                    "name": "Haoyu Wang"
                },
                {
                    "authorId": "3215702",
                    "name": "Yanchi Liu"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2204622281",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2218495127",
                    "name": "Chris White"
                },
                {
                    "authorId": "144966687",
                    "name": "Quanquan Gu"
                },
                {
                    "authorId": "2188744953",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2151579859",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "75c08892179fc478f87d7020b5daff9fca4f3389",
            "title": "Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models",
            "abstract": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a \u201cchatbot\u201d, and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2059988349",
                    "name": "Chen Ling"
                },
                {
                    "authorId": "50879401",
                    "name": "Xujiang Zhao"
                },
                {
                    "authorId": "2117727751",
                    "name": "Jiaying Lu"
                },
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "2182238045",
                    "name": "Can Zheng"
                },
                {
                    "authorId": "4142921",
                    "name": "Junxiang Wang"
                },
                {
                    "authorId": "2123930262",
                    "name": "Tanmoy Chowdhury"
                },
                {
                    "authorId": "2110425042",
                    "name": "Yun-Qing Li"
                },
                {
                    "authorId": "2112821580",
                    "name": "Hejie Cui"
                },
                {
                    "authorId": "2211987764",
                    "name": "Tian-yu Zhao"
                },
                {
                    "authorId": "2218486790",
                    "name": "Amit Panalkar"
                },
                {
                    "authorId": "145859270",
                    "name": "Wei Cheng"
                },
                {
                    "authorId": null,
                    "name": "Haoyu Wang"
                },
                {
                    "authorId": "3215702",
                    "name": "Yanchi Liu"
                },
                {
                    "authorId": "1766853",
                    "name": "Zhengzhang Chen"
                },
                {
                    "authorId": "2204622281",
                    "name": "Haifeng Chen"
                },
                {
                    "authorId": "2218495127",
                    "name": "Chris White"
                },
                {
                    "authorId": "9937103",
                    "name": "Quanquan Gu"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2151579859",
                    "name": "Liang Zhao"
                }
            ]
        },
        {
            "paperId": "574fc262f2ba9ceb5a1323308ae0ebe4a905f89c",
            "title": "Beyond Square-root Error: New Algorithms for Differentially Private All Pairs Shortest Distances",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "1750274",
                    "name": "Jie Gao"
                },
                {
                    "authorId": "145669921",
                    "name": "Jalaj Upadhyay"
                },
                {
                    "authorId": "2118470220",
                    "name": "Chen Wang"
                }
            ]
        },
        {
            "paperId": "1abd266736bb3b60e9ea51db020e417e1a3fe07e",
            "title": "On the Robustness and Generalization of Deep Learning Driven Full Waveform Inversion",
            "abstract": "The data-driven approach has been demonstrated as a promising technique to solve complicated scientific problems. Full Waveform Inversion (FWI) is commonly epitomized as an image-to-image translation task, which motivates the use of deep neural networks as an end-to-end solution. Despite being trained with synthetic data, the deep learning-driven FWI is expected to perform well when evaluated with sufficient real-world data. In this paper, we study such properties by asking: how robust are these deep neural networks and how do they generalize? For robustness, we prove the upper bounds of the deviation between the predictions from clean and noisy data. Moreover, we demonstrate an interplay between the noise level and the additional gain of loss. For generalization, we prove a norm-based generalization error upper bound via a stability-generalization framework. Experimental results on seismic FWI datasets corroborate with the theoretical results, shedding light on a better understanding of utilizing Deep Learning for complicated scientific applications.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "2108138012",
                    "name": "Youzuo Lin"
                }
            ]
        },
        {
            "paperId": "2d13799dcbd0aefb08c379f58cd6004b1376ca33",
            "title": "OpenFWI: Large-scale Multi-structural Benchmark Datasets for Full Waveform Inversion",
            "abstract": "Full waveform inversion (FWI) is widely used in geophysics to reconstruct high-resolution velocity maps from seismic data. The recent success of data-driven FWI methods results in a rapidly increasing demand for open datasets to serve the geophysics community. We present OpenFWI, a collection of large-scale multi-structural benchmark datasets, to facilitate diversified, rigorous, and reproducible research on FWI. In particular, OpenFWI consists of 12 datasets (2.1TB in total) synthesized from multiple sources. It encompasses diverse domains in geophysics (interface, fault, CO2 reservoir, etc.), covers different geological subsurface structures (flat, curve, etc.), and contains various amounts of data samples (2K - 67K). It also includes a dataset for 3D FWI. Moreover, we use OpenFWI to perform benchmarking over four deep learning methods, covering both supervised and unsupervised learning regimes. Along with the benchmarks, we implement additional experiments, including physics-driven methods, complexity analysis, generalization study, uncertainty quantification, and so on, to sharpen our understanding of datasets and methods. The studies either provide valuable insights into the datasets and the performance, or uncover their current limitations. We hope OpenFWI supports prospective research on FWI and inspires future open-source efforts on AI for science. All datasets and related information can be accessed through our website at https://openfwi-lanl.github.io/",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "100684917",
                    "name": "Shihang Feng"
                },
                {
                    "authorId": "2113289242",
                    "name": "Hanchen Wang"
                },
                {
                    "authorId": "1601013874",
                    "name": "Xitong Zhang"
                },
                {
                    "authorId": "144155035",
                    "name": "Peng Jin"
                },
                {
                    "authorId": "144314826",
                    "name": "Yinan Feng"
                },
                {
                    "authorId": "2114309878",
                    "name": "Qili Zeng"
                },
                {
                    "authorId": "2109306087",
                    "name": "Yinpeng Chen"
                },
                {
                    "authorId": "2108138012",
                    "name": "Youzuo Lin"
                }
            ]
        },
        {
            "paperId": "ef7aa6a7e6d57f2220f9a38ae27a9023082a87c3",
            "title": "OpenFWI: Benchmark Seismic Datasets for Machine Learning-Based Full Waveform Inversion",
            "abstract": "We present O PEN FWI, a collection of large-scale open-source benchmark datasets for seismic full waveform inversion (FWI). OpenFWI is the \ufb01rst-of-its-kind in the geoscience and machine learning community to facilitate diversi\ufb01ed, rigorous, and reproducible research on machine learning-based FWI. OpenFWI (as shown in \ufb01gure 1) includes datasets of multiple scales, encompasses diverse domains, and covers various levels of model complexity. Along with the dataset, we also perform an empirical study on each dataset with a fully-convolutional deep learning model. OpenFWI has been meticulously maintained and will be regularly updated with new data and experimental results. We appreciate the inputs from the community to help us further improve OpenFWI. At the current version, we publish seven datasets in OpenFWI, of which one is speci\ufb01ed for 3D FWI and the rest are for 2D scenarios. All datasets and related information 1 can be accessed through our website at https://openfwi-lanl.github.io/ .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "144314826",
                    "name": "Yinan Feng"
                },
                {
                    "authorId": "100684917",
                    "name": "Shihang Feng"
                },
                {
                    "authorId": "144155035",
                    "name": "Peng Jin"
                },
                {
                    "authorId": "1601013874",
                    "name": "Xitong Zhang"
                },
                {
                    "authorId": "2114309878",
                    "name": "Qili Zeng"
                },
                {
                    "authorId": "2108138012",
                    "name": "Youzuo Lin"
                }
            ]
        },
        {
            "paperId": "78fde53627212b878b93ddc7750ccae1f4f76df7",
            "title": "On the Global Self-attention Mechanism for Graph Convolutional Networks",
            "abstract": "Applying Global Self-attention (GSA) mechanism over features has achieved remarkable success on Convolutional Neural Networks (CNNs). However, it is not clear if Graph Convolutional Networks (GCNs) can similarly benefit from such a technique. In this paper, inspired by the similarity between CNNs and GCNs, we study the impact of the Global Self-attention mechanism on GCNs. We find that consistent with the intuition, the GSA mechanism allows GCNs to capture feature-based vertex relations regardless of edge connections; As a result, the GSA mechanism can introduce extra expressive power to the GCNs. Furthermore, we analyze the impacts of the GSA mechanism on the issues of overfitting and over-smoothing. We prove that the GSA mechanism can alleviate both the overfitting and the over-smoothing issues based on some recent technical developments. Experiments on multiple benchmark datasets illustrate both superior expressive power and less significant overfitting and over-smoothing problems for the GSA-augmented GCNs, which corroborate the intuitions and the theoretical results.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2146562893",
                    "name": "Chen Wang"
                },
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                }
            ]
        },
        {
            "paperId": "34873e9d06c9a88835e920e93df644fa08ff94c6",
            "title": "SAG-VAE: End-to-end Joint Inference of Data Representations and Feature Relations",
            "abstract": "The ability to capture relations within data can provide the much needed inductive bias for robust and interpretable Machine Learning algorithms. Variational Autoencoder (VAE) is a promising candidate for such purpose thanks to their power in data representation inference, but its vanilla form and common variations cannot process feature relations. In this paper, inspired by recent advances in relational learning with graph neural networks, we propose the Self-Attention Graph Variational AutoEncoder (SAG-VAE) model which can simultaneously learn feature relations and data representations in an end-to-end manner. The SAG-VAE is trained by jointly inferring the posterior distribution of two types of latent variables, which respectively represent the data and the feature relations. The feature relations are represented as a graph structure, and the presence of each edge is determined by a Gumbel-Softmax distribution. The generative model is accordingly parameterized by a graph neural network with a special attention mechanism we introduced in the paper. Therefore, the SAG-VAE model can generate via graph convolution and be trained via backpropagation. Experiments based on graphs show that SAG-VAE is capable of approximately retrieving edges and links between vertices based entirely on feature observations. Furthermore, experiments on image data illustrate that the learned feature relations can provide the SAG-VAE robustness against perturbations in image reconstruction and sampling. The learned feature relations as graph adjacency matrices are observed to be structured, which provides intuitive interpretability of the models.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2146562893",
                    "name": "Chen Wang"
                },
                {
                    "authorId": "151483422",
                    "name": "Chengyuan Deng"
                },
                {
                    "authorId": "2072422482",
                    "name": "Vladimir A. Ivanov"
                }
            ]
        }
    ]
}