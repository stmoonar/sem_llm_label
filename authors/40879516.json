{
    "authorId": "40879516",
    "papers": [
        {
            "paperId": "0706d3d5c43269d812d5f931c38fb31b22ef3248",
            "title": "Averaging Rate Scheduler for Decentralized Learning on Heterogeneous Data",
            "abstract": "State-of-the-art decentralized learning algorithms typically require the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the data distribution across the agents can have significant heterogeneity. In this work, we propose averaging rate scheduling as a simple yet effective way to reduce the impact of heterogeneity in decentralized learning. Our experiments illustrate the superiority of the proposed method (~3% improvement in test accuracy) compared to the conventional approach of employing a constant averaging rate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "2212559688",
                    "name": "Sakshi Choudhary"
                },
                {
                    "authorId": "2242959790",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "46face0a60d7413c2d0d32160c5453dd95d70ed6",
            "title": "Towards Two-Stream Foveation-based Active Vision Learning",
            "abstract": "Deep neural network (DNN) based machine perception frameworks process the entire input in a one-shot manner to provide answers to both\"what object is being observed\"and\"where it is located\". In contrast, the\"two-stream hypothesis\"from neuroscience explains the neural processing in the human visual cortex as an active vision system that utilizes two separate regions of the brain to answer the what and the where questions. In this work, we propose a machine learning framework inspired by the\"two-stream hypothesis\"and explore the potential benefits that it offers. Specifically, the proposed framework models the following mechanisms: 1) ventral (what) stream focusing on the input regions perceived by the fovea part of an eye (foveation), 2) dorsal (where) stream providing visual guidance, and 3) iterative processing of the two streams to calibrate visual focus and process the sequence of focused image patches. The training of the proposed framework is accomplished by label-based DNN training for the ventral stream model and reinforcement learning for the dorsal stream model. We show that the two-stream foveation-based learning is applicable to the challenging task of weakly-supervised object localization (WSOL), where the training data is limited to the object class or its attributes. The framework is capable of both predicting the properties of an object and successfully localizing it by predicting its bounding box. We also show that, due to the independent nature of the two streams, the dorsal model can be applied on its own to unseen images to localize objects from different datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2972480",
                    "name": "Timur Ibrayev"
                },
                {
                    "authorId": "30846127",
                    "name": "Amitangshu Mukherjee"
                },
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "2281943684",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "608afab79a714ccc3265411cc03d7027a07ac826",
            "title": "AdaGossip: Adaptive Consensus Step-size for Decentralized Deep Learning with Communication Compression",
            "abstract": "Decentralized learning is crucial in supporting on-device learning over large distributed datasets, eliminating the need for a central server. However, the communication overhead remains a major bottleneck for the practical realization of such decentralized setups. To tackle this issue, several algorithms for decentralized training with compressed communication have been proposed in the literature. Most of these algorithms introduce an additional hyper-parameter referred to as consensus step-size which is tuned based on the compression ratio at the beginning of the training. In this work, we propose AdaGossip, a novel technique that adaptively adjusts the consensus step-size based on the compressed model differences between neighboring agents. We demonstrate the effectiveness of the proposed method through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, Imagenette, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance ($0-2\\%$ improvement in test accuracy) compared to the current state-of-the-art method for decentralized learning with communication compression.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "2288265561",
                    "name": "Abolfazl Hashemi"
                },
                {
                    "authorId": "2242959790",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "a94299289608efa10252a205ce2c12f109e0f7bb",
            "title": "SADDLe: Sharpness-Aware Decentralized Deep Learning with Heterogeneous Data",
            "abstract": "Decentralized training enables learning with distributed datasets generated at different locations without relying on a central server. In realistic scenarios, the data distribution across these sparsely connected learning agents can be significantly heterogeneous, leading to local model over-fitting and poor global model generalization. Another challenge is the high communication cost of training models in such a peer-to-peer fashion without any central coordination. In this paper, we jointly tackle these two-fold practical challenges by proposing SADDLe, a set of sharpness-aware decentralized deep learning algorithms. SADDLe leverages Sharpness-Aware Minimization (SAM) to seek a flatter loss landscape during training, resulting in better model generalization as well as enhanced robustness to communication compression. We present two versions of our approach and conduct extensive experiments to show that SADDLe leads to 1-20% improvement in test accuracy compared to other existing techniques. Additionally, our proposed approach is robust to communication compression, with an average drop of only 1% in the presence of up to 4x compression.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212559688",
                    "name": "Sakshi Choudhary"
                },
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "2242959790",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "14fc27f151a12e141e8cc84da050e64b8d90eb56",
            "title": "CoDeC: Communication-Efficient Decentralized Continual Learning",
            "abstract": "Training at the edge utilizes continuously evolving data generated at different locations. Privacy concerns prohibit the co-location of this spatially as well as temporally distributed data, deeming it crucial to design training algorithms that enable efficient continual learning over decentralized private data. Decentralized learning allows serverless training with spatially distributed data. A fundamental barrier in such distributed learning is the high bandwidth cost of communicating model updates between agents. Moreover, existing works under this training paradigm are not inherently suitable for learning a temporal sequence of tasks while retaining the previously acquired knowledge. In this work, we propose CoDeC, a novel communication-efficient decentralized continual learning algorithm which addresses these challenges. We mitigate catastrophic forgetting while learning a task sequence in a decentralized learning setup by combining orthogonal gradient projection with gossip averaging across decentralized agents. Further, CoDeC includes a novel lossless communication compression scheme based on the gradient subspaces. We express layer-wise gradients as a linear combination of the basis vectors of these gradient subspaces and communicate the associated coefficients. We theoretically analyze the convergence rate for our algorithm and demonstrate through an extensive set of experiments that CoDeC successfully learns distributed continual tasks with minimal forgetting. The proposed compression scheme results in up to 4.8x reduction in communication costs with iso-performance as the full communication baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2212559688",
                    "name": "Sakshi Choudhary"
                },
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "3b8e534931f42ad8d98fc31e00574ccf0be879e1",
            "title": "Neighborhood Gradient Mean: An Efficient Decentralized Learning Method for Non-IID Data",
            "abstract": "Decentralized learning algorithms enable the training of deep learning models over large distributed datasets, without the need for a central server. The current state-of-the-art de-centralized algorithms mostly assume the data distributions to be Independent and Identi-cally Distributed (IID). In practical scenarios, the distributed datasets can have significantly different data distributions across the agents. This paper focuses on improving decentralized learning on non-IID data with minimal compute and memory overheads. We propose Neighborhood Gradient Mean (NGM) , a novel decentralized learning algorithm that modifies the local gradients of each agent using self-and cross-gradient information. In particular, the proposed method averages the local gradients with model-variant or data-variant cross-gradients based on the communication budget. Model-variant cross-gradients are derivatives of the received neighbors\u2019 model parameters with respect to the local dataset. Data-variant cross-gradient derivatives of the local model with respect to its neighbors\u2019 datasets. The data-variant cross-gradients are aggregated through an additional communication round. We theoretically analyze the convergence characteristics of NGM and demonstrate its efficiency on non-IID data sampled from various vision and language datasets. Our experiments demonstrate that the proposed method either remains competitive or outperforms (by 0 \u2212 6%) the existing state-of-the-art (SoTA) decentralized learning algorithm on non-IID data with significantly less compute and memory requirements. Further, we show that the model-variant cross-gradient information available locally at each agent can improve the performance on non-IID data by 2 \u2212 20% without additional communication cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "20708889",
                    "name": "Sangamesh Kodge"
                },
                {
                    "authorId": "2268786741",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "408984d122f86b7d1741a3a09483841a5fc066d0",
            "title": "Cross-feature Contrastive Loss for Decentralized Deep Learning on Heterogeneous Data",
            "abstract": "The current state-of-the-art decentralized learning algorithms mostly assume the data distribution to be Independent and Identically Distributed (IID). However, in practical scenarios, the distributed datasets can have significantly heterogeneous data distributions across the agents. In this work, we present a novel approach for decentralized learning on heterogeneous data, where data-free knowledge distillation through contrastive loss on cross-features is utilized to improve performance. Cross-features for a pair of neighboring agents are the features (i.e., last hidden layer activations) obtained from the data of an agent with respect to the model parameters of the other agent. We demonstrate the effectiveness of the proposed technique through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, Imagenette, and ImageNet), model architectures, and network topologies. Our experiments show that the proposed method achieves superior performance (0.2 \u2013 4% improvement in test accuracy) compared to other existing techniques for decentralized learning on heterogeneous data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "2242959790",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "a4f01187694c608ae20364d92796d57943da979d",
            "title": "Global Update Tracking: A Decentralized Learning Algorithm for Heterogeneous Data",
            "abstract": "Decentralized learning enables the training of deep learning models over large distributed datasets generated at different locations, without the need for a central server. However, in practical scenarios, the data distribution across these devices can be significantly different, leading to a degradation in model performance. In this paper, we focus on designing a decentralized learning algorithm that is less susceptible to variations in data distribution across devices. We propose Global Update Tracking (GUT), a novel tracking-based method that aims to mitigate the impact of heterogeneous data in decentralized learning without introducing any communication overhead. We demonstrate the effectiveness of the proposed technique through an exhaustive set of experiments on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST, and ImageNette), model architectures, and network topologies. Our experiments show that the proposed method achieves state-of-the-art performance for decentralized learning on heterogeneous data via a $1-6\\%$ improvement in test accuracy compared to other existing techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "3369702",
                    "name": "Abolfazl Hashemi"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "b5c834d985aa3193e37b5a71b4d97b6bc8ae4f99",
            "title": "Homogenizing Non-IID datasets via In-Distribution Knowledge Distillation for Decentralized Learning",
            "abstract": "Decentralized learning enables serverless training of deep neural networks (DNNs) in a distributed manner on multiple nodes. This allows for the use of large datasets, as well as the ability to train with a wide variety of data sources. However, one of the key challenges with decentralized learning is heterogeneity in the data distribution across the nodes. In this paper, we propose In-Distribution Knowledge Distillation (IDKD) to address the challenge of heterogeneous data distribution. The goal of IDKD is to homogenize the data distribution across the nodes. While such data homogenization can be achieved by exchanging data among the nodes sacrificing privacy, IDKD achieves the same objective using a common public dataset across nodes without breaking the privacy constraint. This public dataset is different from the training dataset and is used to distill the knowledge from each node and communicate it to its neighbors through the generated labels. With traditional knowledge distillation, the generalization of the distilled model is reduced because all the public dataset samples are used irrespective of their similarity to the local dataset. Thus, we introduce an Out-of-Distribution (OoD) detector at each node to label a subset of the public dataset that maps close to the local training data distribution. Finally, only labels corresponding to these subsets are exchanged among the nodes and with appropriate label averaging each node is finetuned on these data subsets along with its local data. Our experiments on multiple image classification datasets and graph topologies show that the proposed IDKD scheme is more effective than traditional knowledge distillation and achieves state-of-the-art generalization performance on heterogeneously distributed data with minimal communication overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064867801",
                    "name": "Deepak Ravikumar"
                },
                {
                    "authorId": "145638130",
                    "name": "Gobinda Saha"
                },
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                }
            ]
        },
        {
            "paperId": "60f96291a97a13e9f6145627832ed98bee943dd3",
            "title": "Neighborhood Gradient Clustering: An Efficient Decentralized Learning Method for Non-IID Data Distributions",
            "abstract": "Decentralized learning over distributed datasets can have significantly different data distributions across the agents. The current state-of-the-art decentralized algorithms mostly assume the data distributions to be Independent and Identically Distributed. This paper focuses on improving decentralized learning over non-IID data. We propose \\textit{Neighborhood Gradient Clustering (NGC)}, a novel decentralized learning algorithm that modifies the local gradients of each agent using self- and cross-gradient information. Cross-gradients for a pair of neighboring agents are the derivatives of the model parameters of an agent with respect to the dataset of the other agent. In particular, the proposed method replaces the local gradients of the model with the weighted mean of the self-gradients, model-variant cross-gradients (derivatives of the neighbors' parameters with respect to the local dataset), and data-variant cross-gradients (derivatives of the local model with respect to its neighbors' datasets). The data-variant cross-gradients are aggregated through an additional communication round without breaking the privacy constraints. Further, we present \\textit{CompNGC}, a compressed version of \\textit{NGC} that reduces the communication overhead by $32 \\times$. We theoretically analyze the convergence rate of the proposed algorithm and demonstrate its efficiency over non-IID data sampled from {various vision and language} datasets trained. Our experiments demonstrate that \\textit{NGC} and \\textit{CompNGC} outperform (by $0-6\\%$) the existing SoTA decentralized learning algorithm over non-IID data with significantly less compute and memory requirements. Further, our experiments show that the model-variant cross-gradient information available locally at each agent can improve the performance over non-IID data by $1-35\\%$ without additional communication cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40879516",
                    "name": "Sai Aparna Aketi"
                },
                {
                    "authorId": "20708889",
                    "name": "Sangamesh Kodge"
                },
                {
                    "authorId": "2091913080",
                    "name": "Kaushik Roy"
                }
            ]
        }
    ]
}