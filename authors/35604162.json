{
    "authorId": "35604162",
    "papers": [
        {
            "paperId": "a02dbf0c1d2debc1a825e55f17958853eb3d40aa",
            "title": "Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach",
            "abstract": "In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a ``desired'' classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees and encode context-specific group fairness properties. Evaluation on real-world datasets shows that this approach consistently boosts the quality of collected outcome data and improves the fraction of true positives for all groups, with only a small reduction in predictive utility.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "152276166",
                    "name": "Anay Mehrotra"
                },
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                }
            ]
        },
        {
            "paperId": "9b7cadce8fc34c0d440389f1cfa19dee4af2bace",
            "title": "Designing Closed-Loop Models for Task Allocation",
            "abstract": "Automatically assigning tasks to people is challenging because human performance can vary across tasks for many reasons. This challenge is further compounded in real-life settings in which no oracle exists to assess the quality of human decisions and task assignments made. Instead, we find ourselves in a\"closed\"decision-making loop in which the same fallible human decisions we rely on in practice must also be used to guide task allocation. How can imperfect and potentially biased human decisions train an accurate allocation model? Our key insight is to exploit weak prior information on human-task similarity to bootstrap model training. We show that the use of such a weak prior can improve task allocation accuracy, even when human decision-makers are fallible and biased. We present both theoretical analysis and empirical evaluation over synthetic data and a social media toxicity detection task. Results demonstrate the efficacy of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                }
            ]
        },
        {
            "paperId": "fba0d9aa20862f52ef44cf913ef12e97e0033afe",
            "title": "Social Media Platform Structures and Their Implications (Short Paper)",
            "abstract": "Polarization, the spread of misinformation, and the creation of echo chambers on social media platforms are all phenomena that currently plague our online discourses. Providing an explanation for these phenomena requires an analysis of the broader social system that online content platforms enforce through their platform policies and designs. In this article, I discuss the problematic structures created by current social media platforms which prioritize engagement relations and demonstrate how these structures affect user interactions on the platforms. Parsing these structures provides insight into the platform policies that constrain and influence user actions online and I argue that these structures should be considered when explaining any user\u2019s radical interactions with online communities. Finally, considering the role of platform structures in shaping user actions, I contend that online social media platforms, in their current form, are not just \u201cneutral publishers\u201d anymore.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                }
            ]
        },
        {
            "paperId": "0bb976bce77bea2a0fa75488af4ea12e01117a6b",
            "title": "Addressing Strategic Manipulation Disparities in Fair Classification",
            "abstract": "In real-world classification settings, such as loan application evaluation or content moderation on online platforms, individuals respond to classifier predictions by strategically updating their features to increase their likelihood of receiving a particular (positive) decision (at a certain cost). Yet, when different demographic groups have different feature distributions or pay different update costs, prior work has shown that individuals from minority groups often pay a higher cost to update their features. Fair classification aims to address such classifier performance disparities by constraining the classifiers to satisfy statistical fairness properties. However, we show that standard fairness constraints do not guarantee that the constrained classifier reduces the disparity in strategic manipulation cost. To address such biases in strategic settings and provide equal opportunities for strategic manipulation, we propose a constrained optimization framework that constructs classifiers that lower the strategic manipulation cost for minority groups. We develop our framework by studying theoretical connections between group-specific strategic cost disparity and standard selection rate fairness metrics (e.g., statistical rate and true positive rate). Empirically, we show the efficacy of this approach over multiple real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                }
            ]
        },
        {
            "paperId": "8992513f975d407d6198744123fb3bfdef5a0d7c",
            "title": "Designing Closed Human-in-the-loop Deferral Pipelines",
            "abstract": "In hybrid human-machine deferral frameworks, a classifier can defer uncertain cases to human decision-makers (who are often themselves fallible). Prior work on simultaneous training of such classifier and deferral models has typically assumed access to an oracle during training to obtain true class labels for training samples, but in practice there often is no such oracle. In contrast, we consider a\"closed\"decision-making pipeline in which the same fallible human decision-makers used in deferral also provide training labels. How can imperfect and biased human expert labels be used to train a fair and accurate deferral framework? Our key insight is that by exploiting weak prior information, we can match experts to input examples to ensure fairness and accuracy of the resulting deferral framework, even when imperfect and biased experts are used in place of ground truth labels. The efficacy of our approach is shown both by theoretical analysis and by evaluation on two tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                }
            ]
        },
        {
            "paperId": "e4dc685ef9a2b30d72d8ff111a38662f9e882e38",
            "title": "Diverse Representation via Computational Participatory Elections - Lessons from a Case Study",
            "abstract": "Elections are the central institution of democratic processes, and often the elected body \u2013 in either public or private governance \u2013 is a committee of individuals. To ensure the legitimacy of elected bodies, the electoral processes should guarantee that diverse groups are represented, in particular members of groups that are marginalized due to gender, ethnicity, or other socially salient attributes. To address this challenge of representation, we have designed a novel participatory electoral process coined the Representation Pact, implemented with the support of a computational system. That process explicitly enables voters to flexibly decide on representation criteria in a first round, and then lets them vote for candidates in a second round. After the two rounds, a counting method is applied, which selects the committee of candidates that maximizes the number of votes received in the second round, conditioned on satisfying the criteria provided in the first round. With the help of a detailed use case that applied this process in a primary election of 96 representatives in Switzerland, we explain how this method contributes to fairness in political elections by achieving a better \u201cdescriptive representation\u201d. Further, based on this use case, we identify lessons learnt that are applicable to participatory computational systems used in societal or political contexts. Good practices are identified and presented.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1771698",
                    "name": "Florian Ev\u00e9quoz"
                },
                {
                    "authorId": "2775394",
                    "name": "J. Rochel"
                },
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "2066459737",
                    "name": "L. Celis"
                }
            ]
        },
        {
            "paperId": "16f7627adccc8fc6108b659bc06f34780d3a588a",
            "title": "Towards Unbiased and Accurate Deferral to Multiple Experts",
            "abstract": "Machine learning models are often implemented in cohort with humans in the pipeline, with the model having an option to defer to a domain expert in cases where it has low confidence in its inference. Our goal is to design mechanisms for ensuring accuracy and fairness in such prediction systems that combine machine learning model inferences and domain expert predictions. Prior work on \"deferral systems\" in classification settings has focused on the setting of a pipeline with a single expert and aimed to accommodate the inaccuracies and biases of this expert to simultaneously learn an inference model and a deferral system. Our work extends this framework to settings where multiple experts are available, with each expert having their own domain of expertise and biases. We propose a framework that simultaneously learns a classifier and a deferral system, with the deferral system choosing to defer to one or more human experts in cases of input where the classifier has low confidence. We test our framework on a synthetic dataset and a content moderation dataset with biased synthetic experts, and show that it significantly improves the accuracy and fairness of the final predictions, compared to the baselines. We also collect crowdsourced labels for the content moderation task to construct a real-world dataset for the evaluation of hybrid machine-human frameworks and show that our proposed framework outperforms baselines on this real-world dataset as well.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "1747771",
                    "name": "Matthew Lease"
                },
                {
                    "authorId": "1769861",
                    "name": "K. Kenthapadi"
                }
            ]
        },
        {
            "paperId": "b01688f7586b3ab3ea864cb6698a5aa2dc9c0d67",
            "title": "Auditing for Diversity Using Representative Examples",
            "abstract": "Assessing the diversity of a dataset of information associated with people is crucial before using such data for downstream applications. For a given dataset, this often involves computing the imbalance or disparity in the empirical marginal distribution of a protected attribute (e.g. gender, dialect, etc.). However, real-world datasets, such as images from Google Search or collections of Twitter posts, often do not have protected attributes labeled. Consequently, to derive disparity measures for such datasets, the elements need to hand-labeled or crowd-annotated, which are expensive processes. We propose a cost-effective approach to approximate the disparity of a given unlabeled dataset, with respect to a protected attribute, using a control set of labeled representative examples. Our proposed algorithm uses the pairwise similarity between elements in the dataset and elements in the control set to effectively bootstrap an approximation to the disparity of the dataset. Importantly, we show that using a control set whose size is much smaller than the size of the dataset is sufficient to achieve a small approximation error. Further, based on our theoretical framework, we also provide an algorithm to construct adaptive control sets that achieve smaller approximation errors than randomly chosen control sets. Simulations on two image datasets and one Twitter dataset demonstrate the efficacy of our approach (using random and adaptive control sets) in auditing the diversity of a wide variety of datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                }
            ]
        },
        {
            "paperId": "0c0b5ac3b73f3010cc6a1b73cf9e031cfc26339d",
            "title": "Dialect Diversity in Text Summarization on Twitter",
            "abstract": "Discussions on Twitter involve participation from different communities with different dialects and it is often necessary to summarize a large number of posts into a representative sample to provide a synopsis. Yet, any such representative sample should sufficiently portray the underlying dialect diversity to present the voices of different participating communities representing the dialects. Extractive summarization algorithms perform the task of constructing subsets that succinctly capture the topic of any given set of posts. However, we observe that there is dialect bias in the summaries generated by common summarization approaches, i.e., they often return summaries that under-represent certain dialects. The vast majority of existing \u201cfair\u201d summarization approaches require socially salient attribute labels (in this case, dialect) to ensure that the generated summary is fair with respect to the socially salient attribute. Nevertheless, in many applications, these labels do not exist. Furthermore, due to the ever-evolving nature of dialects in social media, it is unreasonable to label or accurately infer the dialect of every social media post. To correct for the dialect bias, we employ a framework that takes an existing text summarization algorithm as a blackbox and, using a small set of dialect-diverse sentences, returns a summary that is relatively more dialect-diverse. Crucially, this approach does not need the posts being summarized to have dialect labels, ensuring that the diversification process is independent of dialect classification/identification models. We show the efficacy of our approach on Twitter datasets containing posts written in dialects used by different social groups defined by race or gender; in all cases, our approach leads to improved dialect diversity compared to standard text summarization approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                },
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                }
            ]
        },
        {
            "paperId": "4b09b2c7205369d7dde0899f025e1e05675d790f",
            "title": "Fair Classification with Noisy Protected Attributes: A Framework with Provable Guarantees",
            "abstract": "Due to the deployment of classification algorithms in a multitude of applications directly and indirectly affecting people and society, developing methods that are fair with respect to protected attributes such as gender or race is crucial. However, protected attributes in datasets may be inaccurate due to noise in the data collection or if the protected attributes are imputed either in whole or in part. Such inaccuracies can prevent existing fair classification algorithms from achieving their claimed fairness guarantees. Motivated by this, recent works have studied the fair classification problem in which a binary protected attribute is \"noisy\" (the protected type is flipped with a known fixed probability) by either suggesting optimization using tighter statistical or equalized odds constraints to counter the noise or by identifying conditions under which prior equalized odds post-processing algorithms can handle noisy attributes. We extend the study of noise-tolerant fair classification to a very general setting. Our main contribution is an optimization framework for learning a fair classifier in the presence of noisy perturbations in the protected attributes that can be employed with linear and linear-fractional class of fairness constraints, comes with probabilistic guarantees on accuracy and fairness, and can handle multiple, non-binary protected attributes. Empirically, we show that our framework can be used to attain either statistical rate or false positive rate fairness guarantees with a minimal loss in accuracy, even when the noise corruption is large in two real-world datasets. Prior existing noisy fair classification approaches, on the other hand, either do not always achieve the desired fairness levels or suffer a larger loss in accuracy for guaranteeing high fairness compared to our framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144776615",
                    "name": "L. E. Celis"
                },
                {
                    "authorId": "2002075",
                    "name": "Lingxiao Huang"
                },
                {
                    "authorId": "35604162",
                    "name": "Vijay Keswani"
                },
                {
                    "authorId": "1810064",
                    "name": "Nisheeth K. Vishnoi"
                }
            ]
        }
    ]
}