{
    "authorId": "2171130495",
    "papers": [
        {
            "paperId": "0a6bc37a07a37e3573d36e10cc11669eca0ff903",
            "title": "Execution-based Code Generation using Deep Reinforcement Learning",
            "abstract": "The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important to note that PPOCoder is a task-agnostic and model-agnostic framework that can be used across different code generation tasks and PLs. Extensive experiments on three code generation tasks demonstrate the effectiveness of our proposed approach compared to SOTA methods, achieving significant improvements in compilation success rates and functional correctness across different PLs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2037848556",
                    "name": "Parshin Shojaee"
                },
                {
                    "authorId": "2171130495",
                    "name": "Aneesh Jain"
                },
                {
                    "authorId": "2121914227",
                    "name": "Sindhu Tipirneni"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "7e8c707911ee9a695eb1a293178c175063ba9c6b",
            "title": "Team Cadence at MEDIQA-Chat 2023: Generating, augmenting and summarizing clinical dialogue with large language models",
            "abstract": "This paper describes Team Cadence\u2019s winning submission to Task C of the MEDIQA-Chat 2023 shared tasks. We also present the set of methods, including a novel N-pass strategy to summarize a mix of clinical dialogue and an incomplete summarized note, used to complete Task A and Task B, ranking highly on the leaderboard amongst stable and reproducible code submissions. The shared tasks invited participants to summarize, classify and generate patient-doctor conversations. Considering the small volume of training data available, we took a data-augmentation-first approach to the three tasks by focusing on the dialogue generation task, i.e., Task C. It proved effective in improving our models\u2019 performance on Task A and Task B. We also found the BART architecture to be highly versatile, as it formed the base for all our submissions. Finally, based on the results shared by the organizers, we note that Team Cadence was the only team to submit stable and reproducible runs to all three tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32313992",
                    "name": "Ashwyn K Sharma"
                },
                {
                    "authorId": "2223094897",
                    "name": "David I. Feldman"
                },
                {
                    "authorId": "2171130495",
                    "name": "Aneesh Jain"
                }
            ]
        },
        {
            "paperId": "d78d4b2de310824846ca6aa7af3da421b37b60db",
            "title": "Fine-tuning pre-trained extractive QA models for clinical document parsing",
            "abstract": "Electronic health records (EHRs) contain a vast amount of high-dimensional multi-modal data that can accurately represent a patient's medical history. Unfortunately, most of this data is either unstructured or semi-structured, rendering it unsuitable for real-time and retrospective analyses. A remote patient monitoring (RPM) program for Heart Failure (HF) patients needs to have access to clinical markers like EF (Ejection Fraction) or LVEF (Left Ventricular Ejection Fraction) in order to ascertain eligibility and appropriateness for the program. This paper explains a system that can parse echocardiogram reports and verify EF values. This system helps identify eligible HF patients who can be enrolled in such a program. At the heart of this system is a pre-trained extractive QA transformer model that is fine-tuned on custom-labeled data. The methods used to prepare such a model for deployment are illustrated by running experiments on a public clinical dataset like MIMIC-IV-Note. The pipeline can be used to generalize solutions to similar problems in a low-resource setting. We found that the system saved over 1500 hours for our clinicians over 12 months by automating the task at scale.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261451249",
                    "name": "Ashwyn Sharma"
                },
                {
                    "authorId": "2223094897",
                    "name": "David I. Feldman"
                },
                {
                    "authorId": "2171130495",
                    "name": "Aneesh Jain"
                }
            ]
        },
        {
            "paperId": "660fde2f51e025638b8c937bf228ecaa5c5b649c",
            "title": "XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence",
            "abstract": "Recent advances in machine learning have significantly improved the understanding of source code data and achieved good performance on a number of downstream tasks. Open source repositories like GitHub enable this process with rich unlabeled code data. However, the lack of high quality labeled data has largely hindered the progress of several code related tasks, such as program translation, summarization, synthesis, and code search. This paper introduces XLCoST, Cross-Lingual Code SnippeT dataset, a new benchmark dataset for cross-lingual code intelligence. Our dataset contains fine-grained parallel data from 8 languages (7 commonly used programming languages and English), and supports 10 cross-lingual code tasks. To the best of our knowledge, it is the largest parallel dataset for source code both in terms of size and the number of languages. We also provide the performance of several state-of-the-art baseline models for each task. We believe this new dataset can be a valuable asset for the research community and facilitate the development and validation of new methods for cross-lingual code intelligence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145549146",
                    "name": "Ming Zhu"
                },
                {
                    "authorId": "2171130495",
                    "name": "Aneesh Jain"
                },
                {
                    "authorId": "144700168",
                    "name": "Karthik Suresh"
                },
                {
                    "authorId": "2171106643",
                    "name": "Roshan Ravindran"
                },
                {
                    "authorId": "2121914227",
                    "name": "Sindhu Tipirneni"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "9237f964c0a8068e7c5c9fd0767bd38051fdf3df",
            "title": "A JPEG compression resistant steganography scheme for raster graphics images",
            "abstract": "Steganography is the science and art of hiding data into inconspicuous media. Various steganography schemes exist which can be utilized to hide data in digital images without bringing about any perceptible change in the image. Most of these schemes lack the robustness to retain the hidden data after the image has been converted to another format using a lossy compression algorithm. We propose a scheme which hides data in bitmap images, in a way that there is almost no perceptible difference between the original image and this new image and which is also resistant to JPEG compression. In all the tests we were able to retrieve the whole data from an image after we had hidden it in a raster graphics image and the image had been compressed using the JPEG algorithm. JPEG compression is performed independently on blocks of 8x8 pixels in an image while converting it to the JPEG format. The proposed scheme makes use of this property.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171130495",
                    "name": "Aneesh Jain"
                },
                {
                    "authorId": "46354929",
                    "name": "I. Gupta"
                }
            ]
        }
    ]
}