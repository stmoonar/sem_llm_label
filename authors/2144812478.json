{
    "authorId": "2144812478",
    "papers": [
        {
            "paperId": "8d87968533b327065d0a3331ad529f501c097e0e",
            "title": "vONTSS: vMF based semi-supervised neural topic modeling with optimal transport",
            "abstract": "Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state-of-the-art weakly supervised text classification method while achieving similar classification performance. We further prove the equivalence of optimal transport loss and cross-entropy loss at the global minimum.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2144812478",
                    "name": "Xiaoyu Jiang"
                },
                {
                    "authorId": "1757518",
                    "name": "Srinivasan H. Sengamedu"
                },
                {
                    "authorId": "2171429265",
                    "name": "Francis Iannacci"
                },
                {
                    "authorId": "1845781483",
                    "name": "Jinjin Zhao"
                }
            ]
        },
        {
            "paperId": "bf45a7a3d64ce9abe10dbbab853a7c351f7e5a90",
            "title": "KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation",
            "abstract": "In text classification tasks, fine tuning pretrained language models like BERT and GPT-3 yields competitive accuracy; however, both methods require pretraining on large text datasets. In contrast, general topic modeling methods possess the advantage of analyzing documents to extract meaningful patterns of words without the need of pretraining. To leverage topic modeling's unsupervised insights extraction on text classification tasks, we develop the Knowledge Distillation Semi-supervised Topic Modeling (KDSTM). KDSTM requires no pretrained embeddings, few labeled documents and is efficient to train, making it ideal under resource constrained settings. Across a variety of datasets, our method outperforms existing supervised topic modeling methods in classification accuracy, robustness and efficiency and achieves similar performance compare to state of the art weakly supervised text classification methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2144812478",
                    "name": "Xiaoyu Jiang"
                },
                {
                    "authorId": "2053066225",
                    "name": "Jay Desai"
                },
                {
                    "authorId": "2153287302",
                    "name": "Bin Han"
                },
                {
                    "authorId": "2171428787",
                    "name": "Fuqin Yan"
                },
                {
                    "authorId": "2171429265",
                    "name": "Francis Iannacci"
                }
            ]
        },
        {
            "paperId": "ceba8c79009800d151c81fd8db990370c91a04f1",
            "title": "S2vNTM: Semi-supervised vMF Neural Topic Modeling",
            "abstract": "Language model based methods are powerful techniques for text classification. However, the models have several shortcomings. (1) It is difficult to integrate human knowledge such as keywords. (2) It needs a lot of resources to train the models. (3) It relied on large text data to pretrain. In this paper, we propose Semi-Supervised vMF Neural Topic Modeling (S2vNTM) to overcome these difficulties. S2vNTM takes a few seed keywords as input for topics. S2vNTM leverages the pattern of keywords to identify potential topics, as well as optimize the quality of topics' keywords sets. Across a variety of datasets, S2vNTM outperforms existing semi-supervised topic modeling methods in classification accuracy with limited keywords provided. S2vNTM is at least twice as fast as baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2053066225",
                    "name": "Jay Desai"
                },
                {
                    "authorId": "1757518",
                    "name": "Srinivasan H. Sengamedu"
                },
                {
                    "authorId": "2144812478",
                    "name": "Xiaoyu Jiang"
                },
                {
                    "authorId": "2171429265",
                    "name": "Francis Iannacci"
                }
            ]
        },
        {
            "paperId": "5a12520cf26b8c78dff3b5e115678d83648b5866",
            "title": "IPF-LASSO: Integrative L 1-Penalized Regression with Penalty Factors for Prediction Based on Multi-Omics Data",
            "abstract": "As modern biotechnologies advance, it has become increasingly frequent that different modalities of high-dimensional molecular data (termed \u201comics\u201d data in this paper), such as gene expression, methylation, and copy number, are collected from the same patient cohort to predict the clinical outcome. While prediction based on omics data has been widely studied in the last fifteen years, little has been done in the statistical literature on the integration of multiple omics modalities to select a subset of variables for prediction, which is a critical task in personalized medicine. In this paper, we propose a simple penalized regression method to address this problem by assigning different penalty factors to different data modalities for feature selection and prediction. The penalty factors can be chosen in a fully data-driven fashion by cross-validation or by taking practical considerations into account. In simulation studies, we compare the prediction performance of our approach, called IPF-LASSO (Integrative LASSO with Penalty Factors) and implemented in the R package ipflasso, with the standard LASSO and sparse group LASSO. The use of IPF-LASSO is also illustrated through applications to two real-life cancer datasets. All data and codes are available on the companion website to ensure reproducibility.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1751504",
                    "name": "A. Boulesteix"
                },
                {
                    "authorId": "1976901",
                    "name": "R. D. Bin"
                },
                {
                    "authorId": "2144812478",
                    "name": "Xiaoyu Jiang"
                },
                {
                    "authorId": "2059524903",
                    "name": "Mathias Fuchs"
                }
            ]
        }
    ]
}