{
    "authorId": "35584853",
    "papers": [
        {
            "paperId": "028d75496e51943f52c7b2177344a3c089c18058",
            "title": "Fine-grained Hallucination Detection and Editing for Language Models",
            "abstract": "Large language models (LMs) are prone to generate factual errors, which are often called hallucinations. In this paper, we introduce a comprehensive taxonomy of hallucinations and argue that hallucinations manifest in diverse forms, each requiring varying degrees of careful assessments to verify factuality. We propose a novel task of automatic fine-grained hallucination detection and construct a new evaluation benchmark, FavaBench, that includes about one thousand fine-grained human judgments on three LM outputs across various domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B) exhibit diverse types of hallucinations in the majority of their outputs in information-seeking scenarios. We train FAVA, a retrieval-augmented LM by carefully creating synthetic data to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that FAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination detection, and edits suggested by FAVA improve the factuality of LM-generated text.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2279677197",
                    "name": "Abhika Mishra"
                },
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                },
                {
                    "authorId": "2257032956",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                }
            ]
        },
        {
            "paperId": "453ecd6c333cbcafbd900633092c73a63feab8eb",
            "title": "CodeRAG-Bench: Can Retrieval Augment Code Generation?",
            "abstract": "While language models (LMs) have proven remarkably adept at generating code, many programs are challenging for LMs to generate using their parametric knowledge alone. Providing external contexts such as library documentation can facilitate generating accurate and functional code. Despite the success of retrieval-augmented generation (RAG) in various text-oriented tasks, its potential for improving code generation remains under-explored. In this work, we conduct a systematic, large-scale analysis by asking: in what scenarios can retrieval benefit code generation models? and what challenges remain? We first curate a comprehensive evaluation benchmark, CodeRAG-Bench, encompassing three categories of code generation tasks, including basic programming, open-domain, and repository-level problems. We aggregate documents from five sources for models to retrieve contexts: competition solutions, online tutorials, library documentation, StackOverflow posts, and GitHub repositories. We examine top-performing models on CodeRAG-Bench by providing contexts retrieved from one or multiple sources. While notable gains are made in final code generation by retrieving high-quality contexts across various settings, our analysis reveals room for improvement -- current retrievers still struggle to fetch useful contexts especially with limited lexical overlap, and generators fail to improve with limited context lengths or abilities to integrate additional contexts. We hope CodeRAG-Bench serves as an effective testbed to encourage further development of advanced code-oriented RAG methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307569999",
                    "name": "Zora Zhiruo Wang"
                },
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "2118211280",
                    "name": "Xinyan Velocity Yu"
                },
                {
                    "authorId": "2307994808",
                    "name": "Frank F. Xu"
                },
                {
                    "authorId": "2264042161",
                    "name": "Yiqing Xie"
                },
                {
                    "authorId": "2265547593",
                    "name": "Graham Neubig"
                },
                {
                    "authorId": "2264120940",
                    "name": "Daniel Fried"
                }
            ]
        },
        {
            "paperId": "cfa85c8db829dbd2384ea7f130f462e7e7f1f630",
            "title": "Reliable, Adaptable, and Attributable Language Models with Retrieval",
            "abstract": "Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose retrieval-augmented LMs. This involves a reconsideration of datastores and retrievers, the exploration of pipelines with improved retriever-LM interaction, and significant investment in infrastructure for efficient training and inference.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "49164966",
                    "name": "Zexuan Zhong"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "2303396379",
                    "name": "Pang Wei Koh"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2264251662",
                    "name": "Hanna Hajishirzi"
                },
                {
                    "authorId": "2072801764",
                    "name": "Wen-tau Yih"
                }
            ]
        },
        {
            "paperId": "481d29cc4cd7d5d41faed2c3a84bbb19ebc5b027",
            "title": "AfriQA: Cross-lingual Open-Retrieval Question Answering for African Languages",
            "abstract": "African languages have far less in-language content available digitally, making it challenging for question answering systems to satisfy the information needs of users. Cross-lingual open-retrieval question answering (XOR QA) systems -- those that retrieve answer content from other languages while serving people in their native language -- offer a means of filling this gap. To this end, we create AfriQA, the first cross-lingual QA dataset with a focus on African languages. AfriQA includes 12,000+ XOR QA examples across 10 African languages. While previous datasets have focused primarily on languages where cross-lingual QA augments coverage from the target language, AfriQA focuses on languages where cross-lingual answer content is the only high-coverage source of answer content. Because of this, we argue that African languages are one of the most important and realistic use cases for XOR QA. Our experiments demonstrate the poor performance of automatic translation and multilingual retrieval methods. Overall, AfriQA proves challenging for state-of-the-art QA models. We hope that the dataset enables the development of more equitable QA technology.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2166106776",
                    "name": "Odunayo Ogundepo"
                },
                {
                    "authorId": "2352354",
                    "name": "T. Gwadabe"
                },
                {
                    "authorId": "2059157845",
                    "name": "Clara Rivera"
                },
                {
                    "authorId": "144797264",
                    "name": "J. Clark"
                },
                {
                    "authorId": "2884561",
                    "name": "Sebastian Ruder"
                },
                {
                    "authorId": "2518906",
                    "name": "David Ifeoluwa Adelani"
                },
                {
                    "authorId": "1591111757",
                    "name": "Bonaventure F. P. Dossou"
                },
                {
                    "authorId": "2087650847",
                    "name": "Abdoulahat Diop"
                },
                {
                    "authorId": "1573690693",
                    "name": "Claytone Sikasote"
                },
                {
                    "authorId": "2114722803",
                    "name": "Gilles Hacheme"
                },
                {
                    "authorId": "1395556657",
                    "name": "Happy Buzaaba"
                },
                {
                    "authorId": "50203736",
                    "name": "Ignatius M Ezeani"
                },
                {
                    "authorId": "2140114498",
                    "name": "Rooweither Mabuya"
                },
                {
                    "authorId": "1486204986",
                    "name": "Salomey Osei"
                },
                {
                    "authorId": "1591176064",
                    "name": "Chris C. Emezue"
                },
                {
                    "authorId": "66495690",
                    "name": "A. Kahira"
                },
                {
                    "authorId": "7744881",
                    "name": "Shamsuddeen Hassan Muhammad"
                },
                {
                    "authorId": "2175480812",
                    "name": "Akintunde Oladipo"
                },
                {
                    "authorId": "2188832804",
                    "name": "A. Owodunni"
                },
                {
                    "authorId": "2148631756",
                    "name": "A. Tonja"
                },
                {
                    "authorId": "2163094493",
                    "name": "Iyanuoluwa Shode"
                },
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "98725872",
                    "name": "T. Ajayi"
                },
                {
                    "authorId": "2056776870",
                    "name": "Clemencia Siro"
                },
                {
                    "authorId": "2058369323",
                    "name": "Steven Arthur"
                },
                {
                    "authorId": "2056770646",
                    "name": "Mofetoluwa Adeyemi"
                },
                {
                    "authorId": "1452686038",
                    "name": "Orevaoghene Ahia"
                },
                {
                    "authorId": "2047583795",
                    "name": "Aremu Anuoluwapo"
                },
                {
                    "authorId": "117288935",
                    "name": "O. Awosan"
                },
                {
                    "authorId": "73054967",
                    "name": "C. Chukwuneke"
                },
                {
                    "authorId": "2060867724",
                    "name": "Bernard Opoku"
                },
                {
                    "authorId": "88500872",
                    "name": "A. Ayodele"
                },
                {
                    "authorId": "90459153",
                    "name": "V. Otiende"
                },
                {
                    "authorId": "2282726762",
                    "name": "Christine Mwase"
                },
                {
                    "authorId": "81032038",
                    "name": "B. Sinkala"
                },
                {
                    "authorId": "2158994685",
                    "name": "Andre Niyongabo Rubungo"
                },
                {
                    "authorId": "1712191202",
                    "name": "Daniel Ajisafe"
                },
                {
                    "authorId": "2215213107",
                    "name": "Emeka Onwuegbuzia"
                },
                {
                    "authorId": "2216786549",
                    "name": "Habib Mbow"
                },
                {
                    "authorId": "2216795443",
                    "name": "Emile Niyomutabazi"
                },
                {
                    "authorId": "2216889086",
                    "name": "Eunice Mukonde"
                },
                {
                    "authorId": "1588248096",
                    "name": "F. I. Lawan"
                },
                {
                    "authorId": "153795444",
                    "name": "I. Ahmad"
                },
                {
                    "authorId": "122367036",
                    "name": "Jesujoba Oluwadara Alabi"
                },
                {
                    "authorId": "2216891085",
                    "name": "Martin Namukombo"
                },
                {
                    "authorId": "2216786546",
                    "name": "Mbonu Chinedu"
                },
                {
                    "authorId": "2216787102",
                    "name": "Mofya Phiri"
                },
                {
                    "authorId": "2216798369",
                    "name": "Neo Putini"
                },
                {
                    "authorId": "2216787095",
                    "name": "Ndumiso Mngoma"
                },
                {
                    "authorId": "2190281321",
                    "name": "Priscilla Amuok"
                },
                {
                    "authorId": "82370080",
                    "name": "R. Iro"
                },
                {
                    "authorId": "2216878430",
                    "name": "Sonia Adhiambo34"
                }
            ]
        },
        {
            "paperId": "a4867148c2f692efc6c22c3935a59be2d04ea3e9",
            "title": "How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval",
            "abstract": "Various techniques have been developed in recent years to improve dense retrieval (DR), such as unsupervised contrastive learning and pseudo-query generation. Existing DRs, however, often suffer from effectiveness tradeoffs between supervised and zero-shot retrieval, which some argue was due to the limited model capacity. We contradict this hypothesis and show that a generalizable DR can be trained to achieve high accuracy in both supervised and zero-shot retrieval without increasing model size. In particular, we systematically examine the contrastive learning of DRs, under the framework of Data Augmentation (DA). Our study shows that common DA practices such as query augmentation with generative models and pseudo-relevance label creation using a cross-encoder, are often inefficient and sub-optimal. We hence propose a new DA approach with diverse queries and sources of supervision to progressively train a generalizable DR. As a result, DRAGON, our dense retriever trained with diverse augmentation, is the first BERT-base-sized DR to achieve state-of-the-art effectiveness in both supervised and zero-shot evaluations and even competes with models using more complex late interaction (ColBERTv2 and SPLADE++).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "122045993",
                    "name": "Sheng-Chieh Lin"
                },
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "2135230554",
                    "name": "Minghan Li"
                },
                {
                    "authorId": "9185192",
                    "name": "Barlas O\u011fuz"
                },
                {
                    "authorId": "145580839",
                    "name": "Jimmy J. Lin"
                },
                {
                    "authorId": "2121361882",
                    "name": "Yashar Mehdad"
                },
                {
                    "authorId": "2072801764",
                    "name": "Wen-tau Yih"
                },
                {
                    "authorId": "1769736",
                    "name": "Xilun Chen"
                }
            ]
        },
        {
            "paperId": "acbabae091bed349535537b05a9ee80467faf255",
            "title": "TaskWeb: Selecting Better Source Tasks for Multi-task NLP",
            "abstract": "Recent work in NLP has shown promising results in training models on large amounts of tasks to achieve better generalization. However, it is not well-understood how tasks are related, and how helpful training tasks can be chosen for a new task. In this work, we investigate whether knowing task relationships via pairwise task transfer improves choosing one or more source tasks that help to learn a new target task. We provide TaskWeb, a large-scale benchmark of pairwise task transfers for 22 NLP tasks using three different model types, sizes, and adaptation methods, spanning about 25,000 experiments. Then, we design a new method TaskShop based on our analysis of TaskWeb. TaskShop uses TaskWeb to estimate the benefit of using a source task for learning a new target task, and to choose a subset of helpful training tasks for multi-task training. Our method improves overall rankings and top-k precision of source tasks by 10% and 38%, respectively. We also use TaskShop to build much smaller multi-task training sets that improve zero-shot performances across 11 different target tasks by at least 4.3%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117060176",
                    "name": "Joongwon Kim"
                },
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "1387994137",
                    "name": "Gabriel Ilharco"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                }
            ]
        },
        {
            "paperId": "c1c98ef93fb6474837961ef300cf3d8e7d3a0cd0",
            "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
            "abstract": "Despite remarkable advancements in few-shot generalization in natural language processing, most models are developed and evaluated primarily in English. To establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer, we introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across 54 languages in a sequence-to-sequence format and provides a fixed set of few-shot examples and instructions. Using BUFFET, we perform thorough evaluations of ten state-of-the-art multilingual large language models with different transfer methods, namely in-context learning and fine-tuning. Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. Strong multilingual pre-trained or instruction-tuned models such as BLOOM or ChatGPT often lag behind much smaller mT5-base models given the same number of few-shot samples, particularly in low-resource languages. Our analysis suggests avenues for future research in few-shot cross-lingual transfer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "35871436",
                    "name": "Sneha Kudugunta"
                },
                {
                    "authorId": "2118211280",
                    "name": "Xinyan Velocity Yu"
                },
                {
                    "authorId": "3443287",
                    "name": "Terra Blevins"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "1557386977",
                    "name": "Machel Reid"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2884561",
                    "name": "Sebastian Ruder"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                }
            ]
        },
        {
            "paperId": "cb62b09398c0ae0039193a1c98fbabbbc2977ec3",
            "title": "xPQA: Cross-Lingual Product Question Answering in 12 Languages",
            "abstract": "Product Question Answering (PQA) systems are key in e-commerce applications as they provide responses to customers\u2019 questions as they shop for products. While existing work on PQA focuses mainly on English, in practice there is need to support multiple customer languages while leveraging product information available in English. To study this practical industrial task, we present xPQA, a large-scale annotated cross-lingual PQA dataset in 12 languages, and report results in (1) candidate ranking, to select the best English candidate containing the information to answer a non-English question; and (2) answer generation, to generate a natural-sounding non-English answer based on the selected English candidate.We evaluate various approaches involving machine translation at runtime or offline, leveraging multilingual pre-trained LMs, and including or excluding xPQA training data. We find that in-domain data is essential as cross-lingual rankers trained on other domains perform poorly on the PQA task, and that translation-based approaches are most effective for candidate ranking while multilingual finetuning works best for answer generation. Still, there remains a significant performance gap between the English and the cross-lingual test sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2562211",
                    "name": "Xiaoyu Shen"
                },
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "36126076",
                    "name": "B. Byrne"
                },
                {
                    "authorId": "1786911",
                    "name": "A. Gispert"
                }
            ]
        },
        {
            "paperId": "ddbd8fe782ac98e9c64dd98710687a962195dd9b",
            "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
            "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "7806955",
                    "name": "Zeqiu Wu"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2707234",
                    "name": "Avirup Sil"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                }
            ]
        },
        {
            "paperId": "32cdcf593fef78dcf9e1b6aba7f2345310d7cc60",
            "title": "Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources",
            "abstract": "While the NLP community is generally aware of resource disparities among languages, we lack research that quantifies the extent and types of such disparity. Prior surveys estimating the availability of resources based on the number of datasets can be misleading as dataset quality varies: many datasets are automatically induced or translated from English data. To provide a more comprehensive picture of language resources, we examine the characteristics of 156 publicly available NLP datasets. We manually annotate how they are created, including input text and label sources and tools used to build them, and what they study, tasks they address and motivations for their creation. After quantifying the qualitative NLP resource gap across languages, we discuss how to improve data collection in low-resource languages. We survey language-proficient NLP researchers and crowd workers per language, finding that their estimated availability correlates with dataset availability. Through crowdsourcing experiments, we identify strategies for collecting high-quality multilingual data on the Mechanical Turk platform. We conclude by making macro and micro-level suggestions to the NLP community and individual researchers for future multilingual data development.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118211280",
                    "name": "Xinyan Velocity Yu"
                },
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "2174026851",
                    "name": "Trina Chatterjee"
                },
                {
                    "authorId": "2149221827",
                    "name": "Junjie Hu"
                },
                {
                    "authorId": "2890423",
                    "name": "Eunsol Choi"
                }
            ]
        }
    ]
}