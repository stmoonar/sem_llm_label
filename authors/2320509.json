{
    "authorId": "2320509",
    "papers": [
        {
            "paperId": "030690341bd25d05255f6ff69f8cf72a545a6a44",
            "title": "GenEARL: A Training-Free Generative Framework for Multimodal Event Argument Role Labeling",
            "abstract": "Multimodal event argument role labeling (EARL), a task that assigns a role for each event participant (object) in an image is a complex challenge. It requires reasoning over the entire image, the depicted event, and the interactions between various objects participating in the event. Existing models heavily rely on high-quality event-annotated training data to understand the event semantics and structures, and they fail to generalize to new event types and domains. In this paper, we propose GenEARL, a training-free generative framework that harness the power of the modern generative models to understand event task descriptions given image contexts to perform the EARL task. Specifically, GenEARL comprises two stages of generative prompting with a frozen vision-language model (VLM) and a frozen large language model (LLM). First, a generative VLM learns the semantics of the event argument roles and generates event-centric object descriptions based on the image. Subsequently, a LLM is prompted with the generated object descriptions with a predefined template for EARL (i.e., assign an object with an event argument role). We show that GenEARL outperforms the contrastive pretraining (CLIP) baseline by 9.4% and 14.2% accuracy for zero-shot EARL on the M2E2 and SwiG datasets, respectively. In addition, we outperform CLIP-Event by 22% precision on M2E2 dataset. The framework also allows flexible adaptation and generalization to unseen domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "103404553",
                    "name": "Hritik Bansal"
                },
                {
                    "authorId": "2008339028",
                    "name": "Po-Nien Kung"
                },
                {
                    "authorId": "1970636",
                    "name": "P. Brantingham"
                },
                {
                    "authorId": "2257127887",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2293175824",
                    "name": "Nanyun Peng"
                },
                {
                    "authorId": "2285263",
                    "name": "Jean-Baptiste Alayrac"
                },
                {
                    "authorId": "2295673408",
                    "name": "Jeff Donahue"
                },
                {
                    "authorId": "2282139565",
                    "name": "Pauline Luc"
                },
                {
                    "authorId": "19200186",
                    "name": "Antoine Miech"
                },
                {
                    "authorId": "2159207795",
                    "name": "Iain Barr"
                },
                {
                    "authorId": "66535271",
                    "name": "Yana Hasson"
                },
                {
                    "authorId": "3257286",
                    "name": "Karel Lenc"
                },
                {
                    "authorId": "1697879",
                    "name": "A. Mensch"
                },
                {
                    "authorId": "2143434227",
                    "name": "Katie Millican"
                },
                {
                    "authorId": "2295675672",
                    "name": "Malcolm Reynolds"
                },
                {
                    "authorId": "81387328",
                    "name": "Roman Ring"
                },
                {
                    "authorId": "2143538252",
                    "name": "Eliza Rutherford"
                },
                {
                    "authorId": "12159303",
                    "name": "Serkan Cabi"
                },
                {
                    "authorId": "22237490",
                    "name": "Tengda Han"
                },
                {
                    "authorId": "2275190668",
                    "name": "Zhitao Gong"
                },
                {
                    "authorId": "2412073",
                    "name": "Sina Samangooei"
                },
                {
                    "authorId": "2275188033",
                    "name": "Marianne Monteiro"
                },
                {
                    "authorId": "10698483",
                    "name": "Jacob Menick"
                },
                {
                    "authorId": "148016269",
                    "name": "Sebastian Borgeaud"
                },
                {
                    "authorId": "2065040422",
                    "name": "Andy Brock"
                },
                {
                    "authorId": "3208081",
                    "name": "Aida Nematzadeh"
                },
                {
                    "authorId": "7782886",
                    "name": "Sahand Sharifzadeh"
                },
                {
                    "authorId": "90240028",
                    "name": "M. Binkowski"
                },
                {
                    "authorId": "2026369796",
                    "name": "Ricardo Barreira"
                },
                {
                    "authorId": "1689108",
                    "name": "O. Vinyals"
                },
                {
                    "authorId": "2256988668",
                    "name": "Andrew Zisserman"
                },
                {
                    "authorId": "2295674815",
                    "name": "Karen Simonyan. 2022"
                },
                {
                    "authorId": "2135149490",
                    "name": "Anas Awadalla"
                },
                {
                    "authorId": "2295674838",
                    "name": "Irena Gao"
                },
                {
                    "authorId": "2262139047",
                    "name": "Josh Gardner"
                },
                {
                    "authorId": "2295681179",
                    "name": "Jack Hes-sel"
                },
                {
                    "authorId": "2232718046",
                    "name": "Yusuf Hanafy"
                },
                {
                    "authorId": "2288055881",
                    "name": "Wanrong Zhu"
                },
                {
                    "authorId": "12225656",
                    "name": "K. Marathe"
                },
                {
                    "authorId": "1938499056",
                    "name": "Yonatan Bitton"
                },
                {
                    "authorId": "1387466862",
                    "name": "S. Gadre"
                },
                {
                    "authorId": "2191688",
                    "name": "J. Jitsev"
                },
                {
                    "authorId": "2295674810",
                    "name": "Pang Simon Kornblith"
                },
                {
                    "authorId": "2286977942",
                    "name": "Wei Koh"
                },
                {
                    "authorId": "2273910379",
                    "name": "Gabriel Ilharco"
                },
                {
                    "authorId": "52193502",
                    "name": "Mitchell Wortsman"
                },
                {
                    "authorId": "2295675247",
                    "name": "Ludwig Schmidt. 2023"
                },
                {
                    "authorId": "2295674818",
                    "name": "Open-flamingo Hritik"
                },
                {
                    "authorId": "2295674882",
                    "name": "Da Bansal"
                },
                {
                    "authorId": "2295675886",
                    "name": "Masoud Yin"
                },
                {
                    "authorId": "2295675176",
                    "name": "Monajatipoor Kai-Wei"
                },
                {
                    "authorId": "2287292643",
                    "name": "Chang"
                },
                {
                    "authorId": "3429472",
                    "name": "Philipp Blandfort"
                },
                {
                    "authorId": "2282720215",
                    "name": "Desmond U Patton"
                },
                {
                    "authorId": "46733376",
                    "name": "William R. Frey"
                },
                {
                    "authorId": "35862299",
                    "name": "Svebor Karaman"
                },
                {
                    "authorId": "1754397",
                    "name": "Surabhi Bhargava"
                },
                {
                    "authorId": "2261282667",
                    "name": "Tom Brown"
                },
                {
                    "authorId": "2285072384",
                    "name": "Benjamin Mann"
                },
                {
                    "authorId": "2260406867",
                    "name": "Nick Ryder"
                },
                {
                    "authorId": "2065894334",
                    "name": "Melanie Subbiah"
                },
                {
                    "authorId": "2053807409",
                    "name": "Jared Kaplan"
                },
                {
                    "authorId": "6515819",
                    "name": "Prafulla Dhariwal"
                },
                {
                    "authorId": "2072676",
                    "name": "Arvind Neelakantan"
                },
                {
                    "authorId": "67311962",
                    "name": "Pranav Shyam"
                },
                {
                    "authorId": "144864359",
                    "name": "Girish Sastry"
                },
                {
                    "authorId": null,
                    "name": "Wenliang Dai"
                },
                {
                    "authorId": "2295682370",
                    "name": "Junnan Li"
                },
                {
                    "authorId": "2295696034",
                    "name": "Dongxu Li"
                },
                {
                    "authorId": "2295673406",
                    "name": "Anthony Meng"
                },
                {
                    "authorId": "2295674843",
                    "name": "Huat Tiong"
                },
                {
                    "authorId": "2295098794",
                    "name": "Junqi Zhao"
                },
                {
                    "authorId": "2295676948",
                    "name": "Weisheng Wang"
                },
                {
                    "authorId": "2295053334",
                    "name": "Boyang Li"
                },
                {
                    "authorId": "2285490668",
                    "name": "Pascale Fung"
                },
                {
                    "authorId": "2295674806",
                    "name": "Steven Hoi. 2023"
                },
                {
                    "authorId": "2248758018",
                    "name": "George R Doddington"
                },
                {
                    "authorId": "2286058042",
                    "name": "Alexis Mitchell"
                },
                {
                    "authorId": "2295674840",
                    "name": "Mark A Przy-bocki"
                },
                {
                    "authorId": "2252500558",
                    "name": "lan A. Ramshaw"
                },
                {
                    "authorId": "1754963",
                    "name": "Stephanie Strassel"
                },
                {
                    "authorId": "2295673171",
                    "name": "Ralph M Weischedel. 2004"
                },
                {
                    "authorId": "50360470",
                    "name": "Salvatore Giorgi"
                },
                {
                    "authorId": "2008166098",
                    "name": "Sharath Chandra Guntuku"
                },
                {
                    "authorId": "1920132119",
                    "name": "McKenzie Himelein-Wachowiak"
                },
                {
                    "authorId": "1919946482",
                    "name": "Amy Kwarteng"
                },
                {
                    "authorId": "2295685312",
                    "name": "Sy Hwang"
                },
                {
                    "authorId": "2110108343",
                    "name": "Muhammad Rahman"
                },
                {
                    "authorId": "2053093153",
                    "name": "Brenda L. Curtis"
                },
                {
                    "authorId": "2286669708",
                    "name": "Tao Gong"
                },
                {
                    "authorId": "2163740735",
                    "name": "Chengqi Lyu"
                },
                {
                    "authorId": "2293406059",
                    "name": "Shilong Zhang"
                },
                {
                    "authorId": "2283817440",
                    "name": "Yudong Wang"
                },
                {
                    "authorId": "2159678046",
                    "name": "Miao Zheng"
                },
                {
                    "authorId": "2295485978",
                    "name": "Qian Zhao"
                },
                {
                    "authorId": "2029335061",
                    "name": "Kuikun Liu"
                },
                {
                    "authorId": "2266359401",
                    "name": "Wenwei Zhang"
                },
                {
                    "authorId": "2295886816",
                    "name": "Ping Luo"
                },
                {
                    "authorId": "2293709613",
                    "name": "Kai Chen"
                },
                {
                    "authorId": "2295674808",
                    "name": "Multimodal-gpt"
                },
                {
                    "authorId": "2295674336",
                    "name": "Jordan Hoffmann"
                },
                {
                    "authorId": "2295673637",
                    "name": "Arthur Men-sch"
                },
                {
                    "authorId": "118801223",
                    "name": "Elena Buchatskaya"
                },
                {
                    "authorId": "2072572294",
                    "name": "Trevor Cai"
                },
                {
                    "authorId": "2295674558",
                    "name": "Eliza Ruther-ford"
                },
                {
                    "authorId": "2273777914",
                    "name": "Diego de"
                },
                {
                    "authorId": "2276241042",
                    "name": "Las Casas"
                },
                {
                    "authorId": "2258347245",
                    "name": "Lisa Anne Hendricks"
                },
                {
                    "authorId": "2287052456",
                    "name": "Johannes Welbl"
                },
                {
                    "authorId": "2293772267",
                    "name": "Aidan Clark"
                },
                {
                    "authorId": "34809425",
                    "name": "I-Hung Hsu"
                },
                {
                    "authorId": null,
                    "name": "Kuan-Hao Huang"
                },
                {
                    "authorId": "3256207",
                    "name": "Elizabeth Boschee"
                },
                {
                    "authorId": "2295905964",
                    "name": "Scott Miller"
                },
                {
                    "authorId": "2266842956",
                    "name": "Premkumar Natarajan"
                },
                {
                    "authorId": "2261973116",
                    "name": "Mike Lewis"
                },
                {
                    "authorId": "11323179",
                    "name": "Yinhan Liu"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "2320509",
                    "name": "Marjan Ghazvininejad"
                },
                {
                    "authorId": "2260403705",
                    "name": "Abdelrahman Mohamed"
                },
                {
                    "authorId": "2253752918",
                    "name": "Omer Levy"
                },
                {
                    "authorId": "1389924486",
                    "name": "Ves Stoyanov"
                },
                {
                    "authorId": "2295673133",
                    "name": "Luke Zettlemoyer. 2019"
                },
                {
                    "authorId": "2295681138",
                    "name": "Bart"
                }
            ]
        },
        {
            "paperId": "fb58e778b68f91c30806052bf195f20e98a85eff",
            "title": "JPEG-LM: LLMs as Image Generators with Canonical Codec Representations",
            "abstract": "Recent work in image and video generation has been adopting the autoregressive LLM architecture due to its generality and potentially easy integration into multi-modal systems. The crux of applying autoregressive training in language generation to visual generation is discretization -- representing continuous data like images and videos as discrete tokens. Common methods of discretizing images and videos include modeling raw pixel values, which are prohibitively lengthy, or vector quantization, which requires convoluted pre-hoc training. In this work, we propose to directly model images and videos as compressed files saved on computers via canonical codecs (e.g., JPEG, AVC/H.264). Using the default Llama architecture without any vision-specific modifications, we pretrain JPEG-LM from scratch to generate images (and AVC-LM to generate videos as a proof of concept), by directly outputting compressed file bytes in JPEG and AVC formats. Evaluation of image generation shows that this simple and straightforward approach is more effective than pixel-based modeling and sophisticated vector quantization baselines (on which our method yields a 31% reduction in FID). Our analysis shows that JPEG-LM has an especial advantage over vector quantization models in generating long-tail visual elements. Overall, we show that using canonical codec representations can help lower the barriers between language generation and visual generation, facilitating future research on multi-modal language/image/video LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257023881",
                    "name": "Xiaochuang Han"
                },
                {
                    "authorId": "2320509",
                    "name": "Marjan Ghazvininejad"
                },
                {
                    "authorId": "2276205042",
                    "name": "Pang Wei Koh"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "49cbfa5848aaa74ce01d29ee5328f1a2b466ce27",
            "title": "David helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion LMs",
            "abstract": "Diffusion-based language models are emerging as a promising alternative to autoregressive LMs: they approach the competence of autoregressive LMs while offering nuanced controllability at inference time. While autoregressive LMs have benefited immensely from scaling and instruction-based learning, existing studies of diffusion LMs have been conducted on a smaller scale. Starting with a recently proposed diffusion model SSD-LM, in this work we first explore methods to scale it from 0.4B to 13B parameters, proposing techniques to improve its training and inference efficiency, and to finetune the model to follow instructions. Armed with a more powerful, general purpose diffusion LM, we introduce the primary contribution of this work \u2013 SSD-2 \u2013 an approach to easily ensemble at inference time a large general-purpose diffusion LM with smaller, but specialized and contextualized diffusion LMs. We show that SSD-2 facilitates novel ensembles with 100x smaller models that can be customized and deployed by individual users. We find that compared to autoregressive models, the collaboration between diffusion LMs is more effective, leading to higher-quality model responses due to their ability to dynamically incorporate bi-directional contexts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40500540",
                    "name": "Xiaochuang Han"
                },
                {
                    "authorId": "51467955",
                    "name": "Sachin Kumar"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2320509",
                    "name": "Marjan Ghazvininejad"
                }
            ]
        },
        {
            "paperId": "62a7f3c14c15d8f5d1c643ceb991dddb36e3c47c",
            "title": "XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models",
            "abstract": "Large multilingual language models typically rely on a single vocabulary shared across 100+ languages. As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged. This \\textit{vocabulary bottleneck} limits the representational capabilities of multilingual models like XLM-R. In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language. Tokenizations using our vocabulary are typically more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a multilingual language model with a one million token vocabulary. XLM-V outperforms XLM-R on every task we tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity recognition (WikiAnn). XLM-V is particularly effective on low-resource language tasks and outperforms XLM-R by 11.2% and 5.8% absolute on MasakhaNER and Americas NLI, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "25130521",
                    "name": "Davis Liang"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "2132302721",
                    "name": "Rui Hou"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "2320509",
                    "name": "Marjan Ghazvininejad"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "2072010",
                    "name": "Madian Khabsa"
                }
            ]
        },
        {
            "paperId": "64ce6ef1f5cf227bf2bf917c87273386ae16256f",
            "title": "Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation",
            "abstract": "Large language models (LLMs) demonstrate remarkable machine translation (MT) abilities via prompting, even though they were not explicitly trained for this task. However, even given the incredible quantities of data they are trained on, LLMs can struggle to translate inputs with rare words, which are common in low resource or domain transfer scenarios. We show that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts. We propose a novel method, DiPMT, that provides a set of possible translations for a subset of the input words, thereby enabling fine-grained phrase-level prompted control of the LLM. Extensive experiments show that DiPMT outperforms the baseline both in low-resource MT, as well as for out-of-domain MT. We further provide a qualitative analysis of the benefits and limitations of this approach, including the overall level of controllability that is achieved.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2320509",
                    "name": "Marjan Ghazvininejad"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                }
            ]
        },
        {
            "paperId": "7e96a8bc938b47cf805383ef8c079cd852bd64ba",
            "title": "Representation Deficiency in Masked Language Modeling",
            "abstract": "Masked Language Modeling (MLM) has been one of the most prominent approaches for pretraining bidirectional text encoders due to its simplicity and effectiveness. One notable concern about MLM is that the special $\\texttt{[MASK]}$ symbol causes a discrepancy between pretraining data and downstream data as it is present only in pretraining but not in fine-tuning. In this work, we offer a new perspective on the consequence of such a discrepancy: We demonstrate empirically and theoretically that MLM pretraining allocates some model dimensions exclusively for representing $\\texttt{[MASK]}$ tokens, resulting in a representation deficiency for real tokens and limiting the pretrained model's expressiveness when it is adapted to downstream data without $\\texttt{[MASK]}$ tokens. Motivated by the identified issue, we propose MAE-LM, which pretrains the Masked Autoencoder architecture with MLM where $\\texttt{[MASK]}$ tokens are excluded from the encoder. Empirically, we show that MAE-LM improves the utilization of model dimensions for real token representations, and MAE-LM consistently outperforms MLM-pretrained models across different pretraining settings and model sizes when fine-tuned on the GLUE and SQuAD benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "113293663",
                    "name": "Jitin Krishnan"
                },
                {
                    "authorId": "2116420716",
                    "name": "Sinong Wang"
                },
                {
                    "authorId": "2145778781",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "3375249",
                    "name": "Yuning Mao"
                },
                {
                    "authorId": "2087117615",
                    "name": "Han Fang"
                },
                {
                    "authorId": "2320509",
                    "name": "Marjan Ghazvininejad"
                },
                {
                    "authorId": "2111759643",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                }
            ]
        },
        {
            "paperId": "b37690747ba314455b8a992e794b25223e105d1c",
            "title": "BLESS: Benchmarking Large Language Models on Sentence Simplification",
            "abstract": "We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art large language models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS, perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. Our performance benchmark will be available as a resource for the development of future TS methods and evaluation metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1583958852",
                    "name": "Tannon Kew"
                },
                {
                    "authorId": "2229298504",
                    "name": "Alison Chi"
                },
                {
                    "authorId": "2121361476",
                    "name": "Laura V\u00e1squez-Rodr\u00edguez"
                },
                {
                    "authorId": "5112699",
                    "name": "Sweta Agrawal"
                },
                {
                    "authorId": "2260405239",
                    "name": "Dennis Aumiller"
                },
                {
                    "authorId": "69930782",
                    "name": "Fernando Alva-Manchego"
                },
                {
                    "authorId": "2895959",
                    "name": "M. Shardlow"
                },
                {
                    "authorId": "2260405332",
                    "name": "Jason Baumgartner"
                },
                {
                    "authorId": "3447293",
                    "name": "Savvas Zannettou"
                },
                {
                    "authorId": "2260400554",
                    "name": "Brian Keegan"
                },
                {
                    "authorId": "2260400842",
                    "name": "Megan Squire"
                },
                {
                    "authorId": "2260400350",
                    "name": "Jeremy Blackburn. 2020"
                },
                {
                    "authorId": "2044098905",
                    "name": "Sid Black"
                },
                {
                    "authorId": null,
                    "name": "Stella Biderman"
                },
                {
                    "authorId": "2162462983",
                    "name": "Eric Hallahan"
                },
                {
                    "authorId": "2260401329",
                    "name": "Quentin Anthony"
                },
                {
                    "authorId": null,
                    "name": "Leo Gao"
                },
                {
                    "authorId": "2044198157",
                    "name": "Laurence Golding"
                },
                {
                    "authorId": "46350295",
                    "name": "Horace He"
                },
                {
                    "authorId": "2044198134",
                    "name": "Connor Leahy"
                },
                {
                    "authorId": "2049410219",
                    "name": "Kyle McDonell"
                },
                {
                    "authorId": "2241609611",
                    "name": "Jason Phang"
                },
                {
                    "authorId": "15043672",
                    "name": "M. Pieler"
                },
                {
                    "authorId": "2257135738",
                    "name": "Tom B. Brown"
                },
                {
                    "authorId": "2056658938",
                    "name": "Benjamin Mann"
                },
                {
                    "authorId": "2260406867",
                    "name": "Nick Ryder"
                },
                {
                    "authorId": "2065894334",
                    "name": "Melanie Subbiah"
                },
                {
                    "authorId": "2053807409",
                    "name": "Jared Kaplan"
                },
                {
                    "authorId": "6515819",
                    "name": "Prafulla Dhariwal"
                },
                {
                    "authorId": "2072676",
                    "name": "Arvind Neelakantan"
                },
                {
                    "authorId": "67311962",
                    "name": "Pranav Shyam"
                },
                {
                    "authorId": "144864359",
                    "name": "Girish Sastry"
                },
                {
                    "authorId": "2260592138",
                    "name": "Li-Kuang Chen"
                },
                {
                    "authorId": "2229028969",
                    "name": "Yi-Chen Chang"
                },
                {
                    "authorId": "2260400980",
                    "name": "Xi Srinivasan Iyer"
                },
                {
                    "authorId": "2258956777",
                    "name": "Victoria Lin"
                },
                {
                    "authorId": "10721120",
                    "name": "Ramakanth Pasunuru"
                },
                {
                    "authorId": "39980906",
                    "name": "Todor Mihaylov"
                },
                {
                    "authorId": "2257241733",
                    "name": "Daniel Simig"
                },
                {
                    "authorId": "2114104308",
                    "name": "Ping Yu"
                },
                {
                    "authorId": "35752280",
                    "name": "Kurt Shuster"
                },
                {
                    "authorId": "2238056517",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "2260405936",
                    "name": "Punit Qing Liu"
                },
                {
                    "authorId": "2257006866",
                    "name": "Singh Koura"
                },
                {
                    "authorId": "2243015223",
                    "name": "Xian Li"
                },
                {
                    "authorId": "2146367747",
                    "name": "Brian O'Horo"
                },
                {
                    "authorId": "2064737506",
                    "name": "Gabriel Pereyra"
                },
                {
                    "authorId": "2155451431",
                    "name": "Jeff Wang"
                },
                {
                    "authorId": "2065332326",
                    "name": "Christopher Dewan"
                },
                {
                    "authorId": "1709797",
                    "name": "Asli Celikyilmaz"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "2260405041",
                    "name": "Ves Stoyanov. 2023"
                },
                {
                    "authorId": "145755239",
                    "name": "Chao Jiang"
                },
                {
                    "authorId": "80032861",
                    "name": "Mounica Maddela"
                },
                {
                    "authorId": "37852874",
                    "name": "Wuwei Lan"
                },
                {
                    "authorId": "2260403866",
                    "name": "Yang Zhong"
                },
                {
                    "authorId": "2260708048",
                    "name": "Wei Xu"
                },
                {
                    "authorId": "2243164178",
                    "name": "Neural"
                },
                {
                    "authorId": "2259413305",
                    "name": "J. P. Kincaid"
                },
                {
                    "authorId": "69577338",
                    "name": "R. P. Fishburne"
                },
                {
                    "authorId": "2260402923",
                    "name": "R. L. Rogers"
                },
                {
                    "authorId": "2260405058",
                    "name": "Brad S. Chissom. 1975"
                },
                {
                    "authorId": "2295518754",
                    "name": "Hugo Lauren\u00e7on"
                },
                {
                    "authorId": "2113836860",
                    "name": "Lucile Saulnier"
                },
                {
                    "authorId": "2135734748",
                    "name": "Thomas Wang"
                },
                {
                    "authorId": "2003696840",
                    "name": "Christopher Akiki"
                },
                {
                    "authorId": "2186979454",
                    "name": "Albert Villanova"
                },
                {
                    "authorId": "2260131678",
                    "name": "del Moral"
                },
                {
                    "authorId": "1379806208",
                    "name": "Teven Le Scao"
                },
                {
                    "authorId": "51128119",
                    "name": "Leandro von Werra"
                },
                {
                    "authorId": "35966970",
                    "name": "Chenghao Mou"
                },
                {
                    "authorId": "2260133312",
                    "name": "E. G. Ponferrada"
                },
                {
                    "authorId": "2168170616",
                    "name": "Huu Nguyen"
                },
                {
                    "authorId": "2253417398",
                    "name": "Mike Lewis"
                },
                {
                    "authorId": "11323179",
                    "name": "Yinhan Liu"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "2320509",
                    "name": "Marjan Ghazvininejad"
                },
                {
                    "authorId": "2260403705",
                    "name": "Abdelrahman Mohamed"
                },
                {
                    "authorId": "2253752918",
                    "name": "Omer Levy"
                }
            ]
        },
        {
            "paperId": "47e15941c8b157873c8264e4bf50318d1ba5cd18",
            "title": "Natural Language to Code Translation with Execution",
            "abstract": "Generative models of code, pretrained on large corpora of programs, have shown great success in translating natural language to code (Chen et al., 2021; Austin et al., 2021; Li et al., 2022, inter alia). While these models do not explicitly incorporate program semantics (i.e., execution results) during training, they are able to generate correct solutions for many problems. However, choosing a single correct program from a generated set for each problem remains challenging. In this work, we introduce execution result\u2013based minimum Bayes risk decoding (MBR-EXEC) for program selection and show that it improves the few-shot performance of pretrained code models on natural-language-to-code tasks. We select output programs from a generated candidate set by marginalizing over program implementations that share the same semantics. Because exact equivalence is intractable, we execute each program on a small number of test inputs to approximate semantic equivalence. Across datasets, execution or simulated execution significantly outperforms the methods that do not involve program semantics. We find that MBR-EXEC consistently improves over all execution-unaware selection methods, suggesting it as an effective approach for natural language to code translation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8815141",
                    "name": "Freda Shi"
                },
                {
                    "authorId": "47070750",
                    "name": "Daniel Fried"
                },
                {
                    "authorId": "2320509",
                    "name": "Marjan Ghazvininejad"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "8729431",
                    "name": "Sida I. Wang"
                }
            ]
        },
        {
            "paperId": "515cf674fcdced5a7d5bb156dd5fcc1f5290e79b",
            "title": "In-context Examples Selection for Machine Translation",
            "abstract": "Large-scale generative models show an impressive ability to perform a wide range of Natural Language Processing (NLP) tasks using in-context learning, where a few examples are used to describe a task to the model. For Machine Translation (MT), these examples are typically randomly sampled from the development dataset with a similar distribution as the evaluation set. However, it is unclear how the choice of these in-context examples and their ordering impacts the output translation quality. In this work, we aim to understand the properties of good in-context examples for MT in both in-domain and out-of-domain settings. We show that the translation quality and the domain of the in-context examples matter and that 1-shot noisy unrelated example can have a catastrophic impact on output quality. While concatenating multiple random examples reduces the effect of noise, a single good prompt optimized to maximize translation quality on the development dataset can elicit learned information from the pre-trained language model. Adding similar examples based on an n-gram overlap with the test source significantly and consistently improves the translation quality of the outputs, outperforming a strong kNN-MT baseline in 2 out of 4 out-of-domain datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "5112699",
                    "name": "Sweta Agrawal"
                },
                {
                    "authorId": "2110714400",
                    "name": "Chunting Zhou"
                },
                {
                    "authorId": "35084211",
                    "name": "M. Lewis"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "2320509",
                    "name": "Marjan Ghazvininejad"
                }
            ]
        },
        {
            "paperId": "cd471b5ef162906ef3d9a84398b3f98e9ee4bf56",
            "title": "A Review on Language Models as Knowledge Bases",
            "abstract": "Recently, there has been a surge of interest in the NLP community on the use of pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have shown that LMs trained on a sufficiently large (web) corpus will encode a significant amount of knowledge implicitly in its parameters. The resulting LM can be probed for different kinds of knowledge and thus acting as a KB. This has a major advantage over traditional KBs in that this method requires no human supervision. In this paper, we present a set of aspects that we deem a LM should have to fully act as a KB, and review the recent literature with respect to those aspects.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2006905770",
                    "name": "Badr AlKhamissi"
                },
                {
                    "authorId": "2162508414",
                    "name": "Millicent Li"
                },
                {
                    "authorId": "1709797",
                    "name": "Asli Celikyilmaz"
                },
                {
                    "authorId": "1700007",
                    "name": "Mona T. Diab"
                },
                {
                    "authorId": "2320509",
                    "name": "Marjan Ghazvininejad"
                }
            ]
        }
    ]
}