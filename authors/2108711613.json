{
    "authorId": "2108711613",
    "papers": [
        {
            "paperId": "26ebeeb1b9172df34ad21f1000bb6f3c374a222e",
            "title": "Structure Aware Incremental Learning with Personalized Imitation Weights for Recommender Systems",
            "abstract": "Recommender systems now consume large-scale data and play a significant role in improving user experience. Graph Neural Networks (GNNs) have emerged as one of the most effective recommender system models because they model the rich relational information. The ever-growing volume of data can make training GNNs prohibitively expensive. To address this, previous attempts propose to train the GNN models incrementally as new data blocks arrive. \nFeature and structure knowledge distillation techniques have been explored to allow the GNN model to train in a fast incremental fashion while alleviating the catastrophic forgetting problem. \nHowever, preserving the same amount of the historical information for all users is sub-optimal since it fails to take into account the dynamics of each user's change of preferences. \nFor the users whose interests shift substantially, retaining too much of the old knowledge can overly constrain the model, preventing it from quickly adapting to the users\u2019 novel interests. \nIn contrast, for users who have static preferences, model performance can benefit greatly from preserving as much of the user's long-term preferences as possible.\nIn this work, we propose a novel training strategy that adaptively learns personalized imitation weights for each user to balance the contribution from the recent data and the amount of knowledge to be distilled from previous time periods.\nWe demonstrate the effectiveness of learning imitation weights via a comparison on five diverse datasets for three state-of-art structure distillation based recommender systems. The performance shows consistent improvement over competitive incremental learning techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108711613",
                    "name": "Yuening Wang"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "1419478649",
                    "name": "Antonios Valkanas"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "74142381",
                    "name": "Chen Ma"
                },
                {
                    "authorId": "2069718816",
                    "name": "Jianye Hao"
                },
                {
                    "authorId": "2150349871",
                    "name": "Mark Coates"
                }
            ]
        },
        {
            "paperId": "7dc9fe72fac36c58370d7584c1467637bfc66b24",
            "title": "Structure Aware Experience Replay for Incremental Learning in Graph-based Recommender Systems",
            "abstract": "Large-scale recommender systems are integral parts of many services. With the recent rapid growth of accessible data, the need for efficient training methods has arisen. Given the high computational cost of training state-of-the-art graph neural network (GNN) based models, it is infeasible to train them from scratch with every new set of interactions. In this work, we present a novel framework for incrementally training GNN-based models. Our framework takes advantage of an experience reply technique built on top of a structurally aware reservoir sampling method tailored for this setting. This framework addresses catastrophic forgetting, allowing the model to preserve its understanding of users' long-term behavioral patterns while adapting to new trends. Our experiments demonstrate the superior performance of our framework on numerous datasets when combined with state-of-the-art GNN-based models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32732621",
                    "name": "Kian Ahrabian"
                },
                {
                    "authorId": "2110598147",
                    "name": "Yishi Xu"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "2109173156",
                    "name": "Jiapeng Wu"
                },
                {
                    "authorId": "2108711613",
                    "name": "Yuening Wang"
                },
                {
                    "authorId": "144819383",
                    "name": "M. Coates"
                }
            ]
        },
        {
            "paperId": "f6b60e92cb23a2a216e54b67efc15b7858da9968",
            "title": "Graph Structure Aware Contrastive Knowledge Distillation for Incremental Learning in Recommender Systems",
            "abstract": "Personalized recommender systems are playing an increasingly important role for online services. Graph Neural Network (GNN) based recommender models have demonstrated a superior capability to model users' interests thanks to rich relational information encoded in graphs. However, with the ever-growing volume of online information and the high computational complexity of training GNNs, it is difficult to perform frequent updates to provide the most up-to-date recommendations. There have been several attempts towards training GNN models in an incremental fashion to enable faster training times and permit more frequent model updates using the latest training data. The main technique is knowledge distillation, which aims to allow model updates while preserving key aspects of the model that were learned from the historical data. In this work, we develop a novel Graph Structure Aware Contrastive Knowledge Distillation for Incremental Learning in recommender systems, which is tailored to focus on the rich relational information in the recommendation context. We combine the contrastive distillation formulation with intermediate layer distillation to inject layer-level supervision. We demonstrate the effectiveness of our proposed distillation framework for GNN based recommendation systems on four commonly used datasets, showing consistent improvement over state-of-the-art alternatives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108711613",
                    "name": "Yuening Wang"
                },
                {
                    "authorId": "2135319291",
                    "name": "Yingxue Zhang"
                },
                {
                    "authorId": "144819383",
                    "name": "M. Coates"
                }
            ]
        },
        {
            "paperId": "542ec54cb4becb58819abed02cfaf107ab370f76",
            "title": "B2L: a case for buffered and bufferless hybrid NoCs",
            "abstract": "A NoC is a critical shared resource among these concurrently-executing applications, significantly affecting system performance, and energy efficiency. In this paper, we propose B2L, a buffered and bufferless hybrid NoC design that can satisfy application's performance requirements, while the system's energy consumption is minimized. This can be achieved by designing a dedicated buffered and bufferless NoC separately, and dynamically control flits from different apps or within the same app to flow across the two networks according to application's requirement and network status.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145364644",
                    "name": "Juan Fang"
                },
                {
                    "authorId": "153318716",
                    "name": "Sitong Liu"
                },
                {
                    "authorId": "3236381",
                    "name": "Z. Leng"
                },
                {
                    "authorId": "2888403",
                    "name": "Shuying Song"
                },
                {
                    "authorId": "49319082",
                    "name": "Jianhua Wei"
                },
                {
                    "authorId": "2108711613",
                    "name": "Yuening Wang"
                }
            ]
        }
    ]
}