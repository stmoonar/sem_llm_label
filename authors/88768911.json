{
    "authorId": "88768911",
    "papers": [
        {
            "paperId": "8aa6ad95e1039829f2ee99e7d31566b2cdffd5dd",
            "title": "Controllable Chest X-Ray Report Generation from Longitudinal Representations",
            "abstract": "Radiology reports are detailed text descriptions of the content of medical scans. Each report describes the presence/absence and location of relevant clinical findings, commonly including comparison with prior exams of the same patient to describe how they evolved. Radiology reporting is a time-consuming process, and scan results are often subject to delays. One strategy to speed up reporting is to integrate automated reporting systems, however clinical deployment requires high accuracy and interpretability. Previous approaches to automated radiology reporting generally do not provide the prior study as input, precluding comparison which is required for clinical accuracy in some types of scans, and offer only unreliable methods of interpretability. Therefore, leveraging an existing visual input format of anatomical tokens, we introduce two novel aspects: (1) longitudinal representation learning -- we input the prior scan as an additional input, proposing a method to align, concatenate and fuse the current and prior visual information into a joint longitudinal representation which can be provided to the multimodal report generation model; (2) sentence-anatomy dropout -- a training strategy for controllability in which the report generator model is trained to predict only sentences from the original report which correspond to the subset of anatomical regions given as input. We show through in-depth experiments on the MIMIC-CXR dataset how the proposed approach achieves state-of-the-art results while enabling anatomy-wise controllable report generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "88768911",
                    "name": "F. Serra"
                },
                {
                    "authorId": "2257266289",
                    "name": "Chaoyang Wang"
                },
                {
                    "authorId": "2775904",
                    "name": "F. Deligianni"
                },
                {
                    "authorId": "2256995925",
                    "name": "Jeffrey Dalton"
                },
                {
                    "authorId": "2256995491",
                    "name": "Alison Q. O'Neil"
                }
            ]
        },
        {
            "paperId": "a13d7bf7f3192cc38ff497b8c6589fe305ad81e5",
            "title": "Finding-Aware Anatomical Tokens for Chest X-Ray Automated Reporting",
            "abstract": "The task of radiology reporting comprises describing and interpreting the medical findings in radiographic images, including description of their location and appearance. Automated approaches to radiology reporting require the image to be encoded into a suitable token representation for input to the language model. Previous methods commonly use convolutional neural networks to encode an image into a series of image-level feature map representations. However, the generated reports often exhibit realistic style but imperfect accuracy. Inspired by recent works for image captioning in the general domain in which each visual token corresponds to an object detected in an image, we investigate whether using local tokens corresponding to anatomical structures can improve the quality of the generated reports. We introduce a novel adaptation of Faster R-CNN in which finding detection is performed for the candidate bounding boxes extracted during anatomical structure localisation. We use the resulting bounding box feature representations as our set of finding-aware anatomical tokens. This encourages the extracted anatomical tokens to be informative about the findings they contain (required for the final task of radiology reporting). Evaluating on the MIMIC-CXR dataset of chest X-Ray images, we show that task-aware anatomical tokens give state-of-the-art performance when integrated into an automated reporting pipeline, yielding generated reports with improved clinical accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "88768911",
                    "name": "F. Serra"
                },
                {
                    "authorId": "50097023",
                    "name": "Chaoyang Wang"
                },
                {
                    "authorId": "2775904",
                    "name": "F. Deligianni"
                },
                {
                    "authorId": "49694325",
                    "name": "Jeffrey Stephen Dalton"
                },
                {
                    "authorId": "1404000098",
                    "name": "Alison Q. O'Neil"
                }
            ]
        },
        {
            "paperId": "0308c284376e216191d41d52f345102c9ae89f08",
            "title": "CMRE-UoG team at ImageCLEFmedical Caption 2022: Concept Detection and Image Captioning",
            "abstract": "This work presents the proposed solutions of our team for the ImageCLEFmedical Caption 2022 task [1]. This task is structured as two subtasks: (1) Concept Detection subtask \u2013 which consists of detecting Concept Unique Identifiers (CUIs) from the Unified Medical Language System (UMLS) [2] attributed to each image; and (2) the Caption Prediction subtask \u2013 which involves generating an accurate description of the content of the image, based on the concepts detected in the first subtask. For both subtasks, the dataset corresponds to a subset of the Radiology Objects in the COntext (ROCO) dataset [3]. In the Concept Detection subtask, we experiment with two different strategies: a) supervised learning \u2013 we train a Convolutional Neural Network (CNN) [4, 5] to classify the full set of CUIs; b) image retrieval \u2013 we retrieve the top \ud835\udc3e most \u201csimilar\u201d images from the training set based on the cosine similarity score between the image representations (extracted from the last average pooling layer), and combine the associated CUIs using a soft majority voting approach, similar to the ImageCLEFmed Caption 2021 winning approach [6]. Our best submission consists of the second image retrieval approach, for which we used an ensemble of five different CNNs. This approach ranked 2nd with an F1 score equal to 0.451, with a margin of approximately 5 \u00d7 10 \u2212 4 from the 1st position. In the Caption Prediction subtask, we adopt an image encoder-decoder Transformer model [7], which takes as input the image representation \u2013 generated using a CNN image encoder \u2013 and generates a text caption describing the image. Furthermore, we considered a multimodal encoder-decoder Trans-former model, which differs from the previous by taking as an additional input the CUIs extracted from the previous subtask alongside an image representation. Our multimodal approach ranked 6th, with a BLEU score [8] of 0.291, and ranked 1st place in terms of ROUGE [9] (the secondary metric for this subtask), with a score of 0.201.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "88768911",
                    "name": "F. Serra"
                },
                {
                    "authorId": "2775904",
                    "name": "F. Deligianni"
                },
                {
                    "authorId": "145269114",
                    "name": "Jeffrey Dalton"
                },
                {
                    "authorId": "1404000098",
                    "name": "Alison Q. O'Neil"
                }
            ]
        }
    ]
}