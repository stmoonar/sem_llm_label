{
    "authorId": "1958921",
    "papers": [
        {
            "paperId": "68fb64947793d3a15f933e56f471db273b534def",
            "title": "Understanding the Role of Invariance in Transfer Learning",
            "abstract": "Transfer learning is a powerful technique for knowledge-sharing between different tasks. Recent work has found that the representations of models with certain invariances, such as to adversarial input perturbations, achieve higher performance on downstream tasks. These findings suggest that invariance may be an important property in the context of transfer learning. However, the relationship of invariance with transfer performance is not fully understood yet and a number of questions remain. For instance, how important is invariance compared to other factors of the pretraining task? How transferable is learned invariance? In this work, we systematically investigate the importance of representational invariance for transfer learning, as well as how it interacts with other parameters during pretraining. To do so, we introduce a family of synthetic datasets that allow us to precisely control factors of variation both in training and test data. Using these datasets, we a) show that for learning representations with high transfer performance, invariance to the right transformations is as, or often more, important than most other factors such as the number of training samples, the model architecture and the identity of the pretraining classes, b) show conditions under which invariance can harm the ability to transfer representations and c) explore how transferable invariance is between tasks. The code is available at \\url{https://github.com/tillspeicher/representation-invariance-transfer}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2923553",
                    "name": "Till Speicher"
                },
                {
                    "authorId": "17974944",
                    "name": "Vedant Nanda"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                }
            ]
        },
        {
            "paperId": "ba4a42236dcf3950705e4c2c05944521de6843c9",
            "title": "TikTok and the Art of Personalization: Investigating Exploration and Exploitation on Social Media Feeds",
            "abstract": "Recommendation algorithms for social media feeds often function as black boxes from the perspective of users. We aim to detect whether social media feed recommendations are personalized to users, and to characterize the factors contributing to personalization in these feeds. We introduce a general framework to examine a set of social media feed recommendations for a user as a timeline. We label items in the timeline as the result of exploration vs. exploitation of the user's interests on the part of the recommendation algorithm and introduce a set of metrics to capture the extent of personalization across user timelines. We apply our framework to a real TikTok dataset and validate our results using a baseline generated from automated TikTok bots, as well as a randomized baseline. We also investigate the extent to which factors such as video viewing duration, liking, and following drive the personalization of content on TikTok. Our results demonstrate that our framework produces intuitive and explainable results, and can be used to audit and understand personalization in social media feeds.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1397695455",
                    "name": "Karan Vombatkere"
                },
                {
                    "authorId": "2289965399",
                    "name": "Sepehr Mousavi"
                },
                {
                    "authorId": "3447293",
                    "name": "Savvas Zannettou"
                },
                {
                    "authorId": "2289966427",
                    "name": "Franziska Roesner"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                }
            ]
        },
        {
            "paperId": "c29d7cc327042a56e1f1736856ad3903aded2663",
            "title": "Towards Reliable Latent Knowledge Estimation in LLMs: In-Context Learning vs. Prompting Based Factual Knowledge Extraction",
            "abstract": "We propose an approach for estimating the latent knowledge embedded inside large language models (LLMs). We leverage the in-context learning (ICL) abilities of LLMs to estimate the extent to which an LLM knows the facts stored in a knowledge base. Our knowledge estimator avoids reliability concerns with previous prompting-based methods, is both conceptually simpler and easier to apply, and we demonstrate that it can surface more of the latent knowledge embedded in LLMs. We also investigate how different design choices affect the performance of ICL-based knowledge estimation. Using the proposed estimator, we perform a large-scale evaluation of the factual knowledge of a variety of open source LLMs, like OPT, Pythia, Llama(2), Mistral, Gemma, etc. over a large set of relations and facts from the Wikidata knowledge base. We observe differences in the factual knowledge between different model families and models of different sizes, that some relations are consistently better known than others but that models differ in the precise facts they know, and differences in the knowledge of base models and their finetuned counterparts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "73272459",
                    "name": "Qinyuan Wu"
                },
                {
                    "authorId": "2168771748",
                    "name": "Mohammad Aflah Khan"
                },
                {
                    "authorId": "2297737634",
                    "name": "Soumi Das"
                },
                {
                    "authorId": "17974944",
                    "name": "Vedant Nanda"
                },
                {
                    "authorId": "2297671084",
                    "name": "Bishwamittra Ghosh"
                },
                {
                    "authorId": "1396871443",
                    "name": "Camila Kolling"
                },
                {
                    "authorId": "2923553",
                    "name": "Till Speicher"
                },
                {
                    "authorId": "2297670790",
                    "name": "Laurent Bindschaedler"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                },
                {
                    "authorId": "1839624",
                    "name": "Evimaria Terzi"
                }
            ]
        },
        {
            "paperId": "c9a8b5d718e2a8a7f2ab63a662098fd99906fe89",
            "title": "Antitrust, Amazon, and Algorithmic Auditing",
            "abstract": "In digital markets, antitrust law and special regulations aim to ensure that markets remain competitive despite the dominating role that digital platforms play today in everyone's life. Unlike traditional markets, market participant behavior is easily observable in these markets. We present a series of empirical investigations into the extent to which Amazon engages in practices that are typically described as self-preferencing. We discuss how the computer science tools used in this paper can be used in a regulatory environment that is based on algorithmic auditing and requires regulating digital markets at scale.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7660878",
                    "name": "A. Dash"
                },
                {
                    "authorId": "2676839",
                    "name": "Abhijnan Chakraborty"
                },
                {
                    "authorId": "2293665715",
                    "name": "Saptarshi Ghosh"
                },
                {
                    "authorId": "2286311932",
                    "name": "Animesh Mukherjee"
                },
                {
                    "authorId": "2293611951",
                    "name": "Jens Frankenreiter"
                },
                {
                    "authorId": "2286314296",
                    "name": "Stefan Bechtold"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                }
            ]
        },
        {
            "paperId": "32658cbc2e3435b3df667cb494e527a5955d0cde",
            "title": "Diffused Redundancy in Pre-trained Representations",
            "abstract": "Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, ie, any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\\%$ of randomly picked neurons from the penultimate layer of a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstream tasks taken from the VTAB benchmark. We find that the loss and dataset used during pre-training largely govern the degree of diffuse redundancy and the\"critical mass\"of neurons needed often depends on the downstream task, suggesting that there is a task-inherent redundancy-performance Pareto frontier. Our findings shed light on the nature of representations learned by pre-trained deep neural networks and suggest that entire layers might not be necessary to perform many downstream tasks. We investigate the potential for exploiting this redundancy to achieve efficient generalization for downstream tasks and also draw caution to certain possible unintended consequences. Our code is available at \\url{https://github.com/nvedant07/diffused-redundancy}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "17974944",
                    "name": "Vedant Nanda"
                },
                {
                    "authorId": "2923553",
                    "name": "Till Speicher"
                },
                {
                    "authorId": "1718974",
                    "name": "John P. Dickerson"
                },
                {
                    "authorId": "34389431",
                    "name": "S. Feizi"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                },
                {
                    "authorId": "145689461",
                    "name": "Adrian Weller"
                }
            ]
        },
        {
            "paperId": "aeb6ab3fd41224ac67411e03c58ff23ebb4657c9",
            "title": "Dissecting Bitcoin and Ethereum Transactions: On the Lack of Transaction Contention and Prioritization Transparency in Blockchains",
            "abstract": "In permissionless blockchains, transaction issuers include a fee to incentivize miners to include their transactions. To accurately estimate this prioritization fee for a transaction, transaction issuers (or blockchain participants, more generally) rely on two fundamental notions of transparency, namely contention and prioritization transparency. Contention transparency implies that participants are aware of every pending transaction that will contend with a given transaction for inclusion. Prioritization transparency states that the participants are aware of the transaction or prioritization fees paid by every such contending transaction. Neither of these notions of transparency holds well today. Private relay networks, for instance, allow users to send transactions privately to miners. Besides, users can offer fees to miners via either direct transfers to miners' wallets or off-chain payments -- neither of which are public. In this work, we characterize the lack of contention and prioritization transparency in Bitcoin and Ethereum resulting from such practices. We show that private relay networks are widely used and private transactions are quite prevalent. We show that the lack of transparency facilitates miners to collude and overcharge users who may use these private relay networks despite them offering little to no guarantees on transaction prioritization. The lack of these transparencies in blockchains has crucial implications for transaction issuers as well as the stability of blockchains. Finally, we make our data sets and scripts publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145791827",
                    "name": "Johnnatan Messias"
                },
                {
                    "authorId": "2205658848",
                    "name": "Vabuk Pahari"
                },
                {
                    "authorId": "2102739",
                    "name": "B. Chandrasekaran"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                },
                {
                    "authorId": "143710789",
                    "name": "P. Loiseau"
                }
            ]
        },
        {
            "paperId": "bb229a0127d5339391bfac25c4e0ee7ba4b899ad",
            "title": "Understanding Blockchain Governance: Analyzing Decentralized Voting to Amend DeFi Smart Contracts",
            "abstract": "Smart contracts are contractual agreements between participants of a blockchain, who cannot implicitly trust one another. They are software programs that run on top of a blockchain, and we may need to change them from time to time (e.g., to fix bugs or address new use cases). Governance protocols define the means for amending or changing these smart contracts without any centralized authority. They distribute the decision-making power to every user of the smart contract: Users vote on accepting or rejecting every change. In this work, we review and characterize decentralized governance in practice, using Compound and Uniswap -- two widely used governance protocols -- as a case study. We reveal a high concentration of voting power in both Compound and Uniswap: 10 voters hold together 57.86% and 44.72% of the voting power, respectively. Although proposals to change or amend the protocol receive, on average, a substantial number of votes (i.e., 89.39%) in favor within the Compound protocol, they require fewer than three voters to obtain 50% or more votes. We show that voting on Compound proposals can be unfairly expensive for small token holders, and we discover voting coalitions that can further marginalize these users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145791827",
                    "name": "Johnnatan Messias"
                },
                {
                    "authorId": "2205658848",
                    "name": "Vabuk Pahari"
                },
                {
                    "authorId": "2102739",
                    "name": "B. Chandrasekaran"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                },
                {
                    "authorId": "143710789",
                    "name": "P. Loiseau"
                }
            ]
        },
        {
            "paperId": "cef85967fbc8a2586b5b22e289bbd96145f89f92",
            "title": "Pointwise Representational Similarity",
            "abstract": "With the increasing reliance on deep neural networks, it is important to develop ways to better understand their learned representations. Representation similarity measures have emerged as a popular tool for examining learned representations However, existing measures only provide aggregate estimates of similarity at a global level, i.e. over a set of representations for N input examples. As such, these measures are not well-suited for investigating representations at a local level, i.e. representations of a single input example. Local similarity measures are needed, for instance, to understand which individual input representations are affected by training interventions to models (e.g. to be more fair and unbiased) or are at greater risk of being misclassified. In this work, we fill in this gap and propose Pointwise Normalized Kernel Alignment (PNKA), a measure that quantifies how similarly an individual input is represented in two representation spaces. Intuitively, PNKA compares the similarity of an input's neighborhoods across both spaces. Using our measure, we are able to analyze properties of learned representations at a finer granularity than what was previously possible. Concretely, we show how PNKA can be leveraged to develop a deeper understanding of (a) the input examples that are likely to be misclassified, (b) the concepts encoded by (individual) neurons in a layer, and (c) the effects of fairness interventions on learned representations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1396871443",
                    "name": "Camila Kolling"
                },
                {
                    "authorId": "2923553",
                    "name": "Till Speicher"
                },
                {
                    "authorId": "17974944",
                    "name": "Vedant Nanda"
                },
                {
                    "authorId": "2822168",
                    "name": "Mariya Toneva"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                }
            ]
        },
        {
            "paperId": "ee37d6070aefed08ff3290c323cc3e22e549d25f",
            "title": "Analyzing User Engagement with TikTok's Short Format Video Recommendations using Data Donations",
            "abstract": "Short-format videos have exploded on platforms like TikTok, Instagram, and YouTube. Despite this, the research community lacks large-scale empirical studies into how people engage with short-format videos and the role of recommendation systems that offer endless streams of such content. In this work, we analyze user engagement on TikTok using data we collect via a data donation system that allows TikTok users to donate their data. We recruited 347 TikTok users and collected 9.2M TikTok video recommendations they received. By analyzing user engagement, we find that the average daily usage time increases over the users\u2019 lifetime while the user attention remains stable at around 45%. We also find that users like more videos uploaded by people they follow than those recommended by people they do not follow. Our study offers valuable insights into how users engage with short-format videos on TikTok and lessons learned from designing a data donation system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3447293",
                    "name": "Savvas Zannettou"
                },
                {
                    "authorId": "2305842947",
                    "name": "Olivia Nemes-Nemeth"
                },
                {
                    "authorId": "3052971",
                    "name": "Oshrat Ayalon"
                },
                {
                    "authorId": "2133301627",
                    "name": "Angelica Goetzen"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                },
                {
                    "authorId": "2391370",
                    "name": "Elissa M. Redmiles"
                },
                {
                    "authorId": "3268360",
                    "name": "Franziska Roesner"
                }
            ]
        },
        {
            "paperId": "3b7626cedae941b856887bb95482a76377aff2ba",
            "title": "Don\u2019t Throw it Away! The Utility of Unlabeled Data in Fair Decision Making",
            "abstract": "Decision making algorithms, in practice, are often trained on data that exhibits a variety of biases. Decision-makers often aim to take decisions based on some ground-truth target that is assumed or expected to be unbiased, i.e., equally distributed across socially salient groups. In many practical settings, the ground-truth cannot be directly observed, and instead, we have to rely on a biased proxy measure of the ground-truth, i.e., biased labels, in the data. In addition, data is often selectively labeled, i.e., even the biased labels are only observed for a small fraction of the data that received a positive decision. To overcome label and selection biases, recent work proposes to learn stochastic, exploring decision policies via i) online training of new policies at each time-step and ii) enforcing fairness as a constraint on performance. However, the existing approach uses only labeled data, disregarding a large amount of unlabeled data, and thereby suffers from high instability and variance in the learned decision policies at different times. In this paper, we propose a novel method based on a variational autoencoder for practical fair decision-making. Our method learns an unbiased data representation leveraging both labeled and unlabeled data and uses the representations to learn a policy in an online process. Using synthetic data, we empirically validate that our method converges to the optimal (fair) policy according to the ground-truth with low variance. In real-world experiments, we further show that our training approach not only offers a more stable learning process but also yields policies with higher fairness as well as utility than previous approaches.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2135375871",
                    "name": "Miriam Rateike"
                },
                {
                    "authorId": "14124085",
                    "name": "Ayan Majumdar"
                },
                {
                    "authorId": "2072380248",
                    "name": "Olga Mineeva"
                },
                {
                    "authorId": "1958921",
                    "name": "K. Gummadi"
                },
                {
                    "authorId": "144991545",
                    "name": "Isabel Valera"
                }
            ]
        }
    ]
}