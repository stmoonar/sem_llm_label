{
    "authorId": "2065276988",
    "papers": [
        {
            "paperId": "1a0f7ff0c649c8f45c038a040cc08edb5f7c4a87",
            "title": "SIMMC-VR: A Task-oriented Multimodal Dialog Dataset with Situated and Immersive VR Streams",
            "abstract": "Building an AI assistant that can seamlessly converse and instruct humans, in a user-centric situated scenario, requires several essential abilities:(1) spatial and temporal understanding of the situated and real-time user scenes,(2) capability of grounding the actively perceived visuals of users to conversation contexts,and (3) conversational reasoning over past utterances to perform just-in-time assistance.However, we currently lack a large-scale benchmark that captures user\u2013assistant interactions with all of the aforementioned features.To this end, we propose SIMMC-VR, an extension of the SIMMC-2.0 dataset, to a video-grounded task-oriented dialog dataset that captures real-world AI-assisted user scenarios in VR.We propose a novel data collection paradigm that involves(1) generating object-centric multimodal dialog flows with egocentric visual streams and visually-grounded templates,and (2) manually paraphrasing the simulated dialogs for naturalness and diversity while preserving multimodal dependencies. To measure meaningful progress in the field, we propose four tasks to address the new challenges in SIMMC-VR, which require complex spatial-temporal dialog reasoning in active egocentric scenes.We benchmark the proposed tasks with strong multimodal models, and highlight the key capabilities that current models lack for future research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2015467",
                    "name": "Te-Lin Wu"
                },
                {
                    "authorId": "2150275",
                    "name": "Satwik Kottur"
                },
                {
                    "authorId": "2111680936",
                    "name": "Andrea Madotto"
                },
                {
                    "authorId": "2065276988",
                    "name": "Mahmoud Azab"
                },
                {
                    "authorId": "145009056",
                    "name": "Pedro Rodriguez"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "3157053",
                    "name": "Nanyun Peng"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                }
            ]
        },
        {
            "paperId": "3f27a7dad2f93c2b2ea8ad4719f8099bc386e8dc",
            "title": "Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks",
            "abstract": "Searching troves of videos with textual descriptions is a core multimodal retrieval task. Owing to the lack of a purpose-built dataset for text-to-video retrieval, video captioning datasets have been re-purposed to evaluate models by (1) treating captions as positive matches to their respective videos and (2) assuming all other videos to be negatives. However, this methodology leads to a fundamental flaw during evaluation: since captions are marked as relevant only to their original video, many alternate videos also match the caption, which introduces false-negative caption-video pairs. We show that when these false negatives are corrected, a recent state-of-the-art model gains 25% recall points\u2014a difference that threatens the validity of the benchmark itself. To diagnose and mitigate this issue, we annotate and release 683K additional caption-video pairs. Using these, we recompute effectiveness scores for three models on two standard benchmarks (MSR-VTT and MSVD). We find that (1) the recomputed metrics are up to 25% recall points higher for the best models, (2) these benchmarks are nearing saturation for Recall@10, (3) caption length (generality) is related to the number of positives, and (4) annotation costs can be mitigated through sampling. We recommend retiring these benchmarks in their current form, and we make recommendations for future text-to-video retrieval benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145009056",
                    "name": "Pedro Rodriguez"
                },
                {
                    "authorId": "2065276988",
                    "name": "Mahmoud Azab"
                },
                {
                    "authorId": "2003646964",
                    "name": "Becka Silvert"
                },
                {
                    "authorId": "2187443212",
                    "name": "Renato Sanchez"
                },
                {
                    "authorId": "2187431339",
                    "name": "Linzy Labson"
                },
                {
                    "authorId": "2187300386",
                    "name": "Hardik Shah"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                }
            ]
        },
        {
            "paperId": "d6fa5acf4aa2ccee622504c3925d164f8b72f825",
            "title": "Normalized Contrastive Learning for Text-Video Retrieval",
            "abstract": "Cross-modal contrastive learning has led the recent advances in multimodal retrieval with its simplicity and effectiveness. In this work, however, we reveal that cross-modal contrastive learning suffers from incorrect normalization of the sum retrieval probabilities of each text or video instance. Specifically, we show that many test instances are either over- or under-represented during retrieval, significantly hurting the retrieval performance. To address this problem, we propose Normalized Contrastive Learning (NCL) which utilizes the Sinkhorn-Knopp algorithm to compute the instance-wise biases that properly normalize the sum retrieval probabilities of each instance so that every text and video instance is fairly represented during cross-modal retrieval. Empirical study shows that NCL brings consistent and significant gains in text-video retrieval on different model architectures, with new state-of-the-art multimodal retrieval metrics on the ActivityNet, MSVD, and MSR-VTT datasets without any architecture engineering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "134893586",
                    "name": "Yookoon Park"
                },
                {
                    "authorId": "2065276988",
                    "name": "Mahmoud Azab"
                },
                {
                    "authorId": "2142452652",
                    "name": "Bo Xiong"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "1740721",
                    "name": "Florian Metze"
                },
                {
                    "authorId": "38896301",
                    "name": "Gourab Kundu"
                },
                {
                    "authorId": "2197778206",
                    "name": "Kirmani Ahmed"
                }
            ]
        },
        {
            "paperId": "4621240ed38a7b42ad4fc77aa24d111c5d947934",
            "title": "LifeQA: A Real-life Dataset for Video Question Answering",
            "abstract": "We introduce LifeQA, a benchmark dataset for video question answering that focuses on day-to-day real-life situations. Current video question answering datasets consist of movies and TV shows. However, it is well-known that these visual domains are not representative of our day-to-day lives. Movies and TV shows, for example, benefit from professional camera movements, clean editing, crisp audio recordings, and scripted dialog between professional actors. While these domains provide a large amount of data for training models, their properties make them unsuitable for testing real-life question answering systems. Our dataset, by contrast, consists of video clips that represent only real-life scenarios. We collect 275 such video clips and over 2.3k multiple-choice questions. In this paper, we analyze the challenging but realistic aspects of LifeQA, and we apply several state-of-the-art video question answering models to provide benchmarks for future research. The full dataset is publicly available at https://lit.eecs.umich.edu/lifeqa/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145691225",
                    "name": "Santiago Castro"
                },
                {
                    "authorId": "2065276988",
                    "name": "Mahmoud Azab"
                },
                {
                    "authorId": "143935879",
                    "name": "Jonathan C. Stroud"
                },
                {
                    "authorId": "1724416445",
                    "name": "Cristina Noujaim"
                },
                {
                    "authorId": "2108694897",
                    "name": "Ruoyao Wang"
                },
                {
                    "authorId": "153302678",
                    "name": "Jia Deng"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                }
            ]
        },
        {
            "paperId": "09f2c5a7ee398fc00aa19bb588f5a09f8f67e13e",
            "title": "Towards Extracting Medical Family History from Natural Language Interactions: A New Dataset and Baselines",
            "abstract": "We introduce a new dataset consisting of natural language interactions annotated with medical family histories, obtained during interactions with a genetic counselor and through crowdsourcing, following a questionnaire created by experts in the domain. We describe the data collection process and the annotations performed by medical professionals, including illness and personal attributes (name, age, gender, family relationships) for the patient and their family members. An initial system that performs argument identification and relation extraction shows promising results \u2013 average F-score of 0.87 on complex sentences on the targeted relations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065276988",
                    "name": "Mahmoud Azab"
                },
                {
                    "authorId": "1390112328",
                    "name": "Stephane Dadian"
                },
                {
                    "authorId": "2256003",
                    "name": "Vivi Nastase"
                },
                {
                    "authorId": "2113905187",
                    "name": "Lawrence C. An"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                }
            ]
        },
        {
            "paperId": "a445b9c46458a81c277199e8a5912ab302308b0a",
            "title": "Representing Movie Characters in Dialogues",
            "abstract": "We introduce a new embedding model to represent movie characters and their interactions in a dialogue by encoding in the same representation the language used by these characters as well as information about the other participants in the dialogue. We evaluate the performance of these new character embeddings on two tasks: (1) character relatedness, using a dataset we introduce consisting of a dense character interaction matrix for 4,378 unique character pairs over 22 hours of dialogue from eighteen movies; and (2) character relation classification, for fine- and coarse-grained relations, as well as sentiment relations. Our experiments show that our model significantly outperforms the traditional Word2Vec continuous bag-of-words and skip-gram models, demonstrating the effectiveness of the character embeddings we introduce. We further show how these embeddings can be used in conjunction with a visual question answering system to improve over previous results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065276988",
                    "name": "Mahmoud Azab"
                },
                {
                    "authorId": "31686229",
                    "name": "Noriyuki Kojima"
                },
                {
                    "authorId": "153302678",
                    "name": "Jia Deng"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                }
            ]
        },
        {
            "paperId": "b9134015d9782badabde793169bd22645c386836",
            "title": "A Comparative Analysis of Content-based Geolocation in Blogs and Tweets",
            "abstract": "The geolocation of online information is an essential component in any geospatial application. While most of the previous work on geolocation has focused on Twitter, in this paper we quantify and compare the performance of text-based geolocation methods on social media data drawn from both Blogger and Twitter. We introduce a novel set of location specific features that are both highly informative and easily interpretable, and show that we can achieve error rate reductions of up to 12.5% with respect to the best previously proposed geolocation features. We also show that despite posting longer text, Blogger users are significantly harder to geolocate than Twitter users. Additionally, we investigate the effect of training and testing on different media (cross-media predictions), or combining multiple social media sources (multi-media predictions). Finally, we explore the geolocability of social media in relation to three user dimensions: state, gender, and industry.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2093695455",
                    "name": "Konstantinos Pappas"
                },
                {
                    "authorId": "2065276988",
                    "name": "Mahmoud Azab"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                }
            ]
        },
        {
            "paperId": "f8daab1e4f63051b78eb43e98ab723f6c425a6b5",
            "title": "Speaker Naming in Movies",
            "abstract": "We propose a new model for speaker naming in movies that leverages visual, textual, and acoustic modalities in an unified optimization framework. To evaluate the performance of our model, we introduce a new dataset consisting of six episodes of the Big Bang Theory TV show and eighteen full movies covering different genres. Our experiments show that our multimodal model significantly outperforms several competitive baselines on the average weighted F-score metric. To demonstrate the effectiveness of our framework, we design an end-to-end memory network model that leverages our speaker naming model and achieves state-of-the-art results on the subtitles task of the MovieQA 2017 Challenge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065276988",
                    "name": "Mahmoud Azab"
                },
                {
                    "authorId": "2108927052",
                    "name": "Mingzhe Wang"
                },
                {
                    "authorId": "2110001936",
                    "name": "Max Smith"
                },
                {
                    "authorId": "31686229",
                    "name": "Noriyuki Kojima"
                },
                {
                    "authorId": "153302678",
                    "name": "Jia Deng"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                }
            ]
        },
        {
            "paperId": "d11fb2aebf375fa76acaa6c733cca09bee8eb916",
            "title": "Using Word Semantics To Assist English as a Second Language Learners",
            "abstract": "We introduce an interactive interface that aims to help English as a Second Language (ESL) students overcome language related hindrances while reading a text. The interface allows the user to find supplementary information on selected difficult words. The interface is empowered by our lexical substitution engine that provides context-based synonyms for difficult words. We also provide a practical solution for a real-world usage scenario. We demonstrate using the lexical substitution engine \u2010 as a browser extension that can annotate and disambiguate difficult words on any webpage.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065276988",
                    "name": "Mahmoud Azab"
                },
                {
                    "authorId": "2140238",
                    "name": "Chris Hokamp"
                },
                {
                    "authorId": "2105984203",
                    "name": "Rada Mihalcea"
                }
            ]
        },
        {
            "paperId": "059723073dc18b499d47403a07eeb432f6259e94",
            "title": "Dudley North visits North London: Learning When to Transliterate to Arabic",
            "abstract": "We report the results of our work on automating the transliteration decision of named entities for English to Arabic machine translation. We construct a classification-based framework to automate this decision, evaluate our classifier both in the limited news and the diverse Wikipedia domains, and achieve promising accuracy. Moreover, we demonstrate a reduction of translation error and an improvement in the performance of an English-to-Arabic machine translation system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065276988",
                    "name": "Mahmoud Azab"
                },
                {
                    "authorId": "2063374",
                    "name": "Houda Bouamor"
                },
                {
                    "authorId": "1809988",
                    "name": "Behrang Mohit"
                },
                {
                    "authorId": "1723120",
                    "name": "Kemal Oflazer"
                }
            ]
        }
    ]
}