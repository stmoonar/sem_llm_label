{
    "authorId": "2065343184",
    "papers": [
        {
            "paperId": "faffa6c51abb671f3117d94cedeb28f3631d2f6a",
            "title": "FashionNTM: Multi-turn Fashion Image Retrieval via Cascaded Memory",
            "abstract": "Multi-turn textual feedback-based fashion image retrieval focuses on a real-world setting, where users can iteratively provide information to refine retrieval results until they find an item that fits all their requirements. In this work, we present a novel memory-based method, called FashionNTM, for such a multi-turn system. Our framework incorporates a new Cascaded Memory Neural Turing Machine (CM-NTM) approach for implicit state management, thereby learning to integrate information across all past turns to retrieve new images, for a given turn. Unlike vanilla Neural Turing Machine (NTM), our CM-NTM operates on multiple inputs, which interact with their respective memories via individual read and write heads, to learn complex relationships. Extensive evaluation results show that our proposed method outperforms the previous state-of-the-art algorithm by 50.5%, on Multi-turn FashionIQ [60] \u2013 the only existing multi-turn fashion dataset currently, in addition to having a relative improvement of 12.6% on Multi-turn Shoes \u2013 an extension of the singleturn Shoes dataset [5] that we created in this work. Further analysis of the model in a real-world interactive setting demonstrates two important capabilities of our model \u2013 memory retention across turns, and agnosticity to turn order for non-contradictory feedback. Finally, user study results show that images retrieved by FashionNTM were favored by 83.1% over other multi-turn models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151453388",
                    "name": "Anwesan Pal"
                },
                {
                    "authorId": "5002401",
                    "name": "Sahil Wadhwa"
                },
                {
                    "authorId": "26393556",
                    "name": "Ayush Jaiswal"
                },
                {
                    "authorId": "2213162331",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "2109035901",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "147887099",
                    "name": "Rakesh Chada"
                },
                {
                    "authorId": "49824581",
                    "name": "P. Natarajan"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                }
            ]
        },
        {
            "paperId": "660e4ccd27264917729330ff41123c172b1d9672",
            "title": "A Real2Sim2Real Method for Robust Object Grasping with Neural Surface Reconstruction",
            "abstract": "We explore an emerging technique, geometric Real2Sim2Real, in the context of object manipulation. We hypothesize that recent 3D modeling methods provides a path towards building digital replicas of real-world scenes that afford physical simulation and support robust manipulation algorithm learning. Since 6 DOF grasping is one the most important primitives for all manipulation tasks, we study whether geometric Real2Sim2Real can help us train a robust grasping network with high sample efficiency. We propose to reconstruct high-quality meshes from real-world point clouds using state-of-the-art neural surface reconstruction method (the Real2Sim step). Because most simulators take meshes for fast simulation, the reconstructed meshes enable grasp pose labels generation without human efforts. The generated labels can train grasp network that performs robustly in real evaluation scenes (the Sim2Real step). In synthetic and real experiments, we show that the Real2Sim2Real pipeline performs better than baseline grasp networks trained with a ${1}0^{4}\\times$ larger dataset by mimicking geometric shapes of target objects in simulation. We also show that our method has better sample efficiency than training the grasping network with a retrieval-based scene reconstruction method. The benefit of the Real2Sim2Real pipeline comes from 1) decoupling scene modeling and grasp sampling into sub-problems, and 2) both sub-problems can be solved with sufficiently high quality using recent 3D learning algorithms and mesh-based physical simulation techniques. Video presentation available at this link.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153518736",
                    "name": "Luobin Wang"
                },
                {
                    "authorId": "1878932409",
                    "name": "Runlin Guo"
                },
                {
                    "authorId": "144579461",
                    "name": "Q. Vuong"
                },
                {
                    "authorId": "12701031",
                    "name": "Yuzhe Qin"
                },
                {
                    "authorId": "2087042750",
                    "name": "Hao Su"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                }
            ]
        },
        {
            "paperId": "a2f49aef59937cea5d12c2f74d7080bbb2400197",
            "title": "Role of reward shaping in object-goal navigation",
            "abstract": "Deep reinforcement learning approaches have been a popular method for visual navigation tasks in the computer vision and robotics community of late. In most cases, the reward function has a binary structure, i.e., a large positive reward is provided when the agent reaches goal state, and a negative step penalty is assigned for every other state in the environment. A sparse signal like this makes the learning process challenging, specially in big environments, where a large number of sequential actions need to be taken to reach the target. We introduce a reward shaping mechanism which gradually adjusts the reward signal based on distance to the goal. Detailed experiments conducted using the AI2-THOR simulation environment demonstrate the efficacy of the proposed approach for object-goal navigation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1500525865",
                    "name": "Srirangan Madhavan"
                },
                {
                    "authorId": "151453388",
                    "name": "Anwesan Pal"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                }
            ]
        },
        {
            "paperId": "c76a3fb1a15007031fbb47e16ba8dff98d02a9df",
            "title": "Single RGB-D Camera Teleoperation for General Robotic Manipulation",
            "abstract": "We propose a teleoperation system that uses a single RGB-D camera as the human motion capture device. Our system can perform general manipulation tasks such as cloth folding, hammering and 3mm clearance peg in hole. We propose the use of non-Cartesian oblique coordinate frame, dynamic motion scaling and reposition of operator frames to increase the flexibility of our teleoperation system. We hypothesize that lowering the barrier of entry to teleoperation will allow for wider deployment of supervised autonomy system, which will in turn generates realistic datasets that unlock the potential of machine learning for robotic manipulation. Demo of our systems are available online https://sites.google.com/view/manipulation-teleop-with-rgbd",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144579461",
                    "name": "Q. Vuong"
                },
                {
                    "authorId": "12701031",
                    "name": "Yuzhe Qin"
                },
                {
                    "authorId": "1878932409",
                    "name": "Runlin Guo"
                },
                {
                    "authorId": "122024152",
                    "name": "Xiaolong Wang"
                },
                {
                    "authorId": "2087042750",
                    "name": "Hao Su"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                }
            ]
        },
        {
            "paperId": "122da9003959d5fde1879eb55de65a6fb4da11b9",
            "title": "Learning hierarchical relationships for object-goal navigation",
            "abstract": "Direct search for objects as part of navigation poses a challenge for small items. Utilizing context in the form of object-object relationships enable hierarchical search for targets efficiently. Most of the current approaches tend to directly incorporate sensory input into a reward-based learning approach, without learning about object relationships in the natural environment, and thus generalize poorly across domains. We present Memory-utilized Joint hierarchical Object Learning for Navigation in Indoor Rooms (MJOLNIR), a target-driven navigation algorithm, which considers the inherent relationship between target objects, and the more salient contextual objects occurring in its surrounding. Extensive experiments conducted across multiple environment settings show an $82.9\\%$ and $93.5\\%$ gain over existing state-of-the-art navigation methods in terms of the success rate (SR), and success weighted by path length (SPL), respectively. We also show that our model learns to converge much faster than other algorithms, without suffering from the well-known overfitting problem. Additional details regarding the supplementary material and code are available at https://sites.google.com/eng.ucsd.edu/mjolnir.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151453388",
                    "name": "Anwesan Pal"
                },
                {
                    "authorId": "1563918035",
                    "name": "Yiding Qiu"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                }
            ]
        },
        {
            "paperId": "157aac8cdb900ee6fe6265179e6c37a92cbbe1aa",
            "title": "How to pick the domain randomization parameters for sim-to-real transfer of reinforcement learning policies?",
            "abstract": "Recently, reinforcement learning (RL) algorithms have demonstrated remarkable success in learning complicated behaviors from minimally processed input. However, most of this success is limited to simulation. While there are promising successes in applying RL algorithms directly on real systems, their performance on more complex systems remains bottle-necked by the relative data inefficiency of RL algorithms. Domain randomization is a promising direction of research that has demonstrated impressive results using RL algorithms to control real robots. At a high level, domain randomization works by training a policy on a distribution of environmental conditions in simulation. If the environments are diverse enough, then the policy trained on this distribution will plausibly generalize to the real world. A human-specified design choice in domain randomization is the form and parameters of the distribution of simulated environments. It is unclear how to the best pick the form and parameters of this distribution and prior work uses hand-tuned distributions. This extended abstract demonstrates that the choice of the distribution plays a major role in the performance of the trained policies in the real world and that the parameter of this distribution can be optimized to maximize the performance of the trained policies in the real world",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "144579461",
                    "name": "Q. Vuong"
                },
                {
                    "authorId": "2425230",
                    "name": "S. Vikram"
                },
                {
                    "authorId": "2093560213",
                    "name": "H. Su"
                },
                {
                    "authorId": "39219411",
                    "name": "Sicun Gao"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                }
            ]
        },
        {
            "paperId": "516643d89835d9ac40bee7a88ec9516f399c3b9a",
            "title": "Multi-task Batch Reinforcement Learning with Metric Learning",
            "abstract": "We tackle the Multi-task Batch Reinforcement Learning problem. Given multiple datasets collected from different tasks, we train a multi-task policy to perform well in unseen tasks sampled from the same distribution. The task identities of the unseen tasks are not provided. To perform well, the policy must infer the task identity from collected transitions by modelling its dependency on states, actions and rewards. Because the different datasets may have state-action distributions with large divergence, the task inference module can learn to ignore the rewards and spuriously correlate \\textit{only} state-action pairs to the task identity, leading to poor test time performance. To robustify task inference, we propose a novel application of the triplet loss. To mine hard negative examples, we relabel the transitions from the training tasks by approximating their reward functions. When further training is allowed on the unseen tasks, using the trained policy as an initialization leads to significantly faster convergence compared to randomly initialized policies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2258750843",
                    "name": "Jiachen Li"
                },
                {
                    "authorId": "144579461",
                    "name": "Q. Vuong"
                },
                {
                    "authorId": "2108588775",
                    "name": "Shuang Liu"
                },
                {
                    "authorId": "47842126",
                    "name": "Minghua Liu"
                },
                {
                    "authorId": "2474449",
                    "name": "K. Ciosek"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                },
                {
                    "authorId": "2093560213",
                    "name": "H. Su"
                }
            ]
        },
        {
            "paperId": "da47685ad651f764bf749e4ff38bd88b22b3d1fa",
            "title": "Pre-training as Batch Meta Reinforcement Learning with tiMe",
            "abstract": "Pre-training is transformative in supervised learning: a large network trained with large and existing datasets can be used as an initialization when learning a new task. Such initialization speeds up convergence and leads to higher performance. In this paper, we seek to understand what the formalization for pre-training from only existing and observational data in Reinforcement Learning (RL) is and whether it is possible. We formulate the setting as Batch Meta Reinforcement Learning. We identify MDP mis-identification to be a central challenge and motivate it with theoretical analysis. Combining ideas from Batch RL and Meta RL, we propose tiMe, which learns distillation of multiple value functions and MDP embeddings from only existing data. In challenging control tasks and without fine-tuning on unseen MDPs, tiMe is competitive with state-of-the-art model-free RL method trained with hundreds of thousands of environment interactions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "144579461",
                    "name": "Q. Vuong"
                },
                {
                    "authorId": "2108588775",
                    "name": "Shuang Liu"
                },
                {
                    "authorId": "47842126",
                    "name": "Minghua Liu"
                },
                {
                    "authorId": "2474449",
                    "name": "K. Ciosek"
                },
                {
                    "authorId": "2093560213",
                    "name": "H. Su"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                }
            ]
        },
        {
            "paperId": "f6c052e9f54032f2a77d6c825a02547dd57e3368",
            "title": "Darwinian embodied evolution of the learning ability for survival",
            "abstract": "In this article we propose a framework for performing embodied evolution with a limited number of robots, by utilizing time-sharing in subpopulations of virtual agents hosted in each robot. Within this framework, we explore the combination of within-generation learning of basic survival behaviors by reinforcement learning, and evolutionary adaptations over the generations of the basic behavior selection policy, the reward functions, and metaparameters for reinforcement learning. We apply a biologically inspired selection scheme, in which there is no explicit communication of the individuals\u2019 fitness information. The individuals can only reproduce offspring by mating\u2014a pair-wise exchange of genotypes\u2014and the probability that an individual reproduces offspring in its own subpopulation is dependent on the individual\u2019s \u2018\u2018health,\u2019\u2019 that is, energy level, at the mating occasion. We validate the proposed method by comparing it with evolution using standard centralized selection, in simulation, and by transferring the obtained solutions to hardware using two real robots.",
            "fieldsOfStudy": [
                "Psychology",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2169762",
                    "name": "Stefan Elfwing"
                },
                {
                    "authorId": "1773761",
                    "name": "E. Uchibe"
                },
                {
                    "authorId": "1714997",
                    "name": "K. Doya"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                }
            ]
        },
        {
            "paperId": "1c3c80ec3f25fb3e2d4d37078d75cf8bc636811f",
            "title": "Co-evolution of Shaping Rewards and Meta-Parameters in Reinforcement Learning",
            "abstract": "In this article, we explore an evolutionary approach to the optimization of potential-based shaping rewards and meta-parameters in reinforcement learning. Shaping rewards is a frequently used approach to increase the learning performance of reinforcement learning, with regards to both initial performance and convergence speed. Shaping rewards provide additional knowledge to the agent in the form of richer reward signals, which guide learning to high-rewarding states. Reinforcement learning depends critically on a few meta-parameters that modulate the learning updates or the exploration of the environment, such as the learning rate \u03b1, the discount factor of future rewards \u03b3, and the temperature \u03c4 that controls the trade-off between exploration and exploitation in softmax action selection. We validate the proposed approach in simulation using the mountain-car task. We also transfer shaping rewards and meta-parameters, evolutionarily obtained in simulation, to hardware, using a robotic foraging task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2169762",
                    "name": "Stefan Elfwing"
                },
                {
                    "authorId": "1773761",
                    "name": "E. Uchibe"
                },
                {
                    "authorId": "1714997",
                    "name": "K. Doya"
                },
                {
                    "authorId": "2065343184",
                    "name": "Henrik I. Christensen"
                }
            ]
        }
    ]
}