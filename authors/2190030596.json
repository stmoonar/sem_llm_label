{
    "authorId": "2190030596",
    "papers": [
        {
            "paperId": "84f5f9e77aab23c5ca8d68b97c62233867a4a4e1",
            "title": "Noise-Free Audio Signal Processing in Noisy Environment: A Hardware and Algorithm Solution",
            "abstract": "Dealing with background noise is a challenging task in audio signal processing, negatively impacting algorithm performance and system robustness. In this paper, we propose a simple solution that combines recording hardware modification and algorithm improvement to tackle the challenge. The proposed solution could produce clean and noise-free high quality audio recording even in noisy recording environment. Experiment results show that the proposed solution leads to better sound event detection accuracy and speech recognition results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2288325330",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "2288059395",
                    "name": "Zongyi Liu"
                },
                {
                    "authorId": "2288010213",
                    "name": "Shunyan Luo"
                },
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "2288045141",
                    "name": "Shuyi Wang"
                },
                {
                    "authorId": "67038534",
                    "name": "Bruce Ferry"
                }
            ]
        },
        {
            "paperId": "8596588b6b9a705520404a9cfb51449d8d7e8a39",
            "title": "Detecting Content Segments from Online Sports Streaming Events: Challenges and Solutions",
            "abstract": "Developing a client-side segmentation algorithm for on-line sports streaming holds significant importance. For instance, in order to assess the video quality from an end-user perspective such as artifact detection, it is important to initially segment the content within the streaming playback. The challenge lies in localizing the content due to the intricate scene changes between content and non-content sections in popular sports like football, tennis, baseball, and more. Client-side content detection can be implemented in two ways: intrusively, involving the interception of network traffic and parsing service provider data and logs, or non-intrusively, which entails capturing streamed videos from content providers and subjecting them to analysis using computer vision technologies. In this paper, we introduce a non-intrusive framework that leverages a combination of traditional machine learning algorithms and deep neural networks (DNN) to distinguish content sections from noncontent sections across various online sports streaming services. Our algorithm has demonstrated a remarkable level of accuracy and effectiveness in sports broadcasting events, effectively overcoming the complexities introduced by intricate non-content insertion methods during the games.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2288059395",
                    "name": "Zongyi Liu"
                },
                {
                    "authorId": "2288325330",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "2288010213",
                    "name": "Shunyan Luo"
                },
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "2288045141",
                    "name": "Shuyi Wang"
                }
            ]
        },
        {
            "paperId": "681253389d2cc27103753749f4c7556699d55471",
            "title": "Temporal Data Meets LLM - Explainable Financial Time Series Forecasting",
            "abstract": "This paper presents a novel study on harnessing Large Language Models' (LLMs) outstanding knowledge and reasoning abilities for explainable financial time series forecasting. The application of machine learning models to financial time series comes with several challenges, including the difficulty in cross-sequence reasoning and inference, the hurdle of incorporating multi-modal signals from historical news, financial knowledge graphs, etc., and the issue of interpreting and explaining the model results. In this paper, we focus on NASDAQ-100 stocks, making use of publicly accessible historical stock price data, company metadata, and historical economic/financial news. We conduct experiments to illustrate the potential of LLMs in offering a unified solution to the aforementioned challenges. Our experiments include trying zero-shot/few-shot inference with GPT-4 and instruction-based fine-tuning with a public LLM model Open LLaMA. We demonstrate our approach outperforms a few baselines, including the widely applied classic ARMA-GARCH model and a gradient-boosting tree model. Through the performance comparison results and a few examples, we find LLMs can make a well-thought decision by reasoning over information from both textual news and price time series and extracting insights, leveraging cross-sequence information, and utilizing the inherent knowledge embedded within the LLM. Additionally, we show that a publicly available LLM such as Open-LLaMA, after fine-tuning, can comprehend the instruction to generate explainable forecasts and achieve reasonable performance, albeit relatively inferior in comparison to GPT-4.",
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "authors": [
                {
                    "authorId": "2118210915",
                    "name": "Xinli Yu"
                },
                {
                    "authorId": "2141144864",
                    "name": "Zheng Chen"
                },
                {
                    "authorId": "48907594",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "47781311",
                    "name": "Zongying Liu"
                },
                {
                    "authorId": "2220668748",
                    "name": "Yanbin Lu"
                }
            ]
        },
        {
            "paperId": "7d3f0f83fa943529d1516030c6abebd5a89aa44c",
            "title": "High Precision Sound Event Detection based on Transfer Learning using Transposed Convolutions and Feature Pyramid Network",
            "abstract": "We introduce two models for high precision sound event detection leveraging transfer learning. The sound events we detect include \u201cspeech\u201d, \u201cmusic\u201d, and \u201cchime\u201d. Both models consist of a CNN backbone pre-trained using AudioSet for audio classification. To get high precision detection results, the first model employs transposed convolutional layers as the detection head, while the second model uses Feature Pyramid Network(FPN) as the detection head. Experimental results show 98.8% accuracy and 98.6% F1 score on a private test set, from the one using FPN. Both models outperform a two-stage model using LSTM, various model ensembles, and a pre-trained neural network model for audio classification.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114031488",
                    "name": "S. Luo"
                },
                {
                    "authorId": "100888797",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "152613893",
                    "name": "Z. Liu"
                },
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "67038534",
                    "name": "Bruce Ferry"
                }
            ]
        },
        {
            "paperId": "ee210be856f8f782308808775e3dbd64a3e3df45",
            "title": "International Workshop on Multimodal Learning - 2023 Theme: Multimodal Learning with Foundation Models",
            "abstract": "The recent advancements in machine learning and artificial intelligence (particularly foundation models such as BERT, GPT-3, T5, ResNet, etc.) have demonstrated remarkable capabilities and driven significant revolutionary changes to the way we make inferences from complex data. These models represent a fundamental shift in the way data are approached and offer exciting new research directions and opportunities for multimodal learning and data fusion. Given the potential of foundation models to transform the field of multimodal learning, there is a need to bring together experts and researchers to discuss the latest developments in this area, exchange ideas, and identify key research questions and challenges that need to be addressed. By hosting this workshop, we aim to create a forum for researchers to share their insights and expertise on multimodal data fusion and learning using foundation models, and to explore potential new research directions and applications in the rapidly evolving field. We expect contributions from interdisciplinary researchers to study and model interactions between (but not limited to) modalities of language, graphs, time-series, vision, tabular data, sensors, and more. Our workshop will emphasize interdisciplinary work and aim at seeding cross-team collaborations around new tasks, datasets, and models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "49604787",
                    "name": "Fanyou Wu"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "100888797",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "50877490",
                    "name": "G. Karypis"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "dc55a213cf37862af7f8d1675c732a0dd0f3bc76",
            "title": "Detect Audio-Video Temporal Synchronization Errors in Advertisements (Ads)",
            "abstract": "Detecting audio-video (A/V) synchronization error is important to measure end user experience. Today, researches in this domain are mainly focused on contents such as movies or sports. The state of art algorithms usually first detect a specific type of events and then correlate the A/V data within during these events, e.g., find the human chatting events and then correlate the vocals with the lip shapes. Detecting A/V sync errors during Ads, on the other hand, has not received a lot of attentions. Compared with contents, an Ads section do not contain a particular type of events that can be used to detect A/V sync error. For example, many vocals in Ads are either from background narrators or have a very short period of time, so that the popular lip-sync based algorithms won\u2019t work accurately. In this paper, we present a novel algorithm that uses the scene change time features: we first segment out individual Ad from a playback. Then for each pair of temporal adjacent Ads, we compute the scene change time for the video data and the audio data separately, and then build their time difference histogram. Next, we aggregate the histograms from all Ads pairs within one Ads section. Finally, we combine the aggregated histogram to compute the A/V off-sync time values. We show that compared with the traditional lip-sync based algorithms, the new algorithm not only significantly improves the prediction rate, but also increases the prediction accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152613893",
                    "name": "Z. Liu"
                },
                {
                    "authorId": "2193282077",
                    "name": "Devin Chen"
                },
                {
                    "authorId": "100888797",
                    "name": "Yarong Feng"
                },
                {
                    "authorId": "2159011228",
                    "name": "Yuan Ling"
                },
                {
                    "authorId": "2114031488",
                    "name": "S. Luo"
                },
                {
                    "authorId": "2190030596",
                    "name": "Shujing Dong"
                },
                {
                    "authorId": "67038534",
                    "name": "Bruce Ferry"
                }
            ]
        }
    ]
}