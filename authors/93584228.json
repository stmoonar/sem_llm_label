{
    "authorId": "93584228",
    "papers": [
        {
            "paperId": "035e620fbefe8adee0fb4dc6cf232a914779d72c",
            "title": "ChemSpacE: Interpretable and Interactive Chemical Space Exploration",
            "abstract": "Discovering meaningful molecules in the vast combinatorial chemical space has been a long-standing challenge in many fields, from materials science to drug design. Recent progress in machine learning, especially with generative models, shows great promise for automated molecule synthesis. Nevertheless, most molecule generative models remain black-boxes, whose utilities are limited by a lack of interpretability and human participation in the generation process. In this work, we propose Chem ical Spac e E xplorer (ChemSpacE), a simple yet effective method for exploring the chemical space with pre-trained deep generative models. Our method enables users to interact with existing generative models and steer the molecule generation process. We demonstrate the efficacy of ChemSpacE on the molecule optimization task and the latent molecule manipulation task in single-property and multi-property settings. On the molecule optimization task, the performance of ChemSpacE is on par with previous black-box optimization methods yet is considerably faster and more sample efficient. Furthermore, the interface from ChemSpacE facilitates human-in-the-loop chemical space exploration and interactive molecule design. Code and demo are available at https://github.com/yuanqidu/ChemSpacE .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "93584228",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "2108720083",
                    "name": "Xian Liu"
                },
                {
                    "authorId": "2217892863",
                    "name": "Nilay Mahesh Shah"
                },
                {
                    "authorId": "1563693999",
                    "name": "Shengchao Liu"
                },
                {
                    "authorId": "2159191279",
                    "name": "Jieyu Zhang"
                },
                {
                    "authorId": "145291669",
                    "name": "Bolei Zhou"
                }
            ]
        },
        {
            "paperId": "17a48ebfef2ed820f3529f11b9a5acf48a9a0fe5",
            "title": "A new perspective on building efficient and expressive 3D equivariant graph neural networks",
            "abstract": "Geometric deep learning enables the encoding of physical symmetries in modeling 3D objects. Despite rapid progress in encoding 3D symmetries into Graph Neural Networks (GNNs), a comprehensive evaluation of the expressiveness of these networks through a local-to-global analysis lacks today. In this paper, we propose a local hierarchy of 3D isomorphism to evaluate the expressive power of equivariant GNNs and investigate the process of representing global geometric information from local patches. Our work leads to two crucial modules for designing expressive and efficient geometric GNNs; namely local substructure encoding (LSE) and frame transition encoding (FTE). To demonstrate the applicability of our theory, we propose LEFTNet which effectively implements these modules and achieves state-of-the-art performance on both scalar-valued and vector-valued molecular property prediction tasks. We further point out the design space for future developments of equivariant graph neural networks. Our codes are available at \\url{https://github.com/yuanqidu/LeftNet}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "134861298",
                    "name": "Weitao Du"
                },
                {
                    "authorId": "93584228",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "2109120459",
                    "name": "Limei Wang"
                },
                {
                    "authorId": "7841666",
                    "name": "Dieqiao Feng"
                },
                {
                    "authorId": "2118561363",
                    "name": "Guifeng Wang"
                },
                {
                    "authorId": "1743600",
                    "name": "Shuiwang Ji"
                },
                {
                    "authorId": "2188778449",
                    "name": "Carla P. Gomes"
                },
                {
                    "authorId": "2116416699",
                    "name": "Zhixin Ma"
                }
            ]
        },
        {
            "paperId": "3c0f8913ed5c3c27bcec4eb5ea87ca7423a7c2c0",
            "title": "Weighted Sampling without Replacement for Deep Top-k Classification",
            "abstract": "The top-k classification accuracy is a crucial metric in machine learning and is often used to evaluate the performance of deep neural networks. These networks are typically trained using the cross-entropy loss, which optimizes for top-1 classification and is considered optimal in the case of infinite data. However, in real-world scenarios, data is often noisy and limited, leading to the need for more robust losses. In this paper, we propose using the Weighted Sampling Without Replacement (WSWR) method as a learning objective for top-k loss. While traditional methods for evaluating WSWR-based top-k loss are computationally impractical, we show a novel connection between WSWR and Reinforcement Learning (RL) and apply well-established RL algorithms to estimate gradients. We compared our method with recently proposed top-k losses in various regimes of noise and data size for the prevalent use case of k = 5 . Our experimental results reveal that our method consistently outperforms all other methods on the top-k metric for noisy datasets, has more robustness on extreme testing scenarios, and achieves competitive results on training with limited data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7841666",
                    "name": "Dieqiao Feng"
                },
                {
                    "authorId": "93584228",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "2188778449",
                    "name": "Carla P. Gomes"
                },
                {
                    "authorId": "1744679",
                    "name": "B. Selman"
                }
            ]
        },
        {
            "paperId": "45a4e7d72dc559d5a582c89ad02c9f145463e84f",
            "title": "M2Hub: Unlocking the Potential of Machine Learning for Materials Discovery",
            "abstract": "We introduce M$^2$Hub, a toolkit for advancing machine learning in materials discovery. Machine learning has achieved remarkable progress in modeling molecular structures, especially biomolecules for drug discovery. However, the development of machine learning approaches for modeling materials structures lag behind, which is partly due to the lack of an integrated platform that enables access to diverse tasks for materials discovery. To bridge this gap, M$^2$Hub will enable easy access to materials discovery tasks, datasets, machine learning methods, evaluations, and benchmark results that cover the entire workflow. Specifically, the first release of M$^2$Hub focuses on three key stages in materials discovery: virtual screening, inverse design, and molecular simulation, including 9 datasets that covers 6 types of materials with 56 tasks across 8 types of material properties. We further provide 2 synthetic datasets for the purpose of generative tasks on materials. In addition to random data splits, we also provide 3 additional data partitions to reflect the real-world materials discovery scenarios. State-of-the-art machine learning methods (including those are suitable for materials structures but never compared in the literature) are benchmarked on representative tasks. Our codes and library are publicly available at https://github.com/yuanqidu/M2Hub.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "93584228",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "2107962435",
                    "name": "Yingheng Wang"
                },
                {
                    "authorId": "2185656535",
                    "name": "Yin-Hua Huang"
                },
                {
                    "authorId": "2177387927",
                    "name": "Jianan Canal Li"
                },
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "49902007",
                    "name": "T. Xie"
                },
                {
                    "authorId": "35473448",
                    "name": "Chenru Duan"
                },
                {
                    "authorId": "2377945",
                    "name": "J. Gregoire"
                },
                {
                    "authorId": "2082417220",
                    "name": "Carla P. Gomes"
                }
            ]
        },
        {
            "paperId": "7420f10120ca0f237ea90e9c04b1652ccba61792",
            "title": "Xtal2DoS: Attention-based Crystal to Sequence Learning for Density of States Prediction",
            "abstract": "Modern machine learning techniques have been extensively applied to materials science, especially for property prediction tasks. A majority of these methods address scalar property predictions, while more challenging spectral properties remain less emphasized. We formulate a crystal-to-sequence learning task and propose a novel attention-based learning method, Xtal2DoS, which decodes the sequential representation of the material density of states (DoS) properties by incorporating the learned atomic embeddings through attention networks. Experiments show Xtal2DoS is faster than the existing models, and consistently outperforms other state-of-the-art methods on four metrics for two fundamental spectral properties, phonon and electronic DoS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7238328",
                    "name": "Junwen Bai"
                },
                {
                    "authorId": "93584228",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "2107962435",
                    "name": "Yingheng Wang"
                },
                {
                    "authorId": "3184152",
                    "name": "Shufeng Kong"
                },
                {
                    "authorId": "2377945",
                    "name": "J. Gregoire"
                },
                {
                    "authorId": "2064532325",
                    "name": "Carla P. Gomes"
                }
            ]
        },
        {
            "paperId": "8e1472452201fca90a15582ee356d9cf2783d90a",
            "title": "Accurate transition state generation with an object-aware equivariant elementary reaction diffusion model",
            "abstract": "Transition state search is key in chemistry for elucidating reaction mechanisms and exploring reaction networks. The search for accurate 3D transition state structures, however, requires numerous computationally intensive quantum chemistry calculations due to the complexity of potential energy surfaces. Here we developed an object-aware SE(3) equivariant diffusion model that satisfies all physical symmetries and constraints for generating sets of structures-reactant, transition state and product-in an elementary reaction. Provided reactant and product, this model generates a transition state structure in seconds instead of hours, which is typically required when performing quantum-chemistry-based optimizations. The generated transition state structures achieve a median of 0.08\u2009\u00c5 root mean square deviation compared to the true transition state. With a confidence scoring model for uncertainty quantification, we approach an accuracy required for reaction barrier estimation (2.6\u2009kcal\u2009mol-1) by only performing quantum chemistry-based optimizations on 14% of the most challenging reactions. We envision usefulness for our approach in constructing large reaction networks with unknown mechanisms.",
            "fieldsOfStudy": [
                "Physics",
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "35473448",
                    "name": "Chenru Duan"
                },
                {
                    "authorId": "93584228",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "79585457",
                    "name": "Haojun Jia"
                },
                {
                    "authorId": "3434648",
                    "name": "H. Kulik"
                }
            ]
        },
        {
            "paperId": "ca6a325ebecab63ada489066885a211c74c7e9a6",
            "title": "On Separate Normalization in Self-supervised Transformers",
            "abstract": "Self-supervised training methods for transformers have demonstrated remarkable performance across various domains. Previous transformer-based models, such as masked autoencoders (MAE), typically utilize a single normalization layer for both the [CLS] symbol and the tokens. We propose in this paper a simple modification that employs separate normalization layers for the tokens and the [CLS] symbol to better capture their distinct characteristics and enhance downstream task performance. Our method aims to alleviate the potential negative effects of using the same normalization statistics for both token types, which may not be optimally aligned with their individual roles. We empirically show that by utilizing a separate normalization layer, the [CLS] embeddings can better encode the global contextual information and are distributed more uniformly in its anisotropic space. When replacing the conventional normalization layer with the two separate layers, we observe an average 2.7% performance improvement over the image, natural language, and graph domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144201630",
                    "name": "Xiaohui Chen"
                },
                {
                    "authorId": "2143376563",
                    "name": "Yinkai Wang"
                },
                {
                    "authorId": "93584228",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "3242252",
                    "name": "S. Hassoun"
                },
                {
                    "authorId": "1391189766",
                    "name": "Liping Liu"
                }
            ]
        },
        {
            "paperId": "d1f49ea7287120bb7a1e214862388222ce76d9e8",
            "title": "Artificial Intelligence for Science in Quantum, Atomistic, and Continuum Systems",
            "abstract": "Advances in artificial intelligence (AI) are fueling a new paradigm of discoveries in natural sciences. Today, AI has started to advance natural sciences by improving, accelerating, and enabling our understanding of natural phenomena at a wide range of spatial and temporal scales, giving rise to a new area of research known as AI for science (AI4Science). Being an emerging research paradigm, AI4Science is unique in that it is an enormous and highly interdisciplinary area. Thus, a unified and technical treatment of this field is needed yet challenging. This work aims to provide a technically thorough account of a subarea of AI4Science; namely, AI for quantum, atomistic, and continuum systems. These areas aim at understanding the physical world from the subatomic (wavefunctions and electron density), atomic (molecules, proteins, materials, and interactions), to macro (fluids, climate, and subsurface) scales and form an important subarea of AI4Science. A unique advantage of focusing on these areas is that they largely share a common set of challenges, thereby allowing a unified and foundational treatment. A key common challenge is how to capture physics first principles, especially symmetries, in natural systems by deep learning methods. We provide an in-depth yet intuitive account of techniques to achieve equivariance to symmetry transformations. We also discuss other common technical challenges, including explainability, out-of-distribution generalization, knowledge transfer with foundation and large language models, and uncertainty quantification. To facilitate learning and education, we provide categorized lists of resources that we found to be useful. We strive to be thorough and unified and hope this initial effort may trigger more community interests and efforts to further advance AI4Science.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2155975186",
                    "name": "Xuan Zhang"
                },
                {
                    "authorId": "2109120459",
                    "name": "Limei Wang"
                },
                {
                    "authorId": "2211730572",
                    "name": "Jacob Helwig"
                },
                {
                    "authorId": "2004524780",
                    "name": "Youzhi Luo"
                },
                {
                    "authorId": "2084647086",
                    "name": "Cong Fu"
                },
                {
                    "authorId": "14629242",
                    "name": "Yaochen Xie"
                },
                {
                    "authorId": "38813990",
                    "name": "Meng Liu"
                },
                {
                    "authorId": "2107966079",
                    "name": "Yu-Ching Lin"
                },
                {
                    "authorId": "2115510017",
                    "name": "Zhao Xu"
                },
                {
                    "authorId": "1879114760",
                    "name": "Keqiang Yan"
                },
                {
                    "authorId": "95261912",
                    "name": "Keir Adams"
                },
                {
                    "authorId": "47927975",
                    "name": "Maurice Weiler"
                },
                {
                    "authorId": "2118053386",
                    "name": "Xiner Li"
                },
                {
                    "authorId": "2427076",
                    "name": "Tianfan Fu"
                },
                {
                    "authorId": "2204444421",
                    "name": "Yucheng Wang"
                },
                {
                    "authorId": "2119316118",
                    "name": "Haiyang Yu"
                },
                {
                    "authorId": "2118597356",
                    "name": "Yuqing Xie"
                },
                {
                    "authorId": "2119032930",
                    "name": "Xiang Fu"
                },
                {
                    "authorId": "103618574",
                    "name": "A. Strasser"
                },
                {
                    "authorId": "50433504",
                    "name": "Shenglong Xu"
                },
                {
                    "authorId": "2153630672",
                    "name": "Yi Liu"
                },
                {
                    "authorId": "93584228",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "2223648674",
                    "name": "Alexandra Saxton"
                },
                {
                    "authorId": "2055943899",
                    "name": "Hongyi Ling"
                },
                {
                    "authorId": "46877850",
                    "name": "Hannah Lawrence"
                },
                {
                    "authorId": "2124211700",
                    "name": "Hannes St\u00e4rk"
                },
                {
                    "authorId": "1914700964",
                    "name": "Shurui Gui"
                },
                {
                    "authorId": "48870109",
                    "name": "Carl N. Edwards"
                },
                {
                    "authorId": "2068203542",
                    "name": "Nicholas Gao"
                },
                {
                    "authorId": "2036602766",
                    "name": "A. Ladera"
                },
                {
                    "authorId": "3716141",
                    "name": "Tailin Wu"
                },
                {
                    "authorId": "146613894",
                    "name": "E. Hofgard"
                },
                {
                    "authorId": "90258499",
                    "name": "A. M. Tehrani"
                },
                {
                    "authorId": "2151036085",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "102488653",
                    "name": "Ameya Daigavane"
                },
                {
                    "authorId": "2223648322",
                    "name": "Montgomery Bohde"
                },
                {
                    "authorId": "2179190141",
                    "name": "Jerry Kurtin"
                },
                {
                    "authorId": "2111287017",
                    "name": "Qiang Huang"
                },
                {
                    "authorId": "2223648653",
                    "name": "Tuong Phung"
                },
                {
                    "authorId": "1490886784",
                    "name": "Minkai Xu"
                },
                {
                    "authorId": "38009979",
                    "name": "Chaitanya K. Joshi"
                },
                {
                    "authorId": "2054235048",
                    "name": "Simon V. Mathis"
                },
                {
                    "authorId": "3371922",
                    "name": "K. Azizzadenesheli"
                },
                {
                    "authorId": "2030142647",
                    "name": "Ada Fang"
                },
                {
                    "authorId": "1422193589",
                    "name": "A. Aspuru\u2010Guzik"
                },
                {
                    "authorId": "2231179",
                    "name": "E. Bekkers"
                },
                {
                    "authorId": "2149583375",
                    "name": "Michael M. Bronstein"
                },
                {
                    "authorId": "2095762",
                    "name": "M. Zitnik"
                },
                {
                    "authorId": "2047844",
                    "name": "Anima Anandkumar"
                },
                {
                    "authorId": "2490652",
                    "name": "Stefano Ermon"
                },
                {
                    "authorId": "2075355155",
                    "name": "Pietro Lio'"
                },
                {
                    "authorId": "2151886670",
                    "name": "Rose Yu"
                },
                {
                    "authorId": "51249380",
                    "name": "Stephan Gunnemann"
                },
                {
                    "authorId": "1702139",
                    "name": "J. Leskovec"
                },
                {
                    "authorId": "144016781",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "49991208",
                    "name": "Jimeng Sun"
                },
                {
                    "authorId": "1741283",
                    "name": "R. Barzilay"
                },
                {
                    "authorId": "35132120",
                    "name": "T. Jaakkola"
                },
                {
                    "authorId": "13027820",
                    "name": "Connor W. Coley"
                },
                {
                    "authorId": "2067730514",
                    "name": "Xiaoning Qian"
                },
                {
                    "authorId": "2198715748",
                    "name": "Xiaofeng Qian"
                },
                {
                    "authorId": "5485763",
                    "name": "T. Smidt"
                },
                {
                    "authorId": "1743600",
                    "name": "Shuiwang Ji"
                }
            ]
        },
        {
            "paperId": "d3e6702c319cbd5314f0adea6e408562cb104e77",
            "title": "Uncovering Neural Scaling Laws in Molecular Representation Learning",
            "abstract": "Molecular Representation Learning (MRL) has emerged as a powerful tool for drug and materials discovery in a variety of tasks such as virtual screening and inverse design. While there has been a surge of interest in advancing model-centric techniques, the influence of both data quantity and quality on molecular representations is not yet clearly understood within this field. In this paper, we delve into the neural scaling behaviors of MRL from a data-centric viewpoint, examining four key dimensions: (1) data modalities, (2) dataset splitting, (3) the role of pre-training, and (4) model capacity. Our empirical studies confirm a consistent power-law relationship between data volume and MRL performance across these dimensions. Additionally, through detailed analysis, we identify potential avenues for improving learning efficiency. To challenge these scaling laws, we adapt seven popular data pruning strategies to molecular data and benchmark their performance. Our findings underline the importance of data-centric MRL and highlight possible directions for future research.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2109123910",
                    "name": "Dingshuo Chen"
                },
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "47540245",
                    "name": "Jieyu Zhang"
                },
                {
                    "authorId": "93584228",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "2247629250",
                    "name": "Zhixun Li"
                },
                {
                    "authorId": "48873756",
                    "name": "Q. Liu"
                },
                {
                    "authorId": "50425438",
                    "name": "Shu Wu"
                },
                {
                    "authorId": "1693997",
                    "name": "Liang Wang"
                }
            ]
        },
        {
            "paperId": "d8c0ff00e7d8094cd00493923eea3dfffbc9e8ed",
            "title": "MUBen: Benchmarking the Uncertainty of Molecular Representation Models",
            "abstract": "Large molecular representation models pre-trained on massive unlabeled data have shown great success in predicting molecular properties. However, these models may tend to overfit the fine-tuning data, resulting in over-confident predictions on test data that fall outside of the training distribution. To address this issue, uncertainty quantification (UQ) methods can be used to improve the models' calibration of predictions. Although many UQ approaches exist, not all of them lead to improved performance. While some studies have included UQ to improve molecular pre-trained models, the process of selecting suitable backbone and UQ methods for reliable molecular uncertainty estimation remains underexplored. To address this gap, we present MUBen, which evaluates different UQ methods for state-of-the-art backbone molecular representation models to investigate their capabilities. By fine-tuning various backbones using different molecular descriptors as inputs with UQ methods from different categories, we assess the influence of architectural decisions and training strategies. Our study offers insights for selecting UQ for backbone models, which can facilitate research on uncertainty-critical applications in fields such as materials science and drug discovery.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "1527089853",
                    "name": "Yinghao Li"
                },
                {
                    "authorId": "2865034",
                    "name": "Lingkai Kong"
                },
                {
                    "authorId": "93584228",
                    "name": "Yuanqi Du"
                },
                {
                    "authorId": "1633124736",
                    "name": "Yue Yu"
                },
                {
                    "authorId": "8103389",
                    "name": "Yuchen Zhuang"
                },
                {
                    "authorId": "2220303350",
                    "name": "Wenhao Mu"
                },
                {
                    "authorId": "2152735278",
                    "name": "Chao Zhang"
                }
            ]
        }
    ]
}