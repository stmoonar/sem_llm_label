{
    "authorId": "144783569",
    "papers": [
        {
            "paperId": "70dfac2c00f26ea3c372aab8213d8eb8b9ca2062",
            "title": "Ensemble Prosody Prediction For Expressive Speech Synthesis",
            "abstract": "Generating expressive speech with rich and varied prosody continues to be a challenge for Text-to-Speech. Most efforts have focused on sophisticated neural architectures intended to better model the data distribution. Yet, in evaluations it is generally found that no single model is preferred for all input texts. This suggests an approach that has rarely been used before for Text-to-Speech: an ensemble of models.We apply ensemble learning to prosody prediction. We construct simple ensembles of prosody predictors by varying either model architecture or model parameter values.To automatically select amongst the models in the ensemble when performing Text-to-Speech, we propose a novel, and computationally trivial, variance-based criterion. We demonstrate that even a small ensemble of prosody predictors yields useful diversity, which, combined with the proposed selection criterion, outperforms any individual model from the ensemble.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1863888644",
                    "name": "Tian Huey Teh"
                },
                {
                    "authorId": "50161308",
                    "name": "Vivian Hu"
                },
                {
                    "authorId": "1864044654",
                    "name": "D. Mohan"
                },
                {
                    "authorId": "51263680",
                    "name": "Zack Hodari"
                },
                {
                    "authorId": "2213297845",
                    "name": "Christopher G. R. Wallis"
                },
                {
                    "authorId": "90190727",
                    "name": "Tom\u00e1s G\u00f3mez Ibarrondo"
                },
                {
                    "authorId": "1863935733",
                    "name": "Alexandra Torresquintero"
                },
                {
                    "authorId": "2213305354",
                    "name": "James Leoni"
                },
                {
                    "authorId": "1740397",
                    "name": "M. Gales"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "785bbd37811f60124711f7ce625f5e4f4d3ec31d",
            "title": "Spell4TTS: Acoustically-informed spellings for improving text-to-speech pronunciations",
            "abstract": "Ensuring accurate pronunciation is critical for high-quality text-to-speech (TTS). This typically requires a phoneme-based pro-nunciation dictionary, which is labour-intensive and costly to create. Previous work has suggested using graphemes instead of phonemes, but the inevitable pronunciation errors that occur cannot be fixed, since there is no longer a pronunciation dictionary. As an alternative, speech-based self-supervised learning (SSL) models have been proposed for pronunciation control, but these models are computationally expensive to train, produce representations that are not easily interpretable, and capture unwanted non-phonemic information. To address these limitations, we propose Spell4TTS, a novel method that generates acoustically-informed word spellings. Spellings are both inter-pretable and easily edited. The method could be applied to any existing pre-built TTS system. Our experiments show that the method creates word spellings that lead to fewer TTS pronunciation errors than the original spellings, or an Automatic Speech Recognition baseline. Additionally, we observe that pronunciation can be further enhanced by ranking candidates in the space of SSL speech representations, and by incorporating Human-in-the-Loop screening over the top-ranked spellings devised by our method. By working with spellings of words (composed of characters), the method lowers the entry barrier for TTS sys-tem development for languages with limited pronunciation resources. It should reduce the time and cost involved in creating and maintaining pronunciation dictionaries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2058161100",
                    "name": "Jason Fong"
                },
                {
                    "authorId": "40860114",
                    "name": "Hao Tang"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "a6796ecdb80c5ce090cd72860653d76c6512ddd2",
            "title": "Synthesising turn-taking cues using natural conversational data",
            "abstract": "As speech synthesis quality reaches high levels of naturalness for isolated utterances, more work is focusing on the synthesis of context-dependent conversational speech. The role of context in conversation is still poorly understood and many contextual factors can affect an utterances\u2019s prosodic realisation. Most studies incorporating context use rich acoustic or textual embeddings of the previous context, then demonstrate improvements in overall naturalness. Such studies are not informative about what the context embedding represents, or how it affects an utterance\u2019s realisation. So instead, we narrow the focus to a single, explicit contextual factor. In the current work, this is turn-taking. We condition a speech synthesis model on whether an utterance is turn-final. Objective measures and targeted subjective evaluation are used to demonstrate that the model can synthesise turn-taking cues which are perceived by listeners, with results being speaker-dependent.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1430838502",
                    "name": "Johannah O'Mahony"
                },
                {
                    "authorId": "144490056",
                    "name": "Catherine Lai"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "e9b949a558bb243653c3be5cb8b03b4dae19a605",
            "title": "Controllable Speaking Styles Using A Large Language Model",
            "abstract": "Reference-based Text-to-Speech (TTS) models can generate multiple, prosodically-different renditions of the same target text. Such models jointly learn a latent acoustic space during training, which can be sampled from during inference. Controlling these models during inference typically requires finding an appropriate reference utterance, which is non-trivial.Large generative language models (LLMs) have shown excellent performance in various language-related tasks. Given only a natural language query text (the \u2018prompt\u2019), such models can be used to solve specific, context-dependent tasks. Recent work in TTS has attempted similar prompt-based control of novel speaking style generation. Those methods do not require a reference utterance and can, under ideal conditions, be controlled with only a prompt. But existing methods typically require a prompt-labelled speech corpus for jointly training a prompt-conditioned encoder.In contrast, we instead employ an LLM to directly suggest prosodic modifications for a controllable TTS model, using contextual information provided in the prompt. The prompt can be designed for a multitude of tasks. Here, we give two demonstrations: control of speaking style; prosody appropriate for a given dialogue context. The proposed method is rated most appropriate in 50% of cases vs. 31% for a baseline model.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1725471992",
                    "name": "A. Sigurgeirsson"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "f8e9033761a84a0108f9e012fbe62a738a7a9790",
            "title": "Do Prosody Transfer Models Transfer Prosody\u0192",
            "abstract": "Some recent models for Text-to-Speech synthesis aim to transfer the prosody of a reference utterance to the generated target synthetic speech. This is done by using a learned embedding of the reference utterance, which is used to condition speech generation. During training, the reference utterance is identical to the target utterance. Yet, during synthesis, these models are often used to transfer prosody from a reference that differs from the text or speaker being synthesized.To address this inconsistency, we propose to use a different, but prosodically-related, utterance during training too. We believe this should encourage the model to learn to transfer only those characteristics that the reference and target have in common. If prosody transfer methods do indeed transfer prosody they should be able to be trained in the way we propose. However, results show that a model trained under these conditions performs significantly worse than one trained using the target utterance as a reference. To explain this, we hypothesize that prosody transfer models do not learn a transferable representation of prosody, but rather a utterance-level representation which is highly dependent on both the reference speaker and reference text.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1725471992",
                    "name": "A. Sigurgeirsson"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "02fca30a47400b5d7e6bca573fc94cd088d53f6c",
            "title": "Autovocoder: Fast Waveform Generation from a Learned Speech Representation Using Differentiable Digital Signal Processing",
            "abstract": "Most state-of-the-art Text-to-Speech systems use the mel-spectrogram as an intermediate representation, to decompose the task into acoustic modelling and waveform generation.A mel-spectrogram is extracted from the waveform by a simple, fast DSP operation, but generating a high-quality waveform from a mel-spectrogram requires computationally expensive machine learning: a neural vocoder. Our proposed \"autovocoder\" reverses this arrangement. We use machine learning to obtain a representation that replaces the mel-spectrogram, and that can be inverted back to a waveform using simple, fast operations including a differentiable implementation of the inverse STFT.The autovocoder generates a waveform 5 times faster than the DSP-based Griffin-Lim algorithm, and 14 times faster than the neural vocoder HiFi-GAN. We provide perceptual listening test results to confirm that the speech is of comparable quality to HiFi-GAN in the copy synthesis task.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2007311675",
                    "name": "Jacob J. Webber"
                },
                {
                    "authorId": "1401922561",
                    "name": "Cassia Valentini-Botinhao"
                },
                {
                    "authorId": "2190753684",
                    "name": "Evelyn Williams"
                },
                {
                    "authorId": "2763884",
                    "name": "G. Henter"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "38ce8657a95382d246d603e680cfe9ab6067fdfa",
            "title": "Combining conversational speech with read speech to improve prosody in Text-to-Speech synthesis",
            "abstract": "For isolated utterances, speech synthesis quality has improved immensely thanks to the use of sequence-to-sequence models. However, these models are generally trained on read speech and fail to generalise to unseen speaking styles. Recently, more research is focused on the synthesis of expressive and conversational speech. Conversational speech contains many prosodic phenomena that are not present in read speech. We would like to learn these prosodic patterns from data, but unfortunately, many large conversational corpora are unsuitable for speech synthesis due to low audio quality. We investigate whether a data mixing strategy can improve conversational prosody for a target voice based on monologue data from audiobooks by adding real conversational data from podcasts. We filter the podcast data to create a set of 26k question and answer pairs. We evaluate two FastPitch models: one trained on 20 hours of monologue speech from a single speaker, and another trained on 5 hours of monologue speech from that speaker plus 15 hours of questions and answers spoken by nearly 15k speakers. Results from three listening tests show that the second model generates more preferred question prosody.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1430838502",
                    "name": "Johannah O'Mahony"
                },
                {
                    "authorId": "144490056",
                    "name": "Catherine Lai"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "54ad775698d259810ac6125f9907f887eb17f4d4",
            "title": "Speech Audio Corrector: using speech from non-target speakers for one-off correction of mispronunciations in grapheme-input text-to-speech",
            "abstract": "Correct pronunciation is essential for text-to-speech (TTS) systems in production. Most production systems rely on pronouncing dictionaries to perform grapheme-to-phoneme conversion. Unlike end-to-end TTS, this enables pronunciation correction by manually altering the phoneme sequence, but the necessary dictionaries are labour-intensive to create and only exist in a few high-resourced languages. This work demonstrates that accurate TTS pronunciation control can be achieved without a dictionary. Moreover, we show that such control can be performed without requiring any model retraining or fine-tuning, merely by supplying a single correctly-pronounced reading of a word in a different voice and accent at synthesis time. Experimental results show that our proposed system successfully enables one-off correction of mispronunciations in grapheme-based TTS with maintained synthesis quality. This opens the door to production-level TTS in languages and applications where pronunciation dictionaries are unavailable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2058161100",
                    "name": "Jason Fong"
                },
                {
                    "authorId": "2185285049",
                    "name": "Daniel Lyth"
                },
                {
                    "authorId": "2763884",
                    "name": "G. Henter"
                },
                {
                    "authorId": "2112389207",
                    "name": "Hao Tang"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        },
        {
            "paperId": "94f3b624f67b80e469192edc94a8f19d7ed9d9ea",
            "title": "Back to the Future: Extending the Blizzard Challenge 2013",
            "abstract": "Nowadays, speech synthesis technology is synonymous with the use of Deep Learning. To understand more about how synthesis systems have progressed with the advent of Deep Learning requires open-sourced speech resources that connect past and present technologies. This would allow direct comparisons. This paper presents such a resource by extending the 2013 edition of the Blizzard Challenge. Using this extension, we compare top-tier systems from the past to modern technologies in a controlled setting. From this edition, we selected the best representative of each historical synthesis technology, to which we added four systems representing combinations of modern acoustic models and neural vocoders. A large scale subjective evaluation was conducted to evaluate naturalness. Our results show that, as expected, modern technologies generate more natural synthetic speech. However, these systems are still not perceived to be as natural as the human voice. Cru-cially, we also observed that the Mean Opinion Score (MOS) of the historical systems dropped a full MOS point from their scores in the original edition. This demonstrates the relative nature of MOS: it should generally not be reported as an absolute value despite its origin as an absolute category rating.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1971821",
                    "name": "S\u00e9bastien Le Maguer"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                },
                {
                    "authorId": "144686633",
                    "name": "N. Harte"
                }
            ]
        },
        {
            "paperId": "0a490e475700bfb6efea2287840c6ac08cc0bc66",
            "title": "The CSTR entry to the Blizzard Challenge 2021",
            "abstract": "We describe the text-to-speech (TTS) system submitted from The Centre for Speech Technology Research at the University of Edinburgh to the Blizzard Challenge 2021. We participated in the spoke task to build a voice for Peninsular Spanish, where test utterances contain a small number of English words. Our system is trained from monolingual data in Spanish and English, including some Spanish-accented English and Spanish utterances containing English words, but without explicit supervision for these aspects. Input texts are represented using phonological feature vectors to encourage parameter sharing between the two languages despite different phoneme inventories. When synthesizing test utterances, we perform automatic language identi\ufb01cation to provide word-level language embed-dings and apply pronunciation nativization rules to any detected English words to bring them closer to native Spanish phonology. In addition to the results of the main Blizzard Challenge evaluation, we present analysis of the impact of nativization strategy on listener preferences, which may be relevant for evaluation of code-switching TTS in general.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152337381",
                    "name": "Dan Wells"
                },
                {
                    "authorId": "2132314607",
                    "name": "Pilar Oplustil-Gallegos"
                },
                {
                    "authorId": "144783569",
                    "name": "Simon King"
                }
            ]
        }
    ]
}