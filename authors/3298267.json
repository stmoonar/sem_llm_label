{
    "authorId": "3298267",
    "papers": [
        {
            "paperId": "29539616bc9491c377ace58af1d20b3ae4b05a0a",
            "title": "FaithFill: Faithful Inpainting for Object Completion Using a Single Reference Image",
            "abstract": "We present FaithFill, a diffusion-based inpainting object completion approach for realistic generation of missing object parts. Typically, multiple reference images are needed to achieve such realistic generation, otherwise the generation would not faithfully preserve shape, texture, color, and background. In this work, we propose a pipeline that utilizes only a single input reference image -having varying lighting, background, object pose, and/or viewpoint. The singular reference image is used to generate multiple views of the object to be inpainted. We demonstrate that FaithFill produces faithful generation of the object's missing parts, together with background/scene preservation, from a single reference image. This is demonstrated through standard similarity metrics, human judgement, and GPT evaluation. Our results are presented on the DreamBooth dataset, and a novel proposed dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2269463142",
                    "name": "Rupayan Mallick"
                },
                {
                    "authorId": "2305777416",
                    "name": "Amr Abdalla"
                },
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                }
            ]
        },
        {
            "paperId": "5bb727f61796f0b24426c7a5d9e38aec6c4f147c",
            "title": "VisDA 2022 Challenge: Domain Adaptation for Industrial Waste Sorting",
            "abstract": "Label-efficient and reliable semantic segmentation is essential for many real-life applications, especially for industrial settings with high visual diversity, such as waste sorting. In industrial waste sorting, one of the biggest challenges is the extreme diversity of the input stream depending on factors like the location of the sorting facility, the equipment available in the facility, and the time of year, all of which significantly impact the composition and visual appearance of the waste stream. These changes in the data are called ``visual domains'', and label-efficient adaptation of models to such domains is needed for successful semantic segmentation of industrial waste. To test the abilities of computer vision models on this task, we present the VisDA 2022 Challenge on Domain Adaptation for Industrial Waste Sorting. Our challenge incorporates a fully-annotated waste sorting dataset, ZeroWaste, collected from two real material recovery facilities in different locations and seasons, as well as a novel procedurally generated synthetic waste sorting dataset, SynthWaste. In this competition, we aim to answer two questions: 1) can we leverage domain adaptation techniques to minimize the domain gap? and 2) can synthetic data augmentation improve performance on this task and help adapt to changing data distributions? The results of the competition show that industrial waste detection poses a real domain adaptation problem, that domain generalization techniques such as augmentations, ensembling, etc., improve the overall performance on the unlabeled target domain examples, and that leveraging synthetic data effectively remains an open problem. See https://ai.bu.edu/visda-2022/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50996059",
                    "name": "D. Bashkirova"
                },
                {
                    "authorId": "2112144917",
                    "name": "Samarth Mishra"
                },
                {
                    "authorId": "102639817",
                    "name": "Diala Lteif"
                },
                {
                    "authorId": "1388407541",
                    "name": "Piotr Teterwak"
                },
                {
                    "authorId": null,
                    "name": "Donghyun Kim"
                },
                {
                    "authorId": "116159782",
                    "name": "Fadi M. Alladkani"
                },
                {
                    "authorId": "115873160",
                    "name": "James Akl"
                },
                {
                    "authorId": "35131706",
                    "name": "B. \u00c7alli"
                },
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "2142735650",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "2111926793",
                    "name": "Daehan Kim"
                },
                {
                    "authorId": "2059726195",
                    "name": "Min-seok Seo"
                },
                {
                    "authorId": "2053743470",
                    "name": "Youngjin Jeon"
                },
                {
                    "authorId": "2854596",
                    "name": "Dong-geol Choi"
                },
                {
                    "authorId": "17127316",
                    "name": "S. Ettedgui"
                },
                {
                    "authorId": "2711839",
                    "name": "R. Giryes"
                },
                {
                    "authorId": "2047833125",
                    "name": "Shady Abu-Hussein"
                },
                {
                    "authorId": "1896719712",
                    "name": "Binhui Xie"
                },
                {
                    "authorId": "2165375715",
                    "name": "Shuangliang Li"
                }
            ]
        },
        {
            "paperId": "72bf601ed81e44e15bed5a5c359c43b6d4506078",
            "title": "Demo: Leveraging Edge Intelligence for Affective Communication over URLLC",
            "abstract": ".",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2070651214",
                    "name": "I. Amer"
                },
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "1777725",
                    "name": "Sharief M. A. Oteafy"
                },
                {
                    "authorId": "2238276755",
                    "name": "Hossam S. Hassanein"
                }
            ]
        },
        {
            "paperId": "8a570041fb80b01d0684e84c39c04434dc25a506",
            "title": "Practical Disruption of Image Translation Deepfake Networks",
            "abstract": "By harnessing the latest advances in deep learning, image-to-image translation architectures have recently achieved impressive capabilities. Unfortunately, the growing representational power of these architectures has prominent unethical uses. Among these, the threats of (1) face manipulation (\"DeepFakes\") used for misinformation or pornographic use (2) \"DeepNude\" manipulations of body images to remove clothes from individuals, etc. Several works tackle the task of disrupting such image translation networks by inserting imperceptible adversarial attacks into the input image. Nevertheless, these works have limitations that may result in disruptions that are not practical in the real world. Specifically, most works generate disruptions in a white-box scenario, assuming perfect knowledge about the image translation network. The few remaining works that assume a black-box scenario require a large number of queries to successfully disrupt the adversary's image translation network. In this work we propose Leaking Transferable Perturbations (LTP), an algorithm that significantly reduces the number of queries needed to disrupt an image translation network by dynamically re-purposing previous disruptions into new query efficient disruptions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31601235",
                    "name": "Nataniel Ruiz"
                },
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "3011497",
                    "name": "Cihang Xie"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                }
            ]
        },
        {
            "paperId": "aee0f3c37fb52117379e6df2ebe869745c74fcc6",
            "title": "Lasagna: Layered Score Distillation for Disentangled Object Relighting",
            "abstract": "Professional artists, photographers, and other visual content creators use object relighting to establish their photo's desired effect. Unfortunately, manual tools that allow relighting have a steep learning curve and are difficult to master. Although generative editing methods now enable some forms of image editing, relighting is still beyond today's capabilities; existing methods struggle to keep other aspects of the image -- colors, shapes, and textures -- consistent after the edit. We propose Lasagna, a method that enables intuitive text-guided relighting control. Lasagna learns a lighting prior by using score distillation sampling to distill the prior of a diffusion model, which has been finetuned on synthetic relighting data. To train Lasagna, we curate a new synthetic dataset ReLiT, which contains 3D object assets re-lit from multiple light source locations. Despite training on synthetic images, quantitative results show that Lasagna relights real-world images while preserving other aspects of the input image, outperforming state-of-the-art text-guided image editing methods. Lasagna enables realistic and controlled results on natural images and digital art pieces and is preferred by humans over other methods in over 91% of cases. Finally, we demonstrate the versatility of our learning objective by extending it to allow colorization, another form of image editing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50996059",
                    "name": "D. Bashkirova"
                },
                {
                    "authorId": "20686092",
                    "name": "Arijit Ray"
                },
                {
                    "authorId": "2269463142",
                    "name": "Rupayan Mallick"
                },
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "2273884561",
                    "name": "Jianming Zhang"
                },
                {
                    "authorId": "2257273968",
                    "name": "Ranjay Krishna"
                },
                {
                    "authorId": "2257237999",
                    "name": "Kate Saenko"
                }
            ]
        },
        {
            "paperId": "ccb65b9459a5016c0bd917848913a3c44c86a1d3",
            "title": "Hardwiring ViT Patch Selectivity into CNNs using Patch Mixing",
            "abstract": "Vision transformers (ViTs) have significantly changed the computer vision landscape and have periodically exhibited superior performance in vision tasks compared to convolutional neural networks (CNNs). Although the jury is still out on which model type is superior, each has unique inductive biases that shape their learning and generalization performance. For example, ViTs have interesting properties with respect to early layer non-local feature dependence, as well as self-attention mechanisms which enhance learning flexibility, enabling them to ignore out-of-context image information more effectively. We hypothesize that this power to ignore out-of-context information (which we name $\\textit{patch selectivity}$), while integrating in-context information in a non-local manner in early layers, allows ViTs to more easily handle occlusion. In this study, our aim is to see whether we can have CNNs $\\textit{simulate}$ this ability of patch selectivity by effectively hardwiring this inductive bias using Patch Mixing data augmentation, which consists of inserting patches from another image onto a training image and interpolating labels between the two image classes. Specifically, we use Patch Mixing to train state-of-the-art ViTs and CNNs, assessing its impact on their ability to ignore out-of-context patches and handle natural occlusions. We find that ViTs do not improve nor degrade when trained using Patch Mixing, but CNNs acquire new capabilities to ignore out-of-context information and improve on occlusion benchmarks, leaving us to conclude that this training method is a way of simulating in CNNs the abilities that ViTs already possess. We will release our Patch Mixing implementation and proposed datasets for public use. Project page: https://arielnlee.github.io/PatchMixing/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220131789",
                    "name": "Ariel N. Lee"
                },
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "2220964246",
                    "name": "Janavi Kasera"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                },
                {
                    "authorId": "2142735650",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "31601235",
                    "name": "Nataniel Ruiz"
                }
            ]
        },
        {
            "paperId": "00ce21a2772446a70bbc2ae9feaba49577b92fa3",
            "title": "Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing",
            "abstract": "Modern deep neural networks tend to be evaluated on static test sets. One shortcoming of this is the fact that these deep neural networks cannot be easily evaluated for robustness issues with respect to specific scene variations. For example, it is hard to study the robustness of these networks to variations of object scale, object pose, scene lighting and 3D occlusions. The main reason is that collecting real datasets with fine-grained naturalistic variations of sufficient scale can be extremely time-consuming and expensive. In this work, we present Counterfactual Simulation Testing, a counterfactual framework that allows us to study the robustness of neural networks with respect to some of these naturalistic variations by building realistic synthetic scenes that allow us to ask counterfactual questions to the models, ultimately providing answers to questions such as\"Would your classification still be correct if the object were viewed from the top?\"or\"Would your classification still be correct if the object were partially occluded by another object?\". Our method allows for a fair comparison of the robustness of recently released, state-of-the-art Convolutional Neural Networks and Vision Transformers, with respect to these naturalistic variations. We find evidence that ConvNext is more robust to pose and scale variations than Swin, that ConvNext generalizes better to our simulated domain and that Swin handles partial occlusion better than ConvNext. We also find that robustness for all networks improves with network scale and with data scale and variety. We release the Naturalistic Variation Object Dataset (NVD), a large simulated dataset of 272k images of everyday objects with naturalistic variations such as object pose, scale, viewpoint, lighting and occlusions. Project page: https://counterfactualsimulation.github.io",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31601235",
                    "name": "Nataniel Ruiz"
                },
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "3011497",
                    "name": "Cihang Xie"
                },
                {
                    "authorId": "2142735650",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                }
            ]
        },
        {
            "paperId": "b48c0c4490f257b268fde19d74c73596ad0e361e",
            "title": "Temporal Relevance Analysis for Video Action Models",
            "abstract": "In this paper, we provide a deep analysis of temporal modeling for action recognition, an important but underexplored problem in the literature. We first propose a new approach to quantify the temporal relationships between frames captured by CNN-based action models based on layer-wise relevance propagation. We then conduct comprehensive experiments and in-depth analysis to provide a better understanding of how temporal modeling is affected by various factors such as dataset, network architecture, and input frames. With this, we further study some important questions for action recognition that lead to interesting findings. Our analysis shows that there is no strong correlation between temporal relevance and model performance; and action models tend to capture local temporal information, but less long-range dependencies. Our codes and models will be publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33421444",
                    "name": "Quanfu Fan"
                },
                {
                    "authorId": null,
                    "name": "Donghyun Kim"
                },
                {
                    "authorId": "48239920",
                    "name": "Chun-Fu Chen"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                },
                {
                    "authorId": "2142735650",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                }
            ]
        },
        {
            "paperId": "1f5ec5c5bc69ee850d31d70281322e8026c5bd52",
            "title": "Simulated Adversarial Testing of Face Recognition Models",
            "abstract": "Most machine learning models are validated and tested on fixed datasets. This can give an incomplete picture of the capabilities and weaknesses of the model. Such weaknesses can be revealed at test time in the real world. The risks involved in such failures can be loss of profits, loss of time or even loss of life in certain critical applications. In order to alleviate this issue, simulators can be controlled in a finegrained manner using interpretable parameters to explore the semantic image manifold. In this work, we propose a framework for learning how to test machine learning algorithms using simulators in an adversarial manner in order to find weaknesses in the model before deploying it in critical scenarios. We apply this method in a face recognition setup. We show that certain weaknesses of models trained on real data can be discovered using simulated samples. Using our proposed method, we can find adversarial synthetic faces that fool contemporary face recognition models. This demonstrates the fact that these models have weaknesses that are not measured by commonly used validation datasets. We hypothesize that this type of adversarial examples are not isolated, but usually lie in connected spaces in the latent space of the simulator. We present a method to find these adversarial regions as opposed to the typical adversarial points found in the adversarial example literature.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31601235",
                    "name": "Nataniel Ruiz"
                },
                {
                    "authorId": "2780587",
                    "name": "Adam Kortylewski"
                },
                {
                    "authorId": "3256056",
                    "name": "Weichao Qiu"
                },
                {
                    "authorId": "3011497",
                    "name": "Cihang Xie"
                },
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "145081362",
                    "name": "A. Yuille"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                }
            ]
        },
        {
            "paperId": "21e4fdd6cfb8d5448a80340677d9b6b3205296d1",
            "title": "Guided Zoom: Zooming into Network Evidence to Refine Fine-Grained Model Decisions",
            "abstract": "In state-of-the-art deep single-label classification models, the top-<inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"bargal-ieq1-3054303.gif\"/></alternatives></inline-formula> <inline-formula><tex-math notation=\"LaTeX\">$(k=2,3,4, \\dots)$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mo>\u22ef</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"bargal-ieq2-3054303.gif\"/></alternatives></inline-formula> accuracy is usually significantly higher than the top-1 accuracy. This is more evident in fine-grained datasets, where differences between classes are quite subtle. Exploiting the information provided in the top <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"bargal-ieq3-3054303.gif\"/></alternatives></inline-formula> predicted classes boosts the final prediction of a model. We propose Guided Zoom, a novel way in which explainability could be used to improve model performance. We do so by making sure the model has \u201cthe right reasons\u201d for a prediction. The reason/evidence upon which a deep neural network makes a prediction is defined to be the grounding, in the pixel space, for a specific class conditional probability in the model output. Guided Zoom examines how reasonable the evidence used to make each of the top-<inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"bargal-ieq4-3054303.gif\"/></alternatives></inline-formula> predictions is. Test time evidence is deemed reasonable if it is coherent with evidence used to make similar correct decisions at training time. This leads to better informed predictions. We explore a variety of grounding techniques and study their complementarity for computing evidence. We show that Guided Zoom results in an improvement of a model's classification accuracy and achieves state-of-the-art classification performance on four fine-grained classification datasets. Our code is available at <uri>https://github.com/andreazuna89/Guided-Zoom</uri>.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3298267",
                    "name": "Sarah Adel Bargal"
                },
                {
                    "authorId": "144733334",
                    "name": "Andrea Zunino"
                },
                {
                    "authorId": "50980023",
                    "name": "Vitali Petsiuk"
                },
                {
                    "authorId": "1519062024",
                    "name": "Jianming Zhang"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "1727204",
                    "name": "Vittorio Murino"
                },
                {
                    "authorId": "1749590",
                    "name": "S. Sclaroff"
                }
            ]
        }
    ]
}