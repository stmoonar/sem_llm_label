{
    "authorId": "48925358",
    "papers": [
        {
            "paperId": "827141190cb383e6ecf8d3b11fd26bb6747e3b62",
            "title": "Chride at SemEval-2023 Task 10: Fine-tuned Deberta-V3 on Detection of Online Sexism with Hierarchical Loss",
            "abstract": "Sexism is one of the most concerning problems in the internet society. By detecting sexist expressions, we can reduce the offense toward females and provide useful information to understand how sexism occurs. Our work focuses on a newly-published dataset, EDOS, which annotates English sexist expressions from Reddit and categorizes their specific types. Our method is to train a DeBERTaV3 classifier with all three kinds of labels provided by the dataset, including sexist, category, and granular vectors. Our classifier predicts the probability distribution on vector labels and further applies it to represent category and sexist distributions. Our classifier uses its label and finer-grained labels for each classification to calculate the hierarchical loss for optimization. Our experiments and analyses show that using a combination of loss with finer-grained labels generally achieves better performance on sexism detection and categorization. Codes for our implementation can be found at https://github.com/KomeijiForce/SemEval2023_Task10.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144004388",
                    "name": "Letian Peng"
                },
                {
                    "authorId": "48925358",
                    "name": "Bosung Kim"
                }
            ]
        },
        {
            "paperId": "c8ccb9ff52180cf8f03bb39ee131ccf0624f7874",
            "title": "Data Augmentation for Rare Symptoms in Vaccine Side-Effect Detection",
            "abstract": "We study the problem of entity detection and normalization applied to patient self-reports of symptoms that arise as side-effects of vaccines. Our application domain presents unique challenges that render traditional classification methods ineffective: the number of entity types is large; and many symptoms are rare, resulting in a long-tail distribution of training examples per entity type. We tackle these challenges with an autoregressive model that generates standardized names of symptoms. We introduce a data augmentation technique to increase the number of training examples for rare symptoms. Experiments on real-life patient vaccine symptom self-reports show that our approach outperforms strong baselines, and that additional examples improve performance on the long-tail entities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48925358",
                    "name": "Bosung Kim"
                },
                {
                    "authorId": "3115592",
                    "name": "Ndapandula Nakashole"
                }
            ]
        },
        {
            "paperId": "f29e5a78378bd3d8ae8ec7caecf564fe3701c1dd",
            "title": "Zero-shot Triplet Extraction by Template Infilling",
            "abstract": "The task of triplet extraction aims to extract pairs of entities and their corresponding relations from unstructured text. Most existing methods train an extraction model on training data involving specific target relations, and are incapable of extracting new relations that were not observed at training time. Generalizing the model to unseen relations typically requires fine-tuning on synthetic training data which is often noisy and unreliable. We show that by reducing triplet extraction to a template infilling task over a pre-trained language model (LM), we can equip the extraction model with zero-shot learning capabilities and eliminate the need for additional training data. We propose a novel framework, ZETT (ZEro-shot Triplet extraction by Template infilling), that aligns the task objective to the pre-training objective of generative transformers to generalize to unseen relations. Experiments on FewRel and Wiki-ZSL datasets demonstrate that ZETT shows consistent and stable performance, outperforming previous state-of-the-art methods, even when using automatically generated templates. https://github.com/megagonlabs/zett/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48925358",
                    "name": "Bosung Kim"
                },
                {
                    "authorId": "7782351",
                    "name": "Hayate Iso"
                },
                {
                    "authorId": "29995869",
                    "name": "Nikita Bhutani"
                },
                {
                    "authorId": "1842532",
                    "name": "Estevam Hruschka"
                },
                {
                    "authorId": "3115592",
                    "name": "Ndapandula Nakashole"
                }
            ]
        },
        {
            "paperId": "2f5229270917744faeea826aedaf154974901b63",
            "title": "Query Reformulation for Descriptive Queries of Jargon Words Using a Knowledge Graph based on a Dictionary",
            "abstract": "Query reformulation (QR) is a key factor in overcoming the problems faced by the lexical chasm in information retrieval (IR) systems. In particular, when searching for jargon, people tend to use descriptive queries, such as \"a medical examination of the colon\" rather than \"colonoscopy,\" or they often use them interchangeably. Thus, transforming users' descriptive queries into appropriate jargon queries helps to retrieve more relevant documents. In this paper, we propose a new graph-based QR system that uses a dictionary, where the model does not require human-labeled data. Given a descriptive query, our system predicts the corresponding jargon word over a graph consisting of pairs of a headword and its description in the dictionary. First, we train a graph neural network to represent the relational properties between words and to infer a jargon word using compositional information of the descriptive query's words. Moreover, we propose a graph search model that finds the target node in real time using the relevance scores of neighborhood nodes. By adding this fast graph search model to the front of the proposed system, we reduce the reformulating time significantly. Experimental results on two datasets show that the proposed method can effectively reformulate descriptive queries to corresponding jargon words as well as improve retrieval performance under several search frameworks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48925358",
                    "name": "Bosung Kim"
                },
                {
                    "authorId": "73709728",
                    "name": "H. Choi"
                },
                {
                    "authorId": "2148634949",
                    "name": "Haeun Yu"
                },
                {
                    "authorId": "37991450",
                    "name": "Youngjoong Ko"
                }
            ]
        },
        {
            "paperId": "efd2fdb0c1e38129c72168a64c2426f10489d65b",
            "title": "Commonsense Knowledge Augmentation for Low-Resource Languages via Adversarial Learning",
            "abstract": "Commonsense reasoning is one of the ultimate goals of artificial intelligence research because it simulates the human thinking process. However, most commonsense reasoning studies have focused on English because available commonsense knowledge for low-resource languages is scarce due to high construction costs. Translation is one of the typical methods for augmenting data for low-resource languages; however, translation entails ambiguity problems, where one word can be translated into multiple words due to polysemes and homonyms. Previous studies have suggested methods to measure the validity of translated multiple triples by using additional metadata and manually labeled data. However, such handcrafted datasets are not available for many low-resource languages. In this paper, we propose a knowledge augmentation method using adversarial networks that does not require any labeled data. Our adversarial networks can transfer knowledge learned from a resource-rich language to low-resource languages and thus measure the validity score of translated triples even without labeled data. We designed experiments to demonstrate that high-scoring triples obtained by the proposed model can be considered augmented knowledge. The experimental results show that our proposed method for a low-resource language, Korean, achieved 93.7% precision@1 on a manually labeled benchmark. Furthermore, to verify our model for other low-resource languages, we introduced new test sets for knowledge validation in 16 different languages. Our adversarial model obtains strong results for all language test sets. We will release the augmented Korean knowledge and test sets for 16 languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48925358",
                    "name": "Bosung Kim"
                },
                {
                    "authorId": "2109974526",
                    "name": "Juae Kim"
                },
                {
                    "authorId": "37991450",
                    "name": "Youngjoong Ko"
                },
                {
                    "authorId": "1785259",
                    "name": "Jungyun Seo"
                }
            ]
        },
        {
            "paperId": "b1e6aa78db5478be5eaa47697382241c2b7aab1f",
            "title": "Multi-Task Learning for Knowledge Graph Completion with Pre-trained Language Models",
            "abstract": "As research on utilizing human knowledge in natural language processing has attracted considerable attention in recent years, knowledge graph (KG) completion has come into the spotlight. Recently, a new knowledge graph completion method using a pre-trained language model, such as KG-BERT, is presented and showed high performance. However, its scores in ranking metrics such as Hits@k are still behind state-of-the-art models. We claim that there are two main reasons: 1) failure in sufficiently learning relational information in knowledge graphs, and 2) difficulty in picking out the correct answer from lexically similar candidates. In this paper, we propose an effective multi-task learning method to overcome the limitations of previous works. By combining relation prediction and relevance ranking tasks with our target link prediction, the proposed model can learn more relational properties in KGs and properly perform even when lexical similarity occurs. Experimental results show that we not only largely improve the ranking performances compared to KG-BERT but also achieve the state-of-the-art performances in Mean Rank and Hits@10 on the WN18RR dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48925358",
                    "name": "Bosung Kim"
                },
                {
                    "authorId": "2029660343",
                    "name": "Taesuk Hong"
                },
                {
                    "authorId": "37991450",
                    "name": "Youngjoong Ko"
                },
                {
                    "authorId": "1785259",
                    "name": "Jungyun Seo"
                }
            ]
        },
        {
            "paperId": "f8ee9c3a284c6cc5403b0c190411216d03748e4f",
            "title": "An Effective Sentence Similarity Measure Method Based FAQ System Using Self-Attentive Sentence Embedding",
            "abstract": "FAQ(Frequently Asked Question)\ub294 \ud2b9\uc815 \ub3c4\uba54\uc778 \ubd84\uc57c \uc5d0\uc11c \uc0ac\uc6a9\uc790\uac00 \uc790\uc8fc \ubb3b\ub294 \uc9c8\ubb38\uacfc \uc774\uc5d0 \ub300\ud55c \uc751\ub2f5\uc758 \ubaa8\uc74c \uc744 \ub9d0\ud55c\ub2e4. FAQ \uc2dc\uc2a4\ud15c\uc740 \uc9c8\uc758\uc751\ub2f5(Question Answering) \uc2dc\uc2a4\ud15c\uc73c\ub85c \ubd84\ub958\ub418\uc9c0\ub9cc, \uc9c0\uc2dd\uae30\ubc18 \uc9c8\uc758\uc751\ub2f5(Knowledge Based Question Answering), \ub3c5\ud574 \uc9c8\uc758\uc751\ub2f5(Reading Comprehension Question Answering) \uac19\uc740 QA \uc2dc\uc2a4\ud15c\uacfc\ub294 \ub2ec\ub9ac \uc9c8\ubb38\uc774 \uc8fc\uc5b4\uc84c\uc744 \ub54c \uae30\uc874\uc5d0 \uc874\uc7ac\ud558\ub294 FAQ \uc911 \uac00\uc7a5 \uc720\uc0ac\ud55c \uc9c8\uc758\ub97c \ucc3e\uc544 \uc774\uc5d0 \ub300\ud55c \ub2f5\ubcc0\uc744 \uc81c\uacf5\ud558\ub294 \uc77c\uc885\uc758 \ubb38\uc11c \uac80\uc0c9 \uc2dc\uc2a4\ud15c\uc774\ub2e4. \uc815\ubcf4\uac80\uc0c9\uc5d0\uc11c\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \ubb38\uc11c \ub0b4 \ub2e8\uc5b4\uc758 \uac00\uc911\uce58 \ubca1 \ud130\ub85c \ubb38\uc11c\ub97c \ud45c\ud604\ud558\uba70[1], TFIDF, Okapi BM25 \ub4f1\uc758 \uac00\uc911 \uce58 \uacc4\uc0b0\ubc95\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4[2,3]. \ud558\uc9c0\ub9cc \uc774 \ubc29\ubc95\uc740 \uc758 \ubbf8\ub294 \uac19\uc9c0\ub9cc \ud615\ud0dc\ub294 \ub2e4\ub978 \ub2e8\uc5b4 \uac04\uc5d0\ub294 \uc720\uc0ac\ub3c4\ub97c \uce21\uc815\ud560 \uc218 \uc5c6\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \u2018\ub124\ube44\uac8c\uc774\uc158\u2019\uacfc \u2018\ub0b4\ube44\uac8c\uc774\uc158\u2019 \ub450 \ub2e8\uc5b4\ub294 \uc758\ubbf8\uc0c1 \ub3d9\uc77c\ud558\uc9c0\ub9cc \ubcc4\uac1c\ub85c \uc0c9 \uc778(index)\ub418\uc5b4 \ubca1\ud130\uac04 \uc720\uc0ac\ub3c4 \uacc4\uc0b0 \uc2dc \ub450 \ub2e8\uc5b4\uc758 \uad00\ub828\uc131 \uc774 \ubc18\uc601\ub418\uc9c0 \uc54a\ub294\ub2e4. \ub530\ub77c\uc11c \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 \ub2e8\uc5b4 \uc0c9\uc778 (word index)\uc758 \uc6d0\uc790\uc131(atomicity)\uc73c\ub85c \uc778\ud55c \ubb38\uc81c\uc810\uc744 \ubcf4\uc644\ud558\uace0\uc790 \ub525\ub7ec\ub2dd \uae30\ubc18\uc758 \ubb38\uc7a5 \uc784\ubca0\ub529(sentence embedding)\uc744 \uad6c\ucd95\ud558\uace0, \ub2e8\uc5b4 \uac00\uc911\uce58 \ubca1\ud130\uc640 \ubb38\uc7a5 \uc784\ubca0 \ub529\uc744 \uc870\ud569\ud55c \ubb38\uc7a5 \uc720\uc0ac\ub3c4 \uacc4\uc0b0 \ubaa8\ub378\uc744 \uc81c\uc548\ud55c\ub2e4. \ucd5c\uadfc \ub525\ub7ec\ub2dd\uc744 \uc774\uc6a9\ud558\uc5ec \ubb38\uc7a5\uc744 \ud45c\ud604\ud558\ub294 \ub2e4\uc591\ud55c \ubc29\ubc95 \uc774 \uc81c\uc2dc\ub418\uc5c8\ub2e4[4, 5]. \ubcf8 \ub17c\ubb38\uc5d0\uc11c\ub294 \ub450 \uac00\uc9c0 \ubb38\uc7a5 \uc784\ubca0 \ub529 \ubc29\ubc95\uc73c\ub85c \uc2e4\ud5d8\uc744 \uc9c4\ud589\ud558\uc600\ub2e4. \uba3c\uc800 \ub300\ub7c9\uc758 \ucf54\ud37c\uc2a4\ub85c \ubbf8\ub9ac \ud559\uc2b5\ub41c \ub2e8\uc5b4 \uc784\ubca0\ub529(word embedding)[6]\uc744 \uc774\uc6a9\ud558 \uc5ec, \ubb38\uc7a5 \ub0b4 \ubaa8\ub4e0 \ub2e8\uc5b4\uc758 \uc784\ubca0\ub529\uc744 \ud3c9\uade0\ud55c \ubca1\ud130\ub85c \ubb38\uc7a5 \uc744 \ud45c\ud604\ud558\uc600\ub2e4. \ub450 \ubc88\uc9f8\ub85c\ub294 \ubb38\uc7a5 \ub0b4 \ub2e8\uc5b4 \uc784\ubca0\ub529\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c4 \ud589\ub82c\uc5d0 self-attention[7] \uae30\ubc95\uc744 \uc801\uc6a9\ud55c \ud6c4 \ud3c9\uade0\ud558\uc5ec \ubb38\uc7a5 \ubca1\ud130\ub97c \uc0dd\uc131\ud558\uc600\ub2e4. \ub450 \ubc29\ubc95 \ubaa8\ub450 TFIDF \uae30\ubc18 \uc720\uc0ac\ub3c4 \uac80\uc0c9\uc5d0 \ube44\ud574 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc788\uc5c8\uc73c\uba70, selfattention\uc744 \uc801\uc6a9\ud55c \ubb38\uc7a5 \uc784\ubca0\ub529 \uae30\ubc18\uc758 \uc720\uc0ac\ub3c4 \uac80\uc0c9\uc774 \uac00\uc7a5 \ub192\uc740 \uc131\ub2a5\uc744 \ubcf4\uc600\ub2e4. \uc2e4\ud5d8\uc5d0\uc11c\ub294 \uc790\ub3d9\ucc28 \uad00\ub828 \ubd84\uc57c\uc758 FAQ \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud558 \uc600\uace0, 559\uac1c\uc758 \uac80\uc0c9 \ub300\uc0c1 \uc9c8\uc758\uc751\ub2f5 \uc30d \ub370\uc774\ud130\uc640 169\uac1c\uc758 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc5d0 \ub300\ud574 Accuracy@5(\uc0c1\uc704 5\uac1c\uc758 \uac80\uc0c9 \uc815\ud655 \ub3c4)\uc640 Accuracy@1(\uc0c1\uc704 1\uac1c\uc758 \uac80\uc0c9 \uc815\ud655\ub3c4)\uc744 \uce21\uc815\ud558\uc600 \ub2e4. \uc2e4\ud5d8 \uacb0\uacfc TFIDF \uae30\ubc18 \uc720\uc0ac\ub3c4 \uac80\uc0c9 \ubaa8\ub378\uacfc \ube44\uad50\ud558\uc5ec \ubb38\uc7a5 \uc784\ubca0\ub529\uc744 \ucd94\uac00\ud55c \uacbd\uc6b0 Accuracy@1\uc5d0\uc11c \uc57d 5.32% \uc131 \ub2a5\uc774 \ud5a5\uc0c1\ub418\uc5c8\uace0, self-attention\uc744 \uc801\uc6a9\ud55c \uacb0\uacfc \ucd94\uac00\uc801 \uc778 \uc131\ub2a5 \ud5a5\uc0c1\uc774 \uc788\uc5c8\ub2e4.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48925358",
                    "name": "Bosung Kim"
                },
                {
                    "authorId": "2109974526",
                    "name": "Juae Kim"
                },
                {
                    "authorId": "2146055400",
                    "name": "Jeong-Eom Lee"
                },
                {
                    "authorId": "2109589366",
                    "name": "Seona Kim"
                },
                {
                    "authorId": "37991450",
                    "name": "Youngjoong Ko"
                },
                {
                    "authorId": "1785259",
                    "name": "Jungyun Seo"
                }
            ]
        },
        {
            "paperId": "b01c966cc6cd6492e7980992e1646b367e8ca1c4",
            "title": "Implementation of Parallel Local Alignment Method for DNA Sequence using Apache Spark",
            "abstract": "Smith-Waterman(SW) \uc54c\uace0\ub9ac\uc998\uc740 DNA \uc2dc\ud000\uc2a4 \ubd84\uc11d\uc5d0\uc11c \uc911\uc694\ud55c \uc5f0\uc0b0 \uc911 \ud558\ub098\uc778 \uc9c0\uc5ed \uc815\ub82c\uc744 \ucc98\ub9ac\ud558 \ub294 \uc54c\uace0\ub9ac\uc998\uc774\ub2e4. SW \uc54c\uace0\ub9ac\uc998\uc740 \ub3d9\uc801 \ud504\ub85c\uadf8\ub798\ubc0d \ubc29\ubc95\uc73c\ub85c \ucd5c\uc801\uc758 \uacb0\uacfc\ub97c \ub3c4\ucd9c\ud560 \uc218 \uc788\uc9c0\ub9cc \uc218\ud589\uc2dc\uac04\uc774 \ub9e4\uc6b0 \uae38\ub2e4\ub294 \ubb38\uc81c\uac00 \uc788\ub2e4. \uc774\ub97c \ud574\uacb0\ud558\uae30 \uc704\ud574\uc11c \ub2e4\uc218\uc758 \ub178\ub4dc\ub97c \uc774\uc6a9\ud55c \ubcd1\ub82c \ubd84\uc0b0 \ucc98\ub9ac \uae30\ubc18\uc758 SW \uc54c\uace0\ub9ac \uc998\uc774 \uc81c\uc548\ub418\uc5c8\ub2e4. Apache Spark\uc744 \uae30\ubc18\uc73c\ub85c \ud558\ub294 \ubcd1\ub82c \ubd84\uc0b0 DNA \ucc98\ub9ac \ud504\ub808\uc784\uc6cc\ud06c\uc778 ADAM\uc5d0\uc11c\ub3c4 SW \uc54c\uace0\ub9ac\uc998\uc744 \ubcd1\ub82c\ub85c \ucc98\ub9ac\ud558\uace0 \uc788\ub2e4. \ud558\uc9c0\ub9cc, ADAM\uc758 SW \uc54c\uace0\ub9ac\uc998\uc740 Smith-Waterman \uc774 \ub3d9\uc801\ud504\ub85c\uadf8\ub798 \ubc0d \uae30\ubc95\uc774\ub77c\ub294 \ud2b9\uc131\uc744 \uace0\ub824\ud558\uc9c0 \uc54a\uace0 \uc788\uc5b4 \ucd5c\ub300\uc758 \uc131\ub2a5\uc744 \uc5bb\uc9c0 \ubabb\ud558\uace0 \uc788\ub2e4. \uc774 \ub17c\ubb38\uc5d0\uc11c\ub294 ADAM\uc758 \ubcd1\ub82c SW \uc54c\uace0\ub9ac\uc998\uc744 \uac1c\uc120\ud55c\ub2e4. \uc81c\uc548\ud558\ub294 \ubcd1\ub82c SW \uae30\ubc95\uc740 \ub450 \ub2e8\uacc4\uc5d0 \uac78\uccd0 \uc2e4\ud589\ub41c\ub2e4. \uccab \ubc88\uc9f8 \ub2e8\uacc4\uc5d0\uc11c\ub294 \uc9c0\uc5ed \uc815\ub82c \ub300\uc0c1\uc778 DNA \uc2dc\ud000\uc2a4\ub97c \ub2e4\uc218\uc758 \ud30c\ud2f0\uc158(partition)\uc73c\ub85c \ubd84\ud560\ud558\uace0 \ubd84\ud560\ub41c \uac01 \ud30c\ud2f0\uc158\uc5d0 \ub300\ud574\uc11c SW \uc54c\uace0 \ub9ac\uc998\uc744 \ubcd1\ub82c\ub85c \uc218\ud589\ud55c\ub2e4. \ub450 \ubc88\uc9f8 \ub2e8\uacc4\uc5d0\uc11c\ub294 \ud30c\ud2f0\uc158 \uac01\uac01\uc5d0 \ub300\ud574\uc11c \ub3c5\ub9bd\uc801\uc73c\ub85c SW\ub97c \uc801\uc6a9\ud568\uc73c\ub85c\uc368 \ubc1c\uc0dd \ud558\ub294 \uc624\ub958\ub97c \ubcf4\uc644\ud558\ub294 \uacfc\uc815\uc744 \uc5ed\uc2dc \ubcd1\ub82c\ub85c \uc218\ud589\ud55c\ub2e4. \uc81c\uc548\ud558\ub294 \ubcd1\ub82c SW \uc54c\uace0\ub9ac\uc998\uc740 ADAM\uc744 \uae30\ubc18\uc73c\ub85c \uad6c\ud604\ud558\uace0 \uae30\uc874 ADAM\uc758 SW\uc640 \ube44\uad50\ub97c \ud1b5\ud574\uc11c \uc131\ub2a5\uc744 \uc785\uc99d\ud55c\ub2e4. \uc131\ub2a5 \ud3c9\uac00 \uacb0\uacfc \uc81c\uc548\ud558\ub294 \ubcd1\ub82c SW \uc54c\uace0 \ub9ac\uc998\uc774 \uae30\uc874\uc758 SW\uc5d0 \ube44\ud574\uc11c 2\ubc30 \uc774\uc0c1\uc758 \uc88b\uc740 \uc131\ub2a5\uc744 \ub0b4\ub294 \uac83\uc744 \ud655\uc778\ud558\uc600\ub2e4.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48925358",
                    "name": "Bosung Kim"
                },
                {
                    "authorId": "2117157212",
                    "name": "Jinsu Kim"
                },
                {
                    "authorId": "2422171",
                    "name": "Dojin Choi"
                },
                {
                    "authorId": "2141972454",
                    "name": "S. Kim"
                },
                {
                    "authorId": "1703191",
                    "name": "Seokil Song"
                }
            ]
        },
        {
            "paperId": "27e947d1a606d003ab3c9e1acd7913323b0c979d",
            "title": "Ultrasound-Enhanced Multimodal Approaches to Pronunciation Teaching and Learning",
            "abstract": "Second language (L2) pronunciation is one of the most challenging skills to master for adult learners. Accented pronunciation is part of the expression of speakers\u2019 identity, but it could potentially give issues in comprehensibility. Explicit pronunciation instruction from language instructors is often unavailable due to limited class time. Imitating native speakers\u2019 utterances can be done independently from classroom learning, but the absence of feedback makes it difficult for learners to improve their skills. This project takes a multidisciplinary, multimodal approach to pronunciation teaching and learning through a series of video resources, available at http://enunciate.arts.ubc.ca/ . These videos combine external images of a speaker\u2019s head with ultrasound images of their tongue to demonstrate the pronunciation of various sounds. In addition to examples of sounds in isolation, a strong focus to this point has been on the pronunciation of Japanese sounds, with pronunciation instruction incorporating explicit awareness of tongue movements and insights from articulatory phonology. Further stages of the project will include real-time interactive ultrasound tongue visualization and comparative prosody visualization, both of which provide biovisual feedback to L2 learners.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26478267",
                    "name": "Jennifer Abel"
                },
                {
                    "authorId": "67248362",
                    "name": "Blake Allen"
                },
                {
                    "authorId": "72049090",
                    "name": "Strang Burton"
                },
                {
                    "authorId": "70562936",
                    "name": "Misuzu Kazama"
                },
                {
                    "authorId": "48925358",
                    "name": "Bosung Kim"
                },
                {
                    "authorId": "2494829",
                    "name": "Masaki Noguchi"
                },
                {
                    "authorId": "70292648",
                    "name": "Asami Tsuda"
                },
                {
                    "authorId": "49284375",
                    "name": "Noriko Yamane"
                },
                {
                    "authorId": "2160324",
                    "name": "B. Gick"
                }
            ]
        },
        {
            "paperId": "317a649cdb63cb28eb7c3574ea522fd01fd800bd",
            "title": "Learning about problem based learning: Student teachers integrating technology, pedagogy and content knowledge",
            "abstract": "What should constitute knowledge bases that we expect our future teachers to gain related to pedagogically sound technology integration? Employing the Shulman's teacher knowledge base as a theoretical lens, this study examined the complexity of pre-service teachers' technological pedagogical content knowledge (TPCK) in the context of integrating problem based learning (PBL) and information and communications technology (ICT). Ninety-seven pre-service teachers in this study engaged in a collaborative lesson design project where they applied pedagogical knowledge about PBL to design a technology integrated lesson in their subject area of teaching. Data were collected from two sources: survey and lesson design artifacts. Data analyses revealed that while participants had theoretical understandings of pedagogical knowledge about PBL, their lesson designs showed a mismatch among technology tools, content representations, and pedagogical strategies, indicating conflicts in translating pedagogical content knowledge into designing pedagogically sound, technology integrated lessons. The areas that students perceived to be particularly challenging and difficult include: a) generating authentic and ill-structured problems for a chosen content topic, b) finding and integrating ICT tools and resources relevant for the target students and learning activities, and c) designing tasks with a balance between teacher guidance and student independence. The present study suggests the potential of two explanations for such difficulties: lack of intimate connection among beliefs, knowledge, and actions, and insufficient repertoires for teaching with technology for problem based learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38494732",
                    "name": "H. So"
                },
                {
                    "authorId": "48925358",
                    "name": "Bosung Kim"
                }
            ]
        }
    ]
}