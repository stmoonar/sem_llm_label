{
    "authorId": "48570355",
    "papers": [
        {
            "paperId": "8ceddb1ca62f6ddf3e4999c85c19410016af9585",
            "title": "TreeMAN: Tree-enhanced Multimodal Attention Network for ICD Coding",
            "abstract": "ICD coding is designed to assign the disease codes to electronic health records (EHRs) upon discharge, which is crucial for billing and clinical statistics. In an attempt to improve the effectiveness and efficiency of manual coding, many methods have been proposed to automatically predict ICD codes from clinical notes. However, most previous works ignore the decisive information contained in structured medical data in EHRs, which is hard to be captured from the noisy clinical notes. In this paper, we propose a Tree-enhanced Multimodal Attention Network (TreeMAN) to fuse tabular features and textual features into multimodal representations by enhancing the text representations with tree-based features via the attention mechanism. Tree-based features are constructed according to decision trees learned from structured multimodal medical data, which capture the decisive information about ICD coding. We can apply the same multi-label classifier from previous text models to the multimodal representations to predict ICD codes. Experiments on two MIMIC datasets show that our method outperforms prior state-of-the-art ICD coding approaches. The code is available at https://github.com/liu-zichen/TreeMAN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117942578",
                    "name": "Zichen Liu"
                },
                {
                    "authorId": "2108736365",
                    "name": "Xuyuan Liu"
                },
                {
                    "authorId": "2311315",
                    "name": "Yanlong Wen"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "48570355",
                    "name": "Fen Xia"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "f3dcfb082b46643acf29983618c10fbf4dfc54b6",
            "title": "Get a Head Start: Targeted Labeling at Source with Limited Annotation Overhead for Semi-Supervised Learning",
            "abstract": "Semi-supervised learning (SSL), which leverages limited labeled data and a large amount of unlabeled data for model training, has been widely studied to mitigate the requirement for expensive and time-consuming annotations. Recently proposed methods have achieved promising yet unstable results, which presume that initial samples are randomly selected and labeled. For improving the fluctuated performance while saving annotation overhead, effective prior labeling for SSL on the source cluttered unlabeled dataset is challenging but significant. In this paper, we propose a novel criterion and a distribution balance strategy to automatically achieve targeted labeling without access to the test set and any labels. Comprehensive experiments are conducted on commonly-used datasets to demonstrate the effectiveness of our method. Furthermore, targeted labeling is orthogonal to existing framework-centric SSL methods and can achieve state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115717093",
                    "name": "Hui Zhu"
                },
                {
                    "authorId": "152211894",
                    "name": "Yongchun L\u00fc"
                },
                {
                    "authorId": "2233755060",
                    "name": "Qin Ma"
                },
                {
                    "authorId": "2041706708",
                    "name": "Xunyi Zhou"
                },
                {
                    "authorId": "48570355",
                    "name": "Fen Xia"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "2182908718",
                    "name": "Ning Jiang"
                },
                {
                    "authorId": "9272754",
                    "name": "Xiaofang Zhao"
                }
            ]
        },
        {
            "paperId": "f7f58e7be576f548d317a08e588e992720ab4502",
            "title": "Why Dataset Properties Bound the Scalability of Parallel Machine Learning Training Algorithms",
            "abstract": "As the training dataset size and the model size of machine learning increase rapidly, more computing resources are consumed to speedup the training process. However, the scalability and performance reproducibility of parallel machine learning training, which mainly uses stochastic optimization algorithms, are limited. In this paper, we demonstrate that the sample difference in the dataset plays a prominent role in the scalability of parallel machine learning algorithms. We propose to use statistical properties of dataset to measure sample differences. These properties include the variance of sample features, sample sparsity, sample diversity, and similarity in sampling sequences. We choose four types of parallel training algorithms as our research objects: (1) the asynchronous parallel SGD algorithm (Hogwild! algorithm), (2) the parallel model average SGD algorithm (minibatch SGD algorithm), (3) the decentralization optimization algorithm, and (4) the dual coordinate optimization (DADM algorithm). Our results show that the statistical properties of training datasets determine the scalability upper bound of these parallel training algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2946656",
                    "name": "Daning Cheng"
                },
                {
                    "authorId": "2298884755",
                    "name": "Shigang Li"
                },
                {
                    "authorId": "2051537017",
                    "name": "Hanping Zhang"
                },
                {
                    "authorId": "48570355",
                    "name": "Fen Xia"
                },
                {
                    "authorId": "2108032102",
                    "name": "Yunquan Zhang"
                }
            ]
        },
        {
            "paperId": "9f8e3d4cd8b981190ffbc33c7b23212288292712",
            "title": "Using Gradient Based Multikernel Gaussian Process and Meta-Acquisition Function to Accelerate SMBO",
            "abstract": "Automatic machine learning (automl) is a crucial technology in machine learning. Sequential model-based optimisation algorithms (SMBO) (e.g., SMAC, TPE) are state-of-the-art hyperparameter optimisation methods in automl. However, SMBO does not consider known information, like the best hyperparameters high possibility range and gradients. In this paper, we accelerate the traditional SMBO method and name our method as accSMBO. In accSMBO, we build a gradient-based multikernel Gaussian process with a good generalisation ability and we design meta-acquisition function which encourages that SMBO puts more attention on the best hyperparameters high possibility range. In L2 norm regularised logistic loss function experiments, our method exhibited state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2946656",
                    "name": "Daning Cheng"
                },
                {
                    "authorId": "2051537017",
                    "name": "Hanping Zhang"
                },
                {
                    "authorId": "48570355",
                    "name": "Fen Xia"
                },
                {
                    "authorId": "2298884755",
                    "name": "Shigang Li"
                },
                {
                    "authorId": "2108032102",
                    "name": "Yunquan Zhang"
                }
            ]
        },
        {
            "paperId": "caa238d5fba419a22e5305e8136acbb6fe5c2f83",
            "title": "The Scalability for Parallel Machine Learning Training Algorithm: Dataset Matters",
            "abstract": "To gain a better performance, many researchers put more computing resource into an application. However, in the AI area, there is still a lack of a successful large-scale machine learning training application: The scalability and performance reproducibility of parallel machine learning training algorithm are limited and there are a few pieces of research focusing on why these indexes are limited but there are very few research efforts explaining the reasons in essence. In this paper, we propose that the sample difference in dataset plays a more prominent role in parallel machine learning algorithm scalability. Dataset characters can measure sample difference. These characters include the variance of the sample in a dataset, sparsity, sample diversity and similarity in sampling sequence. To match our proposal, we choose four kinds of parallel machine learning training algorithms as our research objects: (1) Asynchronous parallel SGD algorithm (Hogwild! algorithm) (2) Parallel model average SGD algorithm (Mini-batch SGD algorithm) (3) Decenterilization optimization algorithm, (4) Dual Coordinate Optimization (DADM algorithm). These algorithms cover different types of machine learning optimization algorithms. We present the analysis of their convergence proof and design experiments. Our results show that the characters datasets decide the scalability of the machine learning algorithm. What is more, there is an upper bound of parallel scalability for machine learning algorithms.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2946656",
                    "name": "Daning Cheng"
                },
                {
                    "authorId": "2051537017",
                    "name": "Hanping Zhang"
                },
                {
                    "authorId": "48570355",
                    "name": "Fen Xia"
                },
                {
                    "authorId": "2298884755",
                    "name": "Shigang Li"
                },
                {
                    "authorId": "2108032102",
                    "name": "Yunquan Zhang"
                }
            ]
        },
        {
            "paperId": "76bad331965c7987dd7652670f13ed23d11c861d",
            "title": "Asynchronous Parallel Sampling Gradient Boosting Decision Tree",
            "abstract": "With the development of big data technology, Gradient Boosting Decision Tree, i.e. GBDT, becomes one of the most important machine learning algorithms for its accurate output. However, the training process of GBDT needs a lot of computational resources and time. In order to accelerate the training process of GBDT, the asynchronous parallel sampling gradient boosting decision tree, abbr. asynch-SGBDT is proposed in this paper. Via introducing sampling, we adapt the numerical optimization process of traditional GBDT training process into stochastic optimization process and use asynchronous parallel stochastic gradient descent to accelerate the GBDT training process. Meanwhile, the theoretical analysis of asynch-SGBDT is provided by us in this paper. Experimental results show that GBDT training process could be accelerated by asynch-SGBDT. Our asynchronous parallel strategy achieves an almost linear speedup, especially for high-dimensional sparse datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2946656",
                    "name": "Daning Cheng"
                },
                {
                    "authorId": "48570355",
                    "name": "Fen Xia"
                },
                {
                    "authorId": "2298884755",
                    "name": "Shigang Li"
                },
                {
                    "authorId": "2108032102",
                    "name": "Yunquan Zhang"
                }
            ]
        },
        {
            "paperId": "1cb8602af9fd5e956230e431ad10a13a3f5b7742",
            "title": "Improving click-through rate prediction accuracy in online advertising by transfer learning",
            "abstract": "As the main revenue source of Internet companies, online advertising is always a significant topic, where click-through rate (CTR) prediction plays a central role. In online advertising systems, there are often many advertisement products. Due to the competition in the bidding mechanism, some advertising products may get lots of data to train the CTR prediction model while some may lack high-quality data. However, to predict accurate CTR, a large amount of data is needed. Therefore, transfer knowledge from the large product (source) to the small product (target) is necessary. We propose a transfer learning method that iteratively updates the data weights to selectively combine source data with target data for training. To efficiently process huge advertisement data, we design a sampling strategy based on the gradient information, and implement the algorithm with a MapReduce-like machine learning framework. We do experiments on real advertisement datasets. The results show that our approach improves the accuracy of CTR prediction compared to the supervised learning method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9279597",
                    "name": "Yuhan Su"
                },
                {
                    "authorId": "2427514",
                    "name": "Zhongming Jin"
                },
                {
                    "authorId": "47558464",
                    "name": "Ying Chen"
                },
                {
                    "authorId": "1910318",
                    "name": "Xinghai Sun"
                },
                {
                    "authorId": "2322385933",
                    "name": "Yaming Yang"
                },
                {
                    "authorId": "32060850",
                    "name": "Fangzheng Qiao"
                },
                {
                    "authorId": "48570355",
                    "name": "Fen Xia"
                },
                {
                    "authorId": "145738410",
                    "name": "W. Xu"
                }
            ]
        },
        {
            "paperId": "da61570335c3c3d3bd81c957e0dbada03bb4cdf7",
            "title": "Population Density-Based Hospital Recommendation with Mobile LBS Big Data",
            "abstract": "The difficulty of getting medical treatment is one of major livelihood issues in China. Since patients lack prior knowledge about the spatial distribution and the capacity of hospitals, some hospitals have abnormally high or sporadic population densities. This paper presents a new model for estimating the spatiotemporal population density in each hospital based on location-based service (LBS) big data, which would be beneficial to guiding and dispersing outpatients. To improve the estimation accuracy, several approaches are proposed to denoise the LBS data and classify people by detecting their various behaviors. In addition, a long short-term memory (LSTM) based deep learning is presented to predict the trend of population density. By using Baidu large-scale LBS logs database, we apply the proposed model to 113 hospitals in Beijing, P. R. China, and constructed an online hospital recommendation system which can provide users with a hospital rank list basing the real-time population density information and the hospitals' basic information such as hospitals' levels and their distances. We also mine several interesting patterns from these LBS logs by using our proposed system.",
            "fieldsOfStudy": [
                "Computer Science",
                "Geography"
            ],
            "authors": [
                {
                    "authorId": "24038404",
                    "name": "Hanqing Chao"
                },
                {
                    "authorId": "2117107507",
                    "name": "Yuan Cao"
                },
                {
                    "authorId": "47539666",
                    "name": "Junping Zhang"
                },
                {
                    "authorId": "48570355",
                    "name": "Fen Xia"
                },
                {
                    "authorId": "2111405806",
                    "name": "Ye Zhou"
                },
                {
                    "authorId": "3449207",
                    "name": "Hongming Shan"
                }
            ]
        },
        {
            "paperId": "0e4ba334d7edadbea528055b43c2c3e3ffd5a5a4",
            "title": "A General Distributed Dual Coordinate Optimization Framework for Regularized Loss Minimization",
            "abstract": "In modern large-scale machine learning applications, the training data are often partitioned and stored on multiple machines. It is customary to employ the \"data parallelism\" approach, where the aggregated training loss is minimized without moving data across machines. In this paper, we introduce a novel distributed dual formulation for regularized loss minimization problems that can directly handle data parallelism in the distributed setting. This formulation allows us to systematically derive dual coordinate optimization procedures, which we refer to as Distributed Alternating Dual Maximization (DADM). The framework extends earlier studies described in (Boyd et al., 2011; Ma et al., 2015a; Jaggi et al., 2014; Yang, 2013) and has rigorous theoretical analyses. Moreover with the help of the new formulation, we develop the accelerated version of DADM (Acc-DADM) by generalizing the acceleration technique from (Shalev-Shwartz and Zhang, 2014) to the distributed setting. We also provide theoretical results for the proposed accelerated version and the new result improves previous ones (Yang, 2013; Ma et al., 2015a) whose runtimes grow linearly on the condition number. Our empirical studies validate our theory and show that our accelerated approach significantly improves the previous state-of-the-art distributed dual coordinate optimization algorithms.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145119697",
                    "name": "Shun Zheng"
                },
                {
                    "authorId": "1809306",
                    "name": "Jialei Wang"
                },
                {
                    "authorId": "48570355",
                    "name": "Fen Xia"
                },
                {
                    "authorId": "47210460",
                    "name": "W. Xu"
                },
                {
                    "authorId": "50728655",
                    "name": "Tong Zhang"
                }
            ]
        },
        {
            "paperId": "4de9cf632b3baa46af78693c9831a4c1b7618738",
            "title": "Listwise Learning to Rank from Crowds",
            "abstract": "Learning to rank has received great attention in recent years as it plays a crucial role in many applications such as information retrieval and data mining. The existing concept of learning to rank assumes that each training instance is associated with a reliable label. However, in practice, this assumption does not necessarily hold true as it may be infeasible or remarkably expensive to obtain reliable labels for many learning to rank applications. Therefore, a feasible approach is to collect labels from crowds and then learn a ranking function from crowdsourcing labels. This study explores the listwise learning to rank with crowdsourcing labels obtained from multiple annotators, who may be unreliable. A new probabilistic ranking model is first proposed by combining two existing models. Subsequently, a ranking function is trained by proposing a maximum likelihood learning approach, which estimates ground-truth labels and annotator expertise, and trains the ranking function iteratively. In practical crowdsourcing machine learning, valuable side information (e.g., professional grades) about involved annotators is normally attainable. Therefore, this study also investigates learning to rank from crowd labels when side information on the expertise of involved annotators is available. In particular, three basic types of side information are investigated, and corresponding learning algorithms are consequently introduced. Further, the top-k learning to rank from crowdsourcing labels are explored to deal with long training ranking lists. The proposed algorithms are tested on both synthetic and real-world data. Results reveal that the maximum likelihood estimation approach significantly outperforms the average approach and existing crowdsourcing regression methods. The performances of the proposed algorithms are comparable to those of the learning model in consideration reliable labels. The results of the investigation further indicate that side information is helpful in inferring both ranking functions and expertise degrees of annotators.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143958958",
                    "name": "Ou Wu"
                },
                {
                    "authorId": "2053638996",
                    "name": "Qiang You"
                },
                {
                    "authorId": "48570355",
                    "name": "Fen Xia"
                },
                {
                    "authorId": "2109704908",
                    "name": "Lei Ma"
                },
                {
                    "authorId": "40506509",
                    "name": "Weiming Hu"
                }
            ]
        }
    ]
}