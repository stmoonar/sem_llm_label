{
    "authorId": "2170130468",
    "papers": [
        {
            "paperId": "3380d7fd90fee82e5742e4d6c4cd8dd5e3ec79c9",
            "title": "CodeTaxo: Enhancing Taxonomy Expansion with Limited Examples via Code Language Prompts",
            "abstract": "Taxonomies play a crucial role in various applications by providing a structural representation of knowledge. The task of taxonomy expansion involves integrating emerging concepts into existing taxonomies by identifying appropriate parent concepts for these new query concepts. Previous approaches typically relied on self-supervised methods that generate annotation data from existing taxonomies. However, these methods are less effective when the existing taxonomy is small (fewer than 100 entities). In this work, we introduce \\textsc{CodeTaxo}, a novel approach that leverages large language models through code language prompts to capture the taxonomic structure. Extensive experiments on five real-world benchmarks from different domains demonstrate that \\textsc{CodeTaxo} consistently achieves superior performance across all evaluation metrics, significantly outperforming previous state-of-the-art methods. The code and data are available at \\url{https://github.com/QingkaiZeng/CodeTaxo-Pub}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261382250",
                    "name": "Qingkai Zeng"
                },
                {
                    "authorId": "2170130468",
                    "name": "Yuyang Bai"
                },
                {
                    "authorId": "2284243487",
                    "name": "Zhaoxuan Tan"
                },
                {
                    "authorId": "2273843167",
                    "name": "Zhenyu Wu"
                },
                {
                    "authorId": "2114887261",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "2273911868",
                    "name": "Meng Jiang"
                }
            ]
        },
        {
            "paperId": "756ea6f43ac19a5e26a168a602bef1081ce943e7",
            "title": "Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy Induction from Limited Examples",
            "abstract": "Automatic taxonomy induction is crucial for web search, recommendation systems, and question answering. Manual curation of taxonomies is expensive in terms of human effort, making automatic taxonomy construction highly desirable. In this work, we introduce Chain-of-Layer which is an in-context learning framework designed to induct taxonomies from a given set of entities. Chain-of-Layer breaks down the task into selecting relevant candidate entities in each layer and gradually building the taxonomy from top to bottom. To minimize errors, we introduce the Ensemble-based Ranking Filter to reduce the hallucinated content generated at each iteration. Through extensive experiments, we demonstrate that Chain-of-Layer achieves state-of-the-art performance on four real-world benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261382250",
                    "name": "Qingkai Zeng"
                },
                {
                    "authorId": "2170130468",
                    "name": "Yuyang Bai"
                },
                {
                    "authorId": "2093186816",
                    "name": "Zhaoxuan Tan"
                },
                {
                    "authorId": "2114887261",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "2284181828",
                    "name": "Zhenwen Liang"
                },
                {
                    "authorId": "72871419",
                    "name": "Zhihan Zhang"
                },
                {
                    "authorId": "2275403324",
                    "name": "Meng Jiang"
                }
            ]
        },
        {
            "paperId": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
            "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114887261",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2170130468",
                    "name": "Yuyang Bai"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "9e190a86b861cfb612ef36eb3baeec24bf51c776",
            "title": "Detecting Spoilers in Movie Reviews with External Movie Knowledge and User Networks",
            "abstract": "Online movie review platforms are providing crowdsourced feedback for the film industry and the general public, while spoiler reviews greatly compromise user experience. Although preliminary research efforts were made to automatically identify spoilers, they merely focus on the review content itself, while robust spoiler detection requires putting the review into the context of facts and knowledge regarding movies, user behavior on film review platforms, and more. In light of these challenges, we first curate a large-scale network-based spoiler detection dataset LCS and a comprehensive and up-to-date movie knowledge base UKM. We then propose MVSD, a novel Multi-View Spoiler Detection framework that takes into account the external knowledge about movies and user activities on movie review platforms. Specifically, MVSD constructs three interconnecting heterogeneous information networks to model diverse data sources and their multi-view attributes, while we design and employ a novel heterogeneous graph neural network architecture for spoiler detection as node-level classification. Extensive experiments demonstrate that MVSD advances the state-of-the-art on two spoiler detection datasets, while the introduction of external knowledge and user interactions help ground robust spoiler detection. Our data and code are available at https://github.com/Arthur-Heng/Spoiler-Detection",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256778370",
                    "name": "Heng Wang"
                },
                {
                    "authorId": "49039233",
                    "name": "Wenqian Zhang"
                },
                {
                    "authorId": "2170130468",
                    "name": "Yuyang Bai"
                },
                {
                    "authorId": "2093186816",
                    "name": "Zhaoxuan Tan"
                },
                {
                    "authorId": "2114887261",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "2152099796",
                    "name": "Qinghua Zheng"
                },
                {
                    "authorId": "3326677",
                    "name": "Minnan Luo"
                }
            ]
        },
        {
            "paperId": "aa88c5e7374815ad8bc96c82da3775a1417160da",
            "title": "KGQuiz: Evaluating the Generalization of Encoded Knowledge in Large Language Models",
            "abstract": "Large language models (LLMs) demonstrate remarkable performance on knowledge-intensive tasks, suggesting that real-world knowledge is encoded in their model parameters. However, besides explorations on a few probing tasks in limited knowledge domains, it is not well understood how to evaluate LLMs' knowledge systematically and how well their knowledge abilities generalize, across a spectrum of knowledge domains and progressively complex task formats. To this end, we propose KGQuiz, a knowledge-intensive benchmark to comprehensively investigate the knowledge generalization abilities of LLMs. KGQuiz is a scalable framework constructed from triplet-based knowledge, which covers three knowledge domains and consists of five tasks with increasing complexity: true-or-false, multiple-choice QA, blank filling, factual editing, and open-ended knowledge generation. To gain a better understanding of LLMs' knowledge abilities and their generalization, we evaluate 10 open-source and black-box LLMs on the KGQuiz benchmark across the five knowledge-intensive tasks and knowledge domains. Extensive experiments demonstrate that LLMs achieve impressive performance in straightforward knowledge QA tasks, while settings and contexts requiring more complex reasoning or employing domain-specific facts still present significant challenges. We envision KGQuiz as a testbed to analyze such nuanced variations in performance across domains and task formats, and ultimately to understand, evaluate, and improve LLMs' knowledge abilities across a wide spectrum of knowledge domains and tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2170130468",
                    "name": "Yuyang Bai"
                },
                {
                    "authorId": "2114887261",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "2093186816",
                    "name": "Zhaoxuan Tan"
                },
                {
                    "authorId": "2258717872",
                    "name": "Shiqi Lou"
                },
                {
                    "authorId": "2249540815",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2249583325",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "c17b71f31fa708eb01310ff65ab660f2676a12a1",
            "title": "Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models",
            "abstract": "By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge. To this end, we propose Knowledge Card, a modular framework to plug in new factual and relevant knowledge into general-purpose LLMs. We first introduce knowledge cards -- specialized language models trained on corpora from specific domains and sources. Knowledge cards serve as parametric repositories that are selected at inference time to generate background knowledge for the base LLM. We then propose three content selectors to dynamically select and retain information in documents generated by knowledge cards, specifically controlling for relevance, brevity, and factuality of outputs. Finally, we propose two complementary integration approaches to augment the base LLM with the (relevant, factual) knowledge curated from the specialized LMs. Through extensive experiments, we demonstrate that Knowledge Card achieves state-of-the-art performance on six benchmark datasets. Ultimately, Knowledge Card framework enables dynamic synthesis and updates of knowledge from diverse domains. Its modularity will ensure that relevant knowledge can be continuously updated through the collective efforts of the research community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114887261",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "3040379",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "2170130468",
                    "name": "Yuyang Bai"
                },
                {
                    "authorId": "143820870",
                    "name": "Vidhisha Balachandran"
                },
                {
                    "authorId": "3083253",
                    "name": "Tianxing He"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                }
            ]
        },
        {
            "paperId": "7066c88ffd227c74479f29f917f0db73270afa24",
            "title": "TwiBot-22: Towards Graph-Based Twitter Bot Detection",
            "abstract": "Twitter bot detection has become an increasingly important task to combat misinformation, facilitate social media moderation, and preserve the integrity of the online discourse. State-of-the-art bot detection methods generally leverage the graph structure of the Twitter network, and they exhibit promising performance when confronting novel Twitter bots that traditional methods fail to detect. However, very few of the existing Twitter bot detection datasets are graph-based, and even these few graph-based datasets suffer from limited dataset scale, incomplete graph structure, as well as low annotation quality. In fact, the lack of a large-scale graph-based Twitter bot detection benchmark that addresses these issues has seriously hindered the development and evaluation of novel graph-based bot detection approaches. In this paper, we propose TwiBot-22, a comprehensive graph-based Twitter bot detection benchmark that presents the largest dataset to date, provides diversified entities and relations on the Twitter network, and has considerably better annotation quality than existing datasets. In addition, we re-implement 35 representative Twitter bot detection baselines and evaluate them on 9 datasets, including TwiBot-22, to promote a fair comparison of model performance and a holistic understanding of research progress. To facilitate further research, we consolidate all implemented codes and datasets into the TwiBot-22 evaluation framework, where researchers could consistently evaluate new models and datasets. The TwiBot-22 Twitter bot detection benchmark and evaluation framework are publicly available at https://twibot22.github.io/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114887261",
                    "name": "Shangbin Feng"
                },
                {
                    "authorId": "2093186816",
                    "name": "Zhaoxuan Tan"
                },
                {
                    "authorId": "2114831715",
                    "name": "Herun Wan"
                },
                {
                    "authorId": "2152170253",
                    "name": "Ningnan Wang"
                },
                {
                    "authorId": "2117098676",
                    "name": "Zilong Chen"
                },
                {
                    "authorId": "2134483590",
                    "name": "Binchi Zhang"
                },
                {
                    "authorId": "2152099796",
                    "name": "Qinghua Zheng"
                },
                {
                    "authorId": "49039233",
                    "name": "Wenqian Zhang"
                },
                {
                    "authorId": "2161968765",
                    "name": "Zhenyu Lei"
                },
                {
                    "authorId": "2169372154",
                    "name": "Shujie Yang"
                },
                {
                    "authorId": "2169404546",
                    "name": "Xinshun Feng"
                },
                {
                    "authorId": "2261664149",
                    "name": "Qingyue Zhang"
                },
                {
                    "authorId": "2169403637",
                    "name": "Hongrui Wang"
                },
                {
                    "authorId": "2169159066",
                    "name": "Yuhan Liu"
                },
                {
                    "authorId": "2170130468",
                    "name": "Yuyang Bai"
                },
                {
                    "authorId": "2256778370",
                    "name": "Heng Wang"
                },
                {
                    "authorId": "2169253157",
                    "name": "Zijian Cai"
                },
                {
                    "authorId": "2107964144",
                    "name": "Yanbo Wang"
                },
                {
                    "authorId": "1964303146",
                    "name": "Lijing Zheng"
                },
                {
                    "authorId": "2169172454",
                    "name": "Zihan Ma"
                },
                {
                    "authorId": "1737121128",
                    "name": "Jundong Li"
                },
                {
                    "authorId": "3326677",
                    "name": "Minnan Luo"
                }
            ]
        }
    ]
}