{
    "authorId": "145148600",
    "papers": [
        {
            "paperId": "1ae9f490b19b413c4929b818d47afdfbe95ba113",
            "title": "GAVIN",
            "abstract": "Annotation is an effective reading strategy people often undertake while interacting with digital text. It involves highlighting pieces of text and making notes about them. Annotating while reading in a desktop environment is considered trivial but, in a mobile setting where people read while hand-holding devices, the task of highlighting and typing notes on a mobile display is challenging. In this article, we introduce GAVIN, a gaze-assisted voice note-taking application, which enables readers to seamlessly take voice notes on digital documents by implicitly anchoring them to text passages. We first conducted a contextual enquiry focusing on participants\u2019 note-taking practices on digital documents. Using these findings, we propose a method which leverages eye-tracking and machine learning techniques to annotate voice notes with reference text passages. To evaluate our approach, we recruited 32 participants performing voice note-taking. Following, we trained a classifier on the data collected to predict text passage where participants made voice notes. Lastly, we employed the classifier to built GAVIN and conducted a user study to demonstrate the feasibility of the system. This research demonstrates the feasibility of using gaze as a resource for implicit anchoring of voice notes, enabling the design of systems that allow users to record voice notes with minimal effort and high accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1675255094",
                    "name": "A. A. Khan"
                },
                {
                    "authorId": "2508003",
                    "name": "Joshua Newn"
                },
                {
                    "authorId": "48346195",
                    "name": "Ryan M. Kelly"
                },
                {
                    "authorId": "2072389004",
                    "name": "Namrata Srivastava"
                },
                {
                    "authorId": "145148600",
                    "name": "J. Bailey"
                },
                {
                    "authorId": "2520424",
                    "name": "Eduardo Velloso"
                }
            ]
        },
        {
            "paperId": "2f141f80e9954f6d6ad46fba240714038bfb3b9d",
            "title": "Dual Head Adversarial Training",
            "abstract": "Deep neural networks (DNNs) are known to be vulnerable to adversarial examples/attacks, raising concerns about their reliability in safety-critical applications. A number of defense methods have been proposed to train robust DNNs resistant to adversarial attacks, among which adversarial training has so far demonstrated the most promising results. However, recent studies have shown that there exists an inherent tradeoff between accuracy and robustness in adversarially-trained DNNs. In this paper, we propose a novel technique Dual Head Adversarial Training (DH-AT) to further improve the robustness of existing adversarial training methods. Different from existing improved variants of adversarial training, DH-AT modifies both the architecture of the network and the training strategy to seek more robustness. Specifically, DH-AT first attaches a second network head (or branch) to one intermediate layer of the network, then uses a lightweight convolutional neural network (CNN) to aggregate the outputs of the two heads. The training strategy is also adapted to reflect the relative importance of the two heads. We empirically show, on multiple benchmark datasets, that DH-AT can bring notable robustness improvements to existing adversarial training methods. Compared with TRADES, one state-of-the-art adversarial training method, our DH-AT can improve the robustness by 3.4% against PGD40 and 2.3% against AutoAttack, and also improve the clean accuracy by 1.8%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116850896",
                    "name": "Yujing Jiang"
                },
                {
                    "authorId": "2115763552",
                    "name": "Xingjun Ma"
                },
                {
                    "authorId": "144757691",
                    "name": "S. Erfani"
                },
                {
                    "authorId": "145148600",
                    "name": "J. Bailey"
                }
            ]
        },
        {
            "paperId": "728171df555472656a9d9e2f155c16324fd7cca8",
            "title": "Bigram and Unigram Based Text Attack via Adaptive Monotonic Heuristic Search",
            "abstract": "Deep neural networks (DNNs) are known to be vulnerable to adversarial images, while their robustness in text classification are rarely studied. Several lines of text attack methods have been proposed in the literature, such as character-level, word-level, and sentence-level attacks. However, it is still a challenge to minimize the number of word distortions necessary to induce misclassification, while simultaneously ensuring the lexical correctness, syntactic correctness, and semantic similarity. In this paper, we propose the Bigram and Unigram based Monotonic Heuristic Search (BU-MHS) method to examine the vulnerability of deep models. Our method has three major merits. Firstly, we propose to attack text documents not only at the unigram word level but also at the bigram level to avoid producing meaningless outputs. Secondly, we propose a hybrid method to replace the input words with both their synonyms and sememe candidates, which greatly enriches potential substitutions compared to only using synonyms. Lastly, we design a search algorithm, i.e., Monotonic Heuristic Search (MHS), to determine the priority of word replacements, aiming to reduce the modification cost in an adversarial attack. We evaluate the effectiveness of BU-MHS on IMDB, AG's News, and Yahoo! Answers text datasets by attacking four state-of-the-art DNNs models. Experimental results show that our BU-MHS achieves the highest attack success rate by changing the smallest number of words compared with other existing models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150439583",
                    "name": "Xinghao Yang"
                },
                {
                    "authorId": "2140409319",
                    "name": "Weifeng Liu"
                },
                {
                    "authorId": "145148600",
                    "name": "J. Bailey"
                },
                {
                    "authorId": "2064266862",
                    "name": "Dacheng Tao"
                },
                {
                    "authorId": "2157221073",
                    "name": "Wei Liu"
                }
            ]
        },
        {
            "paperId": "843c1cbb71a117e9071b2b96d38e048596574d52",
            "title": "Neural Architecture Search via Combinatorial Multi-Armed Bandit",
            "abstract": "Neural Architecture Search (NAS) has gained significant popularity as an effective tool for designing high performance deep neural networks (DNNs). NAS can be performed via reinforcement learning, evolutionary algorithms, differentiable architecture search or tree-search methods. While significant progress has been made for both reinforcement learning and differentiable architecture search, tree-search methods have so far failed to achieve comparable accuracy or search efficiency. In this paper, we formulate NAS as a Combinatorial Multi-Armed Bandit (CMAB) problem (CMAB-NAS). This allows the decomposition of a large search space into smaller blocks where tree-search methods can be applied more effectively and efficiently. We further leverage a tree-based method called Nested Monte-Carlo Search to tackle the CMAB-NAS problem. On CIFAR-10, our approach discovers a cell structure that achieves a low error rate that is comparable to the state-of-the-art, using only 0.58 GPU days, which is 20 times faster than current tree-search methods. Moreover, the discovered structure transfers well to large-scale datasets such as ImageNet.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1753845931",
                    "name": "Hanxun Huang"
                },
                {
                    "authorId": "9576855",
                    "name": "Xingjun Ma"
                },
                {
                    "authorId": "144757691",
                    "name": "S. Erfani"
                },
                {
                    "authorId": "145148600",
                    "name": "J. Bailey"
                }
            ]
        },
        {
            "paperId": "9a1090c590474190df976bf5a91c3e5cc1a30864",
            "title": "Unlearnable Examples: Making Personal Data Unexploitable",
            "abstract": "The volume of\"free\"data on the internet has been key to the current success of deep learning. However, it also raises privacy concerns about the unauthorized exploitation of personal data for training commercial models. It is thus crucial to develop methods to prevent unauthorized data exploitation. This paper raises the question: \\emph{can data be made unlearnable for deep learning models?} We present a type of \\emph{error-minimizing} noise that can indeed make training examples unlearnable. Error-minimizing noise is intentionally generated to reduce the error of one or more of the training example(s) close to zero, which can trick the model into believing there is\"nothing\"to learn from these example(s). The noise is restricted to be imperceptible to human eyes, and thus does not affect normal data utility. We empirically verify the effectiveness of error-minimizing noise in both sample-wise and class-wise forms. We also demonstrate its flexibility under extensive experimental settings and practicability in a case study of face recognition. Our work establishes an important first step towards making personal data unexploitable to deep learning models.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1753845931",
                    "name": "Hanxun Huang"
                },
                {
                    "authorId": "9576855",
                    "name": "Xingjun Ma"
                },
                {
                    "authorId": "144757691",
                    "name": "S. Erfani"
                },
                {
                    "authorId": "145148600",
                    "name": "J. Bailey"
                },
                {
                    "authorId": "1919541",
                    "name": "Yisen Wang"
                }
            ]
        },
        {
            "paperId": "d7ce16bd94ce756edfac7d3bebbe229638845bc7",
            "title": "A Transferable Technique for Detecting and Localising Segments of Repeating Patterns in Time series",
            "abstract": "In time series data, consecutively repeated patterns occur in many applications, including activity recognition from wearable sensors. Repeating patterns may vary over time and present in various shapes and sizes, which makes their detection a challenging problem. We develop a novel technique, RP-Mask, that can detect and localise segments of consecutively repeated patterns, without prior knowledge about the shape and length of the repeats. Our technique represents time series using recurrence plots (RP), a method for visualising repetition in time series. We identify two key features of recurrence plots-checkerboard patterns and vertical/horizontal lines marking the start and end of checkerboard patterns. We use object recognition on RP images to detect and localise the checkerboard patterns, which are mapped to the segments of consecutively repeating patterns on the underlying time series. Since the collection and labeling of a real world dataset that exhibits all possible variations of a repetition is prohibitive, we demonstrate that our model is able to effectively learn from synthetically curated data and perform equally effective on a real world dataset, while it is noise tolerant. We compare our method to a number of state-of-the-art techniques and show that our method outperforms the state of the art both when trained using real activity recognition and synthetic data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51138554",
                    "name": "Mahtab Mirmomeni"
                },
                {
                    "authorId": "47775409",
                    "name": "Lars Kulik"
                },
                {
                    "authorId": "145148600",
                    "name": "J. Bailey"
                }
            ]
        },
        {
            "paperId": "dcfb420412d76600eb124625f62fb28499af1e8c",
            "title": "On the Convergence and Robustness of Adversarial Training",
            "abstract": "Improving the robustness of deep neural networks (DNNs) to adversarial examples is an important yet challenging problem for secure deep learning. Across existing defense techniques, adversarial training with Projected Gradient Decent (PGD) is amongst the most effective. Adversarial training solves a min-max optimization problem, with the inner maximization generating adversarial examples by maximizing the classification loss, and the outer minimization finding model parameters by minimizing the loss on adversarial examples generated from the inner maximization. A criterion that measures how well the inner maximization is solved is therefore crucial for adversarial training. In this paper, we propose such a criterion, namely First-Order Stationary Condition for constrained optimization (FOSC), to quantitatively evaluate the convergence quality of adversarial examples found in the inner maximization. With FOSC, we find that to ensure better robustness, it is essential to use adversarial examples with better convergence quality at the later stages of training. Yet at the early stages, high convergence quality adversarial examples are not necessary and may even lead to poor robustness. Based on these observations, we propose a dynamic training strategy to gradually increase the convergence quality of the generated adversarial examples, which significantly improves the robustness of adversarial training. Our theoretical and empirical results show the effectiveness of the proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1919541",
                    "name": "Yisen Wang"
                },
                {
                    "authorId": "9576855",
                    "name": "Xingjun Ma"
                },
                {
                    "authorId": "145148600",
                    "name": "J. Bailey"
                },
                {
                    "authorId": "2882166",
                    "name": "Jinfeng Yi"
                },
                {
                    "authorId": "145218984",
                    "name": "Bowen Zhou"
                },
                {
                    "authorId": "9937103",
                    "name": "Quanquan Gu"
                }
            ]
        },
        {
            "paperId": "e0f2a1fd87964e41137bdd6fb6c9749abece41fa",
            "title": "Adversarial Interaction Attack: Fooling AI to Misinterpret Human Intentions",
            "abstract": "Understanding the actions of both humans and artificial intelligence (AI) agents is important before modern AI systems can be fully integrated into our daily life. In this paper, we show that, despite their current huge success, deep learning based AI systems can be easily fooled by subtle adversarial noise to misinterpret the intention of an action in interaction scenarios. Based on a case study of skeleton-based human interactions, we propose a novel adversarial attack on interactions, and demonstrate how DNN-based interaction models can be tricked to predict the participants' reactions in unexpected ways. From a broader perspective, the scope of our proposed attack method is not confined to problems related to skeleton data but can also be extended to any type of problems involving sequential regressions. Our study highlights potential risks in the interaction loop with AI and humans, which need to be carefully addressed when deploying AI systems in safety-critical applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2046991044",
                    "name": "Nodens Koren"
                },
                {
                    "authorId": "143969578",
                    "name": "Qiuhong Ke"
                },
                {
                    "authorId": "2115869684",
                    "name": "Yisen Wang"
                },
                {
                    "authorId": "145148600",
                    "name": "J. Bailey"
                },
                {
                    "authorId": "9576855",
                    "name": "Xingjun Ma"
                }
            ]
        },
        {
            "paperId": "eb7500d05018d849a67bd3452a791d97d49efb27",
            "title": "Are you with me? Measurement of Learners\u2019 Video-Watching Attention with Eye Tracking",
            "abstract": "Video has become an essential medium for learning. However, there are challenges when using traditional methods to measure how learners attend to lecture videos in video learning analytics, such as difficulty in capturing learners\u2019 attention at a fine-grained level. Therefore, in this paper, we propose a gaze-based metric\u2014\u201cwith-me-ness direction\u201d that can measure how learners\u2019 gaze-direction changes when they listen to the instructor\u2019s dialogues in a video-lecture. We analyze the gaze data of 45 participants as they watched a video lecture and measured both the sequences of with-me-ness direction and proportion of time a participant spent looking in each direction throughout the lecture at different levels. We found that although the majority of the time participants followed the instructor\u2019s dialogues, their behaviour of looking-ahead, looking-behind or looking-outside differed by their prior knowledge. These findings open the possibility of using eye-tracking to measure learners\u2019 video-watching attention patterns and examine factors that can influence their attention, thereby helping instructors to design effective learning materials.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2072389004",
                    "name": "Namrata Srivastava"
                },
                {
                    "authorId": "143639391",
                    "name": "Sadia Nawaz"
                },
                {
                    "authorId": "2508003",
                    "name": "Joshua Newn"
                },
                {
                    "authorId": "46515199",
                    "name": "J. Lodge"
                },
                {
                    "authorId": "2520424",
                    "name": "Eduardo Velloso"
                },
                {
                    "authorId": "2065150177",
                    "name": "Sarah M. Erfani"
                },
                {
                    "authorId": "65953975",
                    "name": "D. Ga\u0161evi\u0107"
                },
                {
                    "authorId": "145148600",
                    "name": "J. Bailey"
                }
            ]
        },
        {
            "paperId": "faf6635ba2184959bdc18c09303aa433d35d4800",
            "title": "High Intrinsic Dimensionality Facilitates Adversarial Attack: Theoretical Evidence",
            "abstract": "Machine learning systems are vulnerable to adversarial attack. By applying to the input object a small, carefully-designed perturbation, a classifier can be tricked into making an incorrect prediction. This phenomenon has drawn wide interest, with many attempts made to explain it. However, a complete understanding is yet to emerge. In this paper we adopt a slightly different perspective, still relevant to classification. We consider retrieval, where the output is a set of objects most similar to a user-supplied query object, corresponding to the set of $k$ -nearest neighbors. We investigate the effect of adversarial perturbation on the ranking of objects with respect to a query. Through theoretical analysis, supported by experiments, we demonstrate that as the intrinsic dimensionality of the data domain rises, the amount of perturbation required to subvert neighborhood rankings diminishes, and the vulnerability to adversarial attack rises. We examine two modes of perturbation of the query: either \u2018closer\u2019 to the target point, or \u2018farther\u2019 from it. We also consider two perspectives: \u2018query-centric\u2019, examining the effect of perturbation on the query\u2019s own neighborhood ranking, and \u2018target-centric\u2019, considering the ranking of the query point in the target\u2019s neighborhood set. All four cases correspond to practical scenarios involving classification and retrieval.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1778357",
                    "name": "L. Amsaleg"
                },
                {
                    "authorId": "145148600",
                    "name": "J. Bailey"
                },
                {
                    "authorId": "1992913280",
                    "name": "Am\u00e9lie Barbe"
                },
                {
                    "authorId": "144757691",
                    "name": "S. Erfani"
                },
                {
                    "authorId": "1775704",
                    "name": "T. Furon"
                },
                {
                    "authorId": "4480560",
                    "name": "Michael E. Houle"
                },
                {
                    "authorId": "143651750",
                    "name": "Milo\u0161 Radovanovi\u0107"
                },
                {
                    "authorId": "1817564",
                    "name": "X. Nguyen"
                }
            ]
        }
    ]
}