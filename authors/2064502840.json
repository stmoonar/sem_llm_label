{
    "authorId": "2064502840",
    "papers": [
        {
            "paperId": "746f2b1d9e5c84c184cdd7e5323c19da396f2e22",
            "title": "Feeding What You Need by Understanding What You Learned",
            "abstract": "Machine Reading Comprehension (MRC) reveals the ability to understand a given text passage and answer questions based on it. Existing research works in MRC rely heavily on large-size models and corpus to improve the performance evaluated by metrics such as Exact Match (EM) and F_1. However, such a paradigm lacks sufficient interpretation to model capability and can not efficiently train a model with a large corpus. In this paper, we argue that a deep understanding of model capabilities and data properties can help us feed a model with appropriate training data based on its learning status. Specifically, we design an MRC capability assessment framework that assesses model capabilities in an explainable and multi-dimensional manner. Based on it, we further uncover and disentangle the connections between various data properties and model performance. Finally, to verify the effectiveness of the proposed MRC capability assessment framework, we incorporate it into a curriculum learning pipeline and devise a Capability Boundary Breakthrough Curriculum (CBBC) strategy, which performs a model capability-based training to maximize the data value and improve training efficiency. Extensive experiments demonstrate that our approach significantly improves performance, achieving up to an 11.22% / 8.71% improvement of EM / F_1 on MRC tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108158210",
                    "name": "Xiaoqiang Wang"
                },
                {
                    "authorId": "2116441692",
                    "name": "Bang Liu"
                },
                {
                    "authorId": "2392383",
                    "name": "Fangli Xu"
                },
                {
                    "authorId": "2064502840",
                    "name": "Bowei Long"
                },
                {
                    "authorId": "2118071462",
                    "name": "Siliang Tang"
                },
                {
                    "authorId": "3008832",
                    "name": "Lingfei Wu"
                }
            ]
        },
        {
            "paperId": "de75305ab74c60d2bd19a5bfef22d3fef752cc5f",
            "title": "Graph-augmented Learning to Rank for Querying Large-scale Knowledge Graph",
            "abstract": "Knowledge graph question answering (KGQA) based on information retrieval aims to answer a question by retrieving answer from a large-scale knowledge graph. Most existing methods first roughly retrieve the knowledge subgraphs (KSG) that may contain candidate answer, and then search for the exact answer in the KSG. However, the KSG may contain thousands of candidate nodes since the knowledge graph involved in querying is often of large scale, thus decreasing the performance of answer selection. To tackle this problem, we first propose to partition the retrieved KSG to several smaller sub-KSGs via a new subgraph partition algorithm and then present a graph-augmented learning to rank model to select the top-ranked sub-KSGs from them. Our proposed model combines a novel subgraph matching networks to capture global interactions in both question and subgraphs and an Enhanced Bilateral Multi-Perspective Matching model to capture local interactions. Finally, we apply an answer selection model on the full KSG and the top-ranked sub-KSGs respectively to validate the effectiveness of our proposed graph-augmented learning to rank method. The experimental results on multiple benchmark datasets have demonstrated the effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "116267999",
                    "name": "Hanning Gao"
                },
                {
                    "authorId": "3008832",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "2152319586",
                    "name": "Po Hu"
                },
                {
                    "authorId": "143628849",
                    "name": "Zhihua Wei"
                },
                {
                    "authorId": "2392383",
                    "name": "Fangli Xu"
                },
                {
                    "authorId": "2064502840",
                    "name": "Bowei Long"
                }
            ]
        },
        {
            "paperId": "7e76473619ade2a15c76e288fbc86dfd54e6b938",
            "title": "Meta-Learning for Query Conceptualization at Web Scale",
            "abstract": "Concepts naturally constitute an abstraction for fine-grained entities and knowledge in the open domain. They enable search engines and recommendation systems to enhance user experience by discovering high-level abstraction of a search query and the user intent behind it. In this paper, we study the problem of query conceptualization, which is to find the most appropriate matching concepts for any given search query from a large pool of pre-defined concepts. We propose a coarse-to-fine approach to first reduce the search space for each query through a shortlisting scheme and then identify the matching concepts using pre-trained language models, which are meta-tuned to our query-concept matching task. Our shortlisting scheme involves using a GRU-based Relevant Words Generator (RWG) to first expand and complete the context of the given query and then shortlisting the candidate concepts through a scoring mechanism based on word overlaps. To accurately identify the most appropriate matching concepts for a query, even when the concepts may have zero verbatim overlaps with the query, we meta-fine-tune a BERT pairwise text-matching model under the Reptile meta-learning algorithm, which achieves zero-shot transfer learning on the conceptualization problem. Our two-stage framework can be trained with data completely derived from a search click graph, without requiring any human labelling efforts. For evaluation, we have constructed a large click graph based on more than $7$ million instances of the click history recorded in Tencent QQ browser and performed the query conceptualization task based on a large ontology with $159,148$ unique concepts. Results from a range of evaluation methods, including an offline evaluation procedure on the click graph, human evaluation, online A/B testing and case studies, have demonstrated the superiority of our approach over a number of competitive pre-trained language models and fine-tuned neural network baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7643432",
                    "name": "Fred X. Han"
                },
                {
                    "authorId": "1714907",
                    "name": "Di Niu"
                },
                {
                    "authorId": "2019329",
                    "name": "Haolan Chen"
                },
                {
                    "authorId": "115732587",
                    "name": "Weidong Guo"
                },
                {
                    "authorId": "1892671885",
                    "name": "Shengli Yan"
                },
                {
                    "authorId": "2064502840",
                    "name": "Bowei Long"
                }
            ]
        }
    ]
}