{
    "authorId": "1500530510",
    "papers": [
        {
            "paperId": "c92bca32e8244dfb394c2defa85f93380fb59928",
            "title": "Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels",
            "abstract": "Learning from noisy labels is an important and long-standing problem in machine learning for real applications. One of the main research lines focuses on learning a label corrector to purify potential noisy labels. However, these methods typically rely on strict assumptions and are limited to certain types of label noise. In this paper, we reformulate the label-noise problem from a generative-model perspective, $\\textit{i.e.}$, labels are generated by gradually refining an initial random guess. This new perspective immediately enables existing powerful diffusion models to seamlessly learn the stochastic generative process. Once the generative uncertainty is modeled, we can perform classification inference using maximum likelihood estimation of labels. To mitigate the impact of noisy labels, we propose the $\\textbf{L}$abel-$\\textbf{R}$etrieval-$\\textbf{A}$ugmented (LRA) diffusion model, which leverages neighbor consistency to effectively construct pseudo-clean labels for diffusion training. Our model is flexible and general, allowing easy incorporation of different types of conditional information, $\\textit{e.g.}$, use of pre-trained models, to further boost model performance. Extensive experiments are conducted for evaluation. Our model achieves new state-of-the-art (SOTA) results on all the standard real-world benchmark datasets. Remarkably, by incorporating conditional information from the powerful CLIP model, our method can boost the current SOTA accuracy by 10-20 absolute points in many cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118447329",
                    "name": "Jian Chen"
                },
                {
                    "authorId": "1940556",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "2110786178",
                    "name": "Rohan Sharma"
                },
                {
                    "authorId": "2136355608",
                    "name": "Zhiqiang Xu"
                },
                {
                    "authorId": "1500530510",
                    "name": "Tong Sun"
                },
                {
                    "authorId": "1752041",
                    "name": "Changyou Chen"
                }
            ]
        },
        {
            "paperId": "1303b087efb40f1d00d1f46ad61c955a1a836a34",
            "title": "Dually Noted: Layout-Aware Annotations with Smartphone Augmented Reality",
            "abstract": "Sharing annotations encourages feedback, discussion, and knowledge passing among readers and can be beneficial for personal and public use. Prior augmented reality (AR) systems have expanded these benefits to both digital and printed documents. However, despite smartphone AR now being widely available, there is a lack of research about how to use AR effectively for interactive document annotation. We propose Dually Noted, a smartphone-based AR annotation system that recognizes the layout of structural elements in a printed document for real-time authoring and viewing of annotations. We conducted experience prototyping with eight users to elicit potential benefits and challenges within smartphone AR, and this informed the resulting Dually Noted system and annotation interactions with the document elements. AR annotation is often unwieldy, but during a 12-user empirical study our novel structural understanding component allows Dually Noted to improve precise highlighting and annotation interaction accuracy by 13%, increase interaction speed by 42%, and significantly lower cognitive load over a baseline method without document layout understanding. Qualitatively, participants commented that Dually Noted was a swift and portable annotation experience. Overall, our research provides new methods and insights for how to improve AR annotations for physical documents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113337254",
                    "name": "Jing Qian"
                },
                {
                    "authorId": "2109170829",
                    "name": "Qi Sun"
                },
                {
                    "authorId": "26360698",
                    "name": "Curtis Wigington"
                },
                {
                    "authorId": "1996200457",
                    "name": "Han L. Han"
                },
                {
                    "authorId": "1500530510",
                    "name": "Tong Sun"
                },
                {
                    "authorId": "2058280979",
                    "name": "Jennifer Healey"
                },
                {
                    "authorId": "1854493",
                    "name": "J. Tompkin"
                },
                {
                    "authorId": "1422376447",
                    "name": "Jeffson Huang"
                }
            ]
        },
        {
            "paperId": "4248120adf60d715cfb9ae2e95b4ef32f6b1678e",
            "title": "Learning Adaptive Axis Attentions in Fine-tuning: Beyond Fixed Sparse Attention Patterns",
            "abstract": "We present a comprehensive study of sparse attention patterns in Transformer models. We first question the need for pre-training with sparse attention and present experiments showing that an efficient fine-tuning only approach yields a slightly worse but still competitive model. Then we compare the widely used local attention pattern and the less-well-studied global attention pattern, demonstrating that global patterns have several unique advantages. We also demonstrate that a flexible approach to attention, with different patterns across different layers of the model, is beneficial for some tasks. Drawing on this insight, we propose a novel Adaptive Axis Attention method, which learns\u2014during fine-tuning\u2014different attention patterns for each Transformer layer depending on the downstream task. Rather than choosing a fixed attention pattern, the adaptive axis attention method identifies important tokens\u2014for each task and model layer\u2014and focuses attention on those. It does not require pre-training to accommodate the sparse patterns and demonstrates competitive and sometimes better performance against fixed sparse attention patterns that require resource-intensive pre-training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2240689",
                    "name": "Zihan Wang"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "1859486",
                    "name": "Jason Kuen"
                },
                {
                    "authorId": "7574699",
                    "name": "Handong Zhao"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "1390533012",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                },
                {
                    "authorId": "1500530510",
                    "name": "Tong Sun"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                }
            ]
        },
        {
            "paperId": "64c8b27ef5789b395c59614e402c36d3109e4952",
            "title": "MGDoc: Pre-training with Multi-granular Hierarchy for Document Image Understanding",
            "abstract": "Document images are a ubiquitous source of data where the text is organized in a complex hierarchical structure ranging from fine granularity (e.g., words), medium granularity (e.g., regions such as paragraphs or figures), to coarse granularity (e.g., the whole page). The spatial hierarchical relationships between content at different levels of granularity are crucial for document image understanding tasks. Existing methods learn features from either word-level or region-level but fail to consider both simultaneously. Word-level models are restricted by the fact that they originate from pure-text language models, which only encode the word-level context. In contrast, region-level models attempt to encode regions corresponding to paragraphs or text blocks into a single embedding, but they perform worse with additional word-level features. To deal with these issues, we propose MGDoc, a new multi-modal multi-granular pre-training framework that encodes page-level, region-level, and word-level information at the same time. MGDoc uses a unified text-visual encoder to obtain multi-modal features across different granularities, which makes it possible to project the multi-granular features into the same hyperspace. To model the region-word correlation, we design a cross-granular attention mechanism and specific pre-training tasks for our model to reinforce the model of learning the hierarchy between regions and words. Experiments demonstrate that our proposed model can learn better features that perform well across granularities and lead to improvements in downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1762478",
                    "name": "Zilong Wang"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "67319819",
                    "name": "Chris Tensmeyer"
                },
                {
                    "authorId": "1598478975",
                    "name": "Nikolaos Barmpalios"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                },
                {
                    "authorId": "1500530510",
                    "name": "Tong Sun"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                }
            ]
        },
        {
            "paperId": "6dae41d63eeed6a6aaccbb1c147826bdf34e14d5",
            "title": "Unified Pretraining Framework for Document Understanding",
            "abstract": "Document intelligence automates the extraction of information from documents and supports many business applications. Recent self-supervised learning methods on large-scale unlabeled document datasets have opened up promising directions towards reducing annotation efforts by training models with self-supervised objectives. However, most of the existing document pretraining methods are still language-dominated. We present UDoc, a new unified pretraining framework for document understanding. UDoc is designed to support most document understanding tasks, extending the Transformer to take multimodal embeddings as input. Each input element is composed of words and visual features from a semantic region of the input document image. An important feature of UDoc is that it learns a generic representation by making use of three self-supervised losses, encouraging the representation to model sentences, learn similarities, and align modalities. Extensive empirical analysis demonstrates that the pretraining procedure learns better joint representations and leads to improvements in downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "1859486",
                    "name": "Jason Kuen"
                },
                {
                    "authorId": "2852035",
                    "name": "Vlad I. Morariu"
                },
                {
                    "authorId": "7574699",
                    "name": "Handong Zhao"
                },
                {
                    "authorId": "1598478975",
                    "name": "Nikolaos Barmpalios"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "3115414",
                    "name": "A. Nenkova"
                },
                {
                    "authorId": "1500530510",
                    "name": "Tong Sun"
                }
            ]
        },
        {
            "paperId": "839dc73c1adae268144d9cfb9d70985b2001304f",
            "title": "TiGAN: Text-Based Interactive Image Generation and Manipulation",
            "abstract": "Using natural-language feedback to guide image generation and manipulation can greatly lower the required efforts and skills. This topic has received increased attention in recent years through refinement of Generative Adversarial Networks (GANs); however, most existing works are limited to single-round interaction, which is not reflective of real world interactive image editing workflows. Furthermore, previous works dealing with multi-round scenarios are limited to predefined feedback sequences, which is also impractical. In this paper, we propose a novel framework for Text-based Interactive image generation and manipulation (TiGAN) that responds to users' natural-language feedback. \n TiGAN utilizes the powerful pre-trained CLIP model to understand users' natural-language feedback and exploits contrastive learning for a better text-to-image mapping. To maintain the image consistency during interactions, TiGAN generates intermediate feature vectors aligned with the feedback and selectively feeds these vectors to our proposed generative model. Empirical results on several datasets show that TiGAN improves both interaction efficiency and image quality while better avoids undesirable image manipulation during interactions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1652060942",
                    "name": "Yufan Zhou"
                },
                {
                    "authorId": "1940556",
                    "name": "Ruiyi Zhang"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "67319819",
                    "name": "Chris Tensmeyer"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "1752041",
                    "name": "Changyou Chen"
                },
                {
                    "authorId": "2128691876",
                    "name": "Jinhui Xu"
                },
                {
                    "authorId": "1500530510",
                    "name": "Tong Sun"
                }
            ]
        },
        {
            "paperId": "d70ffdd0f2aa7ac3c0be42a2eac994fcaa9ed308",
            "title": "User-Entity Differential Privacy in Learning Natural Language Models",
            "abstract": "In this paper, we introduce a novel concept of user-entity differential privacy (UeDP) to provide formal privacy protection simultaneously to both sensitive entities in textual data and data owners in learning natural language models (NLMs). To preserve UeDP, we developed a novel algorithm, called UeDP-Alg, optimizing the trade-off between privacy loss and model utility with a tight sensitivity bound derived from seamlessly combining user and sensitive entity sampling processes. An extensive theoretical analysis and evaluation show that our UeDP-Alg outperforms baseline approaches in model utility under the same privacy budget consumption on several NLM tasks, using benchmark datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3450120",
                    "name": "Phung Lai"
                },
                {
                    "authorId": "11032760",
                    "name": "Nhathai Phan"
                },
                {
                    "authorId": "1500530510",
                    "name": "Tong Sun"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "2301580599",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "1598478975",
                    "name": "Nikolaos Barmpalios"
                }
            ]
        },
        {
            "paperId": "a64f6ba2f83ca489e467d611fde38321b340301a",
            "title": "Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models",
            "abstract": "Recent studies indicate that NLU models are prone to rely on shortcut features for prediction, without achieving true language understanding. As a result, these models fail to generalize to real-world out-of-distribution data. In this work, we show that the words in the NLU training set can be modeled as a long-tailed distribution. There are two findings: 1) NLU models have strong preference for features located at the head of the long-tailed distribution, and 2) Shortcut features are picked up during very early few iterations of the model training. These two observations are further employed to formulate a measurement which can quantify the shortcut degree of each training sample. Based on this shortcut measurement, we propose a shortcut mitigation framework LGTR, to suppress the model from making overconfident predictions for samples with large shortcut degree. Experimental results on three NLU benchmarks demonstrate that our long-tailed distribution explanation accurately reflects the shortcut learning behavior of NLU models. Experimental analysis further indicates that LGTR can improve the generalization accuracy on OOD data, while preserving the accuracy on in-distribution data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3432460",
                    "name": "Mengnan Du"
                },
                {
                    "authorId": "1977256",
                    "name": "Varun Manjunatha"
                },
                {
                    "authorId": "39878379",
                    "name": "R. Jain"
                },
                {
                    "authorId": "3191727",
                    "name": "Ruchi Deshpande"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2174964",
                    "name": "Jiuxiang Gu"
                },
                {
                    "authorId": "1500530510",
                    "name": "Tong Sun"
                },
                {
                    "authorId": "48539382",
                    "name": "Xia Hu"
                }
            ]
        },
        {
            "paperId": "ac53992afade1a640c4d3d543ff38d3b381485c7",
            "title": "A Mixed-Reality System to Promote Child Engagement in Remote Intergenerational Storytelling",
            "abstract": "We present a mixed reality (MR) storytelling system designed specifically for multi-generational collaboration with child engagement as a key focus. Our \"Let\u2019s Make a Story\" system comprises a two-sided experience that brings together a remote adult and child to tell a story collaboratively. The child has a mixed reality phone-based application with an augmented manipulative that controls the story\u2019s main character. The remote adult participates through a web-based interface. The adult reads the story to the child and helps the child play the story game by providing them with items they need to clear the scenes.In this paper, we detail the implementation of our system and the results of a user study. Eight remote adult-child pairs experienced both the MR and a traditional paper-based storytelling system. To measure engagement, we used questionnaire analysis, engagement time with the story activity, and the word count of the child\u2019s description of how the story should end. We found that children uniformly preferred the MR system, spent more time engaged with the MR system, and used more words to describe how the story should end incorporating details from the game.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2058280979",
                    "name": "Jennifer Healey"
                },
                {
                    "authorId": "51236420",
                    "name": "Duotun Wang"
                },
                {
                    "authorId": "26360698",
                    "name": "Curtis Wigington"
                },
                {
                    "authorId": "1500530510",
                    "name": "Tong Sun"
                },
                {
                    "authorId": "40120517",
                    "name": "Huaishu Peng"
                }
            ]
        },
        {
            "paperId": "d68363a124c2424f5905e9c7da4b3b368744e6d4",
            "title": "Lets Make A Story Measuring MR Child Engagement",
            "abstract": "We present the result of a pilot study measuring child engagement with the Lets Make A Story system, a novel mixed reality, MR, collaborative storytelling system designed for grandparents and grandchildren. We compare our MR experience against an equivalent paper story experience. The goal of our pilot was to test the system with actual child users and assess the goodness of using metrics of time, user generated story content and facial expression analysis as metrics of child engagement. We find that multiple confounding variables make these metrics problematic including attribution of engagement time, spontaneous non-story related conversation and having the childs full forward face continuously in view during the story. We present our platform and experiences and our finding that the strongest metric was user comments in the post-experiential interview.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51236420",
                    "name": "Duotun Wang"
                },
                {
                    "authorId": "2058280979",
                    "name": "Jennifer Healey"
                },
                {
                    "authorId": "2087009992",
                    "name": "Jing Qian"
                },
                {
                    "authorId": "26360698",
                    "name": "Curtis Wigington"
                },
                {
                    "authorId": "1500530510",
                    "name": "Tong Sun"
                },
                {
                    "authorId": "40120517",
                    "name": "Huaishu Peng"
                }
            ]
        }
    ]
}