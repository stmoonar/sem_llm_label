{
    "authorId": "144050302",
    "papers": [
        {
            "paperId": "1211339f182475eb3136a507324463dd8fd10d10",
            "title": "Progressive Negative Enhancing Contrastive Learning for Image Dehazing and Beyond",
            "abstract": "Image dehazing is a pivotal preliminary step in the advancement of robust intelligent surveillance system. However, it is an extremely challenging ill-posed problem, as it faces severe information degradation when accurately restoring the clean image from its haze-polluted counterpart. This paper proposes a novel Progressive Negative Enhancing (PNE) contrastive learning mechanism to fully exploit various types of negative information, thereby facilitating the traditional positive-oriented objective function for image dehazing. The proposed method can progressively update the negative samples during model training, to steadily squeeze the restored image towards its desired clean target from various directions. Furthermore, considering the image dehazing task as a many-to-one feature mapping problem, we also make an early effort to enhance the robustness of the dehazing model under variational haze densities. Specifically, a novel density-variational dehazing network is proposed to be optimized under the consistency-regularized framework using the proposed PNE learning mechanism. The consistency regularization ensures consistent output given multi-level degraded hazy images, thereby significantly enhancing the robustness of the model in dealing with various hazy scenarios. Extensive experiments demonstrate that the proposed method exhibits superior performance over existing state-of-the-art methods. It achieves average PSNR boosts of 0.60 dB, 0.28 dB and 0.82 dB on dehazing, deraining and desnowing tasks, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2278801426",
                    "name": "De Cheng"
                },
                {
                    "authorId": "2290930424",
                    "name": "Yan Li"
                },
                {
                    "authorId": "2257931483",
                    "name": "Dingwen Zhang"
                },
                {
                    "authorId": "144050302",
                    "name": "Nannan Wang"
                },
                {
                    "authorId": "2244603563",
                    "name": "Jiande Sun"
                },
                {
                    "authorId": "2239964615",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "21d74706c68b333ca3a8949a9cee25000e10c6fe",
            "title": "Masked Attribute Description Embedding for Cloth-Changing Person Re-identification",
            "abstract": "Cloth-changing person re-identification (CC-ReID) aims to match persons who change clothes over long periods. The key challenge in CC-ReID is to extract clothing-independent features, such as face, hairstyle, body shape, and gait. Current research mainly focuses on modeling body shape using multi-modal biological features (such as silhouettes and sketches). However, it does not fully leverage the personal description information hidden in the original RGB image. Considering that there are certain attribute descriptions which remain unchanged after the changing of cloth, we propose a Masked Attribute Description Embedding (MADE) method that unifies personal visual appearance and attribute description for CC-ReID. Specifically, handling variable clothing-sensitive information, such as color and type, is challenging for effective modeling. To address this, we mask the clothing and color information in the personal attribute description extracted through an attribute detection model. The masked attribute description is then connected and embedded into Transformer blocks at various levels, fusing it with the low-level to high-level features of the image. This approach compels the model to discard clothing information. Experiments are conducted on several CC-ReID benchmarks, including PRCC, LTCC, Celeb-reID-light, and LaST. Results demonstrate that MADE effectively utilizes attribute description, enhancing cloth-changing person re-identification performance, and compares favorably with state-of-the-art methods. The code is available at https://github.com/moon-wh/MADE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2299758",
                    "name": "Chunlei Peng"
                },
                {
                    "authorId": "2279039539",
                    "name": "Boyu Wang"
                },
                {
                    "authorId": "2265622692",
                    "name": "Decheng Liu"
                },
                {
                    "authorId": "144050302",
                    "name": "Nannan Wang"
                },
                {
                    "authorId": "2261749919",
                    "name": "Ruimin Hu"
                },
                {
                    "authorId": "2239061108",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "26ac4a10152f2eb778885b378f1bdb33ae03b6a2",
            "title": "Large Pose Face Recognition via Facial Representation Learning",
            "abstract": "Overcoming image acquisition perspectives and face pose variations is a key problem in unconstrained face recognition tasks. One of the practical approaches is by reconstructing the face with extreme pose into a version that is more easily recognized by the discriminator, such as a frontal face. Often, existing methods attempt to balance the accuracy of downstream tasks with human visual perception, but ignore the differences in propensity between the two. Besides, large-scale datasets of profile-frontal paired face images are absent, which further hinders the training of models. In this work, we investigate a variety of face reconstruction approaches and propose a very simple, but very effective method to match face images across different scenes, named facial representation learning (FRL). The core idea of FRL is to introduce a representation generator in front of a pre-trained face recognition model, which can extract face representations from arbitrary faces that are more suitable for recognition model discrimination. In particular, the representation generator reconstructs the facial representation by minimising identity differences from the frontal face and adds pixel-level and adversarial constraints to cater for discriminator preferences. Extensive benchmark experiments show that the proposed method not only achieves better performance than state-of-the-art methods, but also can further squeeze the inference potential of existing face recognition models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265092559",
                    "name": "Jingwei Xin"
                },
                {
                    "authorId": "2218229677",
                    "name": "Zikai Wei"
                },
                {
                    "authorId": "144050302",
                    "name": "Nannan Wang"
                },
                {
                    "authorId": "2149869567",
                    "name": "Jie Li"
                },
                {
                    "authorId": "2239964615",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "34741631e3908642c3887061bd62c2374d5a56bf",
            "title": "STFE: A Comprehensive Video-Based Person Re-Identification Network Based on Spatio-Temporal Feature Enhancement",
            "abstract": "Video-based person re-identification (Re-ID) is designed to retrieve target pedestrians in video sequences under non-overlapping cameras. At present, mainstream approaches post-process the feature map extracted by the convolutional neural network backbone to obtain a global representation or a fine-grained local representation for higher accuracy. However, they still suffer from challenges, such as information loss for global-based methods and spatio-temporal feature fragmentation for local-based methods. To alleviate these problems, this article proposes a Spatio-Temporal Feature Enhancement (STFE) network from a spatio-temporal comprehensive perspective, combining the advantages of the above methods to obtain more comprehensive information from video tracklets. STFE consists of two main modules: Feature Space Projection Module (FSPM) and Global Low-frequency Enhancement Module (GLEM). FSPM mathematically converts continuous video information into a discrete feature space and selectively retains more useful information, thus avoiding spatio-temporal information loss. Meanwhile, FSPM applies global features instead of dividing feature maps spatially, thereby avoiding spatio-temporal feature fragmentation. In addition, GLEM which is based on transformer, acts as a broadband low-pass filter to mine richer global comprehensive information. Finally, by combining FSPM with GLEM, STFE can obtain spatio-temporal comprehensive video representation. Extensive experiments were conducted on two widely-used video Re-ID datasets. The experimental results verify our idea and demonstrate the effectiveness of the proposed STFE with 95.5% Rank-1 accuracy on MARS benchmarks, which surpasses previous state-of-the-arts by a large margin of +4%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1477973008",
                    "name": "Xi Yang"
                },
                {
                    "authorId": "2283085352",
                    "name": "Xian Wang"
                },
                {
                    "authorId": "2275284775",
                    "name": "Liangchen Liu"
                },
                {
                    "authorId": "144050302",
                    "name": "Nannan Wang"
                },
                {
                    "authorId": "2239061108",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "3f1fc24166136968598c91d30886ec085030bdc4",
            "title": "SSRR: Structural Semantic Representation Reconstruction for Visible-Infrared Person Re-Identification",
            "abstract": "Visible-infrared Person Re-identification (VI-ReID) aims to retrieve the images of pedestrian with the same identity from different modalities and cameras given a pedestrian image. To reduce modality discrepancy, existing methods often perform hard partitioning to mine more detail. However, these methods employ only uniform partitioning, without considering pedestrian structure, and lose a lot of pedestrian semantic information. To this end, this paper proposes a structural semantic representation reconstruction (SSRR) method to capture pedestrian semantic information by focusing on pedestrian structure. Specifically, based on the fine-grained features obtained by hard partitioning, we carry out structural reconstruction to obtain the reconstructed features containing semantic information. By adopting the direct link reconstruction structure, the reciprocal learning of fine-grained features and semantic features is ensured. Semantic features are reconstructed based on fine-grained features, and semantic information is beneficial to fine-grained features to better capture pedestrian-related details. In addition, local consistency loss is introduced to ensure the consistency of fine-grained features in the same component location, further enhancing the discriminant of the learned reconstructed representation. Extensive experiments confirm the superiority of our method on two public datasets SYSU-MM01 and RegDB.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1477973008",
                    "name": "Xi Yang"
                },
                {
                    "authorId": "2276929977",
                    "name": "Menghui Tian"
                },
                {
                    "authorId": "2276987725",
                    "name": "Meijie Li"
                },
                {
                    "authorId": "2115491099",
                    "name": "Ziyu Wei"
                },
                {
                    "authorId": "2277168172",
                    "name": "Liu Yuan"
                },
                {
                    "authorId": "144050302",
                    "name": "Nannan Wang"
                },
                {
                    "authorId": "2239061108",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "410998a8391da72b556fc7c0db0ab3ffa33a88ca",
            "title": "Dual-Adversarial Representation Disentanglement for Visible Infrared Person Re-Identification",
            "abstract": "Heterogeneous pedestrian images are captured by visible and infrared cameras with different spectrums, which play an important role in night-time video surveillance. However, visible infrared person re-identification (VI-REID) is still a challenging problem due to the considerable cross-modality discrepancies. To extract modality-invariant features which are discriminative for the person identity, recent studies are inclined to regard modality-specific features as noise and discard them. Actually, the modality-specific characteristics containing background and color information are indispensable for learning modality-shared features. In this paper, we propose a novel Dual-Adversarial Representation Disentanglement (DARD) model to separate modality-specific features from tangled pedestrian representations and effectively learn the robust modality-invariant representations. Specifically, our method employs dual-adversarial learning, incorporating image-level channel exchange and feature-level magnitude change to introduce variations in modality-specific representations. This deliberate perturbation raises the learning difficulty for the model to learn modality-shared features. Simultaneously, to control the changing scope of modality-specific features, bi-constrained noise alleviation is introduced during adversarial learning, keeping the balance of feature generation and adversary. The proposed dual-adversarial learning methodology enhances the robustness against cross-modality visual discrepancy and strengthens the discriminative power of the learned modality-shared representations without introducing additional network parameters. This improvement further elevates the retrieval performance of VI-REID. Extensive experiments with insightful analysis on two cross-modality re-identification datasets verify the effectiveness and superiority of the proposed DARD method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115491099",
                    "name": "Ziyu Wei"
                },
                {
                    "authorId": "1477973008",
                    "name": "Xi Yang"
                },
                {
                    "authorId": "144050302",
                    "name": "Nannan Wang"
                },
                {
                    "authorId": "2239061108",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "46561ae1db082f9f90f19c3ceae8c8d1f8dc6771",
            "title": "Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement",
            "abstract": "Deep neural networks are vulnerable to adversarial samples. Adversarial fine-tuning methods aim to enhance adversarial robustness through fine-tuning the naturally pre-trained model in an adversarial training manner. However, we identify that some latent features of adversarial samples are confused by adversarial perturbation and lead to an unexpectedly increasing gap between features in the last hidden layer of natural and adversarial samples. To address this issue, we propose a disentanglement-based approach to explicitly model and further remove the latent features that cause the feature gap. Specifically, we introduce a feature disentangler to separate out the latent features from the features of the adversarial samples, thereby boosting robustness by eliminating the latent features. Besides, we align features in the pre-trained model with features of adversarial samples in the fine-tuned model, to further benefit from the features from natural samples without confusion. Empirical evaluations on three benchmark datasets demonstrate that our approach surpasses existing adversarial fine-tuning methods and adversarial training baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254232483",
                    "name": "Nuoyan Zhou"
                },
                {
                    "authorId": "2149875537",
                    "name": "Dawei Zhou"
                },
                {
                    "authorId": "2265622692",
                    "name": "Decheng Liu"
                },
                {
                    "authorId": "2239061108",
                    "name": "Xinbo Gao"
                },
                {
                    "authorId": "144050302",
                    "name": "Nannan Wang"
                }
            ]
        },
        {
            "paperId": "4c9d497c654a9e37638c9d72375bfcd6870e2e65",
            "title": "Cooperative Separation of Modality Shared-Specific Features for Visible-Infrared Person Re-Identification",
            "abstract": "Visible-infrared person re-identification (VI-ReID) is a challenging task because the different imaging principles of visible and infrared images bring about huge modality discrepancy. Existing methods primarily address this issue by generating intermediate images to align modality features and establish connections between the visible and infrared modalities. However, the quality of these generated images is often unstable, limiting the effectiveness of such approaches. To overcome this limitation, we propose a novel method called modality shared-specific features cooperative separation. It consists of two key modules: the saliency response module and the cooperative separation module, aimed at alleviating the modality gap. The saliency response module incorporates a location attention mechanism and local features to construct contextual connections and extract local saliency information. Then, the cooperative separation module employs a more concise dual-MLPs as generator to effectively separate shared-specific features. Additionally, we introduce a shared feature refinement mechanism in both the generator and discriminator. By coordinating the shared-specific features, our method achieves secondary separation and extracts purer modality-shared features without specific information. Extensive experiments conducted on the SYSU-MM01 and RegDB public datasets demonstrate that our proposed method performs excellently in VI-ReID.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1477973008",
                    "name": "Xi Yang"
                },
                {
                    "authorId": "2291703124",
                    "name": "Wenjiao Dong"
                },
                {
                    "authorId": "2276987725",
                    "name": "Meijie Li"
                },
                {
                    "authorId": "2115491099",
                    "name": "Ziyu Wei"
                },
                {
                    "authorId": "144050302",
                    "name": "Nannan Wang"
                },
                {
                    "authorId": "2239061108",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "52bb62e004d4f793ec7ada2b0290f4df6f792712",
            "title": "Where Deepfakes Gaze at? Spatial\u2013Temporal Gaze Inconsistency Analysis for Video Face Forgery Detection",
            "abstract": "With the continuous development of generative models on face generation, how to distinguish the real and fake face has become an important problem for security. Because of the continuous improvement on the detection accuracy by facial physiological signals, video face forgery detection based on facial physiological signal analysis has received more and more attention, which has become an important research branch in the field of face forgery detection. Currently, most of the research on forgery detection based on physiological signal analysis use biometric features such as blinking patterns, head swings, heart rate signals, and lip movements. However, there hasn\u2019t been much exploration on the usage of gaze features in face forgery detection. Through the analysis of gaze directions in face videos, we have observed differences in the distribution of gaze direction pattern between the real and forged videos. Specifically, real videos tend to have more concentrated gaze distribution within a short period of time, while forged videos have more dispersed gaze distributions. In this paper, we present a novel Deepfake gaze analysis method named DFGaze, to explore spatial-temporal gaze inconsistency for video face forgery detection. Our method uses the gaze analysis model (GAM) to analyze the gaze features of face video frames, and then applies a spatial-temporal feature aggregator to realize authenticity classification based on gaze features. In order to better mine the authenticity clues in the videos, we further use the texture analysis model (TAM) and attribute analysis model (AAM) to improve the representation ability of spatial-temporal feature differences between real and forged faces. Extensive experiments show that our method can achieve state-of-the-art performance with the help of gaze analysis. The source code is available at https://github.com/ziminMIAO/DFGaze.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2299758",
                    "name": "Chunlei Peng"
                },
                {
                    "authorId": "2186886188",
                    "name": "Zimin Miao"
                },
                {
                    "authorId": "2265622692",
                    "name": "Decheng Liu"
                },
                {
                    "authorId": "144050302",
                    "name": "Nannan Wang"
                },
                {
                    "authorId": "2261749919",
                    "name": "Ruimin Hu"
                },
                {
                    "authorId": "2239964615",
                    "name": "Xinbo Gao"
                }
            ]
        },
        {
            "paperId": "5fcae331e0f26895dda9f92b3eea70b1d2e5f01f",
            "title": "Federated Face Forgery Detection Learning with Personalized Representation",
            "abstract": "Deep generator technology can produce high-quality fake videos that are indistinguishable, posing a serious social threat. Traditional forgery detection methods directly centralized training on data and lacked consideration of information sharing in non-public video data scenarios and data privacy. Naturally, the federated learning strategy can be applied for privacy protection, which aggregates model parameters of clients but not original data. However, simple federated learning can't achieve satisfactory performance because of poor generalization capabilities for the real hybrid-domain forgery dataset. To solve the problem, the paper proposes a novel federated face forgery detection learning with personalized representation. The designed Personalized Forgery Representation Learning aims to learn the personalized representation of each client to improve the detection performance of individual client models. In addition, a personalized federated learning training strategy is utilized to update the parameters of the distributed detection model. Here collaborative training is conducted on multiple distributed client devices, and shared representations of these client models are uploaded to the server side for aggregation. Experiments on several public face forgery detection datasets demonstrate the superior performance of the proposed algorithm compared with state-of-the-art methods. The code is available at \\emph{https://github.com/GANG370/PFR-Forgery.}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265622692",
                    "name": "Decheng Liu"
                },
                {
                    "authorId": "2188054521",
                    "name": "Zhan Dang"
                },
                {
                    "authorId": "2299758",
                    "name": "Chunlei Peng"
                },
                {
                    "authorId": "144050302",
                    "name": "Nannan Wang"
                },
                {
                    "authorId": "2261749919",
                    "name": "Ruimin Hu"
                },
                {
                    "authorId": "2239061108",
                    "name": "Xinbo Gao"
                }
            ]
        }
    ]
}