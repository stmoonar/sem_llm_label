{
    "authorId": "2256776354",
    "papers": [
        {
            "paperId": "69e5ecd0468aba4543bb14327e9769abb3f01ce9",
            "title": "RNR: Teaching Large Language Models to Follow Roles and Rules",
            "abstract": "Instruction fine-tuning (IFT) elicits instruction following capabilities and steers the behavior of large language models (LLMs) via supervised learning. However, existing models trained on open-source IFT datasets only have the ability to follow instructions from users, and often fail to follow complex role and rules specified by developers, a.k.a. system prompts. The ability to follow these roles and rules is essential for deployment, as it ensures that the model safely interacts with users within developer defined guidelines. To improve such role and rule following ability, we propose \\model, an automated data generation pipeline that generates diverse roles and rules from existing IFT instructions, along with corresponding responses. This data can then be used to train models that follow complex system prompts. The models are evaluated on our newly created benchmarks for role and rule following ability, as well as standard instruction-following benchmarks and general NLP tasks. Our framework significantly improves role and rule following capability in LLMs, as evidenced by over 25% increase in pass-rate on rule adherence, i.e. following all requirements, in our experiments with the Alpaca and Ultrachat datasets. Moreover, our models achieves this increase without any regression on popular instruction following benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2322605701",
                    "name": "Kuan Wang"
                },
                {
                    "authorId": "1645394981",
                    "name": "Alexander Bukharin"
                },
                {
                    "authorId": "2274172460",
                    "name": "Haoming Jiang"
                },
                {
                    "authorId": "2322450899",
                    "name": "Qingyu Yin"
                },
                {
                    "authorId": "2312598740",
                    "name": "Zhengyang Wang"
                },
                {
                    "authorId": "2323319914",
                    "name": "Tuo Zhao"
                },
                {
                    "authorId": "2316717970",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "2256776354",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "2273675661",
                    "name": "Bing Yin"
                },
                {
                    "authorId": "2273816798",
                    "name": "Xian Li"
                },
                {
                    "authorId": "2322453332",
                    "name": "Jianshu Chen"
                },
                {
                    "authorId": "2314348843",
                    "name": "Shiyang Li"
                }
            ]
        },
        {
            "paperId": "57404bd8c71e2b17fce63b49229b278b6a66bf13",
            "title": "Situated Natural Language Explanations",
            "abstract": "Natural language is among the most accessible tools for explaining decisions to humans, and large pretrained language models (PLMs) have demonstrated impressive abilities to generate coherent natural language explanations (NLE). The existing NLE research perspectives do not take the audience into account. An NLE can have high textual quality, but it might not accommodate audiences' needs and preference. To address this limitation, we propose an alternative perspective, \\textit{situated} NLE. On the evaluation side, we set up automated evaluation scores. These scores describe the properties of NLEs in lexical, semantic, and pragmatic categories. On the generation side, we identify three prompt engineering techniques and assess their applicability on the situations. Situated NLE provides a perspective and facilitates further research on the generation and evaluation of explanations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8129672",
                    "name": "Zining Zhu"
                },
                {
                    "authorId": "2152631081",
                    "name": "Hao Jiang"
                },
                {
                    "authorId": "7788583",
                    "name": "Jingfeng Yang"
                },
                {
                    "authorId": "7529854",
                    "name": "Sreyashi Nag"
                },
                {
                    "authorId": "2256776354",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "1490651934",
                    "name": "Jie Huang"
                },
                {
                    "authorId": "1921742",
                    "name": "Yifan Gao"
                },
                {
                    "authorId": "2479037",
                    "name": "Frank Rudzicz"
                },
                {
                    "authorId": "2021632793",
                    "name": "Bing Yin"
                }
            ]
        },
        {
            "paperId": "a072dc70413c02e6bcf80769eade6bb0bc4c1f98",
            "title": "Improving Consistency for Text Summarization with Energy Functions",
            "abstract": "Current abstractive summarization models often generate inconsistent content, i.e. texts that are not directly inferable from the source document, are not consistent with respect to world knowledge, or are self-contradictory. These inconsistencies motivate a new consistency taxonomy that we define as faithfulness, factuality, and self-supportiveness. However, most recent work on reducing inconsistency in document summarization only focuses on faithfulness detection and correction while ignoring other inconsistency phenomena, which limits the model\u2019s scalability. To improve the general consistency we introduce EnergySum, where we apply the Residual Energy-based Model by designing energy scorers that reflect each type of consistency. These energy scores are utilized in candidate re-ranking during the sampling process. Experiments on XSUM and CNN/DM datasets show that EnergySum mitigates the trade-off between accuracy and consistency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2273693612",
                    "name": "Qi Zeng"
                },
                {
                    "authorId": "3393799",
                    "name": "Qingyu Yin"
                },
                {
                    "authorId": "2273766311",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2273915435",
                    "name": "Yifan Gao"
                },
                {
                    "authorId": "7529854",
                    "name": "Sreyashi Nag"
                },
                {
                    "authorId": "2274037416",
                    "name": "Zhengyang Wang"
                },
                {
                    "authorId": "2273675661",
                    "name": "Bing Yin"
                },
                {
                    "authorId": "2274622328",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "2256776354",
                    "name": "Chao Zhang"
                }
            ]
        },
        {
            "paperId": "af6da5e89b61e43bf9af2233cb003deea3d4bff1",
            "title": "Knowledge-Selective Pretraining for Attribute Value Extraction",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275053439",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "3393799",
                    "name": "Qingyu Yin"
                },
                {
                    "authorId": "2274037416",
                    "name": "Zhengyang Wang"
                },
                {
                    "authorId": "2047145237",
                    "name": "Chenwei Zhang"
                },
                {
                    "authorId": "2274172460",
                    "name": "Haoming Jiang"
                },
                {
                    "authorId": "2273915435",
                    "name": "Yifan Gao"
                },
                {
                    "authorId": "2273766311",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2273816798",
                    "name": "Xian Li"
                },
                {
                    "authorId": "2256776354",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "2273675661",
                    "name": "Bing Yin"
                },
                {
                    "authorId": "2273814369",
                    "name": "William Wang"
                },
                {
                    "authorId": "2274054096",
                    "name": "Xiao-Dan Zhu"
                }
            ]
        }
    ]
}