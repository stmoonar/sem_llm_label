{
    "authorId": "2319225824",
    "papers": [
        {
            "paperId": "631eb228b8dec5ea3b5a1eb95971bd1cc3b0fb42",
            "title": "Optimal Membership Inference Bounds for Adaptive Composition of Sampled Gaussian Mechanisms",
            "abstract": "Given a trained model and a data sample, membership-inference (MI) attacks predict whether the sample was in the model's training set. A common countermeasure against MI attacks is to utilize differential privacy (DP) during model training to mask the presence of individual examples. While this use of DP is a principled approach to limit the efficacy of MI attacks, there is a gap between the bounds provided by DP and the empirical performance of MI attacks. In this paper, we derive bounds for the \\textit{advantage} of an adversary mounting a MI attack, and demonstrate tightness for the widely-used Gaussian mechanism. We further show bounds on the \\textit{confidence} of MI attacks. Our bounds are much stronger than those obtained by DP analysis. For example, analyzing a setting of DP-SGD with $\\epsilon=4$ would obtain an upper bound on the advantage of $\\approx0.36$ based on our analyses, while getting bound of $\\approx 0.97$ using the analysis of previous work that convert $\\epsilon$ to membership inference bounds. Finally, using our analysis, we provide MI metrics for models trained on CIFAR10 dataset. To the best of our knowledge, our analysis provides the state-of-the-art membership inference bounds for the privacy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "28122602",
                    "name": "Saeed Mahloujifar"
                },
                {
                    "authorId": "2319225824",
                    "name": "Alexandre Sablayrolles"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                }
            ]
        },
        {
            "paperId": "5d96e9061337fcaedcfba2ee2ceb6daa85151568",
            "title": "On the Importance of Difficulty Calibration in Membership Inference Attacks",
            "abstract": "The vulnerability of machine learning models to membership inference attacks has received much attention in recent years. However, existing attacks mostly remain impractical due to having high false positive rates, where non-member samples are often erroneously predicted as members. This type of error makes the predicted membership signal unreliable, especially since most samples are non-members in real world applications. In this work, we argue that membership inference attacks can benefit drastically from \\emph{difficulty calibration}, where an attack's predicted membership score is adjusted to the difficulty of correctly classifying the target sample. We show that difficulty calibration can significantly reduce the false positive rate of a variety of existing attacks without a loss in accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2058604454",
                    "name": "Lauren Watson"
                },
                {
                    "authorId": "144993411",
                    "name": "Chuan Guo"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2319225824",
                    "name": "Alexandre Sablayrolles"
                }
            ]
        },
        {
            "paperId": "bea1187a1f8a68f1a93f0c2fa10d31f93a30f84e",
            "title": "Opacus: User-Friendly Differential Privacy Library in PyTorch",
            "abstract": "We introduce Opacus, a free, open-source PyTorch library for training deep learning models with differential privacy (hosted at opacus.ai). Opacus is designed for simplicity, flexibility, and speed. It provides a simple and user-friendly API, and enables machine learning practitioners to make a training pipeline private by adding as little as two lines to their code. It supports a wide variety of layers, including multi-head attention, convolution, LSTM, GRU (and generic RNN), and embedding, right out of the box and provides the means for supporting other user-defined layers. Opacus computes batched per-sample gradients, providing higher efficiency compared to the traditional\"micro batch\"approach. In this paper we present Opacus, detail the principles that drove its implementation and unique features, and benchmark it against other frameworks for training models with differential privacy as well as standard PyTorch.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36737249",
                    "name": "Ashkan Yousefpour"
                },
                {
                    "authorId": "2107059646",
                    "name": "I. Shilov"
                },
                {
                    "authorId": "2319225824",
                    "name": "Alexandre Sablayrolles"
                },
                {
                    "authorId": "1389630028",
                    "name": "Davide Testuggine"
                },
                {
                    "authorId": "2107060033",
                    "name": "Karthik Prasad"
                },
                {
                    "authorId": "2107060266",
                    "name": "Mani Malek"
                },
                {
                    "authorId": "2131399147",
                    "name": "John Nguyen"
                },
                {
                    "authorId": "2129469303",
                    "name": "Sayan Gosh"
                },
                {
                    "authorId": "2528900",
                    "name": "Akash Bharadwaj"
                },
                {
                    "authorId": "2024689450",
                    "name": "Jessica Zhao"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2065193618",
                    "name": "Ilya Mironov"
                }
            ]
        }
    ]
}