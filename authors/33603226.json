{
    "authorId": "33603226",
    "papers": [
        {
            "paperId": "11ff450ad8d6eb98f5c320106f96652d3509faeb",
            "title": "Survey of Time Series Data Generation in IoT",
            "abstract": "Nowadays, with the rapid growth of the internet of things (IoT), massive amounts of time series data are being generated. Time series data play an important role in scientific and technological research for conducting experiments and studies to obtain solid and convincing results. However, due to privacy restrictions, limited access to time series data is always an obstacle. Moreover, the limited available open source data are often not suitable because of a small quantity and insufficient dimensionality and complexity. Therefore, time series data generation has become an imperative and promising solution. In this paper, we provide an overview of classical and state-of-the-art time series data generation methods in IoT. We classify the time series data generation methods into four major categories: rule-based methods, simulation-model-based methods, traditional machine-learning-based methods, and deep-learning-based methods. For each category, we first illustrate its characteristics and then describe the principles and mechanisms of the methods. Finally, we summarize the challenges and future directions of time series data generation in IoT. The systematic classification and evaluation will be a valuable reference for researchers in the time series data generation field.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153653168",
                    "name": "Chaochen Hu"
                },
                {
                    "authorId": "2228342123",
                    "name": "Zihan Sun"
                },
                {
                    "authorId": "46651866",
                    "name": "Chao Li"
                },
                {
                    "authorId": "33603226",
                    "name": "Yong Zhang"
                },
                {
                    "authorId": "1838264",
                    "name": "Chunxiao Xing"
                }
            ]
        },
        {
            "paperId": "14ccb8bcceb6de10eda6ad08bec242a4f2946497",
            "title": "FateZero: Fusing Attentions for Zero-shot Text-based Video Editing",
            "abstract": "The diffusion-based generative models have achieved remarkable success in text-based image generation. However, since it contains enormous randomness in generation progress, it is still challenging to apply such models for real-world visual content editing, especially in videos. In this paper, we propose FateZero, a zero-shot text-based editing method on real-world videos without per-prompt training or use-specific mask. To edit videos consistently, we propose several techniques based on the pre-trained models. Firstly, in contrast to the straightforward DDIM inversion technique, our approach captures intermediate attention maps during inversion, which effectively retain both structural and motion information. These maps are directly fused in the editing process rather than generated during denoising. To further minimize semantic leakage of the source video, we then fuse self-attentions with a blending mask obtained by cross-attention features from the source prompt. Furthermore, we have implemented a reform of the self-attention mechanism in denoising UNet by introducing spatial-temporal attention to ensure frame consistency. Yet succinct, our method is the first one to show the ability of zero-shot text-driven video style and local attribute editing from the trained text-to-image model. We also have a better zero-shot shape-aware editing ability based on the text-to-video model [52]. Extensive experiments demonstrate our superior temporal consistency and editing capability than previous works.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9672091",
                    "name": "Chenyang Qi"
                },
                {
                    "authorId": "30176430",
                    "name": "Xiaodong Cun"
                },
                {
                    "authorId": "33603226",
                    "name": "Yong Zhang"
                },
                {
                    "authorId": "2121274",
                    "name": "Chenyang Lei"
                },
                {
                    "authorId": "47119707",
                    "name": "Xintao Wang"
                },
                {
                    "authorId": "1387190008",
                    "name": "Ying Shan"
                },
                {
                    "authorId": "143832240",
                    "name": "Qifeng Chen"
                }
            ]
        },
        {
            "paperId": "25327a796cedd469d11da3bf252d7e0bff449f47",
            "title": "An Efficient Algorithm for Distance-based Structural Graph Clustering",
            "abstract": "Structural graph clustering (SCAN) is a classic graph clustering algorithm. In SCAN, a key step is to compute the structural similarity between vertices according to the overlap ratio of one-hop neighborhoods. Given two vertices u and v, existing studies only consider the case when u and v are neighbors. However, the structural similarity between non-neighboring vertices in SCAN is always zero, and using only one-hop neighbors on weighted graphs discards the weights on each edge. Both may not reflect the true closeness of two vertices and may fail to return high-quality clustering results. To tackle this issue, we define and study the distance-based structural graph clustering problem. Given a distance threshold d and two vertices u and v, the structural similarity between u and v is defined as the ratio of their respective neighbors within a distance of no more than d. We show that the newly defined distance-based SCAN achieves better clustering results compared to the vanilla version of SCAN. However, the new definition brings challenges in the computation of final clustering results. To tackle this efficiency issue, we propose DistanceSCAN, an efficient approximate algorithm for solving the distance-based SCAN problem. The main idea of DistanceSCAN is to use all-distances bottom-k sketches (ADS) to speed up the computation of similarities. Given the ADS, we can derive the similarity between two vertices with a bounded cost of O(k). However, to ensure that the estimated similarity has an approximation guarantee, the value of k still needs to be set to as large as thousands. This brings high computational costs when computing the similarities between neighboring vertices. To tackle this issue, we further construct histograms to prune the structural similarity computations of vertices pairs. Extensive experiments on real datasets validate the effectiveness and efficiency of DistanceSCAN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2146384880",
                    "name": "Kaixin Liu"
                },
                {
                    "authorId": "39996718",
                    "name": "Sibo Wang"
                },
                {
                    "authorId": "33603226",
                    "name": "Yong Zhang"
                },
                {
                    "authorId": "1838264",
                    "name": "Chunxiao Xing"
                }
            ]
        },
        {
            "paperId": "32fad3ebb40bc9e827ad8ebb250dfabc5006a17a",
            "title": "Interactive Story Visualization with Multiple Characters",
            "abstract": "Accurate Story visualization requires several necessary elements, such as identity consistency across frames, the alignment between plain text and visual content, and a reasonable layout of objects in images. Most previous works endeavor to meet these requirements by fitting a text-to-image (T2I) model on a set of videos in the same style and with the same characters, e.g., the FlintstonesSV dataset. However, the learned T2I models typically struggle to adapt to new characters, scenes, and styles, and often lack the flexibility to revise the layout of the synthesized images. This paper proposes a system for generic interactive story visualization, capable of handling multiple novel characters and supporting the editing of layout and local structure. It is developed by leveraging the prior knowledge of large language and T2I models, trained on massive corpora. The system comprises four interconnected components: story-to-prompt generation (S2P), text-to-layout generation (T2L), controllable text-to-image generation (C-T2I), and image-to-video animation (I2V). First, the S2P module converts concise story information into detailed prompts required for subsequent stages. Next, T2L generates diverse and reasonable layouts based on the prompts, offering users the ability to adjust and refine the layout to their preferences. The core component, C-T2I, enables the creation of images guided by layouts, sketches, and actor-specific identifiers to maintain consistency and detail across visualizations. Finally, I2V enriches the visualization process by animating the generated images. Extensive experiments and a user study are conducted to validate the effectiveness and flexibility of interactive editing of the proposed system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2087062449",
                    "name": "Yuan Gong"
                },
                {
                    "authorId": "1878778389",
                    "name": "Youxin Pang"
                },
                {
                    "authorId": "30176430",
                    "name": "Xiaodong Cun"
                },
                {
                    "authorId": "2904307",
                    "name": "Menghan Xia"
                },
                {
                    "authorId": "2149052351",
                    "name": "Haoxin Chen"
                },
                {
                    "authorId": "2111542852",
                    "name": "Longyue Wang"
                },
                {
                    "authorId": "33603226",
                    "name": "Yong Zhang"
                },
                {
                    "authorId": "47119707",
                    "name": "Xintao Wang"
                },
                {
                    "authorId": "1387190008",
                    "name": "Ying Shan"
                },
                {
                    "authorId": "3001727",
                    "name": "Yujiu Yang"
                }
            ]
        },
        {
            "paperId": "338b61463f5d77ca12c7651952ceb5af95e1de69",
            "title": "OpenSiteRec: An Open Dataset for Site Recommendation",
            "abstract": "As a representative information retrieval task, site recommendation, which aims at predicting the optimal sites for a brand or an institution to open new branches in an automatic data-driven way, is beneficial and crucial for brand development in modern business. However, there is no publicly available dataset so far and most existing approaches are limited to an extremely small scope of brands, which seriously hinders the research on site recommendation. Therefore, we collect, construct and release an open comprehensive dataset, namely OpenSiteRec, to facilitate and promote the research on site recommendation. Specifically, OpenSiteRec leverages a heterogeneous graph schema to represent various types of real-world entities and relations in four international metropolises. To evaluate the performance of the existing general methods on the site recommendation task, we conduct benchmarking experiments of several representative recommendation models on OpenSiteRec. Furthermore, we also highlight the potential application directions to demonstrate the wide applicability of OpenSiteRec. We believe that our OpenSiteRec dataset is significant and anticipated to encourage the development of advanced methods for site recommendation. OpenSiteRec is available online at https://OpenSiteRec.github.io/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "78145275",
                    "name": "Xinhang Li"
                },
                {
                    "authorId": "2116711312",
                    "name": "Xiang Zhao"
                },
                {
                    "authorId": "2162455919",
                    "name": "Yejing Wang"
                },
                {
                    "authorId": "2146400187",
                    "name": "Yu Liu"
                },
                {
                    "authorId": "2154405613",
                    "name": "Yong Li"
                },
                {
                    "authorId": "2221009794",
                    "name": "Cheng Long"
                },
                {
                    "authorId": "33603226",
                    "name": "Yong Zhang"
                },
                {
                    "authorId": "1838264",
                    "name": "Chunxiao Xing"
                }
            ]
        },
        {
            "paperId": "50dc3561a4a1a808d9efa9e30e261fdcd35683dc",
            "title": "NOFA: NeRF-based One-shot Facial Avatar Reconstruction",
            "abstract": "3D facial avatar reconstruction has been a significant research topic in computer graphics and computer vision, where photo-realistic rendering and flexible controls over poses and expressions are necessary for many related applications. Recently, its performance has been greatly improved with the development of neural radiance fields (NeRF). However, most existing NeRF-based facial avatars focus on subject-specific reconstruction and reenactment, requiring multi-shot images containing different views of the specific subject for training, and the learned model cannot generalize to new identities, limiting its further applications. In this work, we propose a one-shot 3D facial avatar reconstruction framework that only requires a single source image to reconstruct a high-fidelity 3D facial avatar. For the challenges of lacking generalization ability and missing multi-view information, we leverage the generative prior of 3D GAN and develop an efficient encoder-decoder network to reconstruct the canonical neural volume of the source image, and further propose a compensation network to complement facial details. To enable fine-grained control over facial dynamics, we propose a deformation field to warp the canonical volume into driven expressions. Through extensive experimental comparisons, we achieve superior synthesis results compared to several state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149167028",
                    "name": "Wang-Wang Yu"
                },
                {
                    "authorId": "2140245719",
                    "name": "Yanbo Fan"
                },
                {
                    "authorId": "33603226",
                    "name": "Yong Zhang"
                },
                {
                    "authorId": "2125219273",
                    "name": "Xuanxia Wang"
                },
                {
                    "authorId": "2158117676",
                    "name": "Fei Yin"
                },
                {
                    "authorId": "48442720",
                    "name": "Yun-Hao Bai"
                },
                {
                    "authorId": "2157429990",
                    "name": "Yan-Pei Cao"
                },
                {
                    "authorId": "2187307826",
                    "name": "Ying Shan"
                },
                {
                    "authorId": "2149122998",
                    "name": "Yang Wu"
                },
                {
                    "authorId": "2329283",
                    "name": "Zhongqian Sun"
                },
                {
                    "authorId": "143905981",
                    "name": "Baoyuan Wu"
                }
            ]
        },
        {
            "paperId": "52b10ae66d025e99fbb602935e155f97f4f0696f",
            "title": "Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance",
            "abstract": "Creating a vivid video from the event or scenario in our imagination is a truly fascinating experience. Recent advancements in text-to-video synthesis have unveiled the potential to achieve this with prompts only. While text is convenient in conveying the overall scene context, it may be insufficient to control precisely. In this paper, we explore customized video generation by utilizing text as context description and motion structure (e.g. frame- wise depth) as concrete guidance. Our method, dubbed Make-Your-Video, involves joint-conditional video generation using a Latent Diffusion Model that is pre-trained for still image synthesis and then promoted for video generation with the introduction of temporal modules. This two-stage learning scheme not only reduces the computing resources required, but also improves the performance by transferring the rich concepts available in image datasets solely into video generation. Moreover, we use a simple yet effective causal attention mask strategy to enable longer video synthesis, which mitigates the potential quality degradation effectively. Experimental results show the superiority of our method over existing baselines, particularly in terms of temporal coherence and fidelity to users' guidance. In addition, our model enables several intriguing applications that demonstrate potential for practical usage. The code, model weights, and videos are publicly available at our project page: https://doubiiu.github.io/projects/Make-Your-Video/.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2087273800",
                    "name": "Jinbo Xing"
                },
                {
                    "authorId": "2904307",
                    "name": "Menghan Xia"
                },
                {
                    "authorId": "2108135547",
                    "name": "Yuxin Liu"
                },
                {
                    "authorId": "2145915052",
                    "name": "Yuechen Zhang"
                },
                {
                    "authorId": "33603226",
                    "name": "Yong Zhang"
                },
                {
                    "authorId": "2154314501",
                    "name": "Yin-Yin He"
                },
                {
                    "authorId": "2118959474",
                    "name": "Hanyuan Liu"
                },
                {
                    "authorId": "2149052351",
                    "name": "Haoxin Chen"
                },
                {
                    "authorId": "30176430",
                    "name": "Xiaodong Cun"
                },
                {
                    "authorId": "47119707",
                    "name": "Xintao Wang"
                },
                {
                    "authorId": "1387190008",
                    "name": "Ying Shan"
                },
                {
                    "authorId": "50350340",
                    "name": "T. Wong"
                }
            ]
        },
        {
            "paperId": "607d9eb8ceab0533bcfc65e8aced69aee4e40976",
            "title": "IMF: Interactive Multimodal Fusion Model for Link Prediction",
            "abstract": "Link prediction aims to identify potential missing triples in knowledge graphs. To get better results, some recent studies have introduced multimodal information to link prediction. However, these methods utilize multimodal information separately and neglect the complicated interaction between different modalities. In this paper, we aim at better modeling the inter-modality information and thus introduce a novel Interactive Multimodal Fusion (IMF) model to integrate knowledge from different modalities. To this end, we propose a two-stage multimodal fusion framework to preserve modality-specific knowledge as well as take advantage of the complementarity between different modalities. Instead of directly projecting different modalities into a unified space, our multimodal fusion module limits the representations of different modalities independent while leverages bilinear pooling for fusion and incorporates contrastive learning as additional constraints. Furthermore, the decision fusion module delivers the learned weighted average over the predictions of all modalities to better incorporate the complementarity of different modalities. Our approach has been demonstrated to be effective through empirical evaluations on several real-world datasets. The implementation code is available online at https://github.com/HestiaSky/IMF-Pytorch.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "78145275",
                    "name": "Xinhang Li"
                },
                {
                    "authorId": "2197532318",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "2157763562",
                    "name": "Jiaxing Xu"
                },
                {
                    "authorId": "33603226",
                    "name": "Yong Zhang"
                },
                {
                    "authorId": "1838264",
                    "name": "Chunxiao Xing"
                }
            ]
        },
        {
            "paperId": "6d9241c0e8251c831b503d339b5fc47857f72870",
            "title": "Joint Resource Allocation and Task Offloading for Hybrid NOMA-assisted MEC Network with Network Slicing",
            "abstract": "Mobile edge computing (MEC) and non-orthogonal multiple access (NOMA) are regarded as promising technologies in fifth-generation (5G) networks. This paper proposes a hybrid NOMA-assisted MEC network slicing scenario. In one network slice, users are divided into some groups and use hybrid NOMA to occupy channel for communication. A two-step resource allocation and task offloading algorithm is proposed to minimize the total network energy consumption under the premise of maximum delay tolerance. The optimization problem is divided into two subproblems. One subproblem is the optimal resource allocation problem for users within a group, and the other is the user grouping problem. A two-layer deep reinforcement learning (DRL) algorithm based on double deep Q network (DDQN) is used to solve the resource allocation within a group, and a swap matching algorithm is used to solve the user grouping. The simulation results show that the proposed algorithm can effectively reduce the total energy consumption. Compared with DQN and Q-learning, the proposed DRL algorithm has faster convergence speed and better convergence value. Compared with exhaustive search, matching search has lower complexity and similar performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144290773",
                    "name": "Yong Zhang"
                },
                {
                    "authorId": "1682058",
                    "name": "H. Zhang"
                },
                {
                    "authorId": "89843190",
                    "name": "Y. Li"
                },
                {
                    "authorId": "33603226",
                    "name": "Yong Zhang"
                },
                {
                    "authorId": "2100715829",
                    "name": "Siyu Yuan"
                }
            ]
        },
        {
            "paperId": "76accc57b2468873e6a393b77aaec2ba064d3c17",
            "title": "Joint Optimization of the Partition and Scheduling of DNN Tasks in Computing and Network Convergence",
            "abstract": "Computing and network convergence (CNC) is a new network architecture based on computing evolution and network integration. Deep Neural Networks (DNNs) inference imposes a heavy computational burden on mobile devices. In this letter, an end-edge-network-cloud (EENC) collaborative inference architecture is proposed to reduce the DNN inference latency and maximize the computing potential of the CNC. A heuristic Centralized DNN Task Offloading algorithm (CDTO) is proposed for the fine-grained partition and scheduling problems of multiple DNN inference tasks. The CDTO algorithm can significantly reduce the makespan of DNN inference tasks and effectively improve the concurrent capacity of DNN tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144470702",
                    "name": "Zhenyu Zhang"
                },
                {
                    "authorId": "2149252206",
                    "name": "Qin Li"
                },
                {
                    "authorId": "2149373769",
                    "name": "Lu Lu"
                },
                {
                    "authorId": "2169303888",
                    "name": "Da Guo"
                },
                {
                    "authorId": "33603226",
                    "name": "Yong Zhang"
                }
            ]
        }
    ]
}