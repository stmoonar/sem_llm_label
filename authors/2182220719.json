{
    "authorId": "2182220719",
    "papers": [
        {
            "paperId": "037dd3bc76ca7d1b31a2bc4d3b13d3a43ef08a10",
            "title": "Verifiable Differential Privacy For When The Curious Become Dishonest",
            "abstract": "Many applications seek to produce differentially private statistics on sensitive data. Traditional approaches in the centralised model rely on a trusted aggregator to gather the raw data, aggregate statistics and introduce appropriate noise. Recent work has tried to relax the trust assumptions and reduce the need for trusted entities. However, such systems can trade off trust for increased noise and still require complete trust in some participants. Moreover, they do not prevent a malicious entity from introducing adversarial noise to skew the result or unmask some inputs. In this paper, we introduce the notion of \u201cveri\ufb01able differential privacy with covert security\u201d. The purpose is to ensure both privacy of the client\u2019s data and assurance that the output is not subject to any form of adversarial manipulation. The result is that everyone is assured that the noise used for differential privacy has been generated correctly, but no one can determine what the noise was. In the event of a malicious entity attempting to pervert the protocol, their actions will be detected with a constant probability negligibly close to one. We show that such veri\ufb01able privacy is practical and can be implemented at scale.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182220719",
                    "name": "Ari Biswas"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                }
            ]
        },
        {
            "paperId": "f352fe55fe7c795c2d904ebdd06bf91bf2593268",
            "title": "Interactive Proofs For Differentially Private Counting",
            "abstract": "Differential Privacy (DP) is often presented as a strong privacy-enhancing technology with broad applicability and advocated as a de facto standard for releasing aggregate statistics on sensitive data. However, in many embodiments, DP introduces a new attack surface: a malicious entity entrusted with releasing statistics could manipulate the results and use the randomness of DP as a convenient smokescreen to mask its nefariousness. Since revealing the random noise would obviate the purpose of introducing it, the miscreant may have a perfect alibi. To close this loophole, we introduce the idea of Interactive Proofs For Differential Privacy, which requires the publishing entity to output a zero knowledge proof that convinces an efficient verifier that the output is both DP and reliable. Such a definition might seem unachievable, as a verifier must validate that DP randomness was generated faithfully without learning anything about the randomness itself. We resolve this paradox by carefully mixing private and public randomness to compute verifiable DP counting queries with theoretical guarantees and show that it is also practical for real-world deployment. We also demonstrate that computational assumptions are necessary by showing a separation between information-theoretic DP and computational DP under our definition of verifiability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182220719",
                    "name": "Ari Biswas"
                },
                {
                    "authorId": "2254587151",
                    "name": "Graham Cormode"
                }
            ]
        }
    ]
}