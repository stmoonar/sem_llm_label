{
    "authorId": "1741044",
    "papers": [
        {
            "paperId": "0ec7e8b01d5518df1931ac0b59c1ba957a11497a",
            "title": "Exploring the Effect of Randomness on Transferability of Adversarial Samples Against Deep Neural Networks",
            "abstract": "We investigate the transferability of adversarial attacks against deep neural networks (DNNs)\u2014the contagion effect of adversarial attacks that, once deceiving one DNN model, can easily deceive other DNN models built on similar data. We demonstrate that introducing randomness to DNN models can break the curse of the transferability of adversarial attacks, given that the adversary does not have an unlimited attack budget. Two randomization schemes are explored: 1.) a random selection\u2014single or ensemble\u2014from a set of DNNs is surprisingly more robust against the strongest form of complete-knowledge attacks (a.k.a, white box attacks); 2.) after a small Gaussian random noise is added to its learned weights, a DNN model can potentially increase its resilience to adversarial attacks by as much as 74.2%. We compare the two randomization techniques to the Ensemble Adversarial Training technique and show that our randomization techniques are superior under different attack budget constraints. Furthermore, we explore the relationship between attack severity and decision boundary robustness in the version space. Finally, we connect the dots between the effectiveness of randomization to prevent attack transferability and the variability of DNN models through analyzing the differential entropy of sample hypotheses in the hypothesis space.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150920617",
                    "name": "Yan Zhou"
                },
                {
                    "authorId": "1741044",
                    "name": "Murat Kantarcioglu"
                },
                {
                    "authorId": "35476307",
                    "name": "B. Xi"
                }
            ]
        },
        {
            "paperId": "375fe784605688922a0c73f3d390fde32613b173",
            "title": "SAFE-PASS: Stewardship, Advocacy, Fairness and Empowerment in Privacy, Accountability, Security, and Safety for Vulnerable Groups",
            "abstract": "Our vision is to achieve societally responsible secure and trustworthy cyberspace that puts algorithmic and technological checks and balances on the indiscriminate sharing and analysis of data. We achieve this vision in a holistic manner by framing research directions with four major considerations: (i) Expanding knowledge and understanding of security and privacy perceptions and expectations in vulnerable groups, which significantly contribute to their unwillingness to share data, and use that knowledge to drive research in (a) mitigating missing/imbalanced data problems, (b) understanding and modeling security and privacy risks of data sharing, and (c) modeling utility of data sharing. (ii) Developing a risk-adaptive, policy model capable of capturing and articulating security and privacy expectations of users that are relevant in a particular context and develops associated technology to ensure provenance and accountability. (iii) Developing robust AI/ML algorithms that are transparent and explainable with respect to fairness and bias to reduce/eliminate discrimination, misuse, privacy violations, or other cyber-crimes. (iv) Developing models and techniques for a nuanced, contextually adaptive, and graded privacy paradigm that allows trade-offs between privacy and utility. Towards this, in this paper we present the SAFE-PASS framework to provide Stewardship, Advocacy, Fairness and Empowerment in Privacy, Accountability, Security, and Safety for Vulnerable Groups.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257932408",
                    "name": "Indrajit Ray"
                },
                {
                    "authorId": "72558235",
                    "name": "B. Thuraisingham"
                },
                {
                    "authorId": "145033630",
                    "name": "Jaideep Vaidya"
                },
                {
                    "authorId": "144156242",
                    "name": "S. Mehrotra"
                },
                {
                    "authorId": "145138841",
                    "name": "V. Atluri"
                },
                {
                    "authorId": "2240174870",
                    "name": "Indrakshi Ray"
                },
                {
                    "authorId": "1741044",
                    "name": "Murat Kantarcioglu"
                },
                {
                    "authorId": "2207842509",
                    "name": "R. Raskar"
                },
                {
                    "authorId": "2124624117",
                    "name": "Babak Salimi"
                },
                {
                    "authorId": "2226780760",
                    "name": "Steve Simske"
                },
                {
                    "authorId": "1732742",
                    "name": "N. Venkatasubramanian"
                },
                {
                    "authorId": "2151448490",
                    "name": "Vivek K. Singh"
                }
            ]
        },
        {
            "paperId": "385d461dadd7878667dbe6d5ef12896046f9314d",
            "title": "A Risk-Aware Paradigm for Privacy-Preserving Machine Learning",
            "abstract": "Over the years, the increasing use of personal data for training machine learning models raises several privacy concerns, such as membership inference attacks (i.e., detecting whether a given individual is in the training data) and sensitive attribute inference attacks (i.e., predicting some sensitive information about users based on the disclosed ML model). Although privacy solutions have emerged to address individual challenges, there is not a silver-bullet solution for all privacy problems. In this opinion paper, we argue that to develop a complete solution to a multitude of privacy challenges, we need a privacy-risk aware paradigm where tools and techniques targeting individual realistic privacy threats are employed simultaneously to complement each other. To achieve this goal, we propose a generic privacy framework to reason about the optimal combination of existing privacy tools under the set of given attack models and discuss how the proposed framework could be applied in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1741044",
                    "name": "Murat Kantarcioglu"
                }
            ]
        },
        {
            "paperId": "49073dda7e85b2f82e0ca989799c3e7f47df234c",
            "title": "Managing re-identification risks while providing access to the All of Us research program",
            "abstract": "OBJECTIVE\nThe All of Us Research Program makes individual-level data available to researchers while protecting the participants' privacy. This article describes the protections embedded in the multistep access process, with a particular focus on how the data was transformed to meet generally accepted reidentification risk levels.\n\n\nMETHODS\nAt the time of the study, the resource consisted of 329\u00a0084 participants. Systematic amendments were applied to the data to mitigate reidentification risk (eg, generalization of geographic regions, suppression of public events, and randomization of dates). We computed the reidentification risk for each participant using a state-of-the-art adversarial model specifically assuming that it is known that someone is a participant in the program. We confirmed the expected risk is no greater than 0.09, a threshold that is consistent with guidelines from various US state and federal agencies. We further investigated how risk varied as a function of participant demographics.\n\n\nRESULTS\nThe results indicated that 95th percentile of the reidentification risk of all the participants is below current thresholds. At the same time, we observed that risk levels were higher for certain race, ethnic, and genders.\n\n\nCONCLUSIONS\nWhile the reidentification risk was sufficiently low, this does not imply that the system is devoid of risk. Rather, All of Us uses a multipronged data protection strategy that includes strong authentication practices, active monitoring of data misuse, and penalization mechanisms for users who violate terms of service.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2730314",
                    "name": "Weiyi Xia"
                },
                {
                    "authorId": "3081526",
                    "name": "Melissa A. Basford"
                },
                {
                    "authorId": "1884317",
                    "name": "R. Carroll"
                },
                {
                    "authorId": "145248645",
                    "name": "E. Clayton"
                },
                {
                    "authorId": "2114303714",
                    "name": "Paul A. Harris"
                },
                {
                    "authorId": "1741044",
                    "name": "Murat Kantarcioglu"
                },
                {
                    "authorId": "1643800106",
                    "name": "Yongtai Liu"
                },
                {
                    "authorId": "2986560",
                    "name": "Steve Nyemba"
                },
                {
                    "authorId": "1699600",
                    "name": "Yevgeniy Vorobeychik"
                },
                {
                    "authorId": "2483332",
                    "name": "Zhiyu Wan"
                },
                {
                    "authorId": "2137818185",
                    "name": "B. Malin"
                }
            ]
        },
        {
            "paperId": "4a6ef9152daddd00b9884c13f1bcd93a64d0610e",
            "title": "HOLMES: Efficient Distribution Testing for Secure Collaborative Learning",
            "abstract": "Using secure multiparty computation (MPC) , organizations which own sensitive data (e.g., in healthcare, \ufb01nance or law enforcement) can train machine learning models over their joint dataset without revealing their data to each other. At the same time, secure computation restricts operations on the joint dataset, which impedes computation to assess its quality. Without such an assessment, deploying a jointly trained model is potentially illegal. Regulations, such as the European Union\u2019s General Data Protection Regulation (GDPR), require organizations to be legally responsible for the errors, bias, or discrimination caused by their machine learning models. Hence, testing data quality emerges as an indispensable step in secure collaborative learning. However, performing distribution testing is prohibitively expensive using current techniques, as shown in our experiments. We present HOLMES, a protocol for performing distribution testing ef\ufb01ciently . In our experiments, compared with three non-trivial baselines, HOLMES achieves a speedup of more than 10 \u00d7 for classical distribution tests and up to 10 4 \u00d7 for multidimensional tests. The core of HOLMES is a hybrid protocol that integrates MPC with zero-knowledge proofs and a new ZK-friendly and naturally oblivious sketching algo-rithm for multidimensional tests, both with signi\ufb01cantly lower computational complexity and concrete execution costs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "95685556",
                    "name": "I-Tang Chang"
                },
                {
                    "authorId": "40518883",
                    "name": "Katerina Sotiraki"
                },
                {
                    "authorId": "2739794",
                    "name": "Weikeng Chen"
                },
                {
                    "authorId": "1741044",
                    "name": "Murat Kantarcioglu"
                },
                {
                    "authorId": "144963510",
                    "name": "Raluca A. Popa"
                }
            ]
        },
        {
            "paperId": "5ab6c3b0633d1ffb3ae7800596c895189e249306",
            "title": "Graph of Graphs: A New Knowledge Representation Mechanism for Graph Learning (Student Abstract)",
            "abstract": "Supervised graph classification is one of the most actively developing areas in machine learning (ML), with a broad range of domain applications, from social media to bioinformatics. Given a collection of graphs with categorical labels, the goal is to predict correct classes for unlabelled graphs. However, currently available ML tools view each such graph as a standalone entity and, as such, do not account for complex interdependencies among graphs. We propose a novel knowledge representation for graph learning called a {\\it Graph of Graphs} (GoG). The key idea is to construct a new abstraction where each graph in the collection is represented by a node, while an edge then reflects similarity among the graphs. Such similarity can be assessed via a suitable graph distance. As a result, the graph classification problem can be then reformulated as a node classification problem. We show that the proposed new knowledge representation approach not only improves classification performance but substantially enhances robustness against label perturbation attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2222511984",
                    "name": "Zhwiei Zhen"
                },
                {
                    "authorId": "2109199933",
                    "name": "Yuzhou Chen"
                },
                {
                    "authorId": "1741044",
                    "name": "Murat Kantarcioglu"
                },
                {
                    "authorId": "2497230",
                    "name": "Y. Gel"
                }
            ]
        },
        {
            "paperId": "8a7dc1630911808ff2469f3b53038ddf62a661bc",
            "title": "Evading Provenance-Based ML Detectors with Adversarial System Actions",
            "abstract": "The artifact evaluation process is designed to validate the repeatability and usability of the results presented in the research paper \"Evading Provenance-Based ML Detectors with Adversarial System Actions.\" The paper introduces P ROV N-INJA , a novel framework designed to discover adversarial samples, also known as gadgets, specifically tailored for path-based Intrusion Detection Systems (IDS) and Graph Neural Network-based IDS. The primary objective of P ROV N INJA is to identify actions that can successfully evade state-of-the-art IDSs. The evaluation process comprises two main components: training and testing the IDS and generating adversarial examples to evade the IDSs. As a valuable resource, the authors provide a GitHub link that grants access to the source code, data, and scripts necessary for reproducing the results described in the paper. By offering these artifacts, the researchers enable fellow researchers and practitioners to replicate and build upon their work in provenance-based ML detectors. The artifacts include comprehensive software, data, and scripts employed to generate the findings presented in the paper. The accessibility of the GitHub repository ensures transparency. It fosters collaboration among researchers, facilitating advancements in the domain of provenance-based ML detectors and contributing to the overall improvement of security systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218792397",
                    "name": "Kunal Mukherjee"
                },
                {
                    "authorId": "2219236005",
                    "name": "Joshua Wiedemeier"
                },
                {
                    "authorId": "49980880",
                    "name": "Tianhao Wang"
                },
                {
                    "authorId": "2111343761",
                    "name": "James Wei"
                },
                {
                    "authorId": "2156361901",
                    "name": "Feng Chen"
                },
                {
                    "authorId": "2169156186",
                    "name": "Muhyun Kim"
                },
                {
                    "authorId": "1741044",
                    "name": "Murat Kantarcioglu"
                },
                {
                    "authorId": "3344254",
                    "name": "Kangkook Jee"
                }
            ]
        },
        {
            "paperId": "b1a20c224fd6a736971919688f09c789e460ad13",
            "title": "Chainlet Orbits: Topological Address Embedding for the Bitcoin Blockchain",
            "abstract": "The rise of cryptocurrencies like Bitcoin, which enable transactions with a degree of pseudonymity, has led to a surge in various illicit activities, including ransomware payments and transactions on darknet markets. These illegal activities often utilize Bitcoin as the preferred payment method. However, current tools for detecting illicit behavior either rely on a few heuristics and laborious data collection processes or employ computationally inefficient graph neural network (GNN) models that are challenging to interpret. To overcome the computational and interpretability limitations of existing techniques, we introduce an effective solution called Chainlet Orbits. This approach embeds Bitcoin addresses by leveraging their topological characteristics in transactions. By employing our innovative address embedding, we investigate e-crime in Bitcoin networks by focusing on distinctive substructures that arise from illicit behavior. The results of our node classification experiments demonstrate superior performance compared to state-of-the-art methods, including both topological and GNN-based approaches. Moreover, our approach enables the use of interpretable and explainable machine learning models in as little as 15 minutes for most days on the Bitcoin transaction network.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220082386",
                    "name": "Poupak Azad"
                },
                {
                    "authorId": "101047611",
                    "name": "Baris Coskunuzer"
                },
                {
                    "authorId": "1741044",
                    "name": "Murat Kantarcioglu"
                },
                {
                    "authorId": "2486759",
                    "name": "C. Akcora"
                }
            ]
        },
        {
            "paperId": "ba9a9343cc577e599a94e1d6d70be564e77116ea",
            "title": "IoTFlowGenerator: Crafting Synthetic IoT Device Traffic Flows for Cyber Deception",
            "abstract": "Over the years, honeypots emerged as an important security tool to understand attacker intent and deceive attackers to spend time and resources. Recently, honeypots are being deployed for Internet of things (IoT) devices to lure attackers, and learn their behavior. However, most of the existing IoT honeypots, even the high interaction ones, are easily detected by an attacker who can observe honeypot traffic due to lack of real network traffic originating from the honeypot. This implies that, to build better honeypots and enhance cyber deception capabilities, IoT honeypots need to generate realistic network traffic flows. \nTo achieve this goal, we propose a novel deep learning based approach for generating traffic flows that mimic real network traffic due to user and IoT device interactions.A key technical challenge that our approach overcomes is scarcity of device-specific IoT traffic data to effectively train a generator.We address this challenge by leveraging a core generative adversarial learning algorithm for sequences along with domain specific knowledge common to IoT devices.Through an extensive experimental evaluation with 18 IoT devices, we demonstrate that the proposed synthetic IoT traffic generation tool significantly outperforms state of the art sequence and packet generators in remaining indistinguishable from real traffic even to an adaptive attacker.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2215828726",
                    "name": "Joseph Bao"
                },
                {
                    "authorId": "1741044",
                    "name": "Murat Kantarcioglu"
                },
                {
                    "authorId": "1699600",
                    "name": "Yevgeniy Vorobeychik"
                },
                {
                    "authorId": "145231705",
                    "name": "Charles A. Kamhoua"
                }
            ]
        },
        {
            "paperId": "f6861ce39bad377f05337653eae78102f2c0a510",
            "title": "Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features",
            "abstract": "The black-box nature of complex Neural Network (NN)-based models has hindered their widespread adoption in security domains due to the lack of logical explanations and actionable follow-ups for their predictions. To enhance the transparency and accountability of Graph Neural Network (GNN) security models used in system provenance analysis, we propose PROVEXPLAINER, a framework for projecting abstract GNN decision boundaries onto interpretable feature spaces. We first replicate the decision-making process of GNNbased security models using simpler and explainable models such as Decision Trees (DTs). To maximize the accuracy and fidelity of the surrogate models, we propose novel graph structural features founded on classical graph theory and enhanced by extensive data study with security domain knowledge. Our graph structural features are closely tied to problem-space actions in the system provenance domain, which allows the detection results to be explained in descriptive, human language. PROVEXPLAINER allowed simple DT models to achieve 95% fidelity to the GNN on program classification tasks with general graph structural features, and 99% fidelity on malware detection tasks with a task-specific feature package tailored for direct interpretation. The explanations for malware classification are demonstrated with case studies of five real-world malware samples across three malware families.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218792397",
                    "name": "Kunal Mukherjee"
                },
                {
                    "authorId": "2219236005",
                    "name": "Joshua Wiedemeier"
                },
                {
                    "authorId": "49980880",
                    "name": "Tianhao Wang"
                },
                {
                    "authorId": "2169156186",
                    "name": "Muhyun Kim"
                },
                {
                    "authorId": "2156361901",
                    "name": "Feng Chen"
                },
                {
                    "authorId": "1741044",
                    "name": "Murat Kantarcioglu"
                },
                {
                    "authorId": "3344254",
                    "name": "Kangkook Jee"
                }
            ]
        }
    ]
}