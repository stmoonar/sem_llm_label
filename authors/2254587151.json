{
    "authorId": "2254587151",
    "papers": [
        {
            "paperId": "c4c0587bf5623187484704ff70a333ed8ecb4166",
            "title": "Private and Secure Fuzzy Name Matching",
            "abstract": "Modern financial institutions rely on data for many operations, including a need to drive efficiency, enhance services and prevent financial crime. Data sharing across an organisation or between institutions can facilitate rapid, evidence-based decision making, including identifying money laundering and fraud. However, data privacy regulations impose restrictions on data sharing. Privacy-enhancing technologies are being increasingly employed to allow organisations to derive shared intelligence while ensuring regulatory compliance. This paper examines the case in which regulatory restrictions mean a party cannot share data on accounts of interest with another (internal or external) party to identify people that hold an account in each dataset. We observe that the names of account holders may be recorded differently in each data set. We introduce a novel privacy-preserving approach for fuzzy name matching across institutions, employing fully homomorphic encryption with locality-sensitive hashing. The efficiency of the approach is enhanced using a clustering mechanism. The practicality and effectiveness of the proposed approach are evaluated using different datasets. Experimental results demonstrate it takes around 100 and 1000 seconds to search 1000 names from 10k and 100k names, respectively. Moreover, the proposed approach exhibits significant improvement in reducing communication overhead by 30-300 times, using clustering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2037396676",
                    "name": "Harsh Kasyap"
                },
                {
                    "authorId": "1581430664",
                    "name": "U. Atmaca"
                },
                {
                    "authorId": "2258665264",
                    "name": "Carsten Maple"
                },
                {
                    "authorId": "2254587151",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2313735131",
                    "name": "Jiancong He"
                }
            ]
        },
        {
            "paperId": "f10ff6146c91677584d61a34ac2a41ba8af72360",
            "title": "Private and Efficient Federated Numerical Aggregation",
            "abstract": "Aggregating data generated locally by smartphones and other edge devices is vital for distributed applications and system-performance monitoring but carries significant risks when data is mishandled. In this work, we develop and deploy numerical aggregation protocols that ( \ud835\udc56 ) are compatible with several notions of privacy, ( \ud835\udc56\ud835\udc56 ) come with attractive accuracy-privacy tradeoffs when used with differential privacy , ( \ud835\udc56\ud835\udc56\ud835\udc56 ) empirically improve upon prior protocols. Our protocols promote a basic tenet of privacy \u2014 not sharing unnecessary information . For each private value, at most one bit is used. This supports ( \ud835\udc56 ) privacy metering that enables privacy controls and ( \ud835\udc56\ud835\udc56 ) worst-case guarantees not covered by differential privacy. We emphasize ease of implementation, compatibility with existing infrastructure, and compelling empirical performance. We report on our experience deploying the method for online aggregation in an industrial context.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2254587151",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2292423394",
                    "name": "Igor L. Markov"
                },
                {
                    "authorId": "47490833",
                    "name": "H. Srinivas"
                }
            ]
        },
        {
            "paperId": "550891af899eb38af1896aa1f8add8a32e98733a",
            "title": "FLAIM: AIM-based Synthetic Data Generation in the Federated Setting",
            "abstract": "Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We first show that it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that maintains a private proxy of heterogeneity. We simulate our methods across a range of benchmark datasets under different degrees of heterogeneity and show we can improve utility while reducing overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388516394",
                    "name": "Samuel Maddock"
                },
                {
                    "authorId": "2254587151",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2262445634",
                    "name": "Carsten Maple"
                }
            ]
        },
        {
            "paperId": "5e7c0f64a0f228362c69951c2157245ff227258d",
            "title": "Federated Experiment Design under Distributed Differential Privacy",
            "abstract": "Experiment design has a rich history dating back over a century and has found many critical applications across various fields since then. The use and collection of users' data in experiments often involve sensitive personal information, so additional measures to protect individual privacy are required during data collection, storage, and usage. In this work, we focus on the rigorous protection of users' privacy (under the notion of differential privacy (DP)) while minimizing the trust toward service providers. Specifically, we consider the estimation of the average treatment effect (ATE) under DP, while only allowing the analyst to collect population-level statistics via secure aggregation, a distributed protocol enabling a service provider to aggregate information without accessing individual data. Although a vital component in modern A/B testing workflows, private distributed experimentation has not previously been studied. To achieve DP, we design local privatization mechanisms that are compatible with secure aggregation and analyze the utility, in terms of the width of confidence intervals, both asymptotically and non-asymptotically. We show how these mechanisms can be scaled up to handle the very large number of participants commonly found in practice. In addition, when introducing DP noise, it is imperative to cleverly split privacy budgets to estimate both the mean and variance of the outcomes and carefully calibrate the confidence intervals according to the DP noise. Last, we present comprehensive experimental evaluations of our proposed schemes and show the privacy-utility trade-offs in experiment design.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2109085368",
                    "name": "Wei-Ning Chen"
                },
                {
                    "authorId": "2254587151",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2528900",
                    "name": "Akash Bharadwaj"
                },
                {
                    "authorId": "2265647990",
                    "name": "Peter Romov"
                },
                {
                    "authorId": "2266472645",
                    "name": "Ayfer \u00d6zg\u00fcr"
                }
            ]
        },
        {
            "paperId": "f352fe55fe7c795c2d904ebdd06bf91bf2593268",
            "title": "Interactive Proofs For Differentially Private Counting",
            "abstract": "Differential Privacy (DP) is often presented as a strong privacy-enhancing technology with broad applicability and advocated as a de facto standard for releasing aggregate statistics on sensitive data. However, in many embodiments, DP introduces a new attack surface: a malicious entity entrusted with releasing statistics could manipulate the results and use the randomness of DP as a convenient smokescreen to mask its nefariousness. Since revealing the random noise would obviate the purpose of introducing it, the miscreant may have a perfect alibi. To close this loophole, we introduce the idea of Interactive Proofs For Differential Privacy, which requires the publishing entity to output a zero knowledge proof that convinces an efficient verifier that the output is both DP and reliable. Such a definition might seem unachievable, as a verifier must validate that DP randomness was generated faithfully without learning anything about the randomness itself. We resolve this paradox by carefully mixing private and public randomness to compute verifiable DP counting queries with theoretical guarantees and show that it is also practical for real-world deployment. We also demonstrate that computational assumptions are necessary by showing a separation between information-theoretic DP and computational DP under our definition of verifiability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182220719",
                    "name": "Ari Biswas"
                },
                {
                    "authorId": "2254587151",
                    "name": "Graham Cormode"
                }
            ]
        },
        {
            "paperId": "5a4dfb3e019e5fc928a7f0b25ebd03392184a665",
            "title": "Empirical Investigation on Certain Anonymization Strategies for Preserving Privacy of Social Network Data",
            "abstract": "In recent years, the rapid growth of web applications developed the need for private data to be published. Most of the social network data necessitates the data to be available for easy access and conversion of data to graph structure to re-identify sensitive labels of individuals became an impeccable issue. In this paper, we have made a detailed surveyed about the existing techniques that preserve the sensitive data in social network. It is observed that preserving the graph structure and label re-identification by adding some noise nodes to the graph makes significant change in degree is inferred from existing techniques. The anonymization methods for preservation of the private sensitive data based on cluster based approach and graph modification approaches are studied in detail.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2286441343",
                    "name": "Lars Backstrom"
                },
                {
                    "authorId": "2254587151",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2246247413",
                    "name": "Balachander Krishnamurthy"
                }
            ]
        },
        {
            "paperId": "dd5600845edb4795d43d7a63e98081666d9fb9c5",
            "title": "Information Cost Tradeoffs for Augmented Index and Streaming Language Recognition",
            "abstract": "This paper makes three main contributions to the theory of communication complexity and stream computation. First, we present new bounds on the information complexity of AUGMENTED-INDEX. In contrast to analogous results for INDEX by Jain, Radhakrishnan and Sen [J. ACM, 2009], we have to overcome the significant technical challenge that protocols for AUGMENTED-INDEX may violate the ``rectangle property'' due to the inherent input sharing. Second, we use these bounds to resolve an open problem of Magniez, Mathieu and Nayak [STOC, 2010] on the multi-pass complexity of recognizing Dyck languages. This results in a natural separation between the standard multi-pass model and the multi-pass model that permits reverse passes. Third, we present the first passive memory checkers that verify the interaction transcripts of priority queues, stacks, and double-ended queues. We obtain tight upper and lower bounds for these problems, thereby addressing an important sub-class of the memory checking framework of Blum et al. [Algorithmica, 1994].",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285431797",
                    "name": "Amit Chakrabarti"
                },
                {
                    "authorId": "2254587151",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1876542",
                    "name": "Ranganath Kondapally"
                },
                {
                    "authorId": "2285262255",
                    "name": "Andrew McGregor"
                }
            ]
        }
    ]
}