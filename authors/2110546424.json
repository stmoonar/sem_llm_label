{
    "authorId": "2110546424",
    "papers": [
        {
            "paperId": "01911d5badfd2397fba9f0b71c7873071c392b33",
            "title": "HR-MultiWOZ: A Task Oriented Dialogue (TOD) Dataset for HR LLM Agent",
            "abstract": "Recent advancements in Large Language Models (LLMs) have been reshaping Natural Language Processing (NLP) task in several domains. Their use in the field of Human Resources (HR) has still room for expansions and could be beneficial for several time consuming tasks. Examples such as time-off submissions, medical claims filing, and access requests are noteworthy, but they are by no means the sole instances. However the aforementioned developments must grapple with the pivotal challenge of constructing a high-quality training dataset. On one hand, most conversation datasets are solving problems for customers not employees. On the other hand, gathering conversations with HR could raise privacy concerns. To solve it, we introduce HR-Multiwoz, a fully-labeled dataset of 550 conversations spanning 10 HR domains. Our work has the following contributions:(1) It is the first labeled open-sourced conversation dataset in the HR domain for NLP research. (2) It provides a detailed recipe for the data generation procedure along with data analysis and human evaluations. The data generation pipeline is transferrable and can be easily adapted for labeled conversation data generation in other domains. (3) The proposed data-collection pipeline is mostly based on LLMs with minimal human involvement for annotation, which is time and cost-efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2282498869",
                    "name": "Zicheng Huang"
                },
                {
                    "authorId": "2261645232",
                    "name": "Wenxiang Hu"
                },
                {
                    "authorId": "2288381545",
                    "name": "Xi Fang"
                },
                {
                    "authorId": "2282467353",
                    "name": "Rajesh Kumar Cherukuri"
                },
                {
                    "authorId": "40511871",
                    "name": "Naumaan Nayyar"
                },
                {
                    "authorId": "2282467375",
                    "name": "Lorenzo Malandri"
                },
                {
                    "authorId": "1757518",
                    "name": "Srinivasan H. Sengamedu"
                }
            ]
        },
        {
            "paperId": "2046b2da23eb2f79744eb391d902da9cedf87947",
            "title": "Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding - A Survey",
            "abstract": "Recent breakthroughs in large language modeling have facilitated rigorous exploration of their application in diverse tasks related to tabular data modeling, such as prediction, tabular data synthesis, question answering, and table understanding. Each task presents unique challenges and opportunities. However, there is currently a lack of comprehensive review that summarizes and compares the key techniques, metrics, datasets, models, and optimization approaches in this research domain. This survey aims to address this gap by consolidating recent progress in these areas, offering a thorough survey and taxonomy of the datasets, metrics, and methodologies utilized. It identifies strengths, limitations, unexplored territories, and gaps in the existing literature, while providing some insights for future research directions in this vital and rapidly evolving field. It also provides relevant code and datasets references. Through this comprehensive review, we hope to provide interested readers with pertinent references and insightful perspectives, empowering them with the necessary tools and knowledge to effectively navigate and address the prevailing challenges in the field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2288381545",
                    "name": "Xi Fang"
                },
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2287992265",
                    "name": "Fiona Anting Tan"
                },
                {
                    "authorId": "2258747730",
                    "name": "Jiani Zhang"
                },
                {
                    "authorId": "2287978009",
                    "name": "Ziqing Hu"
                },
                {
                    "authorId": "2288038766",
                    "name": "Yanjun Qi"
                },
                {
                    "authorId": "2287927391",
                    "name": "Scott Nickleach"
                },
                {
                    "authorId": "2287926360",
                    "name": "Diego Socolinsky"
                },
                {
                    "authorId": "2287927370",
                    "name": "Srinivasan H. Sengamedu"
                },
                {
                    "authorId": "2263543517",
                    "name": "Christos Faloutsos"
                }
            ]
        },
        {
            "paperId": "65e9acc0b4253b3e5540487be03861b5aeb9e9c5",
            "title": "PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large Language Models",
            "abstract": "Recent advances in large language models (LLMs) demonstrate that their capabilities are comparable, or even superior, to humans in many tasks in natural language processing. Despite this progress, LLMs are still inadequate at social-cognitive reasoning, which humans are naturally good at. Drawing inspiration from psychological research on the links between certain personality traits and Theory-of-Mind (ToM) reasoning, and from prompt engineering research on the hyper-sensitivity of prompts in affecting LLMs capabilities, this study investigates how inducing personalities in LLMs using prompts affects their ToM reasoning capabilities. Our findings show that certain induced personalities can significantly affect the LLMs' reasoning capabilities in three different ToM tasks. In particular, traits from the Dark Triad have a larger variable effect on LLMs like GPT-3.5, Llama 2, and Mistral across the different ToM tasks. We find that LLMs that exhibit a higher variance across personality prompts in ToM also tends to be more controllable in personality tests: personality traits in LLMs like GPT-3.5, Llama 2 and Mistral can be controllably adjusted through our personality prompts. In today's landscape where role-play is a common strategy when using LLMs, our research highlights the need for caution, as models that adopt specific personas with personalities potentially also alter their reasoning abilities in an unexpected manner.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287992265",
                    "name": "Fiona Anting Tan"
                },
                {
                    "authorId": "2242615919",
                    "name": "G. Yeo"
                },
                {
                    "authorId": "2261413304",
                    "name": "Fanyou Wu"
                },
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2212131028",
                    "name": "Vinija Jain"
                },
                {
                    "authorId": "2275226689",
                    "name": "Aman Chadha"
                },
                {
                    "authorId": "2798221",
                    "name": "Kokil Jaidka"
                },
                {
                    "authorId": "2290046479",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2282936905",
                    "name": "See-Kiong Ng"
                }
            ]
        },
        {
            "paperId": "797676dd84b0dff0706c45dca1cdc63a31d23c68",
            "title": "Synthesizing Conversations from Unlabeled Documents using Automatic Response Segmentation",
            "abstract": "In this study, we tackle the challenge of inadequate and costly training data that has hindered the development of conversational question answering (ConvQA) systems. Enterprises have a large corpus of diverse internal documents. Instead of relying on a searching engine, a more compelling approach for people to comprehend these documents is to create a dialogue system. In this paper, we propose a robust dialog synthesising method. We learn the segmentation of data for the dialog task instead of using segmenting at sentence boundaries. The synthetic dataset generated by our proposed method achieves superior quality when compared to WikiDialog, as assessed through machine and human evaluations. By employing our inpainted data for ConvQA retrieval system pre-training, we observed a notable improvement in performance across OR-QuAC benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261413304",
                    "name": "Fanyou Wu"
                },
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2262444977",
                    "name": "Chandan K. Reddy"
                },
                {
                    "authorId": "2287927370",
                    "name": "Srinivasan H. Sengamedu"
                }
            ]
        },
        {
            "paperId": "29e8832f2fcfda85195313c69d56e388f9eec5d9",
            "title": "Approximate, Adapt, Anonymize (3A): a Framework for Privacy Preserving Training Data Release for Machine Learning",
            "abstract": "The availability of large amounts of informative data is crucial for successful machine learning. However, in domains with sensitive information, the release of high-utility data which protects the privacy of individuals has proven challenging. Despite progress in differential privacy and generative modeling for privacy-preserving data release in the literature, only a few approaches optimize for machine learning utility: most approaches only take into account statistical metrics on the data itself and fail to explicitly preserve the loss metrics of machine learning models that are to be subsequently trained on the generated data. In this paper, we introduce a data release framework, 3A (Approximate, Adapt, Anonymize), to maximize data utility for machine learning, while preserving differential privacy. We also describe a specific implementation of this framework that leverages mixture models to approximate, kernel-inducing points to adapt, and Gaussian differential privacy to anonymize a dataset, in order to ensure that the resulting data is both privacy-preserving and high utility. We present experimental evidence showing minimal discrepancy between performance metrics of models trained on real versus privatized datasets, when evaluated on held-out real data. We also compare our results with several privacy-preserving synthetic data generation models (such as differentially private generative adversarial networks), and report significant increases in classification performance metrics compared to state-of-the-art models. These favorable comparisons show that the presented framework is a promising direction of research, increasing the utility of low-risk synthetic data release for machine learning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32247373",
                    "name": "Tamas Madl"
                },
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2065954920",
                    "name": "Olivia Choudhury"
                },
                {
                    "authorId": "2221128497",
                    "name": "Matthew Howard"
                }
            ]
        },
        {
            "paperId": "59d576afc7903eb02251d35ee051a55d1a8a7d31",
            "title": "Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded Dialogue Generation",
            "abstract": "In this work, we propose sequence-level certainty as a common theme over hallucination in Knowledge Grounded Dialogue Generation (KGDG). We explore the correlation between the level of hallucination in model responses and two types of sequence-level certainty: probabilistic certainty and semantic certainty. Empirical results reveal that higher levels of both types of certainty in model responses are correlated with lower levels of hallucination. We further propose Certainty-based Response Ranking (CRR), a decoding-time hallucination mitigation method that samples several response candidates, ranks them based on sequence-level certainty, and outputs the response with the highest certainty level. Aligning with our definitions of sequence-level certainty, we design 2 types of CRR approaches: Probabilistic CRR (P-CRR) and Semantic CRR (S-CRR). P-CRR ranks individually sampled model responses using the arithmetic mean log-probability of the entire sequence. S-CRR approaches certainty estimation from meaning-space, and ranks model response candidates based on their semantic certainty level as measured by an entailment-based Agreement Score (AS). Through extensive experiments across 3 KGDG datasets, 3 decoding methods, and 4 KGDG models, we validate the effectiveness of CRR for reducing hallucination in KGDG task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2263260807",
                    "name": "Yixin Wan"
                },
                {
                    "authorId": "2261413304",
                    "name": "Fanyou Wu"
                },
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "1757518",
                    "name": "Srinivasan H. Sengamedu"
                }
            ]
        },
        {
            "paperId": "708bba8c97394a3882937423705601432817683b",
            "title": "FFPDG: Fast, Fair and Private Data Generation",
            "abstract": "Generative modeling has been used frequently in synthetic data generation. Fairness and privacy are two big concerns for synthetic data. Although Recent GAN [\\cite{goodfellow2014generative}] based methods show good results in preserving privacy, the generated data may be more biased. At the same time, these methods require high computation resources. In this work, we design a fast, fair, flexible and private data generation method. We show the effectiveness of our method theoretically and empirically. We show that models trained on data generated by the proposed method can perform well (in inference stage) on real application scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "1845781483",
                    "name": "Jinjin Zhao"
                },
                {
                    "authorId": "2826055",
                    "name": "F. Iannacci"
                },
                {
                    "authorId": null,
                    "name": "Bo Wang"
                }
            ]
        },
        {
            "paperId": "8d87968533b327065d0a3331ad529f501c097e0e",
            "title": "vONTSS: vMF based semi-supervised neural topic modeling with optimal transport",
            "abstract": "Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state-of-the-art weakly supervised text classification method while achieving similar classification performance. We further prove the equivalence of optimal transport loss and cross-entropy loss at the global minimum.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2144812478",
                    "name": "Xiaoyu Jiang"
                },
                {
                    "authorId": "1757518",
                    "name": "Srinivasan H. Sengamedu"
                },
                {
                    "authorId": "2171429265",
                    "name": "Francis Iannacci"
                },
                {
                    "authorId": "1845781483",
                    "name": "Jinjin Zhao"
                }
            ]
        },
        {
            "paperId": "bf45a7a3d64ce9abe10dbbab853a7c351f7e5a90",
            "title": "KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation",
            "abstract": "In text classification tasks, fine tuning pretrained language models like BERT and GPT-3 yields competitive accuracy; however, both methods require pretraining on large text datasets. In contrast, general topic modeling methods possess the advantage of analyzing documents to extract meaningful patterns of words without the need of pretraining. To leverage topic modeling's unsupervised insights extraction on text classification tasks, we develop the Knowledge Distillation Semi-supervised Topic Modeling (KDSTM). KDSTM requires no pretrained embeddings, few labeled documents and is efficient to train, making it ideal under resource constrained settings. Across a variety of datasets, our method outperforms existing supervised topic modeling methods in classification accuracy, robustness and efficiency and achieves similar performance compare to state of the art weakly supervised text classification methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2144812478",
                    "name": "Xiaoyu Jiang"
                },
                {
                    "authorId": "2053066225",
                    "name": "Jay Desai"
                },
                {
                    "authorId": "2153287302",
                    "name": "Bin Han"
                },
                {
                    "authorId": "2171428787",
                    "name": "Fuqin Yan"
                },
                {
                    "authorId": "2171429265",
                    "name": "Francis Iannacci"
                }
            ]
        },
        {
            "paperId": "ceba8c79009800d151c81fd8db990370c91a04f1",
            "title": "S2vNTM: Semi-supervised vMF Neural Topic Modeling",
            "abstract": "Language model based methods are powerful techniques for text classification. However, the models have several shortcomings. (1) It is difficult to integrate human knowledge such as keywords. (2) It needs a lot of resources to train the models. (3) It relied on large text data to pretrain. In this paper, we propose Semi-Supervised vMF Neural Topic Modeling (S2vNTM) to overcome these difficulties. S2vNTM takes a few seed keywords as input for topics. S2vNTM leverages the pattern of keywords to identify potential topics, as well as optimize the quality of topics' keywords sets. Across a variety of datasets, S2vNTM outperforms existing semi-supervised topic modeling methods in classification accuracy with limited keywords provided. S2vNTM is at least twice as fast as baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110546424",
                    "name": "Weijie Xu"
                },
                {
                    "authorId": "2053066225",
                    "name": "Jay Desai"
                },
                {
                    "authorId": "1757518",
                    "name": "Srinivasan H. Sengamedu"
                },
                {
                    "authorId": "2144812478",
                    "name": "Xiaoyu Jiang"
                },
                {
                    "authorId": "2171429265",
                    "name": "Francis Iannacci"
                }
            ]
        }
    ]
}