{
    "authorId": "1826312",
    "papers": [
        {
            "paperId": "064c4b1b07ea637cc02de0ba9a91281e6a1dee82",
            "title": "Evaluating Subword Tokenization: Alien Subword Composition and OOV Generalization Challenge",
            "abstract": "The popular subword tokenizers of current language models, such as Byte-Pair Encoding (BPE), are known not to respect morpheme boundaries, which affects the downstream performance of the models. While many improved tokenization algorithms have been proposed, their evaluation and cross-comparison is still an open problem. As a solution, we propose a combined intrinsic-extrinsic evaluation framework for subword tokenization. Intrinsic evaluation is based on our new UniMorph Labeller tool that classifies subword tokenization as either morphological or alien. Extrinsic evaluation, in turn, is performed via the Out-of-Vocabulary Generalization Challenge 1.0 benchmark, which consists of three newly specified downstream text classification tasks. Our empirical findings show that the accuracy of UniMorph Labeller is 98%, and that, in all language models studied (including ALBERT, BERT, RoBERTa, and DeBERTa), alien tokenization leads to poorer generalizations compared to morphological tokenization for semantic compositionality of word meanings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2049136",
                    "name": "Khuyagbaatar Batsuren"
                },
                {
                    "authorId": "2311126",
                    "name": "Ekaterina Vylomova"
                },
                {
                    "authorId": "74461595",
                    "name": "Verna Dankers"
                },
                {
                    "authorId": "2297770126",
                    "name": "Tsetsuukhei Delgerbaatar"
                },
                {
                    "authorId": "2287928365",
                    "name": "Omri Uzan"
                },
                {
                    "authorId": "1826312",
                    "name": "Yuval Pinter"
                },
                {
                    "authorId": "2297812276",
                    "name": "G\u00e1bor Bella"
                }
            ]
        },
        {
            "paperId": "5e2c4264bea7638d01c2d65e7893383154989c67",
            "title": "BiVert: Bidirectional Vocabulary Evaluation Using Relations for Machine Translation",
            "abstract": "Neural machine translation (NMT) has progressed rapidly in the past few years, promising improvements and quality translations for different languages. Evaluation of this task is crucial to determine the quality of the translation. Overall, insufficient emphasis is placed on the actual sense of the translation in traditional methods. We propose a bidirectional semantic-based evaluation method designed to assess the sense distance of the translation from the source text. This approach employs the comprehensive multilingual encyclopedic dictionary BabelNet. Through the calculation of the semantic distance between the source and its back translation of the output, our method introduces a quantifiable approach that empowers sentence comparison on the same linguistic level. Factual analysis shows a strong correlation between the average evaluation scores generated by our method and the human assessments across various machine translation systems for English-German language pair. Finally, our method proposes a new multilingual approach to rank MT systems without the need for parallel corpora.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2290074273",
                    "name": "Carinne Cherf"
                },
                {
                    "authorId": "1826312",
                    "name": "Yuval Pinter"
                }
            ]
        },
        {
            "paperId": "6f0a78365bbdc9c4d210ac7788eb7145ba706cf1",
            "title": "Protecting Privacy in Classifiers by Token Manipulation",
            "abstract": "Using language models as a remote service entails sending private information to an untrusted provider. In addition, potential eavesdroppers can intercept the messages, thereby exposing the information. In this work, we explore the prospects of avoiding such data exposure at the level of text manipulation. We focus on text classification models, examining various token mapping and contextualized manipulation functions in order to see whether classifier accuracy may be maintained while keeping the original text unrecoverable. We find that although some token mapping functions are easy and straightforward to implement, they heavily influence performance on the downstream task, and via a sophisticated attacker can be reconstructed. In comparison, the contextualized manipulation provides an improvement in performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151491413",
                    "name": "Re'em Harel"
                },
                {
                    "authorId": "2309170707",
                    "name": "Yair Elboher"
                },
                {
                    "authorId": "1826312",
                    "name": "Yuval Pinter"
                }
            ]
        },
        {
            "paperId": "762f85c97733f1ee01e45ad0471982494eac8c66",
            "title": "OMPar: Automatic Parallelization with AI-Driven Source-to-Source Compilation",
            "abstract": "Manual parallelization of code remains a significant challenge due to the complexities of modern software systems and the widespread adoption of multi-core architectures. This paper introduces OMPar, an AI-driven tool designed to automate the parallelization of C/C++ code using OpenMP pragmas. OMPar integrates Large Language Models (LLMs) through two key components: OMPify, which assesses loop parallelization potential, and MonoCoder-OMP, a new fine-tuned model which generates precise OpenMP pragmas. The evaluation of OMPar follows the same rigorous process applied to traditional tools like source-to-source AutoPar and ICPC compilers: (1) ensuring the generated code compiles and runs correctly in serial form, (2) assessing performance with the gradual addition of threads and corresponding physical cores, and (3) verifying and validating the correctness of the code's output. Benchmarks from HeCBench and ParEval are used to evaluate accuracy and performance. Experimental results demonstrate that OMPar significantly outperforms traditional methods, achieving higher accuracy in identifying parallelizable loops and generating efficient pragmas. Beyond accuracy, OMPar offers advantages such as the ability to work on partial or incomplete codebases and the capacity to continuously learn from new code patterns, enhancing its parallelization capabilities over time. These results underscore the potential of LLMs in revolutionizing automatic parallelization techniques, paving the way for more efficient and scalable parallel computing systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2217344668",
                    "name": "Tal Kadosh"
                },
                {
                    "authorId": "1789829",
                    "name": "N. Hasabnis"
                },
                {
                    "authorId": "2130034885",
                    "name": "Prema Soundararajan"
                },
                {
                    "authorId": "3369353",
                    "name": "Vy A. Vo"
                },
                {
                    "authorId": "2275602826",
                    "name": "Mihai Capota"
                },
                {
                    "authorId": "2256997701",
                    "name": "Nesreen K. Ahmed"
                },
                {
                    "authorId": "1826312",
                    "name": "Yuval Pinter"
                },
                {
                    "authorId": "151504666",
                    "name": "Gal Oren"
                }
            ]
        },
        {
            "paperId": "8442e9a54844ccfa1519d04a0eb8fbc635aa82f0",
            "title": "MPIrigen: MPI Code Generation through Domain-Specific Language Models",
            "abstract": "The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored. This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose an innovative preprocessing for completion only after observing the whole code, thus enabling better completion with a wider context. Comparative analysis against GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation method, demonstrates that MPIrigen excels in generating accurate MPI functions up to 0.8 accuracy in location and function predictions, and with more than 0.9 accuracy for argument predictions. The success of this tailored solution underscores the importance of domain-specific fine-tuning in optimizing language models for parallel computing code generation, paving the way for a new generation of automatic parallelization tools. The sources of this work are available at our GitHub MPIrigen repository: https://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2174442010",
                    "name": "Nadav Schneider"
                },
                {
                    "authorId": "1789829",
                    "name": "N. Hasabnis"
                },
                {
                    "authorId": "3369353",
                    "name": "Vy A. Vo"
                },
                {
                    "authorId": "2217344668",
                    "name": "Tal Kadosh"
                },
                {
                    "authorId": "2232602176",
                    "name": "Neva Krien"
                },
                {
                    "authorId": "2284063149",
                    "name": "Mihai Capotua"
                },
                {
                    "authorId": "2275600490",
                    "name": "Abdul Wasay"
                },
                {
                    "authorId": "2084732439",
                    "name": "G. Tamir"
                },
                {
                    "authorId": "2284062901",
                    "name": "Ted Willke"
                },
                {
                    "authorId": "2256997701",
                    "name": "Nesreen K. Ahmed"
                },
                {
                    "authorId": "1826312",
                    "name": "Yuval Pinter"
                },
                {
                    "authorId": "2275602824",
                    "name": "Timothy Mattson"
                },
                {
                    "authorId": "151504666",
                    "name": "Gal Oren"
                }
            ]
        },
        {
            "paperId": "b10e5a2b622a2da27b1a888a117ad5d9237a31e1",
            "title": "Greed is All You Need: An Evaluation of Tokenizer Inference Methods",
            "abstract": "While subword tokenizers such as BPE and WordPiece are typically used to build vocabularies for NLP models, the method of decoding text into a sequence of tokens from these vocabularies is often left unspecified, or ill-suited to the method in which they were constructed. We provide a controlled analysis of seven tokenizer inference methods across four different algorithms and three vocabulary sizes, performed on a novel intrinsic evaluation suite we curated for English, combining measures rooted in morphology, cognition, and information theory. We show that for the most commonly used tokenizers, greedy inference performs surprisingly well; and that SaGe, a recently-introduced contextually-informed tokenizer, outperforms all others on morphological alignment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287928365",
                    "name": "Omri Uzan"
                },
                {
                    "authorId": "2287933541",
                    "name": "Craig W. Schmidt"
                },
                {
                    "authorId": "2266398345",
                    "name": "Chris Tanner"
                },
                {
                    "authorId": "1826312",
                    "name": "Yuval Pinter"
                }
            ]
        },
        {
            "paperId": "c74326259a24bbba3e5130f0ee42a546ea31301b",
            "title": "Tokenization Is More Than Compression",
            "abstract": "Tokenization is a foundational step in natural language processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document's text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287933541",
                    "name": "Craig W. Schmidt"
                },
                {
                    "authorId": "2266430123",
                    "name": "Varshini Reddy"
                },
                {
                    "authorId": "2288058798",
                    "name": "Haoran Zhang"
                },
                {
                    "authorId": "2287927206",
                    "name": "Alec Alameddine"
                },
                {
                    "authorId": "2287928365",
                    "name": "Omri Uzan"
                },
                {
                    "authorId": "1826312",
                    "name": "Yuval Pinter"
                },
                {
                    "authorId": "2266398345",
                    "name": "Chris Tanner"
                }
            ]
        },
        {
            "paperId": "f914e6e4cd65a4f04a78d4f6af2fd8aa093b79cd",
            "title": "An Analysis of BPE Vocabulary Trimming in Neural Machine Translation",
            "abstract": "We explore threshold vocabulary trimming in Byte-Pair Encoding subword tokenization, a tokenization postprocessing step that replaces rare subwords with their component subwords. The technique is available in popular tokenization libraries but has not been subjected to rigorous scientific scrutiny. While the removal of rare subwords is suggested as best practice in model implementations, both as a means to reduce model size and for improving model performance through robustness, our experiments indicate that, across a large space of hyperparameter settings, vocabulary trimming fails to consistently improve model performance, and is even prone to incurring heavy degradation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51122261",
                    "name": "Marco Cognetta"
                },
                {
                    "authorId": "114789302",
                    "name": "Tatsuya Hiraoka"
                },
                {
                    "authorId": "2269460776",
                    "name": "Naoaki Okazaki"
                },
                {
                    "authorId": "2066518243",
                    "name": "Rico Sennrich"
                },
                {
                    "authorId": "1826312",
                    "name": "Yuval Pinter"
                }
            ]
        },
        {
            "paperId": "160ae9b2cf9c926d67a7b508d783a4f2c61f9cb8",
            "title": "Universal NER: A Gold-Standard Multilingual Named Entity Recognition Benchmark",
            "abstract": "We introduce Universal NER (UNER), an open, community-driven project to develop gold-standard NER benchmarks in many languages. The overarching goal of UNER is to provide high-quality, cross-lingually consistent annotations to facilitate and standardize multilingual NER research. UNER v1 contains 19 datasets annotated with named entities in a cross-lingual consistent schema across 13 diverse languages. In this paper, we detail the dataset creation and composition of UNER; we also provide initial modeling baselines on both in-language and cross-lingual learning settings. We will release the data, code, and fitted models to the public.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2266751453",
                    "name": "Stephen Mayhew"
                },
                {
                    "authorId": "3443287",
                    "name": "Terra Blevins"
                },
                {
                    "authorId": "2077166614",
                    "name": "Shuheng Liu"
                },
                {
                    "authorId": "2266751005",
                    "name": "Marek vSuppa"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "151472158",
                    "name": "Joseph Marvin Imperial"
                },
                {
                    "authorId": "2047947436",
                    "name": "B\u00f6rje F. Karlsson"
                },
                {
                    "authorId": "2266791223",
                    "name": "Peiqin Lin"
                },
                {
                    "authorId": "2266754955",
                    "name": "Nikola Ljubevsi'c"
                },
                {
                    "authorId": "13614871",
                    "name": "Lester James Validad Miranda"
                },
                {
                    "authorId": "2266751446",
                    "name": "Barbara Plank"
                },
                {
                    "authorId": "2003628072",
                    "name": "Arij Riabi"
                },
                {
                    "authorId": "1826312",
                    "name": "Yuval Pinter"
                }
            ]
        },
        {
            "paperId": "17d0cc24fd6a267e7fd59ae62054de3e5c552096",
            "title": "Analyzing Cognitive Plausibility of Subword Tokenization",
            "abstract": "Subword tokenization has become the de-facto standard for tokenization, although comparative evaluations of subword vocabulary quality across languages are scarce. Existing evaluation studies focus on the effect of a tokenization algorithm on the performance in downstream tasks, or on engineering criteria such as the compression rate. We present a new evaluation paradigm that focuses on the cognitive plausibility of subword tokenization. We analyze the correlation of the tokenizer output with the response time and accuracy of human performance on a lexical decision task. We compare three tokenization algorithms across several languages and vocabulary sizes. Our results indicate that the UnigramLM algorithm yields less cognitively plausible tokenization behavior and a worse coverage of derivational morphemes, in contrast with prior work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261086228",
                    "name": "Lisa Beinborn"
                },
                {
                    "authorId": "1826312",
                    "name": "Yuval Pinter"
                }
            ]
        }
    ]
}