{
    "authorId": "2066270999",
    "papers": [
        {
            "paperId": "2b17b4a5af897ef108386643cda60ebfe493fddb",
            "title": "Linear-Time Graph Neural Networks for Scalable Recommendations",
            "abstract": "In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users. The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions. Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems. Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages. Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods. In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender systems to achieve comparable scalability as classic MF approaches while maintaining GNNs' powerful expressiveness for superior prediction accuracy. Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm. Our implementation based on PyTorch is available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284929250",
                    "name": "Jiahao Zhang"
                },
                {
                    "authorId": "2066270999",
                    "name": "Rui Xue"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2285630243",
                    "name": "Xin Xu"
                },
                {
                    "authorId": "2286126060",
                    "name": "Qing Li"
                },
                {
                    "authorId": "2284871288",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "2272987756",
                    "name": "Xiaorui Liu"
                }
            ]
        },
        {
            "paperId": "5ca04cde78542b8d58c4c5b2db91ddd65b56d917",
            "title": "LazyGNN: Large-Scale Graph Neural Networks via Lazy Propagation",
            "abstract": "Recent works have demonstrated the benefits of capturing long-distance dependency in graphs by deeper graph neural networks (GNNs). But deeper GNNs suffer from the long-lasting scalability challenge due to the neighborhood explosion problem in large-scale graphs. In this work, we propose to capture long-distance dependency in graphs by shallower models instead of deeper models, which leads to a much more efficient model, LazyGNN, for graph representation learning. Moreover, we demonstrate that LazyGNN is compatible with existing scalable approaches (such as sampling methods) for further accelerations through the development of mini-batch LazyGNN. Comprehensive experiments demonstrate its superior prediction performance and scalability on large-scale benchmarks. The implementation of LazyGNN is available at https://github.com/RXPHD/Lazy_GNN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066270999",
                    "name": "Rui Xue"
                },
                {
                    "authorId": "2049039664",
                    "name": "Haoyu Han"
                },
                {
                    "authorId": "2937550",
                    "name": "MohamadAli Torkamani"
                },
                {
                    "authorId": "2143385183",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "2124928119",
                    "name": "Xiaorui Liu"
                }
            ]
        },
        {
            "paperId": "b3d634dcd30d24422e82ce118e635ddbd18f1bba",
            "title": "Efficient Large Language Models Fine-Tuning On Graphs",
            "abstract": "Learning from Text-Attributed Graphs (TAGs) has attracted significant attention due to its wide range of real-world applications. The rapid evolution of large language models (LLMs) has revolutionized the way we process textual data, which indicates a strong potential to replace shallow text embedding generally used in Graph Neural Networks (GNNs). However, we find that existing LLM approaches that exploit text information in graphs suffer from inferior computation and data efficiency. In this work, we introduce a novel and efficient approach for the end-to-end fine-tuning of Large Language Models (LLMs) on TAGs, named LEADING. The proposed approach maintains computation cost and memory overhead comparable to the graph-less fine-tuning of LLMs. Moreover, it transfers the rick knowledge in LLMs to downstream graph learning tasks effectively with limited labeled data in semi-supervised learning. Its superior computation and data efficiency are demonstrated through comprehensive experiments, offering a promising solution for a wide range of LLMs and graph learning tasks on TAGs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066270999",
                    "name": "Rui Xue"
                },
                {
                    "authorId": "2273014254",
                    "name": "Xipeng Shen"
                },
                {
                    "authorId": "2272723219",
                    "name": "Ruozhou Yu"
                },
                {
                    "authorId": "2272987756",
                    "name": "Xiaorui Liu"
                }
            ]
        },
        {
            "paperId": "fa061277792ffbde886b99398bc91459724f6eda",
            "title": "Large-Scale Graph Neural Networks: The Past and New Frontiers",
            "abstract": "Graph Neural Networks (GNNs) have gained significant attention in recent years due to their ability to model complex relationships between entities in graph-structured data such as social networks, protein structures, and knowledge graphs. However, due to the size of real-world industrial graphs and the special architecture of GNNs, it is a long-lasting challenge for engineers and researchers to deploy GNNs on large-scale graphs, which significantly limits their applications in real-world applications. In this tutorial, we will cover the fundamental scalability challenges of GNNs, frontiers of large-scale GNNs including classic approaches and some newly emerging techniques, the evaluation and comparison of scalable GNNs, and their large-scale real-world applications. Overall, this tutorial aims to provide a systematic and comprehensive understanding of the challenges and state-of-the-art techniques for scaling GNNs. The summary and discussion on future directions will inspire engineers and researchers to explore new ideas and developments in this rapidly evolving field. The website of this tutorial is available at https://sites.google.com/ncsu.edu/gnnkdd2023tutorial.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066270999",
                    "name": "Rui Xue"
                },
                {
                    "authorId": "2049039664",
                    "name": "Haoyu Han"
                },
                {
                    "authorId": "2187164642",
                    "name": "Tong Zhao"
                },
                {
                    "authorId": "2153429147",
                    "name": "Neil Shah"
                },
                {
                    "authorId": "1736632",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "1390612725",
                    "name": "Xiaorui Liu"
                }
            ]
        },
        {
            "paperId": "b022d23c9915b5e202c5ca9f0f34fbf716f2cbfd",
            "title": "The New Large-Scale RNNLM System Based on Distributed Neuron",
            "abstract": "RNNLM (Recurrent Neural Network Language Model) can save the historical information of the training dataset by the last hidden layer and can also as input for training. It has become an interesting topic in the field of Natural Language Processing research. However, the immense training time overhead is a big problem. The large output layer, hidden layer, last hidden layer and the connections among them will generate enormous matrix in training. It is the main facts to influence the efficiency and scalability. At the same time, output layer class and small hidden layer should decrease the accuracy of RNNLM. In general, the lack of parallel for artificial neuron is main reason for these. We change the structure of RNNLM and design the new large-scale RNNLM by the center of distributed artificial neurons in hidden layer to stimulate the parallel characteristic of biological neuron system. Meanwhile, we change training method, and present the coordination strategy for distributed neuron. At last, the prototype of new large-scale RNNLM system is implemented based on Spark. The testing and analysis results show that the training time overhead is far less than the growth rate of the distributed neuron in hidden layer and size of training dataset. These results show our large-scale RNNLM system has efficiency and scalability advantage.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1890358",
                    "name": "Dejiao Niu"
                },
                {
                    "authorId": "2066270999",
                    "name": "Rui Xue"
                },
                {
                    "authorId": "49282929",
                    "name": "Tao Cai"
                },
                {
                    "authorId": "40348219",
                    "name": "Hai Helen Li"
                },
                {
                    "authorId": "19250395",
                    "name": "Kingsley Effah"
                },
                {
                    "authorId": "2119079785",
                    "name": "Hang Zhang"
                }
            ]
        }
    ]
}