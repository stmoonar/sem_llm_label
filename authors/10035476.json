{
    "authorId": "10035476",
    "papers": [
        {
            "paperId": "0dd88453703f0008019a9a55a364064f0e8aa5d0",
            "title": "PV2TEA: Patching Visual Modality to Textual-Established Information Extraction",
            "abstract": "Information extraction, e.g., attribute value extraction, has been extensively studied and formulated based only on text. However, many attributes can benefit from image-based extraction, like color, shape, pattern, among others. The visual modality has long been underutilized, mainly due to multimodal annotation difficulty. In this paper, we aim to patch the visual modality to the textual-established attribute information extractor. The cross-modality integration faces several unique challenges: (C1) images and textual descriptions are loosely paired intra-sample and inter-samples; (C2) images usually contain rich backgrounds that can mislead the prediction; (C3) weakly supervised labels from textual-established extractors are biased for multimodal training. We present PV2TEA, an encoder-decoder architecture equipped with three bias reduction schemes: (S1) Augmented label-smoothed contrast to improve the cross-modality alignment for loosely-paired image and text; (S2) Attention-pruning that adaptively distinguishes the visual foreground; (S3) Two-level neighborhood regularization that mitigates the label textual bias via reliability estimation. Empirical results on real-world e-Commerce datasets demonstrate up to 11.74% absolute (20.97% relatively) F1 increase over unimodal baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112821580",
                    "name": "Hejie Cui"
                },
                {
                    "authorId": "10035476",
                    "name": "Rongmei Lin"
                },
                {
                    "authorId": "2814303",
                    "name": "Nasser Zalmout"
                },
                {
                    "authorId": "2047145237",
                    "name": "Chenwei Zhang"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2157096355",
                    "name": "Xian Li"
                }
            ]
        },
        {
            "paperId": "03da635ad2b31badf4d6a7e3ccfc713cb48cf0f0",
            "title": "Federated Pruning: Improving Neural Network Efficiency with Federated Learning",
            "abstract": "Automatic Speech Recognition models require large amount of speech data for training, and the collection of such data often leads to privacy concerns. Federated learning has been widely used and is considered to be an effective decentralized tech-nique by collaboratively learning a shared prediction model while keeping the data local on different clients devices. How-ever, the limited computation and communication resources on clients devices present practical dif\ufb01culties for large models. To overcome such challenges, we propose Federated Pruning to train a reduced model under the federated setting, while main-taining similar performance compared to the full model. More-over, the vast amount of clients data can also be leveraged to improve the pruning results compared to centralized training. We explore different pruning schemes and provide empirical evi-dence of the effectiveness of our methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10035476",
                    "name": "Rongmei Lin"
                },
                {
                    "authorId": "2156844",
                    "name": "Yonghui Xiao"
                },
                {
                    "authorId": "1950815",
                    "name": "Tien-Ju Yang"
                },
                {
                    "authorId": "47783130",
                    "name": "Ding Zhao"
                },
                {
                    "authorId": "145719374",
                    "name": "Li Xiong"
                },
                {
                    "authorId": "2064960804",
                    "name": "Giovanni Motta"
                },
                {
                    "authorId": "146687622",
                    "name": "Franccoise Beaufays"
                }
            ]
        },
        {
            "paperId": "5fa5f92da2b21d86dad62f8ff345fe2d21c74dd3",
            "title": "Learning with Hyperspherical Uniformity",
            "abstract": "Due to the over-parameterization nature, neural networks are a powerful tool for nonlinear function approximation. In order to achieve good generalization on unseen data, a suitable inductive bias is of great importance for neural networks. One of the most straightforward ways is to regularize the neural network with some additional objectives. L2 regularization serves as a standard regularization for neural networks. Despite its popularity, it essentially regularizes one dimension of the individual neuron, which is not strong enough to control the capacity of highly over-parameterized neural networks. Motivated by this, hyperspherical uniformity is proposed as a novel family of relational regularizations that impact the interaction among neurons. We consider several geometrically distinct ways to achieve hyperspherical uniformity. The effectiveness of hyperspherical uniformity is justified by theoretical insights and empirical evaluations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36326884",
                    "name": "Weiyang Liu"
                },
                {
                    "authorId": "10035476",
                    "name": "Rongmei Lin"
                },
                {
                    "authorId": "2109343010",
                    "name": "Zhen Liu"
                },
                {
                    "authorId": "2068239517",
                    "name": "Li Xiong"
                },
                {
                    "authorId": "1707625",
                    "name": "B. Scholkopf"
                },
                {
                    "authorId": "145689461",
                    "name": "Adrian Weller"
                }
            ]
        },
        {
            "paperId": "72a7105465aa8d84161eef602fcbe43b0e9f1b35",
            "title": "PAM: Understanding Product Images in Cross Product Category Attribute Extraction",
            "abstract": "Understanding product attributes plays an important role in improving online shopping experience for customers and serves asan integral part for constructing a product knowledge graph. Most existing methods focus on attribute extraction from text description or utilize visual information from product images such as shape and color. Compared to the inputs considered in prior works, a product image in fact contains more information, represented by a rich mixture of words and visual clues with a layout carefully designed to impress customers. This work proposes a more inclusive framework that fully utilizes these different modalities for attribute extraction.Inspired by recent works in visual question answering, we use a transformer based sequence to sequence model to fuse representations of product text, Optical Character Recognition (OCR) tokens and visual objects detected in the product image. The framework is further extended with the capability to extract attribute value across multiple product categories with a single model, by training the decoder to predict both product category and attribute value and conditioning its output on product category. The model provides a unified attribute extraction solution desirable at an e-commerce platform that offers numerous product categories with a diverse body of product attributes. We evaluated the model on two product attributes, one with many possible values and one with a small set of possible values, over 14 product categories and found the model could achieve 15% gain on the Recall and 10% gain on the F1 score compared to existing methods using text-only features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10035476",
                    "name": "Rongmei Lin"
                },
                {
                    "authorId": "2111080293",
                    "name": "Xiang He"
                },
                {
                    "authorId": "2109020722",
                    "name": "J. Feng"
                },
                {
                    "authorId": "2814303",
                    "name": "Nasser Zalmout"
                },
                {
                    "authorId": "2116799460",
                    "name": "Yan Liang"
                },
                {
                    "authorId": "2068239517",
                    "name": "Li Xiong"
                },
                {
                    "authorId": "2143917898",
                    "name": "Xin Dong"
                }
            ]
        },
        {
            "paperId": "0bd0ac6b4e6eb154d32216e1793295ebea1002a7",
            "title": "Orthogonal Over-Parameterized Training",
            "abstract": "The inductive bias of a neural network is largely determined by the architecture and the training algorithm. To achieve good generalization, how to effectively train a neural network is of great importance. We propose a novel orthogonal over-parameterized training (OPT) framework that can provably minimize the hyperspherical energy which characterizes the diversity of neurons on a hypersphere. By maintaining the minimum hyperspherical energy during training, OPT can greatly improve the empirical generalization. Specifically, OPT fixes the randomly initialized weights of the neurons and learns an orthogonal transformation that applies to these neurons. We consider multiple ways to learn such an orthogonal transformation, including unrolling orthogonalization algorithms, applying orthogonal parameterization, and designing orthogonality-preserving gradient descent. For better scalability, we propose the stochastic OPT which performs orthogonal transformation stochastically for partial dimensions of neurons. Interestingly, OPT reveals that learning a proper coordinate system for neurons is crucial to generalization. We provide some insights on why OPT yields better generalization. Extensive experiments validate the superiority of OPT over the standard training.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "36326884",
                    "name": "Weiyang Liu"
                },
                {
                    "authorId": "10035476",
                    "name": "Rongmei Lin"
                },
                {
                    "authorId": "2109343010",
                    "name": "Zhen Liu"
                },
                {
                    "authorId": "50779871",
                    "name": "J. Rehg"
                },
                {
                    "authorId": "2068239517",
                    "name": "Li Xiong"
                },
                {
                    "authorId": "1779453",
                    "name": "Le Song"
                }
            ]
        },
        {
            "paperId": "20fb17e27fd74fb26a5bcbc21a056e8c3c658593",
            "title": "Regularizing Neural Networks via Minimizing Hyperspherical Energy",
            "abstract": "Inspired by the Thomson problem in physics where the distribution of multiple propelling electrons on a unit sphere can be modeled via minimizing some potential energy, hyperspherical energy minimization has demonstrated its potential in regularizing neural networks and improving their generalization power. In this paper, we first study the important role that hyperspherical energy plays in neural network training by analyzing its training dynamics. Then we show that naively minimizing hyperspherical energy suffers from some difficulties due to highly non-linear and non-convex optimization as the space dimensionality becomes higher, therefore limiting the potential to further improve the generalization. To address these problems, we propose the compressive minimum hyperspherical energy (CoMHE) as a more effective regularization for neural networks. Specifically, CoMHE utilizes projection mappings to reduce the dimensionality of neurons and minimizes their hyperspherical energy. According to different designs for the projection mapping, we propose several distinct yet well-performing variants and provide some theoretical guarantees to justify their effectiveness. Our experiments show that CoMHE consistently outperforms existing regularization methods, and can be easily applied to different neural networks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10035476",
                    "name": "Rongmei Lin"
                },
                {
                    "authorId": "36326884",
                    "name": "Weiyang Liu"
                },
                {
                    "authorId": "2109343010",
                    "name": "Zhen Liu"
                },
                {
                    "authorId": null,
                    "name": "Chen Feng"
                },
                {
                    "authorId": "1751019",
                    "name": "Zhiding Yu"
                },
                {
                    "authorId": "50779871",
                    "name": "J. Rehg"
                },
                {
                    "authorId": "2068239517",
                    "name": "Li Xiong"
                },
                {
                    "authorId": "1779453",
                    "name": "Le Song"
                }
            ]
        },
        {
            "paperId": "49336767aeb78f73235e4bc215d4ea87f0ec019d",
            "title": "Compressive Hyperspherical Energy Minimization",
            "abstract": "Recent work on minimum hyperspherical energy (MHE) has demonstrated its potential in regularizing neural networks and improving their generalization. MHE was inspired by the Thomson problem in physics, where the distribution of multiple propelling electrons on a unit sphere can be modeled via minimizing some potential energy. Despite the practical effectiveness, MHE suffers from local minima as their number increases dramatically in high dimensions, limiting MHE from unleashing its full potential in improving network generalization. To address this issue, we propose compressive minimum hyperspherical energy (CoMHE) as an alternative regularization for neural networks. Specifically, CoMHE utilizes a projection mapping to reduce the dimensionality of neurons and minimizes their hyperspherical energy. According to different constructions for the projection matrix, we propose two major variants: random projection CoMHE and angle-preserving CoMHE. Furthermore, we provide theoretical insights to justify its effectiveness. We show that CoMHE consistently outperforms MHE by a significant margin in comprehensive experiments, and demonstrate its diverse applications to a variety of tasks such as image recognition and point cloud recognition.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10035476",
                    "name": "Rongmei Lin"
                },
                {
                    "authorId": "36326884",
                    "name": "Weiyang Liu"
                },
                {
                    "authorId": "2109343010",
                    "name": "Zhen Liu"
                },
                {
                    "authorId": "144066717",
                    "name": "Chen Feng"
                },
                {
                    "authorId": "1751019",
                    "name": "Zhiding Yu"
                },
                {
                    "authorId": "144177248",
                    "name": "James M. Rehg"
                },
                {
                    "authorId": "145719374",
                    "name": "Li Xiong"
                },
                {
                    "authorId": "1779453",
                    "name": "Le Song"
                }
            ]
        },
        {
            "paperId": "13b2bc8101a2a7a0c95412c48f40ef95e798e9fb",
            "title": "Learning towards Minimum Hyperspherical Energy",
            "abstract": "Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics -- Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "36326884",
                    "name": "Weiyang Liu"
                },
                {
                    "authorId": "10035476",
                    "name": "Rongmei Lin"
                },
                {
                    "authorId": "46270580",
                    "name": "Z. Liu"
                },
                {
                    "authorId": "47968201",
                    "name": "Lixin Liu"
                },
                {
                    "authorId": "1751019",
                    "name": "Zhiding Yu"
                },
                {
                    "authorId": "144445933",
                    "name": "Bo Dai"
                },
                {
                    "authorId": "1779453",
                    "name": "Le Song"
                }
            ]
        },
        {
            "paperId": "ad28fb4fbfb45310559a38c049799f0cf6748b6c",
            "title": "Deformable Part Networks",
            "abstract": "In this paper we propose novel Deformable Part Networks (DPNs) to learn {\\em pose-invariant} representations for 2D object recognition. In contrast to the state-of-the-art pose-aware networks such as CapsNet \\cite{sabour2017dynamic} and STN \\cite{jaderberg2015spatial}, DPNs can be naturally {\\em interpreted} as an efficient solver for a challenging detection problem, namely Localized Deformable Part Models (LDPMs) where localization is introduced to DPMs as another latent variable for searching for the best poses of objects over all pixels and (predefined) scales. In particular we construct DPNs as sequences of such LDPM units to model the semantic and spatial relations among the deformable parts as hierarchical composition and spatial parsing trees. Empirically our 17-layer DPN can outperform both CapsNets and STNs significantly on affNIST \\cite{sabour2017dynamic}, for instance, by 19.19\\% and 12.75\\%, respectively, with better generalization and better tolerance to affine transformations.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7969330",
                    "name": "Ziming Zhang"
                },
                {
                    "authorId": "10035476",
                    "name": "Rongmei Lin"
                },
                {
                    "authorId": "2057801297",
                    "name": "Alan Sullivan"
                }
            ]
        },
        {
            "paperId": "dfc15152377b5271fb036b8b192b53c6760773c2",
            "title": "A Deep Deterministic Policy Gradient Approach to Medication Dosing and Surveillance in the ICU",
            "abstract": "Medication dosing in a critical care environment is a complex task that involves close monitoring of relevant physiologic and laboratory biomarkers and corresponding sequential adjustment of the prescribed dose. Misdosing of medications with narrow therapeutic windows (such as intravenous [IV] heparin) can result in preventable adverse events, decrease quality of care and increase cost. Therefore, a robust recommendation system can help clinicians by providing individualized dosing suggestions or corrections to existing protocols. We present a clinician-in-the-loop framework for adjusting IV heparin dose using deep reinforcement learning (RL). Our main objectives were to learn a new IV heparin dosing policy based on the multi-dimensional features of patients, and evaluate the effectiveness of the learned policy in the presence of other confounding factors that may contribute to heparin-related side effects. The data used in the experiments included 2598 intensive care patients from the publicly available MIMIC database and 2310 patients from the Emory University clinical data warehouse. Experimental results suggested that the distance from RL policy had a statistically significant association with anticoagulant complications $(p< 0.05)$, after adjusting for the effects of confounding factors.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "10035476",
                    "name": "Rongmei Lin"
                },
                {
                    "authorId": "49555023",
                    "name": "Matthew D. Stanley"
                },
                {
                    "authorId": "143811844",
                    "name": "M. Ghassemi"
                },
                {
                    "authorId": "144422139",
                    "name": "S. Nemati"
                }
            ]
        }
    ]
}