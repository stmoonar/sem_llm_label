{
    "authorId": "2116486701",
    "papers": [
        {
            "paperId": "1baa0e7f4d84eb571b26dc4f8f31ba9c9df459d2",
            "title": "\"I am the follower, also the boss\": Exploring Different Levels of Autonomy and Machine Forms of Guiding Robots for the Visually Impaired",
            "abstract": "Guiding robots, in the form of canes or cars, have recently been explored to assist blind and low vision (BLV) people. Such robots can provide full or partial autonomy when guiding. However, the pros and cons of different forms and autonomy for guiding robots remain unknown. We sought to fill this gap. We designed autonomy-switchable guiding robotic cane and car. We conducted a controlled lab-study (N=12) and a field study (N=9) on BLV. Results showed that full autonomy received better walking performance and subjective ratings in the controlled study, whereas participants used more partial autonomy in the natural environment as demanding more control. Besides, the car robot has demonstrated abilities to provide a higher sense of safety and navigation efficiency compared with the cane robot. Our findings offered empirical evidence about how the BLV community perceived different machine forms and autonomy, which can inform the design of assistive robots.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39509574",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2187938715",
                    "name": "ZiangL Li"
                },
                {
                    "authorId": "2176147031",
                    "name": "Haole Guo"
                },
                {
                    "authorId": "2188233907",
                    "name": "Luyao Wang"
                },
                {
                    "authorId": "2187935139",
                    "name": "Qihe Chen"
                },
                {
                    "authorId": "2116486701",
                    "name": "Wen-Wen Jiang"
                },
                {
                    "authorId": "2204644498",
                    "name": "Mingming Fan"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                }
            ]
        },
        {
            "paperId": "dd76ab5e69933e77de4d2089f7344c948f79f76d",
            "title": "Annotating Covert Hazardous Driving Scenarios Online: Utilizing Drivers' Electroencephalography (EEG) Signals",
            "abstract": "As autonomous driving systems prevail, it is becoming increasingly critical that the systems learn from databases containing fine-grained driving scenarios. Most databases currently available are human-annotated; they are expensive, time-consuming, and subject to behavioral biases. In this paper, we provide initial evidence supporting a novel technique utilizing drivers' electroencephalography (EEG) signals to implicitly label hazardous driving scenarios while passively viewing recordings of real-road driving, thus sparing the need for manual annotation and avoiding human annotators' behavioral biases during explicit report. We conducted an EEG experiment using real-life and animated recordings of driving scenarios and asked participants to report danger explicitly whenever necessary. Behavioral results showed the participants tended to report danger only when overt hazards (e.g., a vehicle or a pedestrian appearing unexpectedly from behind an occlusion) were in view. By contrast, their EEG signals were enhanced at the sight of both an overt hazard and a covert hazard (e.g., an occlusion signalling possible appearance of a vehicle or a pedestrian from behind). Thus, EEG signals were more sensitive to driving hazards than explicit reports. Further, the Time-Series AI (TSAI, [1]) successfully classified EEG signals corresponding to overt and covert hazards. We discuss future steps necessary to materialize the technique in real life.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2209944245",
                    "name": "Chen Zheng"
                },
                {
                    "authorId": "2209882264",
                    "name": "Muxiao Zi"
                },
                {
                    "authorId": "2116486701",
                    "name": "Wen-Wen Jiang"
                },
                {
                    "authorId": "2188740500",
                    "name": "Mengdi Chu"
                },
                {
                    "authorId": "39509574",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2312163",
                    "name": "Jirui Yuan"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                }
            ]
        },
        {
            "paperId": "321a3765e01dc26a4460fba9ab150856f2f7d25c",
            "title": "3D Dynamic Hand Gesture Recognition with Fused RGB and Depth Images",
            "abstract": "With the advancing of dynamic gesture recognition technology, it is widely used in various interaction scenarios nowadays. However, the three-dimensional dynamic gesture recognition method is easily disturbed by the external environment, such as illumination, background and shadow. To address these problems, we propose a 3D Dynamic Gesture Recognition network model, which use both CNN and LSTM networks and can fuse RGB and depth image information. We conduct experiments with Intel RealSense D415 depth camera and self-built gesture dataset, the results demonstrate that the recognition accuracy of the 3D-DGR model is 99.23%, which is 1.52% higher than the model using only RGB images.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2075312721",
                    "name": "Qi Ye"
                },
                {
                    "authorId": "2115298043",
                    "name": "Yang Bai"
                },
                {
                    "authorId": "2115386499",
                    "name": "Lu Chen"
                },
                {
                    "authorId": "2116486701",
                    "name": "Wen-Wen Jiang"
                }
            ]
        },
        {
            "paperId": "3480639dd6d4609ed706f30d1a62dae555a70aab",
            "title": "An Improved RaftStereo Trained with A Mixed Dataset for the Robust Vision Challenge 2022",
            "abstract": ". Stereo matching is a fundamental problem in computer vision. Despite recent progress by deep learning, improving the robust-ness is ineluctable when deploying stereo-matching models to real-world applications. Di\ufb00erent from the common practices, i.e., developing an elaborate model to achieve robustness, we argue that collecting multiple available datasets for training is a cheaper way to increase generalization ability. Speci\ufb01cally, this report presents an improved RaftStereo [12] trained with a mixed dataset of seven public datasets for the robust vision challenge (denoted as iRaftStereo RVC). When evaluated on the training sets of Middlebury, KITTI-2015, and ETH3D, the model out-performs its counterparts trained with only one dataset, such as the popular Scene\ufb02ow. After \ufb01ne-tuning the pre-trained model on the three datasets of the challenge, it ranks at 2nd place on the stereo leader-board 1 , demonstrating the bene\ufb01ts of mixed dataset pre-training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1490773707",
                    "name": "Hualie Jiang"
                },
                {
                    "authorId": "145166364",
                    "name": "Rui Xu"
                },
                {
                    "authorId": "2116486701",
                    "name": "Wen-Wen Jiang"
                }
            ]
        },
        {
            "paperId": "c804842f38adc719553d2adc225a53cddcfce632",
            "title": "Evaluation of Pedestrian Safety in a High-Fidelity Simulation Environment Framework",
            "abstract": "Pedestrians' safety is a crucial factor in assessing autonomous driving scenarios. However, pedestrian safety evaluation is rarely considered by existing autonomous driving simulation platforms. This paper proposes a pedestrian safety evaluation method for autonomous driving, in which not only the collision events but also the conflict events together with the characteristics of pedestrians are fully considered. Moreover, to apply the pedestrian safety evaluation system, we construct a high-fidelity simulation framework embedded with pedestrian safety-critical characteristics. We demonstrate our simulation framework and pedestrian safety evaluation with a comparative experiment with two kinds of autonomous driving perception algorithms -- single-vehicle perception and vehicle-to-infrastructure (V2I) cooperative perception. The results show that our framework can evaluate different autonomous driving algorithms with detailed and quantitative pedestrian safety indexes. To this end, the proposed simulation method and framework can be used to access different autonomous driving algorithms and evaluate pedestrians' safety performance in future autonomous driving simulations, which can inspire more pedestrian-friendly autonomous driving algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188774806",
                    "name": "Longrui Chen"
                },
                {
                    "authorId": "39509574",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2116486701",
                    "name": "Wen-Wen Jiang"
                },
                {
                    "authorId": "2235529",
                    "name": "Jiangtao Gong"
                },
                {
                    "authorId": "2115734074",
                    "name": "Jiahao Shen"
                },
                {
                    "authorId": "2188740500",
                    "name": "Mengdi Chu"
                },
                {
                    "authorId": "2185491599",
                    "name": "Chuxuan Li"
                },
                {
                    "authorId": "2213448590",
                    "name": "Yifeng Pan"
                },
                {
                    "authorId": "2118898046",
                    "name": "Yifeng Shi"
                },
                {
                    "authorId": "2188739588",
                    "name": "Nairui Luo"
                },
                {
                    "authorId": "2118141841",
                    "name": "Xuming Gao"
                },
                {
                    "authorId": "2312163",
                    "name": "Jirui Yuan"
                },
                {
                    "authorId": "2153104630",
                    "name": "Guyue Zhou"
                },
                {
                    "authorId": "2188826021",
                    "name": "Yaqin Zhang"
                }
            ]
        },
        {
            "paperId": "1f06c4494847c11d5ee9f07e464f20d0467952aa",
            "title": "Research on Unsupervised Coloring Method of Chinese Painting Based on an Improved Generative Adversarial Network",
            "abstract": "Chinese painting elements have excellent application prospects in animation. In order to improve the efficiency and effect of image coloring, an image coloring algorithm for enhancing the generative adversarial network studied. In the original model Pix2PixHD (High\u2010Resolution Image Synthesis and Semantic Manipulation with Conditional GANs) first introduced a self\u2010attention mechanism in the generator to improve the resolution of the image, and then used VGG loss and loss score from critic as the loss function of the generator to enrich the color diversity of the image, and then continue to train the network. The size of the input image is changed, and the learning rate is adjusted to improve the training efficiency and generalization effect. Finally, the network structure optimized by using the local corresponding regularization Pixel\u2010Normalization. Experiments show that the algorithm can simultaneously color the high\u2010resolution image and preserve the underlying contour information of the original image. The subjective visual evaluation and objective quantization index (MSE, PSNR, and SSIM) are better than Pix2Pix. Image\u2010to\u2010image translation with conditional adversarial networks), Pix2PixHD and CycleGAN (Cycle\u2010Consistent Adversarial Networks) methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116486701",
                    "name": "Wen-Wen Jiang"
                },
                {
                    "authorId": "1763547",
                    "name": "Xiaoshu Luo"
                }
            ]
        },
        {
            "paperId": "64bc24ff9a4328b21e8b7af995a977d9e1cd8f19",
            "title": "Fixation Prediction based on Scene Contours",
            "abstract": "Previous works suggest that scene contours play important roles in guiding visual attention. In this study, a computational model is proposed to improve the performance in visual saliency prediction by integrating the low- and mid-level visual cues and evaluate the contribution of scene contours in guiding visual attention. Firstly, we define three kinds of Gestalt principles based on mid-level cues, including contour density, closure, and symmetry to characterize the potential salient regions. In addition, we employ the classical bottom-up methods to generate low-level saliency maps. Finally, the proposed method combines the low-level cues from natural images and the mid-level cues from the corresponding contours to improve the fixation prediction. Experimental results show that the contour-based midlevel cues can remarkably improve the performance of the bottomup models in fixation prediction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066247757",
                    "name": "Tengfei Zhan"
                },
                {
                    "authorId": "2054892341",
                    "name": "M. Ye"
                },
                {
                    "authorId": "2116486701",
                    "name": "Wen-Wen Jiang"
                },
                {
                    "authorId": "6960951",
                    "name": "Yongjie Li"
                },
                {
                    "authorId": "2105451",
                    "name": "Kaifu Yang"
                }
            ]
        },
        {
            "paperId": "6516bd94b28e014d4d4594c00b206171bfd9ccf7",
            "title": "A Video Salient Object Detection Model Guided by Spatio-Temporal Prior",
            "abstract": "Neurobiology researches suggest that the motion information attracts more attention of human visual system than other low-level features such as brightness, color and texture. Consequently, video saliency detection methods not only consider the spatial saliency caused by the underlying features of images, but also the motion information in temporal domain. In this study, we proposes a model of video salient object detection based on a two-pathway framework that the spatio-temporal contrast guides the search for salient targets. Firstly, along the non-selective pathway, which is computed with the intra-frame and inter-frame maps of the color contrast and motion contrast, combining with the previous saliency map, to represent the prior information of the possible target locations. In contrast, the low-level features such as brightness, color and motion features are extracted in the selective pathway to search target accurately. Finally, the Bayesian inference is used to further obtain the optimal results. Experimental results show that our algorithm improves the performance of salient object detection on video compared to the representative method of Contour Guided Visual Search.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116486701",
                    "name": "Wen-Wen Jiang"
                },
                {
                    "authorId": "2787873",
                    "name": "Kai-Fu Yang"
                },
                {
                    "authorId": "6960951",
                    "name": "Yongjie Li"
                }
            ]
        },
        {
            "paperId": "796406e940f99bbf9e34d3c01521e2a971a92ea5",
            "title": "Line Drawings of Natural Scenes Guide Visual Attention",
            "abstract": "Visual search is an important strategy of the human visual system for fast scene perception. The guided search theory suggests that the global layout or other top-down sources of scenes play a crucial role in guiding object searching. In order to verify the specific roles of scene layout and regional cues in guiding visual attention, we executed a psychophysical experiment to record the human fixations on line drawings of natural scenes with an eye-tracking system in this work. We collected the human fixations of ten subjects from 498 natural images and of another ten subjects from the corresponding 996 human-marked line drawings of boundaries (two boundary maps per image) under free-viewing condition. The experimental results show that with the absence of some basic features like color and luminance, the distribution of the fixations on the line drawings has a high correlation with that on the natural images. Moreover, compared to the basic cues of regions, subjects pay more attention to the closed regions of line drawings which are usually related to the dominant objects of the scenes. Finally, we built a computational model to demonstrate that the fixation information on the line drawings can be used to significantly improve the performances of classical bottom-up models for fixation prediction in natural scenes. These results support that Gestalt features and scene layout are important cues for guiding fast visual object searching.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2787873",
                    "name": "Kai-Fu Yang"
                },
                {
                    "authorId": "2116486701",
                    "name": "Wen-Wen Jiang"
                },
                {
                    "authorId": "2066247757",
                    "name": "Tengfei Zhan"
                },
                {
                    "authorId": "6960951",
                    "name": "Yongjie Li"
                }
            ]
        }
    ]
}