{
    "authorId": "46985469",
    "papers": [
        {
            "paperId": "16c64f74ce0e6a59b0709c0d8e66596a5bc08ed6",
            "title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset",
            "abstract": "As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2172404846",
                    "name": "Hugo Laurenccon"
                },
                {
                    "authorId": "2113836860",
                    "name": "Lucile Saulnier"
                },
                {
                    "authorId": "2135734748",
                    "name": "Thomas Wang"
                },
                {
                    "authorId": "2003696840",
                    "name": "Christopher Akiki"
                },
                {
                    "authorId": "46219923",
                    "name": "Albert Villanova del Moral"
                },
                {
                    "authorId": "1379806208",
                    "name": "Teven Le Scao"
                },
                {
                    "authorId": "51128119",
                    "name": "Leandro von Werra"
                },
                {
                    "authorId": "35966970",
                    "name": "Chenghao Mou"
                },
                {
                    "authorId": "79512668",
                    "name": "E. G. Ponferrada"
                },
                {
                    "authorId": "2168170616",
                    "name": "Huu Nguyen"
                },
                {
                    "authorId": "2146695800",
                    "name": "Jorg Frohberg"
                },
                {
                    "authorId": "2125821515",
                    "name": "Mario vSavsko"
                },
                {
                    "authorId": "2113836945",
                    "name": "Quentin Lhoest"
                },
                {
                    "authorId": "1584940075",
                    "name": "Angelina McMillan-Major"
                },
                {
                    "authorId": "13656138",
                    "name": "G\u00e9rard Dupont"
                },
                {
                    "authorId": "103476203",
                    "name": "Stella Biderman"
                },
                {
                    "authorId": "145046059",
                    "name": "Anna Rogers"
                },
                {
                    "authorId": "2190281230",
                    "name": "Loubna Ben Allal"
                },
                {
                    "authorId": "2067891070",
                    "name": "F. Toni"
                },
                {
                    "authorId": "2158858559",
                    "name": "Giada Pistilli"
                },
                {
                    "authorId": "2089233725",
                    "name": "Olivier Nguyen"
                },
                {
                    "authorId": "2099315138",
                    "name": "Somaieh Nikpoor"
                },
                {
                    "authorId": "153528116",
                    "name": "Maraim Masoud"
                },
                {
                    "authorId": "46985469",
                    "name": "Pierre Colombo"
                },
                {
                    "authorId": "144979591",
                    "name": "Javier de la Rosa"
                },
                {
                    "authorId": "2176184659",
                    "name": "Paulo Villegas"
                },
                {
                    "authorId": "1500242049",
                    "name": "Tristan Thrush"
                },
                {
                    "authorId": "29909347",
                    "name": "S. Longpre"
                },
                {
                    "authorId": "47351277",
                    "name": "Sebastian Nagel"
                },
                {
                    "authorId": "20308468",
                    "name": "Leon Weber"
                },
                {
                    "authorId": "2067766446",
                    "name": "M. Mu\u00f1oz"
                },
                {
                    "authorId": "144549416",
                    "name": "Jian Zhu"
                },
                {
                    "authorId": "71075073",
                    "name": "Daniel Alexander van Strien"
                },
                {
                    "authorId": "25098419",
                    "name": "Zaid Alyafeai"
                },
                {
                    "authorId": "90615055",
                    "name": "Khalid Almubarak"
                },
                {
                    "authorId": "1484109150",
                    "name": "Minh Chien Vu"
                },
                {
                    "authorId": "1404791152",
                    "name": "Itziar Gonzalez-Dios"
                },
                {
                    "authorId": "2078619062",
                    "name": "Aitor Soroa Etxabe"
                },
                {
                    "authorId": "46258841",
                    "name": "Kyle Lo"
                },
                {
                    "authorId": "1879591269",
                    "name": "Manan Dey"
                },
                {
                    "authorId": "147846651",
                    "name": "Pedro Ortiz Suarez"
                },
                {
                    "authorId": "2273789852",
                    "name": "Aaron Gokaslan"
                },
                {
                    "authorId": "2795685",
                    "name": "Shamik Bose"
                },
                {
                    "authorId": "2518906",
                    "name": "David Ifeoluwa Adelani"
                },
                {
                    "authorId": null,
                    "name": "Long Phan"
                },
                {
                    "authorId": "2057078797",
                    "name": "H. Tran"
                },
                {
                    "authorId": "52141399",
                    "name": "I. Yu"
                },
                {
                    "authorId": "2053516473",
                    "name": "S. Pai"
                },
                {
                    "authorId": "2164872258",
                    "name": "Jenny Chim"
                },
                {
                    "authorId": "2190280574",
                    "name": "Violette Lepercq"
                },
                {
                    "authorId": "2066663276",
                    "name": "Suzana Ilic"
                },
                {
                    "authorId": "49501003",
                    "name": "Margaret Mitchell"
                },
                {
                    "authorId": "2165225321",
                    "name": "Sasha Luccioni"
                },
                {
                    "authorId": "2262249",
                    "name": "Yacine Jernite"
                }
            ]
        },
        {
            "paperId": "272da9d059a466064421de6da8ee7d428d9ea215",
            "title": "A Halfspace-Mass Depth-Based Method for Adversarial Attack Detection",
            "abstract": "Despite the widespread use of deep learning algorithms, vulnerability to adversarial attacks is still an issue limiting their use in critical applications. Detecting these attacks is thus crucial to build reliable algorithms and has received increasing attention in the last few years. In this paper, we introduce the H alfsp A ce M ass de P th d E tecto R ( HAMPER ), a new method to detect adversarial examples by leveraging the concept of data depths, a statistical notion that provides center-outward ordering of points with respect to (w.r.t.) a probability distribution. In particular, the halfspace-mass (HM) depth exhibits attractive properties such as computational efficiency, which makes it a natural candidate for adversarial attack detection in high-dimensional spaces. Additionally, HM is non differentiable making it harder for attackers to directly attack HAMPER via gradient based-methods. We evaluate HAMPER in the context of supervised adversarial attacks detection across four benchmark datasets. Overall, we empirically show that HAMPER consistently outperforms SOTA methods. In particular, the gains are 13 . 1% (29 . 0%) in terms of AUROC \u2191 (resp. FPR \u2193 95% ) on SVHN, 14 . 6% (25 . 7%) on CIFAR10 and 22 . 6% (49 . 0%) on CIFAR100 compared to the best performing method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111880506",
                    "name": "Marine Picot"
                },
                {
                    "authorId": "1390025987",
                    "name": "Federica Granese"
                },
                {
                    "authorId": "108056624",
                    "name": "Guillaume Staerman"
                },
                {
                    "authorId": "1423797914",
                    "name": "Marco Romanelli"
                },
                {
                    "authorId": "1430784474",
                    "name": "Francisco Messina"
                },
                {
                    "authorId": "1743922",
                    "name": "P. Piantanida"
                },
                {
                    "authorId": "46985469",
                    "name": "Pierre Colombo"
                }
            ]
        },
        {
            "paperId": "44bab2836177f8bf9775e7ca536b8e200757aac7",
            "title": "Hallucinations in Large Multilingual Translation Models",
            "abstract": "Abstract Hallucinated translations can severely undermine and raise safety issues when machine translation systems are deployed in the wild. Previous research on the topic focused on small bilingual models trained on high-resource languages, leaving a gap in our understanding of hallucinations in multilingual models across diverse translation scenarios. In this work, we fill this gap by conducting a comprehensive analysis\u2014over 100 language pairs across various resource levels and going beyond English-centric directions\u2014on both the M2M neural machine translation (NMT) models and GPT large language models (LLMs). Among several insights, we highlight that models struggle with hallucinations primarily in low-resource directions and when translating out of English, where, critically, they may reveal toxic patterns that can be traced back to the training data. We also find that LLMs produce qualitatively different hallucinations to those of NMT models. Finally, we show that hallucinations are hard to reverse by merely scaling models trained with the same data. However, employing more diverse models, trained on different data or with different procedures, as fallback systems can improve translation quality and virtually eliminate certain pathologies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144726818",
                    "name": "Nuno M. Guerreiro"
                },
                {
                    "authorId": "2184781142",
                    "name": "Duarte M. Alves"
                },
                {
                    "authorId": "2123124763",
                    "name": "Jonas Waldendorf"
                },
                {
                    "authorId": "2259100",
                    "name": "B. Haddow"
                },
                {
                    "authorId": "2539211",
                    "name": "Alexandra Birch"
                },
                {
                    "authorId": "46985469",
                    "name": "Pierre Colombo"
                },
                {
                    "authorId": "2069905347",
                    "name": "Andr\u00e9 Martins"
                }
            ]
        },
        {
            "paperId": "7c3025bd4cd8f3064ce481c95124a5102d9307b7",
            "title": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection",
            "abstract": "Out-of-distribution (OOD) detection is a rapidly growing field due to new robustness and security requirements driven by an increased number of AI-based systems. Existing OOD textual detectors often rely on anomaly scores (\\textit{e.g.}, Mahalanobis distance) computed on the embedding output of the last layer of the encoder. In this work, we observe that OOD detection performance varies greatly depending on the task and layer output. More importantly, we show that the usual choice (the last layer) is rarely the best one for OOD detection and that far better results can be achieved, provided that an oracle selects the best layer. We propose a data-driven, unsupervised method to leverage this observation to combine layer-wise anomaly scores. In addition, we extend classical textual OOD benchmarks by including classification tasks with a more significant number of classes (up to 150), which reflects more realistic settings. On this augmented benchmark, we show that the proposed post-aggregation methods achieve robust and consistent results comparable to using the best layer according to an oracle while removing manual feature selection altogether.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2197480753",
                    "name": "Maxime Darrin"
                },
                {
                    "authorId": "108056624",
                    "name": "Guillaume Staerman"
                },
                {
                    "authorId": "2219484288",
                    "name": "Eduardo Dadalto Camara Gomes"
                },
                {
                    "authorId": "47149028",
                    "name": "J. Cheung"
                },
                {
                    "authorId": "1743922",
                    "name": "P. Piantanida"
                },
                {
                    "authorId": "46985469",
                    "name": "Pierre Colombo"
                }
            ]
        },
        {
            "paperId": "925e15febb7fe46435b06b16e02426e1e840bf50",
            "title": "A Functional Data Perspective and Baseline On Multi-Layer Out-of-Distribution Detection",
            "abstract": "A key feature of out-of-distribution (OOD) detection is to exploit a trained neural network by extracting statistical patterns and relationships through the multi-layer classifier to detect shifts in the expected input data distribution. Despite achieving solid results, several state-of-the-art methods rely on the penultimate or last layer outputs only, leaving behind valuable information for OOD detection. Methods that explore the multiple layers either require a special architecture or a supervised objective to do so. This work adopts an original approach based on a functional view of the network that exploits the sample's trajectories through the various layers and their statistical dependencies. It goes beyond multivariate features aggregation and introduces a baseline rooted in functional anomaly detection. In this new framework, OOD detection translates into detecting samples whose trajectories differ from the typical behavior characterized by the training set. We validate our method and empirically demonstrate its effectiveness in OOD detection compared to strong state-of-the-art baselines on computer vision benchmarks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2219484288",
                    "name": "Eduardo Dadalto Camara Gomes"
                },
                {
                    "authorId": "46985469",
                    "name": "Pierre Colombo"
                },
                {
                    "authorId": "108056624",
                    "name": "Guillaume Staerman"
                },
                {
                    "authorId": "88972690",
                    "name": "Nathan Noiry"
                },
                {
                    "authorId": "1743922",
                    "name": "P. Piantanida"
                }
            ]
        },
        {
            "paperId": "a43438b65660e69f7c7341b5f3ced15d5ac98c8d",
            "title": "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks",
            "abstract": "The evaluation of natural language processing (NLP) systems is crucial for advancing the field, but current benchmarking approaches often assume that all systems have scores available for all tasks, which is not always practical. In reality, several factors such as the cost of running baseline, private systems, computational limitations, or incomplete data may prevent some systems from being evaluated on entire tasks. This paper formalize an existing problem in NLP research: benchmarking when some systems scores are missing on the task, and proposes a novel approach to address it. Our method utilizes a compatible partial ranking approach to impute missing data, which is then aggregated using the Borda count method. It includes two refinements designed specifically for scenarios where either task-level or instance-level scores are available. We also introduce an extended benchmark, which contains over 131 million scores, an order of magnitude larger than existing benchmarks. We validate our methods and demonstrate their effectiveness in addressing the challenge of missing system evaluation on an entire task. This work highlights the need for more comprehensive benchmarking approaches that can handle real-world scenarios where not all systems are evaluated on the entire task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2217680859",
                    "name": "Anas Himmi"
                },
                {
                    "authorId": "2217198",
                    "name": "Ekhine Irurozki"
                },
                {
                    "authorId": "88972690",
                    "name": "Nathan Noiry"
                },
                {
                    "authorId": "1696620",
                    "name": "S. Cl\u00e9men\u00e7on"
                },
                {
                    "authorId": "46985469",
                    "name": "Pierre Colombo"
                }
            ]
        },
        {
            "paperId": "022ed9823e99d09eba284ca3457fb59cba3ebfbb",
            "title": "Rainproof: An Umbrella To Shield Text Generators From Out-Of-Distribution Data",
            "abstract": "Implementing effective control mechanisms to ensure the proper functioning and security of deployed NLP models, from translation to chatbots, is essential. A key ingredient to ensure safe system behaviour is Out-Of-Distribution (OOD) detection, which aims to detect whether an input sample is statistically far from the training distribution. Although OOD detection is a widely covered topic in classification tasks, most methods rely on hidden features output by the encoder. In this work, we focus on leveraging soft-probabilities in a black-box framework, i.e. we can access the soft-predictions but not the internal states of the model. Our contributions include: (i) RAINPROOF a Relative informAItioN Projection OOD detection framework; and (ii) a more operational evaluation setting for OOD detection. Surprisingly, we find that OOD detection is not necessarily aligned with task-specific measures. The OOD detector may filter out samples well processed by the model and keep samples that are not, leading to weaker performance. Our results show that RAINPROOF provides OOD detection methods more aligned with task-specific performance metrics than traditional OOD detectors.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2197480753",
                    "name": "Maxime Darrin"
                },
                {
                    "authorId": "1743922",
                    "name": "P. Piantanida"
                },
                {
                    "authorId": "46985469",
                    "name": "Pierre Colombo"
                }
            ]
        },
        {
            "paperId": "46db80d2574061666ea194b4e86ac44fa64745f6",
            "title": "KNIFE: Kernelized-Neural Differential Entropy Estimation",
            "abstract": "Mutual Information (MI) has been widely used as a loss regularizer for training neural networks. This has been particularly effective when learn dis-entangled or compressed representations of high dimensional data. However, differential entropy (DE), another fundamental measure of information, has not found widespread use in neural network training. Although DE offers a potentially wider range of applications than MI, off-the-shelf DE estimators are either non differentiable, computationally intractable or fail to adapt to changes in the underlying distribution. These drawbacks prevent them from being used as regularizers in neural networks training. To address shortcomings in previously proposed estimators for DE, here we introduce K NIFE , a fully parameterized, differentiable kernel-based estimator of DE. The \ufb02exibility of our approach also allows us to construct K NIFE -based estimators for conditional (on either discrete or continuous variables) DE, as well as MI. We empirically validate our method on high-dimensional synthetic data and further apply it to guide the training of neural networks for real-world tasks. Our experiments on a large variety of tasks, including visual domain adaptation, textual fair classi\ufb01cation, and textual \ufb01ne-tuning demonstrate the effectiveness of K NIFE - based estimation. Code can be found at https: //github.com/g-pichler/knife .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "134026950",
                    "name": "Georg Pichler"
                },
                {
                    "authorId": "46985469",
                    "name": "Pierre Colombo"
                },
                {
                    "authorId": "1583144397",
                    "name": "Malik Boudiaf"
                },
                {
                    "authorId": "1735548",
                    "name": "G\u00fcnther Koliander"
                },
                {
                    "authorId": "1743922",
                    "name": "P. Piantanida"
                }
            ]
        },
        {
            "paperId": "5069cebd18c7584dc388c81d8e74344f1725b40f",
            "title": "Of Human Criteria and Automatic Metrics: A Benchmark of the Evaluation of Story Generation",
            "abstract": "Research on Automatic Story Generation (ASG) relies heavily on human and automatic evaluation. However, there is no consensus on which human evaluation criteria to use, and no analysis of how well automatic criteria correlate with them. In this paper, we propose to re-evaluate ASG evaluation. We introduce a set of 6 orthogonal and comprehensive human criteria, carefully motivated by the social sciences literature. We also present HANNA, an annotated dataset of 1,056 stories produced by 10 different ASG systems. HANNA allows us to quantitatively evaluate the correlations of 72 automatic metrics with human criteria. Our analysis highlights the weaknesses of current metrics for ASG and allows us to formulate practical recommendations for ASG evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182520819",
                    "name": "Cyril Chhun"
                },
                {
                    "authorId": "46985469",
                    "name": "Pierre Colombo"
                },
                {
                    "authorId": "2049106",
                    "name": "C. Clavel"
                },
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "82b3a2559b5cf1528348b9d35285868e1c154fce",
            "title": "What are the best systems? New perspectives on NLP Benchmarking",
            "abstract": "In Machine Learning, a benchmark refers to an ensemble of datasets associated with one or multiple metrics together with a way to aggregate different systems performances. They are instrumental in (i) assessing the progress of new methods along different axes and (ii) selecting the best systems for practical use. This is particularly the case for NLP with the development of large pre-trained models (e.g. GPT, BERT) that are expected to generalize well on a variety of tasks. While the community mainly focused on developing new datasets and metrics, there has been little interest in the aggregation procedure, which is often reduced to a simple average over various performance measures. However, this procedure can be problematic when the metrics are on a different scale, which may lead to spurious conclusions. This paper proposes a new procedure to rank systems based on their performance across different tasks. Motivated by the social choice theory, the final system ordering is obtained through aggregating the rankings induced by each task and is theoretically grounded. We conduct extensive numerical experiments (on over 270k scores) to assess the soundness of our approach both on synthetic and real scores (e.g. GLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method yields different conclusions on state-of-the-art systems than the mean-aggregation procedure while being both more reliable and robust.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46985469",
                    "name": "Pierre Colombo"
                },
                {
                    "authorId": "88972690",
                    "name": "Nathan Noiry"
                },
                {
                    "authorId": "2217198",
                    "name": "Ekhine Irurozki"
                },
                {
                    "authorId": "1696620",
                    "name": "S. Cl\u00e9men\u00e7on"
                }
            ]
        }
    ]
}