{
    "authorId": "1708593",
    "papers": [
        {
            "paperId": "2d1fb9cc0fea3df724772a0837dbb92a01f2ce67",
            "title": "Benchmarking Spectral Graph Neural Networks: A Comprehensive Study on Effectiveness and Efficiency",
            "abstract": "With the recent advancements in graph neural networks (GNNs), spectral GNNs have received increasing popularity by virtue of their specialty in capturing graph signals in the frequency domain, demonstrating promising capability in specific tasks. However, few systematic studies have been conducted on assessing their spectral characteristics. This emerging family of models also varies in terms of designs and settings, leading to difficulties in comparing their performance and deciding on the suitable model for specific scenarios, especially for large-scale tasks. In this work, we extensively benchmark spectral GNNs with a focus on the frequency perspective. We analyze and categorize over 30 GNNs with 27 corresponding filters. Then, we implement these spectral models under a unified framework with dedicated graph computations and efficient training schemes. Thorough experiments are conducted on the spectral models with inclusive metrics on effectiveness and efficiency, offering practical guidelines on evaluating and selecting spectral GNNs with desirable performance. Our implementation enables application on larger graphs with comparable performance and less overhead, which is available at: https://github.com/gdmnl/Spectral-GNN-Benchmark.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1940000983",
                    "name": "Ningyi Liao"
                },
                {
                    "authorId": "2143856420",
                    "name": "Haoyu Liu"
                },
                {
                    "authorId": "41126872",
                    "name": "Zulun Zhu"
                },
                {
                    "authorId": "2261689385",
                    "name": "Siqiang Luo"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "43b839deec89604bb21e888d994bbf15889b921d",
            "title": "OCCAM: Towards Cost-Efficient and Accuracy-Aware Image Classification Inference",
            "abstract": "Image classification is a fundamental building block for a majority of computer vision applications. With the growing popularity and capacity of machine learning models, people can easily access trained image classifiers as a service online or offline. However, model use comes with a cost and classifiers of higher capacity usually incur higher inference costs. To harness the respective strengths of different classifiers, we propose a principled approach, OCCAM, to compute the best classifier assignment strategy over image classification queries (termed as the optimal model portfolio) so that the aggregated accuracy is maximized, under user-specified cost budgets. Our approach uses an unbiased and low-variance accuracy estimator and effectively computes the optimal solution by solving an integer linear programming problem. On a variety of real-world datasets, OCCAM achieves 40% cost reduction with little to no accuracy drop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123966440",
                    "name": "Dujian Ding"
                },
                {
                    "authorId": "2305484959",
                    "name": "Bicheng Xu"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "882d9f8704766d47aa85a30837353876f960dec6",
            "title": "In-depth Analysis of Densest Subgraph Discovery in a Unified Framework",
            "abstract": "As a fundamental topic in graph mining, Densest Subgraph Discovery (DSD) has found a wide spectrum of real applications. Several DSD algorithms, including exact and approximation algorithms, have been proposed in the literature. However, these algorithms have not been systematically and comprehensively compared under the same experimental settings. In this paper, we first propose a unified framework to incorporate all DSD algorithms from a high-level perspective. We then extensively compare representative DSD algorithms over a range of graphs -- from small to billion-scale -- and examine the effectiveness of all methods. Moreover, we suggest new variants of the DSD algorithms by combining the existing techniques, which are up to 10 X faster than the state-of-the-art algorithm with the same accuracy guarantee. Finally, based on the findings, we offer promising research opportunities. We believe that a deeper understanding of the behavior of existing algorithms can provide new valuable insights for future research. The codes are released at https://anonymous.4open.science/r/DensestSubgraph-245A",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2304048038",
                    "name": "Yingli Zhou"
                },
                {
                    "authorId": "2304304432",
                    "name": "Qingshuo Guo"
                },
                {
                    "authorId": "2305530492",
                    "name": "Yi Yang"
                },
                {
                    "authorId": "2266709093",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "8eed9db033f4a4c95de86f94d5fb161138d9a9f3",
            "title": "Autoregressive + Chain of Thought = Recurrent: Recurrence's Role in Language Models' Computability and a Revisit of Recurrent Transformer",
            "abstract": "The Transformer architecture excels in a variety of language modeling tasks, outperforming traditional neural architectures such as RNN and LSTM. This is partially due to its elimination of recurrent connections, which allows for parallel training and a smoother flow of gradients. However, this move away from recurrent structures places the Transformer model at the lower end of Chomsky's computational hierarchy, imposing limitations on its computational abilities. Consequently, even advanced Transformer-based models face considerable difficulties in tasks like counting, string reversal, and multiplication. These tasks, though seemingly elementary, require a level of computational complexity that exceeds the capabilities of the Transformer architecture. Concurrently, the emergence of ``Chain of Thought\"(CoT) prompting has enabled Transformer-based language models to tackle tasks that were previously impossible or poorly executed. In this work, we thoroughly investigate the influence of recurrent structures in neural models on their reasoning abilities and computability, contrasting the role autoregression plays in the neural models' computational power. We then shed light on how the CoT approach can mimic recurrent computation and act as a bridge between autoregression and recurrence in the context of language models. It is this approximated recurrence that notably improves the model's performance and computational capacity. Moreover, we revisit recent recurrent-based Transformer model designs, focusing on their computational abilities through our proposed concept of ``recurrence-completeness\"and identify key theoretical limitations in models like Linear Transformer and RWKV. Through this, we aim to provide insight into the neural model architectures and prompt better model design.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2321451878",
                    "name": "Xiang Zhang"
                },
                {
                    "authorId": "2302739012",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "96a4f254f777b2f85d4e771dd536315e6c874dd2",
            "title": "Predicting Cascading Failures with a Hyperparametric Diffusion Model",
            "abstract": "In this paper, we study cascading failures in power grids through the lens of information diffusion models. Similar to the spread of rumors or influence in an online social network, it has been observed that failures (outages) in a power grid can spread contagiously, driven by viral spread mechanisms. We employ a stochastic diffusion model that is Markovian (memoryless) and local (the activation of one node, i.e., transmission line, can only be caused by its neighbors). Our model integrates viral diffusion principles with physics-based concepts, by correlating the diffusion weights (contagion probabilities between transmission lines) with the hyperparametric Information Cascades (IC) model. We show that this diffusion model can be learned from traces of cascading failures, enabling accurate modeling and prediction of failure propagation. This approach facilitates actionable information through well-understood and efficient graph analysis methods and graph diffusion simulations. Furthermore, by leveraging the hyperparametric model, we can predict diffusion and mitigate the risks of cascading failures even in unseen grid configurations, whereas existing methods falter due to a lack of training data. Extensive experiments based on a benchmark power grid and simulations therein show that our approach effectively captures the failure diffusion phenomena and guides decisions to strengthen the grid, reducing the risk of large-scale cascading failures. Additionally, we characterize our model's sample complexity, improving upon the existing bound.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2306254569",
                    "name": "Bin Xiang"
                },
                {
                    "authorId": "2285888569",
                    "name": "Bogdan Cautis"
                },
                {
                    "authorId": "2290291188",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "2306259146",
                    "name": "Olga Mula"
                },
                {
                    "authorId": "1713586",
                    "name": "D. Niyato"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "97b6f4357d1e3ab40a7ee60acb5260a948e3641d",
            "title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing",
            "abstract": "Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123966440",
                    "name": "Dujian Ding"
                },
                {
                    "authorId": "2297849625",
                    "name": "Ankur Mallick"
                },
                {
                    "authorId": "2298452007",
                    "name": "Chi Wang"
                },
                {
                    "authorId": "2253669181",
                    "name": "Robert Sim"
                },
                {
                    "authorId": "2153292652",
                    "name": "Subhabrata Mukherjee"
                },
                {
                    "authorId": "3898805",
                    "name": "Victor R\u00fchle"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                }
            ]
        },
        {
            "paperId": "dadb2272f0a3def68f6b3eef5fd26b74d864a3cb",
            "title": "DetoxLLM: A Framework for Detoxification with Explanations",
            "abstract": "Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose DetoxLLM, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated parallel corpus. We further introduce explanation to promote transparency and trustworthiness. DetoxLLM additionally offers a unique paraphrase detector especially dedicated for the detoxification task to tackle the non-detoxifiable cases. Through experimental analysis, we demonstrate the effectiveness of our cross-platform corpus and the robustness of DetoxLLM against adversarial toxicity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "118865912",
                    "name": "Md. Tawkat Islam Khondaker"
                },
                {
                    "authorId": "2302739012",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "df7bcd2a81f93ed29a65b3ecd8179b8ef3171803",
            "title": "DuRE: Dual Contrastive Self Training for Semi-Supervised Relation Extraction",
            "abstract": "Document-level Relation Extraction (RE) aims to extract relation triples from documents. Existing document-RE models typically rely on supervised learning which requires substantial labeled data. To alleviate the amount of human supervision, Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models whenever labeled data is insufficient. However, existing ST methods in RE fail to tackle the challenge of long-tail relations. In this work, we propose DuRE, a novel ST framework to tackle these problems. DuRE jointly models RE classification and text generation as a dual process. In this way, our model could construct and utilize both pseudo text generated from given labels and pseudo labels predicted from available unlabeled text, which are gradually refined during the ST phase. We proposed a contrastive loss to leverage the signal of the RE classifier to improve generation quality. In addition, we propose a self-adaptive way to sample pseudo text from different relation classes. Experiments on two document-level RE tasks show that DuRE significantly boosts recall and F1 score with comparable precision, especially for long-tail relations against several strong baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185845790",
                    "name": "Yuxi Feng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "0f7e41d784014fbce20586a9e6626da70e420a29",
            "title": "KEST: Kernel Distance Based Efficient Self-Training for Improving Controllable Text Generation",
            "abstract": "Self-training (ST) has come to fruition in language understanding tasks by producing pseudo labels, which reduces the labeling bottleneck of language model fine-tuning. Nevertheless, in facilitating semi-supervised controllable language generation, ST faces two key challenges. First, augmented by self-generated pseudo text, generation models tend to over-exploit the previously learned text distribution, suffering from mode collapse and poor generation diversity. Second, generating pseudo text in each iteration is time-consuming, severely decelerating the training process. In this work, we propose KEST, a novel and efficient self-training framework to handle these problems. KEST utilizes a kernel-based loss, rather than standard cross entropy, to learn from the soft pseudo text produced by a shared non-autoregressive generator. We demonstrate both theoretically and empirically that KEST can benefit from more diverse pseudo text in an efficient manner, which allows not only refining and exploiting the previously fitted distribution but also enhanced exploration towards a larger potential text space, providing a guarantee of improved performance. Experiments on three controllable generation tasks demonstrate that KEST significantly improves control accuracy while maintaining comparable text fluency and generation diversity against several strong baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185845790",
                    "name": "Yuxi Feng"
                },
                {
                    "authorId": "3393196",
                    "name": "Xiaoyuan Yi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2187555771",
                    "name": "Xing Xie"
                }
            ]
        },
        {
            "paperId": "1181f53c082a126263b21835a9bff898cab84358",
            "title": "Mitigating Filter Bubbles Under a Competitive Diffusion Model",
            "abstract": "While social networks greatly facilitate information dissemination, they are well known to have contributed to the phenomena of filter bubbles and echo chambers. This in turn can lead to societal polarization and erosion of trust in public institutions. Mitigating filter bubbles is an urgent open problem. Recently, approaches based on the influence maximization paradigm have been proposed in our community for mitigating filter bubbles by balancing exposure to opposing viewpoints. However, existing works ignore the inherent competition between the adoption of opposing viewpoints by users. In this paper, we propose a realistic model for the filter bubble problem, which unlike previous work, captures thecompetition between opposing opinions propagating in a network as well as thecomplementary nature of the reward forexposing users to both those opinions. We formulate an optimization problem for mitigating filter bubbles under our model. We establish several evidences of the intrinsic difficulty in developing constant approximation to the problem and develop a heuristic and two instance-dependent approximation algorithms. Our experiments over 4 real datasets show that our heuristic far outperforms two state-of-the-art baselines as well as other algorithms in both efficiency and mitigating filter bubbles. We also empirically demonstrate that our best heuristic performs close to the optimal objective, which is obtained by utilizing the theoretical bounds of our approximation algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3274987",
                    "name": "Prithu Banerjee"
                },
                {
                    "authorId": "47151943",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "4a1205d42efdc0519c2b474ab758dc8ac665a0ec",
            "title": "A Survey of Densest Subgraph Discovery on Large Graphs",
            "abstract": "With the prevalence of graphs for modeling complex relationships among objects, the topic of graph mining has attracted a great deal of attention from both academic and industrial communities in recent years. As one of the most fundamental problems in graph mining, the densest subgraph discovery (DSD) problem has found a wide spectrum of real applications, such as discovery of filter bubbles in social media, finding groups of actors propagating misinformation in social media, social network community detection, graph index construction, regulatory motif discovery in DNA, fake follower detection, and so on. Theoretically, DSD closely relates to other fundamental graph problems, such as network flow and bipartite matching. Triggered by these applications and connections, DSD has garnered much attention from the database, data mining, theory, and network communities. In this survey, we first highlight the importance of DSD in various real-world applications and the unique challenges that need to be addressed. Subsequently, we classify existing DSD solutions into several groups, which cover around 50 research papers published in many well-known venues (e.g., SIGMOD, PVLDB, TODS, WWW), and conduct a thorough review of these solutions in each group. Afterwards, we analyze and compare the models and solutions in these works. Finally, we point out a list of promising future research directions. It is our hope that this survey not only helps researchers have a better understanding of existing densest subgraph models and solutions, but also provides insights and identifies directions for future study.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153435456",
                    "name": "Wensheng Luo"
                },
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "85bc4cf4d623b5984ec0be338575dd09cc823140",
            "title": "Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts",
            "abstract": "Weight-sharing supernets are crucial for performance estimation in cutting-edge neural architecture search (NAS) frameworks. Despite their ability to generate diverse subnetworks without retraining, the quality of these subnetworks is not guaranteed due to weight sharing. In NLP tasks like machine translation and pre-trained language modeling, there is a significant performance gap between supernet and training from scratch for the same model architecture, necessitating retraining post optimal architecture identification. This study introduces a solution called mixture-of-supernets, a generalized supernet formulation leveraging mixture-of-experts (MoE) to enhance supernet model expressiveness with minimal training overhead. Unlike conventional supernets, this method employs an architecture-based routing mechanism, enabling indirect sharing of model weights among subnetworks. This customization of weights for specific architectures, learned through gradient descent, minimizes retraining time, significantly enhancing training efficiency in NLP. The proposed method attains state-of-the-art (SoTA) performance in NAS for fast machine translation models, exhibiting a superior latency-BLEU tradeoff compared to HAT, the SoTA NAS framework for machine translation. Furthermore, it excels in NAS for building memory-efficient task-agnostic BERT models, surpassing NAS-BERT and AutoDistil across various model sizes. The code can be found at: https://github.com/UBC-NLP/MoS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065043351",
                    "name": "Ganesh Jawahar"
                },
                {
                    "authorId": "3274411",
                    "name": "Haichuan Yang"
                },
                {
                    "authorId": "2760194",
                    "name": "Yunyang Xiong"
                },
                {
                    "authorId": "2109370860",
                    "name": "Zechun Liu"
                },
                {
                    "authorId": "2848320",
                    "name": "Dilin Wang"
                },
                {
                    "authorId": null,
                    "name": "Fei Sun"
                },
                {
                    "authorId": "49141099",
                    "name": "Meng Li"
                },
                {
                    "authorId": "35164325",
                    "name": "Aasish Pappu"
                },
                {
                    "authorId": "9185192",
                    "name": "Barlas O\u011fuz"
                },
                {
                    "authorId": "2302739012",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2065915235",
                    "name": "Raghuraman Krishnamoorthi"
                },
                {
                    "authorId": "144137037",
                    "name": "Vikas Chandra"
                }
            ]
        },
        {
            "paperId": "b7e95c79a8279f2d7a2a9b24df0504a52e7c0c81",
            "title": "Stance Detection with Explanations",
            "abstract": "Identification of stance has recently gained a lot of attention with the extreme growth of fake news and filter bubbles. Over the last decade, many feature-based and deep-learning approaches have been proposed to solve stance detection. However, almost none of the existing works focus on providing a meaningful explanation for their prediction. In this work, we study stance detection with an emphasis on generating explanations for the predicted stance by capturing the pivotal argumentative structure embedded in a document. We propose to build a stance tree that utilizes rhetorical parsing to construct an evidence tree and to use Dempster Shafer Theory to aggregate the evidence. Human studies show that our unsupervised technique of generating stance explanations outperforms the SOTA extractive summarization method in terms of informativeness, non-redundancy, coverage, and overall quality. Furthermore, experiments show that our explanation-based stance prediction excels or matches the performance of the SOTA model on various benchmark datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2274423374",
                    "name": "Rudra Ranajee Saha"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2255504718",
                    "name": "Raymond T. Ng"
                }
            ]
        },
        {
            "paperId": "d9a47e5bd80268a1b879917d90b042db5e3de6af",
            "title": "LLM Performance Predictors are good initializers for Architecture Search",
            "abstract": "In this work, we utilize Large Language Models (LLMs) for a novel use case: constructing Performance Predictors (PP) that estimate the performance of specific deep neural network architectures on downstream tasks. We create PP prompts for LLMs, comprising (i) role descriptions, (ii) instructions for the LLM, (iii) hyperparameter definitions, and (iv) demonstrations presenting sample architectures with efficiency metrics and `training from scratch' performance. In machine translation (MT) tasks, GPT-4 with our PP prompts (LLM-PP) achieves a SoTA mean absolute error and a slight degradation in rank correlation coefficient compared to baseline predictors. Additionally, we demonstrate that predictions from LLM-PP can be distilled to a compact regression model (LLM-Distill-PP), which surprisingly retains much of the performance of LLM-PP. This presents a cost-effective alternative for resource-intensive performance estimation. Specifically, for Neural Architecture Search (NAS), we introduce a Hybrid-Search algorithm (HS-NAS) employing LLM-Distill-PP for the initial search stages and reverting to the baseline predictor later. HS-NAS performs similarly to SoTA NAS, reducing search hours by approximately 50%, and in some cases, improving latency, GFLOPs, and model size. The code can be found at: https://github.com/UBC-NLP/llmas.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065043351",
                    "name": "Ganesh Jawahar"
                },
                {
                    "authorId": "2302739012",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "123966440",
                    "name": "Dujian Ding"
                }
            ]
        },
        {
            "paperId": "ee48b3830900e602cb33b278f06a977fc1215127",
            "title": "PACT: Pretraining with Adversarial Contrastive Learning for Text Classification",
            "abstract": "We present PACT ( P retraining with A dversarial C ontrastive Learning for T ext Classification), a novel self-supervised framework for text classification. Instead of contrasting against in-batch negatives, a popular approach in the literature, PACT mines negatives closer to the anchor representation. PACT operates by en-dowing the standard pretraining mechanisms of BERT with adversarial contrastive learning objectives, allowing for effective joint optimization of token-and sentence-level pretraining of the BERT model. Our experiments on 13 diverse datasets including token-level, single-sentence, and sentence-pair text classification tasks show that PACT achieves consistent improvements over SOTA baselines. We further show that PACT regularizes both token-level and sentence-level embedding spaces into more uniform representations, thereby alleviating the undesirable anisotropic phenomenon of language models. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "118865912",
                    "name": "Md. Tawkat Islam Khondaker"
                },
                {
                    "authorId": "2302739012",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "f576c5afac6243baeb0962d9be31afccf0b0009f",
            "title": "MOSER: Scalable Network Motif Discovery using Serial Test",
            "abstract": "\n Given a graph\n G\n , a motif (e.g., 3-node clique) is a fundamental building block for\n G.\n Recently, motif-based graph analysis has attracted much attention due to its efficacy in tasks such as clustering, ranking, and link prediction. These tasks require Network Motif Discovery (NMD) at the early stage to identify the motifs of\n G.\n However, existing NMD solutions have two drawbacks: (1) Lack of theoretical guarantees on the quality of the samples generated using the existing methods, and (2) inefficient algorithms, which are not scalable for large graphs. These limitations hinder the exploration of motifs for analyzing large graphs. To address the above issues, we propose a novel solution named MOSER (\n MO\n tif Discovery using\n SER\n ial Test). This novel NMD framework leverages a significance testing method known as the serial test, which differs from the existing solutions. We further propose two fast incremental subgraph counting algorithms, allowing MOSER to scale to larger graphs than ever possible before. Extensive experimental results show that using MOSER can improve the state-of-the-art up to 5 orders of magnitude in efficiency and that the motifs found by MOSER facilitate downstream tasks such as link prediction.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280162863",
                    "name": "Mohammad Matin Najafi"
                },
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "2274742185",
                    "name": "Xiaodong Li"
                },
                {
                    "authorId": "2114454192",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "fee8081b5e5bd877433cba83626f1a35e9533ef6",
            "title": "Mid-Career Researcher, huh?",
            "abstract": "You just got promoted to Associate Professor. Like most things in life, whether joys or sorrows, the joy of this accomplishment will not last forever. However, that doesn't mean that you should not look back and reflect on years of hard work and tenacity that you have put in which have earned you this promotion, so first of all, congratulations! Take a moment to savor this accomplishment. On the other hand, it would be a mistake to not ask the question, what just changed about me. Let's see. You now have tenure and you have been promoted to a senior rank. In one sense, that translates to less stress, but in another, you do have to wonder whether it necessarily does mean less stress. On the flip side, you should also take advantage of the opportunity to ask, what are some new freedoms I have just earned. The stress component is driven by partly knowing, but also partly being unsure of, the expectations from a newly minted Associate Professor. The freedom component stems from knowing that you are now tenured, which hopefully means that you can embark on more daring, high risk projects, even if you don't feel like you know quite how to negotiate the trade-off between risk and impact.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "030f4f2078dc64b2148340dbedccef9f45c8e202",
            "title": "Automatic Detection of Entity-Manipulated Text using Factual Knowledge",
            "abstract": "In this work, we focus on the problem of distinguishing a human written news article from a news article that is created by manipulating entities in a human written news article (e.g., replacing entities with factually incorrect entities). Such manipulated articles can mislead the reader by posing as a human written news article. We propose a neural network based detector that detects manipulated news articles by reasoning about the facts mentioned in the article. Our proposed detector exploits factual knowledge via graph convolutional neural network along with the textual information in the news article. We also create challenging datasets for this task by considering various strategies to generate the new replacement entity (e.g., entity generation from GPT-2). In all the settings, our proposed model either matches or outperforms the state-of-the-art detector in terms of accuracy. Our code and data are available at https://github.com/UBC-NLP/manipulated_entity_detection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065043351",
                    "name": "Ganesh Jawahar"
                },
                {
                    "authorId": "1388437494",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "049f0b8367347a34bf023d0240c16079158c6b37",
            "title": "On a Quest for Combating Filter Bubbles and Misinformation",
            "abstract": "The advent of social networks and media has made it easier than ever for users to access up-to-date information as well as share news and views on matters of the world with many of their peers. Unfortunately, it has also led to increased societal polarization as well as deteriorating trust in institutions. Two of the problems that are blamed for this are filter bubbles and misinformation. Filter bubbles are the result of excessive personalization which has the benefit of enhancing relevance but comes at the price of limiting the exposure of users to a specific viewpoint. They are amplified by the so-called echo chambers that exist in social media, whereby members of a community mutually reinforce a fixed opinion or viewpoint on an issue. Misinformation, on the other hand, tends to propagate through the network, and studies show it does so faster and more virally than truth. Both problems manifest themselves in the form of groups of actors working in concert and providing mutual reinforcement. How can we recognize these groups? Having detected them, how can we counteract these problems? The first question can benefit from an examination of techniques developed to search for communities or more generally dense subgraphs from an underlying network. As for the second question, a natural approach for countering filter bubbles is to launch some kind of counter-campaign to try and enhance the balance in users' exposure to viewpoints. Countermeasures for misinformation propagating through a network, on the other hand, are manifold and can depend on who is planning the countermeasure. For example, the network host can intervene and take steps to limit the propagation of misinformation, but these actions come with a cost. Besides the political sensitivity and cost of limiting freedom of expression, what if the intervention was by mistake done on genuine information? As another example, a third party interested in countering the propagation of misinformation may launch a counter-campaign. Interestingly, some of the ideas behind designing such campaigns have strong connections to a classic problem called Influence Maximization, studied in a very different context, driven by different applications like viral marketing, infection containment, and revenue or welfare maximization. In this talk, we will examine research on detecting dense subgraphs as well as competitive influence maximization and discuss how that can inspire techniques for addressing the two problems above.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "16402ca4c10344fe997a554f3592bcb1e0beb9da",
            "title": "A Convex-Programming Approach for Efficient Directed Densest Subgraph Discovery",
            "abstract": "Given a directed graph G, the directed densest subgraph (DDS) problem refers to finding a subgraph from G, whose density is the highest among all subgraphs of G. The DDS problem is fundamental to a wide range of applications, such as fake follower detection and community mining. Theoretically, the DDS problem closely connects to other essential graph problems, such as network flow and bipartite matching. However, existing DDS solutions suffer from efficiency and scalability issues. In this paper, we develop a convex-programming-based solution by transforming the DDS problem into a set of linear programs. Based on the duality of linear programs, we develop efficient exact and approximation algorithms. Especially, our approximation algorithm can support flexible parameterized approximation guarantees. We have performed an extensive empirical evaluation of our approaches on eight real large datasets. The results show that our proposed algorithms are up to five orders of magnitude faster than the state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "151480433",
                    "name": "Xiaolin Han"
                }
            ]
        },
        {
            "paperId": "1f204be1d1b041db4f7fadee96e2ecec077baca4",
            "title": "FirmCore Decomposition of Multilayer Networks",
            "abstract": "A key graph mining primitive is extracting dense structures from graphs, and this has led to interesting notions such as k-cores which subsequently have been employed as building blocks for capturing the structure of complex networks and for designing efficient approximation algorithms for challenging problems such as finding the densest subgraph. In applications such as biological, social, and transportation networks, interactions between objects span multiple aspects. Multilayer (ML) networks have been proposed for accurately modeling such applications. In this paper, we present FirmCore, a new family of dense subgraphs in ML networks, and show that it satisfies many of the nice properties of k-cores in single-layer graphs. Unlike the state of the art core decomposition of ML graphs, FirmCores have a polynomial time algorithm, making them a powerful tool for understanding the structure of massive ML networks. We also extend FirmCore for directed ML graphs. We show that FirmCores and directed FirmCores can be used to obtain efficient approximation algorithms for finding the densest subgraphs of ML graphs and their directed counterparts. Our extensive experiments over several real ML graphs show that our FirmCore decomposition algorithm is significantly more efficient than known algorithms for core decompositions of ML graphs. Furthermore, it returns solutions of matching or better quality for the densest subgraph problem over (possibly directed) ML graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2163400060",
                    "name": "Farnoosh Hashemi"
                },
                {
                    "authorId": "46211294",
                    "name": "Ali Behrouz"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "23fe77fd81fe9eaad8e8c7164ce759d2ca0f65f4",
            "title": "On Efficient Approximate Queries over Machine Learning Models",
            "abstract": "\n The question of answering queries over ML predictions has been gaining attention in the database community. This question is challenging because finding high quality answers by invoking an\n oracle\n such as a human expert or an expensive deep neural network model on every single item in the DB and then applying the query, can be prohibitive. We develop a novel unified framework for approximate query answering by leveraging a\n proxy\n to minimize the oracle usage of finding high quality answers for both Precision-Target (PT) and Recall-Target (RT) queries. Our framework uses a judicious combination of invoking the expensive oracle on data samples and applying the cheap proxy on the DB objects. It relies on two assumptions. Under the P\n roxy\n Q\n uality\n assumption, we develop two algorithms: PQA that efficiently finds high quality answers with high probability and no oracle calls, and PQE, a heuristic extension that achieves empirically good performance with a small number of oracle calls. Alternatively, under the C\n ore\n S\n et\n C\n losure\n assumption, we develop two algorithms: CSC that efficiently returns high quality answers with high probability and minimal oracle usage, and CSE, which extends it to more general settings. Our extensive experiments on five real-world datasets on both query types, PT and RT, demonstrate that our algorithms outperform the state-of-the-art and achieve high result quality with provable statistical guarantees.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123966440",
                    "name": "Dujian Ding"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "27c00c75ea9b2e71dc70e5a2708c5f065fe170a7",
            "title": "Small Character Models Match Large Word Models for Autocomplete Under Memory Constraints",
            "abstract": "Autocomplete is a task where the user inputs a piece of text, termed prompt, which is conditioned by the model to generate semantically coherent continuation. Existing works for this task have primarily focused on datasets (e.g., email, chat) with high frequency user prompt patterns (or focused prompts) where word-based language models have been quite effective. In this work, we study the more challenging open-domain setting consisting of low frequency user prompt patterns (or broad prompts, e.g., prompt about 93rd academy awards) and demonstrate the effectiveness of character-based language models. We study this problem under memory-constrained settings (e.g., edge devices and smartphones), where character-based representation is effective in reducing the overall model size (in terms of parameters). We use WikiText-103 benchmark to simulate broad prompts and demonstrate that character models rival word models in exact match accuracy for the autocomplete task, when controlled for the model size. For instance, we show that a 20M parameter character model performs similar to an 80M parameter word model in the vanilla setting. We further propose novel methods to improve character models by incorporating inductive bias in the form of compositional information and representation transfer from large word models. Datasets and code used in this work are available at https://github.com/UBC-NLP/char_autocomplete.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145123979",
                    "name": "Ganesh Jawahar"
                },
                {
                    "authorId": "2153292652",
                    "name": "Subhabrata Mukherjee"
                },
                {
                    "authorId": "1780951",
                    "name": "Debadeepta Dey"
                },
                {
                    "authorId": "2302739012",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2157424631",
                    "name": "C. C. T. Mendes"
                },
                {
                    "authorId": "144977605",
                    "name": "Gustavo de Rosa"
                },
                {
                    "authorId": "47973411",
                    "name": "S. Shah"
                }
            ]
        },
        {
            "paperId": "2ca647dab6459225526823e240d7eb4f2d660668",
            "title": "FirmTruss Community Search in Multilayer Networks",
            "abstract": "\n In applications such as biological, social, and transportation networks, interactions between objects span multiple aspects. For accurately modeling such applications, multilayer networks have been proposed. Community search allows for personalized community discovery and has a wide range of applications in large real-world networks. While community search has been widely explored for single-layer graphs, the problem for multilayer graphs has just recently attracted attention. Existing community models in multilayer graphs have several limitations, including disconnectivity, free-rider effect, resolution limits, and inefficiency. To address these limitations, we study the problem of community search over large multilayer graphs. We first introduce\n FirmTruss\n , a novel dense structure in multilayer networks, which extends the notion of truss to multilayer graphs. We show that FirmTrusses possess nice structural and computational properties and bring many advantages compared to the existing models. Building on this, we present a new community model based on FirmTruss, called\n FTCS\n , and show that finding an FTCS community is NP-hard. We propose two efficient 2-approximation algorithms, and show that no polynomial-time algorithm can have a better approximation guarantee unless P = NP. We propose an index-based method to further improve the efficiency of the algorithms. We then consider attributed multilayer networks and propose a new community model based on network homophily. We show that community search in attributed multilayer graphs is NP-hard and present an effective and efficient approximation algorithm. Experimental studies on real-world graphs with ground-truth communities validate the quality of the solutions we obtain and the efficiency of the proposed algorithms.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46211294",
                    "name": "Ali Behrouz"
                },
                {
                    "authorId": "2163400060",
                    "name": "Farnoosh Hashemi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "345eede126d94ecc2ee130808edf458886ef67d3",
            "title": "Cross-Platform and Cross-Domain Abusive Language Detection with Supervised Contrastive Learning",
            "abstract": "The prevalence of abusive language on different online platforms has been a major concern that raises the need for automated cross-platform abusive language detection. However, prior works focus on concatenating data from multiple platforms, inherently adopting Empirical Risk Minimization (ERM) method. In this work, we address this challenge from the perspective of domain generalization objective. We design SCL-Fish, a supervised contrastive learning integrated meta-learning algorithm to detect abusive language on unseen platforms. Our experimental analysis shows that SCL-Fish achieves better performance over ERM and the existing state-of-the-art models. We also show that SCL-Fish is data-efficient and achieves comparable performance with the large-scale pre-trained models upon finetuning for the abusive language detection task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "118865912",
                    "name": "Md. Tawkat Islam Khondaker"
                },
                {
                    "authorId": "2302739012",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "4a423e6ba10b76f8ded161ea4ce79e1a9be2f856",
            "title": "Misinformation Mitigation under Differential Propagation Rates and Temporal Penalties",
            "abstract": "\n We propose an information propagation model that captures important temporal aspects that have been well observed in the dynamics of fake news diffusion, in contrast with the diffusion of truth. The model accounts for differential propagation rates of truth and misinformation and for user reaction times. We study a time-sensitive variant of the\n misinformation mitigation\n problem, where\n k\n seeds are to be selected to activate a truth campaign so as to minimize the number of users that adopt misinformation propagating through a social network. We show that the resulting objective is non-submodular and employ a sandwiching technique by defining submodular upper and lower bounding functions, providing data-dependent guarantees. In order to enable the use of a reverse sampling framework, we introduce a weighted version of reverse reachability sets that captures the associated differential propagation rates and establish a key equivalence between weighted set coverage probabilities and mitigation with respect to the sandwiching functions. Further, we propose an offline reverse sampling framework that provides (1 - 1/\n e\n - \u03f5)-approximate solutions to our bounding functions and introduce an importance sampling technique to reduce the sample complexity of our solution. Finally, we show how our framework can provide an anytime solution to the problem. Experiments over five datasets show that our approach outperforms previous approaches and is robust to uncertainty in the model parameters.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053261077",
                    "name": "Michael Simpson"
                },
                {
                    "authorId": "2163400060",
                    "name": "Farnoosh Hashemi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "5165de3cd4f8dc9d88e82d55f4798013d57cc0f1",
            "title": "AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation",
            "abstract": "Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows for adaptive compute -- where different amounts of computations are used for different tokens in the input. Adaptivity comes naturally from routing decisions which send tokens to experts of different sizes. AutoMoE code, data, and trained models are available at https://aka.ms/AutoMoE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065043351",
                    "name": "Ganesh Jawahar"
                },
                {
                    "authorId": "2153292652",
                    "name": "Subhabrata Mukherjee"
                },
                {
                    "authorId": "46522098",
                    "name": "Xiaodong Liu"
                },
                {
                    "authorId": "2152658577",
                    "name": "Young Jin Kim"
                },
                {
                    "authorId": "2302739012",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                },
                {
                    "authorId": "121645690",
                    "name": "S\u00e9bastien Bubeck"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ]
        },
        {
            "paperId": "5d510da7c5d1b2a268c8a7b1c42ac51c4664b3f9",
            "title": "Voting-based Opinion Maximization",
            "abstract": "We investigate the novel problem of voting-based opinion maximization in a social network: Find a given number of seed nodes for a target campaigner, in the presence of other competing campaigns, so as to maximize a voting-based score for the target campaigner at a given time horizon.The bulk of the influence maximization literature assumes that social network users can switch between only two discrete states, inactive and active, and the choice to switch is frozen upon one-time activation. In reality, even when having a preferred opinion, a user may not completely despise the other opinions, and the preference level may vary over time due to social influence. To this end, we employ models rooted in opinion formation and diffusion, and use several voting-based scores to determine a user\u2019s vote for each of the multiple campaigners at a given time horizon.Our problem is NP-hard and non-submodular for various scores. We design greedy seed selection algorithms with quality guarantees for our scoring functions via sandwich approximation. To improve the efficiency, we develop random walk and sketch-based opinion computation, with quality guarantees. Empirical results validate our effectiveness, efficiency, and scalability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150029298",
                    "name": "Arkaprava Saha"
                },
                {
                    "authorId": "8766005",
                    "name": "Xiangyu Ke"
                },
                {
                    "authorId": "2108514592",
                    "name": "Arijit Khan"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "5d7a7dfcc73d2d245b9eb3662e9f4e390b2486db",
            "title": "DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation",
            "abstract": "Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models when labeled data is insufficient. However, it remains challenging to incorporate ST into attribute-controllable language generation. Augmented only by self-generated pseudo text, generation models over-exploit the previously learned text space and fail to explore a larger one, suffering from a restricted generalization boundary and limited controllability. In this work, we propose DuNST, a novel ST framework to tackle these problems. DuNST jointly models text generation and classification as a dual process and further perturbs and escapes from the collapsed space by adding two kinds of flexible noise. In this way, our model could construct and utilize both pseudo text generated from given labels and pseudo labels predicted from available unlabeled text, which are gradually refined during the ST phase. We theoretically demonstrate that DuNST can be regarded as enhancing the exploration of the potentially larger real text space while maintaining exploitation, guaranteeing improved performance. Experiments on three controllable generation tasks show that DuNST significantly boosts control accuracy with comparable generation fluency and diversity against several strong baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185845790",
                    "name": "Yuxi Feng"
                },
                {
                    "authorId": "3393196",
                    "name": "Xiaoyuan Yi"
                },
                {
                    "authorId": "2108045320",
                    "name": "Xiting Wang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2110971997",
                    "name": "Xing Xie"
                }
            ]
        },
        {
            "paperId": "e0a5634fd334b4a07e8782a0aae15dca31a17be7",
            "title": "A Benchmark Study of Contrastive Learning for Arabic Social Meaning",
            "abstract": "Contrastive learning (CL) has brought significant progress to various NLP tasks. Despite such a progress, CL has not been applied to Arabic NLP. Nor is it clear how much benefits it could bring to particular classes of tasks such as social meaning (e.g., sentiment analysis, dialect identification, hate speech detection). In this work, we present a comprehensive benchmark study of state-of-the-art supervised CL methods on a wide array of Arabic social meaning tasks. Through an extensive empirical analysis, we show that CL methods outperform vanilla finetuning on most of the tasks. We also show that CL can be data efficient and quantify this efficiency, demonstrating the promise of these methods in low-resource settings vis-a-vis the particular downstream tasks (especially label granularity).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "118865912",
                    "name": "Md. Tawkat Islam Khondaker"
                },
                {
                    "authorId": "17771023",
                    "name": "El Moatez Billah Nagoudi"
                },
                {
                    "authorId": "1397289779",
                    "name": "AbdelRahim Elmadany"
                },
                {
                    "authorId": "2065312024",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "ed4bd341b9652ab281ec84172f88b931ff2d0648",
            "title": "AutoMoE: Neural Architecture Search for Efficient Sparsely Activated Transformers",
            "abstract": "Neural architecture search (NAS) has demonstrated promising results on identifying efficient Transformer architectures which outperform manually designed ones for natural language tasks like neural machine translation (NMT). Existing NAS methods operate on a space of dense architectures, where all of the subarchitecture weights are activated for every input. Motivated by the recent advances in sparsely activated models like the Mixture-of-Experts (MoE) model, we introduce sparse architectures with conditional computation into the NAS search space. Given this expressive search space which subsumes prior densely activated architectures, we develop a new framework AutoMoE to search for efficient sparsely activated sub-Transformers. AutoMoE-generated sparse models obtain (i) 3\u00d7 FLOPs reduction over manually designed dense Transformers and (ii) 23% FLOPs reduction over state-of-the-art NAS-generated dense sub-Transformers with parity in BLEU score on benchmark datasets for NMT. AutoMoE consists of three training phases: (a) Heterogeneous search space design with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?); (b) SuperNet training that jointly trains several subnetworks sampled from the large search space by weight-sharing; (c) Evolutionary search for the architecture with the optimal trade-off between task performance and computational constraint like FLOPs and latency. AutoMoE code, data and trained models are available at https://github.com/microsoft/AutoMoE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065043351",
                    "name": "Ganesh Jawahar"
                },
                {
                    "authorId": "2153292652",
                    "name": "Subhabrata Mukherjee"
                },
                {
                    "authorId": "46522098",
                    "name": "Xiaodong Liu"
                },
                {
                    "authorId": "2152658577",
                    "name": "Young Jin Kim"
                },
                {
                    "authorId": "2302739012",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2072795428",
                    "name": "A. Awadallah"
                },
                {
                    "authorId": "121645690",
                    "name": "S\u00e9bastien Bubeck"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ]
        },
        {
            "paperId": "251eefcad91f05e8babce1ced33beb1e2268c4ed",
            "title": "To Intervene or Not To Intervene: Cost based Intervention for Combating Fake News",
            "abstract": "Social media platforms provide valuable and powerful means with which users can share content, comment, and communicate. They also suffer from abuse through the dissemination of fake news and misinformation. While a fair amount of work has been done on detecting fake news, on the complementary problem of limiting its propagation, progress has been modest. Once an item is detected as fake, a social media company can intervene on the item and take an appropriate action, including hard intervention (e.g., removing an account) and soft intervention (e.g., labeling the item as \"suspicious\"). Given that fake news detectors are not 100% reliable, we study the problem of developing a cost aware intervention policy which decides whether to intervene based on the truthiness and popularity of the item. Our solution, Solomon, consists of three modular components - truthiness estimation, popularity estimation (with and without intervention), and intervention policy. Our extensive experiments on real and fake news from multiple domains show that Solomon can perform effective intervention.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                },
                {
                    "authorId": "2053261077",
                    "name": "Michael Simpson"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "2ae7d4ab81a3a577e914fbc9876286f3bee7e865",
            "title": "Stealthy Targeted Data Poisoning Attack on Knowledge Graphs",
            "abstract": "A host of different KG embedding techniques have emerged recently and have been empirically shown to be very effective in accurately predicting missing facts in a KG, thus improving its coverage and quality. Unfortunately, embedding techniques can fall prey to adversarial data poisoning attack. In this form of attack, facts may be added to or deleted from a KG, called performing perturbations, that results in the manipulation of the plausibility of target facts in a KG. While recent works confirm this intuition, the attacks considered there ignore the risk of exposure. Intuitively, an attack is of limited value if it is highly likely to be caught, i.e., exposed. To address this, we introduce a notion of the exposure risk and propose a novel problem of attacking a KG by means of perturbations where the goal is to maximize the manipulation of the target fact\u2019s plausibility while keeping the risk of exposure under a given budget. We design a deep reinforcement learning-based framework, called RATA, that learns to use low-risk perturbations without compromising on the performance, i.e., manipulation of target fact plausibility. We test the performance of RATA against recently proposed strategies for KG attacks, on two different benchmark datasets and on different kinds of target facts. Our experiments show that RATA achieves state-of-the-art performance even while using a fraction of the risk.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3274987",
                    "name": "Prithu Banerjee"
                },
                {
                    "authorId": "2074100",
                    "name": "Lingyang Chu"
                },
                {
                    "authorId": "2144288655",
                    "name": "Yong Zhang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "49680751",
                    "name": "Lanjun Wang"
                }
            ]
        },
        {
            "paperId": "3561c268f27a017d7a69ffde9ca2101ae0e9f9e1",
            "title": "On Directed Densest Subgraph Discovery",
            "abstract": "Given a directed graph G, the directed densest subgraph (DDS) problem refers to the finding of a subgraph from G, whose density is the highest among all the subgraphs of G. The DDS problem is fundamental to a wide range of applications, such as fraud detection, community mining, and graph compression. However, existing DDS solutions suffer from efficiency and scalability problems: on a 3,000-edge graph, it takes three days for one of the best exact algorithms to complete. In this article, we develop an efficient and scalable DDS solution. We introduce the notion of [x, y]-core, which is a dense subgraph for G, and show that the densest subgraph can be accurately located through the [x, y]-core with theoretical guarantees. Based on the [x, y]-core, we develop exact and approximation algorithms. We further study the problems of maintaining the DDS over dynamic directed graphs and finding the weighted DDS on weighted directed graphs, and we develop efficient non-trivial algorithms to solve these two problems by extending our DDS algorithms. We have performed an extensive evaluation of our approaches on 15 real large datasets. The results show that our proposed solutions are up to six orders of magnitude faster than the state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2402302",
                    "name": "Wenjie Zhang"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                }
            ]
        },
        {
            "paperId": "3fdd66a0eed6ee259c1c91321f866ed1f737f340",
            "title": "Exploring Text-to-Text Transformers for English to Hinglish Machine Translation with Synthetic Code-Mixing",
            "abstract": "We describe models focused at the understudied problem of translating between monolingual and code-mixed language pairs. More specifically, we offer a wide range of models that convert monolingual English text into Hinglish (code-mixed Hindi and English). Given the recent success of pretrained language models, we also test the utility of two recent Transformer-based encoder-decoder models (i.e., mT5 and mBART) on the task finding both to work well. Given the paucity of training data for code-mixing, we also propose a dependency-free method for generating code-mixed texts from bilingual distributed representations that we exploit for improving language model performance. In particular, armed with this additional data, we adopt a curriculum learning approach where we first finetune the language models on synthetic data then on gold code-mixed data. We find that, although simple, our synthetic code-mixing method is competitive with (and in some cases is even superior to) several standard methods (backtranslation, method based on equivalence constraint theory) under a diverse set of conditions. Our work shows that the mT5 model, finetuned following the curriculum learning procedure, achieves best translation performance (12.67 BLEU). Our models place first in the overall ranking of the English-Hinglish official shared task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145123979",
                    "name": "Ganesh Jawahar"
                },
                {
                    "authorId": "17771023",
                    "name": "El Moatez Billah Nagoudi"
                },
                {
                    "authorId": "1388437494",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "4231b64ec70cb28dd5001d09aa741d118d4df813",
            "title": "Efficient Directed Densest Subgraph Discovery",
            "abstract": "Given a directed graph G, the directed densest subgraph (DDS) problem refers to the finding of a subgraph from G, whose density is the highest among all the subgraphs of G. The DDS problem is fundamental to a wide range of applications, such as fraud detection, community mining, and graph compression. However, existing DDS solutions suffer from efficiency and scalability problems: on a threethousand- edge graph, it takes three days for one of the best exact algorithms to complete. In this paper, we develop an efficient and scalable DDS solution. We introduce the notion of [x, y]-core, which is a dense subgraph for G, and show that the densest subgraph can be accurately located through the [x, y]-core with theoretical guarantees. Based on the [x, y]-core, we develop both exact and approximation algorithms. We have performed an extensive evaluation of our approaches on eight real large datasets. The results show that our proposed solutions are up to six orders of magnitude faster than the state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2402302",
                    "name": "Wenjie Zhang"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                }
            ]
        },
        {
            "paperId": "781c06e8e1ae3a378f90eb2e18170fda7925c7fe",
            "title": "Efficient and Effective Algorithms for Revenue Maximization in Social Advertising",
            "abstract": "We consider the revenue maximization problem in social advertising, where a social network platform owner needs to select seed users for a group of advertisers, each with a payment budget, such that the total expected revenue that the owner gains from the advertisers by propagating their ads in the network is maximized. Previous studies on this problem show that it is intractable and present approximation algorithms. We revisit this problem from a fresh perspective and develop novel efficient approximation algorithms, both under the setting where an exact influence oracle is assumed and under one where this assumption is relaxed. Our approximation ratios significantly improve upon the previous ones. Furthermore, we empirically show, using extensive experiments on four datasets, that our algorithms considerably outperform the existing methods on both the solution quality and computation efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112027880",
                    "name": "Kai Han"
                },
                {
                    "authorId": "2121624031",
                    "name": "Benwei Wu"
                },
                {
                    "authorId": "144066821",
                    "name": "Jing Tang"
                },
                {
                    "authorId": "2047128232",
                    "name": "Shuang Cui"
                },
                {
                    "authorId": "3222408",
                    "name": "\u00c7igdem Aslay"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "c8ef7d8c88c04b78fe81271c6128a75d1710a05d",
            "title": "MAYUR: Map conflAtion using earlY prUning and Rank join",
            "abstract": "OpenStreetMap (OSM) is a collaborative good quality crowd-sourced geospatial database (GDB). The quality of OSM is generally very good, it lacks good coverage in many parts of the world. A natural approach for extending its coverage is to conflate missing spatial features from other GDBs into OSM, but this is laborious and time-consuming. We propose a system MAYUR solving road network conflation between two vector GDBs, representing the GDBs as a graph of road intersections (vertices) and road segments (edges). MAYUR is based on a novel map matching framework that adapts the classic Rank Join in databases, where each edge of the reference GDB is modeled as a relation. Our algorithm finds the best matching between a reference and target GDB, respecting the connectivity of the road network. While classic Rank Join in databases gets quickly inefficient on instances with more than 10 relations, MAYUR's enhanced Rank Join incorporates three optimizations that boost the algorithm's efficiency, making it scale to our problem setting featuring hundreds to thousands of relations. Our manual evaluation of MAYUR conflation results on sidewalks in OSM and Boston Open Data shows an impressive 98.65% precision and 99.55% recall.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "23493112",
                    "name": "Gorisha Agarwal"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2149395276",
                    "name": "Xiaoming Gao"
                },
                {
                    "authorId": "98248322",
                    "name": "Kevin Ventullo"
                },
                {
                    "authorId": "2136958854",
                    "name": "Saurav Mohapatra"
                },
                {
                    "authorId": "2114709708",
                    "name": "Saikat Basu"
                }
            ]
        },
        {
            "paperId": "128d207b22353cbde0ed321156dc5187befa25cf",
            "title": "A High Precision Pipeline for Financial Knowledge Graph Construction",
            "abstract": "Motivated by applications such as question answering, fact checking, and data integration, there is significant interest in constructing knowledge graphs by extracting information from unstructured information sources, particularly text documents. Knowledge graphs have emerged as a standard for structured knowledge representation, whereby entities and their inter-relations are represented and conveniently stored as (subject,predicate,object) triples in a graph that can be used to power various downstream applications. The proliferation of financial news sources reporting on companies, markets, currencies, and stocks presents an opportunity for extracting valuable knowledge about this crucial domain. In this paper, we focus on constructing a knowledge graph automatically by information extraction from a large corpus of financial news articles. For that purpose, we develop a high precision knowledge extraction pipeline tailored for the financial domain. This pipeline combines multiple information extraction techniques with a financial dictionary that we built, all working together to produce over 342,000 compact extractions from over 288,000 financial news articles, with a precision of 78% at the top-100 extractions.The extracted triples are stored in a knowledge graph making them readily available for use in downstream applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2022646957",
                    "name": "Sarah Elhammadi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "2053261077",
                    "name": "Michael Simpson"
                },
                {
                    "authorId": "2422046",
                    "name": "Baoxing Huai"
                },
                {
                    "authorId": "2108271253",
                    "name": "Zhefeng Wang"
                },
                {
                    "authorId": "49680751",
                    "name": "Lanjun Wang"
                }
            ]
        },
        {
            "paperId": "4ff242ce1d4cb0ee026d7888a0976cc441e61e63",
            "title": "Maximizing Social Welfare in a Competitive Diffusion Model",
            "abstract": "\n Influence maximization (IM) has garnered a lot of attention in the literature owing to applications such as viral marketing and infection containment. It aims to select a small number of seed users to adopt an item such that adoption propagates to a large number of users in the network. Competitive IM focuses on the propagation of competing items in the network. Existing works on competitive IM have several limitations. (1) They fail to incorporate economic incentives in users' decision making in item adoptions. (2) Majority of the works aim to maximize the adoption of one particular item, and ignore the collective role that different items play. (3) They focus mostly on one aspect of competition - pure competition. To address these concerns we study competitive IM under a utility-driven propagation model called UIC, and study social welfare maximization. The problem in general is not only NP-hard but also NP-hard to approximate within any constant factor. We, therefore, devise instant dependent efficient approximation algorithms for the general case as well as a (1 - 1/\n e - \u2208\n )-approximation algorithm for a restricted setting. Our algorithms outperform different baselines on competitive IM, both in terms of solution quality and running time on large real networks under both synthetic and real utility configurations.\n",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3274987",
                    "name": "Prithu Banerjee"
                },
                {
                    "authorId": "47151943",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "8d13e821be3308ccdd5e28b8e816b78fd1cd135b",
            "title": "Summarizing Hierarchical Multidimensional Data",
            "abstract": "Data scientists typically analyze and extract insights from large multidimensional data sets such as US census data, enterprise sales data, and so on. But before sophisticated machine learning and statistical methods are employed, it is useful to build and explore concise summaries of the data set. While a variety of summaries have been proposed over the years, the goal of creating a concise summary of multidimensional data that can provide worst-case accuracy guarantees has remained elusive. In this paper, we propose Tree Summaries, which attain this challenging goal over arbitrary hierarchical multidimensional data sets. Intuitively, a Tree Summary is a weighted \"embedded tree\" in the lattice that is the cross-product of the dimension hierarchies; individual data values can be efficiently estimated by looking up the weight of their unique closest ancestor in the Tree Summary. We study the problems of generating lossless as well as (given a desired worst-case accuracy guarantee a) lossy Tree Summaries. We develop a polynomial-time algorithm that constructs the optimal (i.e., most concise) Tree Summary for each of these problems; this is a surprising result given the NP-hardness of constructing a variety of other optimal summaries over multidimensional data. We complement our analytical results with an empirical evaluation of our algorithm, and demonstrate with a detailed set of experiments on real and synthetic data sets that our algorithm outperforms prior methods in terms of conciseness of summaries or accuracy of estimation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2080286891",
                    "name": "Alexandra Kim"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "9438bc5626b2d9a771cecc7a41ecabf6639db53c",
            "title": "Automatic Detection of Machine Generated Text: A Critical Survey",
            "abstract": "Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by TGM from human written text play a vital role in mitigating such misuse of TGMs. Recently, there has been a flurry of works from both natural language processing (NLP) and machine learning (ML) communities to build accurate detectors for English. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2065043351",
                    "name": "Ganesh Jawahar"
                },
                {
                    "authorId": "1388437494",
                    "name": "Muhammad Abdul-Mageed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "971c244dd3eb9432e7b63692a755271a2f1203c4",
            "title": "TSA: A Truthful Mechanism for Social Advertising",
            "abstract": "Social advertising exploits the interconnectivity of users in social networks to spread advertisement and generate user engagements. A lot of research has focused on how to select the best subset of users in a social network to maximize the number of engagements or the generated revenue of the advertisement. However, there is a lack of studies that consider the advertiser's value-per-engagement, i.e., how much an advertiser is maximally willing to pay for each engagement. Prior work on social advertising is based on the classical framework of influence maximization. In this paper, we propose a model where advertisers compete in an auction mechanism for the influential users within a social network. The auction mechanism can dynamically determine payments for advertisers based on their reported values. The main problem is to find auctions which incentivize advertisers to truthfully reveal their values, and also respect each advertiser's budget constraint. To tackle this problem, we propose a new truthful auction mechanism called TSA. Compared with existing approaches on real and synthetic datasets, TSA performs significantly better in terms of generated revenue.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3431016",
                    "name": "Tobias Grubenmann"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "a4b62f87a8e13a214da65df48953ed10e3f44bf4",
            "title": "Efficient Algorithms for Densest Subgraph Discovery on Large Directed Graphs",
            "abstract": "Given a directed graph G, the directed densest subgraph (DDS) problem refers to the finding of a subgraph from G, whose density is the highest among all the subgraphs of G. The DDS problem is fundamental to a wide range of applications, such as fraud detection, community mining, and graph compression. However, existing DDS solutions suffer from efficiency and scalability problems: on a three-thousand-edge graph, it takes three days for one of the best exact algorithms to complete. In this paper, we develop an efficient and scalable DDS solution. We introduce the notion of [x, y]-core, which is a dense subgraph for G, and show that the densest subgraph can be accurately located through the [x, y]-core with theoretical guarantees. Based on the [x, y]-core, we develop exact and approximation algorithms. We have performed an extensive evaluation of our approaches on eight real large datasets. The results show that our proposed solutions are up to six orders of magnitude faster than the state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2402302",
                    "name": "Wenjie Zhang"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                }
            ]
        },
        {
            "paperId": "43cc567fd98c6c5cb192911ffe3ad7f9da4ffca4",
            "title": "LINC: A Motif Counting Algorithm for Uncertain Graphs",
            "abstract": "In graph applications (e.g., biological and social networks), various analytics tasks (e.g., clustering and community search) are carried out to extract insight from large and complex graphs. Central to these tasks is the counting of the number of motifs, which are graphs with a few nodes. Recently, researchers have developed several fast motif counting algorithms. Most of these solutions assume that graphs are deterministic, i.e., the graph edges are certain to exist. However, due to measurement and statistical prediction errors, this assumption may not hold, and hence the analysis quality can be affected. To address this issue, we examine how to count motifs on uncertain graphs, whose edges only exist probabilistically. Particularly, we propose a solution framework that can be used by existing deterministic motif counting algorithms. We further propose an approximation algorithm. Extensive experiments on real datasets show that our algorithms are more effective and efficient than existing solutions. PVLDB Reference Format: Chenhao Ma, Reynold Cheng, Laks V.S. Lakshmanan, Tobias Grubenmann, Yixiang Fang, and Xiaodong Li. LINC: A Motif Counting Algorithm for Uncertain Graphs. PVLDB, 13(2): 155-168, 2019. DOI: https://doi.org/10.14778/3364324.3364330",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108786828",
                    "name": "Chenhao Ma"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "3431016",
                    "name": "Tobias Grubenmann"
                },
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "2144442653",
                    "name": "Xiaodong Li"
                }
            ]
        },
        {
            "paperId": "527d5b624094a99a782a5be1d8396456a55b7340",
            "title": "Combating Fake News: A Data Management and Mining Perspective",
            "abstract": "Fake news is a major threat to global democracy resulting in diminished trust in government, journalism and civil society. The public popularity of social media and social networks has caused a contagion of fake news where conspiracy theories, disinformation and extreme views \ufb02ourish. Detection and mitigation of fake news is one of the fundamental problems of our times and has attracted widespread attention. While fact checking websites such as snopes, politifact and major companies such as Google, Facebook, and Twitter have taken preliminary steps towards addressing fake news, much more remains to be done. As an interdisciplinary topic, various facets of fake news have been studied by communities as diverse as machine learning, databases, journalism, political science and many more. The objective of this tutorial is two-fold. First, we wish to familiarize the database community with the e\ufb00orts by other communities on combating fake news. We provide a panoramic view of the state-of-the-art of research on various aspects including detection, propagation, mitigation, and intervention of fake news. Next, we provide a concise and intuitive summary of prior research by the database community and discuss how it could be used to counteract fake news. The tutorial covers research from areas such as data integration, truth discovery and fusion, probabilistic databases, knowledge graphs and crowdsourcing from the lens of fake news. E\ufb00ective tools for addressing fake news could only be built by leveraging the synergistic relationship between database and other research communities. We hope that our tutorial provides an impetus towards such synthesis of ideas and the creation of new ones.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2053261077",
                    "name": "Michael Simpson"
                },
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                }
            ]
        },
        {
            "paperId": "6893a0f39d52c80cddb00fcd0f630a4e5585c5df",
            "title": "Community Search over Big Graphs",
            "abstract": "Abstract Communities serve as basic structural building blocks for understanding the organization of many real-world networks, including social, biological, collaboration, and communication network...",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39261035",
                    "name": "Xin Huang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "9097705",
                    "name": "Jianliang Xu"
                }
            ]
        },
        {
            "paperId": "97932ab77940df76c8b81fca51be061302195779",
            "title": "Efficient Algorithms for Densest Subgraph Discovery",
            "abstract": "\n Densest subgraph discovery\n (DSD) is a fundamental problem in graph mining. It has been studied for decades, and is widely used in various areas, including network science, biological analysis, and graph databases. Given a graph\n G\n , DSD aims to find a subgraph\n D\n of\n G\n with the highest density (e.g., the number of edges over the number of vertices in\n D\n ). Because DSD is difficult to solve, we propose a new solution paradigm in this paper. Our main observation is that the densest subgraph can be accurately found through a\n k\n -core (a kind of dense subgraph of\n G\n ), with theoretical guarantees. Based on this intuition, we develop efficient exact and approximation solutions for DSD. Moreover, our solutions are able to find the densest subgraphs for a wide range of graph density definitions, including clique-based- and general pattern-based density. We have performed extensive experimental evaluation on both real and synthetic datasets. Our results show that our algorithms are up to four orders of magnitude faster than existing approaches.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3164974",
                    "name": "Yixiang Fang"
                },
                {
                    "authorId": "2114076598",
                    "name": "Kaiqiang Yu"
                },
                {
                    "authorId": "1966497",
                    "name": "Reynold Cheng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2873542",
                    "name": "Xuemin Lin"
                }
            ]
        },
        {
            "paperId": "a14b27262f107f5ea4b642d7bf092015d5243ee6",
            "title": "Efficient Approximation Algorithms for Adaptive Seed Minimization",
            "abstract": "As a dual problem of influence maximization, the seed minimization problem asks for the minimum number of seed nodes to influence a required number \u03b7 of users in a given social network G. Existing algorithms for seed minimization mostly consider the non-adaptive setting, where all seed nodes are selected in one batch without observing how they may influence other users. In this paper, we study seed minimization in the adaptive setting, where the seed nodes are selected in several batches, such that the choice of a batch may exploit information about the actual influence of the previous batches. We propose a novel algorithm, ASTI, which addresses the adaptive seed minimization problem in $O\\Big(\\frac\u03b7 \\cdot (m+n) \\varepsilon^2 \u0142n n \\Big)$ expected time and offers an approximation guarantee of $\\frac(\u0142n \u03b7+1)^2 (1 - (1-1/b)^b) (1-1/e)(1-\\varepsilon) $ in expectation, where \u03b7 is the targeted number of influenced nodes, b is size of each seed node batch, and $\\varepsilon \\in (0, 1)$ is a user-specified parameter. To the best of our knowledge, ASTI is the first algorithm that provides such an approximation guarantee without incurring prohibitive computation overhead. With extensive experiments on a variety of datasets, we demonstrate the effectiveness and efficiency of ASTI over competing methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "144066821",
                    "name": "Jing Tang"
                },
                {
                    "authorId": "2112440523",
                    "name": "Keke Huang"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2158001218",
                    "name": "Xueyan Tang"
                },
                {
                    "authorId": "1735962",
                    "name": "Aixin Sun"
                },
                {
                    "authorId": "2053311839",
                    "name": "A. Lim"
                }
            ]
        },
        {
            "paperId": "58922c9d094bbf0c5344b3506650ec7c6178a804",
            "title": "Maximizing Welfare in Social Networks under A Utility Driven Influence Diffusion model",
            "abstract": "Motivated by applications such as viral marketing, the problem of influence maximization (IM) has been extensively studied in the literature. The goal is to select a small number of users to adopt an item such that it results in a large cascade of adoptions by others. Existing works have three key limitations. (1) They do not account for economic considerations of a user in buying/adopting items. (2) Most studies on multiple items focus on competition, with complementary items receiving limited attention. (3) For the network owner, maximizing social welfare is important to ensure customer loyalty, which is not addressed in prior work in the IM literature. In this paper, we address all three limitations and propose a novel model called UIC that combines utility-driven item adoption with influence propagation over networks. Focusing on the mutually complementary setting, we formulate the problem of social welfare maximization in this novel setting. We show that while the objective function is neither submodular nor supermodular, surprisingly a simple greedy allocation algorithm achieves a factor of (1-1/e-\u03b5) of the optimum expected social welfare. We develop bundleGRD, a scalable version of this approximation algorithm, and demonstrate, with comprehensive experiments on real and synthetic datasets, that it significantly outperforms all baselines.",
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "authors": [
                {
                    "authorId": "3274987",
                    "name": "Prithu Banerjee"
                },
                {
                    "authorId": "47151943",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "67dc19da49fb853308733413407aacf06333bb44",
            "title": "Influence Maximization in Online Social Networks",
            "abstract": "Starting with the earliest studies showing that the spread of new trends, information, and innovations is closely related to the social influence exerted on people by their social networks, the research on social influence theory took off, providing remarkable evidence on social influence induced viral phenomena. Fueled by the extreme popularity of online social networks and social media, computational social influence has emerged as a subfield of data mining whose goal is to analyze and optimize social influence using computational frameworks such as algorithm design and theoretical modeling. One of the fundamental problems in this field is the problem of influence maximization, primarily motivated by the application of viral marketing. The objective is to identify a small set of users in a social network who, when convinced to adopt a product, shall influence others in the network in a manner that leads to a large number of adoptions. In this tutorial, we extensively survey the research on social influence propagation and maximization, with a focus on the recent algorithmic and theoretical advances. To this end, we provide detailed reviews of the latest research effort devoted to (i) improving the efficiency and scalability of the influence maximization algorithms; (ii) context-aware modeling of the influence maximization problem to better capture real-world marketing scenarios; (iii) modeling and learning of real-world social influence; (iv) bridging the gap between social advertising and viral marketing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3222408",
                    "name": "\u00c7igdem Aslay"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "25880134",
                    "name": "Weixu Lu"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                }
            ]
        },
        {
            "paperId": "74f61fe0c553538148ba1949efccac54db8fb76f",
            "title": "Cohort Representation and Exploration",
            "abstract": "The abundant availability of health-care data calls for effective analysis methods which help medical experts gain a better understanding of their data. While the focus has been largely on prediction, \"representation\" and \"exploration\" of health-care data have received little attention. In this paper, we introduce CORE, a framework for representing and exploring patient cohorts. Obtaining a readable and succinct representation of health data of a cohort is challenging because cohorts often consist of hundreds of patients whose medical actions are of various types and occur at different points in time. We extend the Needleman-Wunsch algorithm for sequence matching to handle temporal sequences, and propose \"trajectory families\", a customized index to efficiently compare and aggregate patient trajectories into a cohort representation. We define cohort exploration as finding similar cohorts to a given cohort. This problem is challenging because the potential number of similar cohorts is huge. We propose a two-staged approach based on limiting the search space to \"contrast cohorts\" and then computing their similarity to the given cohort. To speed up cohort similarity computation, we use \"event sets\" in the same spirit as the double dictionary encoding proposed for keyword search. We run qualitative and quantitative experiments on real data to explore the efficiency and usefulness of CORE. We show that CORE representations reduce time-to-insight from hours to seconds and help medical experts find insights better than state-of-the-art Visual Analytics tools.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "d65df5c3f4b18b51eabde3553bf0ba8533e18042",
            "title": "FastQRE: Fast Query Reverse Engineering",
            "abstract": "We study the problem of Query Reverse Engineering (QRE), where given a database and an output table, the task is to find a simple project-join SQL query that generates that table when applied on the database. This problem is known for its efficiency challenge due to mainly two reasons. First, the problem has a very large search space and its various variants are known to be NP-hard. Second, executing even a single candidate SQL query can be very computationally expensive. In this work we propose a novel approach for solving the QRE problem efficiently. Our solution outperforms the existing state of the art by 2-3 orders of magnitude for complex queries, resolving those queries in seconds rather than days, thus making our approach more practical in real-life settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "0478ea6bae603a089be1488d6e0d34b234427ac3",
            "title": "Model-Independent Online Learning for Influence Maximization",
            "abstract": "We consider influence maximization (IM) in social networks, which is the problem of maximizing the number of users that become aware of a product by selecting a set of \"seed\" users to expose the product to. While prior work assumes a known model of information diffusion, we propose a novel parametrization that not only makes our framework agnostic to the underlying diffusion model, but also statistically efficient to learn from data. We give a corresponding monotone, submodular surrogate function, and show that it is a good approximation to the original IM objective. We also consider the case of a new marketer looking to exploit an existing social network, while simultaneously learning the factors governing information propagation. For this, we propose a pairwise-influence semi-bandit feedback model and develop a LinUCB-based bandit algorithm. Our model-independent analysis shows that our regret bound has a better (as compared to previous work) dependence on the size of the network. Experimental evaluation suggests that our framework is robust to the underlying diffusion model and can efficiently learn a near-optimal solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1711940",
                    "name": "Sharan Vaswani"
                },
                {
                    "authorId": "1681967",
                    "name": "B. Kveton"
                },
                {
                    "authorId": "39761651",
                    "name": "Zheng Wen"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145610994",
                    "name": "Mark W. Schmidt"
                }
            ]
        },
        {
            "paperId": "2ca9d57b883507f64244d47632c6efe306986dc3",
            "title": "Community Search over Big Graphs: Models, Algorithms, and Opportunities",
            "abstract": "Communities serve as basic structures for understanding the organization of many real-world networks, such as social, biological, collaboration, and communication networks. Recently, community search over large graphs has attracted significantly increasing attention, from simple and static graphs to evolving, attributed, location-based graphs. Different from the well-studied problem of community detection that finds all communities in an entire network, community search is to find the cohesive communities w.r.t. the query nodes. In this tutorial, we survey the state-of-the-art of community search on various kinds of networks across different application areas such as densely-connected community search, attributed community search, social circle discovery, and querying geosocial groups. We first highlight the challenges posed by the community search problems. We continue the presentation of their principles, methodologies, algorithms, and applications, and give a comprehensive comparison of the state-of-the-art techniques. This tutorial finally concludes by offering future directions for research in this important and growing area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39261035",
                    "name": "Xin Huang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "9097705",
                    "name": "Jianliang Xu"
                }
            ]
        },
        {
            "paperId": "6d07996dafc1cb3f20317b17398a058fd62a2683",
            "title": "Revisiting the Stop-and-Stare Algorithms for Influence Maximization",
            "abstract": "Influence maximization is a combinatorial optimization problem that finds important applications in viral marketing, feed recommendation, etc. Recent research has led to a number of scalable approximation algorithms for influence maximization, such as TIM+ and IMM, and more recently, SSA and D-SSA. The goal of this paper is to conduct a rigorous theoretical and experimental analysis of SSA and D-SSA and compare them against the preceding algorithms. In doing so, we uncover inaccuracies in previously reported technical results on the accuracy and efficiency of SSA and D-SSA, which we set right. We also attempt to reproduce the original experiments on SSA and D-SSA, based on which we provide interesting empirical insights. Our evaluation confirms some results reported from the original experiments, but it also reveals anomalies in some other results and sheds light on the behavior of SSA and D-SSA in some important settings not considered previously. We also report on the performance of SSA-Fix, our modification to SSA in order to restore the approximation guarantee that was claimed for but not enjoyed by SSA. Overall, our study suggests that there exist opportunities for further scaling up influence maximization with approximation guarantees.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112440523",
                    "name": "Keke Huang"
                },
                {
                    "authorId": "39996718",
                    "name": "Sibo Wang"
                },
                {
                    "authorId": "2698019",
                    "name": "Glenn S. Bevilacqua"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "78880dc70d7b055bb8babc735c927b2041b35451",
            "title": "Exploring Rated Datasets with Rating Maps",
            "abstract": "Online rated datasets have become a source for large-scale population studies for analysts and a means for end-users to achieve routine tasks such as finding a book club. Existing systems however only provide limited insights into the opinions of different segments of the rater population. In this paper, we develop a framework for finding and exploring population segments and their opinions. We propose rating maps, a collection of (population segment, rating distribution) pairs, where a segment, e.g., {18-29 year old males in CA} has a rating distribution in the form of a histogram that aggregates its ratings for a set of items (e.g., movies starring Russel Crowe). We formalize the problem of building rating maps dynamically given desired input distributions. Our problem raises two challenges: (i) the choice of an appropriate measure for comparing rating distributions, and (ii) the design of efficient algorithms to find segments. We show that the Earth Mover's Distance (EMD) is well-adapted to comparing rating distributions and prove that finding segments whose rating distribution is close to input ones is NP-complete. We propose an efficient algorithm for building Partition Decision Trees and heuristics for combining the resulting partitions to further improve their quality. Our experiments on real and synthetic datasets validate the utility of rating maps for both analysts and end-users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "3036804",
                    "name": "Sofia Kleisarchaki"
                },
                {
                    "authorId": "9999137",
                    "name": "Naresh Kumar Kolloju"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "7ba050aa995c9293a1e224a46f3463088d33bf68",
            "title": "Refutations on \"Debunking the Myths of Influence Maximization: An In-Depth Benchmarking Study\"",
            "abstract": "In a recent SIGMOD paper titled \"Debunking the Myths of Influence Maximization: An In-Depth Benchmarking Study\", Arora et al. [1] undertake a performance benchmarking study of several well-known algorithms for influence maximization. In the process, they contradict several published results, and claim to have unearthed and debunked several \"myths\" that existed around the research of influence maximization. It is the goal of this article to examine their claims objectively and critically, and refute the erroneous ones. Our investigation discovers that first, the overall experimental methodology in Arora et al. [1] is flawed and leads to scientifically incorrect conclusions. Second, the paper [1] is riddled with issues specific to a variety of influence maximization algorithms, including buggy experiments, and draws many misleading conclusions regarding those algorithms. Importantly, they fail to appreciate the trade-off between running time and solution quality, and did not incorporate it correctly in their experimental methodology. In this article, we systematically point out the issues present in [1] and refute 11 of their misclaims.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                },
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "2112440523",
                    "name": "Keke Huang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "9d774a0f976407296a1585750010722867baf1fe",
            "title": "Combating the Cold Start User Problem in Model Based Collaborative Filtering",
            "abstract": "For tackling the well known cold-start user problem in model-based recommender systems, one approach is to recommend a few items to a cold-start user and use the feedback to learn a profile. The learned profile can then be used to make good recommendations to the cold user. In the absence of a good initial profile, the recommendations are like random probes, but if not chosen judiciously, both bad recommendations and too many recommendations may turn off a user. We formalize the cold-start user problem by asking what are the $b$ best items we should recommend to a cold-start user, in order to learn her profile most accurately, where $b$, a given budget, is typically a small number. We formalize the problem as an optimization problem and present multiple non-trivial results, including NP-hardness as well as hardness of approximation. We furthermore show that the objective function, i.e., the least square error of the learned profile w.r.t. the true user profile, is neither submodular nor supermodular, suggesting efficient approximations are unlikely to exist. Finally, we discuss several scalable heuristic approaches for identifying the $b$ best items to recommend to the user and experimentally evaluate their performance on 4 real datasets. Our experiments show that our proposed accelerated algorithms significantly outperform the prior art in runnning time, while achieving similar error in the learned user profile as well as in the rating predictions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9963020",
                    "name": "Sampoorna Biswas"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1702973",
                    "name": "Senjuti Basu Roy"
                }
            ]
        },
        {
            "paperId": "a3a3102b6643496a0e96b4bcf8eefa0c7e35334b",
            "title": "Diffusion Independent Semi-Bandit Influence Maximization",
            "abstract": "We consider \\emph{influence maximization} (IM) in social networks, which is the problem of maximizing the number of users that become aware of a product by selecting a set of \"seed\" users to expose the product to. While prior work assumes a known model of information diffusion, we propose a parametrization in terms of pairwise reachability which makes our framework agnostic to the underlying diffusion model. We give a corresponding monotone, submodular surrogate function, and show that it is a good approximation to the original IM objective. We also consider the case of a new marketer looking to exploit an existing social network, while simultaneously learning the factors governing information propagation. For this, we propose a pairwise-influence semi-bandit feedback model and develop a LinUCB-based bandit algorithm. Our model-independent regret analysis shows that our bound on the cumulative regret has a better (as compared to previous work) dependence on the size of the network. By using the graph Laplacian eigenbasis to construct features, we describe a practical LinUCB implementation. Experimental evaluation suggests that our framework is robust to the underlying diffusion model and can efficiently learn a near-optimal solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1711940",
                    "name": "Sharan Vaswani"
                },
                {
                    "authorId": "1681967",
                    "name": "B. Kveton"
                },
                {
                    "authorId": "39761651",
                    "name": "Zheng Wen"
                },
                {
                    "authorId": "1678622",
                    "name": "M. Ghavamzadeh"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145610994",
                    "name": "Mark W. Schmidt"
                }
            ]
        },
        {
            "paperId": "bae558eacbe4e48eb77023c617802a9e08572680",
            "title": "Horde of Bandits using Gaussian Markov Random Fields",
            "abstract": "The gang of bandits (GOB) model \\cite{cesa2013gang} is a recent contextual bandits framework that shares information between a set of bandit problems, related by a known (possibly noisy) graph. This model is useful in problems like recommender systems where the large number of users makes it vital to transfer information between users. Despite its effectiveness, the existing GOB model can only be applied to small problems due to its quadratic time-dependence on the number of nodes. Existing solutions to combat the scalability issue require an often-unrealistic clustering assumption. By exploiting a connection to Gaussian Markov random fields (GMRFs), we show that the GOB model can be made to scale to much larger graphs without additional assumptions. In addition, we propose a Thompson sampling algorithm which uses the recent GMRF sampling-by-perturbation technique, allowing it to scale to even larger problems (leading to a \"horde\" of bandits). We give regret bounds and experimental results for GOB with Thompson sampling and epoch-greedy algorithms, indicating that these methods are as good as or significantly better than ignoring the graph or adopting a clustering-based approach. Finally, when an existing graph is not available, we propose a heuristic for learning it on the fly and show promising results.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1711940",
                    "name": "Sharan Vaswani"
                },
                {
                    "authorId": "145610994",
                    "name": "Mark W. Schmidt"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "cad5f1f62f277b54b158c5bc60407ac6b3b723f2",
            "title": "Split Regularized Regression",
            "abstract": "Abstract We propose an approach for fitting linear regression models that splits the set of covariates into groups. The optimal split of the variables into groups and the regularized estimation of the regression coefficients are performed by minimizing an objective function that encourages sparsity within each group and diversity among them. The estimated coefficients are then pooled together to form the final fit. Our procedure works on top of a given penalized linear regression estimator (e.g., Lasso, elastic net) by fitting it to possibly overlapping groups of features, encouraging diversity among these groups to reduce the correlation of the corresponding predictions. For the case of two groups, elastic net penalty and orthogonal predictors, we give a closed form solution for the regression coefficients in each group. We establish the consistency of our method with the number of predictors possibly increasing with the sample size. An extensive simulation study and real-data applications show that in general the proposed method improves the prediction accuracy of the base estimator used in the procedure. Possible extensions to GLMs and other models are discussed. The supplemental material for this article, available online, contains the proofs of our theoretical results and the full results of our simulation study.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "89188121",
                    "name": "A. Christidis"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2024002",
                    "name": "Ezequiel Smucler"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "194265af6c574bcff4fa5eee167ba511cdb0a650",
            "title": "Attribute-Driven Community Search",
            "abstract": "\n Recently, community search over graphs has gained significant interest. In applications such as analysis of protein-protein interaction (PPI) networks, citation graphs, and collaboration networks, nodes tend to have attributes. Unfortunately, most previous community search algorithms ignore attributes and result in communities with poor cohesion w.r.t. their node attributes. In this paper, we study the problem of attribute-driven community search, that is, given an undirected graph\n G\n where nodes are associated with attributes, and an input query\n Q\n consisting of nodes\n \n V\n q\n \n and attributes\n \n W\n q\n \n , find the communities containing\n \n V\n q\n \n , in which most community members are densely inter-connected and have similar attributes.\n \n \n We formulate this problem as finding attributed truss communities (ATC), i.e., finding connected and close k-truss subgraphs containing\n \n V\n q\n \n , with the largest attribute relevance score. We design a framework of desirable properties that good score function should satisfy. We show that the problem is NP-hard. However, we develop an efficient greedy algorithmic framework to iteratively remove nodes with the least popular attributes, and shrink the graph into an ATC. In addition, we also build an elegant index to maintain\n k\n -truss structure and attribute information, and propose efficient query processing algorithms. Extensive experiments on large real-world networks with ground-truth communities show that our algorithms significantly outperform the state of the art and demonstrates their efficiency and effectiveness.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39261035",
                    "name": "Xin Huang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "2d80790af60c33de9cc89da980aa47fce6c78bb4",
            "title": "Truss Decomposition of Probabilistic Graphs: Semantics and Algorithms",
            "abstract": "A key operation in network analysis is the discovery of cohesive subgraphs. The notion of $k$-truss has gained considerable popularity in this regard, based on its rich structure and efficient computability. However, many complex networks such as social, biological and communication networks feature uncertainty, best modeled using probabilities. Unfortunately the problem of discovering k-trusses in probabilistic graphs has received little attention to date. In this paper, given a probabilistic graph G, number k and parameter \u03b3 --(0,1], we define a (k,\u03b3)-truss as a maximal connected subgraph H \u2286 G, in which for each edge, the probability that it is contained in at least (k-2) triangles is at least \u03b3. We develop an efficient dynamic programming algorithm for decomposing a probabilistic graph into such maximal (k,\u03b3)-trusses. The above definition of a (k,\u03b3)-truss is local in that the \"witness\" graphs that has the (k-2) triangles containing an edge in H may be quite different for distinct edges. Hence, we also propose: a global (k,\u03b3)-truss, which in addition to being a local (k,\u03b3)-truss, has to satisfy the condition that the probability that H contains a k-truss is at least \u03b3. We show that unlike local (k,\u03b3)-trusses, the global (k,\u03b3)-truss decomposition on a probabilistic graph is intractable. We propose a novel sampling technique which enables approximate discovery of global (k,\u03b3)-trusses with high probability. Our extensive experiments on real datasets demonstrate the efficacy of our proposed approach and the usefulness of local and global (k,\u03b3)-truss.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39261035",
                    "name": "Xin Huang"
                },
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "2eef12da43f6fe61dff7346e6d250639ee675f42",
            "title": "Nepal: a path query language for communication networks",
            "abstract": "Communication networks are typically large, dynamic and extremely complicated. To deploy, maintain, and trouble-shoot such networks, it is essential to understand how network elements---such as servers, switches, virtual machines, and virtual network functions---are connected to one another, and be able to discover communication paths between them. It is also essential to understand how connections change over time, and be able to pose time-travel queries to retrieve information about past network states. This problem is becoming more acute with the advent of software defined networks, where network functions are virtualized and managed in a cloud infrastructure. We represent a communication network inventory as a graph where the nodes are network entities and edges represent relationships between them, e.g. hosted-on, communicates-with, etc. Querying such a graph, e.g. for troubleshooting, using existing graph query languages is too cumbersome for network analysts. Thus, in this paper we present Nepal---a network path query language, which is designed to effectively retrieve desired paths from a network graph. The main novelty of Nepal is to consider paths as first-class citizens of the language, which achieves closure under composition while maintaining simplicity. We demonstrate the capabilities of Nepal by examples and discuss query evaluation. We illustrate how path queries can simplify the extraction of information from a dynamic inventory of a multi-layer network and can be used for troubleshooting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145633895",
                    "name": "T. Johnson"
                },
                {
                    "authorId": "1742591",
                    "name": "Y. Kanza"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1745049",
                    "name": "Vladislav Shkapenyuk"
                }
            ]
        },
        {
            "paperId": "498923c3484cb7dade20673d8362d24b0894a2c3",
            "title": "Attribute Truss Community Search",
            "abstract": "Recently, community search over graphs has attracted significant attention and many algorithms have been developed for finding dense subgraphs from large graphs that contain given query nodes. In applications such as analysis of protein protein interaction (PPI) networks, citation graphs, and collaboration networks, nodes tend to have attributes. Unfortunately, previously developed community search algorithms ignore these attributes and result in communities with poor cohesion w.r.t. their node attributes. In this paper, we study the problem of attribute-driven community search, that is, given an undirected graph $G$ where nodes are associated with attributes, and an input query $Q$ consisting of nodes $V_q$ and attributes $W_q$, find the communities containing $V_q$, in which most community members are densely inter-connected and have similar attributes. \nWe formulate our problem of finding attributed truss communities (ATC), as finding all connected and close k-truss subgraphs containing $V_q$, that are locally maximal and have the largest attribute relevance score among such subgraphs. We design a novel attribute relevance score function and establish its desirable properties. The problem is shown to be NP-hard. However, we develop an efficient greedy algorithmic framework, which finds a maximal $k$-truss containing $V_q$, and then iteratively removes the nodes with the least popular attributes and shrinks the graph so as to satisfy community constraints. We also build an elegant index to maintain the known $k$-truss structure and attribute information, and propose efficient query processing algorithms. Extensive experiments on large real-world networks with ground-truth communities shows the efficiency and effectiveness of our proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39261035",
                    "name": "Xin Huang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "8bb47e8900deaf371f40b3dec925b5ab35224ead",
            "title": "Proceedings of the Third International Workshop on Exploratory Search in Databases and the Web",
            "abstract": "The purpose of the ExploreDB workshop is to bring together researchers and practitioners that approach data exploration from different angles, ranging from data management, information retrieval to data visualization and human computer interaction, in order to study the emerging needs and objectives for data exploration, as well as the challenges and problems that need to be tackled.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702973",
                    "name": "Senjuti Basu Roy"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                },
                {
                    "authorId": "1680709",
                    "name": "Georgia Koutrika"
                },
                {
                    "authorId": "1722086",
                    "name": "Mirek Riedewald"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "8c04a9ad9a3c357cd1fbd1b63cbe801d9c8fc687",
            "title": "Report on the First International Workshop on Exploratory Search in Databases and the Web (ExploreDB 2014)",
            "abstract": "The second ExploreDB 2015 workshop intends to bring together researchers and practitioners from different fields, ranging from data management and information retrieval to data visualization and human computer interaction. The workshop program consisted of two keynote talks and six peer-reviewed research papers. The first keynote talk titled 'Explore-By-Example: A New Database Service for Interactive Data Exploration' was given by Prof. Yanlei Diao from the University of Massachusetts at Amherst. Prof. Diao pointed out that while computing power, memory size, and the ability to collect data are growing exponentially, human ability to understand data remains practically flat. In the second keynote, titled 'Principled Optimization Frameworks for Query Reformulation of Database Queries', Prof. Gautam Das from the University of Texas at Arlington focused on solutions for the many-answers and the empty-answers problems. He proposed to address both problems through ranked retrieval. Xiaoyu Ge, Panos Chrysanthis and Alexandros Labrinidis ('Preferential Diversity') explored how to achieve personalization through preferences on result diversity. Diversity was also the focus in 'Diversifying with Few Regrets, But too Few to Mention' by Zaeem Hussain, Hina Khan and Mohamed Sharaf.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1680709",
                    "name": "Georgia Koutrika"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1722086",
                    "name": "Mirek Riedewald"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                }
            ]
        },
        {
            "paperId": "ac05093b651cd21241ac03a399727c6829a2050f",
            "title": "Revenue Maximization in Incentivized Social Advertising",
            "abstract": "Incentivized social advertising, an emerging marketing model, provides monetization opportunities not only to the owners of the social networking platforms but also to their influential users by offering a \"cut\" on the advertising revenue. We consider a social network (the host) that sells ad-engagements to advertisers by inserting their ads, in the form of promoted posts, into the feeds of carefully selected \"initial endorsers\" or seed users: these users receive monetary incentives in exchange for their endorsements. The endorsements help propagate the ads to the feeds of their followers. Whenever any user engages with an ad, the host is paid some fixed amount by the advertiser, and the ad further propagates to the feed of her followers, potentially recursively. In this context, the problem for the host is is to allocate ads to influential users, taking into account the propensity of ads for viral propagation, and carefully apportioning the monetary budget of each of the advertisers between incentives to influential users and ad-engagement costs, with the rational goal of maximizing its own revenue.We show that, taking all important factors into account, the problem of revenue maximization in incentivized social advertising corresponds to the problem of monotone submodular function maximization, subject to a partition matroid constraint on the ads-to-seeds allocation, and submodular knapsack constraints on the advertisers' budgets. We show that this problem is NP-hard and devise two greedy algorithms with provable approximation guarantees, which differ in their sensitivity to seed user incentive costs.Our approximation algorithms require repeatedly estimating the expected marginal gain in revenue as well as in advertiser payment. By exploiting a connection to the recent advances made in scalable estimation of expected influence spread, we devise efficient and scalable versions of our two greedy algorithms. An extensive experimental assessment confirms the high quality of our proposal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3222408",
                    "name": "\u00c7igdem Aslay"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                }
            ]
        },
        {
            "paperId": "bb3afc3f16b8fd8bd7a558fe3312faa7a2e8c103",
            "title": "Report on the Third International Workshop on Exploratory Search in Databases and the Web (ExploreDB 2016)",
            "abstract": "The traditional way of interaction between a user and a database system is through queries, for which the correctness and completeness of their answers are key challenges. Structured query languages, such as SQL, XQuery, and SPARQL, allow users to submit queries that may precisely identify their information needs, but often require users to be familiar with the structure of data, the content of the database, and also have a clear understanding of their needs. As databases get larger and accessible to a more diverse audience, new forms of data exploration and interaction become increasingly more attractive to aid users navigate through the information space and overcome the challenges of information overload [6, 5]. The Web represents the largest and most complex repository of content. Users seek information through two predominant modes: by browsing or by searching. In the first mode, the interaction between the user and the data repository is driven directly by the user\u2019s needs interpretation. In the latter mode, a search engine typically mediates the user-data interactions and the process starts with the user entering query-terms that act as surrogates for the user information goals. Commonly, independently from data models and query languages, the query results are presented to the user as a ranked list. Clearly, there is a need to develop novel paradigms for exploratory user-data interactions that emphasize user context [13] and interactivity with the goal of facilitating exploration, retrieval, and assimilation of information. A huge number of applications need an exploratory form of querying. Ranked retrieval techniques is a first step in this direction [1, 3]. Recently, several new aspects for exploratory search, such as preferences [12], diversity [14], novelty [9], surprise [10] and serendipity [4], are gaining increasing importance. From a different perspective, recommender systems tend to anticipate user needs by suggesting the most appropriate to the users information [11], while a new line of research in the area of exploratory search is fueled by the growth of online social interactions within social networks and Web communities [2]. Overall, the query-answering task needs to be further enhanced to capture the intent that the user may have in mind during querying. Exploratory search techniques are of great assistance that facilitates and guides users to focus on the relevant aspects of their search results. To sum up, the field of data exploration is diverse in terms of research directions and potential user base. Hence, the ExploreDB workshop intends to bring together researchers and practitioners from different fields, ranging from data management and information retrieval to data visualization and human computer interaction. Its goal is to study the emerging needs and objectives for data exploration, as well as the challenges and problems that need to be tackled, and to nourish interdisciplinary synergies. We summarize the outcomes of the third workshop instance held in conjunction with ACM SIGMOD 2016 in San Francisco, USA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702973",
                    "name": "Senjuti Basu Roy"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                },
                {
                    "authorId": "1680709",
                    "name": "Georgia Koutrika"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1722086",
                    "name": "Mirek Riedewald"
                }
            ]
        },
        {
            "paperId": "bfc5406e1bd3c3b3e87adb2e288ea2173a730ce0",
            "title": "Report on the Second International Workshop on Exploratory Search in Databases and the Web (ExploreDB 2015)",
            "abstract": "Georgia Koutrika HP Labs, Palo Alto koutrika@hp.com Laks V.S. Lakshmanan Department of Computer Science, University of British Columbia laks@cs.ubc.ca Mirek Riedewald College of Computer and Information Science, Northeastern University, Boston mirek@ccs.neu.edu Mohamed A. Sharaf School of ITEE, University of Queensland, Australia m.sharaf@uq.edu.au Kostas Stefanidis ICS-FORTH, Heraklion kstef@ics.forth.gr",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1680709",
                    "name": "Georgia Koutrika"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1722086",
                    "name": "Mirek Riedewald"
                },
                {
                    "authorId": "144400852",
                    "name": "M. Sharaf"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                }
            ]
        },
        {
            "paperId": "e297cb9fdd7d4c4fb914ad6c12c361d01688652a",
            "title": "Adaptive Influence Maximization in Social Networks: Why Commit when You can Adapt?",
            "abstract": "Most previous work on influence maximization in social networks is limited to the non-adaptive setting in which the marketer is supposed to select all of the seed users, to give free samples or discounts to, up front. A disadvantage of this setting is that the marketer is forced to select all the seeds based solely on a diffusion model. If some of the selected seeds do not perform well, there is no opportunity to course-correct. A more practical setting is the adaptive setting in which the marketer initially selects a batch of users and observes how well seeding those users leads to a diffusion of product adoptions. Based on this market feedback, she formulates a policy for choosing the remaining seeds. In this paper, we study adaptive offline strategies for two problems: (a) MAXSPREAD -- given a budget on number of seeds and a time horizon, maximize the spread of influence and (b) MINTSS -- given a time horizon and an expected number of target users to be influenced, minimize the number of seeds that will be required. In particular, we present theoretical bounds and empirical results for an adaptive strategy and quantify its practical benefit over the non-adaptive strategy. We evaluate adaptive and non-adaptive policies on three real data sets. We conclude that while benefit of going adaptive for the MAXSPREAD problem is modest, adaptive policies lead to significant savings for the MINTSS problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1711940",
                    "name": "Sharan Vaswani"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "e772263cc76198e2c9f5105662ee07117b26586e",
            "title": "Viral marketing 2.0",
            "abstract": "Over the last decade, there has been considerable excitement and research on the study and exploitation of the spread of information and influence over networks. Tremendous advances have been made on the prototypical problem of selecting a small number of seed users to activate over a social network such that the number of activated nodes in an expected sense is maximized, under several standard information diffusion models. Scalable heuristics, but more notably scalable approximation algorithms, have been developed in the recent years. Unfortunately, the state of the art has several shortcomings. Firstly, most of the research has focused on a simplistic setting where one marketing campaign is active at a time. While there has been some work on modeling and optimizing for competing diffusions, the key role played by the network owner in a campaign has been overlooked. Secondly, the relationship and contract needed between the network owner and the advertisers is not captured. Thirdly, in real life, relationships between multiple campaigns may be more complex than just pure competition. Finally, most of the studies assume that the seeds must be chosen all at once before the campaign starts with no opportunity to observe the performance of seeds chosen earlier and course-correct as needed. We make a call to arms for opening up the framework of viral marketing to allow for more expressive business models and seed selection strategies, and present some recent research from our group that addresses the modeling and computational challenges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "076bca45148240b67340f0fe7f03a2f6f450da18",
            "title": "K-Medoids Computation Model Framework for Security in Distributed Architecture Networks",
            "abstract": "In this paper, a method was proposed to maintain the networks with low cost for more processing of data. It contains simple framework to maintain the data in refine method and for secure data transfer. And also more data maintenance with clustering capability with secure mode.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2286125432",
                    "name": "Lalitha Ariyapalli"
                },
                {
                    "authorId": "72772113",
                    "name": "Rajendra Kumar Ganiya"
                },
                {
                    "authorId": "2286304981",
                    "name": "P. S. Kumar"
                },
                {
                    "authorId": "2286143842",
                    "name": "Pui K. Fong"
                },
                {
                    "authorId": "1403846894",
                    "name": "J. Weber-Jahnke"
                },
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2296962",
                    "name": "A. Monreale"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                },
                {
                    "authorId": "1687574",
                    "name": "Tamir Tassa"
                },
                {
                    "authorId": "2115017071",
                    "name": "Dror J. Cohen"
                },
                {
                    "authorId": "2258960325",
                    "name": "C. Clifton"
                },
                {
                    "authorId": "2243292428",
                    "name": "Murat Kantarcioglu"
                },
                {
                    "authorId": "2286106840",
                    "name": "Xiaodong Lin"
                },
                {
                    "authorId": "2289171459",
                    "name": "Michael Y. Zhu"
                },
                {
                    "authorId": "2286111238",
                    "name": "C. R. Giannella"
                },
                {
                    "authorId": "2286460292",
                    "name": "Stanley R. M. Oliveira"
                },
                {
                    "authorId": "2286143815",
                    "name": "Osmar R. Za \u0308\u0131ane"
                }
            ]
        },
        {
            "paperId": "18b4e5d8e497bcb1049547300dd2e1053dd3e5e1",
            "title": "Approximate Closest Community Search in Networks",
            "abstract": "Recently, there has been significant interest in the study of the community search problem in social and information networks: given one or more query nodes, find densely connected communities containing the query nodes. However, most existing studies do not address the \"free rider\" issue, that is, nodes far away from query nodes and irrelevant to them are included in the detected community. Some state-of-the-art models have attempted to address this issue, but not only are their formulated problems NP-hard, they do not admit any approximations without restrictive assumptions, which may not always hold in practice. \n \nIn this paper, given an undirected graph G and a set of query nodes Q, we study community search using the k-truss based community model. We formulate our problem of finding a closest truss community (CTC), as finding a connected k-truss subgraph with the largest k that contains Q, and has the minimum diameter among such subgraphs. We prove this problem is NP-hard. Furthermore, it is NP-hard to approximate the problem within a factor (2-e), for any e > 0. However, we develop a greedy algorithmic framework, which first finds a CTC containing Q, and then iteratively removes the furthest nodes from Q, from the graph. The method achieves 2-approximation to the optimal solution. To further improve the efficiency, we make use of a compact truss index and develop efficient algorithms for k-truss identification and maintenance as nodes get eliminated. In addition, using bulk deletion optimization and local exploration strategies, we propose two more efficient algorithms. One of them trades some approximation quality for efficiency while the other is a very efficient heuristic. Extensive experiments on 6 real-world networks show the effectiveness and efficiency of our community model and search algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39261035",
                    "name": "Xin Huang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                },
                {
                    "authorId": "144488755",
                    "name": "Hong Cheng"
                }
            ]
        },
        {
            "paperId": "59d012a02cb12496823bbb6e7501901e19445de4",
            "title": "From Competition to Complementarity: Comparative Influence Diffusion and Maximization",
            "abstract": "Influence maximization is a well-studied problem that asks for a small set of influential users from a social network, such that by targeting them as early adopters, the expected total adoption through influence cascades over the network is maximized. However, almost all prior work focuses on cascades of a single propagating entity or purely-competitive entities. In this work, we propose the Comparative Independent Cascade (Com-IC) model that covers the full spectrum of entity interactions from competition to complementarity. In Com-IC, users' adoption decisions depend not only on edge-level information propagation, but also on a node-level automaton whose behavior is governed by a set of model parameters, enabling our model to capture not only competition, but also complementarity, to any possible degree. We study two natural optimization problems, Self Influence Maximization and Complementary Influence Maximization, in a novel setting with complementary entities. Both problems are NP-hard, and we devise efficient and effective approximation algorithms via non-trivial techniques based on reverse-reachable sets and a novel \"sandwich approximation\" strategy. The applicability of both techniques extends beyond our model and problems. Our experiments show that the proposed algorithms consistently outperform intuitive baselines on four real-world social networks, often by a significant margin. In addition, we learn model parameters from real user action logs.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                },
                {
                    "authorId": null,
                    "name": "Wei Chen"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "789136d3d750cb967cdd69911a4ef3684815677b",
            "title": "Influence Maximization with Bandits",
            "abstract": "We consider the problem of \\emph{influence maximization}, the problem of maximizing the number of people that become aware of a product by finding the `best' set of `seed' users to expose the product to. Most prior work on this topic assumes that we know the probability of each user influencing each other user, or we have data that lets us estimate these influences. However, this information is typically not initially available or is difficult to obtain. To avoid this assumption, we adopt a combinatorial multi-armed bandit paradigm that estimates the influence probabilities as we sequentially try different seed sets. We establish bounds on the performance of this procedure under the existing edge-level feedback as well as a novel and more realistic node-level feedback. Beyond our theoretical results, we describe a practical implementation and experimentally demonstrate its efficiency and effectiveness on four real datasets.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1711940",
                    "name": "Sharan Vaswani"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "7c84bdec01951e9fd42ac661a8de9e5e6b8ea475",
            "title": "From Group Recommendations to Group Formation",
            "abstract": "There has been significant recent interest in the area of group recommendations, where, given groups of users of a recommender system, one wants to recommend top-$k$ items to a group that maximize the satisfaction of the group members, according to a chosen semantics of group satisfaction. Examples semantics of satisfaction of a recommended itemset to a group include the so-called least misery (LM) and aggregate voting (AV). We consider the complementary problem of how to form groups such that the users in the formed groups are most satisfied with the suggested top-k recommendations. We assume that the recommendations will be generated according to one of the two group recommendation semantics -- LM or AV. Rather than assuming groups are given, or rely on ad hoc group formation dynamics, our framework allows a strategic approach for forming groups of users in order to maximize satisfaction. We show that the problem is NP-hard to solve optimally under both semantics. Furthermore, we develop two efficient algorithms for group formation under LM and show that they achieve bounded absolute error. We develop efficient heuristic algorithms for group formation under AV. We validate our results and demonstrate the scalability and effectiveness of our group formation algorithms on two large real data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1702973",
                    "name": "Senjuti Basu Roy"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "144207288",
                    "name": "R. Liu"
                }
            ]
        },
        {
            "paperId": "25feaaaf2b8faeacc1468d37286b0e5a60625dfb",
            "title": "Optimal recommendations under attraction, aversion, and social influence",
            "abstract": "People's interests are dynamically evolving, often affected by external factors such as trends promoted by the media or adopted by their friends. In this work, we model interest evolution through dynamic interest cascades: we consider a scenario where a user's interests may be affected by (a) the interests of other users in her social circle, as well as (b) suggestions she receives from a recommender system. In the latter case, we model user reactions through either attraction or aversion towards past suggestions. We study this interest evolution process, and the utility accrued by recommendations, as a function of the system's recommendation strategy. We show that, in steady state, the optimal strategy can be computed as the solution of a semi-definite program (SDP). Using datasets of user ratings, we provide evidence for the existence of aversion and attraction in real-life data, and show that our optimal strategy can lead to significantly improved recommendations over systems that ignore aversion and attraction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "1776006",
                    "name": "Stratis Ioannidis"
                },
                {
                    "authorId": "1738297",
                    "name": "Smriti Bhagat"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "2c46c45d21d26488ad2b9227f8360bcbdf204f8a",
            "title": "Show Me the Money: Dynamic Recommendations for Revenue Maximization",
            "abstract": "Recommender Systems (RS) play a vital role in applications such as e-commerce and on-demand content streaming. Research on RS has mainly focused on the customer perspective, i.e., accurate prediction of user preferences and maximization of user utilities. As a result, most existing techniques are not explicitly built for revenue maximization, the primary business goal of enterprises. In this work, we explore and exploit a novel connection between RS and the profitability of a business. As recommendations can be seen as an information channel between a business and its customers, it is interesting and important to investigate how to make strategic dynamic recommendations leading to maximum possible revenue. To this end, we propose a novel revenue model that takes into account a variety of factors including prices, valuations, saturation effects, and competition amongst products. Under this model, we study the problem of finding revenue-maximizing recommendation strategies over a finite time horizon. We show that this problem is NP-hard, but approximation guarantees can be obtained for a slightly relaxed version, by establishing an elegant connection to matroid theory. Given the prohibitively high complexity of the approximation algorithm, we also design intelligent heuristics for the original problem. Finally, we conduct extensive experiments on two real and synthetic datasets and demonstrate the efficiency, scalability, and effectiveness our algorithms, and that they significantly outperform several intuitive baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "2118435622",
                    "name": "Shanshan Chen"
                },
                {
                    "authorId": "40662871",
                    "name": "Keqian Li"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "2fd6e38ddbbb2150a3f8a4e2c4df4a12e1a27736",
            "title": "Modeling non-progressive phenomena for influence propagation",
            "abstract": "Most previous work on modeling influence propagation has focused on progressive models, i.e., once a node is influenced (active) the node stays in that state and cannot become inactive. However, this assumption is unrealistic in many settings where nodes can transition between active and inactive states. For instance, a user of a social network may stop using an app and become inactive, but again activate when instigated by a friend, or when the app adds a new feature or releases a new version. In this work, we study such non-progressive phenomena and propose an efficient model of influence propagation. Specifically, we model influence propagation as a continuous-time Markov process with 2 states: active and inactive. Such a model is both highly scalable (we evaluated on graphs with over 2 million nodes), 17-20 times faster, and more accurate for estimating the spread of influence, as compared with state-of-the-art progressive models for several applications where nodes may switch states.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2879000",
                    "name": "Vincent Yun Lou"
                },
                {
                    "authorId": "1738297",
                    "name": "Smriti Bhagat"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1711940",
                    "name": "Sharan Vaswani"
                }
            ]
        },
        {
            "paperId": "3a3767fb0069b84f793942e67ed1c8e46daa8599",
            "title": "On social event organization",
            "abstract": "Online platforms, such as Meetup and Plancast, have recently become popular for planning gatherings and event organization. However, there is a surprising lack of studies on how to effectively and efficiently organize social events for a large group of people through such platforms. In this paper, we study the key computational problem involved in organization of social events, to our best knowledge, for the first time. We propose the Social Event Organization (SEO) problem as one of assigning a set of events for a group of users to attend, where the users are socially connected with each other and have innate levels of interest in those events. As a first step toward Social Event Organization, we introduce a formal definition of a restricted version of the problem and show that it is NP-hard and is hard to approximate. We propose efficient heuristic algorithms that improve upon simple greedy algorithms by incorporating the notion of phantom events and by using look-ahead estimation. Using synthetic datasets and three real datasets including those from the platforms Meetup and Plancast, we experimentally demonstrate that our greedy heuristics are scalable and furthermore outperform the baseline algorithms significantly in terms of achieving superior social welfare.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40662871",
                    "name": "Keqian Li"
                },
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "1738297",
                    "name": "Smriti Bhagat"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "40592227",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "4b05d596cec2a4609293cda62350f715aecd0e5b",
            "title": "Modeling impression discounting in large-scale recommender systems",
            "abstract": "Recommender systems have become very important for many online activities, such as watching movies, shopping for products, and connecting with friends on social networks. User behavioral analysis and user feedback (both explicit and implicit) modeling are crucial for the improvement of any online recommender system. Widely adopted recommender systems at LinkedIn such as \"People You May Know\" and \"Endorsements\" are evolving by analyzing user behaviors on impressed recommendation items. In this paper, we address modeling impression discounting of recommended items, that is, how to model user's no-action feedback on impressed recommended items. The main contributions of this paper include (1) large-scale analysis of impression data from LinkedIn and KDD Cup; (2) novel anti-noise regression techniques, and its application to learn four different impression discounting functions including linear decay, inverse decay, exponential decay, and quadratic decay; (3) applying these impression discounting functions to LinkedIn's \"People You May Know\" and \"Endorsements\" recommender systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "117308261",
                    "name": "Pei Lee"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "40118098",
                    "name": "Mitul Tiwari"
                },
                {
                    "authorId": "9121988",
                    "name": "Sam Shah"
                }
            ]
        },
        {
            "paperId": "4fa1913ecee973d451dfaf255652c2cc3ac3a41a",
            "title": "CAST: A Context-Aware Story-Teller for Streaming Social Content",
            "abstract": "Online social streams such as Twitter timelines, forum discussions and email threads have emerged as important channels for information propagation. Mining transient stories and their correlations implicit in social streams is a challenging task, since these streams are noisy and surge quickly. In this paper, we propose CAST, which is a context-aware story-teller that discovers new stories from social streams and tracks their structural context on the fly to build a vein of stories. More precisely, we model the social stream as a capillary network, and define stories by a new cohesive subgraph type called (k,d)-Core in the capillary network. We propose deterministic and randomized context search to support the iceberg query, which builds the story vein as social streams flow. We perform detailed experimental study on real Twitter streams and the results demonstrate the creativity and value of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "117308261",
                    "name": "Pei Lee"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736641",
                    "name": "E. Milios"
                }
            ]
        },
        {
            "paperId": "8544c337edcca99983da298ca9a94e96e8fda193",
            "title": "Auto-play: A Data Mining Approach to ODI Cricket Simulation and Prediction",
            "abstract": "Cricket is a popular sport played by 16 countries, is the second most watched sport in the world after soccer, and enjoys a multi-million dollar industry. There is tremendous interest in simulating cricket and more importantly in predicting the outcome of games, particularly in their one-day international format. The complex rules governing the game, along with the numerous natural parameters affecting the outcome of a cricket match present significant challenges for accurate prediction. Multiple diverse parameters, including but not limited to cricketing skills and performances, match venues and even weather conditions can significantly affect the outcome of a game. The sheer number of parameters, along with their interdependence and variance create a non-trivial challenge to create an accurate quantitative model of a game Unlike other sports such as basketball and baseball which are well researched from a sports analytics perspective, for cricket, these tasks have yet to be investigated in depth. In this paper, we build a prediction system that takes in historical match data as well as the instantaneous state of a match, and predicts future match events culminating in a victory or loss. We model the game using a subset of match parameters, using a combination of linear regression and nearestneighbor clustering algorithms. We describe our model and algorithms and finally present quantitative results, demonstrating the performance of our algorithms in predicting the number of runs scored, one of the most important determinants of match outcome.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3236192",
                    "name": "V. Sankaranarayanan"
                },
                {
                    "authorId": "1765604",
                    "name": "Junaed Sattar"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "92440b94f792c397a802787894a7d4da778f7931",
            "title": "Viral Marketing Meets Social Advertising: Ad Allocation with Minimum Regret",
            "abstract": "Social advertisement is one of the fastest growing sectors in the digital advertisement landscape: ads in the form of promoted posts are shown in the feed of users of a social networking platform, along with normal social posts; if a user clicks on a promoted post, the host (social network owner) is paid a fixed amount from the advertiser. In this context, allocating ads to users is typically performed by maximizing click-through-rate, i.e., the likelihood that the user will click on the ad. However, this simple strategy fails to leverage the fact the ads can propagate virally through the network, from endorsing users to their followers. \n \nIn this paper, we study the problem of allocating ads to users through the viral-marketing lenses. We show that allocation that takes into account the propensity of ads for viral propagation can achieve significantly better performance. However, uncontrolled virality could be undesirable for the host as it creates room for exploitation by the advertisers: hoping to tap uncontrolled virality, an advertiser might declare a lower budget for its marketing campaign, aiming at the same large outcome with a smaller cost. \n \nThis creates a challenging trade-off: on the one hand, the host aims at leveraging virality and the network effect to improve advertising efficacy, while on the other hand the host wants to avoid giving away free service due to uncontrolled virality. We formalize this as the problem of ad allocation with minimum regret, which we show is NP-hard and inapproximable w.r.t. any factor. However, we devise an algorithm that provides approximation guarantees w.r.t. the total budget of all advertisers. We develop a scalable version of our approximation algorithm, which we extensively test on four real-world data sets, confirming that our algorithm delivers high quality solutions, is scalable, and significantly outperforms several natural baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3222408",
                    "name": "\u00c7igdem Aslay"
                },
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "9a66e6c635ec2e3701b9059fb25e618a4fb3cb15",
            "title": "Exploratory Search in Databases and the Web",
            "abstract": "The traditional way a user interacts with a database system is through queries. Structured query languages, such as SQL for relational data, XQuery for XML, and SPARQL for RDF data, allow users to submit queries that may precisely capture their information needs, but users need to be familiar with the underlying ontology and data structure and of course the query language itself. Moreover, users need to some extent be familiar with the content of the database and have a clear understanding of their information needs. These requirements stand as the weaknesses of this interaction mode. As data stored in databases grows in unprecedented rates and becomes accessible to diverse and less technically oriented audience, new forms of data exploration and interaction become increasingly more attractive. The World Wide Web represents the largest and arguably the most complex repository of content. Users seek information on the web through two predominant modes: by browsing or by searching. In the first mode, the interaction between the user and the data repository is driven directly by the user\u2019s interpretation of their information need and their information foraging constraints. In the latter mode, a search engine typically mediates the user-data interactions and the process starts with the user entering query-terms that act as surrogates for the user information goals. Freetext queries allow end-users a simple way to express their information needs independently from the underlying data model and structure, as well as from a specific query language. Given a query, the most common strategy has been to present the results as a ranked list. Users have to subsequently peruse the list to satisfy their information needs through browsing the links and/or by issuing further queries. However, the information in the web gets rapidly diversified both in terms of its complexity as well as in terms of the media through which the information is encoded, spanning from large amounts of unstructured and semi-structured data to semantically rich available knowledge. Increasing de-",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1680709",
                    "name": "Georgia Koutrika"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1722086",
                    "name": "Mirek Riedewald"
                },
                {
                    "authorId": "12619004",
                    "name": "Kostas Stefanidis"
                }
            ]
        },
        {
            "paperId": "9edef972f382b156b9822cb2d546f0dbfb75aedd",
            "title": "Incremental cluster evolution tracking from highly dynamic network data",
            "abstract": "Dynamic networks are commonly found in the current web age. In scenarios like social networks and social media, dynamic networks are noisy, are of large-scale and evolve quickly. In this paper, we focus on the cluster evolution tracking problem on highly dynamic networks, with clear application to event evolution tracking. There are several previous works on data stream clustering using a node-by-node approach for maintaining clusters. However, handling of bulk updates, i.e., a subgraph at a time, is critical for achieving acceptable performance over very large highly dynamic networks. We propose a subgraph-by-subgraph incremental tracking framework for cluster evolution in this paper. To effectively illustrate the techniques in our framework, we consider the event evolution tracking task in social streams as an application, where a social stream and an event are modeled as a dynamic post network and a dynamic cluster respectively. By monitoring through a fading time window, we introduce a skeletal graph to summarize the information in the dynamic network, and formalize cluster evolution patterns using a group of primitive evolution operations and their algebra. Two incremental computation algorithms are developed to maintain clusters and track evolution patterns as time rolls on and the network evolves. Our detailed experimental evaluation on large Twitter datasets demonstrates that our framework can effectively track the complete set of cluster evolution patterns from highly dynamic networks on the fly.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "117308261",
                    "name": "Pei Lee"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736641",
                    "name": "E. Milios"
                }
            ]
        },
        {
            "paperId": "db476a9eda0d2dcf3d0c618a3eb8a025c703d58e",
            "title": "Recommending user generated item lists",
            "abstract": "Existing recommender systems mostly focus on recommending individual items which users may be interested in. User-generated item lists on the other hand have become a popular feature in many applications. E.g., Goodreads provides users with an interface for creating and sharing interesting book lists. These user-generated item lists complement the main functionality of the corresponding application, and intuitively become an alternative way for users to browse and discover interesting items to be consumed. Unfortunately, existing recommender systems are not designed for recommending user-generated item lists. In this work, we study properties of these user-generated item lists and propose a Bayesian ranking model, called LIRE for recommending them. The proposed model takes into consideration users' previous interactions with both item lists and with individual items. Furthermore, we propose in LIRE a novel way of weighting items within item lists based on both position of items, and personalized list consumption pattern. Through extensive experiments on a real item list dataset from Goodreads, we demonstrate the effectiveness of our proposed LIRE model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2139492244",
                    "name": "Yidan Liu"
                },
                {
                    "authorId": "2112816239",
                    "name": "M. Xie"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "e8f6fe44e76a540626ef485837aeea12cec8b02d",
            "title": "Generating Top-k Packages via Preference Elicitation",
            "abstract": "There are several applications, such as play lists of songs or movies, and shopping carts, where users are interested in finding top-k packages, consisting of sets of items. In response to this need, there has been a recent flurry of activity around extending classical recommender systems (RS), which are effective at recommending individual items, to recommend packages, or sets of items. The few recent proposals for package RS suffer from one of the following drawbacks: they either rely on hard constraints which may be difficult to be specified exactly by the user or on returning Pareto-optimal packages which are too numerous for the user to sift through. To overcome these limitations, we propose an alternative approach for finding personalized top-k packages for users, by capturing users' preferences over packages using a linear utility function which the system learns. Instead of asking a user to specify this function explicitly, which is unrealistic, we explicitly model the uncertainty in the utility function and propose a preference elicitation-based framework for learning the utility function through feedback provided by the user. We propose several sampling-based methods which, given user feedback, can capture the updated utility function. We develop an efficient algorithm for generating top-k packages using the learned utility function, where the rank ordering respects any of a variety of ranking semantics proposed in the literature. Through extensive experiments on both real and synthetic datasets, we demonstrate the efficiency and effectiveness of the proposed system for finding top-k packages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112816239",
                    "name": "M. Xie"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "20590156",
                    "name": "P. Wood"
                }
            ]
        },
        {
            "paperId": "29f0c51ef0889abef500db0c1d284d93a8d87bc8",
            "title": "Privacy-Preserving Mining of Association Rules From Outsourced Transaction Databases",
            "abstract": "Spurred by developments such as cloud computing, there has been considerable recent interest in the paradigm of data mining-as-a-service. A company (data owner) lacking in expertise or computational resources can outsource its mining needs to a third party service provider (server). However, both the items and the association rules of the outsourced database are considered private property of the corporation (data owner). To protect corporate privacy, the data owner transforms its data and ships it to the server, sends mining queries to the server, and recovers the true patterns from the extracted patterns received from the server. In this paper, we study the problem of outsourcing the association rule mining task within a corporate privacy-preserving framework. We propose an attack model based on background knowledge and devise a scheme for privacy preserving outsourced mining. Our scheme ensures that each transformed item is indistinguishable with respect to the attacker's background knowledge, from at least k-1 other transformed items. Our comprehensive experiments on a very large and real transaction database demonstrate that our techniques are effective, scalable, and protect privacy.",
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "authors": [
                {
                    "authorId": "1685102",
                    "name": "F. Giannotti"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2296962",
                    "name": "A. Monreale"
                },
                {
                    "authorId": "1693341",
                    "name": "D. Pedreschi"
                },
                {
                    "authorId": "2108465033",
                    "name": "Wendy Hui Wang"
                }
            ]
        },
        {
            "paperId": "62befb0bb058a87e559f1a48641108e6ec839cd7",
            "title": "Validating Network Value of Influencers by Means of Explanations",
            "abstract": "Recently, there has been significant interest in social influence analysis. One of the central problems in this area is the problem of identifying influencers, such that by convincing these users to perform a certain action (like buying a new product), a large number of other users get influenced to follow the action. The client of such an application is essentially a marketer who would target these influencers for marketing a given new product, say by providing free samples or discounts. It is natural that before committing resources for targeting an influencer the marketer would be interested in validating the influence (or network value) of influencers returned. This requires digging deeper into such analytical questions as: who are their followers, on what actions (or products) they are influential, etc. However, the current approaches to identifying influencers largely work as a black box in this respect. The goal of this paper is to open up the black box, address these questions and provide informative and crisp explanations for validating the network value of influencers. We formulate the problem of providing explanations (called PROXI) as a discrete optimization problem of feature selection. We show that PROXI is not only NP-hard to solve exactly, it is NP-hard to approximate within any reasonable factor. Nevertheless, we show interesting properties of the objective function and develop an intuitive greedy heuristic. We perform detailed experimental analysis on two real world datasets - Twitter and Flixster, and show that our approach is useful in generating concise and insightful explanations of the influence distribution of users and that our greedy algorithm is effective and efficient with respect to several baselines.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2698019",
                    "name": "Glenn S. Bevilacqua"
                },
                {
                    "authorId": "145311756",
                    "name": "S. Clare"
                },
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "6b8110d92fe0d94c4cfa65d86afe382fd4811b58",
            "title": "KeySee: supporting keyword search on evolving events in social streams",
            "abstract": "Online social streams such as Twitter/Facebook timelines and forum discussions have emerged as prevalent channels for information dissemination. As these social streams surge quickly, information overload has become a huge problem. Existing keyword search engines on social streams like Twitter Search are not successful in overcoming the problem, because they merely return an overwhelming list of posts, with little aggregation or semantics. In this demo, we provide a new solution called \\keysee by grouping posts into events, and track the evolution patterns of events as new posts stream in and old posts fade out. Noise and redundancy problems are effectively addressed in our system. Our demo supports refined keyword query on evolving events by allowing users to specify the time span and designated evolution pattern. For each event result, we provide various analytic views such as frequency curves, word clouds and GPS distributions. We deploy \\keysee on real Twitter streams and the results show that our demo outperforms existing keyword search engines on both quality and usability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "117308261",
                    "name": "Pei Lee"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736641",
                    "name": "E. Milios"
                }
            ]
        },
        {
            "paperId": "7545f0620626d3a2a633954a8e5bd150d9544899",
            "title": "HeteroMF: recommendation in heterogeneous information networks using context dependent factor models",
            "abstract": "With the growing amount of information available online, recommender systems are starting to provide a viable alternative and complement to search engines, in helping users to find objects of interest. Methods based on Matrix Factorization (MF) models are the state-of-the-art in recommender systems. The input to MF is user feedback, in the form of a rating matrix. However, users can be engaged in interactions with multiple types of entities across different contexts, leading to multiple rating matrices. In other words, users can have interactions in a heterogeneous information network. Generally, in a heterogeneous network, entities from any two entity types can have interactions with a weight (rating) indicating the level of endorsement. Collective Matrix Factorization (CMF) has been proposed to address the recommendation problem in heterogeneous networks. However, a main issue with CMF is that entities share the same latent factor across different contexts. This is particularly problematic in two cases: Latent factors for entities that are cold-start in a context will be learnt mainly based on the data from other contexts where these entities are not cold-start, and therefore the factors are not properly learned for the cold-start context. Also, if a context has more data compared to another context, then the dominant context will dominate the learning process for the latent factors for entities shared in these two contexts. In this paper, we propose a context-dependent matrix factorization model, HeteroMF, that considers a general latent factor for entities of every entity type and context-dependent latent factors for every context in which the entities are involved. We learn a general latent factor for every entity and transfer matrices for every context to convert the general latent factors into a context-dependent latent factor. Experiments on two real life datasets from Epinions and Flixster demonstrate that HeteroMF substantially outperforms CMF, particularly for cold-start entities and for contexts where interactions in one contexts are dominated by other contexts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35209354",
                    "name": "Mohsen Jamali"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "92c401de089a053c8dff54661ae9e33862152f23",
            "title": "IPS: An Interactive Package Configuration System for Trip Planning",
            "abstract": "When planning a trip, one essential task is to find a set of Places-of-Interest (POIs) which can be visited during the trip. Using existing travel guides or websites such as Lonely Planet and TripAdvisor, the user has to either manually work out a desirable set of POIs or take pre-configured travel packages; the former can be time consuming while the latter lacks flexibility. In this demonstration, we propose an Interactive Package configuration System (IPS), which visualizes different candidate packages on a map, and enables users to configure a travel package through simple interactions, i.e., comparing packages and fixing/removing POIs from a package. Compared with existing trip planning systems, we believe IPS strikes the right balance between flexibility and manual effort.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112816239",
                    "name": "M. Xie"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "20590156",
                    "name": "P. Wood"
                }
            ]
        },
        {
            "paperId": "a4f0fccddce5127797e322ec599e81d7d8f10320",
            "title": "Efficient top-k query answering using cached views",
            "abstract": "Top-k query processing has recently received a significant amount of attention due to its wide application in information retrieval, multimedia search and recommendation generation. In this work, we consider the problem of how to efficiently answer a top-k query by using previously cached query results. While there has been some previous work on this problem, existing algorithms suffer from either limited scope or lack of scalability. In this paper, we propose two novel algorithms for handling this problem. The first algorithm LPTA+ provides significantly improved efficiency compared to the state-of-the-art LPTA algorithm [26] by reducing the number of expensive linear programming problems that need to be solved. The second algorithm we propose leverages a standard space partition-based index structure in order to avoid many of the drawbacks of LPTA-based algorithms, thereby further improving the efficiency of query processing. Through extensive experiments on various datasets, we demonstrate that our algorithms significantly outperform the state of the art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112816239",
                    "name": "M. Xie"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "20590156",
                    "name": "P. Wood"
                }
            ]
        },
        {
            "paperId": "c9d2b98ed844959947f87b8655dc36a7d632c0fa",
            "title": "Fair Recommendations for Online Barter Exchange Networks",
            "abstract": "Of late online social networks have become popular, with interest spanning various aspects including search, analysis/mining, and their potential use for item barter exchange markets. The idea is that users can leverage their social network for exchanging items they possess with other users. The problem of generating recommendations for item exchanges between users, consisting of synchronous exchange cycles has been investigated[2]. In this paper, we identify the shortcomings of the above exchange model and propose an asynchronous model that makes use of credit points. Rather than insist on exchanging items synchronously, we award points to users whenever they give items to other users, which can be redeemed later. Points and their redemption raise an issue of fairness which intuitively means users who contribute more should have a greater priority over others for receiving items they wish for. We focus on fairness maximization and prove that it is NPhard and cannot be approximated within any factor in polynomial time unless P=NP. We then develop efficient heuristic algorithms, and experimentally demonstrate their effectiveness and scalability on both synthetic data and a real dataset from readitswapit.co.uk.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1678458",
                    "name": "Z. Abbassi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2112816239",
                    "name": "M. Xie"
                }
            ]
        },
        {
            "paperId": "f5ef58926c2cc0e36b6206623184ac0a3db40ca7",
            "title": "The bang for the buck: fair competitive viral marketing from the host perspective",
            "abstract": "The key algorithmic problem in viral marketing is to identify a set of influential users (called seeds) in a social network, who, when convinced to adopt a product, shall influence other users in the network, leading to a large number of adoptions. When two or more players compete with similar products on the same network we talk about competitive viral marketing, which so far has been studied exclusively from the perspective of one of the competing players. In this paper we propose and study the novel problem of competitive viral marketing from the perspective of the host, i.e., the owner of the social network platform. The host sells viral marketing campaigns as a service to its customers, keeping control of the selection of seeds. Each company specifies its budget and the host allocates the seeds accordingly. From the host's perspective, it is important not only to choose the seeds to maximize the collective expected spread, but also to assign seeds to companies so that it guarantees the \"bang for the buck\" for all companies is nearly identical, which we formalize as the fair seed allocation problem. We propose a new propagation model capturing the competitive nature of viral marketing. Our model is intuitive and retains the desired properties of monotonicity and submodularity. We show that the fair seed allocation problem is NP-hard, and develop an efficient algorithm called Needy Greedy. We run experiments on three real-world social networks, showing that our algorithm is effective and scalable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "f9ec8ed763a854d7659c148580f2b837db94792b",
            "title": "Event Evolution Tracking from Streaming Social Posts",
            "abstract": "Online social post streams such as Twitter timelines and forum discussions have emerged as important channels for information dissemination. They are noisy, informal, and surge quickly. Real life events, which may happen and evolve every minute, are perceived and circulated in post streams by social users. Intuitively, an event can be viewed as a dense cluster of posts with a life cycle sharing the same descriptive words. There are many previous works on event detection from social streams. However, there has been surprisingly little work on tracking the evolution patterns of events, e.g., birth/death, growth/decay, merge/split, which we address in this paper. To define a tracking scope, we use a sliding time window, where old posts disappear and new posts appear at each moment. Following that, we model a social post stream as an evolving network, where each social post is a node, and edges between posts are constructed when the post similarity is above a threshold. We propose a framework which summarizes the information in the stream within the current time window as a ``sketch graph'' composed of ``core'' posts. We develop incremental update algorithms to handle highly dynamic social streams and track event evolution patterns in real time. Moreover, we visualize events as word clouds to aid human perception. Our evaluation on a real data set consisting of 5.2 million posts demonstrates that our method can effectively track event dynamics in the whole life cycle from very large volumes of social streams on the fly.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "117308261",
                    "name": "Pei Lee"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736641",
                    "name": "E. Milios"
                }
            ]
        },
        {
            "paperId": "4508e10535e3977f239b1d86d5e139e048f3db6a",
            "title": "Declarative Entity Resolution via Matching Dependencies and Answer Set Programs",
            "abstract": "Entity resolution (ER) is an important and common problem in data cleaning. It is about identifying and merging records in a database that represent the same real-world entity. Recently, matching dependencies (MDs) have been introduced and investigated as declarative rules that specify ER. An ER process induced by MDs over a dirty instance leads to multiple clean instances, in general. In this work, we present disjunctive answer set programs (with stable model semantics) that capture through their models the class of alternative clean instances obtained after an ER process based on MDs. With these programs, we can obtain clean answers to queries, i.e. those that are invariant under the clean instances, by skeptically reasoning from the program. We investigate the ER programs in terms of expressive power for the ER task at hand. As an important special and practical case of ER, we provide a declarative reconstruction of the so-called union-case ER methodology, as presented through a generic approach to ER (the so-called Swoosh approach).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32667881",
                    "name": "Z. Bahmani"
                },
                {
                    "authorId": "1699123",
                    "name": "L. Bertossi"
                },
                {
                    "authorId": "3058376",
                    "name": "Solmaz Kolahi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "560f3a49aa926dd2670de9c174e616a76fa4cea0",
            "title": "Profit Maximization over Social Networks",
            "abstract": "Influence maximization is the problem of finding a set of influential users in a social network such that the expected spread of influence under a certain propagation model is maximized. Much of the previous work has neglected the important distinction between social influence and actual product adoption. However, as recognized in the management science literature, an individual who gets influenced by social acquaintances may not necessarily adopt a product (or technology), due, e.g., to monetary concerns. In this work, we distinguish between influence and adoption by explicitly modeling the states of being influenced and of adopting a product. We extend the classical Linear Threshold (LT) model to incorporate prices and valuations, and factor them into users' decision-making process of adopting a product. We show that the expected profit function under our proposed model maintains submodularity under certain conditions, but no longer exhibits monotonicity, unlike the expected influence spread function. To maximize the expected profit under our extended LT model, we employ an unbudgeted greedy framework to propose three profit maximization algorithms. The results of our detailed experimental study on three real-world datasets demonstrate that of the three algorithms, PAGE, which assigns prices dynamically based on the profit potential of each candidate seed, has the best performance both in the expected profit achieved and in running time.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "authors": [
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "6ae7bcda00a4c3751fbe528cffd95f73da6d8017",
            "title": "Efficient extraction of ontologies from domain specific text corpora",
            "abstract": "Extracting ontological relationships (e.g., ISA and HASA) from free-text repositories (e.g., engineering documents and instruction manuals) can improve users' queries, as well as benefit applications built for these domains. Current methods to extract ontologies from text usually miss many meaningful relationships because they either concentrate on single-word terms and short phrases or neglect syntactic relationships between concepts in sentences. We propose a novel pattern-based algorithm to find ontological relationships between complex concepts by exploiting parsing information to extract multi-word concepts and nested concepts. Our procedure is iterative: we tailor the constrained sequential pattern mining framework to discover new patterns. Our experiments on three real data sets show that our algorithm consistently and significantly outperforms previous representative ontology extraction algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118910181",
                    "name": "Tianyu Li"
                },
                {
                    "authorId": "2360227",
                    "name": "P. Chubak"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "144876103",
                    "name": "R. Pottinger"
                }
            ]
        },
        {
            "paperId": "80424ee63e25678cc73164b1eb3a9054ddda7dde",
            "title": "On Top-k Structural Similarity Search",
            "abstract": "Search for objects similar to a given query object in a network has numerous applications including web search and collaborative filtering. We use the notion of structural similarity to capture the commonality of two objects in a network, e.g., if two nodes are referenced by the same node, they may be similar. Meeting-based methods including SimRank and P-Rank capture structural similarity very well. Deriving inspiration from PageRank, SimRank has gained popularity by a natural intuition and domain independence. Since it's computationally expensive, subsequent work has focused on optimizing and approximating the computation of SimRank. In this paper, we approach SimRank from a top-k querying perspective where given a query node v, we are interested in finding the top-k nodes that have the highest SimRank score w.r.t. v. The only known approaches for answering such queries are either a naive algorithm of computing the similarity matrix for all node pairs or computing the similarity vector by comparing the query node v with each other node independently, and then picking the top-k. None of these approaches can handle top-k structural similarity search efficiently by scaling to very large graphs consisting of millions of nodes. We propose an algorithmic framework called TopSim based on transforming the top-k SimRank problem on a graph G to one of finding the top-k nodes with highest authority on the product graph G G. We further accelerate Top Sim by merging similarity paths and develop a more efficient algorithm called Top Sim-SM. Two heuristic algorithms, Trun-Top Sim-SM and Prio-Top Sim-SM, are also proposed to approximate Top Sim-SM on scale-free graphs to trade accuracy for speed, based on truncated random walk and prioritizing propagation respectively. We analyze the accuracy and performance of Top Sim family algorithms and report the results of a detailed experimental study.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "117308261",
                    "name": "Pei Lee"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "144666776",
                    "name": "J. Yu"
                }
            ]
        },
        {
            "paperId": "906b86157267fd8e850af3d529ba8fa774bb1ebb",
            "title": "Maximizing product adoption in social networks",
            "abstract": "One of the key objectives of viral marketing is to identify a small set of users in a social network, who when convinced to adopt a product will influence others in the network leading to a large number of adoptions in an expected sense. The seminal work of Kempe et al. [13] approaches this as the problem of influence maximization. This and other previous papers tacitly assume that a user who is influenced (or, informed) about a product necessarily adopts the product and encourages her friends to adopt it. However, an influenced user may not adopt the product herself, and yet form an opinion based on the experiences of her friends, and share this opinion with others. Furthermore, a user who adopts the product may not like the product and hence not encourage her friends to adopt it to the same extent as another user who adopted and liked the product. This is independent of the extent to which those friends are influenced by her. Previous works do not account for these phenomena.\n We argue that it is important to distinguish product adoption from influence. We propose a model that factors in a user's experience (or projected experience) with a product. We adapt the classical Linear Threshold (LT) propagation model by defining an objective function that explicitly captures product adoption, as opposed to influence. We show that under our model, adoption maximization is NP-hard and the objective function is monotone and submodular, thus admitting an approximation algorithm. We perform experiments on three real popular social networks and show that our model is able to distinguish between influence and adoption, and predict product adoption much more accurately than approaches based on the classical LT model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738297",
                    "name": "Smriti Bhagat"
                },
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "b3471c62c0d3735b36f551fae6427b6d573411c0",
            "title": "RecMax: exploiting recommender systems for fun and profit",
            "abstract": "In recent times, collaborative filtering based Recommender Systems (RS) have become extremely popular. While research in recommender systems has mostly focused on improving the accuracy of recommendations, in this paper, we look at the \"flip\" side of a RS. That is, instead of improving existing recommender algorithms, we ask whether we can use an existing operational RS to launch a targeted marketing campaign. To this end, we propose a novel problem called RecMax that aims to select a set of \"seed\" users for a marketing campaign for a new product, such that if they endorse the product by providing relatively high ratings, the number of other users to whom the product is recommended by the underlying RS algorithm is maximum. We motivate RecMax with real world applications. We show that seeding can make a substantial difference, if done carefully. We prove that RecMax is not only NP-hard to solve optimally, it is NP-hard to even approximate within any reasonable factor. Given this hardness, we explore several natural heuristics on 3 real world datasets - Movielens, Yahoo! Music and Jester Joke and report our findings. We show that even though RecMax is hard to approximate, simple natural heuristics may provide impressive gains, for targeted marketing using RS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "36c0b81a2ef2505e5c1c763c1abc25cdd72903f2",
            "title": "A Data-Based Approach to Social Influence Maximization",
            "abstract": "Influence maximization is the problem of finding a set of users in a social network, such that by targeting this set, one maximizes the expected spread of influence in the network. Most of the literature on this topic has focused exclusively on the social graph, overlooking historical data, i.e., traces of past action propagations. In this paper, we study influence maximization from a novel data-based perspective. In particular, we introduce a new model, which we call credit distribution, that directly leverages available propagation traces to learn how influence flows in the network and uses this to estimate expected influence spread. Our approach also learns the different levels of influence-ability of users, and it is time-aware in the sense that it takes the temporal nature of influence into account. \n \nWe show that influence maximization under the credit distribution model is NP-hard and that the function that defines expected spread under our model is submodular. Based on these, we develop an approximation algorithm for solving the influence maximization problem that at once enjoys high accuracy compared to the standard approach, while being several orders of magnitude faster and more scalable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "3d84a02dfe4fc904aa729b8b7ca51fd8c97a9dc7",
            "title": "SIMPATH: An Efficient Algorithm for Influence Maximization under the Linear Threshold Model",
            "abstract": "There is significant current interest in the problem of influence maximization: given a directed social network with influence weights on edges and a number k, find k seed nodes such that activating them leads to the maximum expected number of activated nodes, according to a propagation model. Kempe et al. showed, among other things, that under the Linear Threshold Model, the problem is NP-hard, and that a simple greedy algorithm guarantees the best possible approximation factor in PTIME. However, this algorithm suffers from various major performance drawbacks. In this paper, we propose Simpath, an efficient and effective algorithm for influence maximization under the linear threshold model that addresses these drawbacks by incorporating several clever optimizations. Through a comprehensive performance study on four real data sets, we show that Simpath consistently outperforms the state of the art w.r.t. running time, memory consumption and the quality of the seed set chosen, measured in terms of expected influence spread achieved.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "4b983a1a0b370a4ea87c2be4f2c7c53b13edb4f6",
            "title": "CELF++: optimizing the greedy algorithm for influence maximization in social networks",
            "abstract": "Kempe et al. [4] (KKT) showed the problem of influence maximization is NP-hard and a simple greedy algorithm guarantees the best possible approximation factor in PTIME. However, it has two major sources of inefficiency. First, finding the expected spread of a node set is #P-hard. Second, the basic greedy algorithm is quadratic in the number of nodes. The first source is tackled by estimating the spread using Monte Carlo simulation or by using heuristics[4, 6, 2, 5, 1, 3]. Leskovec et al. proposed the CELF algorithm for tackling the second. In this work, we propose CELF++ and empirically show that it is 35-55% faster than CELF.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "2153424520",
                    "name": "Wei Lu"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "79a58bb8e6a7a155237fe5a5b8cbd0bafeb68488",
            "title": "CompRec-Trip: A composite recommendation system for travel planning",
            "abstract": "Classical recommender systems provide users with a list of recommendations where each recommendation consists of a single item, e.g., a book or a DVD. However, applications such as travel planning can benefit from a system capable of recommending packages of items, under a user-specified budget and in the form of sets or sequences. In this context, there is a need for a system that can recommend top-k packages for the user to choose from. In this paper, we propose a novel system, CompRec-Trip, which can automatically generate composite recommendations for travel planning. The system leverages rating information from underlying recommender systems, allows flexible package configuration and incorporates users' cost budgets on both time and money. Furthermore, the proposed CompRec-Trip system has a rich graphical user interface which allows users to customize the returned composite recommendations and take into account external local information.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112816239",
                    "name": "M. Xie"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "20590156",
                    "name": "P. Wood"
                }
            ]
        },
        {
            "paperId": "906f879163317e487374e889d00a7c59ec476ae2",
            "title": "TopRecs: Top-k algorithms for item-based collaborative filtering",
            "abstract": "Recommender systems help users find their items of interest from large data collections with little effort. Collaborative filtering (CF) is one of the most popular approaches for making recommendations. While significant work has been done on improving accuracy of CF methods, some of the most popular CF approaches are limited in terms of scalability and efficiency. The size of data in modern recommender systems is growing rapidly in terms of both new users and items and new ratings. Item-based recommendation is one of the CF approaches used widely in practice. It computes and uses an item-item similarity matrix in order to predict unknown ratings. Previous works on item-based CF method confirm its usefulness in providing high quality top-k results. In this paper, we design a scalable algorithm for top-k recommendations using this method. We achieve this by probabilistic modeling of the similarity matrix. A unique challenge here is that the ratings that are aggregated to produce the aggregate predicted score for a user should be obtained from different lists for different candidate items and the aggregate function is non-monotone. We propose a layered architecture for CF systems that facilitates computation of the most relevant items for a given user. We design efficient top-k algorithms and data structures in order to achieve high scalability. Our algorithm is based on abstracting the key computation of a CF algorithm in terms of two operations -- probe and explore. The algorithm uses a cost-based optimization whereby we express the overall cost as a function of a similarity threshold and determine its optimal value for minimizing the cost. We empirically evaluate our theoretical results on a large real world dataset. Our experiments show our exact top-k algorithm achieves better scalability compared to solid baseline algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36868092",
                    "name": "M. Khabbaz"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "91619aba3c36cd779ee25659898dfd8b8696a55d",
            "title": "Finding Heavy Paths in Graphs: A Rank Join Approach",
            "abstract": "Graphs have been commonly used to model many applications. A natural problem which abstracts applications such as itinerary planning, playlist recommendation, and flow analysis in information networks is that of finding the heaviest path(s) in a graph. More precisely, we can model these applications as a graph with non-negative edge weights, along with a monotone function such as sum, which aggregates edge weights into a path weight, capturing some notion of quality. We are then interested in finding the top-k heaviest simple paths, i.e., the $k$ simple (cycle-free) paths with the greatest weight, whose length equals a given parameter $\\ell$. We call this the \\emph{Heavy Path Problem} (HPP). It is easy to show that the problem is NP-Hard. \nIn this work, we develop a practical approach to solve the Heavy Path problem by leveraging a strong connection with the well-known Rank Join paradigm. We first present an algorithm by adapting the Rank Join algorithm. We identify its limitations and develop a new exact algorithm called HeavyPath and a scalable heuristic algorithm. We conduct a comprehensive set of experiments on three real data sets and show that HeavyPath outperforms the baseline algorithms significantly, with respect to both $\\ell$ and $k$. Further, our heuristic algorithm scales to longer lengths, finding paths that are empirically within 50% of the optimum solution or better under various settings, and takes only a fraction of the running time compared to the exact algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36868092",
                    "name": "M. Khabbaz"
                },
                {
                    "authorId": "1738297",
                    "name": "Smriti Bhagat"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "a66a983e5af2242a6ec5041c1b07f3fc1a6c9dd5",
            "title": "Trajectory anonymity in publishing personal mobility data",
            "abstract": "Recent years have witnessed pervasive use of location-aware devices such as GSM mobile phones, GPS-enabled PDAs, location sensors, and active RFID tags. The use of these devices generates a huge collection of spatio-temporal data, variously called moving object data, trajectory data, or moblity data. These data can be used for various data analysis purposes such as city traffic control, mobility management, urban planning, and location-based service advertisements. Clearly, the spatio-temporal data so collected may help an attacker to discover personal and sensitive information like user habits, social customs, religious and sexual preferences of individuals. Consequently, it raises serious concerns about privacy. Simply replacing users' real identifiers (name, SSN, etc.) with pseudonyms is insufficient to guarantee anonymity. The problem is that due to the existence of quasi-identifiers, i.e., spatio-temporal data points that can be linked to external information to re-identify individuals, the attacker may be able to trace the anonymous spatio-temporal data back to individuals.\n In this survey, we discuss recent advancement on anonymity preserving data publishing of moving object databases in an off-line fashion. We first introduce several anonymity models, then we describe in detail some of the proposed techniques to enforce trajectory anonymity, discussing their merits and limitations. We conclude by identifying challenging open problems that need attention.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2108465033",
                    "name": "Wendy Hui Wang"
                }
            ]
        },
        {
            "paperId": "c04a77c348e76c67383825ae2a121e3a9993e61e",
            "title": "TopRecs+: Pushing the Envelope on Recommender Systems",
            "abstract": "Spurred by the advances in collaborative filtering, by applications that form the core business of companies such as Amazon and Netflix, and indeed by incentives such as the famous Netflix Prize, research on recommender systems has become quite mature and sophisticated algorithms that enjoy high prediction accuracy have been developed [1]. Most of this research has been concerned with what we regard as first generation recommender systems. Ever since the database community got interested in recommender systems, people have begun asking questions related to functionality. This includes developing flexible recommender systems which can efficiently compute top-k items within their framework [18] and using recommender systems to design packages subject to user specified constraints [11]. Significant effort has been dedicated to improving accuracy of recommendations. Many of the recommendation algorithms, while highly accurate, have scalability issues. The number of items managed by modern information systems is growing rapidly. Therefore, scalability is one of the serious issues for future generation recommender systems. Recommendation methods try to capture personalized patterns in user feedback data by making assumptions and keeping dense summaries of data. User feedback is typically represented in the form of a sparse matrix that stores existing ratings of users on items. There are two groups of methods \u2013 modelbased and memory-based [1]. Model-based methods assume there is a lower dimensional underlying parametric model that has generated the ratings matrix. These methods aim at finding optimal parameter values for the model, given the observed data. Memory-based methods, on the other hand, calculate similarities between users or items and use these similarities for aggregating existing ratings and predicting unknown ratings. Thus, any item recommendation process has two steps: (1) an off-line training phase that captures personalized profiles (either as a model or as a similarity matrix); (2) an online recommendation generation process that uses the latest up-to-date model or similarity matrix to return top-k recommendations for a user. Any approach for improving scalability of item recommendation needs to pay attention to both profile building and recommendation generation. In section 2, we show how better scalability can be achieved in both aspects for one of the most popular and practical recommendation algorithms. In addition to efficiency and scalability, an important limitation of classical recommender systems is that they only provide recommendations consisting of single items, e.g., books or DVDs. It has been recognized several applications call for composite recommendations consisting of sets, lists or other collections. For example, in trip planning, a user is interested in suggestions for a set of places to visit, or points of interest (POI). If the recommender system only provides a ranked list of POIs, the user has to manually figure out the most suitable set of POIs, which is often non-trivial as there may be a cost to visiting each place (time, price, etc.), and the",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36868092",
                    "name": "M. Khabbaz"
                },
                {
                    "authorId": "2112816239",
                    "name": "M. Xie"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "c271555dd4a68e712b3858231dd55ebf89b9cbca",
            "title": "Efficient rank join with aggregation constraints",
            "abstract": "We show aggregation constraints that naturally arise in several applications can enrich the semantics of rank join queries, by allowing users to impose their application-specific preferences in a declarative way. By analyzing the properties of aggregation constraints, we develop efficient deterministic and probabilistic algorithms which can push the aggregation constraints inside the rank join framework. Through extensive experiments on various datasets, we show that in many cases our proposed algorithms can significantly outperform the naive approach of applying the state-of-the-art rank join algorithm followed by post-filtering to discard results violating the constraints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112816239",
                    "name": "M. Xie"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "20590156",
                    "name": "P. Wood"
                }
            ]
        },
        {
            "paperId": "c528fac79f69cb474b8144011aab92b5ce264a0c",
            "title": "Adding structure to top-k: from items to expansions",
            "abstract": "Keyword based search interfaces are extremely popular as a means for efficiently discovering items of interest from a huge collection, as evidenced by the success of search engines like Google and Bing. However, most of the current search services still return results as a flat ranked list of items. Considering the huge number of items which can match a query, this list based interface can be very difficult for the user to explore and find important items relevant to their search needs. In this work, we consider a search scenario in which each item is annotated with a set of keywords. E.g., in Web 2.0 enabled systems such as flickr and del.icio.us, it is common for users to tag items with keywords. Based on this annotation information, we can automatically group query result items into different expansions of the query corresponding to subsets of keywords. We formulate and motivate this problem within a top-k query processing framework, but as that of finding the top-k most important expansions. Then we study additional desirable properties for the set of expansions returned, and formulate the problem as an optimization problem of finding the best k expansions satisfying all the desirable properties. We propose several efficient algorithms for this problem. Our problem is similar in spirit to recent works on automatic facets generation, but has the important difference and advantage that we don't need to assume the existence of pre-defined categorical hierarchy which is critical for these works. Through extensive experiments on both real and synthetic datasets, we show our proposed algorithms are both effective and efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143594515",
                    "name": "Xueyao Liang"
                },
                {
                    "authorId": "2112816239",
                    "name": "M. Xie"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "dd3c85737bbe4676aa33a67260b698725c8a0488",
            "title": "Fast Matrix Computations for Pairwise and Columnwise Commute Times and Katz Scores",
            "abstract": "Abstract We explore methods for approximating the commute time and Katz score between a pair of nodes. These methods are based on the approach of matrices, moments, and quadrature developed in the numerical linear algebra community. They rely on the Lanczos process and provide upper and lower bounds on an estimate of the pairwise scores. We also explore methods to approximate the commute times and Katz scores from a node to all other nodes in the graph. Here, our approach for the commute times is based on a variation of the conjugate gradient algorithm, and it provides an estimate of all the diagonals of the inverse of a matrix. Our technique for the Katz scores is based on exploiting an empirical localization property of the Katz matrix. We adapt algorithms used for personalized PageRank computing to these Katz scores and theoretically show that this approach is convergent. We evaluate these methods on 17 real-world graphs ranging in size from 1000 to 1,000,000 nodes. Our results show that our pairwise commute-time method and columnwise Katz algorithm both have attractive theoretical properties and empirical performance.",
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "2467965",
                    "name": "Pooya Esfandiar"
                },
                {
                    "authorId": "1757913",
                    "name": "D. Gleich"
                },
                {
                    "authorId": "1805855",
                    "name": "C. Greif"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "fca21657698a21c74e4b989e8b2061de116ae7f8",
            "title": "We challenge you to certify your updates",
            "abstract": "Correctness of data residing in a database is vital. While integrity constraint enforcement can often ensure data consistency, it is inadequate to protect against updates that involve careless, unintentional errors, e.g., whether a specified update to an employee's record was for the intended employee. We propose a novel approach that is complementary to existing integrity enforcement techniques, to guard against such erroneous updates.\n Our approach is based on (a) updaters providing an update certificate with each database update, and (b) the database system verifying the correctness of the update certificate provided before performing the update. We formalize a certificate as a (challenge, response) pair, and characterize good certificates as those that are easy for updaters to provide and, when correct, give the system enough confidence that the update was indeed intended. We present algorithms that efficiently enumerate good challenges, without exhaustively exploring the search space of all challenges. We experimentally demonstrate that (i) databases have many good challenges, (ii) these challenges can be efficiently identified, (iii) certificates can be quickly verified for correctness, (iv) under natural models of an updater's knowledge of the database, update certificates catch a high percentage of the erroneous updates without imposing undue burden on the updaters performing correct updates, and (v) our techniques are robust across a wide range of challenge parameter settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111240614",
                    "name": "Su Chen"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "21968ae000669eb4cf03718a0d97e23a6bf75926",
            "title": "Learning influence probabilities in social networks",
            "abstract": "Recently, there has been tremendous interest in the phenomenon of influence propagation in social networks. The studies in this area assume they have as input to their problems a social graph with edges labeled with probabilities of influence between users. However, the question of where these probabilities come from or how they can be computed from real social network data has been largely ignored until now. Thus it is interesting to ask whether from a social graph and a log of actions by its users, one can build models of influence. This is the main problem attacked in this paper. In addition to proposing models and algorithms for learning the model parameters and for testing the learned models to make predictions, we also develop techniques for predicting the time by which a user may be expected to perform an action. We validate our ideas and techniques using the Flickr data set consisting of a social graph with 1.3M nodes, 40M edges, and an action log consisting of 35M tuples referring to 300K distinct actions. Beyond showing that there is genuine influence happening in a real social network, we show that our techniques have excellent prediction performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "61801551f2bc5cad4ce8a40be8b49f2262f31e4f",
            "title": "Breaking out of the box of recommendations: from items to packages",
            "abstract": "Classical recommender systems provide users with a list of recommendations where each recommendation consists of a single item, e.g., a book or DVD. However, several applications can benefit from a system capable of recommending packages of items, in the form of sets. Sample applications include travel planning with a limited budget (price or time) and twitter users wanting to select worthwhile tweeters to follow given that they can deal with only a bounded number of tweets. In these contexts, there is a need for a system that can recommend top-k packages for the user to choose from.\n Motivated by these applications, we consider composite recommendations, where each recommendation comprises a set of items. Each item has both a value (rating) and a cost associated with it, and the user specifies a maximum total cost (budget) for any recommended set of items. Our composite recommender system has access to one or more component recommender system, focusing on different domains, as well as to information sources which can provide the cost associated with each item. Because the problem of generating the top recommendation (package) is NP-complete, we devise several approximation algorithms for generating top-k packages as recommendations. We analyze their efficiency as well as approximation quality. Finally, using two real and two synthetic data sets, we subject our algorithms to thorough experimentation and empirical analysis. Our findings attest to the efficiency and quality of our approximation algorithms for top-k packages compared to exact algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112816239",
                    "name": "M. Xie"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "20590156",
                    "name": "P. Wood"
                }
            ]
        },
        {
            "paperId": "af5a50e56c4b30a70cf6ee71cad1e98d789200a4",
            "title": "Approximation Analysis of Influence Spread in Social Networks",
            "abstract": "In the context of influence propagation in a social graph, we can identify three orthogonal dimensions - the number of seed nodes activated at the beginning (known as budget), the expected number of activated nodes at the end of the propagation (known as expected spread or coverage), and the time taken for the propagation. We can constrain one or two of these and try to optimize the third. In their seminal paper (12), Kempe, Kleinberg and Tardos (KKT) constrained the budget, left time unconstrained, and maximized the coverage. In this paper, we study alternative optimization problems: MINSEED and MINTIME. In MINSEED, a coverage thresholdis given and the task is to find the minimum size seed set such that by activating it, at leastnodes are eventually activated in the expected sense. In MINTIME, a coverage thresholdand a budget threshold k are given, and the task is to find a seed set of size at most k such that by activating it, at leastnodes are activated in the expected sense, in the minimum possible time. It turns out both these problems are NP-hard. Given the hardness of the problems, we naturally turn to the subject of approximation algo- rithms. For MINSEED, we develop a bicriteria approximation by exploiting its relationship to the problem of Real-valued Submodular Set Cover (RSSC). We prove a generic inapproximabil- ity result for RSSC suggesting that improving this approximation factor is likely to be hard. For MINTIME we show that even bicriteria and tricriteria approximations are hard under several conditions. Our proof exploits the relationship between MINTIME and the problem of Robust Asymmetric k-center (RAKC). We show, however, that if we allow the budget for number of seeds k to be boosted by a logarithmic factor and allow the coverage to fall short, then the problem can be solved exactly in PTIME, i.e., we can achieve the required coverage within the time achieved by the optimal solution to MINTIME with budget k and coverage threshold \ufffd. Finally, we show the value of the approximation algorithms, by conducting an experimental comparison of their quality against that achieved by various heuristics.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "0b2a33315399027cdad3a3437d575835b2ff9899",
            "title": "Logic in databases: report on the LID 2008 workshop",
            "abstract": "Th e Lo gic in Da tab ase s (LI DO0 8) w ork sh op w as h eld at th eDI S d ep artm en t of OLa S ap ien za O u n ivers ity, R om e, Italy,b etw ee n M ay 19-2 0, 2008.LI DO 08 w as estab lish ed as a foru m for b rin gin g toget h er re-se arch ers an d p rac tition ers, fro m th e acad em ia an d th e in -d u stry , wh o are fo cu sin g on all logica l asp ec ts of d ata m an -agem en t.LI DO 08 w as a con s u en ce of th ree su cce ssfu l p ast ev en ts:\u00a5 LI DO 96,an in tern ation alw ork sh op on Lo gic in Da tab ases ,wh ich LI DO0 8 d eriv es its n am e fro m ;\u00a5 IIDB O06, an intern ation al w ork sh op on In con sisten cyan d In com p let en ess in Da tab ase s;\u00a5 LA A ICO 06 , an in tern ation al w orks h op on L og ical A s-p ec ts an d A p p lication s of In teg rity Co n stra ints.In ord er to gu ara n tee its con tinu ity, a S te erin g Co m m ittee,ch aired b y G eo rg G ottlob , w as fou n d ed ; its m em b ers are A n -d rea Ca l`o, Ja n Ch om ick i, H en n in g Ch ristian sen , La ks V .S .La k sh m an an , Da v id e M artin en gh i, Din o P ed res ch i, Jef Wi -jse n , an d Ca rlo Za n iolo.Th is w orks h op w as org an ized b y A n d rea C al`o, La ks V .S .La k sh m an an , an d Da v id e M artin en gh i; it attra ct ed 18 p a-p er su b m ission s ou t of wh ich 7 w ere sele ct ed for lon g p re -se n ta tion an d 6 for sh ort p re sen tation at th e w orksh op ; th ew ork sh op attra ct ed aro u n d 50 reg istered p articip an ts. De-tails an d p res en tation s are availab le at th e w orks h op W ebsite: http://conferenze.dei.polimi.it/lid2008/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35161586",
                    "name": "A. Cal\u00ed"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "3152861",
                    "name": "D. Martinenghi"
                }
            ]
        },
        {
            "paperId": "161675ae07abfbd2ca63b5d03258dfaed11ed82f",
            "title": "Anonymizing moving objects: how to hide a MOB in a crowd?",
            "abstract": "Moving object databases (MOD) have gained much interest in recent years due to the advances in mobile communications and positioning technologies. Study of MOD can reveal useful information (e.g., traffic patterns and congestion trends) that can be used in applications for the common benefit. In order to mine and/or analyze the data, MOD must be published, which can pose a threat to the location privacy of a user. Indeed, based on prior knowledge of a user's location at several time points, an attacker can potentially associate that user to a specific moving object (MOB) in the published database and learn her position information at other time points.\n In this paper, we study the problem of privacy-preserving publishing of moving object database. Unlike in microdata, we argue that in MOD, there does not exist a fixed set of quasi-identifier (QID) attributes for all the MOBs. Consequently the anonymization groups of MOBs (i.e., the sets of other MOBs within which to hide) may not be disjoint. Thus, there may exist MOBs that can be identified explicitly by combining different anonymization groups. We illustrate the pitfalls of simple adaptations of classical k-anonymity and develop a notion which we prove is robust against privacy attacks. We propose two approaches, namely extreme-union and symmetric anonymization, to build anonymization groups that provably satisfy our proposed k-anonymity requirement, as well as yield low information loss. We ran an extensive set of experiments on large real-world and synthetic datasets of vehicular traffic. Our results demonstrate the effectiveness of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2106849",
                    "name": "Roman Yarovoy"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2108465033",
                    "name": "Wendy Hui Wang"
                }
            ]
        },
        {
            "paperId": "323c2f1ed4bd17013e30d31495ace253e4356dca",
            "title": "Getting recommender systems to think outside the box",
            "abstract": "We examine the case of over-specialization in recommender systems, which results from returning items that are too similar to those previously rated by the user. We propose Outside-The-Box (otb) recommendation, which takes some risk to help users make fresh discoveries, while maintaining high relevance. The proposed formalization relies on item regions and attempts to identify regions that are under-exposed to the user. We develop a recommendation algorithm which achieves a compromise between relevance and risk to find otb items. We evaluate this approach on the MovieLens data set and compare our otb recommendations against conventional recommendation strategies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1678458",
                    "name": "Z. Abbassi"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1749789",
                    "name": "Sergei Vassilvitskii"
                },
                {
                    "authorId": "39931037",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "393b79ca65b59146b081cbb15a3749549e65b2aa",
            "title": "On approximating optimum repairs for functional dependency violations",
            "abstract": "We study the problem of repairing an inconsistent database that violates a set of functional dependencies by making the smallest possible value modifications. For an inconsistent database, we define an optimum repair as a database that satisfies the functional dependencies, and minimizes, among all repairs, a distance measure that depends on the number of corrections made in the database and the weights of tuples modified. We show that like other versions of the repair problem, checking the existence of a repair within a certain distance of a database is NP-complete. We also show that finding a constant-factor approximation for the optimum repair for any set of functional dependencies is NP-hard. Furthermore, there is a small constant and a set of functional dependencies, for which finding an approximate solution for the optimum repair within the factor of that constant is also NP-hard. Then we present an approximation algorithm that for a fixed set of functional dependencies and an arbitrary input inconsistent database, produces a repair whose distance to the database is within a constant factor of the optimum repair distance. We finally show how the approximation algorithm can be used in data cleaning using a recent extension to functional dependencies, called conditional functional dependencies.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3058376",
                    "name": "Solmaz Kolahi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "495db3dd7a88abe99f85c2ccf7d89c4e7c4f5737",
            "title": "SocialScope: Enabling Information Discovery on Social Content Sites",
            "abstract": "Recently, many content sites have started encouraging their users to engage in social activities such as adding buddies on Yahoo! Travel and sharing articles with their friends on New York Times. This has led to the emergence of {\\em social content sites}, which is being facilitated by initiatives like OpenID (this http URL) and OpenSocial (this http URL). These community standards enable the open access to users' social profiles and connections by individual content sites and are bringing content-oriented sites and social networking sites ever closer. The integration of content and social information raises new challenges for {\\em information management and discovery} over such sites. We propose a logical architecture, named \\kw{SocialScope}, consisting of three layers, for tackling the challenges. The {\\em content management} layer is responsible for integrating, maintaining and physically accessing the content and social data. The {\\em information discovery} layer takes care of analyzing content to derive interesting new information, and interpreting and processing the user's information need to identify relevant information. Finally, the {\\em information presentation} layer explores the discovered information and helps users better understand it in a principled way. We describe the challenges in each layer and propose solutions for some of those challenges. In particular, we propose a uniform algebraic framework, which can be leveraged to uniformly and flexibly specify many of the information discovery and analysis tasks and provide the foundation for the optimization of those tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "39931037",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "538229ccc6b3cdd3f4162fe13d03791234b991e6",
            "title": "Discovering Conditional Functional Dependencies",
            "abstract": "This paper investigates the discovery of conditional functional dependencies (CFDs). CFDs are a recent extension of functional dependencies (FDs) by supporting patterns of semantically related constants, and can be used as rules for cleaning relational data. However, finding CFDs is an expensive process that involves intensive manual effort. To effectively identify data cleaning rules, we develop techniques for discovering CFDs from sample relations. We provide three methods for CFD discovery. The first, referred to as CFDMiner, is based on techniques for mining closed itemsets, and is used to discover constant CFDs, namely, CFDs with constant patterns only. The other two algorithms are developed for discovering general CFDs. The first algorithm, referred to as CTANE, is a levelwise algorithm that extends TANE, a well-known algorithm for mining FDs. The other, referred to as FastCFD, is based on the depthfirst approach used in FastFD, a method for discovering FDs. It leverages closed-itemset mining to reduce search space. Our experimental results demonstrate the following. (a) CFDMiner can be multiple orders of magnitude faster than CTANE and FastCFD for constant CFD discovery. (b) CTANE works well when a given sample relation is large, but it does not scale well with the arity of the relation. (c) FastCFD is far more efficient than CTANE when the arity of the relation is large.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "144502903",
                    "name": "W. Fan"
                },
                {
                    "authorId": "1729031",
                    "name": "Floris Geerts"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1937633",
                    "name": "Ming Xiong"
                }
            ]
        },
        {
            "paperId": "554e2d33b2f584947e856dbfbd0d881d9334cf1e",
            "title": "Recommendation Diversification Using Explanations",
            "abstract": "We introduce the novel notion of {\\em explanation-based diversification} to address the well-known problem of {\\em over-specialization} in item recommendations. Over-specialization in recommender systems leads to result sets with items that are too similar to one another, thus reducing the diversity of results and limiting user choices. Traditionally, the problem is addressed through {\\em attribute-based diversification}|grouping items in the result set that share many common attributes (e.g., genre for movies) and selecting only a limited number of items from each group. It is, however, not always applicable, especially for social content recommendations. For example, attributes may not be available as in the case of recommending URLs for users of del.icio.us. Explanation-based diversification provides a novel and complementary alternative|it leverages the {\\em reason for which a particular item is being recommended} (i.e., explanation)|for diversifying the results, without the need to access the attributes of the items. In this paper, we formally define the problem of {\\em explanation-based diversification} and, without going into the details of the actual diversification process, demonstrate its effectiveness on a real world data set, Yahoo!~Movies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39931037",
                    "name": "Cong Yu"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "7077b22d83a39b628748ef4da492bbb2ea5bc9dd",
            "title": "Battling Predictability and Overconcentration in Recommender Systems",
            "abstract": "Today\u2019s recommendation systems have evolved far beyond the initial approaches from more than a decade ago, and have seen a great deal of commercial success (c.f., Amazon and Netflix). However, they are still far from perfect, and face tremendous challenges in increasing the overall utility of the recommendations. These challenges are present in all stages of the recommendation process. They begin with the cold start problem: given a new user how do we recommend items to the user without forcing her to give feedback on a starter set of items. Similarly, given a new item introduced into the system, how do we know when (and to whom) we should recommend it. A different problem is that of data sparsity. Although every system has \u2018super\u2019 raters, that is people who rate thousands (and in some cases tens of thousands) of items, the number of such people is low. In fact, most people give feedback on only a handful of items, resulting in a large, but very sparse dataset. The data also exhibits the long tail phenomenon, with the majority of items getting just a handful of ratings. However, as recent studies have shown, understanding the tail is crucial\u2014while no item in the tail is rated by too many people, the vast majority of people rate at least a few items in the tail [8]. In this work, we focus on a different question facing recommender systems: what determines the utility of a recommendation? In the past, accuracy, usually measured on some held out dataset, has served as the gold standard; indeed this was the only measure used by the famed Netflix prize. However, we must be careful to remember that accuracy is not identical to relevancy or usefulness of a recommendation. Accuracy is the leading component\u2014if the accuracy of recommendations is low the system is not trusted by the user. However, once a recommender system achieves some accuracy bar, obtaining even higher accuracy does not necessarily make a recommendation more relevant. Indeed, recent work (e.g., see Sharma and Carenini [12]) has criticized the use of mean absolute error (MAE) as the sole measure of effectiveness of a recommender system and has proposed alternative measures inspired by decision theory. In this paper, we take a different tack and contend that once an accuracy bar (which may be based on a measure such as MAE) is achieved, diversity and freshness of recommendations play a crucial role in making a recommendation more relevant and useful to a user. Diversity One of the problems plaguing recommender systems overly focused on accuracy is overconcentration. This is the problem of recommending a set of items, in which all of the items are too similar to each other. For example, the system may learn that a user Alice has a certain penchant for fantasy books and may",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1749789",
                    "name": "Sergei Vassilvitskii"
                },
                {
                    "authorId": "1746900",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "796dc41ae2dbb31e5758cae3c62894100610c92b",
            "title": "On Efficient Recommendations for Online Exchange Markets",
            "abstract": "Presently several marketplace applications over online social networks are gaining popularity. An important class of applications is online market exchange of items. Examples include peerflix.com and readitswapit.co.uk. We model this problem as a social network where each user has two associated lists. The item list consists of items the user is willing to give away to other users. The wish list consists of items the user is interested in receiving. A transaction involves a user giving an item to another user. Users are motivated to transact in expectation of realizing their wishes. Wishes may be realized by a pair of users swapping items corresponding to each other's wishes, but more generally by means of users exchanging items through a cycle, where each user gives an item to the next user in the cycle, in accordance with the receiving user's wishes.The problem we consider is how to efficiently generate recommendations for item exchange cycles, for users in a social network. Each cycle has a value which is determined by the number of items exchanged through the cycle. We focus on the problem of generating recommendations under two models. In the deterministic model, the value of a recommendation is the total number of items exchanged through cycles. In the probabilistic model, there is a probability associated with a user transacting with another user and a user being willing to trade an item for another. The value of a recommendation then is the expected number of items exchanged. We show that under both models, the problem of determining an optimal recommendation is NP-complete and develop efficient approximation algorithms for both. We show that our algorithms have guaranteed approximation factors of 2k (for greedy), 2k \u22121 (for local search), and(2k + 1)/3 (for combination of greedy and local search) where k is the max cycle length. We also develop a so-called maximal algorithm, which does not have an approximation guarantee but is more efficient. We conduct a comprehensive set of experiments. Our experiments show that in practice, the approximation quality achieved by maximal is competitive w.r.t. that of the other algorithms. On the other hand, maximal outperforms all other algorithms on scalability w.r.t. network size.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1678458",
                    "name": "Z. Abbassi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "a25eeff91f60e7e3f4ca5bc2e8a4c693b76c7f3c",
            "title": "It takes variety to make a world: diversification in recommender systems",
            "abstract": "Recommendations in collaborative tagging sites such as del.icio.us and Yahoo! Movies, are becoming increasingly important, due to the proliferation of general queries on those sites and the ineffectiveness of the traditional search paradigm to address those queries. Regardless of the underlying recommendation strategy, item-based or user-based, one of the key concerns in producing recommendations, is over-specialization, which results in returning items that are too homogeneous. Traditional solutions rely on post-processing returned items to identify those which differ in their attribute values (e.g., genre and actors for movies). Such approaches are not always applicable when intrinsic attributes are not available (e.g., URLs in del.icio.us). In a recent paper [20], we introduced the notion of explanation-based diversity and formalized the diversification problem as a compromise between accuracy and diversity. In this paper, we develop efficient diversification algorithms built upon this notion. The algorithms explore compromises between accuracy and diversity. We demonstrate their efficiency and effectiveness in diversification on two real life data sets: del.icio.us and Yahoo! Movies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39931037",
                    "name": "Cong Yu"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                }
            ]
        },
        {
            "paperId": "a3b55a8a459202270700d0e3de60644638ec60d4",
            "title": "Privacy-Preserving Data Publishing: A Constraint-Based Clustering Approach",
            "abstract": "Privacy-preserving data publishing has drawn much research interest recently. In this chapter, we address this topic from the viewpoint of constrained clustering, i.e., the problem of finding clusters that satisfy certain user-specified constraints. More specifically, we begin with the problem of clustering under aggregate constraints (without privacy considerations) and explain how traditional algorithms for the unconstrained problem (e.g., the c-means algorithm) break down in the presence of constraints. From there, we develop scalable algorithms that overcome this problem and finally illustrate how our algorithm can also be used for privacy-preserving data publishing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1699730",
                    "name": "A. Tung"
                },
                {
                    "authorId": "145325584",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                }
            ]
        },
        {
            "paperId": "d87ab30f27dce63170d09c0078f30b3e29b6ee35",
            "title": "On domination game analysis for microeconomic data mining",
            "abstract": "Game theory is a powerful tool for analyzing the competitions among manufacturers in a market. In this article, we present a study on combining game theory and data mining by introducing the concept of domination game analysis. We present a multidimensional market model, where every dimension represents one attribute of a commodity. Every product or customer is represented by a point in the multidimensional space, and a product is said to \u201cdominate\u201d a customer if all of its attributes can satisfy the requirements of the customer. The expected market share of a product is measured by the expected number of the buyers in the customers, all of which are equally likely to buy any product dominating him. A Nash equilibrium is a configuration of the products achieving stable expected market shares for all products. We prove that Nash equilibrium in such a model can be computed in polynomial time if every manufacturer tries to modify its product in a round robin manner. To further improve the efficiency of the computation, we also design two algorithms for the manufacturers to efficiently find their best response to other products in the market.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109512262",
                    "name": "Zhenjie Zhang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1699730",
                    "name": "A. Tung"
                }
            ]
        },
        {
            "paperId": "df9459e06227f429465f3bcea3591ed13f5a0d77",
            "title": "GuruMine: A Pattern Mining System for Discovering Leaders and Tribes",
            "abstract": "In this demo we introduce GuruMine, a pattern mining system for the discovery of leaders, i.e., influential users in social networks, and their tribes, i.e., a set of users usually influenced by the same leader over several actions. GuruMine is built upon a novel pattern mining framework for leaders discovery, that we introduced in [1]. In particular, we consider social networks where users perform actions. Actions may be as simple as tagging resources (urls) as in del.icio.us, rating songs as in Yahoo! Music, or movies as in Yahoo! Movies, or users buying gadgets such as cameras, handholds, etc. and blogging a review on the gadgets. The assumption is that actions performed by a user can be seen by their network friends. Users seeing their friends actions are sometimes tempted to perform those actions. On the basis of the propagation of such influence, in [1] we provided various notion of leaders and developed algorithms for their efficient discovery. GuruMine provides users with a friendly graphical interface for selecting the actions of interest, and the kind of leaders to mine. The set of parameters driving the pattern discovery process can be iteratively refined, and the result is updated, if possible without incurring a completely new computation. Once a set of leaders has been extracted, GuruMine can easily validate them on a set of actions unseen during the pattern mining, by analyzing the portion of network reached by the influence of the selected leaders on the unseen actions. GuruMine also offers various visualizations over the social networks: the propagation of an action, the leaders, their tribes, and the interactions between different leaders and tribes. In this demo we will show: (i) how the pattern mining process can be driven towards the discovery of a good set of leaders, (ii) the ease of use of GuruMine system, and (iii) its outstanding performances on large real-world social networks and actions databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1791452",
                    "name": "Byung-Won On"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "15f182d2cd66c5aebbf64f2a2fac684613d14177",
            "title": "On disclosure risk analysis of anonymized itemsets in the presence of prior knowledge",
            "abstract": "Decision makers of companies often face the dilemma of whether to release data for knowledge discovery, vis-a-vis the risk of disclosing proprietary or sensitive information. Among the various methods employed for \u201csanitizing\u201d the data prior to disclosure, we focus in this article on anonymization, given its widespread use in practice. We do due diligence to the question \u201cjust how safe is the anonymized data?\u201d We consider both those scenarios when the hacker has no information and, more realistically, when the hacker may have partial information about items in the domain. We conduct our analyses in the context of frequent set mining and address the safety question at two different levels: (i) how likely of being cracked (i.e., re-identified by a hacker), are the identities of individual items and (ii) how likely are sets of items cracked? For capturing the prior knowledge of the hacker, we propose a belief function, which amounts to an educated guess of the frequency of each item. For various classes of belief functions which correspond to different degrees of prior knowledge, we derive formulas for computing the expected number of cracks of single items and for itemsets, the probability of cracking the itemsets. While obtaining, exact values for more general situations is computationally hard, we propose a series of heuristics called the O-estimates. They are easy to compute and are shown fairly accurate, justified by empirical results on real benchmark datasets. Based on the O-estimates, we propose a recipe for the decision makers to resolve their dilemma. Our recipe operates at two different levels, depending on whether the data owner wants to reason in terms of single items or sets of items (or both). Finally, we present techniques for ascertaining a hacker's knowledge of correlation in terms of co-occurrence of items likely. This information regarding the hacker's knowledge can be incorporated into our framework of disclosure risk analysis and we present experimental results demonstrating how this knowledge affects the heuristic estimates we have developed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "30522013",
                    "name": "G. Ramesh"
                }
            ]
        },
        {
            "paperId": "182ec2147e579af0eded206219b17c295569bebe",
            "title": "Efficient network aware search in collaborative tagging sites",
            "abstract": "The popularity of collaborative tagging sites presents a unique opportunity to explore keyword search in a context where query results are determined by the opinion of a network of taggers related to a seeker. In this paper, we present the first in-depth study of network-aware search. We investigate efficient top-k processing when the score of an answer is computed as its popularity among members of a seeker's network. We argue that obvious adaptations of top-k algorithms are too space-intensive, due to the dependence of scores on the seeker's network. We therefore develop algorithms based on maintaining score upper-bounds. The global upper-bound approach maintains a single score upper-bound for every pair of item and tag, over the entire collection of users. The resulting bounds are very coarse. We thus investigate clustering seekers based on similar behavior of their networks. We show that finding the optimal clustering of seekers is intractable, but we provide heuristic methods that give substantial time improvements. We then give an optimization that can benefit smaller populations of seekers based on clustering of taggers. Our results are supported by extensive experiments on del.icio.us datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1750856",
                    "name": "Michael Benedikt"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                }
            ]
        },
        {
            "paperId": "218085ff06a013059fad7a65cc3147579c4e0211",
            "title": "Personalizing XML Full Text Search in PIMENTO",
            "abstract": "In PIMENTO we advocate a novel approach to XML search that leverages user information \nto return more relevant query answers. This approach is based on formalizing \n{em user profiles} in terms of {em scoping rules} which are used to rewrite an input query, \nand of {em ordering rules} which are combined with query scoring to customize the ranking \nof query answers to specific users.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1791376",
                    "name": "I. Fundulaki"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "772574eeb0653b1801c152323cc1b7384b255a55",
            "title": "What-if OLAP Queries with Changing Dimensions",
            "abstract": "In a data warehouse, real-world activities can trigger changes to dimensions and their hierarchical structure. E.g., organizations can be reorganized over time causing changes to reporting structure. Product pricing changes in select markets can result in changes to bundled options in those markets. Much of the previous work on trend analysis on data warehouses has mainly focused on efficient evaluation of complex aggregations (e.g., data cube) and data-driven hypothetical scenarios. In this paper, we consider hypothetical scenarios driven by changes to dimension hierarchies and introduce the notion of perspectives. Perspectives are parameters such as time or location that drive changes in other dimensions. We demonstrate how perspectives aid in capturing a whole suite of what-if analysis queries. We propose various semantics for OLAP queries under perspectives and develop techniques for the efficient evaluation of such queries. We have implemented our techniques on the Essbase OLAP engine which fundamentally supports changing dimensions, and conducted a comprehensive set of experiments. Our results demonstrate the feasibility, scalability, and utility of our techniques for evaluating what-if queries with perspectives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2514405",
                    "name": "Alexander Russakovsky"
                },
                {
                    "authorId": "2148551",
                    "name": "Vaishnavi Sashikanth"
                }
            ]
        },
        {
            "paperId": "86aedcbfca96b7af6e5cafd3104fdc49f9fdfb66",
            "title": "Proceedings of the 2008 ACM SIGMOD international conference on Management of data",
            "abstract": "Welcome to SIGMOD 2008! We think you will find both the conference and the setting to be invigorating. The natural timeless beauty of British Columbia will provide a fitting counterpoint to the dynamism of our field in which large scale, high performance, and ever more intelligent database systems are being conceived and deployed. This dynamism is reflected in our (extreme) keynote presentations, tutorials, research papers, demonstrations, industrial papers, and product presentations. The only unfortunate side of our program is that the five parallel session structure may prevent you from hearing every talk in which you are interested. \n \nThe conference statistics give an indication of how SIGMOD's selectivity. Out of 435 submitted research papers, we accepted 78; out of 40 submitted industrial papers, we accepted 15; out of 94 demo submissions, we accepted 30; and out of 15 tutorial submissions, we accepted 5. Reviewing is an imperfect art, so we may have rejected some papers that we should have accepted, but we hope the written reviews have helped authors improve their papers for future submissions. \n \nThe main methodological innovation in SIGMOD this year has been the repeatability option. Papers submitting experiments were invited to submit code and data to enable the pioneering members of the repeatability committee to verify that the experiments worked as advertised. Any paper satisfying the repeatability criteria will include the sentence \"The results in this paper were verified by the SIGMOD repeatability committee.\" The goal is to count our field among the repeatable sciences and to pave the way for the archiving of code and data. The response to this initiative has been overwhelmingly positive and we look forward to a greater participation by all members of the community in the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "1695878",
                    "name": "D. Shasha"
                }
            ]
        },
        {
            "paperId": "944b208972152566f1c450658a7e60279e3f840c",
            "title": "Offline matching approximation algorithms in exchange markets",
            "abstract": "Motivated by several marketplace applications on rapidly growing online social networks, we study the problem of efficient offline matching algorithms for online exchange markets. We consider two main models of one-shot markets and exchange markets over time. For one-shot markets, we study three main variants of the problem: one-to-one exchange market problem, exchange market problem with short cycles, and probabilistic exchange market problem. We show that all the above problems are NP-hard, and propose heuristics and approximation algorithms for these problems. Experiments show that the number of items exchanged will increase when exchanges through cycles are allowed. Exploring algorithms for markets over time is an interesting direction for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1678458",
                    "name": "Z. Abbassi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "b69cedb77bdcc9804f8509e8659a7554a0c8011f",
            "title": "Discovering leaders from community actions",
            "abstract": "We introduce a novel frequent pattern mining approach to discover leaders and tribes in social networks. In particular, we consider social networks where users perform actions. Actions may be as simple as tagging resources (urls) as in del.icio.us, rating songs as in Yahoo! Music, or movies as in Yahoo! Movies, or users buying gadgets such as cameras, handhelds, etc. and blogging a review on the gadgets. The assumption is that actions performed by a user can be seen by their network friends. Users seeing their friends' actions are sometimes tempted to perform those actions. We are interested in the problem of studying the propagation of such \"influence\", and on this basis, identifying which users are leaders when it comes to setting the trend for performing various actions. We consider alternative definitions of leaders based on frequent patterns and develop algorithms for their efficient discovery. Our definitions are based on observing the way influence propagates in a time window, as the window is moved in time. Given a social graph and a table of user actions, our algorithms can discover leaders of various flavors by making one pass over the actions table. We run detailed experiments to evaluate the utility and scalability of our algorithms on real-life data. The results of our experiments confirm on the one hand, the efficiency of the proposed algorithm, and on the other hand, the effectiveness and relevance of the overall framework. To the best of our knowledge, this the first frequent pattern based approach to social network mining.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145784723",
                    "name": "Amit Goyal"
                },
                {
                    "authorId": "1705764",
                    "name": "F. Bonchi"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "077aa450df7d83231bb23dfcd568c56717f88691",
            "title": "Personalizing XML Search in PIMENTO",
            "abstract": "XML search is increasing in popularity as more and larger XML repositories are becoming available. The accuracy of XML search varies across different systems and a lot of effort is put into designing scoring functions tailored to specific users and datasets. We argue that there is no one scoring function that fits all and advocate incorporating user profiles into XML search to personalize query answers by accounting for user profiles. First, we propose a framework for defining user profiles and for enforcing them during query processing. Second, we adapt the well-known top-k pruning to account for user profiles. Finally, we present effectiveness and efficiency experiments which show that query personalization in XML search dramatically improves the accuracy of query results while incurring negligible processing overhead. This work is in the context of the Pimento project which aims at improving the relevance of searching structured and unstructured content.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1791376",
                    "name": "I. Fundulaki"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "20959743000e75cd959e07cd2c8dd212125bb6d5",
            "title": "Complex Group-By Queries for XML",
            "abstract": "The popularity of XML as a data exchange standard has led to the emergence of powerful XML query languages like XQuery and studies on XML query optimization. Of late, there is considerable interest in analytical processing of XML data. As pointed out by Borkar and Carey, even for data integration, there is a compelling need for performing various group-by style aggregate operations. A core operator needed for analytics is the group-by operator, which is widely used in relational as well as OLAP database applications. XQuery requires group-by operations to be simulated using nesting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3151128",
                    "name": "Chaitanya S. Gokhale"
                },
                {
                    "authorId": "48659330",
                    "name": "Nitin Gupta"
                },
                {
                    "authorId": "40366356",
                    "name": "P. Kumar"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "152167091",
                    "name": "B. Prakash"
                }
            ]
        },
        {
            "paperId": "55c3597ebe9ce20c3f2374be4dda465726335403",
            "title": "X^ 3: A Cube Operator for XML OLAP",
            "abstract": "With increasing amounts of data being exchanged and even generated or stored in XML, a natural question is how to perform OLAP on XML data, which can be structurally heterogeneous (e.g., parse trees) and/or marked-up text documents. A core operator for OLAP is the data cube. While the relational cube can be extended in a straightforward way to XML, we argue such an extension would not address the specific issues posed by XML. While in a relational warehouse, facts are flat records and dimensions may have hierarchies, in an XML warehouse, both facts and dimensions may be hierarchical. Second, XML is flexible: (a) an element may have missing or repeated subelements; (b) different instances of the same element type may have different structure. We identify the challenges introduced by these features of XML for cube definition and computation. We propose a definition for cube adapted for XML data warehouse, including a suitably generalized specification mechanism. We define a cube lattice over the aggregates so defined. We then identify properties of this cube lattice that can be leveraged to allow optimized computation of the cube. Finally, we present the results of an extensive performance evaluation experiment gauging the behavior of alternative algorithms for cube computation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3301260",
                    "name": "Nuwee Wiwatwattana"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "bb0ed68db881097278981ef6c7eed44605a5901d",
            "title": "Preservation Of Patterns and Input-Output Privacy",
            "abstract": "Privacy preserving data mining so far has mainly focused on the data collector scenario where individuals supply their personal data to an untrusted collector in exchange for value. In this scenario, random perturbation has proved to be very successful. An equally compelling, but overlooked scenario, is that of a data custodian, which either owns the data or is explicitly entrusted with ensuring privacy of individual data. In this scenario, we show that it is possible to minimize disclosure while guaranteeing no outcome change. We conduct our investigation in the context of building a decision tree and propose transformations that preserve the exact decision tree. We show with a detailed set of experiments that they provide substantial protection to both input data privacy and mining output privacy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33801213",
                    "name": "Shaofeng Bu"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "30522013",
                    "name": "G. Ramesh"
                }
            ]
        },
        {
            "paperId": "12085b227abe270aa7a4681fcaa33b57015a269a",
            "title": "Efficient secure query evaluation over encrypted XML databases",
            "abstract": "Motivated by the \"database-as-service\" paradigm wherein data owned by a client is hosted on a third-party server, there is significant interest in secure query evaluation over encrypted databases. We consider this problem for XML databases. We consider an attack model where the attacker may possess exact knowledge about the domain values and their occurrence frequencies, and we wish to protect sensitive structural information as well as value associations. We capture such security requirements using a novel notion of security constraints. For security reasons, sensitive parts of the hosted database are encrypted. There is a tension between data security and efficiency of query evaluation for different granularities of encryption. We show that finding an optimal, secure encryption scheme is NP-hard. For speeding up query processing, we propose to keep metadata, consisting of structure and value indices, on the server. We want to prevent the server, or an attacker who gains access to the server, from learning sensitive information in the database. We propose security properties for such a hosted XML database system to satisfy and prove that our proposal satisfies these properties. Intuitively, this means the attacker cannot improve his prior belief probability distribution about which candidate database led to the given encrypted database, by looking at the encrypted database as well as the metadata. We also prove that by observing a series of queries and their answers, the attacker cannot improve his prior belief probability distribution over which sensitive queries (structural or value associations) hold in the hosted database. Finally, we demonstrate with a detailed set of experiments that our techniques enable efficient query processing while satisfying the security properties defined in the paper.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108465033",
                    "name": "Wendy Hui Wang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "1d6156b17ef02f510394f78c3c57b9a1d93af2b4",
            "title": "Making Designer Schemas with Colors",
            "abstract": "XML schema design has two opposing goals: elimination of update anomalies requires that the schema be as normalized as possible; yet higher query performance and simpler query expression are often obtained through the use of schemas that permit redundancy. In this paper, we show that the recently proposed MCT data model, which extends XML by adding colors, can be used to address this dichotomy effectively. Specifically, we formalize the intuition of anomaly avoidance in MCT using notions of node normal and edge normal forms, and the goal of efficient query processing using notions of association recoverability and direct recoverability. We develop algorithms for transforming design specifications given as ER diagrams into MCT schemas that are in a node or edge normal form and satisfy association or direct recoverability. Experimental results using a wide variety of ER diagrams validate the benefits of our design methodology.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3301260",
                    "name": "Nuwee Wiwatwattana"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "259d2777b7d0a07d8a1b67e0a2201e06712172fc",
            "title": "Probabilistic privacy analysis of published views",
            "abstract": "Among techniques for ensuring privacy in data publishing, k-anonymity and publishing of views on private data are quite popular. In this paper, we consider data publishing by views and develop a probability framework for the analysis of privacy breach. We propose two attack models and derive the probability of privacy breach for each model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108465033",
                    "name": "Wendy Hui Wang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "790644a1a2dd8256a3b3ac96fb55556ef7594614",
            "title": "Expressive power of an algebra for data mining",
            "abstract": "The relational data model has simple and clear foundations on which significant theoretical and systems research has flourished. By contrast, most research on data mining has focused on algorithmic issues. A major open question is: what's an appropriate foundation for data mining, which can accommodate disparate mining tasks? We address this problem by presenting a database model and an algebra for data mining. The database model is based on the 3W-model introduced by Johnson et al. [2000]. This model relied on black box mining operators. A main contribution of this article is to open up these black boxes, by using generic operators in a data mining algebra. Two key operators in this algebra are regionize, which creates regions (or models) from data tuples, and a restricted form of looping called mining loop. Then the resulting data mining algebra MA is studied and properties concerning expressive power and complexity are established. We present results in three directions: (1) expressiveness of the mining algebra; (2) relations with alternative frameworks, and (3) interactions between regionize and mining loop.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709830",
                    "name": "T. Calders"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "1717880",
                    "name": "J. Paredaens"
                }
            ]
        },
        {
            "paperId": "a5c42aaf0691c20ae52a961ae6ed3d013c49b8e2",
            "title": "Answering tree pattern queries using views",
            "abstract": "We study the query answering using views (QAV) problem for tree pattern queries. Given a query and a view, the QAV problem is traditionally formulated in two ways: (i) find an equivalent rewriting of the query using only the view, or (ii) find a maximal contained rewriting using only the view. The former is appropriate for classical query optimization and was recently studied by Xu and Ozsoyoglu for tree pattern queries (TP). However, for information integration, we cannot rely on equivalent rewriting and must instead use maximal contained rewriting as shown by Halevy. Motivated by this, we study maximal contained rewriting for TP, a core subset of XPath, both in the absence and presence of a schema. In the absence of a schema, we show there are queries whose maximal contained rewriting (MCR) can only be expressed as the union of exponentially many TPs. We characterize the existence of a maximal contained rewriting and give a polynomial time algorithm for testing the existence of an MCR. We also give an algorithm for generating the MCR when one exists. We then consider QAV in the presence of a schema. We characterize the existence of a maximal contained rewriting when the schema contains no recursion or union types, and show that it consists of at most one TP. We give an efficient polynomial time algorithm for generating the maximal contained rewriting whenever it exists. Finally, we discuss QAV in the presence of recursive schemas.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2108465033",
                    "name": "Wendy Hui Wang"
                },
                {
                    "authorId": "144764902",
                    "name": "Zheng Zhao"
                }
            ]
        },
        {
            "paperId": "04a56dc2d5a5013390d3c3963fe9382f99003022",
            "title": "MDL Summarization with Holes",
            "abstract": "Summarization of query results is an important problem for many OLAP applications. The Minimum Description Length principle has been applied in various studies to provide summaries. In this paper, we consider a new approach of applying the MDL principle. We study the problem of finding summaries of the form S \u0398 H for k-d cubes with tree hierarchies. The S part generalizes the query results, while the H part describes all the exceptions to the generalizations. The optimization problem is to minimize the combined cardinalities of S and H. We first characterize the problem by showing that solving the 1-d problem can be done in time linear to the size of hierarchy, but solving the 2-d problem is NP-hard. We then develop three different heuristics, based on a greedy approach, a dynamic programming approach and a quadratic programming approach. We conduct a comprehensive experimental evaluation. Both the dynamic programming algorithm and the greedy algorithm can be used for different circumstances. Both produce summaries that are significantly shorter than those generated by state-of-the-art alternatives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "33801213",
                    "name": "Shaofeng Bu"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                }
            ]
        },
        {
            "paperId": "0bb969181b1e71e9c784d21df51d22e3c2a50d48",
            "title": "HePToX: Marrying XML and Heterogeneity in Your P2P Databases",
            "abstract": "We present HePToX, a full-fledged peer-to-peer database system that efficiently handles XML data heterogeneity. In a highly dynamic P2P network, it is unrealistic for a peer entering the network to be forced to agree on a global mediated schema, or to perform heavy-weight operations for mapping its schema to neighboring schemas. In our demo, we show that to enter the HePToX network a peer user is only asked to draw a simple set of visual annotations to a few other schemas. We show how the mapping rules are then automatically generated and how efficient query translation is performed on top of these mappings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1699192",
                    "name": "A. Bonifati"
                },
                {
                    "authorId": "31595920",
                    "name": "E. Chang"
                },
                {
                    "authorId": "2069429354",
                    "name": "Terence Ho"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "144876103",
                    "name": "R. Pottinger"
                }
            ]
        },
        {
            "paperId": "2c758f93ad09962d4913e82a5f0405a15c1f0992",
            "title": "To do or not to do: the dilemma of disclosing anonymized data",
            "abstract": "Decision makers of companies often face the dilemma of whether to release data for knowledge discovery, vis a vis the risk of disclosing proprietary or sensitive information. While there are various \"sanitization\" methods, in this paper we focus on anonymization, given its widespread use in practice. We give due diligence to the question of \"just how safe the anonymized data is\", in terms of protecting the true identities of the data objects. We consider both the scenarios when the hacker has no information, and more realistically, when the hacker may have partial information about items in the domain. We conduct our analyses in the context of frequent set mining. We propose to capture the prior knowledge of the hacker by means of a belief function, where an educated guess of the frequency of each item is assumed. For various classes of belief functions, which correspond to different degrees of prior knowledge, we derive formulas for computing the expected number of \"cracks\". While obtaining the exact values for the more general situations is computationally hard, we propose a heuristic called the O-estimate. It is easy to compute, and is shown to be accurate empirically with real benchmark datasets. Finally, based on the O-estimates, we propose a recipe for the decision makers to resolve their dilemma.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "30522013",
                    "name": "G. Ramesh"
                }
            ]
        },
        {
            "paperId": "34c5dd85239ec5442aeb4ef361eefd59c1c3f842",
            "title": "HepToX: Heterogeneous Peer to Peer XML Databases",
            "abstract": "study a collection of heterogeneous XML databases maintain- ing similar and related information, exchanging data via a peer to peer overlay network. In this setting, a mediated global schema is unrealistic. Yet, users/applications wish to query the d atabases via one peer using its schema. We have recently developed Hep- ToX, a P2P Heterogeneous XML database system. A key idea is that whenever a peer enters the system, it establishes an acquain- tance with a small number of peer databases, possibly with dif- ferent schema. The peer administrator provides correspondences between the local schema and the acquaintance schema using an informal and intuitive notation of arrows and boxes. We develop a novel algorithm that infers a set of precise mapping rules between the schemas from these visual annotations. We pin down a seman- tics of query translation given such mapping rules, and present a novel query translation algorithm for a simple but expressive frag- ment of XQuery, that employs the mapping rules in either direction. We show the translation algorithm is correct. Finally, we demon- strate the utility and scalability of our ideas and algorith ms with a detailed set of experiments on top of the Emulab, a large scale P2P network emulation testbed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1699192",
                    "name": "A. Bonifati"
                },
                {
                    "authorId": "31595920",
                    "name": "E. Chang"
                },
                {
                    "authorId": "2069429354",
                    "name": "Terence Ho"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "e05b9639d73b0f7ab57ee4fcfcf8263a4f479223",
            "title": "Personalizing XML Text Search in PimenT",
            "abstract": "A growing number of text-rich XML repositories are being made available. As a result, more efforts have been deployed to provide XML full-text search that combines querying structure with complex conditions on text ranging from simple keyword search to sophisticated proximity search composed with stemming and thesaurus. However, one of the key challenges in full-text search is to match users' expectations and determine the most relevant answers to a full-text query. In this context, we propose query personalization as a way to take user profiles into account in order to customize query answers based on individual users' needs.We present PIMENT, a system that enables query personalization by query rewriting and answer ranking. PIMENT is composed of a profile repository that stores user profiles, a query customizer that rewrites user queries based on user profiles and, a ranking module to rank query answers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1791376",
                    "name": "I. Fundulaki"
                },
                {
                    "authorId": "40424649",
                    "name": "Prateek Jain"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "3ac2d60f1060f487c1357fc71f13071255d1622a",
            "title": "Extracting relational data from HTML repositories",
            "abstract": "There is a vast amount of valuable information in HTML documents, widely distributed across the World Wide Web and across corporate intranets. Unfortunately, HTML is mainly presentation oriented and hard to query. In this paper, we develop a system to extract desired information (records) from thousands of HTML documents, starting from a small set of examples. Duplicates in the result are automatically detected and eliminated. We propose a novel method to estimate the current coverage of results by the system, based on capture-recapture models with unequal capture probabilities. We also propose techniques for estimating the error rate of the extracted information and an interactive the technique for enhancing information quality. To evaluate the method and ideas proposed in this paper, we conducted an extensive set of experiments. Our experimental results validate the effectiveness and utility of our system, and demonstrate interesting tradeoffs between running time of information extraction and coverage of results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109960347",
                    "name": "Ruth Yuee Zhang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1977240",
                    "name": "R. Zamar"
                }
            ]
        },
        {
            "paperId": "3c43d0c9a1d7864e7d819179940d857f8633d4ce",
            "title": "A compressed accessibility map for XML",
            "abstract": "XML is the undisputed standard for data representation and exchange. As companies transact business over the Internet, letting authorized customers directly access, and even modify, XML data offers many advantages in terms of cost, accuracy, and timeliness. Given the complex business relationships between companies, and the sensitive nature of information, access must be provided selectively, using sophisticated access control specifications. Using the specification directly to determine if a user has access to an XML data item can be extremely inefficient. The alternative of fully materializing, for each data item, the users authorized to access it can be space-inefficient. In this article, we introduce a compressed accessibility map (CAM) as a space- and time-efficient solution to the access control problem for XML data. A CAM compactly identifies the XML data items to which a user has access, by exploiting structural locality of accessibility in tree-structured data. We present a CAM lookup algorithm for determining if a user has access to a data item that takes time proportional to the product of the depth of the item in the XML data and logarithm of the CAM size. We develop an algorithm for building an optimal size CAM that takes time linear in the size of the XML data set. While optimality cannot be preserved incrementally under data item updates, we provide an algorithm for incrementally maintaining near-optimality. Finally, we experimentally demonstrate the effectiveness of the CAM for multiple users on a variety of real and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144478228",
                    "name": "Ting Yu"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                }
            ]
        },
        {
            "paperId": "7119d7634fec66c252539481ff035c470c837ae2",
            "title": "Tree logical classes for efficient evaluation of XQuery",
            "abstract": "XML is widely praised for its flexibility in allowing repeated and missing sub-elements. However, this flexibility makes it challenging to develop a bulk algebra, which typically manipulates sets of objects with identical structure. A set of XML elements, say of type book, may have members that vary greatly in structure, e.g. in the number of author sub-elements. This kind of heterogeneity may permeate the entire document in a recursive fashion: e.g., different authors of the same or different book may in turn greatly vary in structure. Even when the document conforms to a schema, the flexible nature of schemas for XML still allows such significant variations in structure among elements in a collection. Bulk processing of such heterogeneous sets is problematic.In this paper, we introduce the notion of logical classes (LC) of pattern tree nodes, and generalize the notion of pattern tree matching to handle node logical classes. This abstraction pays off significantly in allowing us to reason with an inherently heterogeneous collection of elements in a uniform, homogeneous way. Based on this, we define a Tree Logical Class (TLC) algebra that is capable of handling the heterogeneity arising in XML query processing, while avoiding redundant work. We present an algorithm to obtain a TLC algebra expression from an XQuery statement (for a large fragment of XQuery). We show how to implement the TLC algebra efficiently, introducing the nest-join as an important physical operator for XML query processing. We show that evaluation plans generated using the TLC algebra not only are simpler but also perform better than those generated by competing approaches. TLC is the algebra used in the Timber [8] system developed at the University of Michigan.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2822695",
                    "name": "Stelios Paparizos"
                },
                {
                    "authorId": "50118077",
                    "name": "Yuqing Wu"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                }
            ]
        },
        {
            "paperId": "c17a1ffc1ee5ef8a21f509202b5e5dbe13f8b96a",
            "title": "Colorful XML: one hierarchy isn't enough",
            "abstract": "XML has a tree-structured data model, which is used to uniformly represent structured as well as semi-structured data, and also enable concise query specification in XQuery, via the use of its XPath (twig) patterns. This in turn can leverage the recently developed technology of structural join algorithms to evaluate the query efficiently. In this paper, we identify a fundamental tension in XML data modeling: (i) data represented as deep trees (which can make effective use of twig patterns) are often un-normalized, leading to update anomalies, while (ii) normalized data tends to be shallow, resulting in heavy use of expensive value-based joins in queries.Our solution to this data modeling problem is a novel multi-colored trees (MCT) logical data model, which is an evolutionary extension of the XML data model, and permits trees with multi-colored nodes to signify their participation in multiple hierarchies. This adds significant semantic structure to individual data nodes. We extend XQuery expressions to navigate between structurally related nodes, taking color into account, and also to create new colored trees as restructurings of an MCT database. While MCT serves as a significant evolutionary extension to XML as a logical data model, one of the key roles of XML is for information exchange. To enable exchange of MCT information, we develop algorithms for optimally serializing an MCT database as XML. We discuss alternative physical representations for MCT databases, using relational and native XML databases, and describe an implementation on top of the Timber native XML database. Experimental evaluation, using our prototype implementation, shows that not only are MCT queries/updates more succinct and easier to express than equivalent shallow tree XML queries, but they can also be significantly more efficient to evaluate than equivalent deep and shallow tree XML queries/updates.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1789318",
                    "name": "M. Scannapieco"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "3301260",
                    "name": "Nuwee Wiwatwattana"
                }
            ]
        },
        {
            "paperId": "d3c8324fa4861531686db0cf2fa279bd313541d9",
            "title": "FleXPath: flexible structure and full-text querying for XML",
            "abstract": "Querying XML data is a well-explored topic with powerful database-style query languages such as XPath and XQuery set to become W3C standards. An equally compelling paradigm for querying XML documents is full-text search on textual content. In this paper, we study fundamental challenges that arise when we try to integrate these two querying paradigms.While keyword search is based on approximate matching, XPath has exact match semantics. We address this mismatch by considering queries on structure as a \"template\", and looking for answers that best match this template and the full-text search. To achieve this, we provide an elegant definition of relaxation on structure and define primitive operators to span the space of relaxations. Query answering is now based on ranking potential answers on structural and full-text search conditions. We set out certain desirable principles for ranking schemes and propose natural ranking schemes that adhere to these principles. We develop efficient algorithms for answering top-K queries and discuss results from a comprehensive set of experiments that demonstrate the utility and scalability of the proposed framework and algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "40652387",
                    "name": "Shashank Pandit"
                }
            ]
        },
        {
            "paperId": "0e6c65b8c366d6f925b8221e01aceddbf931ef19",
            "title": "TIMBER: a native system for querying XML",
            "abstract": "XML has become ubiquitous, and XML data has to be managed in databases. The current industry standard is to map XML data into relational tables and store this information in a relational database. Such mappings create both expressive power problems and performance problems.In the TIMBER [7] project we are exploring the issues involved in storing XML in native format. We believe that the key intellectual contribution of this system is a comprehensive set-at-a-time query processing ability in a native XML store, with all the standard components of relational query processing, including algebraic rewriting and a cost-based optimizer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2822695",
                    "name": "Stelios Paparizos"
                },
                {
                    "authorId": "51461409",
                    "name": "S. Al-Khalifa"
                },
                {
                    "authorId": "144030084",
                    "name": "Adriane P. Chapman"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2183125",
                    "name": "Andrew Nierman"
                },
                {
                    "authorId": "49111633",
                    "name": "J. Patel"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "3301260",
                    "name": "Nuwee Wiwatwattana"
                },
                {
                    "authorId": "50118077",
                    "name": "Yuqing Wu"
                },
                {
                    "authorId": "71884697",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "2ad85ec8ac6a37ab4111bf9354300e8579f86f49",
            "title": "LockX: a system for efficiently querying secure XML",
            "abstract": "1. MOTIVATION Companies are using the Web for information dissemination, sparking interest in models and efficient mechanisms for controlled access to information. In this context, securing XML documents is important. Much of the work on XML access control to date has studied models for the specification of XML access control policies, focusing on issues such as granularity of access and conflict resolution. However, there has been little work on enforcement of access control policies for queries. A naive two-step solution to secure query evaluation is to first compute query results, and then use access control policies to filter the results. Consider the XML database of an online-seller, which has information on books and customers. Assume that a specific user is allowed access to books and not to customer information. If only query results are filtered for accessibility, the XPath query:",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2690683",
                    "name": "SungRan Cho"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "3523791df44ca600ca6b6801a9d6850ab6666e89",
            "title": "Online Mining of Changes from Data Streams: Research Problems and Preliminary Results",
            "abstract": "As data streams are gaining prominence in a growing number of emerging applications, advanced analysis and mining of data streams is becoming increasingly important. While there are some recent studies on mining data streams, we would like to ask the following essential question: What are the distinct features of mining data streams compared to mining other kinds of data? In this paper, we take the following position: online mining of the changes in data streams is one of the core issues. We propose some interesting research problems and highlight the inherent challenges. Moreover, we sketch some preliminary results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2147292665",
                    "name": "Guozhu Dong"
                },
                {
                    "authorId": "145325584",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145525190",
                    "name": "J. Pei"
                },
                {
                    "authorId": "2109590665",
                    "name": "Haixun Wang"
                },
                {
                    "authorId": "144019071",
                    "name": "Philip S. Yu"
                }
            ]
        },
        {
            "paperId": "3c40c424d7be2fadd8253bca7234e6cfadc6709f",
            "title": "Mining unexpected rules by pushing user dynamics",
            "abstract": "Unexpected rules are interesting because they are either previously unknown or deviate from what prior user knowledge would suggest. In this paper, we study three important issues that have been previously ignored in mining unexpected rules. First, the unexpectedness of a rule depends on how the user prefers to apply the prior knowledge to a given scenario, in addition to the knowledge itself. Second, the prior knowledge should be considered right from the start to focus the search on unexpected rules. Third, the unexpectedness of a rule depends on what other rules the user has seen so far. Thus, only rules that remain unexpected given what the user has seen should be considered interesting. We develop an approach that addresses all three problems above and evaluate it by means of experiments focusing on finding interesting rules.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1751643",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "2493239",
                    "name": "Yuelong Jiang"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "5e11d615566ab80d6ae445305643e6ff8e745d2f",
            "title": "QC-trees: an efficient summary structure for semantic OLAP",
            "abstract": "Recently, a technique called quotient cube was proposed as a summary structure for a data cube that preserves its semantics, with applications for online exploration and visualization. The authors showed that a quotient cube can be constructed very efficiently and it leads to a significant reduction in the cube size. While it is an interesting proposal, that paper leaves many issues unaddressed. Firstly, a direct representation of a quotient cube is not as compact as possible and thus still wastes space. Secondly, while a quotient cube can in principle be used for answering queries, no specific algorithms were given in the paper. Thirdly, maintaining any summary structure incrementally against updates is an important task, a topic not addressed there. In this paper, we propose an efficient data structure called QC-tree and an efficient algorithm for directly constructing it from a base table, solving the first problem. We give efficient algorithms that address the remaining questions. We report results from an extensive performance study that illustrate the space and time savings achieved by our algorithms over previous ones (wherever they exist).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145525190",
                    "name": "J. Pei"
                },
                {
                    "authorId": "2119326694",
                    "name": "Yan Zhao"
                }
            ]
        },
        {
            "paperId": "63ff96a0e32a3f7ca1ff5fc4a55976b304a2cca7",
            "title": "Efficient dynamic mining of constrained frequent sets",
            "abstract": "Data mining is supposed to be an iterative and exploratory process. In this context, we are working on a project with the overall objective of developing a practical computing environment for the human-centered exploratory mining of frequent sets. One critical component of such an environment is the support for the dynamic mining of constrained frequent sets of items. Constraints enable users to impose a certain focus on the mining process; dynamic means that, in the middle of the computation, users are able to (i) change (such as tighten or relax) the constraints and/or (ii) change the minimum support threshold, thus having a decisive influence on subsequent computations. In a real-life situation, the available buffer space may be limited, thus adding another complication to the problem.In this article, we develop an algorithm, called DCF, for Dynamic Constrained Frequent-set computation. This algorithm is enhanced with a few optimizations, exploiting a lightweight structure called a segment support map. It enables DCF to (i) obtain sharper bounds on the support of sets of items, and to (ii) better exploit properties of constraints. Furthermore, when handling dynamic changes to constraints, DCF relies on the concept of a delta member generating function, which generates precisely the sets of items that satisfy the new but not the old constraints. Our experimental results show the effectiveness of these enhancements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1726081",
                    "name": "C. Leung"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                }
            ]
        },
        {
            "paperId": "9c3720af3680ccf95e6408420b6b3a8fcc34660d",
            "title": "XML Interoperability",
            "abstract": "We study the problem of interoperability among XML data sources. We propose a lightweight infrastructure for this purpose that derives its inspiration from the recent semantic web initiative. Our approach does not require either source-to-source or source-to-global mappings. Instead, it is based on enriching local sources with semantic declarations so as to enable interoperability. These declarations expose the semantics of the information content of sources by mapping the concepts present therein to a common (application specific) vocabulary, in the spirit of RDF. In addition to this infrastructure, we discuss tools that may assist in generating semantic declarations, and formulation of global queries and address some interesting issues in query processing and optimization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1713262",
                    "name": "F. Sadri"
                }
            ]
        },
        {
            "paperId": "ce5bdc111b72e4b593df109a3ca03fd35a608064",
            "title": "Information Integration and the Semantic Web",
            "abstract": "Information integration and interoperability among information sources are related problems that have received significant attention since early days of computer information processing. Initially, for a few decades, the focus was on integration/interoperability for a relatively small number of sources. This is the setting encountered in traditional business and service applications, for example when two companies merge or several services interoperate (which requires the integration of their information systems). Much of the work in this context of federated or multi-databases focused on integrating schemas by defining a global schema in an expressive data model and defining mappings from local schemas to the global one [19]. More recently, in the context of integration of data sources on the internet, the so-called global-as-view (GAV) and local-as-view (LAV) paradigms have emerged out of projects such as TSIMMIS [20] and Information Manifold (IM) [12]. Recently, the advent of XML as a standard for online data interchange holds much promise toward promoting interoperability and data integration. The focus has also shifted to one of providing integration and interoperability among a large number of independent and autonomous information sources. But XML, being a syntactic model, in itself cannot make interoperability happen automatically. Two main challenges to overcome are: (i) data sources may model XML data in heterogeneous ways, e.g., using different nestings or groupings or interchanging elements and attributes, and (ii) sources may employ different terminology, a classic problem even in multi-database interoperability. While earlier approaches to integration can be extended to handle XML, they suffer from the significant overhead of having to design a commonly agreed upon global schema. Can we interoperate without this overhead? Indeed, there are a few recent proposals that do overcome the need for a global schema \u2013 Halevy et al. [9] and Miller et al. [15]. In a nutshell, both of these approaches rely on source to source mappings. One problem here is that requiring such mappings for all pairs is too tedious and cumbersome since the mappings, even if generated semi-automatically, still require significant manual intervention to create. In order to mitigate this, one can merely insist, as [9] does, that the graph of pairs with available mappings be connected. A second problem is that when a source si is mapped to source sj , if sj does not have some of the concepts present in si, then they will be lost. E.g., si may include the ISBN for all its books while sj may not. Independently of all this, recently there has been much excitement around the Semantic Web [18] initiative, coming as it does with its own host of technologies such as resource description framework (RDF) [16] and ontology description languages such as DAML+OIL and OWL [5, 14]. The promise of the Semantic Web is to expose the semantics of the information content of web resources (including text, audio, video, etc.) thus taking the web to a higher semantic level, enabling easy exchange of data and applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1713262",
                    "name": "F. Sadri"
                }
            ]
        },
        {
            "paperId": "b5c405718cc897c0226f78e3c268e4942a5bcae1",
            "title": "Efficient OLAP query processing in distributed data warehouses",
            "abstract": "The success of Internet applications has led to an explosive growth in the demand for bandwidth from ISPs. Managing an IP network includes complex data analysis that can often be expressed as OLAP queries. Current day OLAP tools assume the availability of the detailed data in a centralized warehouse. However, the inherently distributed nature of the data collection (e.g., flow-level traffic statistics are gathered at network routers) and the huge amount of data extracted at each collection point (of the order of several gigabytes per day for large IP networks) makes such an approach highly impractical. The natural solution to this problem is to maintain a distributed data warehouse, consisting of multiple local data warehouses (sites) adjacent to the collection points, together with a coordinator. In order for such a solution to make sense, we need a technology for distributed processing of complex OLAP queries. We have developed the Skalla system for this task. We conducted an experimental study of the Skalla evaluation scheme using TPC(R) data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2477303",
                    "name": "M. Akinde"
                },
                {
                    "authorId": "2272920683",
                    "name": "Michael H. B\u00f6hlen"
                },
                {
                    "authorId": "145633895",
                    "name": "T. Johnson"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "d801c51b4c2379d7a32856716ccf9cb2c7b8e449",
            "title": "Exploiting succinct constraints using FP-trees",
            "abstract": "Since its introduction, frequent-set mining has been generalized to many forms, which include constrained data mining. The use of constraints permits user focus and guidance, enables user exploration and control, and leads to effective pruning of the search space and efficient mining of frequent itemsets. In this paper, we focus on the use of succinct constraints. In particular, we propose a novel algorithm called FPS to mine frequent itemsets satisfying succinct constraints. The FPS algorithm avoids the generate-and-test paradigm by exploiting succinctness properties of the constraints in a FP-tree based framework. In terms of functionality, our algorithm is capable of handling not just the succinct aggregate constraint, but any succinct constraint in general. Moreover, it handles multiple succinct constraints. In terms of performance, our algorithm is more efficient and effective than existing FP-tree based constrained frequent-set mining algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1726081",
                    "name": "C. Leung"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                }
            ]
        },
        {
            "paperId": "4e852e4a5e4a54c5947c227682729c9902b7b360",
            "title": "Reminiscences on influential papers",
            "abstract": "First, together with Agrawal, Imielinski, and Swami's SIGMOD'93 paper: \"Mining Association Rules between Sets of Items in Large Databases,\" it identifies a new and important task in data mining: association rule mining, i.e., finding frequent patterns or itemsets (sets of items) that occur frequently together in large databases. This has proven truly useful for frequent pattern or association mining, dependency or correlation analysis, etc., with many applications. Some following studies have shown that it is also usefifl for associatiombased classification, sequential or structured pattern analysis, constraint-based mining, cluster analysis, semantic data compression, data cube computation, and so on. Identification of a crucial research problem itself makes the paper distinct from many others.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144812095",
                    "name": "K. A. Ross"
                },
                {
                    "authorId": "2065116446",
                    "name": "James R. Hamilton"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "143670300",
                    "name": "L. Wong"
                }
            ]
        },
        {
            "paperId": "6cdb3e2e43ff5be37cf053fb018b9d4fa30e2a77",
            "title": "Minimization of tree pattern queries",
            "abstract": "Tree patterns forms a natural basis to query tree-structured data such as XML and LDAP. Since the efficiency of tree pattern matching against a tree-structured database depends on the size of the pattern, it is essential to identify and eliminate redundant nodes in the pattern and do so as quickly as possible. In this paper, we study tree pattern minimization both in the absence and in the presence of integrity constraints (ICs) on the underlying tree-structured database.\nWhen no ICs are considered, we call the process of minimizing a tree pattern, constraint-independent minimization. We develop a polynomial time algorithm called CIM for this purpose. CIM's efficiency stems from two key properties: (i) a node cannot be redundant unless its children are, and (ii) the order of elimination of redundant nodes is immaterial. When ICs are considered for minimization, we refer to it as constraint-dependent minimization. For tree-structured databases, required child/descendant and type co-occurrence ICs are very natural. Under such ICs, we show that the minimal equivalent query is unique. We show the surprising result that the algorithm obtained by first augmenting the tree pattern using ICS, and then applying CIM, always finds the unique minimal equivalent query; we refer to this algorithm as ACIM. While ACIM is also polynomial time, it can be expensive in practice because of its inherent non-locality. We then present a fast algorithm, CDM, that identifies and eliminates local redundancies due to ICs, based on propagating \u201cinformation labels\u201d up the tree pattern. CDM can be applied prior to ACIM for improving the minimization efficiency. We complement our analytical results with an experimental study that shows the effectiveness of our tree pattern minimization techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2690683",
                    "name": "SungRan Cho"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "94c5d00ff8600ab57432c965db1b9c140b1c2d99",
            "title": "On dual mining: from patterns to circumstances, and back",
            "abstract": "Previous work on frequent item set mining has focused on finding all itemsets that are frequent in a specified part of a database. We motivate the dual question of finding under what circumstances a given item set satisfies a pattern of interest (e.g., frequency) in a database. Circumstances form a lattice that generalizes the instance lattice associated with datacube. Exploiting this, we adapt known cube algorithms and propose our own, minCirc, for mining the strongest (e.g., minimal) circumstances under which an itemset satisfies a pattern. Our experiments show that minCirc is competitive with the adapted algorithms. We motivate mining queries involving migration between item set and circumstance lattices and propose the notion of Armstrong Basis as a structure that provides efficient support for such migration queries, as well as a simple algorithm for computing it.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1788571",
                    "name": "G. Grahne"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2109074603",
                    "name": "Xiaohong Wang"
                },
                {
                    "authorId": "2112817823",
                    "name": "M. Xie"
                }
            ]
        },
        {
            "paperId": "b3054581f46943752e4bc0ab0f1772929f158795",
            "title": "SchemaSQL: An extension to SQL for multidatabase interoperability",
            "abstract": "We provide a principled extension of SQL, called SchemaSQL, that offers the capability of uniform manipulation of data and schema in relational multidatabase systems. We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavor of SQL while supporting querying of both data and schema. (2) It can be used to transform data in a database in a structure substantially different from original database, in which data and schema may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits \"horizontal\" aggregation and even aggregation over more general \"blocks\" of information. (5) SchemaSQL provides a useful facility for interoperability and data/schema manipulation in relational multidatabase systems. We provide many examples to illustrate our claims. We clearly spell out the formal semantics of SchemaSQL that accounts for all these features. We describe an architecture for the implementation of SchemaSQL and develop implementation algorithms based on available database technology that allows for powerful integration of SQL based relational DBMS. We also discuss the applicability of SchemaSQL for handling semantic heterogeneity arising in a multidatabase system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1713262",
                    "name": "F. Sadri"
                },
                {
                    "authorId": "2026217",
                    "name": "Iyer N. Subramanian"
                }
            ]
        },
        {
            "paperId": "e563de7cb164dcfc624b1aa0373d2698f3e40ab5",
            "title": "Mining frequent itemsets with convertible constraints",
            "abstract": "Recent work has highlighted the importance of the constraint based mining paradigm in the context of frequent itemsets, associations, correlations, sequential patterns, and many other interesting patterns in large databases. The authors study constraints which cannot be handled with existing theory and techniques. For example, avg(S) /spl theta/ /spl nu/, median(S) /spl theta/ /spl nu/, sum(S) /spl theta/ /spl nu/ (S can contain items of arbitrary values) (/spl theta//spl isin/{/spl ges/, /spl les/}), are customarily regarded as \"tough\" constraints in that they cannot be pushed inside an algorithm such as a priori. We develop a notion of convertible constraints and systematically analyze, classify, and characterize this class. We also develop techniques which enable them to be readily pushed deep inside the recently developed FP-growth algorithm for frequent itemset mining. Results from our detailed experiments show the effectiveness of the techniques developed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145525190",
                    "name": "J. Pei"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "354e6ff1e18a55ba9335fb4b4c20ff544061cae3",
            "title": "Geo-Spatial Clustering with User-Specified Constraints",
            "abstract": "Capturing application semantics and allowing a human analyst to express his focus in mining have been the motivation for several recent studies on constrained mining. In this paper, we introduce and study the problem of constrained clustering\u2014finding clusters that satisfy certain user-specified constraints. We argue that this problem arises naturally in practice. Two types of constraints are discussed in this paper. The first type of constraints are imposed by physical obstacles that exist in the region of clustering. The second type of constraints are SQL constraints which every cluster must satisfy. We provide a prelimary introduction to both types of constraints and discuss some techniques for solving them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1699730",
                    "name": "A. Tung"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145325584",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "3ddd481614d297de417a648766ba654c435e8d5a",
            "title": "The 3W Model and Algebra for Unified Data Mining",
            "abstract": "Real data mining/analysis applications call for a framework which adequately supports knowledge discovery as a multi-step process, where the input of one mining operation can be the output of another. Previous studies, primarily focusing on fast computation of one speci c mining task at a time, ignore this vital issue. Motivated by this observation, we develop a unied model supporting all major mining and analysis tasks. Our model consists of three distinct worlds, corresponding to intensional and extensional dimensions, and to data sets. The notion of dimension is a centerpiece of the model. Equipped with hierarchies, dimensions integrate the output of seemingly dissimilar mining and analysis operations in a clean manner. We propose an algebra, called the dimension algebra, for manipulating (intensional) dimensions, as well as operators that serve as \\bridges\" between the worlds. We demonstrate by examples that several real data mining processes can be captured using our model and algebra. We demonstrate the naturality of the algebra by establishing several identities. Finally, we discuss e cient implementation of the proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145633895",
                    "name": "T. Johnson"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                }
            ]
        },
        {
            "paperId": "7371904c5136a1227ed8d62372f2bb0a084f08c4",
            "title": "Efficient mining of constrained correlated sets",
            "abstract": "Studies the problem of efficiently computing correlated item sets satisfying given constraints. We call them valid correlated item sets. It turns out that constraints can have subtle interactions with correlated item sets, depending on their underlying properties. We show that, in general, the set of minimal valid correlated item sets does not coincide with that of minimal correlated item sets that are valid, and we characterize classes of constraints for which these sets coincide. We delineate the meaning of these two spaces and give algorithms for computing them. We also give an analytical evaluation of their performance and validate our analysis with a detailed experimental evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1788571",
                    "name": "G. Grahne"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2109074603",
                    "name": "Xiaohong Wang"
                }
            ]
        },
        {
            "paperId": "8fa441e6b77c9d119e38f2e41c8a8a014fc89570",
            "title": "The segment support map: scalable mining of frequent itemsets",
            "abstract": "Since its introduction, frequent set mining has been generalized to many forms, including online mining with Carma, and constrained mining with CAP. Regardless, scalability is always an important aspect of the development. In this paper, we propose a novel structure called segment support map to help mining of frequent itemsets of the various forms. A light-weight structure, the segment support map improves the performance of frequent-set mining algorithms by: (i) obtaining sharper bounds on the support of itemsets, and/or (ii) better exploiting properties of constraints. Our experimental results show the effectiveness of the segment support map.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1726081",
                    "name": "C. Leung"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                }
            ]
        },
        {
            "paperId": "11d53f76ded766c77d65b01f97bf0038972746da",
            "title": "Interactive Mining of Correlations - A Constraint Perspective",
            "abstract": "The problem of nding minimal sets of objects, that are correlated and have statistically signiicant number of occurrences in a database, has recently received considerable attention, as an alternative to association rules. A case for making mining human-centered, interactive, and exploratory via application speciic constraints was recently made in the papers 14, 12]. The technical results of these papers were established in the context of eecient computation of frequent sets satisfying user speciied constraints. In this preliminary report, we examine the problem of eecient computation of correlated sets satisfying given constraints. We discuss three algorithms for this purpose and report on the results of their experimental evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1788571",
                    "name": "G. Grahne"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2109074603",
                    "name": "Xiaohong Wang"
                }
            ]
        },
        {
            "paperId": "20dd934a841f775ed18e0d2f8e887f363cbb9099",
            "title": "Querying network directories",
            "abstract": "Heirarchically structured directories have recently proliferated with the growth of the Internet, and are being used to store not only address books and contact information for people, but also personal profiles, network resource information, and network and service policies. These systems provide a means for managing scale and heterogeneity, while allowing for conceptual unity and autonomy across multiple directory servers in the network, in a way for superior to what conventional relational or object-oriented databases offer. Yet, in deployed systems today, much of the data is modeled in an ad hoc manner, and many of the more sophisticated \u201cqueries\u201d involve navigational access.\nIn this paper, we develop the core of a formal data model for network directories, and propose a sequence of efficiently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the efficiency of each algorithm in terms of its I/O complexity. Our data model and query languages share the flexibility and utility of the recent proposals for semi-structured data models, while at the same time effectively addressing the specific needs of network directory applications, which we demonstrate by means of a representative real-life example.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1778524",
                    "name": "D. Vista"
                }
            ]
        },
        {
            "paperId": "2185c5944d3da1dd763acece6be2fefd6a232ea5",
            "title": "Constraint-Based Multidimensional Data Mining",
            "abstract": "Although many data-mining methodologies and systems have been developed in recent years, the authors contend that by and large, present mining models lack human involvement, particularly in the form of guidance and user control. They believe that data mining is most effective when the computer does what it does best-like searching large databases or counting-and users do what they do best, like specifying the current mining session's focus. This division of labor is best achieved through constraint-based mining, in which the user provides restraints that guide a search. Mining can also be improved by employing a multidimensional, hierarchical view of the data. Current data warehouse systems have provided a fertile ground for systematic development of this multidimensional mining. Together, constraint-based and multidimensional techniques can provide a more ad hoc, query-driven process that effectively exploits the semantics of data than those supported by current standalone data-mining systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                }
            ]
        },
        {
            "paperId": "275621632943d88b5403b33c440a9b944819a903",
            "title": "Snakes and sandwiches: optimal clustering strategies for a data warehouse",
            "abstract": "Physical layout of data is a crucial determinant of performance in a data warehouse. The optimal clustering of data on disk, for minimizing expected I/O, depends on the query workload. In practice, we often have a reasonable sense of the likelihood of different classes of queries, e.g., 40% of the queries concern calls made from some specific telephone number in some month. In this paper, we address the problem of finding an optimal clustering of records of a fact table on disk, given an expected workload in the form of a probability distribution over query classes.\nAttributes in a data warehouse fact table typically have hierarchies defined on them (by means of auxiliary dimension tables). The product of the dimensional hierarchy levels forms a lattice and leads to a natural notion of query classes. Optimal clustering in this context is a combinatorially explosive problem with a huge search space (doubly exponential in number of hierarchy levels). We identify an important subclass of clustering strategies called lattice paths, and present a dynamic programming algorithm for finding the optimal lattice path clustering, in time linear in the lattice size. We additionally propose a technique called snaking, which when applied to a lattice path, always reduces its cost. For a representative class of star schemas, we show that for every workload, there is a snaked lattice path which is globally optimal. Further, we prove that the clustering obtained by applying snaking to the optimal lattice path is never much worse than the globally optimal snaked lattice path clustering. We complement our analyses and validate the practical utility of our techniques with experiments using TPC-D benchmark data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "3c7da5d87776db54d04978700f01f3c41156d3e2",
            "title": "What can Hierarchies do for Data Warehouses?",
            "abstract": "Data in a warehouse typically has multiple dimensions of interest, such as location, time, and product. It is well-recognized that these dimensions have hierarchies deened on them, such as \\store-city-state-region\" for location. The standard way to model such data is with a star/snowwake schema. However, current approaches do not give a rst-class status to dimensions. Consequently, a substantial class of interesting queries involving dimension hierarchies and their interaction with the fact tables are quite verbose to write, hard to read, and diicult to optimize. We propose the SQL(H) model and a natural extension to the SQL query language, that gives a rst-class status to dimensions, and we pin down its semantics. Our model permits structural and schematic heterogeneity in dimension hierarchies, situations often arising in practice that cannot be modeled satisfactorily using the star/snowwake approach. We show using examples that sophisticated queries involving dimension hierarchies and their interplay with aggregation can be expressed concisely in SQL(H). By comparison, expressing such queries in SQL would involve a union of numerous complex sequences of joins. Finally, we develop an eecient implementation strategy for computing SQL queries, based on an algorithm for hierarchical joins, and the use of dimension indexes. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "431911fb9bd8ff56b64054bb15f1bc682d363e7c",
            "title": "Exploratory mining via constrained frequent set queries",
            "abstract": "Although there have been many studies on data mining, to date there have been few research prototypes or commercial systems supporting comprehensive query-driven mining, which encourages interactive exploration of the data. Our thesis is that constraint constructs and the optimization they induce play a pivotal role in mining queries, thus substantially enhancing the usefulness and performance of the mining system. This is based on the analogy of declarative query languages like SQL and query optimization which have made relational databases so successful. To this end, our proposed demo is not yet another data mining system, but of a new paradigm in data mining - mining with constraints, as the important first step towards supporting ad-hoc mining in DBMS.\nIn this demo, we will show a prototype exploratory mining system that implements constraint-based mining query optimization methods proposed in [5]. We will demonstrate how a user can interact with the system for exploratory data mining and how efficiently the system may execute optimized data mining queries. The prototype system will include all the constraint pushing techniques for mining association rules outlined in [5], and will include additional capabilities for mining other kinds of rules for which the computation of constrained frequent sets forms the core first step.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145325584",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "47183752",
                    "name": "T. Mah"
                }
            ]
        },
        {
            "paperId": "55dc78fefa9ebb3059b11c5bf37591d18bb6d712",
            "title": "Optimization of constrained frequent set queries with 2-variable constraints",
            "abstract": "Currently, there is tremendous interest in providing ad-hoc mining capabilities in database management systems. As a first step towards this goal, in [15] we proposed an architecture for supporting constraint-based, human-centered, exploratory mining of various kinds of rules including associations, introduced the notion of constrained frequent set queries (CFQs), and developed effective pruning optimizations for CFQs with 1-variable (1-var) constraints.\nWhile 1-var constraints are useful for constraining the antecedent and consequent separately, many natural examples of CFQs illustrate the need for constraining the antecedent and consequent jointly, for which 2-variable (2-var) constraints are indispensable. Developing pruning optimizations for CFQs with 2-var constraints is the subject of this paper. But this is a difficult problem because: (i) in 2-var constraints, both variables keep changing and, unlike 1-var constraints, there is no fixed target for pruning; (ii) as we show, \u201cconventional\u201d monotonicity-based optimization techniques do not apply effectively to 2-var constraints.\nThe contributions are as follows. (1) We introduce a notion of quasi-succinctness, which allows a quasi-succinct 2-var constraint to be reduced to two succinct 1-var constraints for pruning. (2) We characterize the class of 2-var constraints that are quasi-succinct. (3) We develop heuristic techniques for non-quasi-succinct constraints. Experimental results show the effectiveness of all our techniques. (4) We propose a query optimizer for CFQs and show that for a large class of constraints, the computation strategy generated by the optimizer is ccc-optimal, i.e., minimizing the effort incurred w.r.t. constraint checking and support counting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "145325584",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "2064270136",
                    "name": "Alex T. Pang"
                }
            ]
        },
        {
            "paperId": "939d608664c002fa96d7d171d466015c0f73e261",
            "title": "On Efficiently Implementing SchemaSQL on an SQL Database System",
            "abstract": "SchemaSQL is a recently proposed extension to SQL for enabling multi-database interoperability. Several recently identi(cid:12)ed applications for SchemaSQL, however, mainly rely on its ability to treat data and schema labels in a uniform manner, and call for an e(cid:14)cient implementation of it on a single RDBMS. We (cid:12)rst develop a logical algebra for SchemaSQL by combining classical relational algebra with four restructuring operators { unfold, fold, split, and unite { originally introduced in the context of the tabular data model by Gyssens et al. [GLS96], and suitably adapted to (cid:12)t the needs of SchemaSQL. We give an algorithm for translating SchemaSQL queries/views involving restructuring, into the logical algebra above. We also provide physical algebraic operators which are useful for query optimization. Using the various operators as a vehicle, we give several alternate implementation strategies for SchemaSQL queries/views. All the proposed strategies can be implemented non-intrusively on top of existing relational DBMS, in that they do not require any additions to the existing set of plan operators. We conducted a series of performance experiments based on TPC-D benchmark data, us-ing the IBM DB2 DBMS running on Windows/NT. In addition to showing the relative tradeo(cid:11)s between various alternate strategies, our experiments show the feasibility of implementing SchemaSQL on top of",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1713262",
                    "name": "F. Sadri"
                },
                {
                    "authorId": "2026217",
                    "name": "Iyer N. Subramanian"
                }
            ]
        },
        {
            "paperId": "9f7e42a89a188d478aac17924cfdc939ff394971",
            "title": "Towards a Toolkit for Data Analysis and Mining",
            "abstract": "Data analysis and mining is typically a multistep process, involving an iterated process of data space partitioning, aggregate computation, and data transformations. These activities typically require steps that cross disciplines, software packages, and le formats. In this paper, we articulate the vision behind some of our ongoing work of developing a data and computational model that relies on a single concept { data space partition { to unify many aspects of data mining and analysis. The uniied model would simplify the task of the data analyst, and would present opportunities for query optimization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145633895",
                    "name": "T. Johnson"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "af5a145735128e1aa4782f8eb8981ff5e3292b2a",
            "title": "Revisiting the Hierarchical Data Model",
            "abstract": "SUMMARY Much of the data we deal with every day is organized hierarchically: file systems, library classification schemes and yellow page categories are salient examples. Business data too, benefits from a hierarchical organization, and indeed the hierarchical data model was quite prevalent thirty years ago. Due to the recently increased importance of X.500/LDAP directories, which are hierarchical, and the prevalence of aggregation hierarchies in datacubes, there is now renewed interest in the hierarchical organization of data. In this paper, we develop a framework for a modern hierarchical data model, substantially improved from the original version by taking advantage of the lessons learned in the relational database context. We argue that this new hierarchical data model has many benefits with respect",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2094211307",
                    "name": "Nonmembers"
                }
            ]
        },
        {
            "paperId": "bb1213cbfcc41f342c4d96ecf7d2237995207d3e",
            "title": "Interestingness and Pruning of Mined Patterns",
            "abstract": "We study the following question: when can a mined pattern, which may be an association, a correlation, ratio rule, or any other, be regarded as interesting? Previous approaches to answering this question have been largely numeric. Speciically, we show that the presence of some rules may make others redundant, and therefore uninteresting. We articulate these principles and formalize them in the form of pruning rules. Pruning rules, when applied to a collection of mined patterns, can be used to eliminate redundant ones. As a concrete instance, we applied our pruning rules on association rules/positive association rules derived from a census database, and demonstrate that signiicant pruning results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145081804",
                    "name": "Devavrat Shah"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1704729",
                    "name": "K. Ramamritham"
                },
                {
                    "authorId": "143761034",
                    "name": "S. Sudarshan"
                }
            ]
        },
        {
            "paperId": "c777150ab228c2fca9149eff704dbd190b30cbc8",
            "title": "Hierarchical or relational? A case for a modern hierarchical data model",
            "abstract": "Much of the data we deal with every day is organized hierarchically: file systems, library classification schemes and yellow page categories are salient examples. Business data too, benefits from a hierarchical organization, and indeed the hierarchical data model was quite prevalent thirty years ago. Due to the recently increased importance of X.500/LDAP directories, which are hierarchical, and the prevalence of aggregation hierarchies in datacubes, there is now renewed interest in the hierarchical organization of data. We develop a framework for a modern hierarchical data model, substantially improved from the original version by taking advantage of the lessons learned in the relational database context. We argue that this new hierarchical data model has many benefits with respect to the ubiquitous flat relational data model. We argue also that this model is well-suited for representing XML, and for interchange of information across heterogeneous databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "247c247cffe712ff3219983d3ccb42699130fff0",
            "title": "Exploratory mining and pruning optimizations of constrained associations rules",
            "abstract": "From the standpoint of supporting human-centered discovery of knowledge, the present-day model of mining association rules suffers from the following serious shortcomings: (i) lack of user exploration and control, (ii) lack of focus, and (iii) rigid notion of relationships. In effect, this model functions as a black-box, admitting little user interaction in between. We propose, in this paper, an architecture that opens up the black-box, and supports constraint-based, human-centered exploratory mining of associations. The foundation of this architecture is a rich set of constraint constructs, including domain, class, and SQL-style aggregate constraints, which enable users to clearly specify what associations are to be mined. We propose constrained association queries as a means of specifying the constraints to be satisfied by the antecedent and consequent of a mined association.\nIn this paper, we mainly focus on the technical challenges in guaranteeing a level of performance that is commensurate with the selectivities of the constraints in an association query. To this end, we introduce and analyze two properties of constraints that are critical to pruning: anti-monotonicity and succinctness. We then develop characterizations of various constraints into four categories, according to these properties. Finally, we describe a mining algorithm called CAP, which achieves a maximized degree of pruning for all categories of constraints. Experimental results indicate that CAP can run much faster, in some cases as much as 80 times, than several basic algorithms. This demonstrates how important the succinctness and anti-monotonicity properties are, in delivering the performance guarantee.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145325584",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "2064270136",
                    "name": "Alex T. Pang"
                }
            ]
        },
        {
            "paperId": "3afe8cd8c363a1b3c6823abf74b841db1357c796",
            "title": "nD-SQL: A Multi-Dimensional Language for Interoperability and OLAP",
            "abstract": "We propose a multi-dimensional language called nD-SQL with the following features: (i) nD-SQL supports queries that interoperate amongst multiple relational sources with heterogeneous schemas, including RDBMS and relational data marts, overcoming the mismatch between data and schema; (ii) it supports complex forms of restructuring that permit the visualization of ndimensional data using the three physical dimensions of the relational model, viz., row, column, and relation; (iii) it captures sophisticated aggregations involving multiple granularities, to an arbitrary degree of resolution compared to CUBE, ROLLUP, and DRILLDOWN. We propose a formal model for a federation of relational sources and illustrate nD-SQL against it. We propose an extension to relational algebra, called restructuring relational algebra (RRA), capable of restructuring and aggregation. We propose an architecture for the implementation of an nD-SqL server, based on translating nD-SC/L queries into equivalent RRA expressions, which are then optimized. We are currently implementing an nD-SQL server on the PC platform based on these ideas.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802290",
                    "name": "Fr\u00e9d\u00e9ric Gingras"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "ed8d9bfec50f5b132fe4cfe54fd735d2daf71d00",
            "title": "On querying spreadsheets",
            "abstract": "Considers the problem of querying the data in applications such as spreadsheets and word processors. This problem has several motivations from the perspective of data integration, interoperability and OLAP. We provide an architecture for realizing interoperability among such diverse applications and address the challenges that arise specifically in the context of querying data stored in spreadsheet applications. A fundamental challenge is the lack of a well-defined schema. We propose a framework in which the user can specify the layout of data in a spreadsheet, based on his perception of the important concepts underlying that data. Layout specifications can be viewed as the \"physical schema\" of a spreadsheet. We motivate the concept of an abstract database machine (ADM) that uses the layout specifications to provide a relational view of the data in spreadsheet applications and, similar to a DBMS, supports efficient querying of the spreadsheet data. We develop a methodology for building ADMs for spreadsheets and describe our implementation of an ADM for Microsoft Excel applications, based on the above methodology. Our implementation platform is IBM PCs running Windows NT, Microsoft Office and OLE 2.0. We demonstrate the generality and practicality of our approach by developing a formal characterization of the class of spreadsheets that can be handled in our framework. Our results show that the approach is capable of handling a broad class of naturally occurring spreadsheet applications. This work is part of an office tool integration project.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2026217",
                    "name": "Iyer N. Subramanian"
                },
                {
                    "authorId": "144476963",
                    "name": "N. Goyal"
                },
                {
                    "authorId": "119803948",
                    "name": "R. Krishnamurthy"
                }
            ]
        },
        {
            "paperId": "1fb61f574cd0e1165acad66f5674ec6034f23b23",
            "title": "ProbView: a flexible probabilistic database system",
            "abstract": "Probability theory is mathematically the best understood paradigm for modeling and manipulating uncertain information. Probabilities of complex events can be computed from those of basic events on which they depend, using any of a number of strategies. Which strategy is appropriate depends very much on the known interdependencies among the events involved. Previous work on probabilistic databases has assumed a fixed and restrictivecombination strategy (e.g., assuming all events are pairwise independent). In this article, we characterize, using postulates, whole classes of strategies for conjunction, disjunction, and negation, meaningful from the viewpoint of probability theory. (1) We propose a probabilistic relational data model and a genericprobabilistic relational algebra that neatly captures various strategiessatisfying the postulates, within a single unified framework. (2) We show that as long as the chosen strategies can be computed in polynomial time, queries in the positive fragment of the probabilistic relational algebra have essentially the same data complexity as classical relational algebra. (3) We establish various containments and equivalences between algebraic expressions, similar in spirit to those in classical algebra. (4) We develop algorithms for maintaining materialized probabilistic views. (5) Based on these ideas, we have developed a prototype probabilistic database system called ProbView on top of Dbase V.0. We validate our complexity results with experiments and show that rewriting certain types of queries to other equivalent forms often yields substantial savings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "144491891",
                    "name": "N. Leone"
                },
                {
                    "authorId": "47552022",
                    "name": "Robert B. Ross"
                },
                {
                    "authorId": "1728462",
                    "name": "V. S. Subrahmanian"
                }
            ]
        },
        {
            "paperId": "3e1160650c503d8ffb85c09e44561ecdbcf05348",
            "title": "Languages for multi-database interoperability",
            "abstract": "Database system technology which there is a moliferation has reached a of inderrendent stage now in svstems storing and manipul~ting enormous amount of data. Unfortunately, these systems typically have their own data models, communication processing protocols, query processing systems, concurrency control protocols, consistency management, and other similar aspects of database systems. There is also an increasing need for In teroperability among these systems. Though considerable amount of research has been done in the area of database interoperability, most of it has resulted in solutions that are ad-hoc and procedural. We have developed a declarative environment in which multiple heterogeneous databases interoperate by sharing, interpreting, and manipulating information, in a uniform way. An important criterion for Interacting with multiple databases is the abiIity to query them in a manner independent of the discrepancies in their structure and data semantics. In this demo, we exhibk two languages for querying across multiple databaaes which store semantically similar data using heterogeneous schema, ss well as for restructuring the queried data: (1) SchemaLog \u2013 a language that has its foundations in logic and (2) SchemaSQL \u2013 a language based on a principled extension to SQL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1802290",
                    "name": "Fr\u00e9d\u00e9ric Gingras"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2026217",
                    "name": "Iyer N. Subramanian"
                },
                {
                    "authorId": "1849486",
                    "name": "Despina Papoulis"
                },
                {
                    "authorId": "2085544",
                    "name": "Nematollaah Shiri"
                }
            ]
        },
        {
            "paperId": "5e4aa2aa30d63776336293d5faabc373f8000a73",
            "title": "A foundation for integrating heterogeneous data sources",
            "abstract": "We study the foundations of the integration issues that arise in a federation of heterogeneous data sources, possibly storing related information. Some of the notable features of our approach, motivated by the shortcomings of existing technology, include (a) the ability to share data across multiple heterogeneous data sources, (b) the ability to manipulate the meta-data (schema) component of a data source in the same vein as data can be manipulated, and (c) the ability to query besides well-structured data sources (such as relational databases), semi-structured data sources (such as the HTML documents on the World Wide Web). Our approach is declarative and is based on a simple logic called SchemaLog. SchemaLog's syntax is higher-order but it enjoys a first-order semantics. We present a formal account of the semantics of SchemaLog by developing a model theory, a proof theory, and a fixpoint theory. \nSchemaLog can be implemented on top of existing database systems in a 'non-intrusive' way. Realizing an efficient implementation of a SchemaLog-based system warrants the study of the calculus and algebraic languages underlying SchemaLog. We develop a new algebra by extending the conventional relational algebra with some new operations that are capable of manipulating both data and schema information in a federation of databases. We also develop a calculus language inspired by SchemaLog. Based on the calculus language, we study varying notions of safety that naturally arise in a federation scenario. \nOne of our primary concerns in this dissertation has been the practical relevance and industrial impact of our contributions. In this vein and inspired by the SchemaLog experience, we develop a principled extension of SQL, called SchemaSQL. SchemaSQL is downward compatible with SQL syntax and semantics and is capable of (a) representing data in a database, in a structure substantially different from the original database, in which data and meta-data may be interchanged, (b) creating views whose schema is dynamically dependent on the input database, (c) expressing novel aggregation (over rows, and in general blocks of information) operations, in the spirit of some of the functionalities needed in OLAP applications, and (d) providing a great facility for interoperability and data, meta-data management in multidatabase systems. \nLegacy as well as non-traditional information systems constitute an important fragment of the data sources available in real-life. We demonstrate that SchemaLog can be naturally extended to support non-relational systems as well. In particular, we address the fundamental problem of retrieving specific information of interest to the user, from the enormous number of resources that are available on the Web. With this in mind, and inspired by SchemaLog, we develop a simple logic called WebLog and illustrate the simplicity and power of WebLog for Web querying and restructuring using a variety of applications involving real-life information in the Web. (Abstract shortened by UMI.)",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "66230271",
                    "name": "N. Subramanian"
                }
            ]
        },
        {
            "paperId": "8686d73085b55dbac848a4b4b7bfb4a2d7a617a7",
            "title": "A Foundation for Multi-dimensional Databases",
            "abstract": "We present a multi-dimensional database model, which we believe can serve as a conceptual model for On-Line Analytical Processing (OLAP)-based applications. Apart from providing the functionalities necessary for OLAP-based applications, the main feature of the model we propose is a clear separation between structural aspects and the contents. This separation of concerns allows us to define data manipulation languages in a reasonably simple, transparent way. In particular, we show that the data cube operator can be expressed easily. Concretely, we define an algebra and a calculus and show them to be equivalent. We conclude by comparing our approach to related work. The conceptual multi-dimensional database model developed here is orthogonal to its implementation, which is not a subject of the present paper.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1714694",
                    "name": "M. Gyssens"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "2ded88e9d7685a91d3793e30cdb8d1893c183f72",
            "title": "Tables as a paradigm for querying and restructuring (extended abstract)",
            "abstract": "Tables are one of the most natural representations of real-life data. Previous table-based data models (such as relational, nested relational, and complex objects models) capture only a limited variety of real-life tables. In this paper, we study the foundations of tabular representations of data. We propose the tabular database model for handling a broad class of natural data representations and develop tabular algebraas a language for querying and restructuring tabular data. We show that the tabular algebra is complete for a very general class of transformations and show that several languages designed for very di(cid:11)erent purposes can naturally be embedded into the tabular model. We also demonstrate the applicability of our model as a theoretical foundation for on-line analytical processing (OLAP), an emerging technology for complementing the robust data management and transaction processing of DBMS with powerful tools for data analysis.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1714694",
                    "name": "M. Gyssens"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2026217",
                    "name": "Iyer N. Subramanian"
                }
            ]
        },
        {
            "paperId": "3e6d68e5c650f4e0ad9dfd7bffa9f7c005c1f591",
            "title": "On implementing SchemaLog\u2014a database programming language",
            "abstract": "Efficient implementation of advanced database programming languages call for investigating novel architectures and algorithms. In this paper, we discuss our implementation of SchemaLo& a logic-based database programming language, capable of offering a powerful platform for a variety of database applications involving data/meta-data querying and restructuring. Our architecture for the implementation is based on compiling SchemaLog constructs into an extended version of relational algebra called SchemaLog. Based on this algebra, we develop a top-down algorithm for evaluating SchemaLog programs. We discuss three alternative storage stmctures for the implementation and study their effect on the efficiency of implementation. For each storage structure, we propose strategies for implementing our algebraic operators. We have implemented all these strategies on top of Microsoft Access DBMS running on Windows 3.1, and have run an extensive set of experiments for evaluating the efficiency of alternative strategies under a varied mix of querying and restructuring operations. We discuss the results of our experiments and conclude with a discussion of a graphic user interface for SchemaLog program development, that has also been implemented.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39208021",
                    "name": "A. Andrews"
                },
                {
                    "authorId": "2085544",
                    "name": "Nematollaah Shiri"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2026217",
                    "name": "Iyer N. Subramanian"
                }
            ]
        },
        {
            "paperId": "5609f8819f23dfc5844636925d730999d179b90c",
            "title": "A Parametric Approach to Deductive Databases with Uncertainty",
            "abstract": "Numerous frameworks have been proposed in recent years for deductive databases with uncertainty. On the basis of how uncertainty is associated with the facts and rules in a program, we classify these frameworks into implication-based (IB) and annotation-based (AB) frameworks. We take the IB approach and propose a generic framework, called the parametric framework, as a unifying umbrella for IB frameworks. We develop the declarative, fixpoint, and proof-theoretic semantics of programs in our framework and show their equivalence. Using the framework as a basis, we then study the query optimization problem of containment of conjunctive queries in this framework and establish necessary and sufficient conditions for containment for several classes of parametric conjunctive queries. Our results yield tools for use in the query optimization for large classes of query programs in IB deductive databases with uncertainty.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2085544",
                    "name": "Nematollaah Shiri"
                }
            ]
        },
        {
            "paperId": "d06b1a521cc1553ea8e823702005f33283074360",
            "title": "A declarative language for querying and restructuring the Web",
            "abstract": "World Wide Web is a hypertext based, distributed information system that provides access to vast amounts of information in the Internet. A fundamental problem with the Web is the difficulty of retrieving specific information of interest to the user, from the enormous number of resources that are available. We develop a simple logic called WebLog that is capable of retrieving information from HTML (Hypertext Markup Language) documents in the Web. WebLog is inspired by SchemaLog, a logic for multidatabase interoperability. We demonstrate the suitability of WebLog for: querying and restructuring Web information; exploiting partial knowledge users might have on the information being queried; and dealing with the dynamic nature of information in the Web. We illustrate the simplicity and power of WebLog using a variety of applications involving real life information in the Web.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1713262",
                    "name": "F. Sadri"
                },
                {
                    "authorId": "2026217",
                    "name": "Iyer N. Subramanian"
                }
            ]
        },
        {
            "paperId": "e06d32d936666825090eb97606b100f475de3313",
            "title": "SchemaSQL - A Language for Interoperability in Relational Multi-Database Systems",
            "abstract": "We provide a principled extension of SQL, called SchemaSQL , that offers the capability of uniform manipulation of data and meta-data in relational multi-database systems. We develop a precise syntax and semantics of SchemaSQL in a manner that extends traditional SQL syntax and semantics, and demonstrate the following. (1) SchemaSQL retains the flavour of SQL while supporting querying of both data and meta-data. (2) It can be used to represent data in a database in a structure substantially different from original database, in which data and meta-data may be interchanged. (3) It also permits the creation of views whose schema is dynamically dependent on the contents of the input instance. (4) While aggregation in SQL is restricted to values occurring in one column at a time, SchemaSQL permits \u201chorizontal\u201d aggregation and even aggregation over more general \u201cblocks\u201d of information. (5) SchemaSQL provides a great facility for interoperability and data/meta-data management in relational multi-database systems. We provide many examples to illustrate our claims. We outline an architecture for the implementation of SchemaSQL and discuss implementation algorithms based on available database technology that allows for powerful integration of SQL based relational DBMS. l This work was supported by grants from the Natural Sciences and Engineering Research Council of Canada (NSERC), the National Science Foundation (NSF), and The University of North Carolina at Greensboro. t Dept of Computer Science, Concordia University, Montreal, Canada. {laks,subbu}Ocs.concordia.ca $ Dept of Mathematical Sciences, University of North Carolina, Greensboro, NC. sadri@uncg.edu Permission to copy without fee all or port of this material is gmnted provided that the copies ore not, mode or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its dote appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires o fee and/or special permission from the Endowment. Proceedings of the 22nd VLDB Conference Mumbai(Bombay), India, 1996",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1713262",
                    "name": "F. Sadri"
                },
                {
                    "authorId": "2026217",
                    "name": "Iyer N. Subramanian"
                }
            ]
        },
        {
            "paperId": "2815c3b728de4c572dcc0e7ca52e47c0ca7f1c00",
            "title": "A Declarative Semantics for Behavioral Inheritance and Conflict Resolution",
            "abstract": "We propose a novel semantics for object oriented deductive databases in the direc tion of F logic to logically account for behavioral inheritance con ict resolution in multiple inheritance hierarchies and overriding We introduce the ideas of with drawal locality and inheritability of properties i e methods and signatures Ex ploiting these ideas we develop a declarative semantics of behavioral inheritance and overriding without having to resort to non monotonic reasoning Con ict res olution in our model can be achieved both via speci cation and by detection The possibility of speci cation based con ict resolution through withdrawal allows users to de ne inheritance preference We present a formal account of the semantics of our language by de ning a model theory proof theory and a xpoint theory We also show that the di erent characterizations of our language are equivalent",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1698677",
                    "name": "H. Jamil"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "7c7df6323f11e255cb88324d0333a67192efcf88",
            "title": "Pushing semantics inside recursion: A general framework for semantic optimization of recursive queries",
            "abstract": "We consider a class of linear query programs and integrity constraints and develop methods for (i) computing the residues and (ii) pushing them inside the recursive programs, minimizing redundant computation and run-time overhead. We also discuss applications of our strategy to intelligent query answering.<<ETX>>",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1735147",
                    "name": "R. Missaoui"
                }
            ]
        },
        {
            "paperId": "5af960239d8c0433335862f7aba84232f517bcd1",
            "title": "Formal Methods in Databases and Software Engineering, Proceedings of the Workshop on Formal Methods in Databases and Software Engineering, Montreal, Canada, 15-16 May 1992",
            "abstract": "Transaction Logic: An (Early) Expose.- Aggregate Operations in the Information Source Tracking Method.- An Incremental Concept Formation Approach for Learning from Databases.- The Tecton Proof System.- Modeling Time in Information Systems.- A Unified Framework for Database Specification: Functional Approach.- Using VDM Within an Object-Oriented Framework.- Software Engineering Environments - What Do We Want?.- Efficient Deduction and Induction: Key to the Success of Data-Intensive Knowledge-Base Systems.- On Querying Temporal Deductive Databases.- Intuitionistic Interpretation of Deductive Databases with Incomplete Information.- Author Index.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1740658",
                    "name": "V. Alagar"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1713262",
                    "name": "F. Sadri"
                }
            ]
        },
        {
            "paperId": "73783c0b83b4bfd6b3b2a2a87df79ee09f84e40d",
            "title": "Efficient Parallel Algorithms for Finding Chordless Cycles in Graphs",
            "abstract": "We present simple and efficient parallel algorithms for obtaining a chordless cycle of length greater than or equal to k in a graph whenever such a cycle exists. Our results generalize and simplify existing results for detecting chordless cycles.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2063021951",
                    "name": "N. Chandrasekharan"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2418427",
                    "name": "M. Medidi"
                }
            ]
        },
        {
            "paperId": "b58aa9048e1fc2b41edf51e02b41af3bd13c04ab",
            "title": "Homomorphic tree embeddings and their applications to recursive program optimization",
            "abstract": "The problems of stage-preserving linearization and one-boundedness are studied for a class of nonlinear single rule recursive programs, and syntactic characterizations are developed for both. The characterizations lead to a polynomial-time algorithm for the former and a linear-time algorithm for the latter. Stage-preserving linearization results in a significant improvement in evaluation efficiency, compared to a linearization that does not preserve stages. The class of nonlinear strips that are stage-preserving linearizable includes several classes of programs that can be linearized only using a mix of left and right linear rules, as well as programs that cannot be linearized using previously known techniques. The study makes use of a technique based on the notion of homomorphic tree embeddings.<<ETX>>",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "32781174",
                    "name": "K. Ashraf"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "5d0ba04059cdc5cf7f0f6f176e49885cd6675c84",
            "title": "ORLOG : A Logic For Semantic Object-Oriented Models",
            "abstract": "We argue that powerful models for supporting next genera tion database and knowledge base applications can be built by extending semantic data models SDMs in the direction of Object Oriented OO modeling SDMs possess several of the OO features as well as additional useful features e g relationships and the associated constraints which have no counterpart in the OO models We present a conceptual semantic OO data model called Object Relationship OR model OR model captures most of the semantic modeling as well as OO modeling features needed for the new database applications As a second step we lay down the logical foun dations of the OR model and develop a logical language called ORLOG in the spirit of deductive database languages such as Datalog providing a basis for high level OO programming Use of a logical language like Orlog opens up the possibility of applying knowledge representation techniques for seman tic and OO data modeling We illustrate our conceptual and logical models with a substantial design example",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053580620",
                    "name": "M. Jamil"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "b8dce7c04a00b5837ee4a18366c066aaecdbe1e8",
            "title": "Deductive Databases with Incomplete Information",
            "abstract": "We consider query processing in deductive databases with incomplete information in the form of null values. We motivate the problem of extracting the maximal information from a (deductive) database in response to queries, and formalize this in the form of conditional answers. We give a sound and complete top-down proof procedure for generating conditional answers. We also extend the well-known magic sets method to handle null values, and show that the transformed program executed by semi-naive evaluation (with minor extensions) is correct in the sense that it will generate all and only valid conditional answers w.r.t. the original program.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1729734",
                    "name": "F. Dong"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                }
            ]
        },
        {
            "paperId": "e709f87a1d650a5fd685eb1a337df2d585943837",
            "title": "On semantic query optimization in deductive databases",
            "abstract": "The focus is semantic query optimization in the presence of integrity constraints such as inclusion dependencies and context dependencies (CDs). The authors provide the motivation for the type of integrity constraints considered and for the work at large. They introduce CDs formally and illustrate their power in capturing semantics with an example. An inference mechanism is described for reasoning with these constraints. Sufficient conditions for testing redundancy of atoms in rules and rules in programs are described, and polynomial time algorithms are provided for detecting and eliminating such redundancies. The technique uniformly applies to recursive as well as nonrecursive queries. The approach is illustrated with examples. The discussion and examples are presented in terms of recursive rules only.<<ETX>>",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1735147",
                    "name": "R. Missaoui"
                }
            ]
        },
        {
            "paperId": "8dd11e1616a3777bb6c16712d345f17c005cecf0",
            "title": "Structural query optimization\u2014a uniform framework for semantic query optimization in deductive databases",
            "abstract": "Recursion is at once a source of increased expressive power and high complexity for query languages like Datalog. A major question therefore is efficient processing of recursive queries. The first line of attack to process a recursive Datalog program P is to try to see if we can remove its recursivity; this is equivalent to testing whether there is a nonrecursive Datalog program which is equivalent to P. If this is the case, then P is said to be bounded [CGKV, GMSV, I, Nl, NS]. In general, the problem of testing whether a Datalog program is bounded is known to be undecidable even for linear programs with one idb predicate [GMSV]. Some other decidable and undecidable results on boundedness of Datalog programs are presented in [A, CGKV, K, Nl, NS, S, Var]. For a detailed summary of the present state-of-the-art of this problem see [KA]. None of the positive results above take into account the integrity constraints that may be satisfied by the predicates in the program.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "143750276",
                    "name": "H. J. Hern\u00e1ndez"
                }
            ]
        }
    ]
}