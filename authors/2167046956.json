{
    "authorId": "2167046956",
    "papers": [
        {
            "paperId": "450a0d2ba0d689537230e774219a7695ede1533e",
            "title": "Retrieval-Oriented Knowledge for Click-Through Rate Prediction",
            "abstract": "Click-through rate (CTR) prediction is crucial for personalized online services. Sample-level retrieval-based models, such as RIM, have demonstrated remarkable performance. However, they face challenges including inference inefficiency and high resource consumption due to the retrieval process, which hinder their practical application in industrial settings. To address this, we propose a universal plug-and-play \\underline{r}etrieval-\\underline{o}riented \\underline{k}nowledge (\\textbf{\\name}) framework that bypasses the real retrieval process. The framework features a knowledge base that preserves and imitates the retrieved \\&aggregated representations using a decomposition-reconstruction paradigm. Knowledge distillation and contrastive learning optimize the knowledge base, enabling the integration of retrieval-enhanced representations with various CTR models. Experiments on three large-scale datasets demonstrate \\name's exceptional compatibility and performance, with the neural knowledge base serving as an effective surrogate for the retrieval pool. \\name surpasses the teacher model while maintaining superior inference efficiency and demonstrates the feasibility of distilling knowledge from non-parametric methods using a parametric approach. These results highlight \\name's strong potential for real-world applications and its ability to transform retrieval-based methods into practical solutions. Our implementation code is available to support reproducibility in \\url{https://github.com/HSLiu-Initial/ROK.git}.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167046956",
                    "name": "Huanshuo Liu"
                },
                {
                    "authorId": "2258709565",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2238203237",
                    "name": "Menghui Zhu"
                },
                {
                    "authorId": "2144908858",
                    "name": "Jianghao Lin"
                },
                {
                    "authorId": "79494403",
                    "name": "Jiarui Qin"
                },
                {
                    "authorId": "2290248265",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "2298987734",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "4dae796cf6a66e94bbd6d209b17855a0f057cc04",
            "title": "Evaluating the External and Parametric Knowledge Fusion of Large Language Models",
            "abstract": "Integrating external knowledge into large language models (LLMs) presents a promising solution to overcome the limitations imposed by their antiquated and static parametric memory. Prior studies, however, have tended to over-reliance on external knowledge, underestimating the valuable contributions of an LLMs' intrinsic parametric knowledge. The efficacy of LLMs in blending external and parametric knowledge remains largely unexplored, especially in cases where external knowledge is incomplete and necessitates supplementation by their parametric knowledge. We propose to deconstruct knowledge fusion into four distinct scenarios, offering the first thorough investigation of LLM behavior across each. We develop a systematic pipeline for data construction and knowledge infusion to simulate these fusion scenarios, facilitating a series of controlled experiments. Our investigation reveals that enhancing parametric knowledge within LLMs can significantly bolster their capability for knowledge integration. Nonetheless, we identify persistent challenges in memorizing and eliciting parametric knowledge, and determining parametric knowledge boundaries. Our findings aim to steer future explorations on harmonizing external and parametric knowledge within LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298987734",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2303799244",
                    "name": "Yuyang Zhang"
                },
                {
                    "authorId": "2238716271",
                    "name": "Xiaoguang Li"
                },
                {
                    "authorId": "2304129925",
                    "name": "Wenxuan Shi"
                },
                {
                    "authorId": "2304076618",
                    "name": "Haonan Xu"
                },
                {
                    "authorId": "2167046956",
                    "name": "Huanshuo Liu"
                },
                {
                    "authorId": "2282544603",
                    "name": "Yasheng Wang"
                },
                {
                    "authorId": "2284986217",
                    "name": "Lifeng Shang"
                },
                {
                    "authorId": "2249841180",
                    "name": "Qun Liu"
                },
                {
                    "authorId": "2297898895",
                    "name": "Yong Liu"
                },
                {
                    "authorId": "2284295184",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "cb1bfd79445b7c4838cf59b6a93c898a8270f42e",
            "title": "CtrlA: Adaptive Retrieval-Augmented Generation via Probe-Guided Control",
            "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising solution for mitigating hallucinations of large language models (LLMs) with retrieved exter-nal knowledge. Adaptive RAG enhances this approach by dynamically assessing the retrieval necessity, aiming to balance external and internal knowledge usage. However, existing adaptive RAG methods primarily realize retrieval on demand by relying on superficially verbalize-based or probability-based feedback of LLMs, or directly fine-tuning LLMs via carefully crafted datasets, resulting in unreliable retrieval necessity decisions, heavy extra costs, and sub-optimal response generation. We present the first attempts to delve into the internal states of LLMs to mitigate such issues by introducing an effective probe-guided adaptive RAG framework, termed C TRL A. Specifically, C TRL A employs an honesty probe to regulate the LLM\u2019s behavior by manipulating its representations for increased honesty, and a confidence probe to monitor the internal states of LLM and assess confidence levels, determining the retrieval necessity during generation. Experiments show that C TRL A is superior to existing adaptive RAG methods on a diverse set of tasks, the honesty control can effectively make LLMs more honest and confidence monitoring is proven to be a promising indicator of retrieval trigger. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2167046956",
                    "name": "Huanshuo Liu"
                },
                {
                    "authorId": "2298263602",
                    "name": "Hao Zhang"
                },
                {
                    "authorId": "2681038",
                    "name": "Zhijiang Guo"
                },
                {
                    "authorId": "2275185317",
                    "name": "Kuicai Dong"
                },
                {
                    "authorId": "2297935844",
                    "name": "Xiangyang Li"
                },
                {
                    "authorId": "2297951506",
                    "name": "Yi Quan Lee"
                },
                {
                    "authorId": "2284302963",
                    "name": "Cong Zhang"
                },
                {
                    "authorId": "2297898895",
                    "name": "Yong Liu"
                }
            ]
        },
        {
            "paperId": "884f8a29a7979a1f05223f432d9a149ac4dcc6b4",
            "title": "PLATE: A Prompt-Enhanced Paradigm for Multi-Scenario Recommendations",
            "abstract": "With the explosive growth of commercial applications of recommender systems, multi-scenario recommendation (MSR) has attracted considerable attention, which utilizes data from multiple domains to improve their recommendation performance simultaneously. However, training a unified deep recommender system (DRS) may not explicitly comprehend the commonality and difference among domains, whereas training an individual model for each domain neglects the global information and incurs high computation costs. Likewise, fine-tuning on each domain is inefficient, and recent advances that apply the prompt tuning technique to improve fine-tuning efficiency rely solely on large-sized transformers. In this work, we propose a novel prompt-enhanced paradigm for multi-scenario recommendation. Specifically, a unified DRS backbone model is first pre-trained using data from all the domains in order to capture the commonality across domains. Then, we conduct prompt tuning with two novel prompt modules, capturing the distinctions among various domains and users. Our experiments on Douban, Amazon, and Ali-CCP datasets demonstrate the effectiveness of the proposed paradigm with two noticeable strengths: (i) its great compatibility with various DRS backbone models, and (ii) its high computation and storage efficiency with only 6% trainable parameters in prompt tuning phase. The implementation code is available for easy reproduction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223878432",
                    "name": "Yuhao Wang"
                },
                {
                    "authorId": "2116711669",
                    "name": "Xiangyu Zhao"
                },
                {
                    "authorId": "92633145",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2112246463",
                    "name": "Qidong Liu"
                },
                {
                    "authorId": "3339005",
                    "name": "Huifeng Guo"
                },
                {
                    "authorId": "2167046956",
                    "name": "Huanshuo Liu"
                },
                {
                    "authorId": "2116640929",
                    "name": "Yichao Wang"
                },
                {
                    "authorId": "144142354",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "65390627197123d54b50753f29428a7d2b59c024",
            "title": "Asymptotic normality of error distribution estimator in autoregressive models",
            "abstract": "Abstract In this article, we consider the asymptotic distribution of residual distribution estimator in the first order autoregressive models with positively associated or negatively associated random errors. Under mild regularity assumptions, some asymptotic normality results for residual distribution estimator are obtained when the autoregressive model is a stationary process or an explosive process. In addition, some simulations of estimated curves and mean integrated square errors are illustrated, which agree with our theoretical results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112380301",
                    "name": "Shipeng Wu"
                },
                {
                    "authorId": "2814001",
                    "name": "Wenzhi Yang"
                },
                {
                    "authorId": "2167046956",
                    "name": "Huanshuo Liu"
                },
                {
                    "authorId": "39703662",
                    "name": "Mingchen Gao"
                }
            ]
        }
    ]
}