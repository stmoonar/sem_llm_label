{
    "authorId": "152945656",
    "papers": [
        {
            "paperId": "060c886bcf59508b61946646fc1a12ac449cfdfb",
            "title": "Neuromodulation Gated Transformer",
            "abstract": "We introduce a novel architecture, the Neuromodulation Gated Transformer (NGT), which is a simple implementation of neuromodulation in transformers via a multiplicative effect. We compare it to baselines and show that it results in the best average performance on the SuperGLUE benchmark validation sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216490249",
                    "name": "Kobe Knowles"
                },
                {
                    "authorId": "6835173",
                    "name": "Joshua Bensemann"
                },
                {
                    "authorId": "2992071",
                    "name": "Diana Benavides Prado"
                },
                {
                    "authorId": "51071778",
                    "name": "Vithya Yogarajan"
                },
                {
                    "authorId": "2582677",
                    "name": "Michael Witbrock"
                },
                {
                    "authorId": "152945656",
                    "name": "G. Dobbie"
                },
                {
                    "authorId": "2144353312",
                    "name": "Yang Chen"
                }
            ]
        },
        {
            "paperId": "0dd76ffeb315860e5e6407e020d74a04e24876e6",
            "title": "Source Inference Attacks: Beyond Membership Inference Attacks in Federated Learning",
            "abstract": "Federated learning (FL) is a popular approach to facilitate privacy-aware machine learning since it allows multiple clients to collaboratively train a global model without granting others access to their private data. It is, however, known that FL can be vulnerable to membership inference attacks (MIAs), where the training records of the global model can be distinguished from the testing records. Surprisingly, research focusing on the investigation of the source inference problem appears to be lacking. We also observe that identifying a training record's source client can result in privacy breaches extending beyond MIAs. Seeking to contribute to the literature gap, we take the first step to investigate source privacy in FL. Specifically, we propose a new inference attack (hereafter referred to as source inference attack\u2013SIA), designed to facilitate an honest-but-curious server to identify the training record's source client. The proposed SIAs leverage the Bayesian theorem to implement the attack in a non-intrusive manner without deviating from the defined FL protocol. We then evaluate SIAs in three different FL frameworks to show that in existing FL frameworks, the clients sharing gradients, model parameters, or predictions on a public dataset will leak such source information to the server. The experimental results validate the efficacy of the proposed SIAs, e.g., an attack success rate of 67.1% (baseline 10%) can be achieved when the clients share model parameters with the server. Comprehensive ablation studies demonstrate that the success of an SIA is directly related to the overfitting of the local models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109102414",
                    "name": "Hongsheng Hu"
                },
                {
                    "authorId": "2249842769",
                    "name": "Xuyun Zhang"
                },
                {
                    "authorId": "2175708",
                    "name": "Z. Salcic"
                },
                {
                    "authorId": "2260836022",
                    "name": "Lichao Sun"
                },
                {
                    "authorId": "116022805",
                    "name": "K. Choo"
                },
                {
                    "authorId": "152945656",
                    "name": "G. Dobbie"
                }
            ]
        },
        {
            "paperId": "5215a52205943e08c886948dc7881938ec5ee230",
            "title": "Diversity, Equity and Inclusion Activities in Database Conferences: A 2022 Report",
            "abstract": "The Diversity, Equity and Inclusion (DEI) initiative started as the Diversity/Inclusion initiative in 2020 [4]. The current report summarizes our activities in 2022. Our responsibility as a community is to ensure that attendees of DB conferences feel included, irrespective of their scientific perspective and personal background. One of the first steps was to establish the role of the DEI chairs at DB Conferences, with the DEI team dedicated to providing leadership to help our community achieve this goal. In this leadership role, the DEI team is advising DEI chairs at DB conferences, serving as a memory of DEI events at conferences, building an agreed-upon vision, and committing to working together to devise a set of measures for achieving DEI. That is pursued via actions led by our core members (Figure 1) and liaisons of individual executive bodies (Figure 2): REACH OUT collects data and experiences from our community. INCLUDE monitors and recommends inclusion efforts. ORGANIZE focuses on in-conference organization efforts, such as adopting a code of conduct. INFORM communicates through various channels. SUPPORT coordinates DEI support from executive bodies and sponsors. SCOUT collates DEI efforts from other communities. COORDINATE manages all actions. Two new actions: MEDIA preserves and disseminates the digital media produced by DEI@DB events. ETHICS establishes and promotes ethics guidelines for publications in our community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "143970078",
                    "name": "D. Agrawal"
                },
                {
                    "authorId": "1710637",
                    "name": "Yael Amsterdamer"
                },
                {
                    "authorId": "1730344",
                    "name": "S. Bhowmick"
                },
                {
                    "authorId": "1401945543",
                    "name": "Jes\u00fas Camacho-Rodr\u00edguez"
                },
                {
                    "authorId": "1726425",
                    "name": "B. Catania"
                },
                {
                    "authorId": "2091879100",
                    "name": "K. Panos"
                },
                {
                    "authorId": "2081044488",
                    "name": "Chrysanthis"
                },
                {
                    "authorId": "1692732",
                    "name": "C. Curino"
                },
                {
                    "authorId": "145025853",
                    "name": "J. Darmont"
                },
                {
                    "authorId": "152945656",
                    "name": "G. Dobbie"
                },
                {
                    "authorId": "1709353",
                    "name": "A. El Abbadi"
                },
                {
                    "authorId": "2223141115",
                    "name": "Avrilia"
                },
                {
                    "authorId": "2223141462",
                    "name": "Floratou"
                },
                {
                    "authorId": "2178387374",
                    "name": "Juliana Freire"
                },
                {
                    "authorId": "2153832",
                    "name": "Alekh Jindal"
                },
                {
                    "authorId": "1685532",
                    "name": "V. Kalogeraki"
                },
                {
                    "authorId": "51205357",
                    "name": "Sujaya Maiyya"
                },
                {
                    "authorId": "2079019460",
                    "name": "Alexandra"
                },
                {
                    "authorId": "2223137576",
                    "name": "Meliou"
                },
                {
                    "authorId": "37168010",
                    "name": "Madhulika Mohanty"
                },
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                },
                {
                    "authorId": "2149251297",
                    "name": "Fatma \u00d6zcan"
                },
                {
                    "authorId": "3139922",
                    "name": "L. Peterfreund"
                },
                {
                    "authorId": "145492471",
                    "name": "Wenny Rahayu"
                },
                {
                    "authorId": "2092368",
                    "name": "S. Sadiq"
                },
                {
                    "authorId": "2062657988",
                    "name": "Sana Sellami"
                },
                {
                    "authorId": "2388459",
                    "name": "Utku Sirin"
                },
                {
                    "authorId": "2131764025",
                    "name": "Wang-Chiew Tan"
                },
                {
                    "authorId": "2080239484",
                    "name": "Bhavani"
                },
                {
                    "authorId": "101679060",
                    "name": "Thuraisingham"
                },
                {
                    "authorId": "1392679676",
                    "name": "Neeraja"
                },
                {
                    "authorId": "2223141037",
                    "name": "Yadwadkar"
                },
                {
                    "authorId": "3010003",
                    "name": "Victor Zakhary"
                },
                {
                    "authorId": "2117848168",
                    "name": "Meihui Zhang"
                }
            ]
        },
        {
            "paperId": "8d1b472a23fe477213dd8d926e87be2b4a50aca1",
            "title": "Disentanglement of Latent Representations via Sparse Causal Interventions",
            "abstract": "The process of generating data such as images is controlled by independent and unknown factors of variation. The retrieval of these variables has been studied extensively in the disentanglement, causal representation learning, and independent component analysis fields. Recently, approaches merging these domains together have shown great success. Instead of directly representing the factors of variation, the problem of disentanglement can be seen as finding the interventions on one image that yield a change to a single factor. Following this assumption, we introduce a new method for disentanglement inspired by causal dynamics that combines causality theory with vector-quantized variational autoencoders. Our model considers the quantized vectors as causal variables and links them in a causal graph. It performs causal interventions on the graph and generates atomic transitions affecting a unique factor of variation in the image. We also introduce a new task of action retrieval that consists of finding the action responsible for the transition between two images. We test our method on standard synthetic and real-world disentanglement datasets. We show that it can effectively disentangle the factors of variation and perform precise interventions on high-level semantic attributes of an image without affecting its quality, even with imbalanced data distributions.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2138967383",
                    "name": "Ga\u00ebl Gendron"
                },
                {
                    "authorId": "2819135",
                    "name": "M. Witbrock"
                },
                {
                    "authorId": "152945656",
                    "name": "G. Dobbie"
                }
            ]
        },
        {
            "paperId": "a563fa042b16533d23829d76e7e17bf19a05891c",
            "title": "Large Language Models Are Not Strong Abstract Reasoners",
            "abstract": "Large Language Models have shown tremendous performance on a large variety of natural language processing tasks, ranging from text comprehension to common sense reasoning. \n\nHowever, the mechanisms responsible for this success remain opaque, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally circumscribed. Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data. Evaluating deep neural architectures on this task could give insight into their potential limitations regarding reasoning and their broad generalisation abilities, yet this is currently an under-explored area. In this paper, we introduce a new benchmark for evaluating language models beyond memorization on abstract reasoning tasks. We perform extensive evaluations of state-of-the-art LLMs, showing that they currently achieve very limited performance in contrast with other natural language tasks, even when applying techniques that have been shown to improve performance on other NLP tasks. We argue that guiding LLM generation to follow causal paths could help improve the generalisation and reasoning abilities of LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138967383",
                    "name": "Ga\u00ebl Gendron"
                },
                {
                    "authorId": "1491516930",
                    "name": "Qiming Bao"
                },
                {
                    "authorId": "2819135",
                    "name": "M. Witbrock"
                },
                {
                    "authorId": "152945656",
                    "name": "G. Dobbie"
                }
            ]
        },
        {
            "paperId": "a8382a42fc84c257b8548bdb0af9c77d095104a3",
            "title": "A Survey of Methods, Challenges and Perspectives in Causality",
            "abstract": "Deep Learning models have shown success in a large variety of tasks by extracting correlation patterns from high-dimensional data but still struggle when generalizing out of their initial distribution. As causal engines aim to learn mechanisms independent from a data distribution, combining Deep Learning with Causality can have a great impact on the two fields. In this paper, we further motivate this assumption. We perform an extensive overview of the theories and methods for Causality from different perspectives, with an emphasis on Deep Learning and the challenges met by the two domains. We show early attempts to bring the fields together and the possible perspectives for the future. We finish by providing a large variety of applications for techniques from Causality.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2138967383",
                    "name": "Ga\u00ebl Gendron"
                },
                {
                    "authorId": "2819135",
                    "name": "M. Witbrock"
                },
                {
                    "authorId": "152945656",
                    "name": "G. Dobbie"
                }
            ]
        },
        {
            "paperId": "cbef1cd3651e0f7cbd4d8bbea749b1983b5bc54e",
            "title": "Effectiveness of Debiasing Techniques: An Indigenous Qualitative Analysis",
            "abstract": "An indigenous perspective on the effectiveness of debiasing techniques for pre-trained language models (PLMs) is presented in this paper. The current techniques used to measure and debias PLMs are skewed towards the US racial biases and rely on pre-defined bias attributes (e.g.\"black\"vs\"white\"). Some require large datasets and further pre-training. Such techniques are not designed to capture the underrepresented indigenous populations in other countries, such as M\\=aori in New Zealand. Local knowledge and understanding must be incorporated to ensure unbiased algorithms, especially when addressing a resource-restricted society.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51071778",
                    "name": "Vithya Yogarajan"
                },
                {
                    "authorId": "152945656",
                    "name": "G. Dobbie"
                },
                {
                    "authorId": "2319565",
                    "name": "H. Gouk"
                }
            ]
        },
        {
            "paperId": "d6a80bbcbe92bd38582c80fc598ff4450c42f602",
            "title": "Moving a Bootcamp-Style Computer Science Programme Online: An Experience Report",
            "abstract": "The Postgraduage Certificate in Information Technology at the University of Auckland is a bootcamp-style transition programme for students without prior programming experience. The programme's part-time variant, designed for working professionals, has traditionally been offered with intensive evening classes, with extensive tutor and instructor support. Spurred on by the COVID-19 pandemic, our University has moved this part-time variant fully online. Benefits of the online offering include a wider, more inclusive audience and flexibility of student learning schedules. However, it has introduced several pedagogical challenges, including but not limited to how instructors can continue to provide meaningful support, feedback, and student engagement in an online environment. In this paper, we reflect on our experience, analyse qualitative and quantitative feedback such as student evaluations and grades, and provide recommendations for those wishing to undertake a similar transformation in the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2751263",
                    "name": "A. Meads"
                },
                {
                    "authorId": "2074851",
                    "name": "Yu-Cheng Tu"
                },
                {
                    "authorId": "152945656",
                    "name": "G. Dobbie"
                }
            ]
        },
        {
            "paperId": "2e7920001f78b5ded7364f6b5ea74553c87b477a",
            "title": "Membership Inference via Backdooring",
            "abstract": "Recently issued data privacy regulations like GDPR (General Data Protection Regulation) grant individuals the right to be forgotten. In the context of machine learning, this requires a model to forget about a training data sample if requested by the data owner (i.e., machine unlearning). As an essential step prior to machine unlearning, it is still a challenge for a data owner to tell whether or not her data have been used by an unauthorized party to train a machine learning model. Membership inference is a recently emerging technique to identify whether a data sample was used to train a target model, and seems to be a promising solution to this challenge. However, straightforward adoption of existing membership inference approaches fails to address the challenge effectively due to being originally designed for attacking membership privacy and suffering from several severe limitations such as low inference accuracy on well-generalized models. In this paper, we propose a novel membership inference approach inspired by the backdoor technology to address the said challenge. Specifically, our approach of Membership Inference via Backdooring (MIB) leverages the key observation that a backdoored model behaves very differently from a clean model when predicting on deliberately marked samples created by a data owner. Appealingly, MIB requires data owners' marking a small number of samples for membership inference and only black-box access to the target model, with theoretical guarantees for inference results. We perform extensive experiments on various datasets and deep neural network architectures, and the results validate the efficacy of our approach, e.g., marking only 0.1% of the training dataset is practically sufficient for effective membership inference.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109102414",
                    "name": "Hongsheng Hu"
                },
                {
                    "authorId": "2175708",
                    "name": "Z. Salcic"
                },
                {
                    "authorId": "152945656",
                    "name": "G. Dobbie"
                },
                {
                    "authorId": "2144194281",
                    "name": "Jinjun Chen"
                },
                {
                    "authorId": "49755259",
                    "name": "Lichao Sun"
                },
                {
                    "authorId": "1992681025",
                    "name": "Xuyun Zhang"
                }
            ]
        },
        {
            "paperId": "aa1862f93cfba97505fb2247ce9f879cb80f5e30",
            "title": "Data and model bias in artificial intelligence for healthcare applications in New Zealand",
            "abstract": "Introduction Developments in Artificial Intelligence (AI) are adopted widely in healthcare. However, the introduction and use of AI may come with biases and disparities, resulting in concerns about healthcare access and outcomes for underrepresented indigenous populations. In New Zealand, M\u0101ori experience significant inequities in health compared to the non-Indigenous population. This research explores equity concepts and fairness measures concerning AI for healthcare in New Zealand. Methods This research considers data and model bias in NZ-based electronic health records (EHRs). Two very distinct NZ datasets are used in this research, one obtained from one hospital and another from multiple GP practices, where clinicians obtain both datasets. To ensure research equality and fair inclusion of M\u0101ori, we combine expertise in Artificial Intelligence (AI), New Zealand clinical context, and te ao M\u0101ori. The mitigation of inequity needs to be addressed in data collection, model development, and model deployment. In this paper, we analyze data and algorithmic bias concerning data collection and model development, training and testing using health data collected by experts. We use fairness measures such as disparate impact scores, equal opportunities and equalized odds to analyze tabular data. Furthermore, token frequencies, statistical significance testing and fairness measures for word embeddings, such as WEAT and WEFE frameworks, are used to analyze bias in free-form medical text. The AI model predictions are also explained using SHAP and LIME. Results This research analyzed fairness metrics for NZ EHRs while considering data and algorithmic bias. We show evidence of bias due to the changes made in algorithmic design. Furthermore, we observe unintentional bias due to the underlying pre-trained models used to represent text data. This research addresses some vital issues while opening up the need and opportunity for future research. Discussions This research takes early steps toward developing a model of socially responsible and fair AI for New Zealand's population. We provided an overview of reproducible concepts that can be adopted toward any NZ population data. Furthermore, we discuss the gaps and future research avenues that will enable more focused development of fairness measures suitable for the New Zealand population's needs and social structure. One of the primary focuses of this research was ensuring fair inclusions. As such, we combine expertise in AI, clinical knowledge, and the representation of indigenous populations. This inclusion of experts will be vital moving forward, proving a stepping stone toward the integration of AI for better outcomes in healthcare.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51071778",
                    "name": "Vithya Yogarajan"
                },
                {
                    "authorId": "152945656",
                    "name": "G. Dobbie"
                },
                {
                    "authorId": "1929752",
                    "name": "Sharon Leitch"
                },
                {
                    "authorId": "37184382",
                    "name": "T. Keegan"
                },
                {
                    "authorId": "6835173",
                    "name": "Joshua Bensemann"
                },
                {
                    "authorId": "2582677",
                    "name": "Michael Witbrock"
                },
                {
                    "authorId": "8847998",
                    "name": "V. Asrani"
                },
                {
                    "authorId": "143687193",
                    "name": "D. Reith"
                }
            ]
        }
    ]
}