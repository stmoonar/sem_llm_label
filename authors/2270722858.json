{
    "authorId": "2270722858",
    "papers": [
        {
            "paperId": "0e3d0165a5fc838b17a242855f13d8a2513f653b",
            "title": "A Survey on Neural Question Generation: Methods, Applications, and Prospects",
            "abstract": "In this survey, we present a detailed examination of the advancements in Neural Question Generation (NQG), a field leveraging neural network techniques to generate relevant questions from diverse inputs like knowledge bases, texts, and images. The survey begins with an overview of NQG's background, encompassing the task's problem formulation, prevalent benchmark datasets, established evaluation metrics, and notable applications. It then methodically classifies NQG approaches into three predominant categories: structured NQG, which utilizes organized data sources, unstructured NQG, focusing on more loosely structured inputs like texts or visual content, and hybrid NQG, drawing on diverse input modalities. This classification is followed by an in-depth analysis of the distinct neural network models tailored for each category, discussing their inherent strengths and potential limitations. The survey culminates with a forward-looking perspective on the trajectory of NQG, identifying emergent research trends and prospective developmental paths. Accompanying this survey is a curated collection of related research papers, datasets, and codes, all of which are available on GitHub. This provides an extensive reference for those delving into NQG.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119113081",
                    "name": "Shasha Guo"
                },
                {
                    "authorId": "2053831159",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "2287979930",
                    "name": "Cuiping Li"
                },
                {
                    "authorId": "2270722858",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "134243c4a2479c99662416ecbe6a42448f461f1d",
            "title": "CIRP: Cross-Item Relational Pre-training for Multimodal Product Bundling",
            "abstract": "Product bundling has been a prevailing marketing strategy that is beneficial in the online shopping scenario. Effective product bundling methods depend on high-quality item representations, which need to capture both the individual items' semantics and cross-item relations. However, previous item representation learning methods, either feature fusion or graph learning, suffer from inadequate cross-modal alignment and struggle to capture the cross-item relations for cold-start items. Multimodal pre-train models could be the potential solutions given their promising performance on various multimodal downstream tasks. However, the cross-item relations have been under-explored in the current multimodal pre-train models. To bridge this gap, we propose a novel and simple framework Cross-Item Relational Pre-training (CIRP) for item representation learning in product bundling. Specifically, we employ a multimodal encoder to generate image and text representations. Then we leverage both the cross-item contrastive loss (CIC) and individual item's image-text contrastive loss (ITC) as the pre-train objectives. Our method seeks to integrate cross-item relation modeling capability into the multimodal encoder, while preserving the in-depth aligned multimodal semantics. Therefore, even for cold-start items that have no relations, their representations are still relation-aware. Furthermore, to eliminate the potential noise and reduce the computational cost, we harness a relation pruning module to remove the noisy and redundant relations. We apply the item representations extracted by CIRP to the product bundling model ItemKNN, and experiments on three e-commerce datasets demonstrate that CIRP outperforms various leading representation learning methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51487414",
                    "name": "Yunshan Ma"
                },
                {
                    "authorId": "2118918709",
                    "name": "Y. He"
                },
                {
                    "authorId": "2294573030",
                    "name": "Wenjun Zhong"
                },
                {
                    "authorId": "2260296919",
                    "name": "Xiang Wang"
                },
                {
                    "authorId": "2297672250",
                    "name": "Roger Zimmermann"
                },
                {
                    "authorId": "2270722858",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "23f2776a074987335e3ba54a630add292d0bd390",
            "title": "Filter-based Stance Network for Rumor Verification",
            "abstract": "Rumor verification on social media aims to identify the truth value of a rumor, which is important to decrease the detrimental public effects. A rumor might arouse heated discussions and replies, conveying different stances of users that could be helpful in identifying the rumor. Thus, several works have been proposed to verify a rumor by modelling its entire stance sequence in the time domain. However, these works ignore that such a stance sequence could be decomposed into controversies with different intensities, which could be used to cluster the stance sequences with the same consensus. In addition, the existing stance extractors fail to consider both the impact of all previously posted tweets and the reply chain on obtaining the stance of a new reply. To address the above problems, in this article, we propose a novel stance-based network to aggregate the controversies of the stance sequence for rumor verification, termed Filter-based Stance Network (FSNet). As controversies with different intensities are reflected as the different changes of stances, it is convenient to represent different controversies in the frequency domain, but it is hard in the time domain. Our proposed FSNet decomposes the stance sequence into multiple controversies in the frequency domain and obtains the weighted aggregation of them. Specifically, FSNet consists of two modules: the stance extractor and the filter block. To obtain better stance features toward the source, the stance extractor contains two stages. In the first stage, the tweet representation of each reply is obtained by aggregating information from all previously posted tweets in a conversation. Then, the features of stance toward the source, i.e., rumor-aware stance, are extracted with the reply chains in the second stage. In the filter block module, a rumor-aware stance sequence is constructed by sorting all the tweets of a conversation in chronological order. Fourier Transform thereafter is employed to convert the stance sequence into the frequency domain, where different frequency components reflect controversies of different intensities. Finally, a frequency filter is applied to explore the different contributions of controversies. We supervise our FSNet with both stance labels and rumor labels to strengthen the relations between rumor veracity and crowd stances. Extensive experiments on two benchmark datasets demonstrate that our model substantially outperforms all the baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265514645",
                    "name": "Jun Li"
                },
                {
                    "authorId": "2054618348",
                    "name": "Yi Bin"
                },
                {
                    "authorId": "2109267019",
                    "name": "Yunshan Ma"
                },
                {
                    "authorId": "2258938374",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "2192186592",
                    "name": "Zi Huang"
                },
                {
                    "authorId": "2270722858",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "246ced9521c93e9f267a8f6c370b0b8422757770",
            "title": "FashionReGen: LLM-Empowered Fashion Report Generation",
            "abstract": "Fashion analysis refers to the process of examining and evaluating trends, styles, and elements within the fashion industry to understand and interpret its current state, generating fashion reports. It is traditionally performed by fashion professionals based on their expertise and experience, which requires high labour cost and may also produce biased results for relying heavily on a small group of people. In this paper, to tackle the Fashion Report Generation (FashionReGen) task, we propose an intelligent Fashion Analyzing and Reporting system based the advanced Large Language Models (LLMs), debbed as GPT-FAR. Specifically, it tries to deliver FashionReGen based on effective catwalk analysis, the proposed GPT-FAR system is equipped with several key procedures, namely, catwalk understanding, collective organization and analysis, and report generation. By posing and exploring such an open-ended, complex and domain-specific task of FashionReGen, it is able to test the general capability of LLMs in fashion domain. It also inspires the explorations of more high-level tasks with industrial significance in other domains. Video illustration and more materials of GPT-FAR can be found in https://github.com/CompFashion/FashionReGen.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "65768925",
                    "name": "Yujuan Ding"
                },
                {
                    "authorId": "2109267019",
                    "name": "Yunshan Ma"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2290864893",
                    "name": "Yige Yao"
                },
                {
                    "authorId": "2270722858",
                    "name": "Tat-Seng Chua"
                },
                {
                    "authorId": "2254366521",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "2cdc532135d0aba7655b95690ab42f9f2b527066",
            "title": "Contrastive Pre-training for Deep Session Data Understanding",
            "abstract": "Session data has been widely used for understanding user's behavior in e-commerce. Researchers are trying to leverage session data for different tasks, such as purchase intention prediction, remaining length prediction, recommendation, etc., as it provides context clues about the user's dynamic interests. However, online shopping session data is semi-structured and complex in nature, which contains both unstructured textual data about the products, search queries, and structured user action sequences. Most existing works focus on leveraging the coarse-grained item sequences for specific tasks, while largely ignore the fine-grained information from text and user action details. In this work, we delve into deep session data understanding via scrutinizing the various clues inside the rich information in user sessions. Specifically, we propose to pre-train a general-purpose User Behavior Model (UBM) over large-scale session data with rich details, such as product title, attributes and various kinds of user actions. A two-stage pre-training scheme is introduced to encourage the model to self-learn from various augmentations with contrastive learning objectives, which spans different granularity levels of session data. Then the well-trained session understanding model can be easily fine-tuned for various downstream tasks. Extensive experiments show that UBM better captures the complex intra-item semantic relations, inter-item connections and inter-interaction dependencies, leading to large performance gains as compared to the baselines on several downstream tasks. And it also demonstrates strong robustness when data is sparse.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2290032024",
                    "name": "Zixuan Li"
                },
                {
                    "authorId": "32781973",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "51487414",
                    "name": "Yunshan Ma"
                },
                {
                    "authorId": "2270722858",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "425b541ff4168e921c54ca9c3b449c1fcf985c55",
            "title": "A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting",
            "abstract": "Recently, Large Language Models (LLMs) have demonstrated great potential in various data mining tasks, such as knowledge question answering, mathematical reasoning, and commonsense reasoning. However, the reasoning capability of LLMs on temporal event forecasting has been under-explored. To systematically investigate their abilities in temporal event forecasting, we conduct a comprehensive evaluation of LLM-based methods for temporal event forecasting. Due to the lack of a high-quality dataset that involves both graph and textual data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on this dataset, we design a series of baseline methods, characterized by various input formats and retrieval augmented generation(RAG) modules. From extensive experiments, we find that directly integrating raw texts into the input of LLMs does not enhance zero-shot extrapolation performance. In contrast, incorporating raw texts in specific complex events and fine-tuning LLMs significantly improves performance. Moreover, enhanced with retrieval modules, LLM can effectively capture temporal relational patterns hidden in historical events. Meanwhile, issues such as popularity bias and the long-tail problem still persist in LLMs, particularly in the RAG-based method. These findings not only deepen our understanding of LLM-based event forecasting methods but also highlight several promising research directions.We consider that this comprehensive evaluation, along with the identified research opportunities, will significantly contribute to future research on temporal event forecasting through LLMs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280398884",
                    "name": "He Chang"
                },
                {
                    "authorId": "2269465569",
                    "name": "Chenchen Ye"
                },
                {
                    "authorId": "9168351",
                    "name": "Zhulin Tao"
                },
                {
                    "authorId": "2311656065",
                    "name": "Jie Wu"
                },
                {
                    "authorId": "2311788229",
                    "name": "Zhengmao Yang"
                },
                {
                    "authorId": "51487414",
                    "name": "Yunshan Ma"
                },
                {
                    "authorId": "2280390385",
                    "name": "Xianglin Huang"
                },
                {
                    "authorId": "2270722858",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "4c108232093904ed59954f186a2c146411bb0011",
            "title": "Harnessing Large Language Models for Multimodal Product Bundling",
            "abstract": "Product bundling provides clients with a strategic combination of individual items. And it has gained significant attention in recent years as a fundamental prerequisite for online services. Recent methods utilize multimodal information through sophisticated extractors for bundling, but remain limited by inferior semantic understanding, the restricted scope of knowledge, and an inability to handle cold-start issues. Despite the extensive knowledge and complex reasoning capabilities of large language models (LLMs), their direct utilization fails to process multimodalities and exploit their knowledge for multimodal product bundling. Adapting LLMs for this purpose involves demonstrating the synergies among different modalities and designing an effective optimization strategy for bundling, which remains challenging. To this end, we introduce Bundle-LLM to bridge the gap between LLMs and product bundling tasks. Specifically, we utilize a hybrid item tokenization to integrate multimodal information, where a simple yet powerful multimodal fusion module followed by a trainable projector embeds all non-textual features into a single token. This module not only explicitly exhibits the interplays among modalities but also shortens the prompt length, thereby boosting efficiency. By designing a prompt template, we formulate product bundling as a multiple-choice question given candidate items. Furthermore, we adopt progressive optimization strategy to fine-tune the LLMs for disentangled objectives, achieving effective product bundling capability with comprehensive multimodal semantic understanding. Extensive experiments on four datasets from two application domains show that our approach outperforms a range of state-of-the-art (SOTA) methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2174542939",
                    "name": "Xiaohao Liu"
                },
                {
                    "authorId": "2311656065",
                    "name": "Jie Wu"
                },
                {
                    "authorId": "9168351",
                    "name": "Zhulin Tao"
                },
                {
                    "authorId": "51487414",
                    "name": "Yunshan Ma"
                },
                {
                    "authorId": "2279771878",
                    "name": "Yinwei Wei"
                },
                {
                    "authorId": "2270722858",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "593d3c49cd654fa2b0879ba9ea9a71065e2b9559",
            "title": "A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning",
            "abstract": "This paper presents a pilot study aimed at introducing multi-agent debate into multimodal reasoning. The study addresses two key challenges: the trivialization of opinions resulting from excessive summarization and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are confined to a blueprint graph to prevent opinion trivi-alization through world-level summarization. Moreover, by storing evidence in branches within the graph, BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate BDoG, achieving state-of-the-art results in Science QA and MMBench with significant improvements over previous methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "150068355",
                    "name": "Changmeng Zheng"
                },
                {
                    "authorId": "2293172458",
                    "name": "Dayong Liang"
                },
                {
                    "authorId": "2293233081",
                    "name": "Wengyu Zhang"
                },
                {
                    "authorId": "2115493866",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2270722858",
                    "name": "Tat-Seng Chua"
                },
                {
                    "authorId": "2293397899",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "5aad568d4f02b09af3c282b1f4c20ee0993bc2e6",
            "title": "Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding",
            "abstract": "The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events. We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2304613417",
                    "name": "Zhihan Zhang"
                },
                {
                    "authorId": "2258806194",
                    "name": "Yixin Cao"
                },
                {
                    "authorId": "2269465569",
                    "name": "Chenchen Ye"
                },
                {
                    "authorId": "51487414",
                    "name": "Yunshan Ma"
                },
                {
                    "authorId": "32781973",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "2270722858",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "868d26fe58620d9e5ef41c7089b2454be49da422",
            "title": "Smart Fitting Room: A Generative Approach to Matching-aware Virtual Try-On",
            "abstract": "In current virtual try-on tasks, only the effect of clothing worn on a person is depicted. In practical applications, users still need to select suitable clothing from a vast array of individual clothing items",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2282237274",
                    "name": "Mingzhe Yu"
                },
                {
                    "authorId": "2109267019",
                    "name": "Yunshan Ma"
                },
                {
                    "authorId": "2273764554",
                    "name": "Lei Wu"
                },
                {
                    "authorId": "2282134870",
                    "name": "Kai Cheng"
                },
                {
                    "authorId": "2281908811",
                    "name": "Xue Li"
                },
                {
                    "authorId": "2281868854",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "2270722858",
                    "name": "Tat-Seng Chua"
                }
            ]
        }
    ]
}