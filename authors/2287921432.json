{
    "authorId": "2287921432",
    "papers": [
        {
            "paperId": "5a5f74204ed577bd52e0b1158ee5ab1153958c24",
            "title": "CoDi: Conversational Distillation for Grounded Question Answering",
            "abstract": "Distilling conversational skills into Small Language Models (SLMs) with approximately 1 billion parameters presents significant challenges. Firstly, SLMs have limited capacity in their model parameters to learn extensive knowledge compared to larger models. Secondly, high-quality conversational datasets are often scarce, small, and domain-specific. Addressing these challenges, we introduce a novel data distillation framework named CoDi (short for Conversational Distillation, pronounced\"Cody\"), allowing us to synthesize large-scale, assistant-style datasets in a steerable and diverse manner. Specifically, while our framework is task agnostic at its core, we explore and evaluate the potential of CoDi on the task of conversational grounded reasoning for question answering. This is a typical on-device scenario for specialist SLMs, allowing for open-domain model responses, without requiring the model to\"memorize\"world knowledge in its limited weights. Our evaluations show that SLMs trained with CoDi-synthesized data achieve performance comparable to models trained on human-annotated data in standard metrics. Additionally, when using our framework to generate larger datasets from web data, our models surpass larger, instruction-tuned models in zero-shot conversational grounded reasoning tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287921432",
                    "name": "Patrick Huber"
                },
                {
                    "authorId": "2287921908",
                    "name": "Arash Einolghozati"
                },
                {
                    "authorId": "2304748670",
                    "name": "Rylan Conway"
                },
                {
                    "authorId": "2249757531",
                    "name": "Kanika Narang"
                },
                {
                    "authorId": "2316670006",
                    "name": "Matt Smith"
                },
                {
                    "authorId": "2316562238",
                    "name": "Waqar Nayyar"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "2284079232",
                    "name": "Ahmed Aly"
                },
                {
                    "authorId": "1519979046",
                    "name": "Akshat Shrivastava"
                }
            ]
        },
        {
            "paperId": "685e5d9cbe177ec50129a6ca1ffb912636bb49d2",
            "title": "Small But Funny: A Feedback-Driven Approach to Humor Distillation",
            "abstract": "The emergence of Large Language Models (LLMs) has brought to light promising language generation capabilities, particularly in performing tasks like complex reasoning and creative writing. Consequently, distillation through imitation of teacher responses has emerged as a popular technique to transfer knowledge from LLMs to more accessible, Small Language Models (SLMs). While this works well for simpler tasks, there is a substantial performance gap on tasks requiring intricate language comprehension and creativity, such as humor generation. We hypothesize that this gap may stem from the fact that creative tasks might be hard to learn by imitation alone and explore whether an approach, involving supplementary guidance from the teacher, could yield higher performance. To address this, we study the effect of assigning a dual role to the LLM - as a\"teacher\"generating data, as well as a\"critic\"evaluating the student's performance. Our experiments on humor generation reveal that the incorporation of feedback significantly narrows the performance gap between SLMs and their larger counterparts compared to merely relying on imitation. As a result, our research highlights the potential of using feedback as an additional dimension to data when transferring complex language abilities via distillation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152650432",
                    "name": "Sahithya Ravi"
                },
                {
                    "authorId": "2287921432",
                    "name": "Patrick Huber"
                },
                {
                    "authorId": "1519979046",
                    "name": "Akshat Shrivastava"
                },
                {
                    "authorId": "2287921555",
                    "name": "Aditya Sagar"
                },
                {
                    "authorId": "2284079232",
                    "name": "Ahmed Aly"
                },
                {
                    "authorId": "2121375800",
                    "name": "V. Shwartz"
                },
                {
                    "authorId": "2287921908",
                    "name": "Arash Einolghozati"
                }
            ]
        },
        {
            "paperId": "723a6340ee641e190b22bb47455d05e3b4237179",
            "title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling",
            "abstract": "Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We have made the code publicly available at https://github.com/facebookresearch/FnCTOD",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2284639678",
                    "name": "Zhiyu Zoey Chen"
                },
                {
                    "authorId": "2284594951",
                    "name": "Mike Ross"
                },
                {
                    "authorId": "2287921432",
                    "name": "Patrick Huber"
                },
                {
                    "authorId": "2256132624",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2146396528",
                    "name": "Zhaojiang Lin"
                },
                {
                    "authorId": "2215596266",
                    "name": "Xin Luna Dong"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "2258077708",
                    "name": "Xifeng Yan"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                }
            ]
        }
    ]
}