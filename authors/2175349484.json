{
    "authorId": "2175349484",
    "papers": [
        {
            "paperId": "d1e58774877071848dac227c08eb65843c190beb",
            "title": "CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process",
            "abstract": "Identifying the underlying time-delayed latent causal processes in sequential data is vital for grasping temporal dynamics and making downstream reasoning. While some recent methods can robustly identify these latent causal variables, they rely on strict assumptions about the invertible generation process from latent variables to observed data. However, these assumptions are often hard to satisfy in real-world applications containing information loss. For instance, the visual perception process translates a 3D space into 2D images, or the phenomenon of persistence of vision incorporates historical data into current perceptions. To address this challenge, we establish an identifiability theory that allows for the recovery of independent latent components even when they come from a nonlinear and non-invertible mix. Using this theory as a foundation, we propose a principled approach, CaRiNG, to learn the CAusal RepresentatIon of Non-invertible Generative temporal data with identifiability guarantees. Specifically, we utilize temporal context to recover lost latent information and apply the conditions in our theory to guide the training process. Through experiments conducted on synthetic datasets, we validate that our CaRiNG method reliably identifies the causal process, even when the generation process is non-invertible. Moreover, we demonstrate that our approach considerably improves temporal understanding and reasoning in practical applications.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2155315836",
                    "name": "Guan-Hong Chen"
                },
                {
                    "authorId": "2281910510",
                    "name": "Yifan Shen"
                },
                {
                    "authorId": "2144321586",
                    "name": "Zhenhao Chen"
                },
                {
                    "authorId": "2262495949",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "2116970666",
                    "name": "Yuewen Sun"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "2297195138",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                }
            ]
        },
        {
            "paperId": "0ef4993978f60b5065bac687746bc9857e2c99f7",
            "title": "Partial Identifiability for Domain Adaptation",
            "abstract": "Unsupervised domain adaptation is critical to many real-world applications where label information is unavailable in the target domain. In general, without further assumptions, the joint distribution of the features and the label is not identifiable in the target domain. To address this issue, we rely on the property of minimal changes of causal mechanisms across domains to minimize unnecessary influences of distribution shifts. To encode this property, we first formulate the data-generating process using a latent variable model with two partitioned latent subspaces: invariant components whose distributions stay the same across domains and sparse changing components that vary across domains. We further constrain the domain shift to have a restrictive influence on the changing components. Under mild conditions, we show that the latent variables are partially identifiable, from which it follows that the joint distribution of data and labels in the target domain is also identifiable. Given the theoretical insights, we propose a practical domain adaptation framework called iMSDA. Extensive experimental results reveal that iMSDA outperforms state-of-the-art domain adaptation algorithms on benchmark datasets, demonstrating the effectiveness of our framework.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2069275317",
                    "name": "Lingjing Kong"
                },
                {
                    "authorId": "25106675",
                    "name": "Shaoan Xie"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "8499589",
                    "name": "Yujia Zheng"
                },
                {
                    "authorId": "2155315836",
                    "name": "Guan-Hong Chen"
                },
                {
                    "authorId": "6616282",
                    "name": "P. Stojanov"
                },
                {
                    "authorId": "32662204",
                    "name": "Victor Akinwande"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                }
            ]
        },
        {
            "paperId": "437cfee2a7f7beadf09ad712f71b3265740e44a0",
            "title": "Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment",
            "abstract": "Large-scale pre-trained Vision Language Models (VLMs) have proven effective for zero-shot classification. Despite the success, most traditional VLMs-based methods are restricted by the assumption of partial source supervision or ideal target vocabularies, which rarely satisfy the open-world scenario. In this paper, we aim at a more challenging setting, Realistic Zero-Shot Classification, which assumes no annotation but instead a broad vocabulary. To address the new problem, we propose the Self Structural Semantic Alignment (S3A) framework, which extracts the structural semantic information from unlabeled data while simultaneously self-learning. Our S3A framework adopts a unique Cluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR algorithm includes iterative clustering on images, voting within each cluster to identify initial class candidates from the vocabulary, generating discriminative prompts with large language models to discern confusing candidates, and realigning images and the vocabulary as structural semantic alignment. Finally, we propose to self-train the CLIP image encoder with both individual and structural semantic alignment through a teacher-student learning strategy. Our comprehensive experiments across various generic and fine-grained benchmarks demonstrate that the S3A method substantially improves over existing VLMs-based approaches, achieving a more than 15% accuracy improvement over CLIP on average. Our codes, models, and prompts are publicly released at https://github.com/sheng-eatamath/S3A.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2161256018",
                    "name": "Shengxiang Zhang"
                },
                {
                    "authorId": "40894826",
                    "name": "Muzammal Naseer"
                },
                {
                    "authorId": "9230423",
                    "name": "Guangyi Chen"
                },
                {
                    "authorId": "145314568",
                    "name": "Zhiqiang Shen"
                },
                {
                    "authorId": "2179960157",
                    "name": "Salman A. Khan"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                },
                {
                    "authorId": "2358803",
                    "name": "F. Khan"
                }
            ]
        },
        {
            "paperId": "4ac14a3d12e027056e57c5a05b0414b6b0ff7002",
            "title": "Deep Dag Learning of Effective Brain Connectivity for FMRI Analysis",
            "abstract": "Functional magnetic resonance imaging (fMRI) has become one of the most common imaging modalities for brain function analysis. Recently, graph neural networks (GNN) have been adopted for fMRI analysis with superior performance. Unfortunately, traditional functional brain networks are mainly constructed based on similarities among region of interests (ROIs), which are noisy and can lead to inferior results for GNN models. To better adapt GNNs for fMRI analysis, we propose DABNet, a Deep DAG learning framework based on Brain Networks for fMRI analysis. DABNet adopts a brain network generator module, which harnesses the DAG learning approach to transform the raw time-series into effective brain connectivities. Experiments on two fMRI datasets demonstrate the efficacy of DABNet. The generated brain networks also highlight the prediction-related brain regions and thus provide interpretations for predictions.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1633124736",
                    "name": "Yue Yu"
                },
                {
                    "authorId": "2052319438",
                    "name": "Xuan Kan"
                },
                {
                    "authorId": "2112821580",
                    "name": "Hejie Cui"
                },
                {
                    "authorId": "47462790",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "8499589",
                    "name": "Yujia Zheng"
                },
                {
                    "authorId": "19214393",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "2653121",
                    "name": "Yanqiao Zhu"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                },
                {
                    "authorId": "41034714",
                    "name": "Razieh Nabi"
                },
                {
                    "authorId": "2153202261",
                    "name": "Ying Guo"
                },
                {
                    "authorId": "40422511",
                    "name": "C. Zhang"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                }
            ]
        },
        {
            "paperId": "620d54c32923db997d9312b1d631393a740d2842",
            "title": "GAIN: On the Generalization of Instructional Action Understanding",
            "abstract": "Despite the great success achieved in instructional action understanding by deep learning and mountainous data, deploying trained models to the unseen environment still remains a great challenge, since it requires strong generalizability of models from in-distribution training data to out-of-distribution (OOD) data. In this paper, we introduce a benchmark, named GAIN, to analyze the GeneralizAbility of INstructional action understanding models. In GAIN, we reassemble steps of existing instructional video training datasets to construct the OOD tasks and then collect the corresponding videos. We evaluate the generalizability of models trained on in-distribution datasets with the performance on OOD videos and observe a significant performance drop. We further propose a simple yet effective approach, which cuts off the excessive contextual dependency of action steps by performing causal inference, to provide a potential direction for enhancing the OOD generalizability. In the experiments, we show that this simple approach can improve several baselines on both instructional action segmentation and detection tasks. We expect the introduction of the GAIN dataset will promote future in-depth research on the generalization of instructional video understanding. The project page is https://jun-long-li.github.io/GAIN.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9230423",
                    "name": "Guangyi Chen"
                },
                {
                    "authorId": "35299091",
                    "name": "Yansong Tang"
                },
                {
                    "authorId": "2150472556",
                    "name": "Jinan Bao"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                },
                {
                    "authorId": "48128428",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "1697700",
                    "name": "Jiwen Lu"
                }
            ]
        },
        {
            "paperId": "7ab6c5a5f9d83d04a147ee152aff944a16eca6ce",
            "title": "Subspace Identification for Multi-Source Domain Adaptation",
            "abstract": "Multi-source domain adaptation (MSDA) methods aim to transfer knowledge from multiple labeled source domains to an unlabeled target domain. Although current methods achieve target joint distribution identifiability by enforcing minimal changes across domains, they often necessitate stringent conditions, such as an adequate number of domains, monotonic transformation of latent variables, and invariant label distributions. These requirements are challenging to satisfy in real-world applications. To mitigate the need for these strict assumptions, we propose a subspace identification theory that guarantees the disentanglement of domain-invariant and domain-specific variables under less restrictive constraints regarding domain numbers and transformation properties, thereby facilitating domain adaptation by minimizing the impact of domain shifts on invariant variables. Based on this theory, we develop a Subspace Identification Guarantee (SIG) model that leverages variational inference. Furthermore, the SIG model incorporates class-aware conditional alignment to accommodate target shifts where label distributions change with the domains. Experimental results demonstrate that our SIG model outperforms existing MSDA techniques on various benchmark datasets, highlighting its effectiveness in real-world applications.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "46947549",
                    "name": "Zijian Li"
                },
                {
                    "authorId": "39913331",
                    "name": "Ruichu Cai"
                },
                {
                    "authorId": "2155315836",
                    "name": "Guan-Hong Chen"
                },
                {
                    "authorId": "2257009969",
                    "name": "Boyang Sun"
                },
                {
                    "authorId": "145586380",
                    "name": "Z. Hao"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                }
            ]
        },
        {
            "paperId": "99dadc987c90626e3c0b0adcf694b6a4b5bd3901",
            "title": "Temporally Disentangled Representation Learning under Unknown Nonstationarity",
            "abstract": "In unsupervised causal representation learning for sequential data with time-delayed latent causal influences, strong identifiability results for the disentanglement of causally-related latent variables have been established in stationary settings by leveraging temporal structure. However, in nonstationary setting, existing work only partially addressed the problem by either utilizing observed auxiliary variables (e.g., class labels and/or domain indexes) as side information or assuming simplified latent causal dynamics. Both constrain the method to a limited range of scenarios. In this study, we further explored the Markov Assumption under time-delayed causally related process in nonstationary setting and showed that under mild conditions, the independent latent components can be recovered from their nonlinear mixture up to a permutation and a component-wise transformation, without the observation of auxiliary variables. We then introduce NCTRL, a principled estimation framework, to reconstruct time-delayed latent causal variables and identify their relations from measured sequential data only. Empirical evaluations demonstrated the reliable identification of time-delayed latent causal influences, with our methodology substantially outperforming existing baselines that fail to exploit the nonstationarity adequately and then, consequently, cannot distinguish distribution shifts.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2262495949",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "2087699735",
                    "name": "Weiran Yao"
                },
                {
                    "authorId": "2166103953",
                    "name": "Yewen Fan"
                },
                {
                    "authorId": "2262501910",
                    "name": "Xinshuai Dong"
                },
                {
                    "authorId": "2155315836",
                    "name": "Guan-Hong Chen"
                },
                {
                    "authorId": "9200530",
                    "name": "Juan Carlos Niebles"
                },
                {
                    "authorId": "2262446774",
                    "name": "Eric P. Xing"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                }
            ]
        },
        {
            "paperId": "9c5ed6249d9bf02148074cfa29c3d89e2db48540",
            "title": "Feature Expansion for Graph Neural Networks",
            "abstract": "Graph neural networks aim to learn representations for graph-structured data and show impressive performance, particularly in node classification. Recently, many methods have studied the representations of GNNs from the perspective of optimization goals and spectral graph theory. However, the feature space that dominates representation learning has not been systematically studied in graph neural networks. In this paper, we propose to fill this gap by analyzing the feature space of both spatial and spectral models. We decompose graph neural networks into determined feature spaces and trainable weights, providing the convenience of studying the feature space explicitly using matrix space analysis. In particular, we theoretically find that the feature space tends to be linearly correlated due to repeated aggregations. Motivated by these findings, we propose 1) feature subspaces flattening and 2) structural principal components to expand the feature space. Extensive experiments verify the effectiveness of our proposed more comprehensive feature space, with comparable inference time to the baseline, and demonstrate its efficient convergence capability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188389873",
                    "name": "Jiaqi Sun"
                },
                {
                    "authorId": "2217098681",
                    "name": "Lin Zhang"
                },
                {
                    "authorId": "2155315836",
                    "name": "Guan-Hong Chen"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                },
                {
                    "authorId": "2153917758",
                    "name": "Peng Xu"
                },
                {
                    "authorId": "3001727",
                    "name": "Yujiu Yang"
                }
            ]
        },
        {
            "paperId": "9f0630ff9d256ab89248f87cf2bdb7cee5740d4c",
            "title": "Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer",
            "abstract": "Video-language pre-trained models have shown remarkable success in guiding video question-answering (VideoQA) tasks. However, due to the length of video sequences, training large-scale video-based models incurs considerably higher costs than training image-based ones. This motivates us to leverage the knowledge from image-based pretraining, despite the obvious gaps between image and video domains. To bridge these gaps, in this paper, we propose Tem-adapter, which enables the learning of temporal dynamics and complex semantics by a visual Temporal Aligner and a textual Semantic Aligner. Unlike conventional pretrained knowledge adaptation methods that only concentrate on the downstream task objective, the Temporal Aligner introduces an extra language-guided autoregressive task aimed at facilitating the learning of temporal dependencies, with the objective of predicting future states based on historical clues and language guidance that describes event progression. Besides, to reduce the semantic gap and adapt the textual representation for better event description, we introduce a Semantic Aligner that first designs a template to fuse question and answer pairs as event descriptions and then learns a Transformer decoder with the whole video sequence as guidance for refinement. We evaluate Tem-adapter and different pre-train transferring methods on two VideoQA benchmarks, and the significant performance improvement demonstrates the effectiveness of our method. 1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9230423",
                    "name": "Guangyi Chen"
                },
                {
                    "authorId": "2218494165",
                    "name": "Xiao Liu"
                },
                {
                    "authorId": "2749191",
                    "name": "Guangrun Wang"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                },
                {
                    "authorId": "2231696227",
                    "name": "Philip H.S.Torr"
                },
                {
                    "authorId": "2144524205",
                    "name": "Xiaoping Zhang"
                },
                {
                    "authorId": "35299091",
                    "name": "Yansong Tang"
                }
            ]
        },
        {
            "paperId": "dcb4f2b9b0e6da0d629878d1ad0469aee3df2020",
            "title": "Understanding Masked Autoencoders via Hierarchical Latent Variable Models",
            "abstract": "Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2069275317",
                    "name": "Lingjing Kong"
                },
                {
                    "authorId": "1384374825",
                    "name": "Martin Q. Ma"
                },
                {
                    "authorId": "2155315836",
                    "name": "Guan-Hong Chen"
                },
                {
                    "authorId": "143977260",
                    "name": "E. Xing"
                },
                {
                    "authorId": "1784472",
                    "name": "Yuejie Chi"
                },
                {
                    "authorId": "49933077",
                    "name": "Louis-Philippe Morency"
                },
                {
                    "authorId": "2175349484",
                    "name": "Kun Zhang"
                }
            ]
        }
    ]
}