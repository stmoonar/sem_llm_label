{
    "authorId": "1405923139",
    "papers": [
        {
            "paperId": "245a62577d952a69b1a97f1fa735f595fb73d440",
            "title": "\"Don't Step on My Toes\": Resolving Editing Conflicts in Real-Time Collaboration in Computational Notebooks",
            "abstract": "Real-time collaborative editing in computational notebooks can improve the efficiency of teamwork for data scientists. However, working together through synchronous editing of notebooks introduces new challenges. Data scientists may inadvertently interfere with each others' work by altering the shared codebase and runtime state if they do not set up a social protocol for working together and monitoring their collaborators' progress. In this paper, we propose a real-time collaborative editing model for resolving conflict edits in computational notebooks that introduces three levels of edit protection to help collaborators avoid introducing errors to both the program source code and changes to the runtime state.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1405923139",
                    "name": "A. Wang"
                },
                {
                    "authorId": "2295680868",
                    "name": "Zihan Wu"
                },
                {
                    "authorId": "2214753448",
                    "name": "Christopher Brooks"
                },
                {
                    "authorId": "2295671974",
                    "name": "Steve Oney"
                }
            ]
        },
        {
            "paperId": "4be11057cbfe6c712a63dc04be94d3378ce67864",
            "title": "Colaroid: A Literate Programming Approach for Authoring Explorable Multi-Stage Tutorials",
            "abstract": "Multi-stage programming tutorials are key learning resources for programmers, using progressive incremental steps to teach them how to build larger software systems. A good multi-stage tutorial describes the code clearly, explains the rationale and code changes for each step, and allows readers to experiment as they work through the tutorial. In practice, it is time-consuming for authors to create tutorials with these attributes. In this paper, we introduce Colaroid, an interactive authoring tool for creating high quality multi-stage tutorials. Colaroid tutorials are augmented computational notebooks, where snippets and outputs represent a snapshot of a project, with source code differences highlighted, complete source code context for each snippet, and the ability to load and tinker with any stage of the project in a linked IDE. In two laboratory studies, we found Colaroid makes it easy to create multi-stage tutorials, while offering advantages to readers compared to video and web-based tutorials.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1405923139",
                    "name": "A. Wang"
                },
                {
                    "authorId": "144468073",
                    "name": "Andrew Head"
                },
                {
                    "authorId": "2143853763",
                    "name": "Ashley Ge Zhang"
                },
                {
                    "authorId": "3395961",
                    "name": "Steve Oney"
                },
                {
                    "authorId": "2214753448",
                    "name": "Christopher Brooks"
                }
            ]
        },
        {
            "paperId": "59cad9487a3632a9fb697a9d58c2b30040e9366d",
            "title": "Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis",
            "abstract": "Data science is characterized by evolution: since data science is exploratory, results evolve from moment to moment; since it can be collaborative, results evolve as the work changes hands. While existing tools help data scientists track changes in code, they provide less support for understanding the iterative changes that the code produces in the data. We explore the idea of visualizing differences in datasets as a core feature of exploratory data analysis, a concept we call Diff in the Loop (DITL). We evaluated DITL in a user study with 16 professional data scientists and found it helped them understand the implications of their actions when manipulating data. We summarize these findings and discuss how the approach can be generalized to different data science workflows.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1405923139",
                    "name": "A. Wang"
                },
                {
                    "authorId": "103260170",
                    "name": "Will Epperson"
                },
                {
                    "authorId": "1710751",
                    "name": "R. Deline"
                },
                {
                    "authorId": "2311676",
                    "name": "S. Drucker"
                }
            ]
        },
        {
            "paperId": "7a6836d67156228f6693d14c67e1b7e97ddc04c3",
            "title": "Telling Stories from Computational Notebooks: AI-Assisted Presentation Slides Creation for Presenting Data Science Work",
            "abstract": "Creating presentation slides is a critical but time-consuming task for data scientists. While researchers have proposed many AI techniques to lift data scientists\u2019 burden on data preparation and model selection, few have targeted the presentation creation task. Based on the needs identified from a formative study, this paper presents NB2Slides, an AI system that facilitates users to compose presentations of their data science work. NB2Slides uses deep learning methods as well as example-based prompts to generate slides from computational notebooks, and take users\u2019 input (e.g., audience background) to structure the slides. NB2Slides also provides an interactive visualization that links the slides with the notebook to help users further edit the slides. A follow-up user evaluation with 12 data scientists shows that participants believed NB2Slides can improve efficiency and reduces the complexity of creating slides. Yet, participants questioned the future of full automation and suggested a human-AI collaboration paradigm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153619689",
                    "name": "Chengbo Zheng"
                },
                {
                    "authorId": "36743361",
                    "name": "Dakuo Wang"
                },
                {
                    "authorId": "1405923139",
                    "name": "A. Wang"
                },
                {
                    "authorId": "2159536581",
                    "name": "Xiaojuan Ma"
                }
            ]
        },
        {
            "paperId": "21d6bb1a79a69c207a5d1187ebfce5150b58e441",
            "title": "Graph-Augmented Code Summarization in Computational Notebooks",
            "abstract": "Computational notebooks allow data scientists to express their ideas through a combination of code and documentation. However, data scientists often pay attention only to the code and neglect the creation of the documentation in a notebook. In this work, we present a human-centered automation system, Themisto, that can support users to easily create documentation via three approaches: 1) We have developed and reported a GNN-augmented code documentation generation algorithm in a previous paper, which can generate documentation for a given source code; 2) Themisto also implements a query-based approach to retrieve the online API documentation as the summary for certain types of source code; 3) Lastly, Themistoalso enables a user prompt approach to motivate users to write documentation for some use cases that automation does not work well.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1405923139",
                    "name": "A. Wang"
                },
                {
                    "authorId": "36743361",
                    "name": "Dakuo Wang"
                },
                {
                    "authorId": "14910591",
                    "name": "Xuye Liu"
                },
                {
                    "authorId": "2116666963",
                    "name": "Lingfei Wu"
                }
            ]
        },
        {
            "paperId": "315b453eb889ded846c1685b2cd3ac700f69459c",
            "title": "Documentation Matters: Human-Centered AI System to Assist Data Science Code Documentation in Computational Notebooks",
            "abstract": "Computational notebooks allow data scientists to express their ideas through a combination of code and documentation. However, data scientists often pay attention only to the code, and neglect creating or updating their documentation during quick iterations. Inspired by human documentation practices learned from 80 highly-voted Kaggle notebooks, we design and implement Themisto, an automated documentation generation system to explore how human-centered AI systems can support human data scientists in the machine learning code documentation scenario. Themisto facilitates the creation of documentation via three approaches: a deep-learning-based approach to generate documentation for source code, a query-based approach to retrieve online API documentation for source code, and a user prompt approach to nudge users to write documentation. We evaluated Themisto in a within-subjects experiment with 24 data science practitioners, and found that automated documentation generation techniques reduced the time for writing documentation, reminded participants to document code they would have ignored, and improved participants\u2019 satisfaction with their computational notebook.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1405923139",
                    "name": "A. Wang"
                },
                {
                    "authorId": "36743361",
                    "name": "Dakuo Wang"
                },
                {
                    "authorId": "46178771",
                    "name": "Jaimie Drozdal"
                },
                {
                    "authorId": "1561777696",
                    "name": "Michael J. Muller"
                },
                {
                    "authorId": "2504744",
                    "name": "Soya Park"
                },
                {
                    "authorId": "1927553",
                    "name": "Justin D. Weisz"
                },
                {
                    "authorId": "14910591",
                    "name": "Xuye Liu"
                },
                {
                    "authorId": "3008832",
                    "name": "Lingfei Wu"
                },
                {
                    "authorId": "2391727",
                    "name": "Casey Dugan"
                }
            ]
        },
        {
            "paperId": "43dbd6ec6a7d0a4344f27e7508a9c2a239e92877",
            "title": "What Makes a Well-Documented Notebook? A Case Study of Data Scientists\u2019 Documentation Practices in Kaggle",
            "abstract": "Many data scientists use computational notebooks to test and present their work, as a notebook can weave code and documentation together (computational narrative), and support rapid iteration on code experiments. However, it is not easy to write good documentation in a data science notebook, partially because there is a lack of a corpus of well-documented notebooks as exemplars for data scientists to follow. To cope with this challenge, this work looks at Kaggle \u2014 a large online community for data scientists to host and participate in machine learning competitions \u2014 and considers highly-voted Kaggle notebooks as a proxy for well-documented notebooks. Through a qualitative analysis at both the notebook level and the markdown-cell level, we find these notebooks are indeed well documented in reference to previous literature. Our analysis also reveals nine categories of content that data scientists write in their documentation cells, and these documentation cells often interplay with different stages of the data science lifecycle. We conclude the paper with design implications and future research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1405923139",
                    "name": "A. Wang"
                },
                {
                    "authorId": "36743361",
                    "name": "Dakuo Wang"
                },
                {
                    "authorId": "46178771",
                    "name": "Jaimie Drozdal"
                },
                {
                    "authorId": "14910591",
                    "name": "Xuye Liu"
                },
                {
                    "authorId": "2504744",
                    "name": "Soya Park"
                },
                {
                    "authorId": "3395961",
                    "name": "Steve Oney"
                },
                {
                    "authorId": "47558994",
                    "name": "Christopher A. Brooks"
                }
            ]
        },
        {
            "paperId": "467684107339049b27324bb180a99f713e3c8ecd",
            "title": "PuzzleMe: Leveraging Peer Assessment for In-Class Programming Exercises",
            "abstract": "Peer assessment, as a form of collaborative learning, can engage students in active learning and improve their learning gains. However, current teaching platforms and programming environments provide little support to integrate peer assessment for in-class programming exercises. We identified challenges in conducting such exercises and adopting peer assessment through formative interviews with instructors of introductory programming courses. To address these challenges, we introduce PuzzleMe, a tool to help Computer Science instructors to conduct engaging in-class programming exercises. PuzzleMe leverages peer assessment to support a collaboration model where students provide timely feedback on their peers' work. We propose two assessment techniques tailored to in-class programming exercises: live peer testing and live peer code review. Live peer testing can improve students' code robustness by allowing them to create and share lightweight tests with peers. Live peer code review can improve code understanding by intelligently grouping students to maximize meaningful code reviews. A two-week deployment study revealed that PuzzleMe encourages students to write useful test cases, identify code problems, correct misunderstandings, and learn a diverse set of problem-solving approaches from peers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1405923139",
                    "name": "A. Wang"
                },
                {
                    "authorId": "2144283599",
                    "name": "Yan Chen"
                },
                {
                    "authorId": "152836325",
                    "name": "John Joon Young Chung"
                },
                {
                    "authorId": "47558994",
                    "name": "Christopher A. Brooks"
                },
                {
                    "authorId": "3395961",
                    "name": "Steve Oney"
                }
            ]
        },
        {
            "paperId": "53d35215ef96891b5397644ed6d873004801696e",
            "title": "Facilitating Knowledge Sharing from Domain Experts to Data Scientists for Building NLP Models",
            "abstract": "Data scientists face a steep learning curve in understanding a new domain for which they want to build machine learning (ML) models. While input from domain experts could offer valuable help, such input is often limited, expensive, and generally not in a form readily consumable by a model development pipeline. In this paper, we propose Ziva, a framework to guide domain experts in sharing essential domain knowledge to data scientists for building NLP models. With Ziva, experts are able to distill and share their domain knowledge using domain concept extractors and five types of label justification over a representative data sample. The design of Ziva is informed by preliminary interviews with data scientists, in order to understand current practices of domain knowledge acquisition process for ML development projects. To assess our design, we run a mix-method case-study to evaluate how Ziva can facilitate interaction between domain experts and data scientists. Our results highlight that (1) domain experts are able to use Ziva to provide rich domain knowledge, while maintaining low mental load and stress levels; and (2) data scientists find Ziva\u2019s output helpful for learning essential information about the domain, offering scalability of information, and lowering the burden on domain experts to share knowledge. We conclude this work by experimenting with building NLP models using the Ziva output for our case study.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2504744",
                    "name": "Soya Park"
                },
                {
                    "authorId": "1405923139",
                    "name": "A. Wang"
                },
                {
                    "authorId": "1814905",
                    "name": "B. Kawas"
                },
                {
                    "authorId": "144921048",
                    "name": "Q. Liao"
                },
                {
                    "authorId": "2752280",
                    "name": "David Piorkowski"
                },
                {
                    "authorId": "1994333",
                    "name": "Marina Danilevsky"
                }
            ]
        },
        {
            "paperId": "6e2b1038682cd116b2e38bec19b5721196c41eea",
            "title": "HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks",
            "abstract": "Jupyter notebook allows data scientists to write machine learning code together with its documentation in cells. In this paper, we propose a new task of code documentation generation (CDG) for computational notebooks. In contrast to the previous CDG tasks which focus on generating documentation for single code snippets, in a computational notebook, one documentation in a markdown cell often corresponds to multiple code cells, and these code cells have an inherent structure. We proposed a new model (HAConvGNN) that uses a hierarchical attention mechanism to consider the relevant code cells and the relevant code tokens information when generating the documentation. Tested on a new corpus constructed from well-documented Kaggle notebooks, we show that our model outperforms other baseline models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "14910591",
                    "name": "Xuye Liu"
                },
                {
                    "authorId": "36743361",
                    "name": "Dakuo Wang"
                },
                {
                    "authorId": "1405923139",
                    "name": "A. Wang"
                },
                {
                    "authorId": "3008832",
                    "name": "Lingfei Wu"
                }
            ]
        }
    ]
}