{
    "authorId": "2189476655",
    "papers": [
        {
            "paperId": "f9566cab417a1ca917a59e72cf198ef5d92b187b",
            "title": "Beyond Orthography: Automatic Recovery of Short Vowels and Dialectal Sounds in Arabic",
            "abstract": "This paper presents a novel Dialectal Sound and Vowelization Recovery framework, designed to recognize borrowed and dialectal sounds within phonologically diverse and dialect-rich languages, that extends beyond its standard orthographic sound sets. The proposed framework utilized a quantized sequence of input with(out) continuous pretrained self-supervised representation. We show the efficacy of the pipeline using limited data for Arabic, a dialect-rich language containing more than 22 major dialects. Phonetically correct transcribed speech resources for dialectal Arabic are scarce. Therefore, we introduce ArabVoice15, a first-of-its-kind, curated test set featuring 5 hours of dialectal speech across 15 Arab countries, with phonetically accurate transcriptions, including borrowed and dialect-specific sounds. We described in detail the annotation guideline along with the analysis of the dialectal confusion pairs. Our extensive evaluation includes both subjective -- human perception tests and objective measures. Our empirical results, reported with three test sets, show that with only one and half hours of training data, our model improve character error rate by ~ 7\\% in ArabVoice15 compared to the baseline.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "2261359986",
                    "name": "Ahmed Ali"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                }
            ]
        },
        {
            "paperId": "3627dc4fd36a5f4511ea8de9e07c553efb6e0bce",
            "title": "MyVoice: Arabic Speech Resource Collaboration Platform",
            "abstract": "We introduce MyVoice, a crowdsourcing platform designed to collect Arabic speech to enhance dialectal speech technologies. This platform offers an opportunity to design large dialectal speech datasets; and makes them publicly available. MyVoice allows contributors to select city/country-level fine-grained dialect and record the displayed utterances. Users can switch roles between contributors and annotators. The platform incorporates a quality assurance system that filters out low-quality and spurious recordings before sending them for validation. During the validation phase, contributors can assess the quality of recordings, annotate them, and provide feedback which is then reviewed by administrators. Furthermore, the platform offers flexibility to admin roles to add new data or tasks beyond dialectal speech and word collection, which are displayed to contributors. Thus, enabling collaborative efforts in gathering diverse and large Arabic speech data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2218145245",
                    "name": "Yousseif Elshahawy"
                },
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "145907529",
                    "name": "Ahmed Ali"
                }
            ]
        },
        {
            "paperId": "7e52ffa0ecdaa1b8eb6dc0757de532894bfd24b1",
            "title": "Automatic Pronunciation Assessment - A Review",
            "abstract": "Pronunciation assessment and its application in computer-aided pronunciation training (CAPT) have seen impressive progress in recent years. With the rapid growth in language processing and deep learning over the past few years, there is a need for an updated review. In this paper, we review methods employed in pronunciation assessment for both phonemic and prosodic. We categorize the main challenges observed in prominent research trends, and highlight existing limitations, and available resources. This is followed by a discussion of the remaining challenges and possible directions for future work.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "2261359986",
                    "name": "Ahmed Ali"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                }
            ]
        },
        {
            "paperId": "98b2463a9b3312776c84ecc2a2b0c6bb8d351634",
            "title": "QVoice: Arabic Speech Pronunciation Learning Application",
            "abstract": "This paper introduces a novel Arabic pronunciation learning application QVoice, powered with end-to-end mispronunciation detection and feedback generator module. The application is designed to support non-native Arabic speakers in enhancing their pronunciation skills, while also helping native speakers mitigate any potential influence from regional dialects on their Modern Standard Arabic (MSA) pronunciation. QVoice employs various learning cues to aid learners in comprehending meaning, drawing connections with their existing knowledge of English language, and offers detailed feedback for pronunciation correction, along with contextual examples showcasing word usage. The learning cues featured in QVoice encompass a wide range of meaningful information, such as visualizations of phrases/words and their translations, as well as phonetic transcriptions and transliterations. QVoice provides pronunciation feedback at the character level and assesses performance at the word level.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "1972477667",
                    "name": "Fouad Khnaisser"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "2099539780",
                    "name": "S. Afzal"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                }
            ]
        },
        {
            "paperId": "9f87c8e27a10d71500314e7e21853f5a23efce59",
            "title": "LAraBench: Benchmarking Arabic AI with Large Language Models",
            "abstract": "Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperformed LLMs in zero-shot learning, with a few exceptions. Notably, larger computational models with few-shot learning techniques managed to reduce these performance gaps. Our findings provide valuable insights into the applicability of LLMs for Arabic NLP and speech processing tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1683403",
                    "name": "Ahmed Abdelali"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "2171367840",
                    "name": "Basel Mousi"
                },
                {
                    "authorId": "2466162",
                    "name": "Sabri Boughorbel"
                },
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "2177436744",
                    "name": "Daniel Izham"
                },
                {
                    "authorId": "6415321",
                    "name": "Fahim Dalvi"
                },
                {
                    "authorId": "2762811",
                    "name": "Majd Hawasly"
                },
                {
                    "authorId": "2218353460",
                    "name": "Nizi Nazar"
                },
                {
                    "authorId": "2218145245",
                    "name": "Yousseif Elshahawy"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                },
                {
                    "authorId": "145938140",
                    "name": "Nadir Durrani"
                },
                {
                    "authorId": "1398136050",
                    "name": "Natasa Milic-Frayling"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                }
            ]
        },
        {
            "paperId": "a136c9bec3c3b942d5040cc88a09f814a4f51114",
            "title": "The complementary roles of non-verbal cues for Robust Pronunciation Assessment",
            "abstract": "Research on pronunciation assessment systems focuses on utilizing phonetic and phonological aspects of non-native (L2) speech, often neglecting the rich layer of information hidden within the non-verbal cues. In this study, we proposed a novel pronunciation assessment framework, IntraVerbalPA. % The framework innovatively incorporates both fine-grained frame- and abstract utterance-level non-verbal cues, alongside the conventional speech and phoneme representations. Additionally, we introduce ''Goodness of phonemic-duration'' metric to effectively model duration distribution within the framework. Our results validate the effectiveness of the proposed IntraVerbalPA framework and its individual components, yielding performance that either matches or outperforms existing research works.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "145907529",
                    "name": "Ahmed Ali"
                }
            ]
        },
        {
            "paperId": "a5064b6f851eaae6ff63a6179bc8070f2f2a517d",
            "title": "L1-Aware Multilingual Mispronunciation Detection Framework",
            "abstract": "The phonological discrepancies between a speaker\u2019s native (L1) and the non-native language (L2) serves as a major factor for mispronunciation. This paper introduces a novel multilingual Mispronunciation Detection and Diagnosis (MDD) architecture, L1-MultiMDD, enriched with L1-aware speech representation. An end-to-end speech encoder is trained on the input signal and its corresponding reference phoneme sequence. First, an attention mechanism is deployed to align the input audio with the reference phoneme sequence. Afterwards, the L1-L2-speech embedding are extracted from an auxiliary model, pretrained in a multi-task setup identifying L1 and L2 language, and are infused with the primary network. Finally, the L1-MultiMDD is then optimized for a unified multilingual phoneme recognition task using connectionist temporal classification (CTC) loss for the target languages: English, Arabic, and Mandarin. Our experiments demonstrate the effectiveness of the proposed L1-MultiMDD framework on both seen \u2013 L2-ARTIC, LATIC, and AraVoiceL2v2; and unseen \u2013 EpaDB and Speechocean762 datasets. The consistent gains in PER, and false rejection rate (FRR) across all target languages confirm our approach\u2019s robustness, efficacy, and generalizability.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "2240531645",
                    "name": "Shammur Absar Chwodhury"
                },
                {
                    "authorId": "145907529",
                    "name": "Ahmed Ali"
                }
            ]
        },
        {
            "paperId": "aeda849e7fd006c4d8c04d328a00c1eba5dce962",
            "title": "Multi-View Multi-Task Representation Learning for Mispronunciation Detection",
            "abstract": "The disparity in phonology between learner's native (L1) and target (L2) language poses a significant challenge for mispronunciation detection and diagnosis (MDD) systems. This challenge is further intensified by lack of annotated L2 data. This paper proposes a novel MDD architecture that exploits multiple `views' of the same input data assisted by auxiliary tasks to learn more distinctive phonetic representation in a low-resource setting. Using the mono- and multilingual encoders, the model learn multiple views of the input, and capture the sound properties across diverse languages and accents. These encoded representations are further enriched by learning articulatory features in a multi-task setup. Our reported results using the L2-ARCTIC data outperformed the SOTA models, with a phoneme error rate reduction of 11.13% and 8.60% and absolute F1 score increase of 5.89%, and 2.49% compared to the single-view mono- and multilingual systems, with a limited L2 dataset.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "145907529",
                    "name": "Ahmed Ali"
                }
            ]
        },
        {
            "paperId": "30f0718795045de35bd06c4b71da064d30d21898",
            "title": "SpeechBlender: Speech Augmentation Framework for Mispronunciation Data Generation",
            "abstract": "The lack of labeled second language (L2) speech data is a major challenge in designing mispronunciation detection models. We introduce SpeechBlender - a fine-grained data augmentation pipeline for generating mispronunciation errors to overcome such data scarcity. The SpeechBlender utilizes varieties of masks to target different regions of phonetic units, and use the mixing factors to linearly interpolate raw speech signals while augmenting pronunciation. The masks facilitate smooth blending of the signals, generating more effective samples than the `Cut/Paste' method. Our proposed technique achieves state-of-the-art results, with Speechocean762, on ASR dependent mispronunciation detection models at phoneme level, with a 2.0% gain in Pearson Correlation Coefficient (PCC) compared to the previous state-of-the-art [1]. Additionally, we demonstrate a 5.0% improvement at the phoneme level compared to our baseline. We also observed a 4.6% increase in F1-score with Arabic AraVoiceL2 testset.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "2099539780",
                    "name": "S. Afzal"
                }
            ]
        }
    ]
}