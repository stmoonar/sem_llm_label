{
    "authorId": "144060189",
    "papers": [
        {
            "paperId": "7264f5e5599e8b04b5b1c683adaadbcce6b76f8a",
            "title": "TabLog: Test-Time Adaptation for Tabular Data Using Logic Rules",
            "abstract": "We consider the problem of test-time adaptation of predictive models trained on tabular data. Effective solution of this problem requires adaptation of predictive models trained on the source domain to a target domain, using only unlabeled target domain data, without access to source domain data. Existing test-time adaptation methods for tabular data have difficulty coping with the heterogeneous features and their complex dependencies inherent in tabular data. To overcome these limitations, we consider test-time adaptation in the setting wherein the logical structure of the rules is assumed to remain invariant despite distribution shift between source and target domains whereas the numerical parameters associated with the rules and the weights assigned to them can vary to accommodate distribution shift. TabLog discretizes numerical features, models dependencies between heterogeneous features, introduces a novel contrastive loss for coping with distribution shift, and presents an end-to-end framework for efficient training and test-time adaptation by taking advantage of a logical neural network representation of a rule ensemble. We present results of experiments using several benchmark data sets that demonstrate TabLog is competitive with or improves upon the state-of-the-art methods for test-time adaptation of predictive models trained on tabular data. Our code is available at https:// github.com/WeijieyingRen/TabLog .",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2279021755",
                    "name": "Weijieying Ren"
                },
                {
                    "authorId": "2185014510",
                    "name": "Xiaoting Li"
                },
                {
                    "authorId": "2306819169",
                    "name": "Huiyuan Chen"
                },
                {
                    "authorId": "144060189",
                    "name": "Vineeth Rakesh"
                },
                {
                    "authorId": "2307041276",
                    "name": "Zhuoyi Wang"
                },
                {
                    "authorId": "2273930014",
                    "name": "Mahashweta Das"
                },
                {
                    "authorId": "2256590866",
                    "name": "Vasant Honavar"
                }
            ]
        },
        {
            "paperId": "e682e4a39c3597d3be8a921919989077047db2dd",
            "title": "Transformer-based Models for Long-Form Document Matching: Challenges and Empirical Analysis",
            "abstract": "Recent advances in the area of long document matching have primarily focused on using transformer-based models for long document encoding and matching. There are two primary challenges associated with these models. Firstly, the performance gain provided by transformer-based models comes at a steep cost \u2013 both in terms of the required training time and the resource (memory and energy) consumption. The second major limitation is their inability to handle more than a pre-defined input token length at a time. In this work, we empirically demonstrate the effectiveness of simple neural models (such as feed-forward networks, and CNNs) and simple embeddings (like GloVe, and Paragraph Vector) over transformer-based models on the task of document matching. We show that simple models outperform the more complex BERT-based models while taking significantly less training time, energy, and memory. The simple models are also more robust to variations in document length and text perturbations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36701727",
                    "name": "Akshita Jha"
                },
                {
                    "authorId": "2124016792",
                    "name": "Adithya Samavedhi"
                },
                {
                    "authorId": "144060189",
                    "name": "Vineeth Rakesh"
                },
                {
                    "authorId": "3195704",
                    "name": "J. Chandrashekar"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "82a7ad7d219e45115206ebe3845af39e0d74d568",
            "title": "SmartQuery: An Active Learning Framework for Graph Neural Networks through Hybrid Uncertainty Reduction",
            "abstract": "Graph neural networks have achieved significant success in representation learning. However, the performance gains come at a cost; acquiring comprehensive labeled data for training can be prohibitively expensive. Active learning mitigates this issue by searching the unexplored data space and prioritizing the selection of data to maximize model's performance gain. In this paper, we propose a novel method SMARTQUERY, a framework to learn a graph neural network with very few labeled nodes using a hybrid uncertainty reduction function. This is achieved using two key steps: (a) design a multi-stage active graph learning framework by exploiting diverse explicit graph information and (b) introduce label propagation to efficiently exploit known labels to assess the implicit embedding information. Using a comprehensive set of experiments on three network datasets, we demonstrate the competitive performance of our method against state-of-the-arts on very few labeled data (up to 5 labeled nodes per class).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185014510",
                    "name": "Xiaoting Li"
                },
                {
                    "authorId": "2461369",
                    "name": "Yuhang Wu"
                },
                {
                    "authorId": "144060189",
                    "name": "Vineeth Rakesh"
                },
                {
                    "authorId": "2410838",
                    "name": "Yusan Lin"
                },
                {
                    "authorId": "2145058012",
                    "name": "Hao Yang"
                },
                {
                    "authorId": "2148956204",
                    "name": "Fei Wang"
                }
            ]
        },
        {
            "paperId": "e930df86e5234bd0ec6bd942936065b74cca0780",
            "title": "bigg2vec: Fast and Memory-Efficient Representation Learning for Billion-Scale Graphs on a Single Machine",
            "abstract": "Node embeddings obtained from information networks have been widely adopted for representing knowledge and driving various information retrieval and machine learning tasks. However, training node embeddings is computationally intensive, making it difficult to scale to larger graphs. Most existing works have addressed the scalability challenge by simply adding more hardware resources. For example, a common approach to speed up the training process is to distribute model computation across multiple machines and GPUs. This paper takes an orthogonal approach towards scalability by addressing the problem of computation complexity in training embeddings. We present bigg2vec for scaling up the embedding training process. bigg2vec introduces a novel polar coordinate-based system for internal representation and computation. It provides the following benefits: (a) It significantly reduces compute and memory requirements while improving embedding quality and (b) uses a novel graph organization to generate high-quality negative samples (this reduces the number of negative samples needed for training, which is especially beneficial f or skewed graphs). We have deployed bigg2vec to generate embeddings for multiple AI models within Visa. Our Global Personalized Restaurant Recommender System (GPR) is one such project that uses bigg2vec to periodically generate embeddings for over 450 million nodes connected by more than 3 billion edges. bigg2vec generates higher quality embeddings while training them faster than state-of-the-art methods on a single CPU-based machine.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31819608",
                    "name": "M. Bendre"
                },
                {
                    "authorId": "40308435",
                    "name": "Mahashweta Das"
                },
                {
                    "authorId": "144060189",
                    "name": "Vineeth Rakesh"
                },
                {
                    "authorId": "2148956204",
                    "name": "Fei Wang"
                },
                {
                    "authorId": "2145058012",
                    "name": "Hao Yang"
                }
            ]
        },
        {
            "paperId": "74784d758960f07bd6ef9131585f262970450f34",
            "title": "Efficacy of Bayesian Neural Networks in Active Learning",
            "abstract": "Obtaining labeled data for machine learning tasks can be prohibitively expensive. Active learning mitigates this issue by exploring the unlabeled data space and prioritizing the selection of data that can best improve the model performance. A common approach to active learning is to pick a small sample of data for which the model is most uncertain. In this paper, we explore the efficacy of Bayesian neural networks for active learning, which naturally models uncertainty by learning distribution over the weights of neural networks. By performing a comprehensive set of experiments, we show that Bayesian neural networks are more efficient than ensemble based techniques in capturing uncertainty. Our findings also reveal some key drawbacks of the ensemble techniques, which was recently shown to be more effective than Monte Carlo dropouts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144060189",
                    "name": "Vineeth Rakesh"
                },
                {
                    "authorId": "2767349",
                    "name": "Swayambhoo Jain"
                }
            ]
        },
        {
            "paperId": "86e0c6e038ba928b091329950400b8debe79363f",
            "title": "Supervised Contrastive Learning for Interpretable Long Document Comparison",
            "abstract": "Recent advancements in deep learning techniques have transformed the area of semantic text matching. However, most of the state-of-the-art models are designed to operate with short documents such as tweets, user reviews, comments, etc., and have fundamental limitations when applied to long-form documents such as scientific papers, legal documents, and patents. When handling such long documents, there are three primary challenges: (i) The presence of different contexts for the same word throughout the document, (ii) Small sections of contextually similar text between two documents, but dissimilar text in the remaining parts \u2013 this defies the basic understanding of \"similarity\", and (iii) The coarse nature of a single global similarity measure which fails to capture the heterogeneity of the document content. In this paper, we describe CoLDE : Co ntrastive L ong D ocument E ncoder \u2013 a transformer-based framework that addresses these challenges and allows for interpretable comparisons of long documents. CoLDE uses unique positional embeddings and a multi-headed chunkwise attention layer in conjunction with a contrastive learning framework to capture similarity at three different levels: (i) high-level similarity scores between a pair of documents, (ii) similarity scores between different sections within and across documents, and (iii) similarity scores between different chunks in the same document and also other documents. These fine-grained similarity scores aid in better interpretability. We evaluate CoLDE on three long document datasets namely, ACL Anthology publications, Wikipedia articles, and USPTO patents. Besides outperforming the state-of-the-art methods on the document comparison task, CoLDE also proves interpretable and robust to changes in document length and text perturbations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36701727",
                    "name": "Akshita Jha"
                },
                {
                    "authorId": "144060189",
                    "name": "Vineeth Rakesh"
                },
                {
                    "authorId": "3195704",
                    "name": "J. Chandrashekar"
                },
                {
                    "authorId": "2124016792",
                    "name": "Adithya Samavedhi"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "e9fe90baf1b811411943d73f40daf4d947f9102e",
            "title": "Supervised Contrastive Learning for Interpretable Long-Form Document Matching",
            "abstract": "Recent advancements in deep learning techniques have transformed the area of semantic text matching (STM). However, most state-of-the-art models are designed to operate with short documents such as tweets, user reviews, comments, and so on. These models have fundamental limitations when applied to long-form documents such as scientific papers, legal documents, and patents. When handling such long documents, there are three primary challenges: (i) the presence of different contexts for the same word throughout the document, (ii) small sections of contextually similar text between two documents, but dissimilar text in the remaining parts (this defies the basic understanding of \u201csimilarity\u201d), and (iii) the coarse nature of a single global similarity measure which fails to capture the heterogeneity of the document content. In this article, we describe CoLDE: Contrastive Long Document Encoder\u2014a transformer-based framework that addresses these challenges and allows for interpretable comparisons of long documents. CoLDE uses unique positional embeddings and a multi-headed chunkwise attention layer in conjunction with a supervised contrastive learning framework to capture similarity at three different levels: (i) high-level similarity scores between a pair of documents, (ii) similarity scores between different sections within and across documents, and (iii) similarity scores between different chunks in the same document and across other documents. These fine-grained similarity scores aid in better interpretability. We evaluate CoLDE on three long document datasets namely, ACL Anthology publications, Wikipedia articles, and USPTO patents. Besides outperforming the state-of-the-art methods on the document matching task, CoLDE is also robust to changes in document length and text perturbations and provides interpretable results. The code for the proposed model is publicly available at https://github.com/InterDigitalInc/CoLDE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36701727",
                    "name": "Akshita Jha"
                },
                {
                    "authorId": "144060189",
                    "name": "Vineeth Rakesh"
                },
                {
                    "authorId": "3195704",
                    "name": "J. Chandrashekar"
                },
                {
                    "authorId": "2124016792",
                    "name": "Adithya Samavedhi"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "fe66c9140489e201125f50d07d1e13ce7e8190ab",
            "title": "Project Recommendation Using Heterogeneous Traits in Crowdfunding",
            "abstract": "\n \n Crowdfunding has gained widespread popularity in recent years. By funding entrepreneurs with creative minds, it is gradually taking over the role of venture capitalists who provide the much needed seed capital to jump start business ventures. Despite the huge success of the crowdfunding platforms, not every project is successful in reaching its funding goal. Therefore, in this paper, we intend to answer the following question \"what set of features determine a project's success?\". We begin by studying the dynamics of Kickstarter, a popular reward-based crowdfunding platform, and the impact of social networks on this platform. Contrary to previous studies, our analysis is not restricted to project-based features alone; instead, we expand the features into four different categories: temporal traits, personal traits, geo-location traits, and network traits. Using a comprehensive dataset of 18K projects and 116K tweets, we provide several unique insights about these features and their effects on the success of Kickstarter projects. Based on these insights, we build a supervised learning framework to learn a model that can recommend a set of investors to Kickstarter projects. By utilizing features from the first three days of project duration alone, we show that our results are significantly better than the previous studies.\n \n",
            "fieldsOfStudy": [
                "Business",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144060189",
                    "name": "Vineeth Rakesh"
                },
                {
                    "authorId": "1795455",
                    "name": "J. Choo"
                },
                {
                    "authorId": "144417522",
                    "name": "Chandan K. Reddy"
                }
            ]
        },
        {
            "paperId": "e1aab4a0ea725b793050bd992ee5c31429814a0e",
            "title": "User Modeling and Churn Prediction in Over-the-top Media Services",
            "abstract": "We address the problem of customer retention ( churn ) in applications installed on over the top (OTT) streaming devices. In the first part of our work, we analyze various behavioral characteristics of users that drive application usage. By examining a variety of statistical measures, we answer the following questions: (1) how do users allocate time across various applications? , (2) how consistently do users engage with their devices? and (3) how likely are dormant users liable to becoming active again? In the second part, we leverage these insights to design interpretable churn prediction models that learn the latent characteristics of users by prioritizing the specifications of the users. Specifically, we propose the following models: (1) Attention LSTM (ALSTM), where churn prediction is done using a single level of attention by weighting on individual time frames (temporal-level attention) and (2) Neural Churn Prediction Model (NCPM), a more comprehensive model that uses two levels of attentions, one for measuring the temporality of each feature and another to measure the influence across features (feature-level attention). Using a series of experiments, we show that our models provide good churn prediction accuracy with interpretable reasoning. We believe that the data analysis, feature engineering and modeling techniques presented in this work can help organizations better understand the reason behind user churn on OTT devices.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144060189",
                    "name": "Vineeth Rakesh"
                },
                {
                    "authorId": "2007951311",
                    "name": "Ajith Pudiyavitil"
                },
                {
                    "authorId": "3195704",
                    "name": "J. Chandrashekar"
                }
            ]
        },
        {
            "paperId": "62f248e958bfca689959ec55927218f3b1c1c9f2",
            "title": "Deep generative models for recommender systems",
            "abstract": "This chapter introduces some recent trends in generative and deep-learning (DL) models for hybrid recommendation systems that have proven to be extremely effective in integrating different modalities of data. It is organized into three main sections. The first section considers classic algorithms such as probabilistic matrix factorization and latent Dirichlet allocation and illustrates the generative principle of a hybrid recommendation model called collaborative topic regression that jointly models the latent interests of users and items. The second section presents recommendation models that are exclusively based on DL techniques. This includes models such as Restricted Boltzmann-machine-based CF, autoencoder (AE)-based recommendation, neural CF and recurrent recommender network. Finally, the third section explains models such as collaborative denoising AE and collaborative variational AE that integrates PGMs with DL to create a generative DL framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144060189",
                    "name": "Vineeth Rakesh"
                },
                {
                    "authorId": "2893721",
                    "name": "Suhang Wang"
                },
                {
                    "authorId": "145896397",
                    "name": "Huan Liu"
                }
            ]
        }
    ]
}