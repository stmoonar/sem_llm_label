{
    "authorId": "51973597",
    "papers": [
        {
            "paperId": "4c3a4956cb2f088359c50ed38b0102f6742d449b",
            "title": "Neural (Tangent Kernel) Collapse",
            "abstract": "This work bridges two important concepts: the Neural Tangent Kernel (NTK), which captures the evolution of deep neural networks (DNNs) during training, and the Neural Collapse (NC) phenomenon, which refers to the emergence of symmetry and structure in the last-layer features of well-trained classification DNNs. We adopt the natural assumption that the empirical NTK develops a block structure aligned with the class labels, i.e., samples within the same class have stronger correlations than samples from different classes. Under this assumption, we derive the dynamics of DNNs trained with mean squared (MSE) loss and break them into interpretable phases. Moreover, we identify an invariant that captures the essence of the dynamics, and use it to prove the emergence of NC in DNNs with block-structured NTK. We provide large-scale numerical experiments on three common DNN architectures and three benchmark datasets to support our theory.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51973597",
                    "name": "Mariia Seleznova"
                },
                {
                    "authorId": "1658955361",
                    "name": "Dana Weitzner"
                },
                {
                    "authorId": "2711839",
                    "name": "R. Giryes"
                },
                {
                    "authorId": "3125779",
                    "name": "Gitta Kutyniok"
                },
                {
                    "authorId": "146876955",
                    "name": "H. Chou"
                }
            ]
        },
        {
            "paperId": "9bd860ba5d3bde708f37fc9d66f1fe60a6d444a4",
            "title": "Neural Tangent Kernel Beyond the Infinite-Width Limit: Effects of Depth and Initialization",
            "abstract": "Neural Tangent Kernel (NTK) is widely used to analyze overparametrized neural networks due to the famous result by Jacot et al. (2018): in the infinite-width limit, the NTK is deterministic and constant during training. However, this result cannot explain the behavior of deep networks, since it generally does not hold if depth and width tend to infinity simultaneously. In this paper, we study the NTK of fully-connected ReLU networks with depth comparable to width. We prove that the NTK properties depend significantly on the depth-to-width ratio and the distribution of parameters at initialization. In fact, our results indicate the importance of the three phases in the hyperparameter space identified in Poole et al. (2016): ordered, chaotic and the edge of chaos (EOC). We derive exact expressions for the NTK dispersion in the infinite-depth-and-width limit in all three phases and conclude that the NTK variability grows exponentially with depth at the EOC and in the chaotic phase but not in the ordered phase. We also show that the NTK of deep networks may stay constant during training only in the ordered phase and discuss how the structure of the NTK matrix changes during training.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51973597",
                    "name": "Mariia Seleznova"
                },
                {
                    "authorId": "3125779",
                    "name": "Gitta Kutyniok"
                }
            ]
        },
        {
            "paperId": "3b1b81f56bc3b93b45593071f975376b5a2060f1",
            "title": "Guided exploration of user groups",
            "abstract": "Finding a set of users of interest serves several applications in behavioral analytics. Often times, identifying users requires to explore the data and gradually choose potential targets. This is a special case of Exploratory Data Analysis (EDA), an iterative and tedious process. In this paper, we formalize and solve the problem of guided exploration of user groups whose purpose is to find target users. We model exploration as an iterative decision-making process, where an agent is shown a set of groups, chooses users from those groups, and selects the best action to move to the next step. To solve our problem, we apply reinforcement learning to discover an efficient exploration strategy from a simulated agent experience, and propose to use the learned strategy to recommend an exploration policy that can be applied to the same task for any dataset. Our framework accepts a wide class of exploration actions and does not need to gather exploration logs. Our experiments show that the agent naturally captures manual exploration by human analysts, and succeeds to learn an interpretable and transferable exploration policy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51973597",
                    "name": "Mariia Seleznova"
                },
                {
                    "authorId": "1403851743",
                    "name": "Behrooz Omidvar-Tehrani"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2060863572",
                    "name": "Eric Simon"
                }
            ]
        },
        {
            "paperId": "5c584fbe9943bca4a5b5e17476b1cafe96864c06",
            "title": "Analyzing Finite Neural Networks: Can We Trust Neural Tangent Kernel Theory?",
            "abstract": "Neural Tangent Kernel (NTK) theory is widely used to study the dynamics of infinitely-wide deep neural networks (DNNs) under gradient descent. But do the results for infinitely-wide networks give us hints about the behaviour of real finite-width ones? In this paper we study empirically when NTK theory is valid in practice for fully-connected ReLu and sigmoid networks. We find out that whether a network is in the NTK regime depends on the hyperparameters of random initialization and network's depth. In particular, NTK theory does not explain behaviour of sufficiently deep networks initialized so that their gradients explode: the kernel is random at initialization and changes significantly during training, contrary to NTK theory. On the other hand, in case of vanishing gradients DNNs are in the NTK regime but become untrainable rapidly with depth. We also describe a framework to study generalization properties of DNNs by means of NTK theory and discuss its limits.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "51973597",
                    "name": "Mariia Seleznova"
                },
                {
                    "authorId": "3125779",
                    "name": "Gitta Kutyniok"
                }
            ]
        },
        {
            "paperId": "ad2bff246d6261cf2436b5c946b999f033ca4430",
            "title": "Towards Large-Scale Exploratory Search over Heterogeneous Sources",
            "abstract": "Since time immemorial, people have been looking for ways to organize scientific knowledge into some systems to facilitate search and discovery of new ideas. The problem was partially solved in the pre-Internet era using library classifications, but nowadays it is nearly impossible to classify all scientific and popular scientific knowledge manually. There is a clear gap between the diversity and the amount of data available on the Internet and the algorithms for automatic structuring of such data. In our preliminary study, we approach the problem of knowledge discovery on web-scale data with diverse text sources and propose an algorithm to aggregate multiple collections into a single hierarchical topic model. We implement a web service named Rysearch to demonstrate the concept of topical exploratory search and make it available online.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "51973597",
                    "name": "Mariia Seleznova"
                },
                {
                    "authorId": "49179156",
                    "name": "Anton Belyy"
                },
                {
                    "authorId": "51967824",
                    "name": "A. Sholokhov"
                }
            ]
        }
    ]
}