{
    "authorId": "46277026",
    "papers": [
        {
            "paperId": "3c743f9aeddc8a076f90b924bfb3bd0b9d6870ee",
            "title": "Semi-Supervised Graph Ultra-Sparsifier Using Reweighted \u21131 Optimization",
            "abstract": "Graph representation learning with the family of graph convolution networks (GCN) provides powerful tools for prediction on graphs. As graphs grow with more edges, the GCN family suffers from sub-optimal generalization performance due to task-irrelevant connections. Recent studies solve this problem by using graph sparsification in neural networks. However, graph sparsification cannot generate ultra-sparse graphs while simultaneously maintaining the performance of the GCN family. To address this problem, we propose Graph Ultra-sparsifier, a semi-supervised graph sparsifier with dynamically-updated regularization terms based on the graph convolution. The graph ultra-sparsifier can generate ultra-sparse graphs while maintaining the performance of the GCN family with the ultra-sparse graphs as inputs. In the experiments, when compared to the state-of-the-art graph sparsifiers, our graph ultra-sparsifier generates ultra-sparse graphs and these ultra-sparse graphs can be used as inputs to maintain the performance of GCN and its variants in node classification tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46277026",
                    "name": "Jiayu Li"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "28044622",
                    "name": "Shengmin Jin"
                },
                {
                    "authorId": "2281410",
                    "name": "R. Zafarani"
                }
            ]
        },
        {
            "paperId": "1175cdec2cabcdd73a47d450becd5ebded35eccb",
            "title": "A Spectral Measure for Network Robustness: Assessment, Design, and Evolution",
            "abstract": "A robust system should perform well under random failures or targeted attacks, and networks have been widely used to model the underlying structure of complex systems such as communication, infrastructure, and transportation networks. Hence, network robustness becomes critical to understanding system robustness. In this paper, we propose a spectral measure for network robustness: the second spectral moment $m_{2}$ of the network. Our results show that a smaller second spectral moment $m_{2}$ indicates a more robust network. We demonstrate both theoretically and with extensive empirical studies that the second spectral moment can help (1) capture various traditional measures of network robustness; (2) assess the robustness of networks; (3) design networks with controlled robustness; and (4) study how complex networked systems (e.g., power systems) behave under cascading failures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "28044622",
                    "name": "Shengmin Jin"
                },
                {
                    "authorId": "1878936428",
                    "name": "Rui Ma"
                },
                {
                    "authorId": "46277026",
                    "name": "Jiayu Li"
                },
                {
                    "authorId": "26921002",
                    "name": "Sara Eftekharnejad"
                },
                {
                    "authorId": "2281410",
                    "name": "R. Zafarani"
                }
            ]
        },
        {
            "paperId": "cc5e7bf0432543d842badab51b002fcee913c406",
            "title": "AdverSparse: An Adversarial Attack Framework for Deep Spatial-Temporal Graph Neural Networks",
            "abstract": "Spatial-temporal graph have been widely observed in various domains such as neuroscience, climate research, and transportation engineering. The state-of-the-art models of spatialtemporal graphs rely on Graph Neural Networks (GNNs) to obtain explicit representations for such networks and to discover hidden spatial dependencies in them. These models have demonstrated superior performance in various tasks. In this paper, we propose a sparse adversarial attack framework AdverSparse to illustrate that when only a few key connections are removed in such graphs, hidden spatial dependencies learned by such spatial-temporal models are significantly impacted, leading to various issues such as increasing prediction errors. We formulate the adversarial attack as an optimization problem and solve it by the Alternating Direction Method of Multipliers (ADMM). Experiments show that AdverSparse can find and remove key connections in these graphs, leading to malfunctioning models, even in models capable of learning hidden spatial dependencies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46277026",
                    "name": "Jiayu Li"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "28044622",
                    "name": "Shengmin Jin"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "2281410",
                    "name": "R. Zafarani"
                }
            ]
        },
        {
            "paperId": "2ed818bce541b35b0751f4660552f7eed36c9e04",
            "title": "Universal Approximation Property and Equivalence of Stochastic Computing-Based Neural Networks and Binary Neural Networks",
            "abstract": "Large-scale deep neural networks are both memory and computation-intensive, thereby posing stringent requirements on the computing platforms. Hardware accelerations of deep neural networks have been extensively investigated. Specific forms of binary neural networks (BNNs) and stochastic computing-based neural networks (SCNNs) are particularly appealing to hardware implementations since they can be implemented almost entirely with binary operations. \nDespite the obvious advantages in hardware implementation, these approximate computing techniques are questioned by researchers in terms of accuracy and universal applicability. Also it is important to understand the relative pros and cons of SCNNs and BNNs in theory and in actual hardware implementations. In order to address these concerns, in this paper we prove that the \u201cideal\u201d SCNNs and BNNs satisfy the universal approximation property with probability 1 (due to the stochastic behavior), which is a new angle from the original approximation property. The proof is conducted by first proving the property for SCNNs from the strong law of large numbers, and then using SCNNs as a \u201cbridge\u201d to prove for BNNs. Besides the universal approximation property, we also derive an appropriate bound for bit length M in order to provide insights for the actual neural network implementations. Based on the universal approximation property, we further prove that SCNNs and BNNs exhibit the same energy complexity. In other words, they have the same asymptotic energy consumption with the growth of network size. We also provide a detailed analysis of the pros and cons of SCNNs and BNNs for hardware implementations and conclude that SCNNs are more suitable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257348403",
                    "name": "Yanzhi Wang"
                },
                {
                    "authorId": "2949135",
                    "name": "Zheng Zhan"
                },
                {
                    "authorId": "2257314989",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "2257337366",
                    "name": "Jian Tang"
                },
                {
                    "authorId": "1421267787",
                    "name": "Siyue Wang"
                },
                {
                    "authorId": "46277026",
                    "name": "Jiayu Li"
                },
                {
                    "authorId": "2055908317",
                    "name": "Bo Yuan"
                },
                {
                    "authorId": "2257210006",
                    "name": "Wujie Wen"
                },
                {
                    "authorId": "2257370653",
                    "name": "Xue Lin"
                }
            ]
        },
        {
            "paperId": "178bfb528d75d6aff6d055a3ba2988ede8b6b154",
            "title": "On the Universal Approximation Property and Equivalence of Stochastic Computing-based Neural Networks and Binary Neural Networks",
            "abstract": "Large-scale deep neural networks are both memory intensive and computation-intensive, thereby posing stringent requirements on the computing platforms. Hardware accelerations of deep neural networks have been extensively investigated in both industry and academia. Specific forms of binary neural networks (BNNs) and stochastic computing based neural networks (SCNNs) are particularly appealing to hardware implementations since they can be implemented almost entirely with binary operations. Despite the obvious advantages in hardware implementation, these approximate computing techniques are questioned by researchers in terms of accuracy and universal applicability. Also it is important to understand the relative pros and cons of SCNNs and BNNs in theory and in actual hardware implementations. In order to address these concerns, in this paper we prove that the \"ideal\" SCNNs and BNNs satisfy the universal approximation property with probability 1 (due to the stochastic behavior). The proof is conducted by first proving the property for SCNNs from the strong law of large numbers, and then using SCNNs as a \"bridge\" to prove for BNNs. Based on the universal approximation property, we further prove that SCNNs and BNNs exhibit the same energy complexity. In other words, they have the same asymptotic energy consumption with the growing of network size. We also provide a detailed analysis of the pros and cons of SCNNs and BNNs for hardware implementations and conclude that SCNNs are more suitable for hardware.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                },
                {
                    "authorId": "2949135",
                    "name": "Zheng Zhan"
                },
                {
                    "authorId": "46277026",
                    "name": "Jiayu Li"
                },
                {
                    "authorId": "2115854503",
                    "name": "Jian Tang"
                },
                {
                    "authorId": "144460251",
                    "name": "Bo Yuan"
                },
                {
                    "authorId": "2116734918",
                    "name": "Liang Zhao"
                },
                {
                    "authorId": "35420329",
                    "name": "Wujie Wen"
                },
                {
                    "authorId": "1421267787",
                    "name": "Siyue Wang"
                },
                {
                    "authorId": "145282404",
                    "name": "X. Lin"
                }
            ]
        },
        {
            "paperId": "5f8ad66b4212d42a7db60e3867e8e2dbaf596a33",
            "title": "Progressive Weight Pruning of Deep Neural Networks using ADMM",
            "abstract": "Deep neural networks (DNNs) although achieving human-level performance in many domains, have very large model size that hinders their broader applications on edge computing devices. Extensive research work have been conducted on DNN model compression or pruning. However, most of the previous work took heuristic approaches. This work proposes a progressive weight pruning approach based on ADMM (Alternating Direction Method of Multipliers), a powerful technique to deal with non-convex optimization problems with potentially combinatorial constraints. Motivated by dynamic programming, the proposed method reaches extremely high pruning rate by using partial prunings with moderate pruning rates. Therefore, it resolves the accuracy degradation and long convergence time problems when pursuing extremely high pruning ratios. It achieves up to 34 times pruning rate for ImageNet dataset and 167 times pruning rate for MNIST dataset, significantly higher than those reached by the literature work. Under the same number of epochs, the proposed method also achieves faster convergence and higher compression rates. The codes and pruned DNN models are released in the link bit.ly/2zxdlss",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35539001",
                    "name": "Shaokai Ye"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "145054784",
                    "name": "Kaiqi Zhang"
                },
                {
                    "authorId": "46277026",
                    "name": "Jiayu Li"
                },
                {
                    "authorId": "46321210",
                    "name": "Kaidi Xu"
                },
                {
                    "authorId": "2116589060",
                    "name": "Yunfei Yang"
                },
                {
                    "authorId": "143796160",
                    "name": "Fuxun Yu"
                },
                {
                    "authorId": "2115854503",
                    "name": "Jian Tang"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "143743061",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "2143734919",
                    "name": "Xiang Chen"
                },
                {
                    "authorId": "145282404",
                    "name": "X. Lin"
                },
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                }
            ]
        },
        {
            "paperId": "64db2e2c76aa3f028b6866f91795a7c005a3f13b",
            "title": "ADAM-ADMM: A Unified, Systematic Framework of Structured Weight Pruning for DNNs",
            "abstract": "Weight pruning methods of deep neural networks have been demonstrated to achieve a good model pruning ratio without loss of accuracy, thereby alleviating the significant computation/storage requirements of large-scale DNNs. Structured weight pruning methods have been proposed to overcome the limitation of irregular network structure and demonstrated actual GPU acceleration. However, the pruning ratio and GPU acceleration are limited when accuracy needs to be maintained. In this work, we overcome pruning ratio and GPU acceleration limitations by proposing a unified, systematic framework of structured weight pruning for DNNs, named ADAM-ADMM. It is a framework that can be used to induce different types of structured sparsity, such as filter-wise, channel-wise, and shape-wise sparsity, as well non-structured sparsity. The proposed framework incorporates stochastic gradient descent with ADMM, and can be understood as a dynamic regularization method in which the regularization target is analytically updated in each iteration. A significant improvement in structured weight pruning ratio is achieved without loss of accuracy, along with fast convergence rate. With a small sparsity degree of 33.3% on the convolutional layers, we achieve 1.64% accuracy enhancement for the AlexNet model. This is obtained by mitigation of overfitting. Without loss of accuracy on the AlexNet model, we achieve 2.58x and 3.65x average measured speedup on two GPUs, clearly outperforming the prior work. The average speedups reach 2.77x and 7.5x when allowing a moderate accuracy loss of 2%. In this case the model compression for convolutional layers is 13.2x, corresponding to 10.5x CPU speedup. Our experiments on ResNet model and on other datasets like UCF101 and CIFAR-10 demonstrate the consistently higher performance of our framework. Our models and codes are released at this https URL",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "145054784",
                    "name": "Kaiqi Zhang"
                },
                {
                    "authorId": "35539001",
                    "name": "Shaokai Ye"
                },
                {
                    "authorId": "46277026",
                    "name": "Jiayu Li"
                },
                {
                    "authorId": "2115854503",
                    "name": "Jian Tang"
                },
                {
                    "authorId": "35420329",
                    "name": "Wujie Wen"
                },
                {
                    "authorId": "145282404",
                    "name": "X. Lin"
                },
                {
                    "authorId": "1713097",
                    "name": "M. Fardad"
                },
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                }
            ]
        },
        {
            "paperId": "7bf77ee8d41b999aff3f36884c6cc365c6bd2dba",
            "title": "FaceGANs: Stable Generative Adversarial Networks with High-Quality Images",
            "abstract": "Generative Adversarial Networks (GANs) have shown impressive performance in producing images highly similar to original dataset under unsupervised learning. However, the losses of discriminator and generator are highly fluctuated, which affects the quality of fake images produced by the generator. In this work, we propose Face Generative Adversarial Networks (FaceGANs). Compared to the conventional GANs, our new structure can stabilize the loss fluctuation of discriminator and generator. It also improves the capabilities of generator and discriminator. In order to fully investigate FaceGANs, we compare the performance of FaceGANs with Deep Convolution Generative Adversarial Network (DCGANs) on the Celeba dataset. Experimental results show that our FaceGANs structure can fast generate images with better quality than DCGANs in a facial reconstruction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46277026",
                    "name": "Jiayu Li"
                },
                {
                    "authorId": "2949135",
                    "name": "Zheng Zhan"
                },
                {
                    "authorId": "2881873",
                    "name": "Caiwen Ding"
                },
                {
                    "authorId": "2136922252",
                    "name": "Yanzhi Wang"
                }
            ]
        },
        {
            "paperId": "a29c2ecd113242f7103413d86f29560feab10a61",
            "title": "Structured Weight Matrices-Based Hardware Accelerators in Deep Neural Networks: FPGAs and ASICs",
            "abstract": "Both industry and academia have extensively investigated hardware accelerations. To address the demands in increasing computational capability and memory requirement, in this work, we propose the structured weight matrices (SWM)-based compression technique for both Field Programmable Gate Array (FPGA) and application-specific integrated circuit (ASIC) implementations. In the algorithm part, the SWM-based framework adopts block-circulant matrices to achieve a fine-grained tradeoff between accuracy and compression ratio. The SWM-based technique can reduce computational complexity from O(n2) to O(nlog n) and storage complexity from O(n2) to O(n) for each layer and both training and inference phases. For FPGA implementations on deep convolutional neural networks (DCNNs), we achieve at least 152X and 72X improvement in performance and energy efficiency, respectively using the SWM-based framework, compared with the baseline of IBM TrueNorth processor under same accuracy constraints using the data set of MNIST, SVHN, and CIFAR-10. For FPGA implementations on long short term memory (LSTM) networks, the proposed SWM-based LSTM can achieve up to 21X enhancement in performance and 33.5X gains in energy efficiency compared with the ESE accelerator. For ASIC implementations, the proposed SWM-based ASIC design exhibits impressive advantages in terms of power, throughput, and energy efficiency. Experimental results indicate that this method is greatly suitable for applying DNNs onto both FPGAs and mobile/IoT devices.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2881873",
                    "name": "Caiwen Ding"
                },
                {
                    "authorId": "3456943",
                    "name": "Ao Ren"
                },
                {
                    "authorId": "9347641",
                    "name": "Geng Yuan"
                },
                {
                    "authorId": "151480882",
                    "name": "Xiaolong Ma"
                },
                {
                    "authorId": "46277026",
                    "name": "Jiayu Li"
                },
                {
                    "authorId": "2152354569",
                    "name": "Ning Liu"
                },
                {
                    "authorId": "144460251",
                    "name": "Bo Yuan"
                },
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                }
            ]
        },
        {
            "paperId": "d1f1d5de6e56f6195409013731a6ecfe78f2f78a",
            "title": "A Unified Framework of DNN Weight Pruning and Weight Clustering/Quantization Using ADMM",
            "abstract": "Many model compression techniques of Deep Neural Networks (DNNs) have been investigated, including weight pruning, weight clustering and quantization, etc. Weight pruning leverages the redundancy in the number of weights in DNNs, while weight clustering/quantization leverages the redundancy in the number of bit representations of weights. They can be effectively combined in order to exploit the maximum degree of redundancy. However, there lacks a systematic investigation in literature towards this direction. \nIn this paper, we fill this void and develop a unified, systematic framework of DNN weight pruning and clustering/quantization using Alternating Direction Method of Multipliers (ADMM), a powerful technique in optimization theory to deal with non-convex optimization problems. Both DNN weight pruning and clustering/quantization, as well as their combinations, can be solved in a unified manner. For further performance improvement in this framework, we adopt multiple techniques including iterative weight quantization and retraining, joint weight clustering training and centroid updating, weight clustering retraining, etc. The proposed framework achieves significant improvements both in individual weight pruning and clustering/quantization problems, as well as their combinations. For weight pruning alone, we achieve 167x weight reduction in LeNet-5, 24.7x in AlexNet, and 23.4x in VGGNet, without any accuracy loss. For the combination of DNN weight pruning and clustering/quantization, we achieve 1,910x and 210x storage reduction of weight data on LeNet-5 and AlexNet, respectively, without accuracy loss. Our codes and models are released at the link this http URL",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35539001",
                    "name": "Shaokai Ye"
                },
                {
                    "authorId": "3417475",
                    "name": "Tianyun Zhang"
                },
                {
                    "authorId": "145054784",
                    "name": "Kaiqi Zhang"
                },
                {
                    "authorId": "46277026",
                    "name": "Jiayu Li"
                },
                {
                    "authorId": "1994395238",
                    "name": "Jiaming Xie"
                },
                {
                    "authorId": "2117875640",
                    "name": "Yun Liang"
                },
                {
                    "authorId": "143743061",
                    "name": "Sijia Liu"
                },
                {
                    "authorId": "145282404",
                    "name": "X. Lin"
                },
                {
                    "authorId": "46393431",
                    "name": "Yanzhi Wang"
                }
            ]
        }
    ]
}