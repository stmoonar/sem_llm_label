{
    "authorId": "1798404",
    "papers": [
        {
            "paperId": "29b98349742bcfe0b91784a2fb1837b094a13b98",
            "title": "SocialQuotes: Learning Contextual Roles of Social Media Quotes on the Web",
            "abstract": "Web authors frequently embed social media to support and enrich their content, creating the potential to derive web-based, cross-platform social media representations that can enable more effective social media retrieval systems and richer scientific analyses. As step toward such capabilities, we introduce a novel language modeling framework that enables automatic annotation of roles that social media entities play in their embedded web context. Using related communication theory, we liken social media embeddings to quotes, formalize the page context as structured natural language signals, and identify a taxonomy of roles for quotes within the page context. We release SocialQuotes, a new data set built from the Common Crawl of over 32 million social quotes, 8.3k of them with crowdsourced quote annotations. Using SocialQuotes and the accompanying annotations, we provide a role classification case study, showing reasonable performance with modern-day LLMs, and exposing explainable aspects of our framework via page content ablations. We also classify a large batch of un-annotated quotes, revealing interesting cross-domain, cross-platform role distributions on the web.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1798404",
                    "name": "John Palowitch"
                },
                {
                    "authorId": "2625177",
                    "name": "Hamidreza Alvari"
                },
                {
                    "authorId": "2275184164",
                    "name": "Mehran Kazemi"
                },
                {
                    "authorId": "2312396120",
                    "name": "Tanvir Amin"
                },
                {
                    "authorId": "2065812052",
                    "name": "Filip Radlinski"
                }
            ]
        },
        {
            "paperId": "3857dfbfae68658bb0c504986317beabbbb06115",
            "title": "Towards Realistic Synthetic User-Generated Content: A Scaffolding Approach to Generating Online Discussions",
            "abstract": "The emergence of synthetic data represents a pivotal shift in modern machine learning, offering a solution to satisfy the need for large volumes of data in domains where real data is scarce, highly private, or difficult to obtain. We investigate the feasibility of creating realistic, large-scale synthetic datasets of user-generated content, noting that such content is increasingly prevalent and a source of frequently sought information. Large language models (LLMs) offer a starting point for generating synthetic social media discussion threads, due to their ability to produce diverse responses that typify online interactions. However, as we demonstrate, straightforward application of LLMs yields limited success in capturing the complex structure of online discussions, and standard prompting mechanisms lack sufficient control. We therefore propose a multi-step generation process, predicated on the idea of creating compact representations of discussion threads, referred to as scaffolds. Our framework is generic yet adaptable to the unique characteristics of specific social media platforms. We demonstrate its feasibility using data from two distinct online discussion platforms. To address the fundamental challenge of ensuring the representativeness and realism of synthetic data, we propose a portfolio of evaluation measures to compare various instantiations of our framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1680484",
                    "name": "K. Balog"
                },
                {
                    "authorId": "1798404",
                    "name": "John Palowitch"
                },
                {
                    "authorId": "2316325676",
                    "name": "Barbara Ikica"
                },
                {
                    "authorId": "2065812052",
                    "name": "Filip Radlinski"
                },
                {
                    "authorId": "2625177",
                    "name": "Hamidreza Alvari"
                },
                {
                    "authorId": "2316326121",
                    "name": "Mehdi Manshadi"
                }
            ]
        },
        {
            "paperId": "86adb156ad650ba609da7f8ddca8434961ce8478",
            "title": "Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning",
            "abstract": "Large language models (LLMs) have showcased remarkable reasoning capabilities, yet they remain susceptible to errors, particularly in temporal reasoning tasks involving complex temporal logic. Existing research has explored LLM performance on temporal reasoning using diverse datasets and benchmarks. However, these studies often rely on real-world data that LLMs may have encountered during pre-training or employ anonymization techniques that can inadvertently introduce factual inconsistencies. In this work, we address these limitations by introducing novel synthetic datasets specifically designed to assess LLM temporal reasoning abilities in various scenarios. The diversity of question types across these datasets enables systematic investigation into the impact of the problem structure, size, question type, fact order, and other factors on LLM performance. Our findings provide valuable insights into the strengths and weaknesses of current LLMs in temporal reasoning tasks. To foster further research in this area, we are open-sourcing the datasets and evaluation framework used in our experiments: https://huggingface.co/datasets/baharef/ToT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3422551",
                    "name": "Bahare Fatemi"
                },
                {
                    "authorId": "2283308055",
                    "name": "Mehran Kazemi"
                },
                {
                    "authorId": "40900939",
                    "name": "Anton Tsitsulin"
                },
                {
                    "authorId": "1666667717",
                    "name": "Karishma Malkan"
                },
                {
                    "authorId": "2306260471",
                    "name": "Jinyeong Yim"
                },
                {
                    "authorId": "1798404",
                    "name": "John Palowitch"
                },
                {
                    "authorId": "2306121535",
                    "name": "Sungyong Seo"
                },
                {
                    "authorId": "101461191",
                    "name": "Jonathan J. Halcrow"
                },
                {
                    "authorId": "2271808",
                    "name": "Bryan Perozzi"
                }
            ]
        },
        {
            "paperId": "8daccc22d52c278b9468075fec06b205fed53d71",
            "title": "Into the Unknown: Generating Geospatial Descriptions for New Environments",
            "abstract": "Similar to vision-and-language navigation (VLN) tasks that focus on bridging the gap between vision and language for embodied navigation, the new Rendezvous (RVS) task requires reasoning over allocentric spatial relationships (independent of the observer's viewpoint) using non-sequential navigation instructions and maps. However, performance substantially drops in new environments with no training data. Using opensource descriptions paired with coordinates (e.g., Wikipedia) provides training data but suffers from limited spatially-oriented text resulting in low geolocation resolution. We propose a large-scale augmentation method for generating high-quality synthetic data for new environments using readily available geospatial data. Our method constructs a grounded knowledge-graph, capturing entity relationships. Sampled entities and relations (`shop north of school') generate navigation instructions via (i) generating numerous templates using context-free grammar (CFG) to embed specific entities and relations; (ii) feeding the entities and relation into a large language model (LLM) for instruction generation. A comprehensive evaluation on RVS, showed that our approach improves the 100-meter accuracy by 45.83% on unseen environments. Furthermore, we demonstrate that models trained with CFG-based augmentation achieve superior performance compared with those trained with LLM-based augmentation, both in unseen and seen environments. These findings suggest that the potential advantages of explicitly structuring spatial information for text-based geospatial reasoning in previously unknown, can unlock data-scarce scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1405328226",
                    "name": "Tzuf Paz-Argaman"
                },
                {
                    "authorId": "1798404",
                    "name": "John Palowitch"
                },
                {
                    "authorId": "2287841273",
                    "name": "Sayali Kulkarni"
                },
                {
                    "authorId": "2799181",
                    "name": "Reut Tsarfaty"
                },
                {
                    "authorId": "2287827633",
                    "name": "Jason Baldridge"
                }
            ]
        },
        {
            "paperId": "e8b195e915257d8dc9ae91ec7b054217b964899a",
            "title": "Where Do We Go From Here? Multi-scale Allocentric Relational Inferencefrom Natural Spatial Descriptions",
            "abstract": "The concept of acquired spatial knowledge is crucial in spatial cognitive research, particularly when it comes to communicating routes. However, NLP navigation studies often overlook the impact of acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., \u2018it will be on your right\u2019) that require reasoning over the agent\u2019s local perception. These instructions are typically given in a sequence of steps, with each action-step explicitly mentioned and followed by a landmark that the agent can use to verify that they are on the correct path (e.g., \u2018turn right and then you will see...\u2019). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its compositionality. These instructions typically contain allocentric relations, are non-sequential, with implicit actions and multiple spatial relations without any verification (e.g., \u2018south of Central Park and a block north of a police station\u2019). This paper introduces the Rendezvous (RVS) task and dataset, which includes 10,404 examples of English geospatial instructions for reaching a target location using map-knowledge. Our analysis reveals that RVS exhibits a richer use of spatial allocentric relations, and requires resolving more spatial relations simultaneously compared to previous text-based navigation benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1405328226",
                    "name": "Tzuf Paz-Argaman"
                },
                {
                    "authorId": "2287841273",
                    "name": "Sayali Kulkarni"
                },
                {
                    "authorId": "1798404",
                    "name": "John Palowitch"
                },
                {
                    "authorId": "2287827633",
                    "name": "Jason Baldridge"
                },
                {
                    "authorId": "2799181",
                    "name": "Reut Tsarfaty"
                }
            ]
        },
        {
            "paperId": "2be6351a9e6ebe95d729e23700305c2bf3d219d7",
            "title": "Examining the Effects of Degree Distribution and Homophily in Graph Learning Models",
            "abstract": "Despite a surge in interest in GNN development, homogeneity in benchmarking datasets still presents a fundamental issue to GNN research. GraphWorld is a recent solution which uses the Stochastic Block Model (SBM) to generate diverse populations of synthetic graphs for benchmarking any GNN task. Despite its success, the SBM imposed fundamental limitations on the kinds of graph structure GraphWorld could create. In this work we examine how two additional synthetic graph generators can improve GraphWorld's evaluation; LFR, a well-established model in the graph clustering literature and CABAM, a recent adaptation of the Barabasi-Albert model tailored for GNN benchmarking. By integrating these generators, we significantly expand the coverage of graph space within the GraphWorld framework while preserving key graph properties observed in real-world networks. To demonstrate their effectiveness, we generate 300,000 graphs to benchmark 11 GNN models on a node classification task. We find GNN performance variations in response to homophily, degree distribution and feature signal. Based on these findings, we classify models by their sensitivity to the new generators under these properties. Additionally, we release the extensions made to GraphWorld on the GitHub repository, offering further evaluation of GNN performance on new graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223756267",
                    "name": "Mustafa Yasir"
                },
                {
                    "authorId": "1798404",
                    "name": "John Palowitch"
                },
                {
                    "authorId": "40900939",
                    "name": "Anton Tsitsulin"
                },
                {
                    "authorId": "1404639619",
                    "name": "Long Tran-Thanh"
                },
                {
                    "authorId": "2271808",
                    "name": "Bryan Perozzi"
                }
            ]
        },
        {
            "paperId": "0802a59d053f05f7b5aca74b6d7b070f8e78c278",
            "title": "Graph Generative Model for Benchmarking Graph Neural Networks",
            "abstract": "As the field of Graph Neural Networks (GNN) continues to grow, it experiences a corresponding increase in the need for large, real-world datasets to train and test new GNN models on challenging, realistic problems. Unfortunately, such graph datasets are often generated from online, highly privacy-restricted ecosystems, which makes research and development on these datasets hard, if not impossible. This greatly reduces the amount of benchmark graphs available to researchers, causing the field to rely only on a handful of publicly-available datasets. To address this problem, we introduce a novel graph generative model, Computation Graph Transformer (CGT) that learns and reproduces the distribution of real-world graphs in a privacy-controlled way. More specifically, CGT (1) generates effective benchmark graphs on which GNNs show similar task performance as on the source graphs, (2) scales to process large-scale graphs, (3) incorporates off-the-shelf privacy modules to guarantee end-user privacy of the generated graph. Extensive experiments across a vast body of graph generative models show that only our model can successfully generate privacy-controlled, synthetic substitutes of large-scale real-world graphs that can be effectively used to benchmark GNN models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50736424",
                    "name": "Minji Yoon"
                },
                {
                    "authorId": "2109035755",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "1798404",
                    "name": "John Palowitch"
                },
                {
                    "authorId": "2271808",
                    "name": "Bryan Perozzi"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                }
            ]
        },
        {
            "paperId": "2866eb812228481f4c3890dbd3fb72a112d928c9",
            "title": "Zero-shot Transfer Learning within a Heterogeneous Graph via Knowledge Transfer Networks",
            "abstract": "Data continuously emitted from industrial ecosystems such as social or e-commerce platforms are commonly represented as heterogeneous graphs (HG) composed of multiple node/edge types. State-of-the-art graph learning methods for HGs known as heterogeneous graph neural networks (HGNNs) are applied to learn deep context-informed node representations. However, many HG datasets from industrial applications suffer from label imbalance between node types. As there is no direct way to learn using labels rooted at different node types, HGNNs have been applied to only a few node types with abundant labels. We propose a zero-shot transfer learning module for HGNNs called a Knowledge Transfer Network (KTN) that transfers knowledge from label-abundant node types to zero-labeled node types through rich relational information given in the HG. KTN is derived from the theoretical relationship, which we introduce in this work, between distinct feature extractors for each node type given in an HGNN model. KTN improves performance of 6 different types of HGNN models by up to 960% for inference on zero-labeled node types and outperforms state-of-the-art transfer learning baselines by up to 73% across 18 different transfer learning tasks on HGs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50736424",
                    "name": "Minji Yoon"
                },
                {
                    "authorId": "1798404",
                    "name": "John Palowitch"
                },
                {
                    "authorId": "1389613483",
                    "name": "Dustin Zelle"
                },
                {
                    "authorId": null,
                    "name": "Ziniu Hu"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                },
                {
                    "authorId": "2271808",
                    "name": "Bryan Perozzi"
                }
            ]
        },
        {
            "paperId": "537eb35f9051bfe8f8f2a5fd5231a75f6a81049e",
            "title": "Scalable Privacy-enhanced Benchmark Graph Generative Model for Graph Convolutional Networks",
            "abstract": "A surge of interest in Graph Convolutional Networks (GCN) has produced thou-sands of GCN variants, with hundreds introduced every year. In contrast, many GCN models re-use only a handful of benchmark datasets as many graphs of interest, such as social or commercial networks, are proprietary. We propose a new graph generation problem to enable generating a diverse set of benchmark graphs for GCNs following the distribution of a source graph \u2014 possibly proprietary \u2014 with three requirements: 1) benchmark effectiveness as a substitute for the source graph for GCN research, 2) scalability to process large-scale real-world graphs, and 3) a privacy guarantee for end-users. With a novel graph encoding scheme, we reframe large-scale graph generation problem into medium-length sequence generation problem and apply the strong generation power of the Transformer architecture to the graph domain. Extensive experiments across a vast body of graph generative models show that our model can successfully generate benchmark graphs with the realistic graph structure, node attributes, and node labels required to benchmark GCNs on node classi\ufb01cation tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50736424",
                    "name": "Minji Yoon"
                },
                {
                    "authorId": "46220633",
                    "name": "Yue Wu"
                },
                {
                    "authorId": "1798404",
                    "name": "John Palowitch"
                },
                {
                    "authorId": "2271808",
                    "name": "Bryan Perozzi"
                },
                {
                    "authorId": "145124475",
                    "name": "R. Salakhutdinov"
                }
            ]
        },
        {
            "paperId": "79847a6027001eddc19b7639d2660e6d50ec987c",
            "title": "Synthetic Graph Generation to Benchmark Graph Learning",
            "abstract": "Graph learning algorithms have attained state-of-the-art performance on many graph analysis tasks such as node classification, link prediction, and clustering. It has, however, become hard to track the field's burgeoning progress. One reason is due to the very small number of datasets used in practice to benchmark the performance of graph learning algorithms. This shockingly small sample size (~10) allows for only limited scientific insight into the problem. In this work, we aim to address this deficiency. We propose to generate synthetic graphs, and study the behaviour of graph learning algorithms in a controlled scenario. We develop a fully-featured synthetic graph generator that allows deep inspection of different models. We argue that synthetic graph generations allows for thorough investigation of algorithms and provides more insights than overfitting on three citation datasets. In the case study, we show how our framework provides insight into unsupervised and supervised graph neural network models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40900939",
                    "name": "Anton Tsitsulin"
                },
                {
                    "authorId": "35806328",
                    "name": "Benedek Rozemberczki"
                },
                {
                    "authorId": "1798404",
                    "name": "John Palowitch"
                },
                {
                    "authorId": "2271808",
                    "name": "Bryan Perozzi"
                }
            ]
        }
    ]
}