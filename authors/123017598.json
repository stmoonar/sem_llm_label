{
    "authorId": "123017598",
    "papers": [
        {
            "paperId": "06a5bb29acbe993452b6d941ebb039bb11207881",
            "title": "WebQuIn-LD: A Method of Integrating Web Query Interfaces Based on Linked Data",
            "abstract": "The deep web is a huge source of domain-specific information (sale of houses, medical information, e-commerce, science, etc) stored in database servers accessible through HTML forms called web query interfaces (WQIs). Information in the deep web is retrieved by querying one database server at a time, which results inefficient. A more attractive approach is to create an integrated WQI (IWQI) that acts as single entry point to query several database servers at a time for a given domain. Schema matching and string (labels in WQIs) comparison have been the most popular techniques to create IWQIs. In this work, we propose a new method for the integration of web forms based on linked data and the VDIS (View-based Data Integration System) architecture. We present WebQuIn-LD, an alternative and novel approach relying on linked data principles to combine individual WQIs into a single IWQI for a given domain is presented. WebQuIn-LD follows a data integration system architecture, starting from the wrapping of domain-specific WQIs until the creation of the IWQI. A domain-independent ontology is created to describe WQI elements as linked data resources and to exploit semantic integration between the WQI\u2019s elements. WebQuIn-LD was evaluated on performance metrics (precision, recall, and F1) using the state-of-the-art WQIs datasets for different domains (airfares, books, autos, jobs, music, movies, hotels, jobs). The obtained results demonstrate the effectiveness of the linked data approach presented in this work for the WQI integration problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123017598",
                    "name": "Julio Hernandez"
                },
                {
                    "authorId": "1405706729",
                    "name": "H. Marin-Castro"
                },
                {
                    "authorId": "1400298663",
                    "name": "M. Morales-Sandoval"
                }
            ]
        },
        {
            "paperId": "61a5e011a8a4b69eb1446e78f62cadd6b24f1872",
            "title": "A Semantic Focused Web Crawler Based on a Knowledge Representation Schema",
            "abstract": "The Web has become the main source of information in the digital world, expanding to heterogeneous domains and continuously growing. By means of a search engine, users can systematically search over the web for particular information based on a text query, on the basis of a domain-unaware web search tool that maintains real-time information. One type of web search tool is the semantic focused web crawler (SFWC); it exploits the semantics of the Web based on some ontology heuristics to determine which web pages belong to the domain defined by the query. An SFWC is highly dependent on the ontological resource, which is created by domain human experts. This work presents a novel SFWC based on a generic knowledge representation schema to model the crawler\u2019s domain, thus reducing the complexity and cost of constructing a more formal representation as the case when using ontologies. Furthermore, a similarity measure based on the combination of the inverse document frequency (IDF) metric, standard deviation, and the arithmetic mean is proposed for the SFWC. This measure filters web page contents in accordance with the domain of interest during the crawling task. A set of experiments were run over the domains of computer science, politics, and diabetes to validate and evaluate the proposed novel crawler. The quantitative (harvest ratio) and qualitative (Fleiss\u2019 kappa) evaluations demonstrate the suitability of the proposed SFWC to crawl the Web using a knowledge representation schema instead of a domain ontology.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "123017598",
                    "name": "Julio Hernandez"
                },
                {
                    "authorId": "1405706729",
                    "name": "H. Marin-Castro"
                },
                {
                    "authorId": "1400298663",
                    "name": "M. Morales-Sandoval"
                }
            ]
        },
        {
            "paperId": "30596da17f14943eb98f428844e100724e28456d",
            "title": "DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction Corpus",
            "abstract": "In the past decade, the DBpedia community has put significant amount of effort on developing technical infrastructure and methods for efficient extraction of structured information from Wikipedia. These efforts have been primarily focused on harvesting, refinement and publishing semi-structured information found in Wikipedia articles, such as information from infoboxes, categorization information, images, wikilinks and citations. Nevertheless, still vast amount of valuable information is contained in the unstructured Wikipedia article texts. In this paper, we present DBpedia NIF - a large-scale and multilingual knowledge extraction corpus. The aim of the dataset is two-fold: to dramatically broaden and deepen the amount of structured information in DBpedia, and to provide large-scale and multilingual language resource for development of various NLP and IR task. The dataset provides the content of all articles for 128 Wikipedia languages. We describe the dataset creation process and the NLP Interchange Format (NIF) used to model the content, links and the structure the information of the Wikipedia articles. The dataset has been further enriched with about 25% more links and selected partitions published as Linked Data. Finally, we describe the maintenance and sustainability plans, and selected use cases of the dataset from the TextExt knowledge extraction challenge.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "123017598",
                    "name": "Julio Hernandez"
                },
                {
                    "authorId": "37683106",
                    "name": "Markus Ackermann"
                },
                {
                    "authorId": "35114918",
                    "name": "Amit Kirschenbaum"
                },
                {
                    "authorId": "2024066",
                    "name": "Sebastian Hellmann"
                }
            ]
        },
        {
            "paperId": "a8abea4600259577bce298ff67c7c802c2356d84",
            "title": "A Strategy for the Integration of Named Entity Extraction and Linking Results",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1410228481",
                    "name": "Jos\u00e9-L\u00e1zaro Mart\u00ednez-Rodr\u00edguez"
                },
                {
                    "authorId": "123017598",
                    "name": "Julio Hernandez"
                },
                {
                    "authorId": "1399158616",
                    "name": "I. Lopez-Arevalo"
                },
                {
                    "authorId": "1403652408",
                    "name": "Ana B. R\u00edos-Alvarado"
                }
            ]
        }
    ]
}