{
    "authorId": "1804618",
    "papers": [
        {
            "paperId": "0030c0e60e3a9f76aa904d8bd5266ea4496f8981",
            "title": "Exploring Fusion Techniques in Multimodal AI-Based Recruitment: Insights from FairCVdb",
            "abstract": "Despite the large body of work on fairness-aware learning for individual modalities like tabular data, images, and text, less work has been done on multimodal data, which fuses various modalities for a comprehensive analysis. In this work, we investigate the fairness and bias implications of multimodal fusion techniques in the context of multimodal AI-based recruitment systems using the FairCVdb dataset. Our results show that early-fusion closely matches the ground truth for both demographics, achieving the lowest MAEs by integrating each modality's unique characteristics. In contrast, late-fusion leads to highly generalized mean scores and higher MAEs. Our findings emphasise the significant potential of early-fusion for accurate and fair applications, even in the presence of demographic biases, compared to late-fusion. Future research could explore alternative fusion strategies and incorporate modality-related fairness constraints to improve fairness. For code and additional insights, visit: https://github.com/Swati17293/Multimodal-AI-Based-Recruitment-FairCVdb",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2312756830",
                    "name": "Swati Swati"
                },
                {
                    "authorId": "47762613",
                    "name": "Arjun Roy"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                }
            ]
        },
        {
            "paperId": "1f7e184311bac39d6eb22f8ca3a21340a6363287",
            "title": "Towards Cohesion-Fairness Harmony: Contrastive Regularization in Individual Fair Graph Clustering",
            "abstract": "Conventional fair graph clustering methods face two primary challenges: i) They prioritize balanced clusters at the expense of cluster cohesion by imposing rigid constraints, ii) Existing methods of both individual and group-level fairness in graph partitioning mostly rely on eigen decompositions and thus, generally lack interpretability. To address these issues, we propose iFairNMTF, an individual Fairness Nonnegative Matrix Tri-Factorization model with contrastive fairness regularization that achieves balanced and cohesive clusters. By introducing fairness regularization, our model allows for customizable accuracy-fairness trade-offs, thereby enhancing user autonomy without compromising the interpretability provided by nonnegative matrix tri-factorization. Experimental evaluations on real and synthetic datasets demonstrate the superior flexibility of iFairNMTF in achieving fairness and clustering performance.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "33340594",
                    "name": "Siamak Ghodsi"
                },
                {
                    "authorId": "9417855",
                    "name": "Seyed Amjad Seyedi"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                }
            ]
        },
        {
            "paperId": "52441fd040c57b7fbb17357390c968d3460ffdc3",
            "title": "Effector: A Python package for regional explanations",
            "abstract": "Global feature effect methods explain a model outputting one plot per feature. The plot shows the average effect of the feature on the output, like the effect of age on the annual income. However, average effects may be misleading when derived from local effects that are heterogeneous, i.e., they significantly deviate from the average. To decrease the heterogeneity, regional effects provide multiple plots per feature, each representing the average effect within a specific subspace. For interpretability, subspaces are defined as hyperrectangles defined by a chain of logical rules, like age's effect on annual income separately for males and females and different levels of professional experience. We introduce Effector, a Python library dedicated to regional feature effects. Effector implements well-established global effect methods, assesses the heterogeneity of each method and, based on that, provides regional effects. Effector automatically detects subspaces where regional effects have reduced heterogeneity. All global and regional effect methods share a common API, facilitating comparisons between them. Moreover, the library's interface is extensible so new methods can be easily added and benchmarked. The library has been thoroughly tested, ships with many tutorials (https://xai-effector.github.io/) and is available under an open-source license at PyPi (https://pypi.org/project/effector/) and Github (https://github.com/givasile/effector).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187300032",
                    "name": "Vasilis Gkolemis"
                },
                {
                    "authorId": "2165480599",
                    "name": "Christos Diou"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                },
                {
                    "authorId": "2243286151",
                    "name": "Theodore Dalamagas"
                },
                {
                    "authorId": "1686924",
                    "name": "B. Bischl"
                },
                {
                    "authorId": "151051067",
                    "name": "J. Herbinger"
                },
                {
                    "authorId": "2254264498",
                    "name": "Giuseppe Casalicchio"
                }
            ]
        },
        {
            "paperId": "9e71ca998b3a6caabdd10274e5f53d3d6d483cee",
            "title": "Sum of Group Error Differences: A Critical Examination of Bias Evaluation in Biometric Verification and a Dual-Metric Measure",
            "abstract": "Biometric Verification (BV) systems often exhibit accuracy disparities across different demographic groups, leading to biases in BV applications. Assessing and quantifying these biases is essential for ensuring the fairness of BV systems. However, existing bias evaluation metrics in BV have limitations, such as focusing exclusively on match or non-match error rates, overlooking bias on demographic groups with performance levels falling between the best and worst performance levels, and neglecting the magnitude of the bias present. This paper presents an in-depth analysis of the limitations of current bias evaluation metrics in BV and, through experimental analysis, demonstrates their contextual suitability, merits, and limitations. Additionally, it introduces a novel general-purpose bias evaluation measure for BV, the \u201cSum of Group Error Differences $(\\mathbf{SED}_{G})$\u201d. Our experimental results on controlled synthetic datasets demonstrate the effectiveness of demographic bias quantification when using existing metrics and our own proposed measure. We discuss the applicability of the bias evaluation metrics in a set of simulated demographic bias scenarios and provide scenario-based metric recommendations. Our code is publicly available under https://github.com/alaaobeid/SEDG.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2298041863",
                    "name": "Alaa Elobaid"
                },
                {
                    "authorId": "3448759",
                    "name": "Nathan Ramoly"
                },
                {
                    "authorId": "2298041329",
                    "name": "Lara Younes"
                },
                {
                    "authorId": "2237546962",
                    "name": "Symeon Papadopoulos"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                },
                {
                    "authorId": "1715604",
                    "name": "Y. Kompatsiaris"
                }
            ]
        },
        {
            "paperId": "ecf8cba5cf48d4814549dbd5435c9f6c4804074c",
            "title": "Adversarial Robustness of VAEs across Intersectional Subgroups",
            "abstract": "Despite advancements in Autoencoders (AEs) for tasks like dimensionality reduction, representation learning and data generation, they remain vulnerable to adversarial attacks. Variational Autoencoders (VAEs), with their probabilistic approach to disentangling latent spaces, show stronger resistance to such perturbations compared to deterministic AEs; however, their resilience against adversarial inputs is still a concern. This study evaluates the robustness of VAEs against non-targeted adversarial attacks by optimizing minimal sample-specific perturbations to cause maximal damage across diverse demographic subgroups (combinations of age and gender). We investigate two questions: whether there are robustness disparities among subgroups, and what factors contribute to these disparities, such as data scarcity and representation entanglement. Our findings reveal that robustness disparities exist but are not always correlated with the size of the subgroup. By using downstream gender and age classifiers and examining latent embeddings, we highlight the vulnerability of subgroups like older women, who are prone to misclassification due to adversarial perturbations pushing their representations toward those of other subgroups.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2243009308",
                    "name": "Chethan Krishnamurthy Ramanaik"
                },
                {
                    "authorId": "47762613",
                    "name": "Arjun Roy"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                }
            ]
        },
        {
            "paperId": "5b896c68945433b4135ec4c28b86e99ddd5a6908",
            "title": "RHALE: Robust and Heterogeneity-aware Accumulated Local Effects",
            "abstract": "Accumulated Local Effects (ALE) is a widely-used explainability method for isolating the average effect of a feature on the output, because it handles cases with correlated features well. However, it has two limitations. First, it does not quantify the deviation of instance-level (local) effects from the average (global) effect, known as heterogeneity. Second, for estimating the average effect, it partitions the feature domain into user-defined, fixed-sized bins, where different bin sizes may lead to inconsistent ALE estimations. To address these limitations, we propose Robust and Heterogeneity-aware ALE (RHALE). RHALE quantifies the heterogeneity by considering the standard deviation of the local effects and automatically determines an optimal variable-size bin-splitting. In this paper, we prove that to achieve an unbiased approximation of the standard deviation of local effects within each bin, bin splitting must follow a set of sufficient conditions. Based on these conditions, we propose an algorithm that automatically determines the optimal partitioning, balancing the estimation bias and variance. Through evaluations on synthetic and real datasets, we demonstrate the superiority of RHALE compared to other methods, including the advantages of automatic bin splitting, especially in cases with correlated features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187300032",
                    "name": "Vasilis Gkolemis"
                },
                {
                    "authorId": "2243286151",
                    "name": "Theodore Dalamagas"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                },
                {
                    "authorId": "2165480599",
                    "name": "Christos Diou"
                }
            ]
        },
        {
            "paperId": "6592ccf3c100b62690e353a695dcfddeec3b24eb",
            "title": "Explaining text classifiers through progressive neighborhood approximation with realistic samples",
            "abstract": "The importance of neighborhood construction in local explanation methods has been already highlighted in the literature. And several attempts have been made to improve neighborhood quality for high-dimensional data, for example, texts, by adopting generative models. Although the generators produce more realistic samples, the intuitive sampling approaches in the existing solutions leave the latent space underexplored. To overcome this problem, our work, focusing on local model-agnostic explanations for text classifiers, proposes a progressive approximation approach that refines the neighborhood of a to-be-explained decision with a careful two-stage interpolation using counterfactuals as landmarks. We explicitly specify the two properties that should be satisfied by generative models, the reconstruction ability and the locality-preserving property, to guide the selection of generators for local explanation methods. Moreover, noticing the opacity of generative models during the study, we propose another method that implements progressive neighborhood approximation with probability-based editions as an alternative to the generator-based solution. The explanation results from both methods consist of word-level and instance-level explanations benefiting from the realistic neighborhood. Through exhaustive experiments, we qualitatively and quantitatively demonstrate the effectiveness of the two proposed methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1752876325",
                    "name": "Yi Cai"
                },
                {
                    "authorId": "3310376",
                    "name": "Arthur Zimek"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                },
                {
                    "authorId": "1721084",
                    "name": "G. Wunder"
                }
            ]
        },
        {
            "paperId": "68b7f0e2d37061c23dcdc32ebd73fb51789c2d55",
            "title": "Affinity Clustering Framework for Data Debiasing Using Pairwise Distribution Discrepancy",
            "abstract": "Group imbalance, resulting from inadequate or unrepresentative data collection methods, is a primary cause of representation bias in datasets. Representation bias can exist with respect to different groups of one or more protected attributes and might lead to prejudicial and discriminatory outcomes toward certain groups of individuals; in cases where a learning model is trained on such biased data. This paper presents MASC, a data augmentation approach that leverages affinity clustering to balance the representation of non-protected and protected groups of a target dataset by utilizing instances of the same protected attributes from similar datasets that are categorized in the same cluster as the target dataset by sharing instances of the protected attribute. The proposed method involves constructing an affinity matrix by quantifying distribution discrepancies between dataset pairs and transforming them into a symmetric pairwise similarity matrix. A non-parametric spectral clustering is then applied to this affinity matrix, automatically categorizing the datasets into an optimal number of clusters. We perform a step-by-step experiment as a demo of our method to show the procedure of the proposed data augmentation method and evaluate and discuss its performance. A comparison with other data augmentation methods, both pre- and post-augmentation, is conducted, along with a model evaluation analysis of each method. Our method can handle non-binary protected attributes so, in our experiments, bias is measured in a non-binary protected attribute setup w.r.t. racial groups distribution for two separate minority groups in comparison with the majority group before and after debiasing. Empirical results imply that our method of augmenting dataset biases using real (genuine) data from similar contexts can effectively debias the target datasets comparably to existing data augmentation strategies.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "33340594",
                    "name": "Siamak Ghodsi"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                }
            ]
        },
        {
            "paperId": "8a545843b0902b1d8c1773d770f4113a5e665f94",
            "title": "FairBranch: Fairness Conflict Correction on Task-group Branches for Fair Multi-Task Learning",
            "abstract": "The generalisation capacity of Multi-Task Learning ( MTL ), gets limited when unrelated tasks negatively impact each other by updating shared parameters with conflicting gradients. This is known as negative transfer and leads to a drop in MTL accuracy compared to single-task learning ( STL ). Lately, there has been a growing focus on the fairness of MTL models, requiring the optimization of both accuracy and fairness for individual tasks. Analogously to negative transfer for accuracy, task-specific fairness considerations might adversely affect the fairness of other tasks when there is a conflict of fairness loss gradients between the jointly learned tasks - we refer to this as bias transfer . To address both negative-and bias-transfer in MTL , we propose a novel method called FairBranch , which branches the MTL model by assessing the similarity of learned parameters, thereby grouping related tasks to alleviate negative transfer. Moreover, it incorporates fairness loss gradient conflict correction between ad-joining task-group branches to address bias transfer within these task groups. Our experiments on tabular and visual MTL problems show that FairBranch outperforms state-of-the-art MTL s on both fairness and accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47762613",
                    "name": "Arjun Roy"
                },
                {
                    "authorId": "2036760",
                    "name": "C. Koutlis"
                },
                {
                    "authorId": "2237546962",
                    "name": "Symeon Papadopoulos"
                },
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                }
            ]
        },
        {
            "paperId": "aac4c300e38cf60d7b2542f13fbfbef5f5eafed6",
            "title": "Keynote Speech 2: Bias and Discrimination in AI Systems: From Single-Identity Dimensions to Multi-Discrimination",
            "abstract": "AI-driven decision-making has become pervasive in various aspects of our lives, impacting everyone, everywhere, at any time. However, concerns have arisen regarding the discriminatory effects of AI, as evidenced across diverse domains like content recommendation, healthcare, predictive policing, and autonomous driving. The field of fairness-aware machine learning aims to address bias and discrimination in AI/ML models, but most existing approaches focus on discrimination related to a single protected attribute, such as gender or race. In reality, human identities are multi-dimensional, and discrimination can arise from multiple protected characteristics, for example a combination of gender, race and age. In this talk, I will explore fairness and discrimination in supervised learning for tabular data, progressing from traditional single-attribute discrimination to addressing the complexities of multi-dimensional discrimination.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1804618",
                    "name": "Eirini Ntoutsi"
                }
            ]
        }
    ]
}