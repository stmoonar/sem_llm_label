{
    "authorId": "2007227598",
    "papers": [
        {
            "paperId": "56ce2e63d91c8ec1a999f552cbf9918c39006b12",
            "title": "Better Representations via Adversarial Training in Pre-Training: A Theoretical Perspective",
            "abstract": "Pre-training is known to generate universal representations for downstream tasks in large-scale deep learning such as large language models. Existing literature, e.g., \\cite{kim2020adversarial}, empirically observe that the downstream tasks can inherit the adversarial robustness of the pre-trained model. We provide theoretical justifications for this robustness inheritance phenomenon. Our theoretical results reveal that feature purification plays an important role in connecting the adversarial robustness of the pre-trained model and the downstream tasks in two-layer neural networks. Specifically, we show that (i) with adversarial training, each hidden node tends to pick only one (or a few) feature; (ii) without adversarial training, the hidden nodes can be vulnerable to attacks. This observation is valid for both supervised pre-training and contrastive learning. With purified nodes, it turns out that clean training is enough to achieve adversarial robustness in downstream tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "152136097",
                    "name": "Yue Xing"
                },
                {
                    "authorId": "2261476161",
                    "name": "Xiaofeng Lin"
                },
                {
                    "authorId": "2281748333",
                    "name": "Qifan Song"
                },
                {
                    "authorId": "2313040855",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "2154595116",
                    "name": "Guang Cheng"
                }
            ]
        },
        {
            "paperId": "a550cce0685621e7653d1deb6e810a0e1331ce9e",
            "title": "VidLA: Video-Language Alignment at Scale",
            "abstract": "In this paper, we propose VidLA, an approach for video-language alignment at scale. There are two major limitations of previous video-language alignment approaches. First, they do not capture both short-range and long-range temporal dependencies and typically employ complex hierarchical deep network architectures that are hard to integrate with existing pretrained image-text foundation models. To effectively address this limitation, we instead keep the network architecture simple and use a set of data tokens that operate at different temporal resolutions in a hierarchical manner, accounting for the temporally hierarchical nature of videos. By employing a simple two-tower architecture, we are able to initialize our video-language model with pretrained image-text foundation models, thereby boosting the final performance. Second, existing video-language alignment works struggle due to the lack of semantically aligned large-scale training data. To overcome it, we leverage recent LLMs to curate the largest video-language dataset to date with better visual grounding. Furthermore, unlike existing video-text datasets which only contain short clips, our dataset is enriched with video clips of varying durations to aid our temporally hierarchical data to-kens in extracting better representations at varying temporal scales. Overall, empirical results show that our proposed approach surpasses state-of-the-art methods on Multiple retrieval benchmarks, especially on longer videos, and performs competitively on classification benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9247631",
                    "name": "Mamshad Nayeem Rizve"
                },
                {
                    "authorId": "2293171909",
                    "name": "Fan Fei"
                },
                {
                    "authorId": "2293171991",
                    "name": "Jayakrishnan Unnikrishnan"
                },
                {
                    "authorId": "2293171936",
                    "name": "Son Tran"
                },
                {
                    "authorId": "2293171962",
                    "name": "Benjamin Yao"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "2258333118",
                    "name": "Mubarak Shah"
                },
                {
                    "authorId": "3191220",
                    "name": "Trishul M. Chilimbi"
                }
            ]
        },
        {
            "paperId": "c5274ab0368de03859793840945dc865bcef1044",
            "title": "Robust Multi-Task Learning with Excess Risks",
            "abstract": "Multi-task learning (MTL) considers learning a joint model for multiple tasks by optimizing a convex combination of all task losses. To solve the optimization problem, existing methods use an adaptive weight updating scheme, where task weights are dynamically adjusted based on their respective losses to prioritize difficult tasks. However, these algorithms face a great challenge whenever label noise is present, in which case excessive weights tend to be assigned to noisy tasks that have relatively large Bayes optimal errors, thereby overshadowing other tasks and causing performance to drop across the board. To overcome this limitation, we propose Multi-Task Learning with Excess Risks (ExcessMTL), an excess risk-based task balancing method that updates the task weights by their distances to convergence instead. Intuitively, ExcessMTL assigns higher weights to worse-trained tasks that are further from convergence. To estimate the excess risks, we develop an efficient and accurate method with Taylor approximation. Theoretically, we show that our proposed algorithm achieves convergence guarantees and Pareto stationarity. Empirically, we evaluate our algorithm on various MTL benchmarks and demonstrate its superior performance over existing methods in the presence of label noise. Our code is available at https://github.com/yifei-he/ExcessMTL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182670937",
                    "name": "Yifei He"
                },
                {
                    "authorId": "2282595036",
                    "name": "Shiji Zhou"
                },
                {
                    "authorId": "2282971167",
                    "name": "Guojun Zhang"
                },
                {
                    "authorId": "2310438793",
                    "name": "Hyokun Yun"
                },
                {
                    "authorId": "2280583879",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "3191220",
                    "name": "Trishul M. Chilimbi"
                },
                {
                    "authorId": "2283183420",
                    "name": "Han Zhao"
                }
            ]
        },
        {
            "paperId": "e2adac41613679dd8f0ae28d72cfa0ea14e8fd10",
            "title": "Diffusion Models For Multi-Modal Generative Modeling",
            "abstract": "Diffusion-based generative modeling has been achieving state-of-the-art results on various generation tasks. Most diffusion models, however, are limited to a single-generation modeling. Can we generalize diffusion models with the ability of multi-modal generative training for more generalizable modeling? In this paper, we propose a principled way to define a diffusion model by constructing a unified multi-modal diffusion model in a common diffusion space. We define the forward diffusion process to be driven by an information aggregation from multiple types of task-data, e.g., images for a generation task and labels for a classification task. In the reverse process, we enforce information sharing by parameterizing a shared backbone denoising network with additional modality-specific decoder heads. Such a structure can simultaneously learn to generate different types of multi-modal data with a multi-task loss, which is derived from a new multi-modal variational lower bound that generalizes the standard diffusion model. We propose several multimodal generation settings to verify our framework, including image transition, masked-image training, joint image-label and joint image-representation generative modeling. Extensive experimental results on ImageNet indicate the effectiveness of our framework for various multi-modal generative modeling, which we believe is an important research direction worthy of more future explorations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2312933839",
                    "name": "Changyou Chen"
                },
                {
                    "authorId": "2313056852",
                    "name": "Han Ding"
                },
                {
                    "authorId": "2582063",
                    "name": "Bunyamin Sisman"
                },
                {
                    "authorId": "2313040855",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2312923976",
                    "name": "Ouye Xie"
                },
                {
                    "authorId": "2312924808",
                    "name": "Benjamin Z. Yao"
                },
                {
                    "authorId": "2312923840",
                    "name": "Son Dinh Tran"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                }
            ]
        },
        {
            "paperId": "191bd8f008f80883bba1fa38908e2c836a5f7bbe",
            "title": "ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models",
            "abstract": "Knowledge Distillation (KD) is one of the most effective approaches to deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the large-scale models to smaller student models.Prior KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacher\u2019s soft labels and predictions can further improve student generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new framework and loss function that preserves the semantic similarities of teacher and student training examples. We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for task-specific knowledge distillation on the GLUE benchmark.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047971184",
                    "name": "Jianyi Zhang"
                },
                {
                    "authorId": "2042620448",
                    "name": "Aashiq Muhamed"
                },
                {
                    "authorId": "2078216680",
                    "name": "Aditya Anantharaman"
                },
                {
                    "authorId": "2107926840",
                    "name": "Guoyin Wang"
                },
                {
                    "authorId": "1752041",
                    "name": "Changyou Chen"
                },
                {
                    "authorId": "144474630",
                    "name": "Kai Zhong"
                },
                {
                    "authorId": "2057040752",
                    "name": "Qi Cui"
                },
                {
                    "authorId": "2110290078",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "3191220",
                    "name": "Trishul M. Chilimbi"
                },
                {
                    "authorId": "2213153798",
                    "name": "Yiran Chen"
                }
            ]
        },
        {
            "paperId": "1c3b47117fb0877ecd9e7fc9e8db7b7948432d8f",
            "title": "ForeSeer: Product Aspect Forecasting Using Temporal Graph Embedding",
            "abstract": "Developing text mining approaches to mine aspects from customer reviews has been well-studied due to its importance in understanding customer needs and product attributes. In contrast, it remains unclear how to predict the future emerging aspects of a new product that currently has little review information. This task, which we named product aspect forecasting, is critical for recommending new products, but also challenging because of the missing reviews. Here, we propose ForeSeer, a novel textual mining and product embedding approach progressively trained on temporal product graphs for this novel product aspect forecasting task. ForeSeer transfers reviews from similar products on a large product graph and exploits these reviews to predict aspects that might emerge in future reviews. A key novelty of our method is to jointly provide review, product, and aspect embeddings that are both time-sensitive and less affected by extremely imbalanced aspect frequencies. We evaluated ForeSeer on a real-world product review system containing 11,536,382 reviews and 11,000 products over 3 years. We observe that ForeSeer substantially outperformed existing approaches with at least 49.1% AUPRC improvement under the real setting where aspect associations are not given. ForeSeer further improves future link prediction on the product graph and the review aspect association prediction. Collectively, Foreseer offers a novel framework for review forecasting by effectively integrating review text, product network, and temporal information, opening up new avenues for online shopping recommendation and e-commerce applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2265222599",
                    "name": "Zixuan Liu"
                },
                {
                    "authorId": "46566733",
                    "name": "G. Hiranandani"
                },
                {
                    "authorId": "2257003517",
                    "name": "Kun Qian"
                },
                {
                    "authorId": "2057479333",
                    "name": "E-Wen Huang"
                },
                {
                    "authorId": "2257074252",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "2691095",
                    "name": "Karthik Subbian"
                },
                {
                    "authorId": "2280550850",
                    "name": "Sheng Wang"
                }
            ]
        },
        {
            "paperId": "24334d3666a7f3dfb33134ad74c931cf1a594604",
            "title": "SST: Semantic and Structural Transformers for Hierarchy-aware Language Models in E-commerce",
            "abstract": "Hierarchies are common structures used to organize data, such as e-commerce hierarchies associated with product data. With these product hierarchies, we aim to learn hierarchy-aware product text embeddings to improve fine-tuning performance on a variety of downstream e-commerce tasks. Existing methods leverage hierarchies by either aligning the text embeddings to separate hierarchical embeddings or by aligning the hierarchical information implicitly within a unified text Transformer. Although these models optimize to predict hierarchy information, performing further fine-tuning on new tasks is non-trivial. To bridge this gap, we propose a pre-training architecture to implicitly encode the hierarchy within the product text and then directly leverage a sub-set of the pre-training model during fine-tuning. Pre-training is done through Semantic and Structural Transformers (SST) where the Semantic-Transformer first encodes the product text into a contextual embedding, which is then used by the Structural-Transformer to infer the product\u2019s path in the hierarchy. Fine-tuning is done using only the initial Semantic-Transformer, now that hierarchy-aware text embeddings are learned. With this design, we eliminate the need of linking each fine-tuning dataset with corresponding hierarchies. This leads to fine-tuning performance improvements on critical e-commerce downstream tasks over the existing state-of-the-art hierarchy models, even when hierarchy data $is$ available during fine-tuning. Moreover, this improvement is consistent even after augmenting our baseline models to support fine-tuning. We conclude by discussing how such implicit structural encodings can be leveraged beyond the e-commerce domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51133706",
                    "name": "Karan Samel"
                },
                {
                    "authorId": "2280741034",
                    "name": "Houyu Zhang"
                },
                {
                    "authorId": "2280563037",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "2281442930",
                    "name": "Haoming Jiang"
                },
                {
                    "authorId": "2280574284",
                    "name": "Qing Ping"
                },
                {
                    "authorId": "2280550850",
                    "name": "Sheng Wang"
                },
                {
                    "authorId": "2280583879",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "3191220",
                    "name": "Trishul M. Chilimbi"
                }
            ]
        },
        {
            "paperId": "3090d5ef973e34e054ed520a118b2df8b16a5702",
            "title": "Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help Multiple Graph Applications",
            "abstract": "Model pre-training on large text corpora has been demonstrated effective for various downstream applications in the NLP domain. In the graph mining domain, a similar analogy can be drawn for pre-training graph models on large graphs in the hope of benefiting downstream graph applications, which has also been explored by several recent studies. However, no existing study has ever investigated the pre-training of text plus graph models on large heterogeneous graphs with abundant textual information (a.k.a. large graph corpora) and then fine-tuning the model on different related downstream applications with different graph schemas. To address this problem, we propose a framework of graph-aware language model pre-training (GaLM) on a large graph corpus, which incorporates large language models and graph neural networks, and a variety of fine-tuning methods on downstream applications. We conduct extensive experiments on Amazon's real internal datasets and large public datasets. Comprehensive empirical results and in-depth analysis demonstrate the effectiveness of our proposed methods along with lessons learned.",
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117714080",
                    "name": "Han Xie"
                },
                {
                    "authorId": "122579067",
                    "name": "Da Zheng"
                },
                {
                    "authorId": "65743795",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "92156482",
                    "name": "Houyu Zhang"
                },
                {
                    "authorId": "40043851",
                    "name": "V. Ioannidis"
                },
                {
                    "authorId": "2118943843",
                    "name": "Xiang Song"
                },
                {
                    "authorId": "40492634",
                    "name": "Q. Ping"
                },
                {
                    "authorId": "2151487092",
                    "name": "Sheng Wang"
                },
                {
                    "authorId": "2695365",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2110290078",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "3191220",
                    "name": "Trishul M. Chilimbi"
                }
            ]
        },
        {
            "paperId": "374dc9612e3507d1d3517492589c177a73be8e21",
            "title": "Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning",
            "abstract": "Contrastive loss has been increasingly used in learning representations from multiple modalities. In the limit, the nature of the contrastive loss encourages modalities to exactly match each other in the latent space. Yet it remains an open question how the modality alignment affects the downstream task performance. In this paper, based on an information-theoretic argument, we first prove that exact modality alignment is sub-optimal in general for down-stream prediction tasks. Hence we advocate that the key of better performance lies in meaningful latent modality structures instead of perfect modality alignment. To this end, we propose three general approaches to construct latent modality structures. Specifically, we design 1) a deep feature separation loss for intra-modality regularization; 2) a Brownian-bridge loss for inter-modality regularization; and 3) a geometric consistency loss for both intra- and intermodality regularization. Extensive experiments are conducted on two popular multi-modal representation learning frameworks: the CLIP-based two-tower model and the ALBEF-based fusion model. We test our model on a variety of tasks including zero/few-shot image classification, image-text retrieval, visual question answering, visual reasoning, and visual entailment. Our method achieves consistent improvements over existing methods, demonstrating the effectiveness and generalizability of our proposed approach on latent modality structure regularization.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113804323",
                    "name": "Qian Jiang"
                },
                {
                    "authorId": "1752041",
                    "name": "Changyou Chen"
                },
                {
                    "authorId": "2146232401",
                    "name": "Han Zhao"
                },
                {
                    "authorId": "2109077977",
                    "name": "Liqun Chen"
                },
                {
                    "authorId": "40492634",
                    "name": "Q. Ping"
                },
                {
                    "authorId": "32629693",
                    "name": "S. D. Tran"
                },
                {
                    "authorId": "2110290078",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "3191220",
                    "name": "Trishul M. Chilimbi"
                }
            ]
        },
        {
            "paperId": "0ad84c4bf7499df6945fc51b24ae2ac779f218ec",
            "title": "Vision-Language Pre-Training with Triple Contrastive Learning",
            "abstract": "Vision-language representation learning largely benefits from image-text alignment through contrastive losses (e.g., InfoNCE loss). The success of this alignment strategy is attributed to its capability in maximizing the mutual information (MI) between an image and its matched text. However, simply performing cross-modal alignment (CMA) ignores data potential within each modality, which may result in degraded representations. For instance, although CMA-based models are able to map image-text pairs close together in the embedding space, they fail to ensure that similar inputs from the same modality stay close by. This problem can get even worse when the pre-training data is noisy. In this paper, we propose triple contrastive learning (TCL) for vision-language pre-training by leveraging both cross-modal and intra-modal self-supervision. Besides CMA, TCL introduces an intra-modal contrastive objective to provide complementary benefits in representation learning. To take advantage of localized and structural information from image and text input, TCL further maximizes the average MI between local regions of image/text and their global summary. To the best of our knowledge, ours is the first work that takes into account local structure information for multi-modality representation learning. Experimental evaluations show that our approach is competitive and achieves the new state of the art on various common downstream vision-language tasks such as image-text retrieval and visual question answering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7788324",
                    "name": "Jinyu Yang"
                },
                {
                    "authorId": "152614324",
                    "name": "Jiali Duan"
                },
                {
                    "authorId": "1930235",
                    "name": "S. Tran"
                },
                {
                    "authorId": "2110290078",
                    "name": "Yi Xu"
                },
                {
                    "authorId": "150293116",
                    "name": "Sampath Chanda"
                },
                {
                    "authorId": "2109077977",
                    "name": "Liqun Chen"
                },
                {
                    "authorId": "2007227598",
                    "name": "Belinda Zeng"
                },
                {
                    "authorId": "3191220",
                    "name": "Trishul M. Chilimbi"
                },
                {
                    "authorId": "1768190",
                    "name": "Junzhou Huang"
                }
            ]
        }
    ]
}