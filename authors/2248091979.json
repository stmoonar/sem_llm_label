{
    "authorId": "2248091979",
    "papers": [
        {
            "paperId": "2c27967c92fda392cdf0654281df5a902bb62240",
            "title": "Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B",
            "abstract": "This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283952275",
                    "name": "Di Zhang"
                },
                {
                    "authorId": "2275029776",
                    "name": "Xiaoshui Huang"
                },
                {
                    "authorId": "2116324147",
                    "name": "Dongzhan Zhou"
                },
                {
                    "authorId": "2248091979",
                    "name": "Yuqiang Li"
                },
                {
                    "authorId": "2283918605",
                    "name": "Wanli Ouyang"
                }
            ]
        },
        {
            "paperId": "4dcefca05cd194746e4e5813636fc54ecffd186f",
            "title": "ChemVLM: Exploring the Power of Multimodal Large Language Models in Chemistry Area",
            "abstract": "Large Language Models (LLMs) have achieved remarkable success and have been applied across various scientific fields, including chemistry. However, many chemical tasks require the processing of visual information, which cannot be successfully handled by existing chemical LLMs. This brings a growing need for models capable of integrating multimodal information in the chemical domain. In this paper, we introduce \\textbf{ChemVLM}, an open-source chemical multimodal large language model specifically designed for chemical applications. ChemVLM is trained on a carefully curated bilingual multimodal dataset that enhances its ability to understand both textual and visual chemical information, including molecular structures, reactions, and chemistry examination questions. We develop three datasets for comprehensive evaluation, tailored to Chemical Optical Character Recognition (OCR), Multimodal Chemical Reasoning (MMCR), and Multimodal Molecule Understanding tasks. We benchmark ChemVLM against a range of open-source and proprietary multimodal large language models on various tasks. Experimental results demonstrate that ChemVLM achieves competitive performance across all evaluated tasks. Our model can be found at https://huggingface.co/AI4Chem/ChemVLM-26B.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109383573",
                    "name": "Junxian Li"
                },
                {
                    "authorId": "2283952275",
                    "name": "Di Zhang"
                },
                {
                    "authorId": "2316089170",
                    "name": "Xunzhi Wang"
                },
                {
                    "authorId": "2316114521",
                    "name": "Zeying Hao"
                },
                {
                    "authorId": "2316236029",
                    "name": "Jingdi Lei"
                },
                {
                    "authorId": "2283844222",
                    "name": "Qian Tan"
                },
                {
                    "authorId": "2316088348",
                    "name": "Cai Zhou"
                },
                {
                    "authorId": "2283881121",
                    "name": "Wei Liu"
                },
                {
                    "authorId": "2190474418",
                    "name": "Weiyun Wang"
                },
                {
                    "authorId": "2305731793",
                    "name": "Zhe Chen"
                },
                {
                    "authorId": "2257133501",
                    "name": "Wenhai Wang"
                },
                {
                    "authorId": "2316055701",
                    "name": "Wei Li"
                },
                {
                    "authorId": "2249003500",
                    "name": "Shufei Zhang"
                },
                {
                    "authorId": "2248285678",
                    "name": "Mao Su"
                },
                {
                    "authorId": "2283918605",
                    "name": "Wanli Ouyang"
                },
                {
                    "authorId": "2248091979",
                    "name": "Yuqiang Li"
                },
                {
                    "authorId": "2116324147",
                    "name": "Dongzhan Zhou"
                }
            ]
        },
        {
            "paperId": "6eb23df05166c772e4c2fbfb0113de0beabd1a43",
            "title": "Large Language Models are In-Context Molecule Learners",
            "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance in biochemical tasks, especially the molecule caption translation task, which aims to bridge the gap between molecules and natural language texts. However, previous methods in adapting LLMs to the molecule-caption translation task required extra domain-specific pre-training stages, suffered weak alignment between molecular and textual spaces, or imposed stringent demands on the scale of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation (ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment from context examples via In-Context Molecule Tuning. Specifically, ICMA incorporates the following three stages: Hybrid Context Retrieval, Post-retrieval Re-ranking, and In-context Molecule Tuning. Initially, Hybrid Context Retrieval utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve informative context examples. Additionally, we also propose Post-retrieval Re-ranking with Sequence Reversal and Random Walk to further improve the quality of retrieval results. Finally, In-Context Molecule Tuning unlocks the in-context molecule learning capability of LLMs with retrieved examples and adapts the parameters of LLMs for the molecule-caption translation task. Experimental results demonstrate that ICMT can empower LLMs to achieve state-of-the-art or comparable performance without extra training corpora and intricate structures, showing that LLMs are inherently in-context molecule learners.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2259858493",
                    "name": "Jiatong Li"
                },
                {
                    "authorId": "2283881121",
                    "name": "Wei Liu"
                },
                {
                    "authorId": "2290248755",
                    "name": "Zhihao Ding"
                },
                {
                    "authorId": "41031455",
                    "name": "Wenqi Fan"
                },
                {
                    "authorId": "2248091979",
                    "name": "Yuqiang Li"
                },
                {
                    "authorId": "2290317675",
                    "name": "Qing Li"
                }
            ]
        },
        {
            "paperId": "9f0bac228c616236c9fb8c25fbee817b1599a929",
            "title": "ChemLLM: A Chemical Large Language Model",
            "abstract": "Large language models (LLMs) have made impressive progress in chemistry applications. However, the community lacks an LLM specifically designed for chemistry. The main challenges are two-fold: firstly, most chemical data and scientific knowledge are stored in structured databases, which limits the model's ability to sustain coherent dialogue when used directly. Secondly, there is an absence of objective and fair benchmark that encompass most chemistry tasks. Here, we introduce ChemLLM, a comprehensive framework that features the first LLM dedicated to chemistry. It also includes ChemData, a dataset specifically designed for instruction tuning, and ChemBench, a robust benchmark covering nine essential chemistry tasks. ChemLLM is adept at performing various tasks across chemical disciplines with fluid dialogue interaction. Notably, ChemLLM achieves results comparable to GPT-4 on the core chemical tasks and demonstrates competitive performance with LLMs of similar size in general scenarios. ChemLLM paves a new path for exploration in chemical studies, and our method of incorporating structured chemical knowledge into dialogue systems sets a new standard for developing LLMs in various scientific fields. Codes, Datasets, and Model weights are publicly accessible at https://hf.co/AI4Chem",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283952275",
                    "name": "Di Zhang"
                },
                {
                    "authorId": "2283881121",
                    "name": "Wei Liu"
                },
                {
                    "authorId": "2283844222",
                    "name": "Qian Tan"
                },
                {
                    "authorId": "2284033855",
                    "name": "Jingdan Chen"
                },
                {
                    "authorId": "2283885582",
                    "name": "Hang Yan"
                },
                {
                    "authorId": "2283880395",
                    "name": "Yuliang Yan"
                },
                {
                    "authorId": "2198621140",
                    "name": "Jiatong Li"
                },
                {
                    "authorId": "8007867",
                    "name": "Weiran Huang"
                },
                {
                    "authorId": "2283845000",
                    "name": "Xiangyu Yue"
                },
                {
                    "authorId": "2116324147",
                    "name": "Dongzhan Zhou"
                },
                {
                    "authorId": "2249003500",
                    "name": "Shufei Zhang"
                },
                {
                    "authorId": "2248285678",
                    "name": "Mao Su"
                },
                {
                    "authorId": "2284252048",
                    "name": "Han-sen Zhong"
                },
                {
                    "authorId": "2248091979",
                    "name": "Yuqiang Li"
                },
                {
                    "authorId": "2283918605",
                    "name": "Wanli Ouyang"
                }
            ]
        },
        {
            "paperId": "d084517f14ee247883de0f4dd58bb923e418157d",
            "title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning",
            "abstract": "This paper presents an advanced mathematical problem-solving framework, LLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with iterative Self-Refine to optimize the reasoning path and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS (SR-MCTS) overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms by fostering a more efficient exploration of solution spaces. Pairwise Preference Reward Model~(PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to model pairwise preferences between solutions, utilizing an Enhanced Borda Count (EBC) method to synthesize these preferences into a global ranking score to find better answers. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior performance in terms of search efficiency and problem-solving capability compared to existing methods like ToT and rStar, particularly in complex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Di Zhang"
                },
                {
                    "authorId": "2302781884",
                    "name": "Jianbo Wu"
                },
                {
                    "authorId": "2316236029",
                    "name": "Jingdi Lei"
                },
                {
                    "authorId": "2324585125",
                    "name": "Tong Che"
                },
                {
                    "authorId": "2324680585",
                    "name": "Jiatong Li"
                },
                {
                    "authorId": "2324761594",
                    "name": "Tong Xie"
                },
                {
                    "authorId": "2275029776",
                    "name": "Xiaoshui Huang"
                },
                {
                    "authorId": "2249003500",
                    "name": "Shufei Zhang"
                },
                {
                    "authorId": "2324544972",
                    "name": "Marco Pavone"
                },
                {
                    "authorId": "2248091979",
                    "name": "Yuqiang Li"
                },
                {
                    "authorId": "2283918605",
                    "name": "Wanli Ouyang"
                },
                {
                    "authorId": null,
                    "name": "Dongzhan Zhou"
                }
            ]
        }
    ]
}