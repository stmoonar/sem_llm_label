{
    "authorId": "1731892",
    "papers": [
        {
            "paperId": "9b26f5b93c6cccaddbcd04362e9e85e19f4d7cb6",
            "title": "Neural axiom network for knowledge graph reasoning",
            "abstract": ". Knowledge graphs (KGs) generally suffer from incompleteness and incorrectness problems due to the automatic and semi-automatic construction process. Knowledge graph reasoning aims to infer new knowledge or detect noises, which is es-sential for improving the quality of knowledge graphs. In recent years, various KG reasoning techniques, such as symbolic- and embedding-based methods, have been proposed and shown strong reasoning ability. Symbolic-based reasoning methods infer missing triples according to prede\ufb01ned rules or ontologies. Although rules and axioms have proven to be effective, it is dif\ufb01cult to obtain them. While embedding-based reasoning methods represent entities and relations of a KG as vectors, and complete the KG via vector computation. However, they mainly rely on structural information, and ignore implicit axiom information that are not prede\ufb01ned in KGs but can be re\ufb02ected from data. That is, each correct triple is also a logically consistent triple, and satis\ufb01es all axioms. In this paper, we propose a novel NeuR al A xiom N etwork ( NeuRAN ) framework that combines explicit structural and implicit axiom information. It only uses existing triples in KGs without introducing additional ontologies. Speci\ufb01cally, the framework consists of a knowledge graph embedding module that preserves the semantics of triples, and \ufb01ve axiom modules that encode \ufb01ve kinds of implicit axioms using entities and relations in triples. These axioms correspond to \ufb01ve typical object property expression axioms de\ufb01ned in OWL2, including ObjectPropertyDomain, ObjectPropertyRange, DisjointObjectProperties, Irre\ufb02exiveObjectProperty and AsymmetricObjectProperty . The knowledge graph embedding module and axiom modules respectively compute the scores that the triple conforms to the semantics and the corresponding axioms. Evaluations on KG reasoning tasks show the ef\ufb01ciency of our method. Compared with knowledge graph embedding models and CKRL, our method achieves comparable performance on noise detection and triple classi\ufb01cation, and achieves signi\ufb01cant performance on link prediction. Compared with TransE and TransH, our method improves the link prediction performance on the Hit@1 metric by 22.4% and 21.2% on WN18RR-10% dataset respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2141562251",
                    "name": "Juan Li"
                },
                {
                    "authorId": null,
                    "name": "Wen Zhang"
                },
                {
                    "authorId": "2150931933",
                    "name": "Xiangnan Chen"
                },
                {
                    "authorId": "1731892",
                    "name": "Jiaoyan Chen"
                },
                {
                    "authorId": "49178307",
                    "name": "Huajun Chen"
                }
            ]
        },
        {
            "paperId": "34fcaf6e28c20805b1f452ec2d14234113a7a6e1",
            "title": "Trigger-Argument based Explanation for Event Detection",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2069570219",
                    "name": "Yong Guan"
                },
                {
                    "authorId": "1731892",
                    "name": "Jiaoyan Chen"
                },
                {
                    "authorId": "1863173",
                    "name": "F. L\u00e9cu\u00e9"
                },
                {
                    "authorId": "9416872",
                    "name": "Jeff Z. Pan"
                },
                {
                    "authorId": "2141562251",
                    "name": "Juan Li"
                },
                {
                    "authorId": "2223161920",
                    "name": "Ru Li"
                }
            ]
        },
        {
            "paperId": "4880d1d788bbb55bf5ee07269ab99ac94f4a5292",
            "title": "Language Model Analysis for Ontology Subsumption Inference",
            "abstract": "Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46968114",
                    "name": "Yuan He"
                },
                {
                    "authorId": "1731892",
                    "name": "Jiaoyan Chen"
                },
                {
                    "authorId": "1402158435",
                    "name": "Ernesto Jim\u00e9nez-Ruiz"
                },
                {
                    "authorId": "145153805",
                    "name": "Hang Dong"
                },
                {
                    "authorId": "145655431",
                    "name": "Ian Horrocks"
                }
            ]
        },
        {
            "paperId": "4fac4cc7c7eb9a52f2cd5efbef7c6fb81682a83c",
            "title": "Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment",
            "abstract": "As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the types of alignment KGs covering bilingual and monolingual, with standard (non-iterative) and iterative training paradigms to evaluate the model performance. Our research indicates that, in the face of modality incompleteness, models succumb to overfitting the modality noise, and exhibit performance oscillations or declines at high rates of missing modality. This proves that the inclusion of additional multi-modal data can sometimes adversely affect EA. To address these challenges, we introduce UMAEA , a robust multi-modal entity alignment approach designed to tackle uncertainly missing and ambiguous visual modalities. It consistently achieves SOTA performance across all 97 benchmark splits, significantly surpassing existing baselines with limited parameters and time consumption, while effectively alleviating the identified limitations of other models. Our code and benchmark data are available at https://github.com/zjukg/UMAEA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2283992213",
                    "name": "Zhuo Chen"
                },
                {
                    "authorId": "80232698",
                    "name": "Lingbing Guo"
                },
                {
                    "authorId": "2112787103",
                    "name": "Yin Fang"
                },
                {
                    "authorId": "2118158068",
                    "name": "Yichi Zhang"
                },
                {
                    "authorId": "1731892",
                    "name": "Jiaoyan Chen"
                },
                {
                    "authorId": "9416872",
                    "name": "Jeff Z. Pan"
                },
                {
                    "authorId": "98177814",
                    "name": "Y. Li"
                },
                {
                    "authorId": "14499025",
                    "name": "Hua-zeng Chen"
                },
                {
                    "authorId": "2155281129",
                    "name": "Wen Zhang"
                }
            ]
        },
        {
            "paperId": "538409811c50d505e1aed49c782227904ff76782",
            "title": "Revisit and Outstrip Entity Alignment: A Perspective of Generative Models",
            "abstract": "Recent embedding-based methods have achieved great successes in exploiting entity alignment from knowledge graph (KG) embeddings of multiple modalities. In this paper, we study embedding-based entity alignment (EEA) from a perspective of generative models. We show that EEA shares similarities with typical generative models and prove the effectiveness of the recently developed generative adversarial network (GAN)-based EEA methods theoretically. We then reveal that their incomplete objective limits the capacity on both entity alignment and entity synthesis (i.e., generating new entities). We mitigate this problem by introducing a generative EEA (GEEA) framework with the proposed mutual variational autoencoder (M-VAE) as the generative model. M-VAE enables entity conversion between KGs and generation of new entities from random noise vectors. We demonstrate the power of GEEA with theoretical analysis and empirical experiments on both entity alignment and entity synthesis tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "80232698",
                    "name": "Lingbing Guo"
                },
                {
                    "authorId": "2283992213",
                    "name": "Zhuo Chen"
                },
                {
                    "authorId": "1731892",
                    "name": "Jiaoyan Chen"
                },
                {
                    "authorId": "49178307",
                    "name": "Huajun Chen"
                }
            ]
        },
        {
            "paperId": "8a872c5efdfd5ab0f357f4cbf86d805269bdd33c",
            "title": "Dual Box Embeddings for the Description Logic EL++",
            "abstract": "OWL ontologies, whose formal semantics are rooted in Description Logic (DL), have been widely used for knowledge representation. Similar to Knowledge Graphs (KGs), ontologies are often incomplete, and maintaining and constructing them has proved challenging. While classical deductive reasoning algorithms use the precise formal semantics of an ontology to predict missing facts, recent years have witnessed growing interest in inductive reasoning techniques that can derive probable facts from an ontology. Similar to KGs, a promising approach is to learn ontology embeddings in a latent vector space, while additionally ensuring they adhere to the semantics of the underlying DL. While a variety of approaches have been proposed, current ontology embedding methods suffer from several shortcomings, especially that they all fail to faithfully model one-to-many, many-to-one, and many-to-many relations and role inclusion axioms. To address this problem and improve ontology completion performance, we propose a novel ontology embedding method named Box2EL for the DL EL++, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles), and models inter-concept relationships using a bumping mechanism. We theoretically prove the soundness of Box2EL and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets on the tasks of subsumption prediction, role assertion prediction, and approximating deductive reasoning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2324784846",
                    "name": "Mathias Jackermeier"
                },
                {
                    "authorId": "1731892",
                    "name": "Jiaoyan Chen"
                },
                {
                    "authorId": "145655431",
                    "name": "Ian Horrocks"
                }
            ]
        },
        {
            "paperId": "a2d765c0563aaf802cf3b70bb69fe6361bb9e316",
            "title": "DeepOnto: A Python Package for Ontology Engineering with Deep Learning",
            "abstract": "Integrating deep learning techniques, particularly language models (LMs), with knowledge representation techniques like ontologies has raised widespread attention, urging the need of a platform that supports both paradigms. Although packages such as OWL API and Jena offer robust support for basic ontology processing features, they lack the capability to transform various types of information within ontologies into formats suitable for downstream deep learning-based applications. Moreover, widely-used ontology APIs are primarily Java-based while deep learning frameworks like PyTorch and Tensorflow are mainly for Python programming. To address the needs, we present DeepOnto, a Python package designed for ontology engineering with deep learning. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more \u201cPythonic\u201d manner and extending its capabilities to incorporate other essential components including reasoning, verbalisation, normalisation, taxonomy, projection, and more. Building on this module, DeepOnto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning methods, primarily pre-trained LMs. In this paper, we also demonstrate the practical utility of DeepOnto through two use-cases: the Digital Health Coaching in Samsung Research UK and the Bio-ML track of the Ontology Alignment Evaluation Initiative (OAEI).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46968114",
                    "name": "Yuan He"
                },
                {
                    "authorId": "1731892",
                    "name": "Jiaoyan Chen"
                },
                {
                    "authorId": "145153805",
                    "name": "Hang Dong"
                },
                {
                    "authorId": "145655431",
                    "name": "Ian Horrocks"
                },
                {
                    "authorId": "2947326",
                    "name": "Carlo Allocca"
                },
                {
                    "authorId": "2415766",
                    "name": "Taehun Kim"
                },
                {
                    "authorId": "3174026",
                    "name": "B. Sapkota"
                }
            ]
        },
        {
            "paperId": "c2f6921c62198e46693a3e554e17f874a1f83ab5",
            "title": "Subsumption Prediction for E-Commerce Taxonomies",
            "abstract": ". Taxonomy plays a key role in e-commerce, categorising items and facilitating both search and inventory management. Concept sub-sumption prediction is critical for taxonomy curation, and has been the subject of several studies, but they do not fully utilise the categorical information available in e-commerce settings. In this paper, we study the characteristics of e-commerce taxonomies, and propose a new subsumption prediction method based on the pre-trained language model BERT that is well adapted to the e-commerce setting. The proposed model utilises textual and structural semantics in a taxonomy, as well as the rich and noisy instance (item) information. We show through extensive evaluation on two large-scale e-commerce taxonomies from eBay and AliOpenKG, that our method offers substantial improvement over strong baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218596749",
                    "name": "Jingchuan Shi"
                },
                {
                    "authorId": "1731892",
                    "name": "Jiaoyan Chen"
                },
                {
                    "authorId": "145153805",
                    "name": "Hang Dong"
                },
                {
                    "authorId": "46503836",
                    "name": "Ishita K. Khan"
                },
                {
                    "authorId": "2187869513",
                    "name": "Lizzie Liang"
                },
                {
                    "authorId": "2128803",
                    "name": "Qunzhi Zhou"
                },
                {
                    "authorId": "2187877689",
                    "name": "Zhe Wu"
                },
                {
                    "authorId": "145655431",
                    "name": "Ian Horrocks"
                }
            ]
        },
        {
            "paperId": "d25f8c388677d287d00ca67d44ef02da2b45f2d9",
            "title": "Large Language Models and Knowledge Graphs: Opportunities and Challenges",
            "abstract": "Large Language Models (LLMs) have taken Knowledge Representation -- and the world -- by storm. This inflection point marks a shift from explicit knowledge representation to a renewed focus on the hybrid representation of both explicit knowledge and parametric knowledge. In this position paper, we will discuss some of the common debate points within the community on LLMs (parametric knowledge) and Knowledge Graphs (explicit knowledge) and speculate on opportunities and visions that the renewed focus brings, as well as related research topics and challenges.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "9416872",
                    "name": "Jeff Z. Pan"
                },
                {
                    "authorId": "2066327465",
                    "name": "S. Razniewski"
                },
                {
                    "authorId": "3245041",
                    "name": "Jan-Christoph Kalo"
                },
                {
                    "authorId": "29001552",
                    "name": "Sneha Singhania"
                },
                {
                    "authorId": "1731892",
                    "name": "Jiaoyan Chen"
                },
                {
                    "authorId": "3081683",
                    "name": "S. Dietze"
                },
                {
                    "authorId": "2362078",
                    "name": "Hajira Jabeen"
                },
                {
                    "authorId": "2008000176",
                    "name": "Janna Omeliyanenko"
                },
                {
                    "authorId": "2155281129",
                    "name": "Wen Zhang"
                },
                {
                    "authorId": "2574504",
                    "name": "Matteo Lissandrini"
                },
                {
                    "authorId": "51119656",
                    "name": "Russa Biswas"
                },
                {
                    "authorId": "144608002",
                    "name": "Gerard de Melo"
                },
                {
                    "authorId": "1699192",
                    "name": "A. Bonifati"
                },
                {
                    "authorId": "2007891598",
                    "name": "Edlira Vakaj"
                },
                {
                    "authorId": "2346796",
                    "name": "M. Dragoni"
                },
                {
                    "authorId": "2235966",
                    "name": "D. Graux"
                }
            ]
        },
        {
            "paperId": "d400ab22e100dd4d24672ad69b4f0546439fb000",
            "title": "Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement",
            "abstract": "Mentions of new concepts appear regularly in texts and require automated approaches to harvest and place them into Knowledge Bases (KB), e.g., ontologies and taxonomies. Existing datasets suffer from three issues, (i) mostly assuming that a new concept is pre-discovered and cannot support out-of-KB mention discovery; (ii) only using the concept label as the input along with the KB and thus lacking the contexts of a concept label; and (iii) mostly focusing on concept placement w.r.t a taxonomy of atomic concepts, instead of complex concepts, i.e., with logical operators. To address these issues, we propose a new benchmark, adapting MedMentions dataset (PubMed abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases sub-category and the broader categories of Clinical finding, Procedure, and Pharmaceutical / biologic product. We provide usage on the evaluation with the dataset for out-of-KB mention discovery and concept placement, adapting recent Large Language Model based methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145153805",
                    "name": "Hang Dong"
                },
                {
                    "authorId": "1731892",
                    "name": "Jiaoyan Chen"
                },
                {
                    "authorId": "46968114",
                    "name": "Yuan He"
                },
                {
                    "authorId": "145655431",
                    "name": "Ian Horrocks"
                }
            ]
        }
    ]
}