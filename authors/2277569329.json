{
    "authorId": "2277569329",
    "papers": [
        {
            "paperId": "40809ba9b1c603897a20e421cacf42eaacdff6f8",
            "title": "An Initial Investigation of Language Adaptation for TTS Systems under Low-resource Scenarios",
            "abstract": "Self-supervised learning (SSL) representations from massively multilingual models offer a promising solution for low-resource language speech tasks. Despite advancements, language adaptation in TTS systems remains an open problem. This paper explores the language adaptation capability of ZMM-TTS, a recent SSL-based multilingual TTS system proposed in our previous work. We conducted experiments on 12 languages using limited data with various fine-tuning configurations. We demonstrate that the similarity in phonetics between the pre-training and target languages, as well as the language category, affects the target language's adaptation performance. Additionally, we find that the fine-tuning dataset size and number of speakers influence adaptability. Surprisingly, we also observed that using paired data for fine-tuning is not always optimal compared to audio-only data. Beyond speech intelligibility, our analysis covers speaker similarity, language identification, and predicted MOS.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2057185712",
                    "name": "Cheng Gong"
                },
                {
                    "authorId": "2239099011",
                    "name": "Erica Cooper"
                },
                {
                    "authorId": "2306431082",
                    "name": "Xin Wang"
                },
                {
                    "authorId": "2007407453",
                    "name": "Chunyu Qiang"
                },
                {
                    "authorId": "2306061299",
                    "name": "Mengzhe Geng"
                },
                {
                    "authorId": "152337381",
                    "name": "Dan Wells"
                },
                {
                    "authorId": "2277569329",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                },
                {
                    "authorId": "2180670416",
                    "name": "Marc Tessier"
                },
                {
                    "authorId": "51183422",
                    "name": "Aidan Pine"
                },
                {
                    "authorId": "144181051",
                    "name": "Korin Richmond"
                },
                {
                    "authorId": "2253664522",
                    "name": "Junichi Yamagishi"
                }
            ]
        },
        {
            "paperId": "9ad6c4aeb2ce9bc72ed3c839deeb65d2879cafd7",
            "title": "AIMDiT: Modality Augmentation and Interaction via Multimodal Dimension Transformation for Emotion Recognition in Conversations",
            "abstract": "Emotion Recognition in Conversations (ERC) is a popular task in natural language processing, which aims to recognize the emotional state of the speaker in conversations. While current research primarily emphasizes contextual modeling, there exists a dearth of investigation into effective multimodal fusion methods. We propose a novel framework called AIMDiT to solve the problem of multimodal fusion of deep features. Specifically, we design a Modality Augmentation Network which performs rich representation learning through dimension transformation of different modalities and parameter-efficient inception block. On the other hand, the Modality Interaction Network performs interaction fusion of extracted inter-modal features and intra-modal features. Experiments conducted using our AIMDiT framework on the public benchmark dataset MELD reveal 2.34% and 2.87% improvements in terms of the Acc-7 and w-F1 metrics compared to the state-of-the-art (SOTA) models.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2309205750",
                    "name": "Sheng Wu"
                },
                {
                    "authorId": "2108354640",
                    "name": "Jiaxing Liu"
                },
                {
                    "authorId": "2277569329",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                }
            ]
        },
        {
            "paperId": "9dcd42423d6dcfe5f3756bd3328680edd1871f9e",
            "title": "Text-to-Speech for Low-Resource Agglutinative Language With Morphology-Aware Language Model Pre-Training",
            "abstract": "Text-to-Speech (TTS) aims to convert the input text to a human-like voice. With the development of deep learning, encoder-decoder based TTS models perform superior performance, in terms of naturalness, in mainstream languages such as Chinese, English, etc. Note that the linguistic information learning capability of the text encoder is the key. However, for TTS of low-resource agglutinative languages, the scale of the $< $text, speech$>$ paired data is limited. Therefore, how to extract rich linguistic information from small-scale text data to enhance the naturalness of the synthesized speech, is an urgent issue that needs to be addressed. In this paper, we first collect a large unsupervised text data for BERT-like language model pre-training, and then adopt the trained language model to extract deep linguistic information for the input text of the TTS model to improve the naturalness of the final synthesized speech. It should be emphasized that in order to fully exploit the prosody-related linguistic information in agglutinative languages, we incorporated morphological information into the language model training and constructed a morphology-aware masking based BERT model (MAM-BERT). Experimental results based on various advanced TTS models validate the effectiveness of our approach. Further comparison of the various data scales also validates the effectiveness of our approach in low-resource scenarios.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268706114",
                    "name": "Rui Liu"
                },
                {
                    "authorId": "2275569490",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "2188993148",
                    "name": "Haolin Zuo"
                },
                {
                    "authorId": "2274096736",
                    "name": "Zhaojie Luo"
                },
                {
                    "authorId": "2277569329",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2268862108",
                    "name": "Guanglai Gao"
                }
            ]
        },
        {
            "paperId": "efd754649f9283fad98aea8c9ce375fd5dd64db0",
            "title": "A Prompt-Based Hierarchical Pipeline for Cross-Domain Slot Filling",
            "abstract": "In task-oriented dialogue systems, slot filling aims to identify the semantic slot types of each token in user utterances. Due to the lack of sufficient supervised data in many scenarios, it is necessary to transfer relevant knowledge by using cross-domain slot filling. Previous studies rely on manually additional meta-information to build the relationships among similar slots across domains, yet not fully utilizing the knowledge learned by language models in the pre-training stage. In this study, we propose a prompt-based hierarchical pipeline (PHP) with three innovations. First, we design a hierarchical pipeline to separately model domain-independent syntactic structures and domain-specific semantic structures, i.e., span detection and slot prediction. Second, we improve the prompt paradigm with discriminative structure to fully utilize pre-trained language models, which reformulates downstream tasks into pre-trained tasks. Finally, we polish our template and verbalizer to effectively utilize task-specific prior knowledge, adding some meta-information and updating their additional trainable parameters. We conducted extensive experiments on three datasets to evaluate our method, and experimental results show that our method significantly outperforms the previous state-of-the-art results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185358961",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2226454143",
                    "name": "Yuhang Li"
                },
                {
                    "authorId": "10785308",
                    "name": "Yuke Si"
                },
                {
                    "authorId": "2277569329",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                }
            ]
        }
    ]
}