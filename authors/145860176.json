{
    "authorId": "145860176",
    "papers": [
        {
            "paperId": "0cbf0a68828bec0855e04b5fe70089511255a4fb",
            "title": "A Variational Neural Architecture for Skill-based Team Formation",
            "abstract": "Team formation is concerned with the identification of a group of experts who have a high likelihood of effectively collaborating with each other to satisfy a collection of input skills. Solutions to this task have mainly adopted graph operations and at least have the following limitations: (1) they are computationally demanding, as they require finding shortest paths on large collaboration networks; (2) they use various types of heuristics to reduce the exploration space over the collaboration network to become practically feasible; therefore, their results are not necessarily optimal; and (3) they are not well-suited for collaboration network structures given the sparsity of these networks. Our work proposes a variational Bayesian neural network architecture that learns representations for teams whose members have collaborated with each other in the past. The learned representations allow our proposed approach to mine teams that have a past collaborative history and collectively cover the requested desirable set of skills. Through our experiments, we demonstrate that our approach shows stronger performance compared to a range of strong team formation techniques from both quantitative and qualitative perspectives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52465784",
                    "name": "Radin Hamidi Rad"
                },
                {
                    "authorId": "144572716",
                    "name": "Hossein Fani"
                },
                {
                    "authorId": "145632843",
                    "name": "E. Bagheri"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                }
            ]
        },
        {
            "paperId": "17c4d654e921467d9b147f8dde72fe7e9baa38d4",
            "title": "Global and Local Differentially Private Release of Count-Weighted Graphs",
            "abstract": "Many complex natural and technological systems are commonly modeled as count-weighted graphs, where nodes represent entities, edges model relationships between them, and edge weights define some counting statistics associated with each relationship. As graph data usually contain sensitive information about entities, preserving privacy when releasing this type of data becomes an important issue. In this context, differential privacy (DP) has become the de facto standard for data release under strong privacy guarantees. When dealing with DP for weighted graphs, most state-of-the-art works assume that the graph topology is known. However, in several real-world applications, the privacy of the graph topology also needs to be ensured. In this paper, we aim to bridge the gap between DP and count-weighted graph data release, considering both graph structure and edge weights as private information. We first adapt the weighted graph DP definition to take into account the privacy of the graph structure. We then develop two novel approaches to privately releasing count-weighted graphs under the notions of global and local DP. We also leverage the post-processing property of DP to improve the accuracy of the proposed techniques considering graph domain constraints. Experiments using real-world graph data demonstrate the superiority of our approaches in terms of utility over existing techniques, enabling subsequent computation of a variety of statistics on the released graph with high utility, in some cases comparable to the non-private results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2164948462",
                    "name": "Felipe T. Brito"
                },
                {
                    "authorId": "144542281",
                    "name": "V. A. E. Farias"
                },
                {
                    "authorId": "24440615",
                    "name": "Cheryl J. Flynn"
                },
                {
                    "authorId": "152632230",
                    "name": "S. Majumdar"
                },
                {
                    "authorId": "40342539",
                    "name": "Javam C. Machado"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "245a1f574c253159689e4205e0b2eb37023057e5",
            "title": "Matching Roles from Temporal Data: Why Joe Biden is not only President, but also Commander-in-Chief",
            "abstract": "We present role matching, a novel, fine-grained integrity constraint on temporal fact data, i.e., (subject, predicate, object, timestamp)-quadruples. A role is a combination of subject and predicate and can be associated with different objects as the real world evolves and the data changes over time. A role matching states that the associated object of two or more roles should always match across time. Once discovered, role matchings can serve as integrity constraints to improve data quality, for instance of structured data in Wikipedia[3]. If violated, role matchings can alert data owners or editors and thus allow them to correct the error. Finding all role matchings is challenging due both to the inherent quadratic complexity of the matching problem and the need to identify true matches based on the possibly short history of the facts observed so far. To address the first challenge, we introduce several blocking methods both for clean and dirty input data. For the second challenge, the matching stage, we show how the entity resolution method Ditto[27] can be adapted to achieve satisfactory performance for the role matching task. We evaluate our method on datasets from Wikipedia infoboxes, showing that our blocking approaches can achieve 95% recall, while maintaining a reduction ratio of more than 99.99%, even in the presence of dirty data. In the matching stage, we achieve a macro F1-score of 89% on our datasets, using automatically generated labels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49625591",
                    "name": "Leon Bornemann"
                },
                {
                    "authorId": "7870886",
                    "name": "Tobias Bleifu\u00df"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "2315516",
                    "name": "F. Nargesian"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "60a156dea677763eefc104a91c220e1808fbd536",
            "title": "iORDER: Mining Implicit Domain Orders",
            "abstract": "In this demonstration paper, we describe iORDER, a tool that identifies implicit domain orders in data, such as Small Medium Large. iORDER extends the machinery of order dependency discovery to identify and rank interesting orders. Using real-world data, we showcase how implicit orders help users interpret the semantics of ordered data, how to interactively validate implicit orders to aid in the discovery process, and how to apply implicit orders to applications including data profiling, data mining and knowledge bases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2224954550",
                    "name": "Alexander Bianchi"
                },
                {
                    "authorId": "74567727",
                    "name": "Reza Karegar"
                },
                {
                    "authorId": "144024329",
                    "name": "P. Godfrey"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                }
            ]
        },
        {
            "paperId": "67356c83193af1868b1e83c5a9ca3281fb393fbd",
            "title": "POEM: Pattern-Oriented Explanations of Convolutional Neural Networks",
            "abstract": "Convolutional Neural Networks (CNNs) are commonly used in computer vision. However, their predictions are difficult to explain, as is the case with many deep learning models. To address this problem, we present POEM, a modular framework that produces patterns of semantic concepts such as shapes and colours to explain image classifier CNNs. POEM identifies patterns such as \"if sofa then living room\", meaning that if an image contains a sofa and the model pays attention to the sofa, then the model classifies the image as a living room. We illustrate the advantages of POEM over existing work using quantitative and qualitative experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150401581",
                    "name": "Vargha Dadvar"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "6869440837949a2abc6f5a1a02a8cf86abbaf36f",
            "title": "Data Quality in Data Streams by Modular Change Point Detection",
            "abstract": "Sensors that collect data from complex systems generate a stream of measurements, for example, measuring CPU utilization of machines in a data center, gathering meteorological data like atmospheric pressure and humidity levels across the USA, or tracking the occupancy of taxis in a large city. Downstream systems use the streamed data in a variety of applications, including training machine learning models and making data-driven decisions as part of automation. This makes data quality critical and requires detecting significant, unexpected, and rapid changes in indicative features of the streaming data. This can be done by detecting change points in the stream \u2013 points where the underlying distribution of a statistical feature of the stream fundamentally changes. In this paper, we discuss different types of change points in the data stream \u2013 changes that indicate a potential data quality problem. We present a modular method for combining operations on data streams to examine data quality in a flexible and adaptable way. Experiments over real-world and synthetic data streams show the effectiveness of the modular approach in comparison to traditional anomaly detection methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1742591",
                    "name": "Y. Kanza"
                },
                {
                    "authorId": "33131414",
                    "name": "Rajat Malik"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2233484766",
                    "name": "Caroline Stone"
                },
                {
                    "authorId": "2990655",
                    "name": "Gordon Woodhull"
                }
            ]
        },
        {
            "paperId": "a53949e5004be279559d003c45e82f4c52f081cf",
            "title": "Through the Fairness Lens: Experimental Analysis and Evaluation of Entity Matching",
            "abstract": "Entity matching (EM) is a challenging problem studied by different communities for over half a century. Algorithmic fairness has also become a timely topic to address machine bias and its societal impacts. Despite extensive research on these two topics, little attention has been paid to the fairness of entity matching.\n Towards addressing this gap, we perform an extensive experimental evaluation of a variety of EM techniques in this paper. We generated two social datasets from publicly available datasets for the purpose of auditing EM through the lens of fairness. Our findings underscore potential unfairness under two common conditions in real-world societies: (i) when some demographic groups are over-represented, and (ii) when names are more similar in some groups compared to others. Among our many findings, it is noteworthy to mention that while various fairness definitions are valuable for different settings, due to EM's class imbalance nature, measures such as positive predictive value parity and true positive rate parity are, in general, more capable of revealing EM unfairness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1927032",
                    "name": "N. Shahbazi"
                },
                {
                    "authorId": "2221226599",
                    "name": "Nikola Danevski"
                },
                {
                    "authorId": "2315516",
                    "name": "F. Nargesian"
                },
                {
                    "authorId": "1717283",
                    "name": "Abolfazl Asudeh"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "b304b7f9f30b9321881bbb6d2fd1bc16bc9eaa50",
            "title": "CREDENCE: Counterfactual Explanations for Document Ranking",
            "abstract": "Towards better explainability in the field of information retrieval, we present CREDENCE, an interactive tool capable of generating counterfactual explanations for document rankers. Embracing the unique properties of the ranking problem, we present counterfactual explanations in terms of document perturbations, query perturbations, and even other documents. Additionally, users may build and test their own perturbations, and extract insights about their query, documents, and ranker.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2205302657",
                    "name": "Joel Rorseth"
                },
                {
                    "authorId": "144024329",
                    "name": "P. Godfrey"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                }
            ]
        },
        {
            "paperId": "cbeb2b1b3a93046a5494cbba75ff5fab96a1e566",
            "title": "EAGER: Explainable Question Answering Using Knowledge Graphs",
            "abstract": "We present EAGER: a tool for answering questions expressed in natural language. Core to EAGER is a modular pipeline for generating a knowledge graph from raw text without human intervention. Notably, EAGER uses the knowledge graph to answer questions and to explain the reasoning behind the derivation of answers. Our demonstration will showcase both the automated knowledge graph generation pipeline and the explainable question answering functionality. Lastly, we outline open problems and directions for future work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2220350485",
                    "name": "Andrew Chai"
                },
                {
                    "authorId": "2160926321",
                    "name": "Alireza Vezvaei"
                },
                {
                    "authorId": "2285679188",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                },
                {
                    "authorId": "22690431",
                    "name": "Morteza Zihayat"
                }
            ]
        },
        {
            "paperId": "e8f00227b9a50929b1534ce4adf2cb7023b4b581",
            "title": "Detecting Stale Data in Wikipedia Infoboxes",
            "abstract": "Today\u2019s fast-paced society is increasingly reliant on correct and up-to-date data. Wikipedia is the world\u2019s most popular source of knowledge, and its infoboxes contain concise semi-structured data with important facts about a page\u2019s topic. However, these data are not always up-to-date: we do not expect Wikipedia editors to update items at the moment their true values change. Also, many pages might not be well maintained and users might forget to update the data, e.g., when they are on holiday. To detect stale data in Wikipedia infoboxes, we combine correlation-based and rule-based approaches trained on different temporal granularities, based on all infobox changes over 15 years of English Wikipedia. We are able to predict 8 . 19% of all changes with a precision of 89 . 69% over a whole year, thus meeting our target precision of",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2214588524",
                    "name": "Malte Barth"
                },
                {
                    "authorId": "2114705333",
                    "name": "Tibor Bleidt"
                },
                {
                    "authorId": "2114710496",
                    "name": "Martin B\u00fc\u00dfemeyer"
                },
                {
                    "authorId": "2114863886",
                    "name": "Fabian Heseding"
                },
                {
                    "authorId": "2148228603",
                    "name": "Niklas K\u00f6hnecke"
                },
                {
                    "authorId": "7870886",
                    "name": "Tobias Bleifu\u00df"
                },
                {
                    "authorId": "49625591",
                    "name": "Leon Bornemann"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "0217aa3acc27d07a2ea2aa53618459943a8d97e2",
            "title": "Effective Explanations for Entity Resolution Models",
            "abstract": "Entity resolution (ER) aims at matching records that refer to the same real-world entity. Although widely studied for the last 50 years, ER still represents a challenging data management problem, and several recent works have started to investigate the opportunity of applying deep learning (DL) techniques to solve this problem. In this paper, we study the fundamental problem of explainability of the DL solution for ER. Understanding the matching predictions of an ER solution is indeed crucial to assess the trustworthiness of the DL model and to discover its biases. We treat the DL model as a black box classifier and - while previous approaches to provide explanations for DL predictions are agnostic to the classification task - we propose the CERTA approach that is aware of the semantics of the ER problem. Our approach produces both saliency explanations, which associate each attribute with a saliency score, and counterfactual explanations, which provide examples of values that can flip the prediction. CERTA builds on a probabilistic framework that aims at computing the explanations evaluating the outcomes produced by using perturbed copies of the input records. We experimentally evaluate CERTA'S explanations of state-of-the-art ER solutions based on DL models using publicly available datasets, and demonstrate the effectiveness of CERTA over recently proposed methods for this problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "16973085",
                    "name": "Tommaso Teofili"
                },
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "2159711950",
                    "name": "Vincenzo Martello"
                },
                {
                    "authorId": "1796590",
                    "name": "P. Merialdo"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "2a38394fd703c0921212b371335fd218d8233527",
            "title": "Hierarchical Entity Resolution using an Oracle",
            "abstract": "In many applications, entity references (i.e., records) and entities need to be organized to capture diverse relationships like type-subtype, is-A (mapping entities to types), and duplicate (mapping records to entities) relationships. However, automatic identification of such relationships is often inaccurate due to noise and heterogeneous representation of records across sources. Similarly, manual maintenance of these relationships is infeasible and does not scale to large datasets. In this work, we circumvent these challenges by considering weak supervision in the form of an oracle to formulate a novel hierarchical ER task. In this setting, records are clustered in a tree-like structure containing records at leaf-level and capturing record-entity (duplicate), entity-type (is-A) and subtype-supertype relationships. For effective use of supervision, we leverage triplet comparison oracle queries that take three records as input and output the most similar pair(s). We develop HierER, a querying strategy that uses record pair similarities to minimize the number of oracle queries while maximizing the identified hierarchical structure. We show theoretically and empirically that HierER is effective under different similarity noise models and demonstrate empirically that HierER can scale up to million-size datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2663974",
                    "name": "Sainyam Galhotra"
                },
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "3269130",
                    "name": "B. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "2e7d854c93876c64b88147b0b698778d63e4b121",
            "title": "abcOD: Mining Band Order Dependencies",
            "abstract": "We present the design of and a demonstration plan for abcOD, a tool for efficiently discovering approximate band conditional order dependencies (abcODs) from data. abcOD utilizes a dynamic programming algorithm based on a longest monotonic band. Using real datasets, we demonstrate how the discovered abcODs can help users understand ordered data semantics, identify potential data quality problems, and interactively clean the data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112727336",
                    "name": "Pei Li"
                },
                {
                    "authorId": "145959440",
                    "name": "J. Jessica"
                },
                {
                    "authorId": "2180225885",
                    "name": "Naida Tania"
                },
                {
                    "authorId": "2272920708",
                    "name": "Michael H. B\u00f6hlen"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                }
            ]
        },
        {
            "paperId": "478a1cae14e62f57a47ce8424c1b664d69f6aa27",
            "title": "DataPrism: Exposing Disconnect between Data and Systems",
            "abstract": "As data is a central component of many modern systems, the cause of a system malfunction may reside in the data, and, specifically, particular properties of data. E.g., a health-monitoring system that is designed under the assumption that weight is reported in lbs will malfunction when encountering weight reported in kilograms. Like software debugging, which aims to find bugs in the source code or runtime conditions, our goal is to debug data to identify potential sources of disconnect between the assumptions about some data and systems that operate on that data. We propose DataPrism, a framework to identify data properties (profiles) that are the root causes of performance degradation or failure of a data-driven system. Such identification is necessary to repair data and resolve the disconnect between data and systems. Our technique is based on causal reasoning through interventions: when a system malfunctions for a dataset, DataPrism alters the data profiles and observes changes in the system's behavior due to the alteration. Unlike statistical observational analysis that reports mere correlations, DataPrism reports causally verified root causes -- in terms of data profiles -- of the system malfunction. We empirically evaluate DataPrism on seven real-world and several synthetic data-driven systems that fail on certain datasets due to a diverse set of reasons. In all cases, DataPrism identifies the root causes precisely while requiring orders of magnitude fewer interventions than prior techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2663974",
                    "name": "Sainyam Galhotra"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                },
                {
                    "authorId": "2283085",
                    "name": "A. Meliou"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "64696edae8dea73d698011d09bed9465b0bfbdd5",
            "title": "POEM: Pattern-Oriented Explanations of CNN Models",
            "abstract": "Deep learning models achieve state-of-the-art performance in many applications, but their prediction decisions are difficult to explain. Various solutions exist in the area of explainable AI, for example to understand individual predictions or to approximate complex models using simpler interpretable ones. We contribute to this body of work with POEM: a tool that produces pattern-oriented explanations of image classification models. POEM explains models that learn hierarchies of concepts, such as Convolutional Neural Networks that detect shapes and objects in images. For example, POEM may identify a pattern of the form \"if bed then bedroom\", indicating that if an image contains a bed and the model pays attention to this region of the image during inference, then the model classifies the image as a bedroom. We present the modular design of POEM, followed by examples of POEM's use in model auditing and detecting errors in training data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150401581",
                    "name": "Vargha Dadvar"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "797bf49b2e626cbe08d0aa1a7f9c6b3eb089e767",
            "title": "Subgraph Representation Learning for Team Mining",
            "abstract": "Team mining is concerned with the identification of a group of experts that are able to collaborate with each other in order to collectively cover a set of required skills. This problem has mainly been addressed either through graph search, which looks for subgraphs that satisfy the skill requirements or through neural architectures that learn a mapping from the skill space to the expert space. An exact graph-based solution to this problem is intractable and its heuristic variants are only able to identify sub-optimal solutions. On the other hand, neural architecture-based solutions are prone to overfitting and simplistically reduce the problem of team formation to one of expert ranking. Our work in this paper proposes an unsupervised heterogeneous skip-gram-based subgraph mining approach that can learn representations for subgraphs in a collaboration network. Unlike previous work, the subgraph representations allow our method to mine teams that have past collaborative history and collectively cover the requested desirable skills. Through our experiments, we demonstrate that our proposed approach is able to outperform a host of state-of-the-art team mining techniques from both quantitative and qualitative perspectives.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52465784",
                    "name": "Radin Hamidi Rad"
                },
                {
                    "authorId": "145632843",
                    "name": "E. Bagheri"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                }
            ]
        },
        {
            "paperId": "7c3af255d08c33581ebe4bb267212ca25fe51565",
            "title": "Publication Culture and Review Processes in the Data Management Community: An Open Discussion",
            "abstract": "The Data Management community has explored many options in recent years to improve our publication culture and review processes, ranging from innovative journal-conference hybrids that decouple publication from presentation, incorporating journal-style reviewing for conference-style papers, requesting code reproducibility and code/data availability, multiple submission deadlines in a year, new categories of papers, informal shepherding processes, guidelines for diversity and inclusion, automated COI check, and so on. This panel seeks to examine our many experiments, comparing them with other CS disciplines, and help determine (i) have our experiments worked? (ii) what has their impact been? and (iii) can we do better?",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1730344",
                    "name": "S. Bhowmick"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2203901",
                    "name": "Stratos Idreos"
                },
                {
                    "authorId": "7337091",
                    "name": "Wolfgang Lehner"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "7eb27fdca7fe2a96590bc048fd9bb45372f64421",
            "title": "Exploring and Analyzing Change: The Janus Project",
            "abstract": "Data change, all the time. The Janus project seeks to address the Variability dimension of Big Data by modeling, exploring, and analyzing such change, providing valuable insights into the evolving real world and ways in which data about it are collected and used. We start by identifying technical challenges that need to be addressed to realize the Janus vision. Towards this end, we have extracted and worked with the histories of various structured datasets, including DBLP, IMDB, open government data, and Wikipedia, for which a detailed history of every edit is available. Our DBChEx (Database Change Explorer) prototype enables interactive exploration of data and schema changes, and we show how DBChEx can help users gain valuable insights by exploring two real-world datasets, IMDB and Wikipedia infoboxes. Based on an analysis of the history of 3.5M tables on the English Wikipedia for a total of 53.8M table versions, we then illustrate the rich history of structured Wikipedia data: we show that tables are created in certain locations, they change their shape, they move, they grow, they shrink, their data change, they vanish, and they re-appear; indeed, each table has a life of its own. Finally, to help automatically interpret the useful knowledge harbored in the history of Wikipedia tables, we present recent results on two technical problems: (i) identifying Natural Keys, a particularly important piece of metadata, which serves as a primary key in tables over time and consists of attributes inherent to an entity, and (ii) matching tables, infoboxes and lists within a Wikipedia page across page revisions. We solve these problems at scale and make the resulting curated datasets available to the community to facilitate future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "7870886",
                    "name": "Tobias Bleifu\u00df"
                },
                {
                    "authorId": "49625591",
                    "name": "Leon Bornemann"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                }
            ]
        },
        {
            "paperId": "87996bd4717245b817410ec9af25ac2bb63e854d",
            "title": "Geospatial accessibility and inclusion by combining contextual filters, the metaverse and ambient intelligence",
            "abstract": "Accessibility and inclusion in information systems provide guidelines and methods for building applications and devices that work for everyone, including for people with disabilities and other diverse characteristics. Systems that apply accessible and inclusive design (AID) provide an easy way to adapt features for people with different capabilities. In geospatial applications and wearable technology, aspects of location and time should be taken into account as well. In this paper we present a visionary approach of applying contextual filters to the input, output and the underlying dataset of geospatial applications, to examine adaptation of systems, devices and applications to different populations and for verification of accessibility and inclusion. The goal of AID is to help developers add accessibility features to their systems and provide transparency regarding lack of accessibility when needed, e.g., make it clear if there are features that prevent usage by people who lack certain abilities. We explain how to apply AID in physical and virtual worlds, like the metaverse, and we describe how ambient intelligence, accessibility maps and virtual worlds can assist in building accessible and inclusive geospatial applications. We present our vision and elaborate on related challenges and research directions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1742591",
                    "name": "Y. Kanza"
                },
                {
                    "authorId": "145846952",
                    "name": "B. Krishnamurthy"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "8be76110f65bd7ffdcb1c775efa6230da65b0fe4",
            "title": "CERTEM: Explaining and Debugging Black-box Entity Resolution Systems with CERTA",
            "abstract": "Entity resolution (ER) aims at identifying record pairs that refer to the same real-world entity. Recent works have focused on deep learning (DL) techniques, to solve this problem. While such works have brought tremendous enhancements in terms of effectiveness in solving the ER problem, understanding their matching predictions is still a challenge, because of the intrinsic opaqueness of DL based solutions. Interpreting and trusting the predictions made by ER systems is crucial for humans in order to employ such methods in decision making pipelines. We demonstrate CERTEM an explanation system for ER based on CERTA, a recently introduced explainability framework for ER, that is able to provide both saliency explanations, which associate each attribute with a saliency score, and counterfactual explanations, which provide examples of values that can flip a prediction. In this demonstration we will showcase how CERTEM can be effectively employed to better understand and debug the behavior of state-of-the-art DL based ER systems on data from publicly available ER benchmarks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "16973085",
                    "name": "Tommaso Teofili"
                },
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "1796590",
                    "name": "P. Merialdo"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "981906b0087f77198319110cccc51e582e6a5e4e",
            "title": "Towards Algorithmic Fairness in Space-Time: Filling in Black Holes",
            "abstract": "New technologies and the availability of geospatial data have drawn attention to spatio-temporal biases present in society. For example: the COVID-19 pandemic highlighted disparities in the availability of broadband service and its role in the digital divide; the environmental justice movement in the United States has raised awareness to health implications for minority populations stemming from historical redlining practices; and studies have found varying quality and coverage in the collection and sharing of open-source geospatial data. Despite the extensive literature on machine learning (ML) fairness, few algorithmic strategies have been proposed to mitigate such biases. In this paper we highlight the unique challenges for quantifying and addressing spatio-temporal biases, through the lens of use cases presented in the scientific literature and media. We envision a roadmap of ML strategies that need to be developed or adapted to quantify and overcome these challenges -- including transfer learning, active learning, and reinforcement learning techniques. Further, we discuss the potential role of ML in providing guidance to policy makers on issues related to spatial fairness.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "24440615",
                    "name": "Cheryl J. Flynn"
                },
                {
                    "authorId": "29829392",
                    "name": "Aritra Guha"
                },
                {
                    "authorId": "152632230",
                    "name": "S. Majumdar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "122347188",
                    "name": "Zhen-Xiong Zhou"
                }
            ]
        },
        {
            "paperId": "d5cf1b4520dac3ed3aaea8da453f191faf49cd77",
            "title": "LedgerView: Access-Control Views on Hyperledger Fabric",
            "abstract": "We present LedgerView -- a system that adds access control views to permissioned blockchains. The approach is motivated by an AT&T application of tracking refurbished devices. A blockchain is a decentralized tamper-resistant ledger managed by a group of peers. It is used in many applications for storing and sharing sensitive information, e.g., monetary transactions, health records, personal documents, etc. But in blockchain, all the peers see all the stored transactions, while in some applications, access to sensitive information should be limited, that is, concealed from peers and users who do not have proper access permissions. In database management systems, sets of records that are visible to some users and concealed from others are defined by views, but existing blockchain systems lack such access-control capabilities. Thus, in this paper, we introduce access-control views for Hyperledger Fabric. We present two types of views -- irrevocable and revocable, according to whether access to sensitive information can or cannot be revoked. We explain how to implement the two types of view by using cryptographic hash functions and encryption keys, and we show how to support Role-Based Access Control (RBAC). Experiments with supply chain transactions illustrate the incurred costs of the views in LedgerView, including latency, transaction rate and storage overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34531659",
                    "name": "Pingcheng Ruan"
                },
                {
                    "authorId": "1742591",
                    "name": "Y. Kanza"
                },
                {
                    "authorId": "1693070",
                    "name": "B. Ooi"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "dbd442bff07d190d6967249121437e16d0259980",
            "title": "Discovering Domain Orders via Order Dependencies",
            "abstract": "Most real-world data come with explicitly defined domain orders; e.g., lexicographic for strings, numeric for integers, and chronological for time. Our goal is to discover implicit domain orders that we do not already know; for instance, that the order of months in the Chinese Lunar calendar is Corner $<$ Apricot $<$ Peach. To do so, we enhance data profiling methods by discovering implicit domain orders in data through order dependencies. We enumerate tractable special cases and show that the general case is NP-complete but can be effectively handled by a SAT solver. We also devise an interestingness measure to rank the discovered implicit domain orders. Based on an extensive suite of experiments with real-world data, we establish the efficacy of our algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "74567727",
                    "name": "Reza Karegar"
                },
                {
                    "authorId": "79434948",
                    "name": "Melicaalsadat Mirsafian"
                },
                {
                    "authorId": "144024329",
                    "name": "P. Godfrey"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                }
            ]
        },
        {
            "paperId": "efb9eaf996f3bb1b59f358461260cb4e471d6aee",
            "title": "Fine-Tuning Dependencies with Parameters",
            "abstract": "Discovering dependencies from data has been studied extensively, with applications in data mining, data integration and data cleaning. Recent work has proposed manually parameterized relaxations of traditional dependencies. For example, a Metric Functional Dependency (MFD) with a parameter \ud835\udeff asserts that any pair of tuples that agree on the antecedent attribute values must have similar (not necessarily equal) values of the consequent attributes within a distance of \ud835\udeff . To avoid human burden, we present a framework to automatically fine-tune parameterized dependencies and a proof-of-concept implementation for MFDs. To fine-tune a dependency, we produce a concise pattern tableau, i.e., conditions representing semantically meaningful subsets of the data, along with parameter values that hold within each sub-set. Our solution is based on a variant of the weighted set cover problem to ensure conciseness, data coverage, and tight parameter values. We demonstrate the efficiency and effectiveness of our approach by analyzing its performance on two real-life datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2160926321",
                    "name": "Alireza Vezvaei"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "22690431",
                    "name": "Morteza Zihayat"
                }
            ]
        },
        {
            "paperId": "010216d37728747f490f4a32f5cf382dec000666",
            "title": "Alaska: A Flexible Benchmark for Data Integration Tasks",
            "abstract": "Data integration is a long-standing interest of the data management community and has many disparate applications, including business, science and government. We have recently witnessed impressive results in specific data integration tasks, such as Entity Resolution, thanks to the increasing availability of benchmarks. A limitation of such benchmarks is that they typically come with their own task definition and it can be difficult to leverage them for complex integration pipelines. As a result, evaluating end-to-end pipelines for the entire data integration process is still an elusive goal. In this work, we present Alaska, the first benchmark based on real-world dataset to support seamlessly multiple tasks (and their variants) of the data integration pipeline. The dataset consists of ~70k heterogeneous product specifications from 71 e-commerce websites with thousands of different product attributes. Our benchmark comes with profiling meta-data, a set of pre-defined use cases with diverse characteristics, and an extensive manually curated ground truth. We demonstrate the flexibility of our benchmark by focusing on several variants of two crucial data integration tasks, Schema Matching and Entity Resolution. Our experiments show that our benchmark enables the evaluation of a variety of methods that previously were difficult to compare, and can foster the design of more holistic data integration solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1791339",
                    "name": "Valter Crescenzi"
                },
                {
                    "authorId": "52195541",
                    "name": "A. D. Angelis"
                },
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "2056875958",
                    "name": "Maurizio Mazzei"
                },
                {
                    "authorId": "1796590",
                    "name": "P. Merialdo"
                },
                {
                    "authorId": "50880083",
                    "name": "Federico Piai"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "0391079800df42acc65e75baf4571e65e1754438",
            "title": "Efficient Discovery of Approximate Order Dependencies",
            "abstract": "Order dependencies (ODs) capture relationships between ordered domains of attributes. Approximate ODs (AODs) capture such relationships even when there exist exceptions in the data. During automated discovery of ODs, validation is the process of verifying whether an OD holds. We present an algorithm for validating approximate ODs with significantly improved runtime performance over existing methods for AODs, and prove that it is correct and has optimal runtime. By replacing the validation step in a leading algorithm for approximate OD discovery with ours, we achieve orders-of-magnitude improvements in performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "74567727",
                    "name": "Reza Karegar"
                },
                {
                    "authorId": "144024329",
                    "name": "P. Godfrey"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                }
            ]
        },
        {
            "paperId": "24fa93cf5377a63a9d1fff5c1db4800beb7d5b34",
            "title": "Effective Keyword Search in Weighted Graphs (Extended Abstract)",
            "abstract": "Real graphs contain edge and node weights, representing penalty, distance or cost. We study the problem of keyword search in weighted node-labeled graphs, in which a query consists of a set of keywords and an answer is a subgraph. We consider three ranking strategies for answer subgraphs: edge weights, node weights, and a bi-objective combination of both node and edge weights. We propose and experimentally evaluate algorithms that optimize these objectives with an approximation ratio of two.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                },
                {
                    "authorId": "22690431",
                    "name": "Morteza Zihayat"
                }
            ]
        },
        {
            "paperId": "50259a80eb00419a1b4f28e16ef3f91d2e8d2dc7",
            "title": "Fraud Buster: Tracking IRSF Using Blockchain While Protecting Business Confidentiality",
            "abstract": "Decentralized delivery of physical or digital items via a sequence of handover actions is common in telecommunication, supply chains, snail mail, email, etc. In decentralized delivery systems, items are passed between carriers, from source to destination, without a central control, and often, by carriers that belong to di\ufb00erent organizations. Delivery failures could be due to faults or the result of malicious actions like fraud, e.g., in International Revenue Share Fraud (IRSF), international phone calls are dropped by fraudulent telecommunication carriers. Tracking item delivery can help detect faults and fraudulent behavior. But the sequence of carriers used for delivery of a speci\ufb01c item is often business con\ufb01dential, and should be revealed only in case of fraud. In this paper, we demonstrate a blockchain-based system, Fraud Buster , for con\ufb01dential tracking of routes in a de-centralized delivery system. In particular, we illustrate the ability to track handover of calls while preserving business con\ufb01dentiality when detecting where calls were dropped. The paper makes the use of a permissioned blockchain for tracking the required information yet revealing only the necessary information, when a fraud occurs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153062568",
                    "name": "Shuaicheng Ma"
                },
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "1742591",
                    "name": "Y. Kanza"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2068238386",
                    "name": "L. Xiong"
                }
            ]
        },
        {
            "paperId": "53546bba4e31a56c663d6c6499229c4ea863e3de",
            "title": "DataExposer: Exposing Disconnect between Data and Systems",
            "abstract": "As data is a central component of many modern systems, the cause of a system malfunction may reside in the data, and, specifically, particular properties of the data. For example, a health-monitoring system that is designed under the assumption that weight is reported in imperial units (lbs) will malfunction when encountering weight reported in metric units (kilograms). Similar to software debugging, which aims to find bugs in the mechanism (source code or runtime conditions), our goal is to debug the data to identify potential sources of disconnect between the assumptions about the data and the systems that operate on that data. Specifically, we seek which properties of the data cause a data-driven system to malfunction. We propose DataExposer, a framework to identify data properties, called profiles, that are the root causes of performance degradation or failure of a system that operates on the data. Such identification is necessary to repair the system and resolve the disconnect between data and system. Our technique is based on causal reasoning through interventions: when a system malfunctions for a dataset, DataExposer alters the data profiles and observes changes in the system's behavior due to the alteration. Unlike statistical observational analysis that reports mere correlations, DataExposer reports causally verified root causes, in terms of data profiles, of the system malfunction. We empirically evaluate DataExposer on three real-world and several synthetic data-driven systems that fail on datasets due to a diverse set of reasons. In all cases, DataExposer identifies the root causes precisely while requiring orders of magnitude fewer interventions than prior techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2663974",
                    "name": "Sainyam Galhotra"
                },
                {
                    "authorId": "3071906",
                    "name": "Anna Fariha"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                },
                {
                    "authorId": "2283085",
                    "name": "A. Meliou"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "7350d7c28806306af90bfe37239917911d701869",
            "title": "BEER: Blocking for Effective Entity Resolution",
            "abstract": "Blocking is a key component of Entity Resolution (ER) that aims to improve efficiency by quickly pruning out non-matching record pairs. However, depending on the noise in the dataset and the distribution of entity cluster sizes, existing techniques can be either (a) too aggressive, such that they help scale but can adversely affect the ER effectiveness, or (b) too permissive, potentially harming ER efficiency. We propose a new methodology of progressive blocking that enables both efficient and effective ER and works across different entity cluster size distributions without manual fine tuning. In this paper, we demonstrate BEER (Blocking for Effective Entity Resolution), the first end-to-end system that leverages intermediate ER output in a feedback loop to refine the blocking result in a data-driven fashion, thereby enabling effective entity resolution. BEER allows the user to explore the different components of the ER pipeline, analyze the effectiveness of alternative blocking techniques and understand the interaction between blocking and ER. BEER supports visualization of the different entities present in a block, explains the change in blocking output with every round of feedback and allows the end-user to interactively compare different techniques. BEER has been developed as open-source software; the code and the demonstration video are available at beer-system.github.io.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2663974",
                    "name": "Sainyam Galhotra"
                },
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "3269130",
                    "name": "B. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "801ddbf7bfaeb2a3cc91697614f5828f4cad9cb8",
            "title": "The Secret Life of Wikipedia Tables",
            "abstract": "Tables on the web, such as those on Wikipedia, are not the static grid of values that they seem to be. Rather, they have a life of their own: they are created under certain circumstances and in certain webpage locations, they change their shape, they move, they grow, they shrink, their data changes, they vanish, and they re-appear. When users look at web tables or when scientists extract data from them, they are most likely not aware that behind each table lies a rich history. For this empirical paper, we have extracted, matched and analyzed the entire history of all 3.5M tables on the English Wikipedia for a total of 53.8M table versions. Based on this enormous dataset of public table histories, we provide various analysis results, such as statistics about lineage sizes, table positions, volatility, change intervals, schema changes, and their editors. Apart from satisfying curiosity, analyzing and understanding the change-behavior of web tables serves various use cases, such as identifying out-of-date values, recognizing systematic changes across tables, and discovering change dependencies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7870886",
                    "name": "Tobias Bleifu\u00df"
                },
                {
                    "authorId": "49625591",
                    "name": "Leon Bornemann"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "8c7297a79c696aa0f67a3f1c89508120ccf74877",
            "title": "Retrieving Skill-Based Teams from Collaboration Networks",
            "abstract": "Given a set of required skills, the objective of the team formation problem is to form a team of experts that cover the required skills. Most existing approaches are based on graph methods, such as minimum-cost spanning trees. These approaches, due to their limited view of the network, fail to capture complex interactions among experts and are computationally intractable. More recent approaches adopt neural architectures to learn a mapping between the skills and experts space. While they are more effective, these techniques face two main limitations: (1) they consider a fixed representation for both skills and experts, and (2) they overlook the significant amount of past collaboration network information. We learn dense representations for skills and experts based on previous collaborations and bootstrap the training process through transfer learning. We also propose to fine-tune the representation of skills and experts while learning the mapping function. Our experiments over the DBLP dataset verify that our proposed architecture is able to outperform the state-of-the-art graph and neural methods over both ranking and quality metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52465784",
                    "name": "Radin Hamidi Rad"
                },
                {
                    "authorId": "145632843",
                    "name": "E. Bagheri"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                }
            ]
        },
        {
            "paperId": "8f933f2fa956a370de583c5f5cc29829615250d7",
            "title": "Structured Object Matching across Web Page Revisions",
            "abstract": "A considerable amount of useful information on the web is (semi-)structured, such as tables and lists. An extensive corpus of prior work addresses the problem of making these human-readable representations interpretable by algorithms. Most of these works focus only on the most recent snapshot of these web objects. However, their evolution over time represents valuable information that has barely been tapped, enabling various applications, including visual change exploration and trust assessment. To realize the full potential of this information, it is critical to match such objects across page revisions.In this work, we present novel techniques that match tables, infoboxes and lists within a page across page revisions. We are, thus, able to extract the evolution of structured information in various forms from a long series of web page revisions. We evaluate our approach on a representative sample of pages and measure the number of correct matches. Our approach achieves a significant improvement in object matching over baselines and over related work.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7870886",
                    "name": "Tobias Bleifu\u00df"
                },
                {
                    "authorId": "49625591",
                    "name": "Leon Bornemann"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "c61ab11d602e32f65e58774ca02452b93f795723",
            "title": "Scalable signal reconstruction for a broad range of applications",
            "abstract": "Signal reconstruction problem (SRP) is an important optimization problem where the objective is to identify a solution to an underdetermined system of linear equations that is closest to a given prior. It has a substantial number of applications in diverse areas, such as network traffic engineering, medical image reconstruction, acoustics, astronomy, and many more. Unfortunately, most of the common approaches for solving SRP do not scale to large problem sizes. We propose a novel and scalable algorithm for solving this critical problem. Specifically, we make four major contributions. First, we propose a dual formulation of the problem and develop the DIRECT algorithm that is significantly more efficient than the state of the art. Second, we show how adapting database techniques developed for scalable similarity joins provides a substantial speedup over DIRECT. Third, we describe several practical techniques that allow our algorithm to scale---on a single machine---to settings that are orders of magnitude larger than previously studied. Finally, we use the database techniques of materialization and reuse to extend our result to dynamic settings where the input to the SRP changes. Extensive experiments on real-world and synthetic data confirm the efficiency, effectiveness, and scalability of our proposal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1717283",
                    "name": "Abolfazl Asudeh"
                },
                {
                    "authorId": "51124778",
                    "name": "Jees Augustine"
                },
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                },
                {
                    "authorId": "3038200",
                    "name": "Azade Nazi"
                },
                {
                    "authorId": "2001017282",
                    "name": "Nan Zhang"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "d0868fd43ed63bbcbd1932e2ee987afc31887df4",
            "title": "Exploring Data Using Pa erns: A Survey and Open Problems",
            "abstract": "We present a survey of data exploration methods that extract multi-dimensional patterns from datasets consisting of dimen-sion and measure attributes. These patterns are designed to summarize common properties of tuples associated with particular values of the measure attributes. We provide a categorization of the characteristics of patterns produced by various solutions to this problem, we point out the pros, cons and performance optimizations of existing methods, and we suggest directions for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "d7455e6a1fe9affc08b99c6361bb6f9ca2717815",
            "title": "Real-World Trajectory Sharing with Local Differential Privacy",
            "abstract": "\n Sharing trajectories is beneficial for many real-world applications, such as managing disease spread through contact tracing and tailoring public services to a population's travel patterns. However, public concern over privacy and data protection has limited the extent to which this data is shared. Local differential privacy enables data sharing in which users share a perturbed version of their data, but existing mechanisms fail to incorporate user-independent public knowledge (e.g., business locations and opening times, public transport schedules, geo-located tweets). This limitation makes mechanisms too restrictive, gives unrealistic outputs, and ultimately leads to low practical utility. To address these concerns, we propose a local differentially private mechanism that is based on perturbing hierarchically-structured, overlapping\n n\n -grams (i.e., contiguous subsequences of length\n n\n ) of trajectory data. Our mechanism uses a multi-dimensional hierarchy over publicly available external knowledge of real-world places of interest to improve the realism and utility of the perturbed, shared trajectories. Importantly, including real-world public data does not negatively affect privacy or efficiency. Our experiments, using real-world data and a range of queries, each with real-world application analogues, demonstrate the superiority of our approach over a range of alternative methods.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2120395798",
                    "name": "Teddy Cunningham"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1787789",
                    "name": "H. Ferhatosmano\u011flu"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "13a6fe8ba31e3e79ab50bb73ddabcfaea0e1f711",
            "title": "Data-driven domain discovery for structured datasets",
            "abstract": "The growing number of open datasets has created new opportunities to derive insights and address important societal problems. These data, however, often come with little or no metadata, in particular about the types of their attributes, thus greatly limiting their utility. In this paper, we address the problem of domain discovery: given a collection of tables, we aim to identify sets of terms that represent instances of a semantic concept or domain. Knowledge of attribute domains not only enables a richer set of queries over dataset collections, but it can also help in data integration. We propose a data-driven approach that leverages value co-occurrence information across a large number of dataset columns to derive robust context signatures and infer domains. We discuss the results of a detailed experimental evaluation, using real urban dataset collections, which show that our approach is robust and outperforms state-of-the-art methods in the presence of incomplete columns, heterogeneous or erroneous data, and scales to datasets with several million distinct terms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "23279097",
                    "name": "Masayo Ota"
                },
                {
                    "authorId": "50700023",
                    "name": "Heiko M\u00fcller"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "40e9f3bed830fd946dd6019aa053d093d8889fb7",
            "title": "Erratum for Discovering Order Dependencies through Order Compatibility (EDBT 2019)",
            "abstract": "A number of extensions to the classical notion of functional dependencies have been proposed to express and enforce application semantics. One of these extensions is that of order dependencies (ODs), which express rules involving order. The article entitled \u201cDiscovering Order Dependencies through Order Compatibility\u201d by Consonni et al., published in the EDBT conference proceedings in March 2019, investigates the OD discovery problem. The authors claim to prove that their OD discovery algorithm, OCDDIS-COVER, is complete , as well as being significantly more efficient in practice than the state-of-the-art. They further claim that the implementation of the existing FASTOD algorithm (ours)\u2014we shared our code base with the authors\u2014which they benchmark against is flawed, as OCDDISCOVER and FASTOD report different sets of ODs over the same data sets. In this rebuttal, we show that their claim of completeness is, in fact, not true. OCDDISCOVER\u2019s pruning rules are overly aggressive, and prune parts of the search space that contain legitimate ODs. This is the reason their approach appears to be \u201cfaster\u201d in practice. Finally, we show that Consonni et al. misinterpret our set-based canonical form for ODs, leading to an incorrect claim that our FASTOD implementation has an error.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                },
                {
                    "authorId": "144024329",
                    "name": "P. Godfrey"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "480b7215ebfba27057b521e8a1fba88a2c15e8c7",
            "title": "Orca-SR",
            "abstract": "Reconstructing a high dimensional unknown signal, using lower dimensional observations is a challenging problem, known as signal reconstruction problem (SRP), with diverse applications including network traffic engineering, medical image reconstruction, and astronomy. Recently the database community has shown significant advancements in solving the SRP problem efficiently, effectively, and in scale by leveraging database techniques such as similarity joins. In this demo, we demonstrate Orca-SR that highlights the benefits of signal reconstruction in scale by demonstrating real-time network traffic flow analysis on large networks that were not possible before. Orca-SR is a web application that enables a user to generate network flow and load the network for interactive analysis of the impact of different traffic patterns on signal reconstruction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51124778",
                    "name": "Jees Augustine"
                },
                {
                    "authorId": "1442194534",
                    "name": "Suraj Shetiya"
                },
                {
                    "authorId": "1717283",
                    "name": "Abolfazl Asudeh"
                },
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                },
                {
                    "authorId": "3038200",
                    "name": "Azade Nazi"
                },
                {
                    "authorId": "1596798563",
                    "name": "Nan Zhang"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "65ff88612cc49943cc3a15fde5a842dcbdccda4f",
            "title": "Natural Key Discovery in Wikipedia Tables",
            "abstract": "Wikipedia is the largest encyclopedia to date. Scattered among its articles, there is an enormous number of tables that contain structured, relational information. In contrast to database tables, these webtables lack metadata, making it difficult to automatically interpret the knowledge they harbor. The natural key is a particularly important piece of metadata, which acts as a primary key and consists of attributes inherent to an entity. Determining natural keys is crucial for many tasks, such as information integration, table augmentation, or tracking changes to entities over time. To address this challenge, we formally define the notion of natural keys and propose a supervised learning approach to automatically detect natural keys in Wikipedia tables using carefully engineered features. Our solution includes novel features that extract information from time (a table\u2019s version history) and space (other similar tables). On a curated dataset of 1,000 Wikipedia table histories, our model achieves 80% F-measure, which is at least 20% more than all related approaches. We use our model to discover natural keys in the entire corpus of Wikipedia tables and provide the dataset to the community to facilitate future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49625591",
                    "name": "Leon Bornemann"
                },
                {
                    "authorId": "7870886",
                    "name": "Tobias Bleifu\u00df"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "1488674669",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "67d8771d6c7f1fca12c7f92e15a0cb8048f93393",
            "title": "Discovering Domain Orders through Order Dependencies",
            "abstract": "Much real-world data come with explicitly defined domain orders; e.g., lexicographic order for strings, numeric for integers, and chronological for time. Our goal is to discover implicit domain orders that we do not already know; for instance, that the order of months in the Chinese Lunar calendar is Corner < Apricot < Peach. To do so, we enhance data profiling methods by discovering implicit domain orders in data through order dependencies. We enumerate tractable special cases and proceed towards the most general case, which we prove is NP-complete. We show that the general case nevertheless can be effectively handled by a SAT solver. We also devise an interestingness measure to rank the discovered implicit domain orders, which we validate with a user study. Based on an extensive suite of experiments with real-world data, we establish the efficacy of our algorithms, and the utility of the domain orders discovered by demonstrating significant added value in three applications (data profiling, query optimization, and data mining).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "74567727",
                    "name": "Reza Karegar"
                },
                {
                    "authorId": "79434948",
                    "name": "Melicaalsadat Mirsafian"
                },
                {
                    "authorId": "144024329",
                    "name": "P. Godfrey"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                }
            ]
        },
        {
            "paperId": "8d13e821be3308ccdd5e28b8e816b78fd1cd135b",
            "title": "Summarizing Hierarchical Multidimensional Data",
            "abstract": "Data scientists typically analyze and extract insights from large multidimensional data sets such as US census data, enterprise sales data, and so on. But before sophisticated machine learning and statistical methods are employed, it is useful to build and explore concise summaries of the data set. While a variety of summaries have been proposed over the years, the goal of creating a concise summary of multidimensional data that can provide worst-case accuracy guarantees has remained elusive. In this paper, we propose Tree Summaries, which attain this challenging goal over arbitrary hierarchical multidimensional data sets. Intuitively, a Tree Summary is a weighted \"embedded tree\" in the lattice that is the cross-product of the dimension hierarchies; individual data values can be efficiently estimated by looking up the weight of their unique closest ancestor in the Tree Summary. We study the problems of generating lossless as well as (given a desired worst-case accuracy guarantee a) lossy Tree Summaries. We develop a polynomial-time algorithm that constructs the optimal (i.e., most concise) Tree Summary for each of these problems; this is a surprising result given the NP-hardness of constructing a variety of other optimal summaries over multidimensional data. We complement our analytical results with an empirical evaluation of our algorithm, and demonstrate with a detailed set of experiments on real and synthetic data sets that our algorithm outperforms prior methods in terms of conciseness of summaries or accuracy of estimation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2080286891",
                    "name": "Alexandra Kim"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "8d8137d4d033503f0b31ad98ad0013530cf89367",
            "title": "Discovering Band Order Dependencies",
            "abstract": "We introduce band ODs to model the semantics of attributes that are monotonically related with small variations without there being an intrinsic violation of semantics. To make band ODs relevant to real-world applications, we make them less strict to hold approximately with some exceptions. Since formulating integrity constraints manually is cumbersome, we study the problem of automatic approximate band OD discovery. We devise an algorithm that determines the optimal solution in polynomial time. We perform a thorough experimental evaluation of our techniques over real-world and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112727336",
                    "name": "Pei Li"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                },
                {
                    "authorId": "2272920708",
                    "name": "Michael H. B\u00f6hlen"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "aa9abec21737025717561a48768469e328718dff",
            "title": "Effective Keyword Search Over Weighted Graphs",
            "abstract": "Real graphs often contain edge and node weights, representing, for instance, penalty, distance or uncertainty. We study the problem of keyword search over weighted node-labeled graphs, in which a query consists of a set of keywords and an answer is a subgraph whose nodes contain the keywords. We evaluate answers using three ranking strategies: optimizing edge weights, optimizing node weights, and a bi-objective combination of both node and edge weights. We prove that optimizing node weights and the bi-objective function are NP-hard. We propose an algorithm that optimizes edge weights and has an approximation ratio of two for the unique node enumeration paradigm. To optimize node weights and the bi-objective function, we propose transformations that distribute node weights onto the edges. We then prove that our transformations allow our algorithm to also optimize node weights and the bi-objective function with the same approximation ratio of two. Notably, the proposed transformations are compatible with existing algorithms that only optimize edge weights. We empirically show that in many natural examples, incorporating node weights (both keyword holders and middle nodes) produces more relevant answers than ranking methods based only on edge weights. Extensive experiments over real-life datasets verify the effectiveness and efficiency of our solution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                },
                {
                    "authorId": "22690431",
                    "name": "Morteza Zihayat"
                }
            ]
        },
        {
            "paperId": "c799aa8aaee428aea90c69f9097a82e580d96885",
            "title": "Smartmedia: Locally & Contextually-Adapted Streaming Media",
            "abstract": "Streaming media is gaining popularity, with numerous new services for video on demand and live broadcast. These services stream requested media content to user devices like smart TVs, personal computers and smartphones. Many of these devices are mobile devices, yet streaming media services do not adapt the delivered content to the context of the request, e.g., the location of the user, the time of the request or who are the people near the viewer. In this paper we illustrate how geofencing and geoblocking can be combined with adaptive streaming media to create a new technology of contextually-adapted streaming media. The new technology has the potential to create a new type of experience for streaming media by applying real-time modification of streamed content according to the viewer and the context, e.g., modify inappropriate content when the media is played in public places, change the length of a video played by a train passenger according to a train schedule, etc. The suggested technology can also be used for preventing password sharing in a non-intrusive way and for hyperlocal geoblocking, to facilitate copyright protection. We discuss the vision of modifying streaming media in real time based on the context, elaborate on some of the challenges in implementing this vision, and present novel applications of this new technology.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1742591",
                    "name": "Y. Kanza"
                },
                {
                    "authorId": "2387879",
                    "name": "D. Gibbon"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2067081288",
                    "name": "Valerie Yip"
                },
                {
                    "authorId": "2900213",
                    "name": "E. Zavesky"
                }
            ]
        },
        {
            "paperId": "15cd4cc94b4d67693323611a31851a7a5feb610f",
            "title": "Exploring Change",
            "abstract": "Data and schema in datasets experience many different kinds of change: values are inserted, deleted or updated; rows appear and disappear; columns are added or repurposed, and so on. In such a dynamic situation, users might wonder: How many changes have there been in the recent minutes, days or years? What kind of changes were made at which points of time? How volatile are the values of a specific property or entity? How dirty is the data? The fact that data changed can hint at different hidden processes: a frequently crowd-updated city name may be controversial; a person whose name has been recently changed may be the target of vandalism; and so on. To interactively explore such changes, we present our DBChEx (Database Change Explorer) prototype system [2]. Using two real-world datasets, IMDB and Wikipedia infoboxes, we illustrate how users can gain valuable insights into data generation processes and data or schema evolution over time by a mix of serendipity and guided investigation using DBChEx. Finally, we identify a range of technical challenges that need to be addressed to fully realize our vision of change exploration [1].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7870886",
                    "name": "Tobias Bleifu\u00df"
                },
                {
                    "authorId": "49625591",
                    "name": "Leon Bornemann"
                },
                {
                    "authorId": "145633895",
                    "name": "T. Johnson"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "28b5c28ba5d0d6e2a38c0b5365ae9c4b860efaef",
            "title": "Bring Order to Data",
            "abstract": "Integrity constraints (ICs) are widely used in business intelligence to express and enforce application semantics. However, finding ICs manually is time consuming, requires the involvement of domain experts, and is prone to human error. Thus, techniques have been proposed to automatically find a variety of ICs. We propose an algorithm to automatically discover order dependencies (ODs). Prior work on OD discovery has factorial complexity, is not complete, and is not concise. We propose an algorithm that finds a complete set of ODs with exponential worst-case time complexity in the number of attributes and linear complexity in the number of tuples. We experimentally show that our algorithm is orders of magnitude faster than the prior state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7910450",
                    "name": "Heydar Davoudi"
                },
                {
                    "authorId": "144024329",
                    "name": "P. Godfrey"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                }
            ]
        },
        {
            "paperId": "37ce25013d2c01b759224835d709ccfa4af0e31d",
            "title": "Effective Discovery of Meaningful Outlier Relationships",
            "abstract": "We propose Predictable Outliers in Data-trendS (PODS), a method that, given a collection of temporal datasets, derives data-driven explanations for outliers by identifying meaningful relationships between them. First, we formalize the notion of meaningfulness, which so far has been informally framed in terms of explainability. Next, since outliers are rare and it is difficult to determine whether their relationships are meaningful, we develop a new criterion that does so by checking if these relationships could have been predicted from non-outliers, i.e., whether we could see the outlier relationships coming. Finally, searching for meaningful outlier relationships between every pair of datasets in a large data collection is computationally infeasible. To address that, we propose an indexing strategy that prunes irrelevant comparisons across datasets, making the approach scalable. We present the results of an experimental evaluation using real datasets and different baselines, which demonstrates the effectiveness, robustness, and scalability of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "112915707",
                    "name": "Aline Bessa"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                },
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "4d7e7dbab3403ba08edf4b5c33e0d7a389729500",
            "title": "Efficient Discovery of Meaningful Outlier Relationships",
            "abstract": "We propose PODS ( Predictable Outliers in Data-trendS ), a method that, given a collection of temporal data sets, derives data-driven explanations for outliers by identifying meaningful relationships between them. First, we formalize the notion of meaningfulness, which so far has been informally framed in terms of explainability. Next, since outliers are rare and it is difficult to determine whether their relationships are meaningful, we develop a new criterion that does so by checking if these relationships could have been predicted from non-outliers, i",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "112915707",
                    "name": "Aline Bessa"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                }
            ]
        },
        {
            "paperId": "5128005eddc7c3bd303d376e2df8bb215e05685d",
            "title": "Ensuring High-Quality Private Data for Responsible Data Science",
            "abstract": "High-quality data is critical for effective data science. As the use of data science has grown, so too have concerns that individuals\u2019 rights to privacy will be violated. This has led to the development of data protection regulations around the globe and the use of sophisticated anonymization techniques to protect privacy. Such measures make it more challenging for the data scientist to understand the data, exacerbating issues of data quality. Responsible data science aims to develop useful insights from the data while fully embracing these considerations. We pose the high-level problem in this article, \u201cHow can a data scientist develop the needed trust that private data has high quality?\u201d We then identify a series of challenges for various data-centric communities and outline research questions for data quality and privacy researchers, which would need to be addressed to effectively answer the problem posed in this article.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1789318",
                    "name": "M. Scannapieco"
                },
                {
                    "authorId": "48902235",
                    "name": "T. Redman"
                }
            ]
        },
        {
            "paperId": "5406a2d681dd518a2330e85210dadf33f1a206e8",
            "title": "Don't Cry Wolf",
            "abstract": "Real world anomaly management systems oversee thousands of dynamic data streams and generate an overwhelming number of alerts. As a consequence, important alerts often go unnoticed until there is a crisis. The absence of ground truth, and the fact that the streams are constantly changing (new content, new applications, software and hardware changes) makes assessing the value of alerts difficult. In order to identify groups of important and actionable alerts, we propose: (1) superalerts that reflect characteristics of persistence, pervasiveness and priority, (2) three types of super-alerting based on three types of aggregations and, (3) corresponding metrics for evaluating them. We demonstrate using real-world entertainment data streams.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115083267",
                    "name": "Philip E. Brown"
                },
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "1742591",
                    "name": "Y. Kanza"
                },
                {
                    "authorId": "2621476",
                    "name": "E. Koutsofios"
                },
                {
                    "authorId": "2286247242",
                    "name": "Rajat Malik"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "67cb92b4e2650b426d5b374ac499d47a896b38cd",
            "title": "Interpreting deep learning models for entity resolution: an experience report using LIME",
            "abstract": "Entity Resolution (ER) seeks to understand which records refer to the same entity (e.g., matching products sold on multiple websites). The sheer number of ways humans represent and misrepresent information about real-world entities makes ER a challenging problem. Deep Learning (DL) has provided impressive results in the field of natural language processing, thus recent works started exploring DL approaches to the ER problem, with encouraging results. However, we are still far from understanding why and when these approaches work in the ER setting. We are developing a methodology, Mojito, to produce explainable interpretations of the output of DL models for the ER task. Our methodology is based on LIME, a popular tool for producing prediction explanations for generic classification tasks. In this paper we report our first experiences in interpreting recent DL models for the ER task. Our results demonstrate the importance of explanations in the DL space, and suggest that, when assessing performance of DL algorithms for ER, accuracy alone may not be sufficient to demonstrate generality and reproducibility in a production environment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "122305078",
                    "name": "Vincenzo Di Cicco"
                },
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "1796590",
                    "name": "P. Merialdo"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "7e73b03aa13a1f02664a86f8fa4e8b8e49a0b996",
            "title": "From Rocks to Pebbles",
            "abstract": "Spatiotemporal streams are prone to data quality issues such as missing, duplicated and delayed data\u2014when data generating sensors malfunction, data transmissions experience problems, or when data are stored or processed improperly. However, many important real-time applications rely on the continuous availability of stream values, e.g., to monitor traffic flow, resource usage, weather phenomena, and so on. Other non real-time applications that support continuous or offline historical analytics also require high quality data to avoid producing misleading output such as false positives, erroneous conclusions, and decisions. In this article, we study the problem of smoothing streams produced by an overlay of sensors. We present nonparametric (data-driven, distribution free) statistical methods to provide an uninterrupted stream of high-quality spatiotemporal data to real-time applications, even when the raw stream suffers data quality issues, such as noise or missing values. Our novel family of robust methods computes smoothed values (SVs) that could be used as proxies for data of questionable quality. The methods make use of a partition of the monitored area into cells to compute SVs based on historical data and the deviation from normalcy in neighboring spatial cells in a way that outperforms standard regression or interpolation. Our methods use incremental computation for efficiency, and they differ in how the deviations are normalized, e.g., with respect to zeroth-order, first-order, and second-order moments. We use three real data sets to run a suite of experiments and empirically demonstrate the superiority of the method that uses normalization with respect to variability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115083267",
                    "name": "Philip E. Brown"
                },
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "1742591",
                    "name": "Y. Kanza"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "82d6aeabc6c6852fb3bfbfc4e4d1f621711ddb04",
            "title": "DBChEx: Interactive Exploration of Data and Schema Change",
            "abstract": "Data exploration is a visually-driven process that is often used as a \ufb01rst step to decide which aspects of a dataset are worth further investigation and analysis. It serves as an important tool to gain a \ufb01rst understanding of a dataset and to generate hypotheses. While there are many tools for exploring static datasets, dynamic datasets that change over time still lack e\ufb00ective exploration support. To address this shortcoming, we present our innovative tool Database Change Explorer (DBChEx) that enables exploration of data and schema change through a set of exploration primitives. Users gain valuable insights into data generation processes and data or schema evolution over time by a mix of serendipity and guided investigation. The tool is a server-client application with a web front-end and an underlying database that stores the history of changes in the data and schema in a data model called the change-cube. Our demonstration of DBChEx shows how users can inter-actively explore data and schema change in two real-world datasets, IMDB and Wikipedia infoboxes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7870886",
                    "name": "Tobias Bleifu\u00df"
                },
                {
                    "authorId": "49625591",
                    "name": "Leon Bornemann"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "aebe1bccd1d17cb003a5f8dba5a7a342110b8a85",
            "title": "Informative Summarization of Numeric Data",
            "abstract": "We consider the following data summarization problem. We are given a dataset including ordinal or numeric explanatory attributes and an outcome attribute. We want to produce a summary of how the explanatory attributes affect the outcome attribute. The summary must be human-interpretable, concise, and informative in the sense that it can accurately approximate the distribution of the outcome attribute. We propose a solution that addresses the fundamental challenge of this problem--handling large numeric domains--and we experimentally show the effectiveness and efficiency of our approach on real datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2038483448",
                    "name": "Michael Vollmer"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "145635095",
                    "name": "Klemens B\u00f6hm"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "bb031c60f638f2cae712f907a7df5049bd366876",
            "title": "Random Sampling for Group-By Queries",
            "abstract": "Random sampling has been widely used in approximate query processing on large databases, due to its potential to significantly reduce resource usage and response times, at the cost of a small approximation error. We consider random sampling for answering the ubiquitous class of group-by queries, which first group data according to one or more attributes, and then aggregate within each group after filtering through a predicate. The challenge with group-by queries is that a sampling method cannot focus on optimizing the quality of a single answer (e.g. the mean of selected data), but must simultaneously optimize the quality of a set of answers (one per group).We present CVOPT, a query- and data-driven sampling framework for a set of group-by queries. To evaluate the quality of a sample, CVOPT defines a metric based on the norm (e.g. \u21132 or \u2113\u221e) of the coefficients of variation (CVs) of different answers, and constructs a stratified sample that provably optimizes the metric. CVOPT can handle group-by queries on data where groups have vastly different statistical characteristics, such as frequencies, means, or variances. CVOPT jointly optimizes for multiple aggregations and multiple group-by clauses, and provides a way to prioritize specific groups or aggregates. It can be tuned to cases when partial information about a query workload is known, such as a data warehouse where queries are run periodically.Our experimental results show that CVOPT outperforms the current state-of-the-art on sample quality and estimation accuracy for group-by queries. On a set of queries on two real-world data sets, CVOPT yields relative errors that are 5\u00d7 smaller than competing approaches, under the same space budget.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3400339",
                    "name": "Trong Duc Nguyen"
                },
                {
                    "authorId": "38159019",
                    "name": "Ming-Hung Shih"
                },
                {
                    "authorId": "153040932",
                    "name": "Sai Sree Parvathaneni"
                },
                {
                    "authorId": "39719831",
                    "name": "Bojian Xu"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1679836",
                    "name": "Srikanta Tirthapura"
                }
            ]
        },
        {
            "paperId": "c04ee2046efb29262c777d31fdcbb2a363a2d6a9",
            "title": "Crowd-Sourced Entity Resolution with Control Queries",
            "abstract": "Entity resolution (ER) seeks to identify which records in a data set refer to the same real-world entity. Given the diversity of ways in which entities can be represented, ER is known to be a challenging task for automated strategies, but relatively easier for expert humans. Nonetheless, also humans can make mistakes. Our contribution is an error correction toolkit that can be leveraged by a variety of hybrid human-machine ER algorithms, based on a formal way for selecting \u201ccontrol queries\u201d for the human experts. We demonstrate empirically that less recent ER algorithms equipped with our tool can perform even better than most recent ER methods with built-in error correction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2663974",
                    "name": "Sainyam Galhotra"
                },
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "3269130",
                    "name": "B. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "c33ed87ce9797950be5437a7a74e97367091f57d",
            "title": "Efficient Signal Reconstruction for a Broad Range of Applications",
            "abstract": "The signal reconstruction problem (SRP) is an important optimization problem where the objective is to identify a solution to an under-determined system of linear equations AX = b that is closest to a given prior. It has a substantial number of applications in diverse areas including network traffic engineering, medical image reconstruction, acoustics, astronomy and many more. Most common approaches for solving SRP do not scale to large problem sizes. In this paper, we propose a dual formulation of this problem and show how adapting database techniques developed for scalable similarity joins provides a significant speedup when the A matrix is sparse and binary. Extensive experiments on real-world and synthetic data show that our approach produces a significant speedup of up to 20x over competing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1717283",
                    "name": "Abolfazl Asudeh"
                },
                {
                    "authorId": "51124778",
                    "name": "Jees Augustine"
                },
                {
                    "authorId": "3038200",
                    "name": "Azade Nazi"
                },
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                },
                {
                    "authorId": "65857992",
                    "name": "Nan Zhang"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "dcd90d0028ee520f012e91bd87452025673e1e80",
            "title": "Errata Note: Discovering Order Dependencies through Order Compatibility",
            "abstract": "A number of extensions to the classical notion of functional dependencies have been proposed to express and enforce application semantics. One of these extensions is that of order dependencies (ODs), which express rules involving order. The article entitled \"Discovering Order Dependencies through Order Compatibility\" by Consonni et al., published in the EDBT conference proceedings in March 2019, investigates the OD discovery problem. They claim to prove that their OD discovery algorithm, OCDDISCOVER, is complete, as well as being significantly more efficient in practice than the state-of-the-art. They further claim that the implementation of the existing FASTOD algorithm (ours)-we shared our code base with the authors-which they benchmark against is flawed, as OCDDISCOVER and FASTOD report different sets of ODs over the same data sets. \nIn this rebuttal, we show that their claim of completeness is, in fact, not true. Built upon their incorrect claim, OCDDISCOVER's pruning rules are overly aggressive, and prune parts of the search space that contain legitimate ODs. This is the reason their approach appears to be \"faster\" in practice. Finally, we show that Consonni et al. misinterpret our set-based canonical form for ODs, leading to an incorrect claim that our FASTOD implementation has an error.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144024329",
                    "name": "P. Godfrey"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                }
            ]
        },
        {
            "paperId": "fcae8cff22706d48a5b932f37546e1fd0a9a3795",
            "title": "Constrained Private Mechanisms for Count Data",
            "abstract": "Concern about how to aggregate sensitive user data without compromising individual privacy is a major barrier to greater availability of data. Differential privacy has emerged as an accepted model to release sensitive information while giving a statistical guarantee for privacy. Many different algorithms are possible to address different target functions. We focus on the core problem of count queries, and seek to design <italic>mechanisms</italic> to release data associated with a group of <inline-formula><tex-math notation=\"LaTeX\">$n$</tex-math><alternatives><mml:math><mml:mi>n</mml:mi></mml:math><inline-graphic xlink:href=\"kulkarni-ieq1-2912179.gif\"/></alternatives></inline-formula> individuals. Prior work has focused on designing mechanisms by raw optimization of a loss function, without regard to the consequences on the results. This can lead to mechanisms with undesirable properties, such as never reporting some outputs (gaps), and overreporting others (spikes). We tame these pathological behaviors by introducing a set of desirable properties that mechanisms can obey. Any combination of these can be satisfied by solving a linear program (LP) which minimizes a cost function, with constraints enforcing the properties. We focus on a particular cost function, and provide explicit constructions that are optimal for certain combinations of properties, and show a closed form for their cost. In the end, there are only a handful of distinct optimal mechanisms to choose between: one is the well-known (truncated) geometric mechanism; the second a novel mechanism that we introduce here, and the remainder are found as the solution to particular LPs. These all avoid the bad behaviors we identify. We demonstrate in a set of experiments on real and synthetic data which is preferable in practice, for different combinations of data distributions, constraints, and privacy parameters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1954876",
                    "name": "Tejas D. Kulkarni"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "0c8a52c3eb1e1b031c69bf18879a1040f1639f02",
            "title": "Big Data Integration for Product Specifications",
            "abstract": "The product domain contains valuable data for many important applications. Given the large and increasing number of sources that provide data about product speci\ufb01cations and the velocity as well as the variety with which such data are available, this domain represents a challenging scenario for developing and evaluating big data integration solutions. In this paper, we present the results of our efforts towards big data integration for product speci\ufb01cations. We present a pipeline that decomposes the problem into different tasks from source and data discovery, to extraction, data linkage, schema alignment and data fusion. Although we present the pipeline as a sequence of tasks, different con\ufb01gurations can be de\ufb01ned depending on the application goals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056435855",
                    "name": "Luciano Barbosa"
                },
                {
                    "authorId": "1791339",
                    "name": "Valter Crescenzi"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1796590",
                    "name": "P. Merialdo"
                },
                {
                    "authorId": "50880083",
                    "name": "Federico Piai"
                },
                {
                    "authorId": "1756080",
                    "name": "Disheng Qiu"
                },
                {
                    "authorId": "2115435801",
                    "name": "Yanyan Shen"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "235b91e9d562707481c2d6db1b0f94313b519eb7",
            "title": "Lessons Learned and Research Agenda for Big Data Integration of Product Specifications",
            "abstract": "The product domain represents a challenging scenario for developing and evaluating big data integration solutions: the number of sources providing product specifications is very large, and ever increasing over time. The volume of available data is impressive, and these data keep changing very frequently. In this paper, we present ongoing efforts, challenges and our research agenda to address big data integration for product specifications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056435855",
                    "name": "Luciano Barbosa"
                },
                {
                    "authorId": "1791339",
                    "name": "Valter Crescenzi"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1796590",
                    "name": "P. Merialdo"
                },
                {
                    "authorId": "50880083",
                    "name": "Federico Piai"
                },
                {
                    "authorId": "1756080",
                    "name": "Disheng Qiu"
                },
                {
                    "authorId": "2115435801",
                    "name": "Yanyan Shen"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "24db3fcc57653281fbdba3661df5bb9e62fe2a67",
            "title": "Robust Entity Resolution using Random Graphs",
            "abstract": "Entity resolution (ER) seeks to identify which records in a data set refer to the same real-world entity. Given the diversity of ways in which entities can be represented, matched and distinguished, ER is known to be a challenging task for automated strategies, but relatively easier for expert humans. In our work, we abstract the knowledge of experts with the notion of a binary oracle. Our oracle can answer questions of the form \"do records u and v refer to the same entity?\" under a flexible error model, allowing for some questions to be more difficult to answer correctly than others. Our contribution is a general error correction tool that can be leveraged by a variety of hybrid-human machine ER algorithms, based on a formal way for selecting indirect \"control queries''. In our experiments we demonstrate that correction-less ER algorithms equipped with our tool can perform even better than recent ER algorithms specifically designed for correcting errors. Our control queries are selected among those that provide strongest connectivity between records of each cluster, based on the concept ofgraph expanders (which are sparse graphs with formal connectivity properties). We give formal performance guarantees for our toolkit and provide experiments on real and synthetic data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2663974",
                    "name": "Sainyam Galhotra"
                },
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "3269130",
                    "name": "B. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "4205094adc47f3aa92961e3b3e2180574558e2f3",
            "title": "Big Data Linkage for Product Specification Pages",
            "abstract": "An increasing number of product pages are available from thousands of web sources, each page associated with a product, containing its attributes and one or more product identifiers. The sources provide overlapping information about the products, using diverse schemas, making web-scale integration extremely challenging. In this paper, we take advantage of the opportunity that sources publish product identifiers to perform big data linkage across sources at the beginning of the data integration pipeline, before schema alignment. To realize this opportunity, several challenges need to be addressed: identifiers need to be discovered on product pages, made difficult by the diversity of identifiers; the main product identifier on the page needs to be identified, made difficult by the many related products presented on the page; and identifiers across pages need to beresolved, made difficult by the ambiguity between identifiers across product categories. We present our RaF (Redundancy as Friend) solution to the problem of big data linkage for product specification pages, which takes advantage of the redundancy of identifiers at a global level, and the homogeneity of structure and semantics at the local source level, to effectively and efficiently link millions of pages of head and tail products across thousands of head and tail sources. We perform a thorough empirical evaluation of our RaF approach using the publicly available Dexter dataset consisting of 1.9M product pages from 7.1k sources of 3.5k websites, and demonstrate its effectiveness in practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1756080",
                    "name": "Disheng Qiu"
                },
                {
                    "authorId": "2056435855",
                    "name": "Luciano Barbosa"
                },
                {
                    "authorId": "1791339",
                    "name": "Valter Crescenzi"
                },
                {
                    "authorId": "1796590",
                    "name": "P. Merialdo"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "4545f59bf5914695d2742562838ab4b063dc279d",
            "title": "Privacy at Scale: Local Differential Privacy in Practice",
            "abstract": "Local differential privacy (LDP), where users randomly perturb their inputs to provide plausible deniability of their data without the need for a trusted party, has been adopted recently by several major technology organizations, including Google, Apple and Microsoft. This tutorial aims to introduce the key technical underpinnings of these deployed systems, to survey current research that addresses related problems within the LDP model, and to identify relevant open problems and research directions for the community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1680133",
                    "name": "S. Jha"
                },
                {
                    "authorId": "1954876",
                    "name": "Tejas D. Kulkarni"
                },
                {
                    "authorId": "1740882",
                    "name": "Ninghui Li"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "49980880",
                    "name": "Tianhao Wang"
                }
            ]
        },
        {
            "paperId": "493f55ee46651a647618a3b5bf82ef233876758c",
            "title": "Robust Entity Resolution Using a CrowdOracle",
            "abstract": "Entity resolution (ER) seeks to identify which records in a data set refer to the same real-world entity. Given the diversity of ways in which entities can be represented, ER is a challenging task for automated strategies, but relatively easier for expert humans. We abstract the knowledge of experts with the notion of a boolean oracle, that can answer questions of the form \u201cdo records u and v refer to the same entity?\u201d, and formally address the problem of maximizing progressive recall and F-measure in an online setting",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "2663974",
                    "name": "Sainyam Galhotra"
                },
                {
                    "authorId": "3269130",
                    "name": "B. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "58606bac5e7c89552ce93a8de14121cf7cecfbcf",
            "title": "Geofences in the sky: herding drones with blockchains and 5G",
            "abstract": "Unmanned aerial vehicles (UAVs), typically also referred to as drones, are gaining popularity and becoming ubiquitous. As the number of drones in the sky rapidly grows, managing the expected high-volume air traffic is becoming a critical challenge. It is essential to prevent collisions, and to protect the public from nuisances like noise or invasion of privacy, and shield from hazards like falling debris. UAV traffic management should comply with regulation, spatiotemporal constraints and limitations of drones. Spatiotemporal constraints could be no-flight zones or areas where drone flight times are restricted. Drone limitations could refer to their speed, flight range, telecommunication capabilities, etc. Furthermore, managing air traffic for UAVs is very different from managing the traffic of self-driving ground vehicles. First, there are no clearly-marked roads in the sky. Second, some UAVs cannot hover and must have a cleared flight path. Third, air traffic should be managed in a 3-dimensional space. In this paper we present a vision of air-traffic control based on geofencing. We discuss three operation modes: centralized, decentralized and a hybrid of the two other modes. We present some of the challenges involved in drone traffic control and illustrate how geofencing could be a useful tool for that, while leveraging the emerging 5G networking technology.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "1742591",
                    "name": "Y. Kanza"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "7fe8958f16b5a89b32828ae0fcb8e7448a5e9d2c",
            "title": "Constrained Private Mechanisms for Count Data",
            "abstract": "Concern about how to aggregate sensitive user data without compromising individual privacy is a major barrier to greater availability of data. Differential privacy has emerged as an accepted model to release sensitive information while giving a statistical guarantee for privacy. Many different algorithms are possible to address different target functions. We focus on the core problem of count queries, and seek to design mechanisms to release data associated with a group of n individuals. Prior work has focused on designing mechanisms by raw optimization of a loss function, without regard to the consequences on the results. This can leads to mechanisms with undesirable properties, such as never reporting some outputs (gaps), and overreporting others (spikes). We tame these pathological behaviors by introducing a set of desirable properties that mechanisms can obey. Any combination of these can be satisfied by solving a linear program (LP) which minimizes a cost function, with constraints enforcing the properties. We focus on a particular cost function, and provide explicit constructions that are optimal for certain combinations of properties, and show a closed form for their cost. In the end, there are only a handful of distinct optimal mechanisms to choose between: one is the well-known (truncated) geometric mechanism; the second a novel mechanism that we introduce here, and the remainder are found as the solution to particular LPs. These all avoid the bad behaviors we identify. We demonstrate in a set of experiments on real and synthetic data which is preferable in practice, for different combinations of data distributions, constraints, and privacy parameters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1954876",
                    "name": "Tejas D. Kulkarni"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "bfb0ab34fd1b7aa6f921463f7868c5e60c24ff44",
            "title": "FASTOD: Bringing Order to Data",
            "abstract": "We present the design of and a demonstration plan for FASTOD, a tool for efficiently discovering (approximate) order dependencies (ODs) from data. FASTOD converts ODs to a novel canonical form which makes it orders of magnitude faster than existing techniques. Using real datasets, we demonstrate how the discovered ODs can help users understand data semantics, identify potential data quality problems, and interactively clean the data. We also demonstrate the effectiveness of our system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144220595",
                    "name": "Alexandar Mihaylov"
                },
                {
                    "authorId": "144024329",
                    "name": "P. Godfrey"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                }
            ]
        },
        {
            "paperId": "d2253f93906b3b5dae55e3f6be8649fa03c67b2a",
            "title": "Variance-Optimal Offline and Streaming Stratified Random Sampling",
            "abstract": "Stratified random sampling (SRS) is a fundamental sampling technique that provides accurate estimates for aggregate queries using a small size sample, and has been used widely for approximate query processing. A key question in SRS is how to partition a target sample size among different strata. While Neyman allocation provides a solution that minimizes the variance of an estimate using this sample, it works under the assumption that each stratum is abundant, i.e., has a large number of data points to choose from. This assumption may not hold in general: one or more strata may be bounded, and may not contain a large number of data points, even though the total data size may be large. \nWe first present VOILA, an offline method for allocating sample sizes to strata in a variance-optimal manner, even for the case when one or more strata may be bounded. We next consider SRS on streaming data that are continuously arriving. We show a lower bound, that any streaming algorithm for SRS must have (in the worst case) a variance that is {\\Omega}(r) factor away from the optimal, where r is the number of strata. We present S-VOILA, a practical streaming algorithm for SRS that is locally variance-optimal in its allocation of sample sizes to different strata. Our result from experiments on real and synthetic data show that VOILA can have significantly (1.4 to 50.0 times) smaller variance than Neyman allocation. The streaming algorithm S-VOILA results in a variance that is typically close to VOILA, which was given the entire input beforehand.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3400339",
                    "name": "Trong Duc Nguyen"
                },
                {
                    "authorId": "38159019",
                    "name": "Ming-Hung Shih"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1679836",
                    "name": "Srikanta Tirthapura"
                },
                {
                    "authorId": "39719831",
                    "name": "Bojian Xu"
                }
            ]
        },
        {
            "paperId": "d65df5c3f4b18b51eabde3553bf0ba8533e18042",
            "title": "FastQRE: Fast Query Reverse Engineering",
            "abstract": "We study the problem of Query Reverse Engineering (QRE), where given a database and an output table, the task is to find a simple project-join SQL query that generates that table when applied on the database. This problem is known for its efficiency challenge due to mainly two reasons. First, the problem has a very large search space and its various variants are known to be NP-hard. Second, executing even a single candidate SQL query can be very computationally expensive. In this work we propose a novel approach for solving the QRE problem efficiently. Our solution outperforms the existing state of the art by 2-3 orders of magnitude for complex queries, resolving those queries in seconds rather than days, thus making our approach more practical in real-life settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "d7c8dec61f940ebb28c09372904a300519bc9317",
            "title": "REGAL+: Reverse Engineering SPJA Queries",
            "abstract": "\n The goal of query reverse engineering is to re-generate the SQL query that produced a given result from some known database. The problem has many real world applications where users need to better understand the lineage and trustworthiness of various data reports even when the authors of those reports are no longer reachable or are unable to provide the required explanations anymore. It gets more challenging as the complexities of both the query and database schema increase. Prior work has addressed the reverse engineering of constrained types of SQL queries and sometimes on constrained schemas, such as single-table schemas. In this demonstration, we present a framework called REGAL\n +\n , which builds upon, and extends prior work to enable the discovery of Select-Project-Join-Aggregation (SPJA) queries over arbitrary schemas. Without any prior schema knowledge or SQL expertise, the user only needs to upload a data report (e.g., as a spreadsheet), and the system will automatically compute and display the queries capable of generating that report from the database.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32457689",
                    "name": "W. Tan"
                },
                {
                    "authorId": "1793178",
                    "name": "Meihui Zhang"
                },
                {
                    "authorId": "1890500",
                    "name": "Hazem Elmeleegy"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "d8949d6aab472a63418227e7be42aa8a6262eece",
            "title": "Answering Range Queries Under Local Differential Privacy",
            "abstract": "Counting the fraction of a population having an input within a specified interval i.e. range count query is a fundamental database operation. Range count queries can also be used to compute other interesting statistics such as quantiles. The framework of differential privacy [6] (DP) is becoming a standard for privacy-preserving data analysis [1]. While many works address the problem of range counting queries in the trusted aggregation model, surprisingly, this problem has not been addressed specifically under untrusted aggregation (local DP [10]). In this work we study the problem of answering 1-dimensional range count queries under the constraint of LDP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1954876",
                    "name": "Tejas D. Kulkarni"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "df7c6f8f7235600bfb996b898bb3e39882074482",
            "title": "Leveraging Similarity Joins for Signal Reconstruction",
            "abstract": "Signal reconstruction problem (SRP) is an important optimization problem where the objective is to identify a solution to an underdetermined system of linear equations that is closest to a given prior. It has a substantial number of applications in diverse areas including network traffic engineering, medical image reconstruction, acoustics, astronomy and many more. Most common approaches for SRP do not scale to large problem sizes. In this paper, we propose a dual formulation of this problem and show how adapting database techniques developed for scalable similarity joins provides a significant speedup. Extensive experiments on real-world and synthetic data show that our approach produces a significant speedup of up to 20x over competing approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1717283",
                    "name": "Abolfazl Asudeh"
                },
                {
                    "authorId": "3038200",
                    "name": "Azade Nazi"
                },
                {
                    "authorId": "51124778",
                    "name": "Jees Augustine"
                },
                {
                    "authorId": "2934941",
                    "name": "Saravanan Thirumuruganathan"
                },
                {
                    "authorId": "47899254",
                    "name": "Nan Zhang"
                },
                {
                    "authorId": "145080425",
                    "name": "Gautam Das"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "e3e3eaa0834ae16afc78641650bab979d7561821",
            "title": "Data Quality: The Role of Empiricism",
            "abstract": "We outline a call to action for promoting empiricism in data quality research. The action points result from an analysis of the landscape of data quality research. The landscape exhibits two dimensions of empiricism in data quality research relating to type of metrics and scope of method. Our study indicates the presence of a data continuum ranging from real to synthetic data, which has implications for how data quality methods are evaluated. The dimensions of empiricism and their inter-relationships provide a means of positioning data quality research, and help expose limitations, gaps and opportunities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2092368",
                    "name": "S. Sadiq"
                },
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "144162611",
                    "name": "J. Freire"
                },
                {
                    "authorId": "1743316",
                    "name": "Ihab F. Ilyas"
                },
                {
                    "authorId": "2288285",
                    "name": "S. Link"
                },
                {
                    "authorId": "2110417078",
                    "name": "Ren\u00e9e J. Miller"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "48667278",
                    "name": "Xiaofang Zhou"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "e4a1e74c0a90eed2804976bb20fe620e5c40bcca",
            "title": "Exploring Change - A New Dimension of Data Analytics",
            "abstract": "Data and metadata in datasets experience many different kinds of change. Values are inserted, deleted or updated; rows appear and disappear; columns are added or repurposed, etc. In such a dynamic situation, users might have many questions related to changes in the dataset, for instance which parts of the data are trustworthy and which are not? Users will wonder: How many changes have there been in the recent minutes, days or years? What kind of changes were made at which points of time? How dirty is the data? Is data cleansing required? The fact that data changed can hint at different hidden processes or agendas: a frequently crowd-updated city name may be controversial; a person whose name has been recently changed may be the target of vandalism; and so on. We show various use cases that benefit from recognizing and exploring such change.\n \n We envision a system and methods to interactively explore such change, addressing the\n variability\n dimension of big data challenges. To this end, we propose a model to capture change and the process of exploring dynamic data to identify salient changes. We provide exploration primitives along with motivational examples and measures for the volatility of data. We identify technical challenges that need to be addressed to make our vision a reality, and propose directions of future work for the data management community.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7870886",
                    "name": "Tobias Bleifu\u00df"
                },
                {
                    "authorId": "49625591",
                    "name": "Leon Bornemann"
                },
                {
                    "authorId": "145633895",
                    "name": "T. Johnson"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "f84b3dc2fbbee546ebcf8095c81ba72284deb9b3",
            "title": "Cover Page",
            "abstract": "Department of Biomedical Engineering Centre for Healthcare Technologies.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "145624228",
                    "name": "Christoph E. Koch"
                },
                {
                    "authorId": "1738210",
                    "name": "Minos N. Garofalakis"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1751802",
                    "name": "K. Aberer"
                },
                {
                    "authorId": "1719183",
                    "name": "W. Klas"
                },
                {
                    "authorId": "1720826",
                    "name": "E. Neuhold"
                }
            ]
        },
        {
            "paperId": "04aed47b71fd3a07a9a9d04ec9c9429a242299a6",
            "title": "Composing Differential Privacy and Secure Computation: A Case Study on Scaling Private Record Linkage",
            "abstract": "Private record linkage (PRL) is the problem of identifying pairs of records that are similar as per an input matching rule from databases held by two parties that do not trust one another. We identify three key desiderata that a PRL solution must ensure: (1) perfect precision and high recall of matching pairs, (2) a proof of end-to-end privacy, and (3) communication and computational costs that scale subquadratically in the number of input records. We show that all of the existing solutions for PRL? including secure 2-party computation (S2PC), and their variants that use non-private or differentially private (DP) blocking to ensure subquadratic cost -- violate at least one of the three desiderata. In particular, S2PC techniques guarantee end-to-end privacy but have either low recall or quadratic cost. In contrast, no end-to-end privacy guarantee has been formalized for solutions that achieve subquadratic cost. This is true even for solutions that compose DP and S2PC: DP does not permit the release of any exact information about the databases, while S2PC algorithms for PRL allow the release of matching records. In light of this deficiency, we propose a novel privacy model, called output constrained differential privacy, that shares the strong privacy protection of DP, but allows for the truthful release of the output of a certain function applied to the data. We apply this to PRL, and show that protocols satisfying this privacy model permit the disclosure of the true matching records, but their execution is insensitive to the presence or absence of a single non-matching record. We find that prior work that combine DP and S2PC techniques even fail to satisfy this end-to-end privacy model. Hence, we develop novel protocols that provably achieve this end-to-end privacy guarantee, together with the other two desiderata of PRL. Our empirical evaluation also shows that our protocols obtain high recall, scale near linearly in the size of the input databases and the output set of matching pairs, and have communication and computational costs that are at least 2 orders of magnitude smaller than S2PC baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144080363",
                    "name": "Xi He"
                },
                {
                    "authorId": "2357165",
                    "name": "Ashwin Machanavajjhala"
                },
                {
                    "authorId": "24440615",
                    "name": "Cheryl J. Flynn"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "348238a981ed271f49d9e07899b8c7202d0cbee8",
            "title": "Reverse Engineering Aggregation Queries",
            "abstract": "Query reverse engineering seeks to re-generate the SQL query that produced a given query output table from a given database. In this paper, we solve this problem for OLAP queries with group-by and aggregation. We develop a novel three-phase algorithm named REGAL 1 for this problem. First, based on a lattice graph structure, we identify a set of group-by candidates for the desired query. Second, we apply a set of aggregation constraints that are derived from the properties of aggregate operators at both the table-level and the group-level to discover candidate combinations of group-by columns and aggregations that are consistent with the given query output table. Finally, we find a multi-dimensional filter, i.e., a conjunction of selection predicates over the base table attributes, that is needed to generate the exact query output table. We conduct an extensive experimental study over the TPC-H dataset to demonstrate the effectiveness and efficiency of our proposal.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "32457689",
                    "name": "W. Tan"
                },
                {
                    "authorId": "1793178",
                    "name": "Meihui Zhang"
                },
                {
                    "authorId": "1890500",
                    "name": "Hazem Elmeleegy"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "3ed5a6332a2c994d03e66d99aa05d786b93c2f67",
            "title": "Mind the Gaps (and Bumps): Statistical Smoothing of Spatiotemporal Streams",
            "abstract": "Spatiotemporal streams are prone to data quality issues such as missing, duplicated and delayed data when data generating sensors malfunction, data transmissions experience problems, or when data is stored or processed improperly. However, many important real-time applications rely on the continuous availability of stream values, e.g., to monitor traffic flow, resource usage, weather phenomena, etc. Other non real-time applications that support continuous or offline historical analytics also require high quality data to avoid producing misleading output such as false positives, erroneous conclusions and decisions. In this paper, we propose the use of nonparametric (data-driven, distribution free) statistical methods to provide an uninterrupted stream of high-quality spatiotemporal data to real-time applications even when the raw stream suffers data quality issues. Our novel family of robust methods makes use of the temporal stream of values in a spatial cell, and the deviation from normalcy in neighboring spatial cells to compute most likely values (MLVs) that could be used as proxies for data of questionable quality. The methods are efficient (incremental computations) and differ in how the deviations are normalized e.g., with respect to zeroth order, first order and second order moments. We use three real data sets to run a suite of experiments, and empirically demonstrate the superiority of the method that uses normalization with respect to variability.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115083267",
                    "name": "Philip E. Brown"
                },
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "1742591",
                    "name": "Y. Kanza"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "49705740b0792b6b3aa75d1330f2c283b3c83abc",
            "title": "Constrained Differential Privacy for Count Data",
            "abstract": "Concern about how to aggregate sensitive user data without compromising individual privacy is a major barrier to greater availability of data. The model of differential privacy has emerged as an accepted model to release sensitive information while giving a statistical guarantee for privacy. Many different algorithms are possible to address different target functions. We focus on the core problem of count queries, and seek to design mechanisms to release data associated with a group of n individuals. Prior work has focused on designing mechanisms by raw optimization of a loss function, without regard to the consequences on the results. This can leads to mechanisms with undesirable properties, such as never reporting some outputs (gaps), and overreporting others (spikes). We tame these pathological behaviors by introducing a set of desirable properties that mechanisms can obey. Any combination of these can be satisfied by solving a linear program (LP) which minimizes a cost function, with constraints enforcing the properties. We focus on a particular cost function, and provide explicit constructions that are optimal for certain combinations of properties, and show a closed form for their cost. In the end, there are only a handful of distinct optimal mechanisms to choose between: one is the well-known (truncated) geometric mechanism; the second a novel mechanism that we introduce here, and the remainder are found as the solution to particular LPs. These all avoid the bad behaviors we identify. We demonstrate in a set of experiments on real and synthetic data which is preferable in practice, for different combinations of data distributions, constraints, and privacy parameters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1954876",
                    "name": "Tejas D. Kulkarni"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "4aa16fd5977f2b575c7cfffd825b5eb19a191537",
            "title": "Scaling Private Record Linkage using Output Constrained Differential Privacy",
            "abstract": "Private record linkage is the problem of identifying pairs of records that are similar as per an input matching rule from databases that are held by two parties that do not trust one another. We identify three key desiderata that a PRL solution must ensure: a proof of end-to-end privacy, communication and computational costs that scale sub-quadratically in the number of input records, perfect precision and high recall of matching pairs. We show that all of the existing solutions for PRL -- including secure 2-party computation (S2PC), and their variants that use non-private or differentially private (DP) blocking -- violate at least one of the three desiderata. In particular, S2PC techniques guarantee end-to-end privacy but have either low recall or high cost. We show that DP blocking based techniques do not provide an end-to-end privacy guarantee as DP does not permit the release of any exact answers (including matching records in PRL). \nIn light of this deficiency, we propose a novel privacy model, called output constrained differential privacy, that shares the strong privacy protection of DP, but allows for the truthful release of the output of a certain function applied to the data. We apply this to PRL, and show that protocols satisfying this privacy model permit the disclosure of the true matching records, but their execution is insensitive to the presence or absence of a single non-matching record. We develop novel protocols that satisfy this end-to-end privacy guarantee and permit a tradeoff between recall, privacy and efficiency. Our empirical evaluations on real and synthetic datasets show that our protocols have high recall, scale near linearly in the size of the input databases (and the output set of matching pairs), and have communication and computational costs that are at least 2 orders of magnitude smaller than S2PC baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144080363",
                    "name": "Xi He"
                },
                {
                    "authorId": "2357165",
                    "name": "Ashwin Machanavajjhala"
                },
                {
                    "authorId": "24440615",
                    "name": "Cheryl J. Flynn"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "4f0919c9b0d609f3928534a0b7eb92d38000108b",
            "title": "Enabling Change Exploration: Vision Paper",
            "abstract": "Data and metadata suffer many different kinds of change: values are inserted, deleted or updated; entities appear and disappear; properties are added or re-purposed, etc. Explicitly recognizing, exploring, and evaluating such change can alert to changes in data ingestion procedures, can help assess data quality, and can improve the general understanding of the dataset and its behavior over time. We propose a data model-independent framework to formalize such change. Our change-cube enables exploration and discovery of such changes to reveal dataset behavior over time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7870886",
                    "name": "Tobias Bleifu\u00df"
                },
                {
                    "authorId": "145633895",
                    "name": "T. Johnson"
                },
                {
                    "authorId": "1818681",
                    "name": "D. Kalashnikov"
                },
                {
                    "authorId": "1683688",
                    "name": "Felix Naumann"
                },
                {
                    "authorId": "1745049",
                    "name": "Vladislav Shkapenyuk"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "64710a4fec7f6c05ab5653464c8793f055fbebdd",
            "title": "A Tool for Statistical Analysis on Network Big Data",
            "abstract": "Due to advances in parallel file systems for big data (i.e. HDFS) and larger capacity hardware (multicore CPUs, large RAM) it is now feasible to manage and query network data in a parallel DBMS supporting SQL, but performing statistical analysis remains a challenge.On the statistics side, the R language is popular, but it presents important limitations: R is limited by main memory, R works in a different address space from query processing, R cannot analyze large disk-resident data sets efficiently, and R has no data management capabilities. Moreover, some R libraries allow R to work in parallel, but without data management capabilities. Considering the challenges and limitations described above, we present a system that allows combining SQL queries and R functions in a seamless manner. We justify a parallel DBMS and the R runtime are two different systems that benefit from a low-level integration. Our parallel DBMS is built on top of HDFS, programmed in Java and C++, with a flexible scale out architecture, whereas R is programmed purely in C. The user or developer can make calls in both directions: (1) R calling SQL, to evaluate analytic queries or retrieve data from materialized views (transferring result tables in RAM in a streaming fashion and analyzing them in R), and vice-versa (2) SQL calling R, allowing SQL to convert relational tables to matrices or vectors and making complex computations on them. We give a summary of network monitoring tasks at ATT and present specific programming examples, showing language calls in both directions (i.e. R calls SQL, SQL calls R).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145050110",
                    "name": "C. Ordonez"
                },
                {
                    "authorId": "145633895",
                    "name": "T. Johnson"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1964142",
                    "name": "Simon Urbanek"
                }
            ]
        },
        {
            "paperId": "88f678623bb3182bcdd2259bffd28ddd8f481b1c",
            "title": "\"Tell me more\" using Ladders in Wikipedia",
            "abstract": "We focus on the problem of \"tell me more\" information related to a given fact in Wikipedia. We use the novel notion of role to link information in an infobox with different places in the text of the same Wikipedia page (space) as well as information across different revisions of the page (time). In this way, it is possible to link together pieces of information that may not represent the same real world entity, yet have served in the same role. To achieve this, we introduce a novel structure called ladder that allows such spatial and temporal linking and we show how to effectively and efficiently construct such structures from Wikipedia data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2147072",
                    "name": "Siarhei Bykau"
                },
                {
                    "authorId": "2108586131",
                    "name": "Jihwan Lee"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "b5483e90086e6bb0c3aa5ee0300dad5c76da7cd3",
            "title": "Geotagging IP Packets for Location-Aware Software-Defined Networking in the Presence of Virtual Network Functions",
            "abstract": "A substantial portion of global telecommunication is based on the Internet Protocol (IP), where IP packets are routed from a source host to a destination host via a communication network. While there is some loose connection between IP addresses and geospatial locations, associating packets to geographic coordinates merely according to IP addresses is hard, and often infeasible in real-time, given the rapidity and prodigious volume of packet traffic via routers and switches. This obstructs using geospatial information about the origin, destination, or route of IP packets or flows. In this paper we introduce a vision of adding geotags to IP packets, to enhance the capabilities of communication networks and of location-based services. We explain how the augmentation can be done flexibly and effectively using two new networking technologies: (1) software defined networking (SDN)---a new architecture that facilitates the ability to control network flows, and (2) network function virtualization (NFV) which allows deploying virtual network services, and chaining such services to one another. We describe new applications that can be built using the enrichment of packets with spatial or temporal properties, including applications related to network security, geofencing and operations support systems. We discuss challenges and research directions in this domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "1742591",
                    "name": "Y. Kanza"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "b9235fd5c62343046631761b81714f7e570ac822",
            "title": "PrivBayes",
            "abstract": "Privacy-preserving data publishing is an important problem that has been the focus of extensive study. The state-of-the-art solution for this problem is differential privacy, which offers a strong degree of privacy protection without making restrictive assumptions about the adversary. Existing techniques using differential privacy, however, cannot effectively handle the publication of high-dimensional data. In particular, when the input dataset contains a large number of attributes, existing methods require injecting a prohibitive amount of noise compared to the signal in the data, which renders the published data next to useless. To address the deficiency of the existing methods, this paper presents PrivBayes, a differentially private method for releasing high-dimensional data. Given a dataset D, PrivBayes first constructs a Bayesian network N, which (i) provides a succinct model of the correlations among the attributes in D and (ii) allows us to approximate the distribution of data in D using a set P of low-dimensional marginals of D. After that, PrivBayes injects noise into each marginal in P to ensure differential privacy and then uses the noisy marginals and the Bayesian network to construct an approximation of the data distribution in D. Finally, PrivBayes samples tuples from the approximate distribution to construct a synthetic dataset, and then releases the synthetic data. Intuitively, PrivBayes circumvents the curse of dimensionality, as it injects noise into the low-dimensional marginals in P instead of the high-dimensional dataset D. Private construction of Bayesian networks turns out to be significantly challenging, and we introduce a novel approach that uses a surrogate function for mutual information to build the model more accurately. We experimentally evaluate PrivBayes on real data and demonstrate that it significantly outperforms existing solutions in terms of accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155659897",
                    "name": "Jun Zhang"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                }
            ]
        },
        {
            "paperId": "c904db06ccd8288157513e3e4c75b6692a16740f",
            "title": "Marginal Release Under Local Differential Privacy",
            "abstract": "Many analysis and machine learning tasks require the availability of marginal statistics on multidimensional datasets while providing strong privacy guarantees for the data subjects. Applications for these statistics range from finding correlations in the data to fitting sophisticated prediction models. In this paper, we provide a set of algorithms for materializing marginal statistics under the strong model of local differential privacy. We prove the first tight theoretical bounds on the accuracy of marginals compiled under each approach, perform empirical evaluation to confirm these bounds, and evaluate them for tasks such as modeling and correlation testing. Our results show that releasing information based on (local) Fourier transformations of the input is preferable to alternatives based directly on (local) marginals.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1954876",
                    "name": "Tejas D. Kulkarni"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "d907fb62fa079075402b7d7e68afdb4872c25e90",
            "title": "Repairing Noisy Graphs",
            "abstract": "Graphs are a flexible way to represent data in a variety of applications, with nodes representing domain-specific entities (e.g., records in record linkage, products and types in an ontology) and edges capturing a variety of relationships between these entities (e.g., an equivalence relationship between records in record linkage, a type-subtype relationship between types in an ontology). Often, the edges in this graph are noisy, in that some edges are missing (i.e., real-world relationships that do not have corresponding edges in the graph) and some edges are spurious (i.e., edges in the graph that do not have corresponding real-world relationships). Directly analyzing noisy graphs can lead to undesirable outcomes, making it important to repair noisy graphs. In this talk, we describe an approach that takes advantage of properties of real-world relationships and their estimated probabilities to ask oracle queries (an abstraction of crowdsourcing) to efficiently repair the noisy graphs. We illustrate this approach for the case of graphs that are unions of cliques (which is the case for record linkage) and graphs that are trees (which is the case for ontologies), and present theoretical and empirical results for these cases. This is joint work with Donatella Firmani, Sainyam Galhotra and Barna Saha.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "edcdc1382deff03c458d5defb2035439f85db0bd",
            "title": "Scalable Informative Rule Mining",
            "abstract": "We present SIRUM: a system for Scalable Informative RUle Mining from multi-dimensional data. Informative rules have recently been studied in several contexts, including data summarization, data cube exploration and data quality. The objective is to produce a small set of rules (patterns) over the values of the dimension attributes that provide the most information about the distribution of a numeric measure attribute. Within SIRUM, we propose several optimizations for tall, wide and distributed datasets. We implemented SIRUM in Spark and observed significant performance and scalability improvements on real datasets due to our optimizations. As a result, SIRUM is able to generate informative rules on much wider and taller datasets than using distributed implementations of the previous state of the art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3270313",
                    "name": "Guoyao Feng"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "1257d0005568e7f206ba4e1a1bbbcc99b64ac75d",
            "title": "Letter from the Special Issue Editors",
            "abstract": "The prevalence of large volumes and varieties of accessible data is profoundly changing the way business, government and individuals approach decision making. Organizational big data investment strategies regarding what data to collect, clean, integrate, and analyze are typically driven by some notion of perceived value. However, the value of the data is inescapably tied to the underlying quality of the data. Although for big data, value and quality may be correlated, they are conceptually different. For example, a complete and accurate list of the books read on April 1, 2016 by the special editors of this issue may not have much value to anyone else. Whereas even partially complete and somewhat noisy GPS data from public transport vehicles may have a high perceived value for transport engineers and urban planners. In spite of significant advances in storage and compute capabilities, the time to value in big data projects often remains unacceptable due to the quality of the underlying data. Poor data quality is being termed as the dark side of big data, inhibiting the effective use of data to discover trusted insights and foresights. Finding the nexus of use and quality is a multifaceted problem encompassing organizational and computational challenges. These challenges are often specific to the type of data (e.g. structured/relational, text, spatial, time series, social/graph, multimedia, RDF/web), the dimension of data quality (e.g. completeness, consistency, timeliness), and the preparatory processes (e.g. data acquisition, profiling, curation, integration) that precede the actual use of the data. Designing a practical strategy for tackling quality issues in big data requires data scientists to bring together these multiple aspects of data type, quality dimension and process within the context of their application setting. In this special issue we have endeavoured to present recent research of some of the leading experts in the field of data quality with the aim of informing the design of such practical strategies. Out of the eight papers, four are on relational/structured data while the remaining four are on time series data, spatio-temporal data, micro-blog data and web data. The papers have targeted a number of data quality dimensions through a range of innovative approaches as outlined below. The first two papers tackle data quality dimensions of meta-data compliance and schema quality. Sebastian Kruse, Thorsten Papenbrock, Hazar Harmouch, and Felix Naumann present data anamnesis as a means of meta-data discovery with an aim to assess the quality and utility of the underlying relational datasets. In the second paper, Henning Kohler, Sebastian Link and Xiaofang Zhou present a method for discovering meaningful certain keys, in the presence of incomplete and inconsistent data with an aim to tackle redundancy and maintain the integrity constraints of the underlying relational data. The next two papers discuss data cleaning in the context of associated data transformation and curation activities. These works are instrumental in evaluating the effectiveness of data cleaning algorithms. A number of data quality dimensions are covered by these papers including value, format and semantic consistency, and business rule compliance. The paper by Ihab Ilyas proposes a decoupling between detecting data errors and the repairing of these errors within a continuous data cleaning life-cycle with humans in the loop. The paper by Patricia C. Arocena, Boris Glavic, Giansalvatore Mecca, Renee J. Miller, Paolo Papotti and Donatello Santoro, outlines the challenges and solutions for benchmarking data curation systems. Spatio-temporal and time-series data are known to suffer from a variety of data quality problems, due to the correlated nature of the data. The paper by Juliana Freire, Aline Bessa, Fernando Chirigati, Huy Vo and Kai Zhao uses exploratory techniques and powerful visualizations to differentiate between error and feature in spatiotemporal data. Tamraparni Dasu, Rong Duan, and Divesh Srivastava in their paper on data quality for temporal streams use statistical distortion as a means of measuring data quality in a near-real time fashion. The papers address a range of data quality dimensions including incompleteness, redundancy, inaccuracy (e.g. GPS noise), format consistency, dependency constraint violation, uniqueness issues and handling duplicates. The final two papers target the elusive data quality dimensions of trustworthiness and understandability. The paper by Wen Hua, Kai Zheng and Xiaofang Zhou focuses on improving the understandability of short-text as found in microblogs towards resolving entity ambiguity. The paper by Xin Luna Dong, Evgeniy Gabrilovich,",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2092368",
                    "name": "S. Sadiq"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "1279d195e7f6a6a502a590de8e6d70c494dc9303",
            "title": "Effective and Complete Discovery of Order Dependencies via Set-based Axiomatization",
            "abstract": "Integrity constraints (ICs) provide a valuable tool for expressing and enforcing application semantics. However, formulating constraints manually requires domain expertise, is prone to human errors, and may be excessively time consuming, especially on large datasets. Hence, proposals for automatic discovery have been made for some classes of ICs, such as functional dependencies (FDs), and recently, order dependencies (ODs). ODs properly subsume FDs, as they can additionally express business rules involving order; e.g., an employee never has a higher salary while paying lower taxes compared with another employee. \nWe address the limitations of prior work on OD discovery which has factorial complexity in the number of attributes, is incomplete (i.e., it does not discover valid ODs that cannot be inferred from the ones found) and is not concise (i.e., it can result in \"redundant\" discovery and overly large discovery sets). We improve significantly on complexity, offer completeness, and define a compact canonical form. This is based on a novel polynomial mapping to a canonical form for ODs, and a sound and complete set of axioms (inference rules) for canonical ODs. This allows us to develop an efficient set-containment, lattice-driven OD discovery algorithm that uses the inference rules to prune the search space. Our algorithm has exponential worst-case time complexity in the number of attributes and linear complexity in the number of tuples. We prove that it produces a complete, minimal set of ODs (i.e., minimal with regards to the canonical representation). Finally, using real and synthetic datasets, we experimentally show orders-of-magnitude performance improvements over the current state-of-the-art algorithm and demonstrate effectiveness of our techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                },
                {
                    "authorId": "144024329",
                    "name": "P. Godfrey"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "36626057",
                    "name": "M. Kargar"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "1a00eb4d0dc5a67bf152185e47f8a7e5d3ab636a",
            "title": "Data Quality for Temporal Streams",
            "abstract": "Temporal data pose unique data quality challenges due to the presence of autocorrelations, trends, seasonality, and gaps in the data. Data streams are a special case of temporal data where velocity, volume and variety present additional layers of complexity in measuring the veracity of the data. In this paper, we discuss a general, widely applicable framework for data quality measurement of streams in a dynamic environment that takes into account the evolving nature of streams. We classify data quality anomalies using four types of constraints, identify violations that could be potential data glitches, and use statistical distortion as a metric for measuring data quality in a near real-time fashion. We illustrate our framework using commercially available streams of NYSE stock prices consisting of aggregates of prices and trading volumes collected every minute over a one year period from November 2011 to November 2012.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "144487977",
                    "name": "Rong Duan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "1f5ae98af7f36bc468376a0b108109ae8251ef12",
            "title": "SourceSight: Enabling Effective Source Selection",
            "abstract": "Recently there has been a rapid increase in the number of data sources and data services, such as cloud-based data markets and data portals, that facilitate the collection, publishing and trading of data. Data sources typically exhibit large heterogeneity in the type and quality of data they provide. Unfortunately, when the number of data sources is large, it is difficult for users to reason about the actual usefulness of sources for their applications and the trade-offs between the benefits and costs of acquiring and integrating sources. In this demonstration we present \\textsc{SourceSight}, a system that allows users to interactively explore a large number of heterogeneous data sources, and discover valuable sets of sources for diverse integration tasks. \\textsc{SourceSight}~uses a novel multi-level source quality index that enables effective source selection at different granularity levels, and introduces a collection of new techniques to discover and evaluate relevant sources for integration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145071799",
                    "name": "Theodoros Rekatsinas"
                },
                {
                    "authorId": "144520191",
                    "name": "A. Deshpande"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1746034",
                    "name": "L. Getoor"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "342d243f0bc4bb3465dba7767b2f4eb22adfd875",
            "title": "Online Event Integration with StoryPivot",
            "abstract": "Modern data integration systems need to process large amounts of data from a variety of data sources and with real-time integration constraints. They are not only employed in enterprises for managing internal data but are also used for a variety of web services that use techniques such as entity resolution or data cleaning in live systems. In this work, we discuss a new generation of data integration systems that operate on (un-)structured data in an online setting, i.e., systems which process continuously modified datasets upon which the integration task is based. We use as an example of such a system an online event integration system called StoryPivot. It observes events extracted from news articles in data sources such as the 'Guardian' or the 'Washington Post' which are integrated to show users the evolution of real-world stories over time. The design decisions for StoryPivot are influenced by the trade-off between maintaining high quality integration results while at the same time building a system that processes and integrates events in near real-time. We evaluate our design decisions with experiments on two real-world datasets and generalize our findings to other data integration tasks that have a similar system setup.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3202556",
                    "name": "Anja Gruenheid"
                },
                {
                    "authorId": "1691108",
                    "name": "Donald Kossmann"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "8717e86b0b12010f51a6b939cf070989355802ee",
            "title": "Online Entity Resolution Using an Oracle",
            "abstract": "Entity resolution (ER) is the task of identifying all records in a database that refer to the same underlying entity. This is an expensive task, and can take a significant amount of money and time; the end-user may want to take decisions during the process, rather than waiting for the task to be completed. We formalize an online version of the entity resolution task, and use an oracle which correctly labels matching and non-matching pairs through queries. In this setting, we design algorithms that seek to maximize progressive recall, and develop a novel analysis framework for prior proposals on entity resolution with an oracle, beyond their worst case guarantees. Finally, we provide both theoretical and experimental analysis of the proposed algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1728335",
                    "name": "D. Firmani"
                },
                {
                    "authorId": "3269130",
                    "name": "B. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "93d13333a2046c658f6cc9f9a77490e0249977fa",
            "title": "Indexing Evolving Events from Tweet Streams",
            "abstract": "Tweet streams provide a variety of real-life and real-time information on social events that dynamically change over time. Although social event detection has been actively studied, how to efficiently monitor evolving events from continuous tweet streams remains open and challenging. One common approach for event detection from text streams is to use single-pass incremental clustering. However, this approach does not track the evolution of events, nor does it address the issue of efficient monitoring in the presence of a large number of events. In this paper, we capture the dynamics of events using four event operations (create, absorb, split, and merge), which can be effectively used to monitor evolving events. Moreover, we propose a novel event indexing structure, called Multi-layer Inverted List (MIL), to manage dynamic event databases for the acceleration of large-scale event search and update. We thoroughly study the problem of nearest neighbour search using MIL based on upper bound pruning, along with incremental index maintenance. Extensive experiments have been conducted on a large-scale real-life tweet dataset. The results demonstrate the promising performance of our event indexing and monitoring methods on both efficiency and effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143754377",
                    "name": "Hongyun Cai"
                },
                {
                    "authorId": "145622169",
                    "name": "Zi Huang"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2112724648",
                    "name": "Qing Zhang"
                }
            ]
        },
        {
            "paperId": "fbe13ea111dc27faac21d7e09f667e163d633315",
            "title": "Estimating Quantiles from the Union of Historical and Streaming Data",
            "abstract": "Modern enterprises generate huge amounts of streaming data, for example, micro-blog feeds, financial data, network monitoring and industrial application monitoring. While Data Stream Management Systems have proven successful in providing support for real-time alerting, many applications, such as network monitoring for intrusion detection and real-time bidding, require complex analytics over historical and real-time data over the data streams. We present a new method to process one of the most fundamental analytical primitives, quantile queries, on the union of historical and streaming data. Our method combines an index on historical data with a memory-efficient sketch on streaming data to answer quantile queries with accuracy-resource tradeoffs that are significantly better than current solutions that are based solely on disk-resident indexes or solely on streaming algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108332714",
                    "name": "S. Singh"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1679836",
                    "name": "Srikanta Tirthapura"
                }
            ]
        },
        {
            "paperId": "0e47b03fefcdc09185927aa4884009cf28081192",
            "title": "StoryPivot: Comparing and Contrasting Story Evolution",
            "abstract": "As the world evolves around us, so does the digital coverage of it. Events of diverse types, associated with different actors and various locations, are continuously captured by multiple information sources such as news articles, blogs, social media etc. day by day. In the digital world, these events are represented through information snippets that contain information on the involved entities, a description of the event, when the event occurred, etc. In our work, we observe that events (and their corresponding digital representations) are often inter-connected, i.e., they form stories which represent evolving relationships between events over time. Take as an example the plane crash in Ukraine in July 2014 which involved multiple entities such as \"Ukraine\", \"Malaysia\", and \"Russia\" and multiple events ranging from the actual crash to the incident investigation and the presentation of the investigator's findings. In this demonstration we present StoryPivot, a framework that helps its users to detect evolving stories in event datasets over time. To resolve stories, we differentiate between story identification, the problem of connecting events over time within a source, and story alignment, the problem of integrating stories across sources. The goal of this demonstration is to present an interactive exploration of both these problems and how events can be dynamically interpreted and put into context in real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3202556",
                    "name": "Anja Gruenheid"
                },
                {
                    "authorId": "1691108",
                    "name": "Donald Kossmann"
                },
                {
                    "authorId": "145071799",
                    "name": "Theodoros Rekatsinas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "0edc885ec324df7d29fd781cfa5b40bdf60cf768",
            "title": "Size-Constrained Weighted Set Cover",
            "abstract": "In this paper, we introduce a natural generalization of Weighted Set Cover and Maximum Coverage, called Size-Constrained Weighted Set Cover. The input is a collection of n elements, a collection of weighted sets over the elements, a size constraint k, and a minimum coverage fraction s\u0302; the output is a sub-collection of up to k sets whose union contains at least s\u0302n elements and whose sum of weights is minimal. We prove the hardness of approximation of this problem, and we present efficient approximation algorithms with provable quality guarantees that are the best possible. In many applications, the elements are data records, and the set collection to choose from is derived from combinations (patterns) of attribute values. We provide optimization techniques for this special case. Finally, we experimentally demonstrate the effectiveness and efficiency of our solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "2146311457",
                    "name": "Feng Li"
                },
                {
                    "authorId": "3269130",
                    "name": "B. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "145be5e5f22e446e2592212fe2b1d1b237a595ee",
            "title": "FIT to Monitor Feed Quality",
            "abstract": "While there has been significant focus on collecting and managing data feeds, it is only now that attention is turning to their quality. In this paper, we propose a principled approach to online data quality monitoring in a dynamic feed environment. Our goal is to alert quickly when feed behavior deviates from expectations. \n \nWe make contributions in two distinct directions. First, we propose novel enhancements to permit a publish-subscribe approach to incorporate data quality modules into the DFMS architecture. Second, we propose novel temporal extensions to standard statistical techniques to adapt them to online feed monitoring for outlier detection and alert generation at multiple scales along three dimensions: aggregation at multiple time intervals to detect at varying levels of sensitivity; multiple lengths of data history for varying the speed at which models adapt to change; and multiple levels of monitoring delay to address lagged data arrival. \n \nFIT, or Feed Inspection Tool, is the result of a successful implementation of our approach. We present several case studies outlining the effective deployment of FIT in real applications along with user testimonials.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "1745049",
                    "name": "Vladislav Shkapenyuk"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "32708141",
                    "name": "Deborah F. Swayne"
                }
            ]
        },
        {
            "paperId": "2b058f0abddfe638559c07222b435576f331af4a",
            "title": "Indexing evolving events from tweet streams",
            "abstract": "Tweet streams provide a variety of real-time information on dynamic social events. Although event detection has been actively studied, most of the existing approaches do not address the issue of efficient event monitoring in the presence of a large number of events detected from continuous tweet streams. In this paper, we capture the dynamics of events using four event operations: creation, absorption, split and merge.We also propose a novel event indexing structure, named Multi-layer Inverted List (MIL), for the acceleration of large-scale event search and update. We thoroughly study the problem of nearest neighbour search using MIL based on upper bound pruning. Extensive experiments have been conducted on a large-scale tweet dataset. The results demonstrate the promising performance of our method in terms of both efficiency and effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143754377",
                    "name": "Hongyun Cai"
                },
                {
                    "authorId": "145622169",
                    "name": "Zi Huang"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2145803224",
                    "name": "Qing Zhang"
                }
            ]
        },
        {
            "paperId": "31d41baf65e8608d89579a75c2ecfb92367d0f8e",
            "title": "DEXTER: Large-Scale Discovery and Extraction of Product Specifications on the Web",
            "abstract": "The web is a rich resource of structured data. There has been an increasing interest in using web structured data for many applications such as data integration, web search and question answering. In this paper, we present Dexter, a system to find product sites on the web, and detect and extract product specifications from them. Since product specifications exist in multiple product sites, our focused crawler relies on search queries and backlinks to discover product sites. To perform the detection, and handle the high diversity of specifications in terms of content, size and format, our system uses supervised learning to classify HTML fragments (e.g., tables and lists) present in web pages as specifications or not. To perform large-scale extraction of the attribute-value pairs from the HTML fragments identified by the specification detector, Dexter adopts two lightweight strategies: a domain-independent and unsupervised wrapper method, which relies on the observation that these HTML fragments have very similar structure; and a combination of this strategy with a previous approach, which infers extraction patterns by annotations generated by automatic but noisy annotators. The results show that our crawler strategy to locate product specification pages is effective: (1) it discovered 1:46AM product specification pages from 3; 005 sites and 9 different categories; (2) the specification detector obtains high values of F-measure (close to 0:9) over a heterogeneous set of product specifications; and (3) our efficient wrapper methods for attribute-value extraction get very high values of precision (0.92) and recall (0.95) and obtain better results than a state-of-the-art, supervised rule-based wrapper.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1756080",
                    "name": "Disheng Qiu"
                },
                {
                    "authorId": "2056435855",
                    "name": "Luciano Barbosa"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2115435801",
                    "name": "Yanyan Shen"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "44d47e86fc7fba569369cb7691bbdbd05589886d",
            "title": "On Axiomatization and Inference Complexity over a Hierarchy of Functional Dependencies",
            "abstract": "Functional dependencies (FDs) have recently been extended for data quality purposes with various notions of similarity instead of strict equality. We study these extensions in this paper. We begin by constructing a hierarchy of dependencies, showing which dependencies generalize others. We then focus on an extension of FDs that we call Antecedent Metric Functional Dependencies (AMFDs). An AMFD asserts that if two tuples have similar but not necessarily equal values of the antecedent attributes, then their consequent values must be equal. We present a theoretical foundation for AMFDs, including a sound and complete axiomatization as well as an inference algorithm. We compare the axiomatization of AMFDs to those of the other dependencies, and we show that while the complexity of inference for some FD extensions is quadratic or even co-NP complete, the inference problem for AMFDs remains linear, as in traditional FDs. We implemented our inference procedure and experimentally verified its efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "519560a7db88fce618c6eb83503064ddf17752a2",
            "title": "Identifying the Extent of Completeness of Query Answers over Partially Complete Databases",
            "abstract": "In many applications including loosely coupled cloud databases, collaborative editing and network monitoring, data from multiple sources is regularly used for query answering. For reasons such as system failures, insufficient author knowledge or network issues, data may be temporarily unavailable or generally nonexistent. Hence, not all data needed for query answering may be available. In this paper, we propose a natural class of completeness patterns, expressed by selections on database tables, to specify complete parts of database tables. We then show how to adapt the operators of relational algebra so that they manipulate these completeness patterns to compute completeness patterns pertaining to query answers. Our proposed algebra is computationally sound and complete with respect to the information that the patterns provide. We show that stronger completeness patterns can be obtained by considering not only the schema but also the database instance and we extend the algebra to take into account this additional information. We develop novel techniques to efficiently implement the computation of completeness patterns on query answers and demonstrate their scalability on real data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2499758",
                    "name": "Simon Razniewski"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "1806700",
                    "name": "W. Nutt"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "54cfaf91ad54026dc6fc5e0c36a0b08752360047",
            "title": "TreeScope: Finding Structural Anomalies In Semi-Structured Data",
            "abstract": "Semi-structured data are prevalent on the web, with formats such as XML and JSON soaring in popularity due to their generality, flexibility and easy customization. However, these very same features make semi-structured data prone to a range of data quality errors, from errors in content to errors in structure. While the former has been well studied, little attention has been paid to structural errors. \n \nIn this demonstration, we present TreeScope, which analyzes semi-structured data sets with the goal of automatically identifying structural anomalies from the data. Our techniques learn robust structural models that have high support, to identify potential errors in the structure. Identified structural anomalies are then concisely summarized to provide plausible explanations of the potential errors. The goal of this demonstration is to enable an interactive exploration of the process of identifying and summarizing structural anomalies in semi-structured data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1883706",
                    "name": "Shanshan Ying"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "3269130",
                    "name": "B. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "62eae99e73546df3a1a78d30e853f3e757fd9fe8",
            "title": "DPT: Differentially Private Trajectory Synthesis Using Hierarchical Reference Systems",
            "abstract": "GPS-enabled devices are now ubiquitous, from airplanes and cars to smartphones and wearable technology. This has resulted in a wealth of data about the movements of individuals and populations, which can be analyzed for useful information to aid in city and traffic planning, disaster preparedness and so on. However, the places that people go can disclose extremely sensitive information about them, and thus their use needs to be filtered through privacy preserving mechanisms. This turns out to be a highly challenging task: raw trajectories are highly detailed, and typically no pair is alike. Previous attempts fail either to provide adequate privacy protection, or to remain sufficiently faithful to the original behavior. \n \nThis paper presents DPT, a system to synthesize mobility data based on raw GPS trajectories of individuals while ensuring strong privacy protection in the form of e-differential privacy. DPT makes a number of novel modeling and algorithmic contributions including (i) discretization of raw trajectories using hierarchical reference systems (at multiple resolutions) to capture individual movements at differing speeds, (ii) adaptive mechanisms to select a small set of reference systems and construct prefix tree counts privately, and (iii) use of direction-weighted sampling for improved utility. While there have been prior attempts to solve the subproblems required to generate synthetic trajectories, to the best of our knowledge, ours is the first system that provides an end-to-end solution. We show the efficacy of our synthetic trajectory generation system using an extensive empirical evaluation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144080363",
                    "name": "Xi He"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2357165",
                    "name": "Ashwin Machanavajjhala"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "7b49c59b2056308b01dcc495966d974e7c7741fc",
            "title": "The elephant in the room: getting value from Big Data",
            "abstract": "Big Data, and its 4 Vs \u2013 volume, velocity, variety, and veracity \u2013 have been at the forefront of societal, scientific and engineering discourse. Arguably the most important 5th V, value, is not talked about as much. How can we make sure that our data is not just big, but also valuable? WebDB 2015 has as its theme \u201cFreshness, Correctness, Quality of Information and Knowledge on the Web\u201d. The workshop attracted 31 submissions, of which the best 9 were selected for presentation at the workshop, and for publication in the proceedings. To set the stage, we have interviewed several prominent members of the data management community, soliciting their opinions on how we can ensure that data is not just available in quantity, but also in quality. In this interview Serge Abiteboul, Oren Etzioni, Divesh Srivastava with Luna Dong, and Gerhard Weikum shared with us their motivation for doing research in the area of data quality, and discussed their current work and their view on the future of the field. This interview appeared as a SIGMOD Blog article.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "69026873",
                    "name": "S. Abiteboul"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1741101",
                    "name": "Oren Etzioni"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1751591",
                    "name": "G. Weikum"
                },
                {
                    "authorId": "1682824",
                    "name": "Julia Stoyanovich"
                },
                {
                    "authorId": "1679784",
                    "name": "Fabian M. Suchanek"
                }
            ]
        },
        {
            "paperId": "8b694bb50071607fb068234d6e08a71706549344",
            "title": "Robust Group Linkage",
            "abstract": "We study the problem of group linkage: linking records that refer to multiple entities in the same group. Applications for group linkage include finding businesses in the same chain, finding social network users from the same organization, and so on. Group linkage faces new challenges compared to traditional entity resolution. First, although different members in the same group can share some similar global values of an attribute, they represent different entities so can also have distinct local values for the same or different attributes, requiring a high tolerance for value diversity. Second, we need to be able to distinguish local values from erroneous values. We present a robust two-stage algorithm: the first stage identifies pivots--maximal sets of records that are very likely to belong to the same group, while being robust to possible erroneous values; the second stage collects strong evidence from the pivots and leverages it for merging more records into the same group, while being tolerant to differences in local values of an attribute. Experimental results show the high effectiveness and efficiency of our algorithm on various real-world data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112726639",
                    "name": "Pei Li"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2582204",
                    "name": "Songtao Guo"
                },
                {
                    "authorId": "1743266",
                    "name": "A. Maurino"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "8e832cc517b0b35a0e213a34e8d46a7309bb5e9e",
            "title": "Combining Quantitative and Logical Data Cleaning",
            "abstract": "Quantitative data cleaning relies on the use of statistical methods to identify and repair data quality problems while logical data cleaning tackles the same problems using various forms of logical reasoning over declarative dependencies. Each of these approaches has its strengths: the logical approach is able to capture subtle data quality problems using sophisticated dependencies, while the quantitative approach excels at ensuring that the repaired data has desired statistical properties. We propose a novel framework within which these two approaches can be used synergistically to combine their respective strengths.\n We instantiate our framework using (i) metric functional dependencies, a type of dependency that generalizes functional dependencies (FDs) to identify inconsistencies in domains where only large differences in metric data are considered to be a data quality problem, and (ii) repairs that modify the inconsistent data so as to minimize statistical distortion, measured using the Earth Mover's Distance. We show that the problem of computing a statistical distortion minimal repair is NP-hard. Given this complexity, we present an efficient algorithm for finding a minimal repair that has a small statistical distortion using EMD computation over semantically related attributes. To identify semantically related attributes, we present a sound and complete axiomatization and an efficient algorithm for testing implication of metric FDs. While the complexity of inference for some other FD extensions is co-NP complete, we show that the inference problem for metric FDs remains linear, as in traditional FDs. We prove that every instance that can be generated by our repair algorithm is set-minimal (with no unnecessary changes). Our experimental evaluation demonstrates that our techniques obtain a considerably lower statistical distortion than existing repair techniques, while achieving similar levels of efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1999516",
                    "name": "Nataliya Prokoshyna"
                },
                {
                    "authorId": "1806938",
                    "name": "Jaroslaw Szlichta"
                },
                {
                    "authorId": "39311225",
                    "name": "Fei Chiang"
                },
                {
                    "authorId": "2110417078",
                    "name": "Ren\u00e9e J. Miller"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "8fe216ae48ca2b7ef0cebf27a156abf57e45b7f2",
            "title": "Private Release of Graph Statistics using Ladder Functions",
            "abstract": "Protecting the privacy of individuals in graph structured data while making accurate versions of the data available is one of the most challenging problems in data privacy. Most efforts to date to perform this data release end up mired in complexity, overwhelm the signal with noise, and are not effective for use in practice. In this paper, we introduce a new method which guarantees differential privacy. It specifies a probability distribution over possible outputs that is carefully defined to maximize the utility for the given input, while still providing the required privacy level. The distribution is designed to form a 'ladder', so that each output achieves the highest 'rung' (maximum probability) compared to less preferable outputs. We show how our ladder framework can be applied to problems of counting the number of occurrences of subgraphs, a vital objective in graph analysis, and give algorithms whose cost is comparable to that of computing the count exactly. Our experimental study confirms that our method outperforms existing methods for counting triangles and stars in terms of accuracy, and provides solutions for some problems for which no effective method was previously known. The results of our algorithms can be used to estimate the parameters of suitable graph models, allowing synthetic graphs to be sampled.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155659897",
                    "name": "Jun Zhang"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                }
            ]
        },
        {
            "paperId": "a46dd28c5c848b6805a5d66dfa6b02b287170303",
            "title": "Knowledge Curation and Knowledge Fusion: Challenges, Models and Applications",
            "abstract": "Large-scale knowledge repositories are becoming increasingly important as a foundation for enabling a wide variety of complex applications. In turn, building high-quality knowledge repositories critically depends on the technologies of knowledge curation and knowledge fusion, which share many similar goals with data integration, while facing even more challenges in extracting knowledge from both structured and unstructured data, across a large variety of domains, and in multiple languages. Our tutorial highlights the similarities and differences between knowledge management and data integration, and has two goals. First, we introduce the Database community to the techniques proposed for the problems of entity linkage and relation extraction by the Knowledge Management, Natural Language Processing, and Machine Learning communities. Second, we give a detailed survey of the work done by these communities in knowledge fusion, which is critical to discover and clean errors present in sources and the many mistakes made in the process of knowledge extraction from sources. Our tutorial is example driven and hopes to build bridges between the Database community and other disciplines to advance research in this important area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "a6aa01de0032367a444b811bec604ee924192179",
            "title": "Fine-grained controversy detection in Wikipedia",
            "abstract": "The advent of Web 2.0 gave birth to a new kind of application where content is generated through the collaborative contribution of many different users. This form of content generation is believed to generate data of higher quality since the \u201cwisdom of the crowds\u201d makes its way into the data. However, a number of specific data quality issues appear within such collaboratively generated data. Apart from normal updates, there are cases of intentional harmful changes known as vandalism as well as naturally occurring disagreements on topics which don't have an agreed upon viewpoint, known as controversies. While much work has focused on identifying vandalism, there has been little prior work on detecting controversies, especially at a fine granularity. Knowing about controversies when processing user-generated content is essential to understand the quality of the data and the trust that should be given to them. Controversy detection is a challenging task, since in the highly dynamic context of user updates, one needs to differentiate among normal updates, vandalisms and actual controversies. We describe a novel technique that finds these controversial issues by analyzing the edits that have been performed on the data over time. We apply the developed technique on Wikipedia, the world's largest known collaboratively generated database and we show that our approach has higher precision and recall than baseline approaches as well as is capable of finding previously unknown controversies.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2147072",
                    "name": "Siarhei Bykau"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "d1b8f3ad22210828beacdef296d5c25faedca032",
            "title": "Scaling up copy detection",
            "abstract": "Recent research shows that copying is prevalent for Deep-Web data and considering copying can significantly improve truth finding from conflicting values. However, existing copy detection techniques do not scale for large sizes and numbers of data sources, so truth finding can be slowed down by one to two orders of magnitude compared with the corresponding techniques that do not consider copying. In this paper, we study how to improve scalability of copy detection on structured data. Our algorithm builds an inverted index for each shared value and processes the index entries in decreasing order of how much the shared value can contribute to the conclusion of copying. We show how we use the index to prune the data items we consider for each pair of sources, and to incrementally refine our results in iterative copy detection. We also apply a sampling strategy with which we are able to further reduce copy-detection time while still obtaining very similar results as on the whole data set. Experiments on various real data sets show that our algorithm can reduce the time for copy detection by two to three orders of magnitude; in other words, truth finding can benefit from copy detection with very little overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116235540",
                    "name": "Xian Li"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2073317592",
                    "name": "Kenneth Lyons"
                },
                {
                    "authorId": "38699354",
                    "name": "W. Meng"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "df8d96793776d1521f865ccd060ac25e92a29240",
            "title": "Finding Quality in Quantity: The Challenge of Discovering Valuable Sources for Integration",
            "abstract": "Data is becoming a commodity of tremendous value for many domains. This is leading to a rapid increase in the number of data sources and public access data services, such as cloud-based data markets and data portals, that facilitate the collection, publishing and trading of data. Data sources typically exhibit wide variety and heterogeneity in the types or schemas of the data they provide, their quality, and the fees they charge for accessing their data. Users who want to build upon such publicly available data, must (i) discover sources that are relevant to their applications, (ii) identify sources that collectively satisfy the quality and budget requirements of their applications, with few effective clues about the quality of the sources, and (iii) repeatedly invest many person-hours in assessing the eventual usefulness of data sources. All three steps require investigating the content of the sources manually, integrating them and evaluating the actual benefit of the integration result for a desired application. Unfortunately, when the number of data sources is large, humans have a limited capability of reasoning about the actual quality of sources and the trade-offs between the benefits and costs of acquiring and integrating sources. In this paper we explore the problems of automatically appraising the quality of data sources and identifying the most valuable sources for diverse applications. We introduce our vision for a new data source management system that automatically assesses the quality of data sources based on a collection of rigorous data quality metrics and enables the automated and interactive discovery of valuable sources for user applications. We argue that the proposed system can dramatically simplify the Discover-Appraise-Evaluate interaction loop that many users follow today to discover sources for their applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145071799",
                    "name": "Theodoros Rekatsinas"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1746034",
                    "name": "L. Getoor"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "0a2b35a88c544cc54265d1bbe232393a7d6dd8eb",
            "title": "Discovering Conservation Rules",
            "abstract": "Many applications process data in which there exists a \u201cconservation law\u201d between related quantities. For example, in traffic monitoring, every incoming event, such as a packet's entering a router or a car's entering an intersection, should ideally have an immediate outgoing counterpart. We propose a new class of constraints-Conservation Rules-that express the semantics and characterize the data quality of such applications. We give confidence metrics that quantify how strongly a conservation rule holds and present approximation algorithms (with error guarantees) for the problem of discovering a concise summary of subsets of the data that satisfy a given conservation rule. Using real data, we demonstrate the utility of conservation rules and we show order-of-magnitude performance improvements of our discovery algorithms over naive approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "1735932",
                    "name": "H. Karloff"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "3269130",
                    "name": "B. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "2faed3265a07adc15edf7eb157a34c08732b1101",
            "title": "PrivBayes: private data release via bayesian networks",
            "abstract": "Privacy-preserving data publishing is an important problem that has been the focus of extensive study. The state-of-the-art goal for this problem is differential privacy, which offers a strong degree of privacy protection without making restrictive assumptions about the adversary. Existing techniques using differential privacy, however, cannot effectively handle the publication of high-dimensional data. In particular, when the input dataset contains a large number of attributes, existing methods require injecting a prohibitive amount of noise compared to the signal in the data, which renders the published data next to useless. To address the deficiency of the existing methods, this paper presents PrivBayes, a differentially private method for releasing high-dimensional data. Given a dataset D, PrivBayes first constructs a Bayesian network N, which (i) provides a succinct model of the correlations among the attributes in D and (ii) allows us to approximate the distribution of data in D using a set P of low-dimensional marginals of D. After that, PrivBayes injects noise into each marginal in P to ensure differential privacy, and then uses the noisy marginals and the Bayesian network to construct an approximation of the data distribution in D. Finally, PrivBayes samples tuples from the approximate distribution to construct a synthetic dataset, and then releases the synthetic data. Intuitively, PrivBayes circumvents the curse of dimensionality, as it injects noise into the low-dimensional marginals in P instead of the high-dimensional dataset D. Private construction of Bayesian networks turns out to be significantly challenging, and we introduce a novel approach that uses a surrogate function for mutual information to build the model more accurately. We experimentally evaluate PrivBayes on real data, and demonstrate that it significantly outperforms existing solutions in terms of accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2155659897",
                    "name": "Jun Zhang"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "33285410",
                    "name": "Xiaokui Xiao"
                }
            ]
        },
        {
            "paperId": "929dc9bb5cde0df227d847c40acaabbee26691bf",
            "title": "Characterizing and selecting fresh data sources",
            "abstract": "Data integration is a challenging task due to the large numbers of autonomous data sources. This necessitates the development of techniques to reason about the benefits and costs of acquiring and integrating data. Recently the problem of source selection (i.e., identifying the subset of sources that maximizes the profit from integration) was introduced as a preprocessing step before the actual integration. The problem was studied for static sources and used the accuracy of data fusion to quantify the integration profit. In this paper, we study the problem of source selection considering dynamic data sources whose content changes over time. We define a set of time-dependent metrics, including coverage, freshness and accuracy, to characterize the quality of integrated data. We show how statistical models for the evolution of sources can be used to estimate these metrics. While source selection is NP-complete, we show that for a large class of practical cases, near-optimal solutions can be found, propose an algorithmic framework with theoretical guarantees for our problem and show its effectiveness with an extensive experimental evaluation on both real-world and synthetic data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145071799",
                    "name": "Theodoros Rekatsinas"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "9af26df2edd37eca936ba261746d239242b488b6",
            "title": "Interpretable and Informative Explanations of Outcomes",
            "abstract": "In this paper, we solve the following data summarization problem: given a multi-dimensional data set augmented with a binary attribute, how can we construct an interpretable and informative summary of the factors affecting the binary attribute in terms of the combinations of values of the dimension attributes? We refer to such summaries as explanation tables. We show the hardness of constructing optimally-informative explanation tables from data, and we propose effective and efficient heuristics. The proposed heuristics are based on sampling and include optimizations related to computing the information content of a summary from a sample of the data. Using real data sets, we demonstrate the advantages of explanation tables compared to related approaches that can be adapted to solve our problem, and we show significant performance benefits of our optimizations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2531258",
                    "name": "Kareem El Gebaly"
                },
                {
                    "authorId": "37973846",
                    "name": "Parag Agrawal"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "a64023de1f09e38d804c489024a38b93aa49c43f",
            "title": "Empirical glitch explanations",
            "abstract": "Data glitches are unusual observations that do not conform to data quality expectations, be they logical, semantic or statistical. By applying data integrity constraints, potentially large sections of data could be flagged as being noncompliant. Ignoring or repairing significant sections of the data could fundamentally bias the results and conclusions drawn from analyses. In the context of Big Data where large numbers and volumes of feeds from disparate sources are integrated, it is likely that significant portions of seemingly noncompliant data are actually legitimate usable data. In this paper, we introduce the notion of Empirical Glitch Explanations - concise, multi-dimensional descriptions of subsets of potentially dirty data - and propose a scalable method for empirically generating such explanatory characterizations. The explanations could serve two valuable functions: (1) Provide a way of identifying legitimate data and releasing it back into the pool of clean data. In doing so, we reduce cleaning-related statistical distortion of the data; (2) Used to refine existing data quality constraints and generate and formalize domain knowledge. We conduct experiments using real and simulated data to demonstrate the scalability of our method and the robustness of explanations. In addition, we use two real world examples to demonstrate the utility of the explanations where we reclaim over 99% of the suspicious data, keeping data repair related statistical distortion close to 0.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "2104305",
                    "name": "J. Loh"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "b49f63f1d4876961a1825f7be0416a5e9c331e3a",
            "title": "Fusing data with correlations",
            "abstract": "Many applications rely on Web data and extraction systems to accomplish knowledge-driven tasks. Web information is not curated, so many sources provide inaccurate, or conflicting information. Moreover, extraction systems introduce additional noise to the data. We wish to automatically distinguish correct data and erroneous data for creating a cleaner set of integrated data. Previous work has shown that a naive voting strategy that trusts data provided by the majority or at least a certain number of sources may not work well in the presence of copying between the sources. However, correlation between sources can be much broader than copying: sources may provide data from complementary domains (negative correlation), extractors may focus on different types of information (negative correlation), and extractors may apply common rules in extraction (positive correlation, without copying). In this paper we present novel techniques modeling correlations between sources and applying it in truth finding. We provide a comprehensive evaluation of our approach on three real-world datasets with different characteristics, as well as on synthetic data, showing that our algorithms outperform the existing state-of-the-art techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39209236",
                    "name": "R. Pochampally"
                },
                {
                    "authorId": "1753537",
                    "name": "A. Sarma"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2283085",
                    "name": "A. Meliou"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "ddf36ee57133fa5fdff848da15b21983dc893a5a",
            "title": "Incremental Record Linkage",
            "abstract": "Record linkage clusters records such that each cluster corresponds to a single distinct real-world entity. It is a crucial step in data cleaning and data integration. In the big data era, the velocity of data updates is often high, quickly making previous linkage results obsolete. This paper presents an end-to-end framework that can incrementally and efficiently update linkage results when data updates arrive. Our algorithms not only allow merging records in the updates with existing clusters, but also allow leveraging new evidence from the updates to fix previous linkage errors. Experimental results on three real and synthetic data sets show that our algorithms can significantly reduce linkage time without sacrificing linkage quality.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3202556",
                    "name": "Anja Gruenheid"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "e4f9e5d5d17368afaf3f3120a33fd60f8fd30052",
            "title": "Data quality: The other face of Big Data",
            "abstract": "In our Big Data era, data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Recent studies have shown that poor quality data is prevalent in large databases and on the Web. Since poor quality data can have serious consequences on the results of data analyses, the importance of veracity, the fourth `V' of big data is increasingly being recognized. In this tutorial, we highlight the substantial challenges that the first three `V's, volume, velocity and variety, bring to dealing with veracity in big data. Due to the sheer volume and velocity of data, one needs to understand and (possibly) repair erroneous data in a scalable and timely manner. With the variety of data, often from a diversity of sources, data quality rules cannot be specified a priori; one needs to let the \u201cdata to speak for itself\u201d in order to discover the semantics of the data. This tutorial presents recent results that are relevant to big data quality management, focusing on the two major dimensions of (i) discovering quality issues from the data itself, and (ii) trading-off accuracy vs efficiency, and identifies a range of open problems for the community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3269130",
                    "name": "B. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "2eb6c98941f7f1b8a94c420f00650c60d34466fc",
            "title": "Compact explanation of data fusion decisions",
            "abstract": "Despite the abundance of useful information on the Web, different Web sources often provide conflicting data, some being out-of-date, inaccurate, or erroneous. Data fusion aims at resolving conflicts and finding the truth. Advanced fusion techniques apply iterative MAP (Maximum A Posteriori) analysis that reasons about trustworthiness of sources and copying relationships between them. Providing explanations for such decisions is important for a better understanding, but can be extremely challenging because of the complexity of the analysis during decision making. This paper proposes two types of explanations for data-fusion results: snapshot explanations take the provided data and any other decision inferred from the data as evidence and provide a high-level understanding of a fusion decision; comprehensive explanations take only the data as evidence and provide an in-depth understanding of a fusion decision. We propose techniques that can efficiently generate correct and compact explanations. Experimental results show that (1) we generate correct explanations, (2) our techniques can significantly reduce the sizes of the explanations, and (3) we can generate the explanations efficiently.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "35c14fd2453b0dc02a02dcf538fbd775e4993e81",
            "title": "UMicS: from anonymized data to usable microdata",
            "abstract": "There is currently a tug-of-war going on surrounding data releases. On one side, there are many strong reasons pulling to release data to other parties: business factors, freedom of information rules, and scientific sharing agreements. On the other side, concerns about individual privacy pull back, and seek to limit releases. Privacy technologies such as differential privacy have been proposed to resolve this deadlock, and there has been much study of how to perform private data release of data in various forms. The focus of such works has been largely on the data owner: what process should they apply to ensure that the released data preserves privacy whilst still capturing the input data distribution accurately. Almost no attention has been paid to the needs of the data user, who wants to make use of the released data within their existing suite of tools and data. The difficulty of making use of data releases is a major stumbling block for the widespread adoption of data privacy technologies. In this paper, instead of proposing new privacy mechanisms for data publishing, we consider the whole data release process, from the data owner to the data user. We lay out a set of principles for privacy tool design that highlights the requirements for interoperability, extensibility and scalability. We put these into practice with UMicS, an end-to-end prototype system to control the release and use of private data. An overarching tenet is that it should be possible to integrate the released data into the data user's systems with the minimum of change and cost. We describe how to instantiate UMicS in a variety of usage scenarios. We show how using data modeling techniques from machine learning can improve the utility, in particular when combined with background knowledge that the data user may possess. We implement UMicS, and evaluate it over a selection of data sets and release cases. We see that UMicS allows for very effective use of released data, while upholding our privacy principles.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "39659954",
                    "name": "E. Shen"
                },
                {
                    "authorId": "2075408461",
                    "name": "Xi Gong"
                },
                {
                    "authorId": "144478228",
                    "name": "Ting Yu"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "44b0e81ad611d5c87c6b63ba886236d8253dc867",
            "title": "Big data integration",
            "abstract": "The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data-driven decision making is sweeping through all aspects of society. Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data. BDI differs from traditional data integration in many dimensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This seminar explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "4893c08f1c76fe4eb7cad2bb496da526b0fa5dd3",
            "title": "Empirical privacy and empirical utility of anonymized data",
            "abstract": "Procedures to anonymize data sets are vital for companies, government agencies and other bodies to meet their obligations to share data without compromising the privacy of the individuals contributing to it. Despite much work on this topic, the area has not yet reached stability. Early models (k-anonymity and \u2113-diversity) are now thought to offer insufficient privacy. Noise-based methods like differential privacy are seen as providing stronger privacy, but less utility. However, across all methods sensitive information of some individuals can often be inferred with relatively high accuracy. In this paper, we reverse the idea of a `privacy attack,' by incorporating it into a measure of privacy. Hence, we advocate the notion of empirical privacy, based on the posterior beliefs of an adversary, and their ability to draw inferences about sensitive values in the data. This is not a new model, but rather a unifying view: it allows us to study several well-known privacy models which are not directly comparable otherwise. We also consider an empirical approach to measuring utility, based on a workload of queries. Consequently, we are able to place different privacy models including differential privacy and early syntactic models on the same scale, and compare their privacy/utility tradeoff. We learn that, in practice, the difference between differential privacy and various syntactic models is less dramatic than previously thought, but there are still clear domination relations between them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "39659954",
                    "name": "E. Shen"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "144478228",
                    "name": "Ting Yu"
                }
            ]
        },
        {
            "paperId": "4e899f3bd316d8865a81c3f93c14e8697efb355f",
            "title": "On Repairing Structural Problems In Semi-structured Data",
            "abstract": "Semi-structured data such as XML are popular for data interchange and storage. However, many XML documents have improper nesting where open - and close-tags are unmatched. Since some semi-structured data (e.g., Latex) have a flexible grammar and since many XML documents lack an accompanying DTD or XSD, we focus on computing a syntactic repair via the edit distance. \n \nTo solve this problem, we propose a dynamic programming algorithm which takes cubic time. While this algorithm is not scalable, well-formed substrings of the data can be pruned to enable faster computation. Unfortunately, there are still cases where the dynamic program could be very expensive; hence, we give branch-and-bound algorithms based on various combinations of two heuristics, called MinCost and MaxBenefit, that trade off between accuracy and efficiency. Finally, we experimentally demonstrate the performance of these algorithms on real data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "3269130",
                    "name": "B. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1883706",
                    "name": "Shanshan Ying"
                }
            ]
        },
        {
            "paperId": "8dc2f761232dac493c6479e424987be4802503c6",
            "title": "Reverse engineering complex join queries",
            "abstract": "We study the following problem: Given a database D with schema G and an output table Out, compute a join query Q that generates OUT from D. A simpler variant allows Q to return a superset of Out. This problem has numerous applications, both by itself, and as a building block for other problems. Related prior work imposes conditions on the structure of Q which are not always consistent with the application, but simplify computation. We discuss several natural SQL queries that do not satisfy these conditions and cannot be discovered by prior work.\n In this paper, we propose an efficient algorithm that discovers queries with arbitrary join graphs. A crucial insight is that any graph can be characterized by the combination of a simple structure, called a star, and a series of merge steps over the star. The merge steps define a lattice over graphs derived from the same star. This allows us to explore the set of candidate solutions in a principled way and quickly prune out a large number of infeasible graphs. We also design several optimizations that significantly reduce the running time. Finally, we conduct an extensive experimental study over a benchmark database and show that our approach is scalable and accurately discovers complex join queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1793178",
                    "name": "Meihui Zhang"
                },
                {
                    "authorId": "1890500",
                    "name": "Hazem Elmeleegy"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "a6647da1bd947c31ad1093788a4fb4d90c6d82fc",
            "title": "Finding interesting correlations with conditional heavy hitters",
            "abstract": "The notion of heavy hitters-items that make up a large fraction of the population - has been successfully used in a variety of applications across sensor and RFID monitoring, network data analysis, event mining, and more. Yet this notion often fails to capture the semantics we desire when we observe data in the form of correlated pairs. Here, we are interested in items that are conditionally frequent: when a particular item is frequent within the context of its parent item. In this work, we introduce and formalize the notion of Conditional Heavy Hitters to identify such items, with applications in network monitoring, and Markov chain modeling. We introduce several streaming algorithms that allow us to find conditional heavy hitters efficiently, and provide analytical results. Different algorithms are successful for different input characteristics. We perform experimental evaluations to demonstrate the efficacy of our methods, and to study which algorithms are most suited for different types of data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1701357",
                    "name": "Katsiaryna Mirylenka"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "fc1dd7f689d6095d0d3bb8101bb66d63742bf78e",
            "title": "Online Ordering of Overlapping Data Sources",
            "abstract": "Data integration systems offer a uniform interface for querying a large number of autonomous and heterogeneous data sources. Ideally, answers are returned as sources are queried and the answer list is updated as more answers arrive. Choosing a good ordering in which the sources are queried is critical for increasing the rate at which answers are returned. However, this problem is challenging since we often do not have complete or precise statistics of the sources, such as their coverage and overlap. It is further exacerbated in the Big Data era, which is witnessing two trends in Deep-Web data: first, obtaining a full coverage of data in a particular domain often requires extracting data from thousands of sources; second, there is often a big variation in overlap between different data sources. \n \nIn this paper we present OASIS, an Online query Answering System for overlappIng Sources. OASIS has three key components for source ordering. First, the Overlap Estimation component estimates overlaps between sources according to available statistics under the Maximum Entropy principle. Second, the Source Ordering component orders the sources according to the new contribution they are expected to provide, and adjusts the ordering based on statistics collected during query answering. Third, the Statistics Enrichment component selects critical missing statistics to enrich at runtime. Experimental results on both real and synthetic data show high efficiency and scalability of our algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2399899",
                    "name": "Mariam Salloum"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1761528",
                    "name": "V. Tsotras"
                }
            ]
        },
        {
            "paperId": "0a1f57b7fabec34fcfa98b8e05f997dbb5723a18",
            "title": "Chronos: Facilitating History Discovery by Linking Temporal Records",
            "abstract": "\n Many data sets contain\n temporal records\n over a long period of time; each record is associated with a time stamp and describes some aspects of a real-world entity at that particular time. From such data, users often wish to search for entities in a particular period and understand the history of one entity or all entities in the data set. A major challenge for enabling such search and exploration is to identify records that describe the same real-world entity over a long period of time; however, linking temporal records is hard given that the values that describe an entity can evolve over time (\n e.g\n ., a person can move from one affiliation to another).\n \n \n We demonstrate the Chronos system which offers users the useful tool for finding real-world entities over time and understanding history of entities in the bibliography domain. The core of Chronos is a temporal record-linkage algorithm, which is tolerant to value evolution over time. Our algorithm can obtain an F-measure of over 0.9 in linking author records and fix errors made by\n DBLP\n . We show how Chronos allows users to explore the history of authors, and how it helps users understand our linkage results by comparing our results with those of existing systems, highlighting differences in the results, explaining our decisions to users, and answering \"what-if\" questions.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112726639",
                    "name": "Pei Li"
                },
                {
                    "authorId": "1515737549",
                    "name": "Haidong Wang"
                },
                {
                    "authorId": "1973493",
                    "name": "C. Tziviskou"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1700438",
                    "name": "X. Liu"
                },
                {
                    "authorId": "1743266",
                    "name": "A. Maurino"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "0e80f1973f9f058f2f9dff54d8e2856d7c456b5f",
            "title": "Detecting Clones, Copying and Reuse on the Web",
            "abstract": "The Web has enabled the availability of a vast amount of useful information in recent years. However, the web technologies that have enabled sources to share their information have also made it easy for sources to copy from each other and often publish without proper attribution. Understanding the copying relationships between sources has many benefits, including helping data providers protect their own rights, improving various aspects of data integration, and facilitating in-depth analysis of information flow. The importance of copy detection has led to a substantial amount of research in many disciplines of Computer Science, based on the type of information considered, such as text, images, videos, software code, and structured data. This seminar explores the similarities and differences between the techniques proposed for copy detection across the different types of information. We also examine the computational challenges associated with large-scale copy detection, indicating how they could be detected efficiently, and identify a range of open problems for the community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "282b4c4b7f9cdfee5c719a7eb212669b2360a126",
            "title": "Proceedings of the 2012 Joint EDBT/ICDT Workshops, Berlin, Germany, March 30, 2012",
            "abstract": "The EDBT/ICDT 2012 joint conference takes place on March 26-30, 2012 in Berlin, Germany. As in previous years, we host a number of high-quality workshops that will be held on the last day of the conference. This year, there is a collection of six EDBT/ICDT 2012 workshops whose themes cover important and timely aspects of databases and their applications, which complement the main conference program.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1762563",
                    "name": "Ismail Ari"
                }
            ]
        },
        {
            "paperId": "46b416896cd48484603447e3aec38a719468f068",
            "title": "Truth Finding on the Deep Web: Is the Problem Solved?",
            "abstract": "The amount of useful information available on the Web has been growing at a dramatic pace in recent years and people rely more and more on the Web to fulfill their information needs. In this paper, we study truthfulness of Deep Web data in two domains where we believed data are fairly clean and data quality is important to people's lives: Stock and Flight. To our surprise, we observed a large amount of inconsistency on data from different sources and also some sources with quite low accuracy. We further applied on these two data sets state-of-the-art data fusion methods that aim at resolving conflicts and finding the truth, analyzed their strengths and limitations, and suggested promising research directions. We wish our study can increase awareness of the seriousness of conflicting data on the Web and in turn inspire more research in our community to tackle this problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116235540",
                    "name": "Xian Li"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2073317592",
                    "name": "Kenneth Lyons"
                },
                {
                    "authorId": "38699354",
                    "name": "W. Meng"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "60090e2fcc0c7094010fad2a7df8542aa25af159",
            "title": "Discovering Conservation Rules",
            "abstract": "Many applications process data in which there exists a ``conservation law'' between related quantities. For example, in traffic monitoring, every incoming event, such as a packet's entering a router or a car's entering an intersection, should ideally have an immediate outgoing counterpart. We propose a new class of constraints -- Conservation Rules -- that express the semantics and characterize the data quality of such applications. We give confidence metrics that quantify how strongly a conservation rule holds and present approximation algorithms (with error guarantees) for the problem of discovering a concise summary of subsets of the data that satisfy a given conservation rule. Using real data, we demonstrate the utility of conservation rules and we show order-of-magnitude performance improvements of our discovery algorithms over naive approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "1735932",
                    "name": "H. Karloff"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "3269130",
                    "name": "B. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "70325f4d1f7502bd06fa54484535b36c8b0c1290",
            "title": "Accurate and efficient private release of datacubes and contingency tables",
            "abstract": "A central problem in releasing aggregate information about sensitive data is to do so accurately while providing a privacy guarantee on the output. Recent work focuses on the class of linear queries, which include basic counting queries, data cubes, and contingency tables. The goal is to maximize the utility of their output, while giving a rigorous privacy guarantee. Most results follow a common template: pick a \u201cstrategy\u201d set of linear queries to apply to the data, then use the noisy answers to these queries to reconstruct the queries of interest. This entails either picking a strategy set that is hoped to be good for the queries, or performing a costly search over the space of all possible strategies. In this paper, we propose a new approach that balances accuracy and efficiency: we show how to improve the accuracy of a given query set by answering some strategy queries more accurately than others. This leads to an efficient optimal noise allocation for many popular strategies, including wavelets, hierarchies, Fourier coefficients and more. For the important case of marginal queries we show that this strictly improves on previous methods, both analytically and empirically. Our results also extend to ensuring that the returned query answers are consistent with an (unknown) data set at minimal extra cost in terms of time and noise.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1757584",
                    "name": "G. Yaroslavtsev"
                }
            ]
        },
        {
            "paperId": "72d5034cd1e92e88e5e11630e3b24f9d739d968d",
            "title": "Less is More: Selecting Sources Wisely for Integration",
            "abstract": "We are often thrilled by the abundance of information surrounding us and wish to integrate data from as many sources as possible. However, understanding, analyzing, and using these data are often hard. Too much data can introduce a huge integration cost, such as expenses for purchasing data and resources for integration and cleaning. Furthermore, including low-quality data can even deteriorate the quality of integration results instead of bringing the desired quality gain. Thus, \"the more the better\" does not always hold for data integration and often \"less is more\". \n \nIn this paper, we study how to select a subset of sources before integration such that we can balance the quality of integrated data and integration cost. Inspired by the Marginalism principle in economic theory, we wish to integrate a new source only if its marginal gain, often a function of improved integration quality, is higher than the marginal cost, associated with data-purchase expense and integration resources. As a first step towards this goal, we focus on data fusion tasks, where the goal is to resolve conflicts from different sources. We propose a randomized solution for selecting sources for fusion and show empirically its effectiveness and scalability on both real-world data and synthetic data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "3269130",
                    "name": "B. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "99126d196c0539e3014b61d91538a29ee2de8c54",
            "title": "Aggregate Query Answering on Possibilistic Data with Cardinality Constraints",
            "abstract": "Uncertainties in data can arise for a number of reasons: when data is incomplete, contains conflicting information or has been deliberately perturbed or coarsened to remove sensitive details. An important case which arises in many real applications is when the data describes a set of possibilities, but with cardinality constraints. These constraints represent correlations between tuples encoding, e.g. that at most two possible records are correct, or that there is an (unknown) one-to-one mapping between a set of tuples and attribute values. Although there has been much effort to handle uncertain data, current systems are not equipped to handle such correlations, beyond simple mutual exclusion and co-existence constraints. Vitally, they have little support for efficiently handling aggregate queries on such data. In this paper, we aim to address some of these deficiencies, by introducing LICM (Linear Integer Constraint Model), which can succinctly represent many types of tuple correlations, particularly a class of cardinality constraints. We motivate and explain the model with examples from data cleaning and masking sensitive data, to show that it enables modeling and querying such data, which was not previously possible. We develop an efficient strategy to answer conjunctive and aggregate queries on possibilistic data by describing how to implement relational operators over data in the model. LICM compactly integrates the encoding of correlations, query answering and lineage recording. In combination with off-the-shelf linear integer programming solvers, our approach provides exact bounds for aggregate queries. Our prototype implementation demonstrates that query answering with LICM can be effective and scalable.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "39659954",
                    "name": "E. Shen"
                },
                {
                    "authorId": "144478228",
                    "name": "Ting Yu"
                }
            ]
        },
        {
            "paperId": "9fe8e35289ed21e222255c2b8a419e846af00bc7",
            "title": "A Dataset Search Engine for the Research Document Corpus",
            "abstract": "A key step in validating a proposed idea or system is to evaluate over a suitable dataset. However, to this date there have been no useful tools for researchers to understand which datasets have been used for what purpose, or in what prior work. Instead, they have to manually browse through papers to find the suitable datasets and their corresponding URLs, which is laborious and inefficient. To better aid the dataset discovery process, and provide a better understanding of how and where datasets have been used, we propose a framework to effectively identify datasets within the scientific corpus. The key technical challenges are identification of datasets, and discovery of the association between a dataset and the URLs where they can be accessed. Based on this, we have built a user friendly web-based search interface for users to conveniently explore the dataset-paper relationships, and find relevant datasets and their properties.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2114053",
                    "name": "Meiyu Lu"
                },
                {
                    "authorId": "35313721",
                    "name": "S. Bangalore"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "dfe51d4292e4e2538dee8db93becf1d382cb59cd",
            "title": "In Search of Truth (on the Deep Web)",
            "abstract": "The Deep Web has enabled the availability of a huge amount of useful information and people have come to rely on it to ful ll their information needs in a variety of domains. We present a recent study on the accuracy of data and the quality of Deep Web sources in two domains where quality is important to people's lives: Stock and Flight. We observe that, even in these domains, the quality of the data is less than ideal, with sources providing con icting, out-of-date and incomplete data. Sources also copy, reformat and modify data from other sources, making it di cult to discover the truth. We describe techniques proposed in the literature to solve these problems, evaluate their strengths on our data, and identify directions for future work in this area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "f212b02bc1fbddd05731d8a91284a534a2149fbc",
            "title": "Differentially private summaries for sparse data",
            "abstract": "Differential privacy is fast becoming the method of choice for releasing data under strong privacy guarantees. A standard mechanism is to add noise to the counts in contingency tables derived from the dataset. However, when the dataset is sparse in its underlying domain, this vastly increases the size of the published data, to the point of making the mechanism infeasible.\n We propose a general framework to overcome this problem. Our approach releases a compact summary of the noisy data with the same privacy guarantee and with similar utility. Our main result is an efficient method for computing the summary directly from the input data, without materializing the vast noisy data. We instantiate this general framework for several summarization methods. Our experiments show that this is a highly practical solution: The summaries are up to 1000 times smaller, and can be computed in less than 1% of the time compared to standard methods. Finally, our framework works with various data transformations, such as wavelets or sketches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "145588096",
                    "name": "Thanh T. L. Tran"
                }
            ]
        },
        {
            "paperId": "06ecdae642454b53936d04a2ce99dadb46d08c23",
            "title": "DivDB",
            "abstract": "With the availability of very large databases, an exploratory query can easily lead to a vast answer set, typically based on an answer's relevance (i.e., top-k, tf-idf) to the user query. Navigating through such an answer set requires huge effort and users give up after perusing through the first few answers, thus some interesting answers hidden further down the answer set can easily be missed. An approach to address this problem is to present the user with the most diverse among the answers based on some diversity criterion. In this demonstration we present DivDB, a system we built to provide query result diversification both for advanced and novice users. For the experienced users, who may want to test the performance of existing and new algorithms, we provide an SQL-based extension to formulate queries with diversification. As for the novice users, who may be more interested in the result rather than how to tune the various algorithms' parameters, the DivDB system allows the user to provide a \"hint\" to the optimizer on speed vs. quality of result. Moreover, novice users can use an interface to dynamically change the tradeoff value between relevance and diversity in the result, and thus visually inspect the result as they interact with this parameter. This is a great feature to the end user because finding a good tradeoff value is a very hard task and it depends on several variables (i.e., query parameters, evaluation algorithms, and dataset properties). In this demonstration we show a study of the DivDB system with two image databases that contain many images of the same object under different settings (e.g., different camera angle). We show how the DivDB helps users to iteratively inspect diversification in the query result, without the need to know how to tune the many different parameters of the several existing algorithms in the DivDB system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145549427",
                    "name": "Marcos R. Vieira"
                },
                {
                    "authorId": "1852790",
                    "name": "H. Razente"
                },
                {
                    "authorId": "2635026",
                    "name": "M. Barioni"
                },
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "104501789",
                    "name": "C. Traina"
                },
                {
                    "authorId": "1761528",
                    "name": "V. Tsotras"
                }
            ]
        },
        {
            "paperId": "1e7fa6d5a0d8be284b520297c7541ddcab55953e",
            "title": "Bistro data feed management system",
            "abstract": "Data feed management is a critical component of many data intensive applications that depend on reliable data delivery to support real-time data collection, correlation and analysis. Data is typically collected from a wide variety of sources and organizations, using a range of mechanisms - some data are streamed in real time, while other data are obtained at regular intervals or collected in an ad hoc fashion. Individual applications are forced to make separate arrangements with feed providers, learn the structure of incoming files, monitor data quality, and trigger any processing necessary. The Bistro data feed manager, designed and implemented at AT&T Labs- Research, simplifies and automates this complex task of data feed management: efficiently handling incoming raw files, identifying data feeds and distributing them to remote subscribers. Bistro supports a flexible specification language to define logical data feeds using the naming structure of physical data files, and to identify feed subscribers. Based on the specification, Bistro matches data files to feeds, performs file normalization and compression, efficiently delivers files, and notifies subscribers using a trigger mechanism. We describe our feed analyzer that discovers the naming structure of incoming data files to detect new feeds, dropped feeds, feed changes, or lost data in an existing feed. Bistro is currently deployed within AT&T Labs and is responsible for the real-time delivery of over 100 different raw feeds, distributing data to several large-scale stream warehouses.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1745049",
                    "name": "Vladislav Shkapenyuk"
                },
                {
                    "authorId": "145633895",
                    "name": "T. Johnson"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "31b58ddd4a558f7d9e41e19ef75993668ee69119",
            "title": "Differentially Private Spatial Decompositions",
            "abstract": "Differential privacy has recently emerged as the de facto standard for private data release. This makes it possible to provide strong theoretical guarantees on the privacy and utility of released data. While it is well-understood how to release data based on counts and simple functions under this guarantee, it remains to provide general purpose techniques to release data that is useful for a variety of queries. In this paper, we focus on spatial data such as locations and more generally any multi-dimensional data that can be indexed by a tree structure. Directly applying existing differential privacy methods to this type of data simply generates noise. We propose instead the class of \"private spatial decompositions'': these adapt standard spatial indexing methods such as quad trees and kd-trees to provide a private description of the data distribution. Equipping such structures with differential privacy requires several steps to ensure that they provide meaningful privacy guarantees. Various basic steps, such as choosing splitting points and describing the distribution of points within a region, must be done privately, and the guarantees of the different building blocks composed to provide an overall guarantee. Consequently, we expose the design space for private spatial decompositions, and analyze some key examples. A major contribution of our work is to provide new techniques for parameter setting and post-processing the output to improve the accuracy of query answers. Our experimental study demonstrates that it is possible to build such decompositions efficiently, and use them to answer a variety of queries privately with high accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "39659954",
                    "name": "E. Shen"
                },
                {
                    "authorId": "2087140581",
                    "name": "Ting Yu"
                }
            ]
        },
        {
            "paperId": "373e6076c0d8b3cd192ce9e4cd5196f147593b01",
            "title": "On query result diversification",
            "abstract": "In this paper we describe a general framework for evaluation and optimization of methods for diversifying query results. In these methods, an initial ranking candidate set produced by a query is used to construct a result set, where elements are ranked with respect to relevance and diversity features, i.e., the retrieved elements should be as relevant as possible to the query, and, at the same time, the result set should be as diverse as possible. While addressing relevance is relatively simple and has been heavily studied, diversity is a harder problem to solve. One major contribution of this paper is that, using the above framework, we adapt, implement and evaluate several existing methods for diversifying query results. We also propose two new approaches, namely the Greedy with Marginal Contribution (GMC) and the Greedy Randomized with Neighborhood Expansion (GNE) methods. Another major contribution of this paper is that we present the first thorough experimental evaluation of the various diversification techniques implemented in a common framework. We examine the methods' performance with respect to precision, running time and quality of the result. Our experimental results show that while the proposed methods have higher running times, they achieve precision very close to the optimal, while also providing the best result quality. While GMC is deterministic, the randomized approach (GNE) can achieve better result quality if the user is willing to tradeoff running time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145549427",
                    "name": "Marcos R. Vieira"
                },
                {
                    "authorId": "1852790",
                    "name": "H. Razente"
                },
                {
                    "authorId": "2635026",
                    "name": "M. Barioni"
                },
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "104501789",
                    "name": "C. Traina"
                },
                {
                    "authorId": "1761528",
                    "name": "V. Tsotras"
                }
            ]
        },
        {
            "paperId": "453c5a0ff2b97746de65153a2ea39799458585e7",
            "title": "Online data fusion",
            "abstract": "The Web contains a significant volume of structured data in various domains, but a lot of data are dirty and erroneous, and they can be propagated through copying. While data integration techniques allow querying structured data on the Web, they take the union of the answers retrieved from different sources and can thus return conflicting information. Data fusion techniques, on the other hand, aim to find the true values, but are designed for offline data aggregation and can take a long time. This paper proposes Solaris, the first online data fusion system. It starts with returning answers from the first probed source, and refreshes the answers as it probes more sources and applies fusion techniques on the retrieved data. For each returned answer, it shows the likelihood that the answer is correct, and stops retrieving data for it after gaining enough confidence that data from the unprocessed sources are unlikely to change the answer. We address key problems in building such a system and show empirically that the system can start returning correct answers quickly and terminate fast without sacrificing the quality of the answers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "26966619",
                    "name": "Xuan Liu"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1693070",
                    "name": "B. Ooi"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "4d6f5aea9725c68dc6867ed3e09c3173e9990320",
            "title": "Discovery of complex glitch patterns: A novel approach to Quantitative Data Cleaning",
            "abstract": "Quantitative Data Cleaning (QDC) is the use of statistical and other analytical techniques to detect, quantify, and correct data quality problems (or glitches). Current QDC approaches focus on addressing each category of data glitch individually. However, in real-world data, different types of data glitches co-occur in complex patterns. These patterns and interactions between glitches offer valuable clues for developing effective domain-specific quantitative cleaning strategies. In this paper, we address the shortcomings of the extant QDC methods by proposing a novel framework, the DEC (Detect-Explore-Clean) framework. It is a comprehensive approach for the definition, detection and cleaning of complex, multi-type data glitches. We exploit the distributions and interactions of different types of glitches to develop data-driven cleaning strategies that may offer significant advantages over blind strategies. The DEC framework is a statistically rigorous methodology for evaluating and scoring glitches and selecting the quantitative cleaning strategies that result in cleaned data sets that are statistically proximal to user specifications. We demonstrate the efficacy and scalability of the DEC framework on very large real-world and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "2280205",
                    "name": "T. Dasu"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "7466d1a809d69435f29cebdd651029566eb702f7",
            "title": "Differentially Private Publication of Sparse Data",
            "abstract": "The problem of privately releasing data is to provide a version of a dataset without revealing sensitive information about the individuals who contribute to the data. The model of differential privacy allows such private release while providing strong guarantees on the output. A basic mechanism achieves differential privacy by adding noise to the frequency counts in the contingency tables (or, a subset of the count data cube) derived from the dataset. However, when the dataset is sparse in its underlying space, as is the case for most multi-attribute relations, then the effect of adding noise is to vastly increase the size of the published data: it implicitly creates a huge number of dummy data points to mask the true data, making it almost impossible to work with. \nWe present techniques to overcome this roadblock and allow efficient private release of sparse data, while maintaining the guarantees of differential privacy. Our approach is to release a compact summary of the noisy data. Generating the noisy data and then summarizing it would still be very costly, so we show how to shortcut this step, and instead directly generate the summary from the input data, without materializing the vast intermediate noisy data. We instantiate this outline for a variety of sampling and filtering methods, and show how to use the resulting summary for approximate, private, query answering. Our experimental study shows that this is an effective, practical solution, with comparable and occasionally improved utility over the costly materialization approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "145588096",
                    "name": "Thanh T. L. Tran"
                }
            ]
        },
        {
            "paperId": "96ca057e072ab3090a761f22c5b281e61a055dbc",
            "title": "Efficient and Effective Analysis of Data Quality using Pattern Tableaux",
            "abstract": "Data Auditor is a system for analyzing data quality via exploring data semantics. Given a user-supplied constraint, such as a functional dependency or an inclusion dependency, the system computes pattern tableaux, which are concise summaries of subsets of the data that satisfy (or fail) the constraint. The engine of Data Auditor is an efficient algorithm for finding these patterns, which defers expensive computation on patterns until needed during search, thereby pruning wasted effort. We demonstrate the utility of our approach on a variety of data as well as the performance gain from employing this algorithm.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "ab27699a94159830a21679de39e918a236e65955",
            "title": "Discovering Pattern Tableaux for Data Quality Analysis: a Case Study",
            "abstract": "In this paper, we present a case study that illustrates the utility of pattern tableau discovery for data quality analysis. Given a usersupplied integrity constraint, such as a boolean predicate expected to be satisfied by every tuple, a functional dependency, or an inclusion dependency, a pattern tableau is a concise summary of subsets of the data that satisfy or fail the constraint. We describe Data Auditor\u2014our system for automatic tableau discovery from data\u2014and we give real-life examples of characterizing data quality in a network monitoring database used by a large Internet Service Provider.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "c3fb56311ad4a58288156cf0b19930d363a7fdb6",
            "title": "Approximate String Processing",
            "abstract": "One of the most important primitive data types in modern data processing is text. Text data are known to have a variety of inconsistencies (e.g., spelling mistakes and representational variations). For that reason, there exists a large body of literature related to approximate processing of text. This monograph focuses specifically on the problem of approximate string matching, where, given a set of strings S and a query string v, the goal is to find all strings s \u2208 S that have a user specified degree of similarity to v. Set S could be, for example, a corpus of documents, a set of web pages, or an attribute of a relational table. The similarity between strings is always defined with respect to a similarity function that is chosen based on the characteristics of the data and application at hand. This work presents a survey of indexing techniques and algorithms specifically designed for approximate string matching. We concentrate on inverted indexes, filtering techniques, and tree data structures that can be used to evaluate a variety of set based and edit based similarity functions. We focus on all-match and top-k flavors of selection and join queries, and discuss the applicability, advantages and disadvantages of each technique for every query type.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "e51c17e08a97330db0eaba89371e0e9becca531d",
            "title": "Large-scale copy detection",
            "abstract": "The Web has enabled the availability of a vast amount of useful information in recent years. However, the web technologies that have enabled sources to share their information have also made it easy for sources to copy from each other and often publish without proper attribution. Understanding the copying relationships between sources has many benefits, including helping data providers protect their own rights, improving various aspects of data integration, and facilitating in-depth analysis of information flow.\n The importance of copy detection has led to a substantial amount of research in many disciplines of Computer Science, based on the type of information considered, such as text, images, videos, software code, and structured data. This tutorial explores the similarities and differences between the techniques proposed for copy detection across the different types of information. We also examine the computational challenges associated with large-scale copy detection, indicating how they could be detected efficiently, and identify a range of open problems for the community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "ec184a34c0bfa161c589ff12ee0633f8056317d5",
            "title": "Automatic discovery of attributes in relational databases",
            "abstract": "In this work we design algorithms for clustering relational columns into attributes, i.e., for identifying strong relationships between columns based on the common properties and characteristics of the values they contain. For example, identifying whether a certain set of columns refers to telephone numbers versus social security numbers, or names of customers versus names of nations. Traditional relational database schema languages use very limited primitive data types and simple foreign key constraints to express relationships between columns. Object oriented schema languages allow the definition of custom data types; still, certain relationships between columns might be unknown at design time or they might appear only in a particular database instance. Nevertheless, these relationships are an invaluable tool for schema matching, and generally for better understanding and working with the data. Here, we introduce data oriented solutions (we do not consider solutions that assume the existence of any external knowledge) that use statistical measures to identify strong relationships between the values of a set of columns. Interpreting the database as a graph where nodes correspond to database columns and edges correspond to column relationships, we decompose the graph into connected components and cluster sets of columns into attributes. To test the quality of our solution, we also provide a comprehensive experimental evaluation using real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1793178",
                    "name": "Meihui Zhang"
                },
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "1693070",
                    "name": "B. Ooi"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "f212aab3f609f7c2300ceb5e6e3b56e5efa2eaac",
            "title": "Summary graphs for relational database schemas",
            "abstract": "Increasingly complex databases need ever more sophisticated tools to help users understand their schemas and interact with the data. Existing tools fall short of either providing the \"big picture,\" or of presenting useful connectivity information. In this paper we define summary graphs, a novel approach for summarizing schemas. Given a set of user-specified query tables, the summary graph automatically computes the most relevant tables and joins for that query set. The output preserves the most informative join paths between the query tables, while meeting size constraints. In the process, we define a novel information-theoretic measure over join edges. Unlike most subgraph extraction work, we allow metaedges (i.e., edges in the transitive closure) to help reduce output complexity. We prove that the problem is NP-Hard, and solve it as an integer program. Our extensive experimental study shows that our method returns high-quality summaries under independent quality measures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112090937",
                    "name": "Xiaoyan Yang"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "fca21657698a21c74e4b989e8b2061de116ae7f8",
            "title": "We challenge you to certify your updates",
            "abstract": "Correctness of data residing in a database is vital. While integrity constraint enforcement can often ensure data consistency, it is inadequate to protect against updates that involve careless, unintentional errors, e.g., whether a specified update to an employee's record was for the intended employee. We propose a novel approach that is complementary to existing integrity enforcement techniques, to guard against such erroneous updates.\n Our approach is based on (a) updaters providing an update certificate with each database update, and (b) the database system verifying the correctness of the update certificate provided before performing the update. We formalize a certificate as a (challenge, response) pair, and characterize good certificates as those that are easy for updaters to provide and, when correct, give the system enough confidence that the update was indeed intended. We present algorithms that efficiently enumerate good challenges, without exhaustively exploring the search space of all challenges. We experimentally demonstrate that (i) databases have many good challenges, (ii) these challenges can be efficiently identified, (iii) certificates can be quickly verified for correctness, (iv) under natural models of an updater's knowledge of the database, update certificates catch a high percentage of the erroneous updates without imposing undue burden on the updaters performing correct updates, and (v) our techniques are robust across a wide range of challenge parameter settings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111240614",
                    "name": "Su Chen"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "0d60fce590359780f62017e472fd4523a5de3f5f",
            "title": "Prediction Promotes Privacy in Dynamic Social Networks",
            "abstract": "Recent work on anonymizing online social networks (OSNs) has looked at privacy preserving techniques for publishing a single instance of the network. However, OSNs evolve and a single instance is inadequate for analyzing their evolution or performing longitudinal data analysis. We study the problem of repeatedly publishing OSN data as the network evolves while preserving privacy of users. Publishing multiple instances independently has privacy risks, since stitching the information together may allow an adversary to identify users. We provide methods to anonymize a dynamic network when new nodes and edges are added to the published network. These methods use link prediction algorithms to model the evolution. Using this predicted graph to perform group-based anonymization, the loss in privacy caused by new edges can be eliminated almost entirely. We propose metrics for privacy loss, and evaluate them for publishing multiple OSN instances.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738297",
                    "name": "Smriti Bhagat"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "17b2424cb7ec084397da11f164e396736db1fd73",
            "title": "Processing top-k join queries",
            "abstract": "We consider the problem of efficiently finding the top-k answers for join queries over web-accessible databases. Classical algorithms for finding top-k answers use branch-and-bound techniques to avoid computing scores of all candidates in identifying the top-k answers. To be able to apply such techniques, it is critical to efficiently compute (lower and upper) bounds and expected scores of candidate answers in an incremental fashion during the evaluation. In this paper, we describe novel techniques for these problems. The first contribution of this paper is a method to efficiently compute bounds for the score of a query result when tuples in tables from the \"FROM\" clause are discovered incrementally, through either sorted or random access. Our second contribution is an algorithm that, given a set of partially evaluated candidate answers, determines a good order in which to access the tables to minimize wasted efforts in the computation of top-k answers. We evaluate our algorithms on a variety of queries and data sets and demonstrate the significant benefits they provide.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2147773",
                    "name": "Minji Wu"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "144173226",
                    "name": "A. Marian"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "193d61f77e426914deac12799adbe3ac466eb556",
            "title": "Minimizing minimality and maximizing utility",
            "abstract": "The principle of anonymization for data sharing has become a very popular paradigm for the preservation of privacy of the data subjects. Since the introduction of k-anonymity, dozens of methods and enhanced privacy definitions have been proposed. However, over-eager attempts to minimize the information lost by the anonymization potentially allow private information to be inferred. Proof-of-concept of this \"minimality attack\" has been demonstrated for a variety of algorithms and definitions [16]. In this paper, we provide a comprehensive analysis and study of this attack, and demonstrate that with care its effect can be almost entirely countered. The attack allows an adversary to increase his (probabilistic) belief in certain facts about individuals over the data. We show that (a) a large class of algorithms are not affected by this attack, (b) for a class of algorithms that have a \"symmetric\" property, the attacker's belief increases by at most a small constant, and (c) even for an algorithm chosen to be highly susceptible to the attack, the attacker's belief when using the attack increases by at most a small constant factor. We also provide a series of experiments that show in all these cases that the confidence about the sensitive value of any individual remains low in practice, while the published data is still useful for its intended purpose. From this, we conclude that the impact of such method-based attacks can be minimized.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1740882",
                    "name": "Ninghui Li"
                },
                {
                    "authorId": "48779264",
                    "name": "Tiancheng Li"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "2dfbd860542f79240f8f58cc042db1dfa562053a",
            "title": "Efficient identification of coupled entities in document collections",
            "abstract": "The relentless pace at which textual data are generated on-line necessitates novel paradigms for their understanding and exploration. To this end, we introduce a methodology for discovering strong entity associations in all the slices (meta-data value restrictions) of a document collection. Since related documents mention approximately the same group of core entities (people, locations, etc.), the groups of coupled entities discovered can be used to expose themes in the document collection. We devise and evaluate algorithms capable of addressing two flavors of our core problem: algorithm THR-ENT for computing all sufficiently strong entity associations and algorithm TOP-ENT for computing the top-k strongest entity associations, for each slice of the document collection.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1796624",
                    "name": "Nikos Sarkas"
                },
                {
                    "authorId": "143652721",
                    "name": "A. Angel"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "3741b589d0016f0f4814bf301258c358a5981859",
            "title": "Load-balanced query dissemination in privacy-aware online communities",
            "abstract": "We propose a novel privacy-preserving distributed infrastructure in which data resides only with the publishers owning it. The infrastructure disseminates user queries to publishers, who answer them at their own discretion. The infrastructure enforces a publisher k-anonymity guarantee, which prevents leakage of information about which publishers are capable of answering a certain query. Given the virtual nature of the global data collection, we study the challenging problem of efficiently locating publishers in the community that contain data items matching a specified query. We propose a distributed index structure, UQDT, that is organized as a union of Query Dissemination Trees (QDTs), and realized on an overlay (i.e., logical) network infrastructure. Each QDT has data publishers as its leaf nodes, and overlay network nodes as its internal nodes; each internal node routes queries to publishers, based on a summary of the data advertised by publishers in its subtrees. We experimentally evaluate design tradeoffs, and demonstrate that UQDT can maximize throughput by preventing any overlay network node from becoming a bottleneck.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3260693",
                    "name": "Emiran Curtmola"
                },
                {
                    "authorId": "50136367",
                    "name": "Alin Deutsch"
                },
                {
                    "authorId": "2060800615",
                    "name": "K. K. Ramakrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "7168749ae370edf27568ee16a89d0cf31b61f211",
            "title": "Enabling real time data analysis",
            "abstract": "Network-based services have become a ubiquitous part of our lives, to the point where individuals and businesses have often come to critically rely on them. Building and maintaining such reliable, high performance network and service infrastructures requires the ability to rapidly investigate and resolve complex service and performance impacting issues. To achieve this, it is important to collect, correlate and analyze massive amounts of data from a diverse collection of data sources in real time. We have designed and implemented a variety of data systems at AT&T Labs-Research to build highly scalable databases that support real time data collection, correlation and analysis, including (a) the Daytona data management system, (b) the DataDepot data warehousing system, (c) the GS tool data stream management system, and (d) the Bistro data feed manager. Together, these data systems have enabled the creation and maintenance of a data warehouse and data analysis infrastructure for troubleshooting complex issues in the network. We describe these data systems and their key research contributions in this paper.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "720061ae3c6d22f7c72481d0df215496f2e4fa66",
            "title": "Record linkage with uniqueness constraints and erroneous values",
            "abstract": "Many data-management applications require integrating data from a variety of sources, where different sources may refer to the same real-world entity in different ways and some may even provide erroneous data. An important task in this process is to recognize and merge the various references that refer to the same entity. In practice, some attributes satisfy a uniqueness constraint---each real-world entity (or most entities) has a unique value for the attribute (e.g., business contact phone, address, and email). Traditional techniques tackle this case by first linking records that are likely to refer to the same real-world entity, and then fusing the linked records and resolving conflicts if any. Such methods can fall short for three reasons: first, erroneous values from sources may prevent correct linking; second, the real world may contain exceptions to the uniqueness constraints and always enforcing uniqueness can miss correct values; third, locally resolving conflicts for linked records may overlook important global evidence. This paper proposes a novel technique to solve this problem. The key component of our solution is to reduce the problem into a k-partite graph clustering problem and consider in clustering both similarity of attribute values and the sources that associate a pair of values in the same record. Thus, we perform global linkage and fusion simultaneously, and can identify incorrect values and differentiate them from alternative representations of the correct value from the beginning. In addition, we extend our algorithm to be tolerant to a few violations of the uniqueness constraints. Experimental results show accuracy and scalability of our technique.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2582204",
                    "name": "Songtao Guo"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "144952466",
                    "name": "R. Zajac"
                }
            ]
        },
        {
            "paperId": "7e9c39d9c158b79a94beb9ad86a028295f22fee3",
            "title": "Advances in Web Technologies and Applications - Proceedings of the 12th Asia-Pacific Web Conference, APWeb 2010: Message from the general chairs and program chairs",
            "abstract": "It is our great pleasure to welcome you to Busan for the 12 International Asia Pacific Web Conference (APWeb 2010). This year\u2019s conference was held in Busan (also known as Pusan), which is Korea\u2019s principal port and the second largest city. This is the first time that APWeb is held outside China. With numerous culturally attractive tourist destinations, scenic coastal areas, and clean sand beaches, Busan is also famous for having hosted such international events as the 14 Asian Games in 2002, several matches of the 2002 Korea/Japan World Cup, and the Pusan International Film Festival.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "24302099",
                    "name": "B. Hong"
                },
                {
                    "authorId": "40035608",
                    "name": "Lizhu Zhou"
                },
                {
                    "authorId": "48667278",
                    "name": "Xiaofang Zhou"
                },
                {
                    "authorId": "144422954",
                    "name": "Wook-Shin Han"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "145024112",
                    "name": "Ge Yu"
                }
            ]
        },
        {
            "paperId": "8a97b67c7d85ce52b72256310256844e4ab5f3c9",
            "title": "Schema extraction",
            "abstract": "Understanding the schema of a complex database is a crucial step in exploratory data analysis. However, gaining such an understanding is challenging for new users for many reasons. First, complex databases often have thousands of inter-linked tables, with little indication of the important tables or the main concepts in the database schema. Second, schemas can be inaccurate, e.g., some foreign/primary key relationships are not known to designers but are inherent in the data, while others become invalid due to data inconsistencies. In this talk, we present an approach to effectively address these challenges and automatically extract an understandable schema from a complex database. The first step in our approach is a robust algorithm to discover foreign/primary key relationships between tables. We present a general rule, termed Randomness, that subsumes a variety of other rules proposed in previous work, and develop efficient approximation algorithms for evaluating randomness, using only two passes over the data. The second step is a principled approach to summarize the schema consisting of tables linked using foreign/primary keys, so that a user can easily identify the main concepts and important tables. We present an information theoretic approach to identify important tables, and an intuitive notion of table similarity that can be used to cluster tables into the main concepts of the schema. We validate our approach using real and synthetic datasets. This is based on joint work [1, 2] with Marios Hadjieleftheriou, Beng Chin Ooi, Cecilia M. Procopiuc, Xiaoyan Yang and Meihui Zhang.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "8e884e35148a1d0b27bd677cb4dd7ee96fc2049b",
            "title": "On multi-column foreign key discovery",
            "abstract": "A foreign/primary key relationship between relational tables is one of the most important constraints in a database. From a data analysis perspective, discovering foreign keys is a crucial step in understanding and working with the data. Nevertheless, more often than not, foreign key constraints are not specified in the data, for various reasons; e.g., some associations are not known to designers but are inherent in the data, while others become invalid due to data inconsistencies. This work proposes a robust algorithm for discovering single-column and multi-column foreign keys. Previous work concentrated mostly on discovering single-column foreign keys using a variety of rules, like inclusion dependencies, column names, and minimum/maximum values. We first propose a general rule, termed Randomness, that subsumes a variety of other rules. We then develop efficient approximation algorithms for evaluating randomness, using only two passes over the data. Finally, we validate our approach via extensive experiments using real and synthetic datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1793178",
                    "name": "Meihui Zhang"
                },
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "1693070",
                    "name": "B. Ooi"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "93599e13bfadc957fa0f60d9bda0ed28c764f882",
            "title": "Privacy in dynamic social networks",
            "abstract": "Anonymization of social networks before they are published or shared has become an important research question. Recent work on anonymizing social networks has looked at privacy preserving techniques for publishing a single instance of the network. However, social networks evolve and a single instance is inadequate for analyzing the evolution of the social network or for performing any longitudinal data analysis. We study the problem of repeatedly publishing social network data as the network evolves, while preserving privacy of users. Publishing multiple instances of the same network independently has privacy risks, since stitching the information together may allow an adversary to identify users in the networks.\n We propose methods to anonymize a dynamic network such that the privacy of users is preserved when new nodes and edges are added to the published network. These methods make use of link prediction algorithms to model the evolution of the social network. Using this predicted graph to perform group-based anonymization, the loss in privacy caused by new edges can be reduced. We evaluate the privacy loss on publishing multiple social network instances using our methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738297",
                    "name": "Smriti Bhagat"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "145846952",
                    "name": "B. Krishnamurthy"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "a20ac76b2a4ab737f02e20d6628aa88ff112b756",
            "title": "Bed-tree: an all-purpose index structure for string similarity search based on edit distance",
            "abstract": "Strings are ubiquitous in computer systems and hence string processing has attracted extensive research effort from computer scientists in diverse areas. One of the most important problems in string processing is to efficiently evaluate the similarity between two strings based on a specified similarity measure. String similarity search is a fundamental problem in information retrieval, database cleaning, biological sequence analysis, and more. While a large number of dissimilarity measures on strings have been proposed, edit distance is the most popular choice in a wide spectrum of applications. Existing indexing techniques for similarity search queries based on edit distance, e.g., approximate selection and join queries, rely mostly on n-gram signatures coupled with inverted list structures. These techniques are tailored for specific query types only, and their performance remains unsatisfactory especially in scenarios with strict memory constraints or frequent data updates. In this paper we propose the Bed-tree, a B+-tree based index structure for evaluating all types of similarity queries on edit distance and normalized edit distance. We identify the necessary properties of a mapping from the string space to the integer space for supporting searching and pruning for these queries. Three transformations are proposed that capture different aspects of information inherent in strings, enabling efficient pruning during the search process on the tree. Compared to state-of-the-art methods on string similarity search, the Bed-tree is a complete solution that meets the requirements of all applications, providing high scalability and fast response time.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109512262",
                    "name": "Zhenjie Zhang"
                },
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "1693070",
                    "name": "B. Ooi"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "b8c94334a5f989e107c306a5e4403385bc338545",
            "title": "Weighted Set-Based String Similarity",
            "abstract": "Consider a universe of tokens, each of which is associated with a weight, and a database consisting of strings that can be represented as subsets of these tokens. Given a query string, also represented as a set of tokens, a weighted string similarity query identi\ufb01es all strings in the database whose similarity to the query is larger than a user speci\ufb01ed threshold. Weighted string similarity queries are useful in applications like data cleaning and integration for \ufb01nding approximate matches in the presence of typographical mistakes, multiple formatting conventions, data transformation errors, etc. We show that this problem has semantic properties that can be exploited to design index structures that support very ef\ufb01cient algorithms for query answering.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "cbf6d58cefa51724910d2dc573dcd08b578d1873",
            "title": "Data Auditor",
            "abstract": "We present Data Auditor, a tool for exploring data quality and data semantics. Given a rule or an integrity constraint and a target relation, Data Auditor computes pattern tableaux, which concisely summarize subsets of the relation that (mostly) satisfy or (mostly) fail the constraint. This paper describes 1) the architecture and user interface of Data Auditor, 2) the supported constraints for testing data consistency and completeness, 3) the heuristics used by Data Auditor to \"tune\" a given constraint or its associated parameters for better fit with the data, and 4) several demonstration scenarios. using real data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "1735932",
                    "name": "H. Karloff"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "d4e3c9cbbab2938ae897877ea10f3e33d01b4498",
            "title": "I4E: interactive investigation of iterative information extraction",
            "abstract": "Information extraction systems are increasingly being used to mine structured information from unstructured text documents. A commonly used unsupervised technique is to build iterative information extraction (IIE) systems that learn task-specific rules, called patterns, to generate the desired tuples. Oftentimes, output from an information extraction system may contain unexpected results which may be due to an incorrect pattern, incorrect tuple, or both. In such scenarios, users and developers of the extraction system could greatly benefit from an investigation tool that can quickly help them reason about and repair the output. In this paper, we develop an approach for interactive post-extraction investigation for IIE systems. We formalize three important phases of this investigation, namely, explain the IIE result, diagnose the influential and problematic components, and repair the output from an information extraction system. We show how to characterize the execution of an IIE system and build a suite of algorithms to answer questions pertaining to each of these phases. We experimentally evaluate our proposed approach over several domains over a Web corpus of about 500 million documents. We show that our approach effectively enables post-extraction investigation, while maximizing the gain from user and developer interaction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1753537",
                    "name": "A. Sarma"
                },
                {
                    "authorId": "1777385",
                    "name": "Alpa Jain"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "fa24cbdfcee53529aacdcb791f6e4e61c5d5b4ee",
            "title": "Global detection of complex copying relationships between sources",
            "abstract": "Web technologies have enabled data sharing between sources but also simplified copying (and often publishing without proper attribution). The copying relationships can be complex: some sources copy from multiple sources on different subsets of data; some co-copy from the same source, and some transitively copy from another. Understanding such copying relationships is desirable both for business purposes and for improving many key components in data integration, such as resolving conflicts across various sources, reconciling distinct references to the same real-world entity, and efficiently answering queries over multiple sources. Recent works have studied how to detect copying between a pair of sources, but the techniques can fall short in the presence of complex copying relationships. In this paper we describe techniques that discover global copying relationships between a set of structured sources. Towards this goal we make two contributions. First, we propose a global detection algorithm that identifies co-copying and transitive copying, returning only source pairs with direct copying. Second, global detection requires accurate decisions on copying direction; we significantly improve over previous techniques on this by considering various types of evidence for copying and correlation of copying on different data items. Experimental results on real-world data and synthetic data show high effectiveness and efficiency of our techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "50819900",
                    "name": "Yifan Hu"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "16cd112b8f3283a5f915b57c3e56ad10ffb45b09",
            "title": "Class-based graph anonymization for social network data",
            "abstract": "The recent rise in popularity of social networks, such as Facebook and MySpace, has created large quantities of data about interactions within these networks. Such data contains many private details about individuals so anonymization is required prior to attempts to make the data more widely available for scientific research. Prior work has considered simple graph data to be anonymized by removing all non-graph information and adding or deleting some edges. Since social network data is richer in details about the users and their interactions, loss of details due to anonymization limits the possibility for analysis. We present a new set of techniques for anonymizing social network data based on grouping the entities into classes, and masking the mapping between entities and the nodes that represent them in the anonymized graph. Our techniques allow queries over the rich data to be evaluated with high accuracy while guaranteeing resilience to certain types of attack. To prevent inference of interactions, we rely on a critical \"safety condition\" when forming these classes. We demonstrate utility via empirical data from social networking settings. We give examples of complex queries that may be posed and show that they can be answered over the anonymized data efficiently and accurately.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1738297",
                    "name": "Smriti Bhagat"
                },
                {
                    "authorId": "145846952",
                    "name": "B. Krishnamurthy"
                }
            ]
        },
        {
            "paperId": "1bbf70755a1716b283fec3a747d1d978faa64bb8",
            "title": "Forward Decay: A Practical Time Decay Model for Streaming Systems",
            "abstract": "Temporal data analysis in data warehouses and datastreaming systems often uses time decay to reduce the importance of older tuples, without eliminating their influence, on the results of the analysis. While exponential time decay is commonly used in practice, other decay functions (e.g. polynomial decay) are not, even though they have been identified as useful. We argue that this is because the usual definitions of time decay are \"backwards\": the decayed weight of a tuple is based on its age, measured backward from the current time. Since this age is constantly changing, such decay is too complex and unwieldy for scalable implementation. In this paper, we propose a new class of \"forward\" decay functions based on measuring forward from a fixed point in time. We show that this model captures the more practical models already known, such as exponential decay and landmark windows, but also includes a wide class of other types of time decay. We provide efficient algorithms to compute a variety of aggregates and draw samples under forward decay, and show that these are easy to implement scalably. Further, we provide empirical evidence that these can be executed in a production data stream management system with little or no overhead compared to the undecayed computations. Our implementation required no extensions to the query language or the DSMS, demonstrating that forward decay represents a practical model of time decay for systems that deal with time-based data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1745049",
                    "name": "Vladislav Shkapenyuk"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "39719831",
                    "name": "Bojian Xu"
                }
            ]
        },
        {
            "paperId": "232d3305b2c4c68a57b357c2cdea7c754891ebcb",
            "title": "Metric Functional Dependencies",
            "abstract": "When merging data from various sources, it is often the case that small variations in data format and interpretation cause traditional functional dependencies (FDs) to be violated, without there being an intrinsic violation of semantics. Examples include differing address formats, or different reported latitude/longitudes for a given address. In this paper, we define metric functional dependencies, which strictly generalize traditional FDs by allowing small differences (controlled by a metric) in values of the consequent attribute of an FD. We present efficient algorithms for the verification problem: determining whether a given metric FD holds for a given relation. We experimentally demonstrate the validity and efficiency of our approach on various data sets that lie in multidimensional spaces.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "3157944",
                    "name": "A. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1747652",
                    "name": "Suresh Venkatasubramanian"
                }
            ]
        },
        {
            "paperId": "245cd97fc34fad6d6e7efb373874e5a42576adfe",
            "title": "What's on the grapevine?",
            "abstract": "User generated content and social media (in the form of blogs, wikis, online video, microblogs, etc) are proliferating online. Grapevine conducts large scale data analysis on the social media collective, distilling and extracting information in real time. It aims to track entities and stories of interest in millions of blog posts, thousands of tweets, news items, etc., daily. Grapevine facilitates the interactive exploration of content, allowing users to discover interesting or surprising stories, optionally narrowed down on a specific demographic of interest (e.g. \"What are Torontonians talking about on blogs?\", \"What are popular stories across news sources in Canada?\", \"What are financiers in Texas blogging about today?\"). Stories of interest can be explored in a variety of ways, such as modifying their scope, obtaining related content (blog posts, news, etc), and examining their temporal evolution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143652721",
                    "name": "A. Angel"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "1796624",
                    "name": "Nikos Sarkas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "35f8bdfa4357e4736fbe84a1622d9a1ac7854089",
            "title": "Efficient Table Anonymization for Aggregate Query Answering",
            "abstract": "Privacy protection is a major concern when microdata is released for ad hoc analyses. Anonymization schemes have to guarantee privacy goals, as well as preserve sufficient information to support reasonably accurate answers to ad hoc queries. In this paper, we focus on the case when the sensitive attributes are numerical (e.g., salary) for which $(k,e)$-anonymity was shown to be an appropriate privacy goal. We develop efficient algorithms for two optimization criteria for $(k,e)$-anonymity schemes, significantly improving on previous results. We evaluate our methods on a large real dataset, and show that they are scalable and accurate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "3b297cf2298d5e68c8d959a3607df63d19d2d55b",
            "title": "Recommending Join Queries via Query Log Analysis",
            "abstract": "Complex ad hoc join queries over enterprise databases are commonly used by business data analysts to understand and analyze a variety of enterprise-wide processes. However, effectively formulating such queries is a challenging task for human users, especially over databases that have large, heterogeneous schemas. In this paper, we propose a novel approach to automatically create join query recommendations based on input-output specifications (i.e.,input tables on which selection conditions are imposed, and output tables whose attribute values must be in the result of the query).The recommended join query graph includes (i) \"intermediate'' tables, and (ii) join conditions that connect the input and output tables via the intermediate tables. Our method is based on analyzing an existing query log over the enterprise database. Borrowing from program slicing techniques, which extract parts of a program that affect the value of a given variable, we first extract \"query slices'' from each query in the log. Given a user specification, we then re-combine appropriate slices to create a new join query graph, which connects the sets of input and output tables via the intermediate tables. We propose and study several quality measures to enable choosing a good join query graph among the many possibilities. Each measure expresses an intuitive notion that there should be sufficient evidence in the log to support our recommendation of the join query graph. We conduct an extensive study using the log of an actual enterprise database system to demonstrate the viability of our novel approach for recommending join queries.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112090937",
                    "name": "Xiaoyan Yang"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "5af0067f72d9599afe6654a0cd9ae41bd258b813",
            "title": "Weighted Set Similarity: Queries and Updates",
            "abstract": "Consider a universe of items, each of which is associated with a weight, and a database consisting of subsets of these items. Given a query set, a weighted set similarity query identifies either (i) all sets in the database whose normalized similarity to the query set is above a pre-specified threshold, or (ii) the sets in the database with the k highest similarity values to the query set. Weighted set similarity queries are useful in applications like data cleaning and integration for finding approximate matches in the presence of typographical mistakes, multiple formatting conventions, transformation errors, etc. We show that this problem has semantic properties that can be exploited to design index structures that support efficient algorithms for answering queries; these algorithms can achieve arbitrarily stronger pruning than the family of Threshold Algorithms. We describe how these index structures can beefficiently updated using lazy propagation in a way that gives strict guarantees on the quality of subsequent query answers. Finally, we illustrate that our proposed ideas work well in practice for real datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "5cbfee354762fcbb9a35ffd79c577630ce297300",
            "title": "Information theory for data management",
            "abstract": "We explore the use of information theory as a tool to express and quantify notions of information content and information transfer for representing and analyzing data, using examples from database design, data integration and data anonymization. We also examine the computational challenges associated with information-theoretic primitives, indicating how they might be computed efficiently.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1747652",
                    "name": "Suresh Venkatasubramanian"
                }
            ]
        },
        {
            "paperId": "63566facc1d0381b79b3e0504867a16b7e783b12",
            "title": "Enabling Content Dissemination Using Efficient and Scalable Multicast",
            "abstract": "Multicast is an approach that uses network and server resources efficiently to distribute information to groups. As networks evolve to become information-centric, users will increasingly demand publish-subscribe based access to fine-grained information, and multicast will need to evolve to (i) manage an increasing number of groups, with a distinct group for each piece of distributable content; (ii) support persistent group membership, as group activity can vary over time, with intense activity at some times, and infrequent (but still critical) activity at others. These requirements raise scalability challenges that are not met by today's multicast techniques. In this paper, we propose the MAD (multicast with adaptive dual-state) architecture to provide efficient multicast service at massive scale. MAD can scalably support a vast number of multicast groups, with varying activity over time, based on two key novel ideas: (i) decouple group membership from forwarding information, and (ii) apply an adaptive dual-state approach to optimize for the different objectives of active and inactive groups. We focus on the scalability characteristics of MAD and demonstrate through analysis, simulation and implementation that the architecture achieves high performance and efficiency.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2912169",
                    "name": "T. Cho"
                },
                {
                    "authorId": "145503699",
                    "name": "M. Rabinovich"
                },
                {
                    "authorId": "2060800615",
                    "name": "K. K. Ramakrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "40600407",
                    "name": "Yin Zhang"
                }
            ]
        },
        {
            "paperId": "66ff64274b57caf3fb573b0ef44feef56ebb913a",
            "title": "Truth Discovery and Copying Detection in a Dynamic World",
            "abstract": "Modern information management applications often require integrating data from a variety of data sources, some of which may copy or buy data from other sources. When these data sources model a dynamically changing world (e.g., people's contact information changes over time, restaurants open and go out of business), sources often provide out-of-date data. Errors can also creep into data when sources are updated often. Given out-of-date and erroneous data provided by different, possibly dependent, sources, it is challenging for data integration systems to provide the true values. Straightforward ways to resolve such inconsistencies (e.g., voting) may lead to noisy results, often with detrimental consequences. \n \nIn this paper, we study the problem of finding true values and determining the copying relationship between sources, when the update history of the sources is known. We model the quality of sources over time by their coverage, exactness and freshness. Based on these measures, we conduct a probabilistic analysis. First, we develop a Hidden Markov Model that decides whether a source is a copier of another source and identifies the specific moments at which it copies. Second, we develop a Bayesian model that aggregates information from the sources to decide the true value for a data item, and the evolution of the true values over time. Experimental results on both real-world and synthetic data show high accuracy and scalability of our techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "70fd8392d104ef35d85b07fbfab1990f3e4145a2",
            "title": "Integrating Conflicting Data: The Role of Source Dependence",
            "abstract": "Many data management applications, such as setting up Web portals, managing enterprise data, managing community data, and sharing scientific data, require integrating data from multiple sources. Each of these sources provides a set of values and different sources can often provide conflicting values. To present quality data to users, it is critical that data integration systems can resolve conflicts and discover true values. Typically, we expect a true value to be provided by more sources than any particular false one, so we can take the value provided by the majority of the sources as the truth. Unfortunately, a false value can be spread through copying and that makes truth discovery extremely tricky. In this paper, we consider how to find true values from conflicting information when there are a large number of sources, among which some may copy from others. \n \nWe present a novel approach that considers dependence between data sources in truth discovery. Intuitively, if two data sources provide a large number of common values and many of these values are rarely provided by other sources (e.g., particular false values), it is very likely that one copies from the other. We apply Bayesian analysis to decide dependence between sources and design an algorithm that iteratively detects dependence and discovers truth from conflicting information. We also extend our model by considering accuracy of data sources and similarity between values. Our experiments on synthetic data as well as real-world data show that our algorithm can significantly improve accuracy of truth discovery and is scalable when there are a large number of data sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "83d6701fccd057ca78645db7a0e72c219c5e869d",
            "title": "Sailing the Information Ocean with Awareness of Currents: Discovery and Application of Source Dependence",
            "abstract": "The Web has enabled the availability of a huge amount of useful information, but has also eased the ability to spread false information and rumors across multiple sources, making it hard to distinguish between what is true and what is not. Recent examples include the premature Steve Jobs obituary, the second bankruptcy of United airlines, the creation of Black Holes by the operation of the Large Hadron Collider, etc. Since it is important to permit the expression of dissenting and conflicting opinions, it would be a fallacy to try to ensure that the Web provides only consistent information. However, to help in separating the wheat from the chaff, it is essential to be able to determine dependence between sources. Given the huge number of data sources and the vast volume of conflicting data available on the Web, doing so in a scalable manner is extremely challenging and has not been addressed by existing work yet. \nIn this paper, we present a set of research problems and propose some preliminary solutions on the issues involved in discovering dependence between sources. We also discuss how this knowledge can benefit a variety of technologies, such as data integration and Web 2.0, that help users manage and access the totality of the available information from various sources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1753537",
                    "name": "A. Sarma"
                },
                {
                    "authorId": "145867172",
                    "name": "X. Dong"
                },
                {
                    "authorId": "144173226",
                    "name": "A. Marian"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "92f8040989ed42649f8e7dca762eaa719f5bd0c4",
            "title": "Sequential Dependencies",
            "abstract": "We study sequential dependencies that express the semantics of data with ordered domains and help identify quality problems with such data. Given an interval g, we write X \u2192g Y to denote that the difference between the Y -attribute values of any two consecutive records, when sorted on X, must be in g. For example, time \u2192(0,\u221e) sequence_number indicates that sequence numbers are strictly increasing over time, whereas sequence_number \u2192[4, 5] time means that the time \"gaps\" between consecutive sequence numbers are between 4 and 5. Sequential dependencies express relationships between ordered attributes, and identify missing (gaps too large), extraneous (gaps too small) and out-of-order data. \n \nTo make sequential dependencies applicable to real-world data, we relax their requirements and allow them to hold approximately (with some exceptions) and conditionally (on various subsets of the data). This paper proposes the notion of conditional approximate sequential dependencies and provides an efficient framework for discovering pattern tableaux, which are compact representations of the subsets of the data (i.e., ranges of values of the ordered attributes) that satisfy the underlying dependency. We present analyses of our proposed algorithms, and experiments on real data demonstrating the efficiency and utility of our framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "1735932",
                    "name": "H. Karloff"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "3157944",
                    "name": "A. Saha"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "aff0b9fcb63ed25fa923b37e19a1b79de7a62737",
            "title": "Estimating the confidence of conditional functional dependencies",
            "abstract": "Conditional functional dependencies (CFDs) have recently been proposed as extensions of classical functional dependencies that apply to a certain subset of the relation, as specified by a pattern tableau. Calculating the support and confidence of a CFD (i.e., the size of the applicable subset and the extent to which it satisfies the CFD)gives valuable information about data semantics and data quality. While computing the support is easier, computing the confidence exactly is expensive if the relation is large, and estimating it from a random sample of the relation is unreliable unless the sample is large. We study how to efficiently estimate the confidence of a CFD with a small number of passes (one or two) over the input using small space. Our solutions are based on a variety of sampling and sketching techniques, and apply when the pattern tableau is known in advance, and also the harder case when this is given after the data have been seen. We analyze our algorithms, and show that they can guarantee a small additive error; we also show that relative errors guarantees are not possible. We demonstrate the power of these methods empirically, with a detailed study using both real and synthetic data. These experiments show that it is possible to estimate the CFD confidence very accurately with summaries which are much smaller than the size of the data they represent.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "144078750",
                    "name": "A. Mcgregor"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2108286220",
                    "name": "Xi Zhang"
                }
            ]
        },
        {
            "paperId": "bbdb8bb0f741682f604c5c5049ca4ca64c436461",
            "title": "Anonymized Data: Generation, models, usage",
            "abstract": "Data anonymization techniques enable publication of detailed information, which permits ad hoc queries and analyses, while guaranteeing the privacy of sensitive information in the data against a variety of attacks. In this tutorial, we aim to present a unified framework of data anonymization techniques, viewed through the lens of data uncertainty. Essentially, anonymized data describes a set of possible worlds that include the original data. We show that anonymization approaches generate different working models of uncertain data, and that the privacy guarantees offered by k-anonymization and l-diversity can be naturally understood in terms of the sets of possible worlds that correspond to the anonymized data. Work in query evaluation over uncertain databases can hence be used for answering ad hoc queries over anonymized data. We identify new research problems for both the Data Anonymization and the Uncertain Data communities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "bec8eba65bdd0a5cd9c43e69ad8e2c47acfa7c71",
            "title": "Exploring a Few Good Tuples from Text Databases",
            "abstract": "Information extraction from text databases is a useful paradigm to populate relational tables and unlock the considerable value hidden in plain-text documents. However, information extraction can be expensive, due to various complex text processing steps necessary in uncovering the hidden data. There are a large number of text databases available, and not every text database is necessarily relevant to every relation. Hence, it is important to be able to quickly explore the utility of running an extractor for a specific relation over a given text database before carrying out the expensive extraction task. In this paper, we present a novel exploration methodology of {\\em finding a few good tuples} for a relation that can be extracted from a database which allows for judging the relevance of the database for the relation. Specifically, we propose the notion of a good(k, $\\ell$) query as one that can return any $k$ tuples for a relation among the top-$\\ell$ fraction of tuples ranked by their aggregated confidence scores, provided by the extractor; if these tuples have high scores, the database can be determined as relevant to the relation. We formalize the access model for information extraction, and investigate efficient query processing algorithms for good(k, $\\ell$) queries, which do not rely on any prior knowledge about the extraction task or the database. We demonstrate the viability of our algorithms using a detailed experimental study with real text databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1777385",
                    "name": "Alpa Jain"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "c00e030aae20f859ab65356a4cd804dcf4f427d9",
            "title": "Summarizing Relational Databases",
            "abstract": "Complex databases are challenging to explore and query by users unfamiliar with their schemas. Enterprise databases often have hundreds of inter-linked tables, so even when extensive documentation is available, new users must spend a considerable amount of time understanding the schema before they can retrieve any information from the database. The problem is aggravated if the documentation is missing or outdated, which may happen with legacy databases. \n \nIn this paper we identify limitations of previous approaches to address this vexing problem, and propose a principled approach to summarizing the contents of a relational database, so that a user can determine at a glance the type of information it contains, and the main tables in which that information resides. Our approach has three components: First, we define the importance of each table in the database as its stable state value in a random walk over the schema graph, where the transition probabilities depend on the entropies of table attributes. This ensures that the importance of a table depends both on its information content, and on how that content relates to the content of other tables in the database. Second, we define a metric space over the tables in a database, such that the distance function is consistent with an intuitive notion of table similarity. Finally, we use a Weighted k-Center algorithm under this distance function to cluster all tables in the database around the most relevant tables, and return the result as our summary. We conduct an extensive experimental study on a benchmark database, comparing our approach with previous methods, as well as with several hybrid models. We show that our approach not only achieves significantly higher accuracy than the previous state of the art, but is also faster and scales linearly with the size of the schema graph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112090937",
                    "name": "Xiaoyan Yang"
                },
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "c05d7d5ad5b154e5118e8e996a26898b6854eb6e",
            "title": "Optimization Techniques for Reactive Network Monitoring",
            "abstract": "We develop a framework for minimizing the communication overhead of monitoring global system parameters in IP networks and sensor networks. A global system predicate is defined as a conjunction of the local properties of different network elements. A typical example is to identify the time windows when the outbound traffic from each network element exceeds a predefined threshold. Our main idea is to optimize the scheduling of local event reporting across network elements for a given network traffic load and local event frequencies. The system architecture consists of N distributed network elements coordinated by a central monitoring station. Each network element monitors a set of local properties and the central station is responsible for identifying the status of global parameters registered in the system. We design an optimal algorithm, the partition and rank (PAR) scheme, when the local events are independent; whereas, when they are dependent, we show that the problem is NP-complete and develop two efficient heuristics: the PAR for dependent events (PAR-D) and adaptive (Ada) algorithms, which adapt well to changing network conditions, and outperform the current state of the art techniques in terms of communication cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1906024",
                    "name": "A. Bulut"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "2064008129",
                    "name": "Anand Meka"
                },
                {
                    "authorId": "1399890865",
                    "name": "Ambuj K. Singh"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "c35afdf22c1c24018056be668d030b2e3740b13d",
            "title": "CPM: Adaptive Video-on-Demand with Cooperative Peer Assists and Multicast",
            "abstract": "We present CPM, a unified approach that exploits server multicast, assisted by peer downloads, to provide efficient video-on-demand (VoD) in a service provider environment. We describe our architecture and show how CPM is designed to dynamically adapt to a wide range of situations including highly different peer-upload bandwidths, content popularity, user request arrival patterns, video library size, and subscriber population. We demonstrate the effectiveness of CPM using simulations (based on an actual implementation codebase) across the range of situations described above and show that CPM does significantly better than traditional unicast, different forms of multicast, as well as peer-to-peer schemes. Along with synthetic parameters, we augment our experiments using data from a deployed VoD service to evaluate the performance of CPM.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "34945576",
                    "name": "V. Gopalakrishnan"
                },
                {
                    "authorId": "1852352",
                    "name": "S. Bhattacharjee"
                },
                {
                    "authorId": "145922660",
                    "name": "K. Ramakrishnan"
                },
                {
                    "authorId": "145272661",
                    "name": "R. Jana"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "c4e6e0866952f6e15080018ddbd0dbfa05452b24",
            "title": "Distribution-based Microdata Anonymization",
            "abstract": "Before sharing to support ad hoc aggregate analyses, microdata often need to be anonymized to protect the privacy of individuals. A variety of privacy models have been proposed for microdata anonymization. Many of these models (e.g., t-closeness) essentially require that, after anonymization, groups of sensitive attribute values follow specified distributions. To support such models, in this paper we study the problem of transforming a group of sensitive attribute values to follow a certain target distribution with minimal data distortion. Specifically, we develop and evaluate a novel methodology that combines the use of sensitive attribute permutation and generalization with the addition of fake sensitive attribute values to achieve this transformation. We identify metrics related to accuracy of aggregate query answers over the transformed data, and develop efficient anonymization algorithms to optimize these accuracy metrics. Using a variety of data sets, we experimentally demonstrate the effectiveness of our techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "144478228",
                    "name": "Ting Yu"
                },
                {
                    "authorId": "2112724648",
                    "name": "Qing Zhang"
                }
            ]
        },
        {
            "paperId": "ca3522e319c74af3fb134dece09474767e6817e7",
            "title": "Type-based categorization of relational attributes",
            "abstract": "In this work we concentrate on categorization of relational attributes based on their data type. Assuming that attribute type/characteristics are unknown or unidentifiable, we analyze and compare a variety of type-based signatures for classifying the attributes based on the semantic type of the data contained therein (e.g., router identifiers, social security numbers, email addresses). The signatures can subsequently be used for other applications as well, like clustering and index optimization/compression. This application is useful in cases where very large data collections that are generated in a distributed, ungoverned fashion end up having unknown, incomplete, inconsistent or very complex schemata and schema level meta-data. We concentrate on heuristically generating type-based attribute signatures based on both local and global computation approaches. We show experimentally that by decomposing data into q-grams and then considering signatures based on q-gram distributions, we achieve very good classification accuracy under the assumption that a large sample of the data is available for building the signatures. Then, we turn our attention to cases where a very small sample of the data is available, and hence accurately capturing the q-gram distribution of a given data type is almost impossible. We propose techniques based on dimensionality reduction and soft-clustering that exploit correlations between attributes to improve classification accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "8238249",
                    "name": "B. Ahmadi"
                },
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "1732690",
                    "name": "T. Seidl"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1747652",
                    "name": "Suresh Venkatasubramanian"
                }
            ]
        },
        {
            "paperId": "db203ff6d53d14488f9463df4c737fa600a47e12",
            "title": "Incremental maintenance of length normalized indexes for approximate string matching",
            "abstract": "Approximate string matching is a problem that has received a lot of attention recently. Existing work on information retrieval has concentrated on a variety of similarity measures TF/IDF, BM25, HMM, etc.) specifically tailored for document retrieval purposes. As new applications that depend on retrieving short strings are becoming popular(e.g., local search engines like YellowPages.com, Yahoo!Local, and Google Maps) new indexing methods are needed, tailored for short strings. For that purpose, a number of indexing techniques and related algorithms have been proposed based on length normalized similarity measures. A common denominator of indexes for length normalized measures is that maintaining the underlying structures in the presence of incremental updates is inefficient, mainly due to data dependent, precomputed weights associated with each distinct token and string. Incorporating updates usually is accomplished by rebuilding the indexes at regular time intervals. In this paper we present a framework that advocates lazy update propagation with the following key feature: Efficient, incremental updates that immediately reflect the new data in the indexes in a way that gives strict guarantees on the quality of subsequent query answers. More specifically, our techniques guarantee against false negatives and limit the number of false positives produced. We implement a fully working prototype and illustrate that the proposed ideas work really well in practice for real datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "de18d42090d6b8fdda0344ee4e090193bad458b2",
            "title": "Small synopses for group-by query verification on outsourced data streams",
            "abstract": "Due to the overwhelming flow of information in many data stream applications, data outsourcing is a natural and effective paradigm for individual businesses to address the issue of scale. In the standard data outsourcing model, the data owner outsources streaming data to one or more third-party servers, which answer queries posed by a potentially large number of clients on the data owner's behalf. Data outsourcing intrinsically raises issues of trust, making outsourced query assurance on data streams a problem with important practical implications. Existing solutions proposed in this model all build upon cryptographic primitives such as signatures and collision-resistant hash functions, which only work for certain types of queries, for example, simple selection/aggregation queries.\n In this article, we consider another common type of queries, namely, \u201cGROUP BY, SUM\u201d queries, which previous techniques fail to support. Our new solutions are not based on cryptographic primitives, but instead use algebraic and probabilistic techniques to compute a small synopsis on the true query result, which is then communicated to the client so as to verify the correctness of the query result returned by the server. The synopsis uses a constant amount of space irrespective of the result size, has an extremely small probability of failure, and can be maintained using no extra space when the query result changes as elements stream by. We then generalize our synopsis to allow some tolerance on the number of erroneous groups, in order to support semantic load shedding on the server. When the number of erroneous groups is indeed tolerable, the synopsis can be strengthened so that we can locate and even correct these errors. Finally, we implement our techniques and perform an empirical evaluation using live network traffic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144744111",
                    "name": "K. Yi"
                },
                {
                    "authorId": "3245752",
                    "name": "Feifei Li"
                },
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "1715771",
                    "name": "G. Kollios"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "0899c2c3c31cd555425008d38a1a671f3e37724a",
            "title": "Validating Multi-column Schema Matchings by Type",
            "abstract": "Validation of multi-column schema matchings is essential for successful database integration. This task is especially difficult when the databases to be integrated contain little overlapping data, as is often the case in practice (e.g., customer bases of different companies). Based on the intuition that values present in different columns related by a schema matching will have similar \";semantic type\";, and that this can be captured using distributions over values (\";statistical types\";), we develop a method for validating 1-1 and compositional schema matchings. Our technique is based on three key technical ideas. First, we propose a generic measure for comparing two columns matched by a schema matching, based on a notion of information-theoretic discrepancy that generalizes the standard geometric discrepancy; this provides the basis for 1:1 matching. Second, we present an algorithm for \";splitting\"; the string values in a column to identify substrings that are likely to match with the values in another column; this enables (multi-column) l:m schema matching. Third, our technique provides an invalidation certificate if it fails to validate a schema matching. We complement our conceptual and algorithmic contributions with an experimental study that demonstrates the effectiveness and efficiency of our technique on a variety of database schemas and data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21526180",
                    "name": "B. Dai"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1699730",
                    "name": "A. Tung"
                },
                {
                    "authorId": "1747652",
                    "name": "Suresh Venkatasubramanian"
                }
            ]
        },
        {
            "paperId": "13833212cbd2c0093febeba9f6a7ce64d9bf7c7b",
            "title": "Efficient Processing of Top-k Queries in Uncertain Databases",
            "abstract": "This work introduces novel polynomial-time algorithms for processing top-k queries in uncertain databases, under the generally adopted model of x-relations. An x-relation consists of a number of x-tuples, and each x-tuple randomly instantiates into one tuple from one or more alternatives. Our results significantly improve the best known algorithms for top-k query processing in uncertain databases, in terms of both running time and memory usage. Focusing on the single-alternative case, the new algorithms are orders of magnitude faster.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144744111",
                    "name": "K. Yi"
                },
                {
                    "authorId": "3245752",
                    "name": "Feifei Li"
                },
                {
                    "authorId": "1715771",
                    "name": "G. Kollios"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "23bc35869a22335e99729f858f5a7df857dade90",
            "title": "Exploring a Few Good Tuples From a Text Database",
            "abstract": "Information extraction from text databases is a useful paradigm to populate relational tables and unlock the considerable value hidden in plain-text documents. However, information extraction can be expensive, due to various complex text processing steps necessary in uncovering the hidden data. There are a large number of text databases available, and not every text database is necessarily relevant to every relation. Hence, it is important to be able to quickly explore the utility of running an extractor for a specific relation over a given text database before carrying out the expensive extraction task. In this paper, we present a novel exploration methodology of finding a few good tuples for a relation that can be extracted from a database which allows for judging the relevance of the database for the relation. Specifically, we propose the notion of a good(k, `) query as one that can return any k tuples for a relation among the top-` fraction of tuples ranked by their aggregated confidence scores, provided by the extractor; if these tuples have high scores, the database can be determined as relevant to the relation. We formalize the access model for information extraction, and investigate efficient query processing algorithms for good(k, `) queries, which do not rely on any prior knowledge about the extraction task or the database. We demonstrate the viability of our algorithms using a detailed experimental study with real text databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1777385",
                    "name": "Alpa Jain"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "2d25c94920cd673cf0bb84f36121a48c2891692a",
            "title": "XTreeNet: democratic community search",
            "abstract": "We describe XTreeNet, a distributed query dissemination engine which facilitates democratization of publishing and efficient data search among members of online communities with powerful full-text queries. This demonstration shows XTreeNet in full action. XTreeNet serves as a proof of concept for democratic community search by proposing a distributed novel infrastructure in which data resides only with the publishers owning it. Expressive user queries are disseminated to publishers. Given the virtual nature of the global data collection (e.g., the union of all local data published in the community) our infrastructure efficiently locates the publishers that contain matching documents with a specified query, processes the complex full-text query at the publisher and returns all relevant documents to querier.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3260693",
                    "name": "Emiran Curtmola"
                },
                {
                    "authorId": "50136367",
                    "name": "Alin Deutsch"
                },
                {
                    "authorId": "40024798",
                    "name": "Dionysios Logothetis"
                },
                {
                    "authorId": "2060800615",
                    "name": "K. K. Ramakrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2285529824",
                    "name": "Kenneth Yocum"
                }
            ]
        },
        {
            "paperId": "3659ce02e01f76df2d857db886fe2187bacc8c20",
            "title": "Randomized Synopses for Query Assurance on Data Streams",
            "abstract": "The overwhelming flow of information in many data stream applications forces many companies to outsource to a third-party the deployment of a data stream management system (DSMS) for performing desired computations. Remote computations intrinsically raise issues of trust, making query execution assurance on data streams a problem with practical implications. Consider a client observing the same data stream as a remote server (e.g., network traffic), that registers a continuous query on the server's DSMS, and receives answers upon request. The client needs to verify the integrity of the results using significantly fewer resources than evaluating the query locally. Towards that goal, we propose a probabilistic algorithm for selection and aggregate/group-by queries, that uses constant space irrespective of the result-set size, has low update cost, and arbitrarily small probability of failure. We generalize this algorithm to allow some tolerance on the number of errors permitted (irrespective of error magnitude), and also discuss the hardness of permitting arbitrary errors of small magnitude. We also perform an empirical evaluation using live network traffic.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144744111",
                    "name": "K. Yi"
                },
                {
                    "authorId": "3245752",
                    "name": "Feifei Li"
                },
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "1715771",
                    "name": "G. Kollios"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "40e6f827c72158e4b0a7dcc20f18448b4dd7e042",
            "title": "Optimizing away joins on data streams",
            "abstract": "Monitoring aggregates on network traffic streams is a compelling application of data stream management systems. Often, streaming aggregation queries involve joining multiple inputs (e.g., client requests and server responses) using temporal join conditions (e.g., within 5 seconds), followed by computation of aggregates (e.g., COUNT) over temporal windows (e.g., every 5 minutes). These types of queries help identify malfunctioning servers (missing responses), malicious clients (bursts of requests during a denial-of-service attack), or improperly configured protocols (short timeout intervals causing many retransmissions). However, while such query expression is natural, its evaluation over massive data streams is inefficient.\n In this paper, we develop rewriting techniques for streaming aggregation queries that join multiple inputs. Our techniques identify conditions under which expensive joins can be optimized away, while providing error bounds for the results of the rewritten queries. The basis of the optimization is a powerful but decidable theory in which constraints over data streams can be formulated. We show the efficiency and accuracy of our solutions via experimental evaluation on real-life IP network data using the Gigascope stream processing engine.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1688834",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "145633895",
                    "name": "T. Johnson"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1725663",
                    "name": "David Toman"
                }
            ]
        },
        {
            "paperId": "422955d1f193e70bebb546b33d53564421c5085a",
            "title": "Fast Indexes and Algorithms for Set Similarity Selection Queries",
            "abstract": "Data collections often have inconsistencies that arise due to a variety of reasons, and it is desirable to be able to identify and resolve them efficiently. Set similarity queries are commonly used in data cleaning for matching similar data. In this work we concentrate on set similarity selection queries: Given a query set, retrieve all sets in a collection with similarity greater than some threshold. Various set similarity measures have been proposed in the past for data cleaning purposes. In this work we concentrate on weighted similarity functions like TF/IDF, and introduce variants that are well suited for set similarity selections in a relational database context. These variants have special semantic properties that can be exploited to design very efficient index structures and algorithms for answering queries efficiently. We present modifications of existing technologies to work for set similarity selection queries. We also introduce three novel algorithms based on the Threshold Algorithm, that exploit the semantic properties of the new similarity measures to achieve the best performance in theory and practice.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "36206298",
                    "name": "Amit Chandel"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "71665acf67c14ae3851c70ee89e3250e0bd31763",
            "title": "Database Exploration Using Join Paths",
            "abstract": "Complex database schemas are challenging to explore and query, due to the exponentially many sequences of join edges in the schema graph, not all of which result in valid join paths. The problem becomes even more difficult when tables exhibit structural heterogeneity, i.e., different join paths are meaningful for different subsets of tuples in the same table. In this paper, we propose effective ways to identify meaningful join paths in complex schemas, and to compute the probability with which different destination tables are reached via join paths from a given source table.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1738196",
                    "name": "Cecilia M. Procopiuc"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "96175ad70dd992975b7d84a53c0d802b3fca02c2",
            "title": "08111 Report - Ranked XML Querying",
            "abstract": "This paper is based on a five-day workshop on \"Ranked XML Querying\" that took place in Schloss Dagstuhl in Germany in March 2008 and was attended by 27 people from three different research communities: database systems (DB), information retrieval (IR), and Web. The seminar title was interpreted in an IR-style \"andish\" sense (it covered also subsets of {Ranking, XML, Querying}, with larger sets being favored) rather than the DB-style strictly conjunctive manner. So in essence, the seminar really addressed the integration of DB and IR technologies with Web 2.0 being an important target area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1691929",
                    "name": "D. Hiemstra"
                },
                {
                    "authorId": "1785829",
                    "name": "T. Roelleke"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1751591",
                    "name": "G. Weikum"
                }
            ]
        },
        {
            "paperId": "cd389c230b2a1dfc751ec4f9f783994e9fa27552",
            "title": "08111 Abstracts Collection - Ranked XML Querying",
            "abstract": "From 09.03. to 14.03.08, the Dagstuhl Seminar 08111 ``Ranked XML Querying'' was held in the International Conference and Research Center (IBFI), \nSchloss Dagstuhl. \nDuring the seminar, several participants presented their current \nresearch, and ongoing work and open problems were discussed. Abstracts of \nthe presentations given during the seminar as well as abstracts of \nseminar results and ideas are put together in this paper. The first section \ndescribes the seminar topics and goals in general. \nLinks to extended abstracts or full papers are provided, if available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1751591",
                    "name": "G. Weikum"
                }
            ]
        },
        {
            "paperId": "dad7338a7290a17c5c2c0101060fe3c1f3f2402e",
            "title": "On generating near-optimal tableaux for conditional functional dependencies",
            "abstract": "Conditional functional dependencies (CFDs) have recently been proposed as a useful integrity constraint to summarize data semantics and identify data inconsistencies. A CFD augments a functional dependency (FD) with a pattern tableau that defines the context (i.e., the subset of tuples) in which the underlying FD holds. While many aspects of CFDs have been studied, including static analysis and detecting and repairing violations, there has not been prior work on generating pattern tableaux, which is critical to realize the full potential of CFDs. \n \nThis paper is the first to formally characterize a \"good\" pattern tableau, based on naturally desirable properties of support, confidence and parsimony. We show that the problem of generating an optimal tableau for a given FD is NP-complete but can be approximated in polynomial time via a greedy algorithm. For large data sets, we propose an \"on-demand\" algorithm providing the same approximation bound, that outperforms the basic greedy algorithm in running time by an order of magnitude. For ordered attributes, we propose the range tableau as a generalization of a pattern tableau, which can achieve even more parsimony. The effectiveness and efficiency of our techniques are experimentally demonstrated on real data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "1735932",
                    "name": "H. Karloff"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2110411857",
                    "name": "Bei Yu"
                }
            ]
        },
        {
            "paperId": "dee6eaaa5c2d4602b2c247d4c24e9e67cbc8c515",
            "title": "DB&IR integration: report on the dagstuhl seminar",
            "abstract": "This paper is based on a five-day workshop on \"Ranked XML Querying\" that took place in Schloss Dagstuhl in Germany in March 2008 and was attended by 27 people from three different research communities: database systems (DB), information retrieval (IR), and Web. The seminar title was interpreted in an IR-style \"andish\" sense (it covered also subsets of {Ranking, XML, Querying}, with larger sets being favored) rather than the DB-style strictly conjunctive manner. So in essence, the seminar really addressed the integration of DB and IR technologies with Web 2.0 being an important target area.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1691929",
                    "name": "D. Hiemstra"
                },
                {
                    "authorId": "1785829",
                    "name": "T. Roelleke"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1751591",
                    "name": "G. Weikum"
                }
            ]
        },
        {
            "paperId": "eb64f57b5c0e2f7fbeaa1b9c2f5f9d970ae82169",
            "title": "Hashed samples: selectivity estimators for set similarity selection queries",
            "abstract": "We study selectivity estimation techniques for set similarity queries. A wide variety of similarity measures for sets have been proposed in the past. In this work we concentrate on the class of weighted similarity measures (e.g., TF/IDF and BM25 cosine similarity and variants) and design selectivity estimators based on a priori constructed samples. First, we study the pitfalls associated with straightforward applications of random sampling, and argue that care needs to be taken in how the samples are constructed; uniform random sampling yields very low accuracy, while query sensitive realtime sampling is more expensive than exact solutions (both in CPU and I/O cost). We show how to build robust samples a priori, based on existing synopses for distinct value estimation. We prove the accuracy of our technique theoretically, and verify its performance experimentally. Our algorithm is orders of magnitude faster than exact solutions and has very small space overhead.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1836901",
                    "name": "Marios Hadjieleftheriou"
                },
                {
                    "authorId": "145235707",
                    "name": "Xiaohui Yu"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "ee9e06c064951967bdb3be337ce00288b8e82500",
            "title": "Finding hierarchical heavy hitters in streaming data",
            "abstract": "Data items that arrive online as streams typically have attributes which take values from one or more hierarchies (time and geographic location, source and destination IP addresses, etc.). Providing an aggregate view of such data is important for summarization, visualization, and analysis. We develop an aggregate view based on certain organized sets of large-valued regions (\u201cheavy hitters\u201d) corresponding to hierarchically discounted frequency counts. We formally define the notion of hierarchical heavy hitters (HHHs). We first consider computing (approximate) HHHs over a data stream drawn from a single hierarchical attribute. We formalize the problem and give deterministic algorithms to find them in a single pass over the input.\n In order to analyze a wider range of realistic data streams (e.g., from IP traffic-monitoring applications), we generalize this problem to multiple dimensions. Here, the semantics of HHHs are more complex, since a \u201cchild\u201d node can have multiple \u201cparent\u201d nodes. We present online algorithms that find approximate HHHs in one pass, with provable accuracy guarantees. The product of hierarchical dimensions forms a mathematical lattice structure. Our algorithms exploit this structure, and so are able to track approximate HHHs using only a small, fixed number of statistics per stored item, regardless of the number of dimensions.\n We show experimentally, using real data, that our proposed algorithms yields outputs which are very similar (virtually identical, in many cases) to offline computations of the exact solutions, whereas straightforward heavy-hitters-based approaches give significantly inferior answer quality. Furthermore, the proposed algorithms result in an order of magnitude savings in data structure size while performing competitively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "0c5b6d684f55804cff6db568fb8eda68514615ab",
            "title": "Efficient and effective explanation of change in hierarchical summaries",
            "abstract": "Dimension attributes in data warehouses are typically hierarchical (e.g., geographic locations in sales data, URLs in Web traffic logs). OLAP tools are used to summarize the measure attributes (e.g., total sales) along a dimension hierarchy, and to characterize changes (e.g., trends and anomalies) in a hierarchical summary over time. When thenumber of changes identified is large (e.g., total sales in many stores differed from their expected values), a parsimonious explanation of the most significant changes is desirable. In this paper, we propose a natural model of parsimonious explanation, as a composition of node weights along the root-to-leaf paths in a dimension hierarchy, which permits changes to be aggregated with maximal generalization along the dimension hierarchy. We formalize this model of explaining changes in hierarchical summaries and investigate the problem of identifying optimally parsimonious explanations on arbitrary rooted one dimensional tree hierarchies. We show that such explanations can be computed efficiently in time essentially proportional to the number of leaves and the depth of the hierarchy. Further, our method can produce parsimonious explanations from the output of any statistical model that provides predictions and confidence intervals, making it widely applicable. Our experiments use real data sets to demonstrate the utility and robustness of our proposed model for explaining significant changes, as well as its superior parsimony compared to alternatives.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2596058",
                    "name": "D. Agarwal"
                },
                {
                    "authorId": "2343165",
                    "name": "D. Barman"
                },
                {
                    "authorId": "1736832",
                    "name": "D. Gunopulos"
                },
                {
                    "authorId": "1693869",
                    "name": "N. Young"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "10116e957b4564d64c95853aa1c9c7958a303733",
            "title": "Multicast Architecture with Adaptive Dual-state",
            "abstract": "Multicast is an approach that uses network and server resources efficiently to distribute information to groups. As networks evolve to become information-centric, users will increasingly demand publish-subscribe based access to fine grained information, and multicast will need to evolve to (i ) manage an increasing number of groups, with a distinct group for each piece of distributable content; (ii) support persi stent group membership, as group activity can vary over time, with intense activity at some times, and infrequent (but still cr tical) activity at others. These requirements raise scalabi lity challenges that are not met by today\u2019s multicast techniques . In this paper, we propose the MAD (Multicast with Adaptive Dual-state) architecture that can scalably support a v ast number of multicast groups, with varying activity over time , based on two key novel ideas: (i) decouple group membership from forwarding information, and (ii) apply an adaptiv e dual-state approach to optimize for the different objectiv es of active and inactive groups. MAD works across administrative boundaries by partitioning routers into \u201cMAD domains, \u201d enabling autonomous decisions in local domains. Using a prototype implementation in the Emulab testbed as well as large-scale simulations, we show that MAD promises to meet the challenges of large-scale information-centric networ ks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2912169",
                    "name": "T. Cho"
                },
                {
                    "authorId": "145503699",
                    "name": "M. Rabinovich"
                },
                {
                    "authorId": "145922660",
                    "name": "K. Ramakrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "40600407",
                    "name": "Yin Zhang"
                }
            ]
        },
        {
            "paperId": "2c2a23b39e4c2f2e2dc61ab1d6d18242f30f871c",
            "title": "Intensional associations between data and metadata",
            "abstract": "There is a growing need to associate a variety of metadata with the underlying data, but a simple, elegant approach to uniformly model and query both the data and the metadata has been elusive. In this paper, we argue that (1) the relational model augmented with queries as data values is a natural way to uniformly model data, arbitrary metadata and their associations, and (2) relational queries with a join mechanism augmented to permit matching of query result relations, instead of only atomic values, is an elegant way to uniformly query across data and metadata. We describe the architecture of a system we have prototyped for this purpose, demonstrate the generality of our approach and evaluate the performance of the system, in comparison with previous proposals for metadata management.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "30c0d6b845dec5000ac3a2d2c6bfac8150dee7b7",
            "title": "Group Linkage",
            "abstract": "Poor quality data is prevalent in databases due to a variety of reasons, including transcription errors, lack of standards for recording database fields, etc. To be able to query and integrate such data, considerable recent work has focused on the record linkage problem, i.e., determine if two entities represented as relational records are approximately the same. Often entities are represented as groups of relational records, rather than individual relational records, e.g., households in a census survey consist of a group of persons. We refer to the problem of determining if two entities represented as groups are approximately the same as group linkage. Intuitively, two groups can be linked to each other if (i) there is high enough similarity between \"matching\" pairs of individual records that constitute the two groups, and (ii) there is a large fraction of such matching record pairs. In this paper, we formalize this intuition and propose a group linkage measure based on bipartite graph matching. Given a data set consisting of a large number of groups, efficiently finding groups with a high group linkage similarity to an input query group requires quickly eliminating the many groups that are unlikely to be desired matches. To enable this task, we present simpler group similarity measures that can be used either during fast pre-processing steps or as approximations to our proposed group linkage measure. These measures can be easily instantiated using SQL, permitting our techniques to be implemented inside the database system itself. We experimentally validate the utility of our measures and techniques using a variety of real and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1791452",
                    "name": "Byung-Won On"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145948198",
                    "name": "Dongwon Lee"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "323011da81c396825df6ec185ac17a21f2272ea0",
            "title": "Aggregate Query Answering on Anonymized Tables",
            "abstract": "Privacy is a serious concern when microdata need to be released for ad hoc analyses. The privacy goals of existing privacy protection approaches (e.g., k-anonymity and l-diversity) are suitable only for categorical sensitive attributes. Since applying them directly to numerical sensitive attributes (e.g., salary) may result in undesirable information leakage, we propose privacy goals to better capture the need of privacy protection for numerical sensitive attributes. Complementing the desire for privacy is the need to support ad hoc aggregate analyses over microdata. Existing generalization-based anonymization approaches cannot answer aggregate queries with reasonable accuracy. We present a general framework of permutation-based anonymization to support accurate answering of aggregate queries and show that, for the same grouping, permutation-based techniques can always answer aggregate queries more accurately than generalization-based approaches. We further propose several criteria to optimize permutations for accurate answering of aggregate queries, and develop efficient algorithms for each criterion.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112724648",
                    "name": "Qing Zhang"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "144478228",
                    "name": "Ting Yu"
                }
            ]
        },
        {
            "paperId": "43b39679a5a37585e31260fc426120f4fe00538d",
            "title": "Fast Identification of Relational Constraint Violations",
            "abstract": "Logical constraints, (e.g., `phone numbers in Toronto can have prefixes 416, 647, 905 only'), are ubiquitous in relational databases. Traditional integrity constraints, such as functional dependencies, are examples of such logical constraints as well. However, under frequent database updates, schema evolution and transformations, they can be easily violated. As a result, tables become inconsistent and data quality is degraded. In this paper we study the problem of validating collections of user defined constraints on a number of relational tables. Our primary goal is to quickly identify which tables violate such constraints. Logical constraints are potentially complex logical formuli, and we demonstrate that they cannot be efficiently evaluated by SQL queries. In order to enable fast identification of constraint violations, we propose to build and maintain specialized logical indices on the relational tables. We choose Boolean Decision Diagrams (BDD) as the index structure to aid in this task. We first propose efficient algorithms to construct and maintain such indices in a space efficient manner. We then describe a set of query re-write rules that aid in the efficient utilization of logical indices during constraint validation. We have implemented our approach on top of a relational database and tested our techniques using large collections of real and synthetic data sets. Our results indicate that utilizing our techniques in conjunction with logical indices during constraint validation offers very significant performance advantages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36206298",
                    "name": "Amit Chandel"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "34777495",
                    "name": "K. Pu"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "49244137b622676788df1ccf89b25b8abe19250d",
            "title": "Parsimonious Explanations of Change in Hierarchical Data",
            "abstract": "Dimension attributes in data warehouses are typically hierarchical, and a variety of OLAP applications (such as point-of-sales analysis and decision support) call for summarizing the measure attributes in fact tables along the hierarchies of these attributes. For example, the total sales at different stores can be summarized hierarchically by geographic location (e.g., state/city/zip_code/store), by time (e.g., year/month/day/hour), or by product category (e.g., clothing/outerwear/jackets/brand). Existing OLAP tools help to summarize and navigate the data at different levels of aggregation (e.g., jackets sold in each state during December 2006) via drill-down and roll-up operators. OLAP tools are also used to characterize changes in these hierarchical summaries over time (e.g., the sales in December 2006 compared to sales in December 2005 over different locations) to detect anomalies and characterize trends. When the number of changes identified is large (e.g., the total sales at many locations differed significantly from their expectations), one seeks explanations. In this paper, we are interested in parsimonious explanations of changes in measure attributes aggregated along an associated dimension attribute hierarchy. We propose a natural model of explanation that makes effective use of the dimension hierarchy and describes changes at the leaf nodes of the hierarchy (e.g., individual stores in the location hierarchy) as a composition of \"node weights\" along each node's root-to-leaf path in the dimension hierarchy; each node weight constitutes an explanatory term. For example, sales in California stores were three times expected sales; sales in San Jose stores were higher by a factor of two (six times expected sales), whereas sales in Los Angeles stores were lower than the statewide increase by a factor of 1.5 (two times expected sales).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2343165",
                    "name": "D. Barman"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1736832",
                    "name": "D. Gunopulos"
                },
                {
                    "authorId": "1693869",
                    "name": "N. Young"
                },
                {
                    "authorId": "2596058",
                    "name": "D. Agarwal"
                }
            ]
        },
        {
            "paperId": "55c3597ebe9ce20c3f2374be4dda465726335403",
            "title": "X^ 3: A Cube Operator for XML OLAP",
            "abstract": "With increasing amounts of data being exchanged and even generated or stored in XML, a natural question is how to perform OLAP on XML data, which can be structurally heterogeneous (e.g., parse trees) and/or marked-up text documents. A core operator for OLAP is the data cube. While the relational cube can be extended in a straightforward way to XML, we argue such an extension would not address the specific issues posed by XML. While in a relational warehouse, facts are flat records and dimensions may have hierarchies, in an XML warehouse, both facts and dimensions may be hierarchical. Second, XML is flexible: (a) an element may have missing or repeated subelements; (b) different instances of the same element type may have different structure. We identify the challenges introduced by these features of XML for cube definition and computation. We propose a definition for cube adapted for XML data warehouse, including a suitably generalized specification mechanism. We define a cube lattice over the aggregates so defined. We then identify properties of this cube lattice that can be leveraged to allow optimized computation of the cube. Finally, we present the results of an extensive performance evaluation experiment gauging the behavior of alternative algorithms for cube computation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3301260",
                    "name": "Nuwee Wiwatwattana"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "5aed7179134f8659a797bce6ad0dd7e24445194e",
            "title": "Using Queries to Associate Metadata with Data",
            "abstract": "As relational databases proliferate and become increasingly complex, both in their internal structure and in their interactions with other databases and applications, there is a growing need to associate a variety of metadata with the underlying data. Even though the need has been apparent, a simple, elegant approach to uniformly model and query both data and metadata has been elusive. In this paper, we argue that the relational model augmented with queries as data values is a natural way to uniformly model data, arbitrary metadata and their association.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "9b1725d100e06bfd43ed5e996288e8c55c7d510b",
            "title": "Estimating the selectivity of approximate string queries",
            "abstract": "Approximate queries on string data are important due to the prevalence of such data in databases and various conventions and errors in string data. We present the VSol estimator, a novel technique for estimating the selectivity of approximate string queries. The VSol estimator is based on inverse strings and makes the performance of the selectivity estimator independent of the number of strings. To get inverse strings we decompose all database strings into overlapping substrings of length q (q-grams) and then associate each q-gram with its inverse string: the IDs of all strings that contain the q-gram. We use signatures to compress inverse strings, and clustering to group similar signatures.\n We study our technique analytically and experimentally. The space complexity of our estimator only depends on the number of neighborhoods in the database and the desired estimation error. The time to estimate the selectivity is independent of the number of database strings and linear with respect to the length of query string. We give a detailed empirical performance evaluation of our solution for synthetic and real-world datasets. We show that VSol is effective for large skewed databases of short strings.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144197858",
                    "name": "A. Mazeika"
                },
                {
                    "authorId": "2272920683",
                    "name": "Michael H. B\u00f6hlen"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "df6f82f555c1a3b44a0ef1c82729954e554ad3b1",
            "title": "Benchmarking declarative approximate selection predicates",
            "abstract": "Declarative data quality has been an active research topic. The fundamental principle behind a declarative approach to data quality is the use of declarative statements to realize data quality primitives on top of any relational data source. A primary advantage of such an approach is the ease of use and integration with existing applications. Over the last few years several similarity predicates have been proposed for common quality primitives (approximate selections, joins, etc) and have been fully expressed using declarative SQL statements. In this paper we propose new similarity predicates along with their declarative realization, based on notions of probabilistic information retrieval. In particular we show how language models and hidden Markov models can be utilized as similarity predicates for data quality and present their full declarative instantiation. We also show how other scoring methods from information retrieval, can be utilized in a similar setting. We then present full declarative specifications of previously proposed similarity predicates in the literature, grouping them into classes according to their primary characteristics. Finally, we present a thorough performance and accuracy study comparing a large number of similarity predicates for data cleaning operations. We quantify both their runtime performance as well as their accuracy for several types of common quality problems encountered in operational databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "36206298",
                    "name": "Amit Chandel"
                },
                {
                    "authorId": "1728091",
                    "name": "Oktie Hassanzadeh"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "1719119",
                    "name": "Mohammad Sadoghi"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "f6fccadd01562210736c3f2478e121302b9d1f92",
            "title": "Propagating Updates in SPIDER",
            "abstract": "SPIDER, developed at AT&T Labs-Research, is a system that efficiently supports flexible string matching against attribute values in large databases, and is extensively used in AT&T. The scoring methodology is based on tf.idf weighting and cosine similarity, and SPIDER maintains indexes containing string tokens and their weights, for fast matching at query time. Given the \"global\" nature of the weights maintained in the indexes, even a few updates to the underlying database tables would necessitate a (near-complete recomputation of the indexes, which can be prohibitively expensive. In this paper, we explore novel techniques to considerably reduce the cost of propagating updates in SPIDER, without a significant degradation of answer accuracy or query performance. We present experimental evidence using real data sets to demonstrate the practical benefits of our techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "2440762",
                    "name": "A. Marathe"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "fa7303f0efa70d524eb448f8ff8f24e1d3a724d6",
            "title": "MMS: Using Queries As Data Values for Metadata Management",
            "abstract": "We demonstrate MMS, a system for storing and managing a variety of metadata in a simple, elegant and uniform way. The system is based on two observations. First, that the relational model augmented with queries as data values is a natural way to uniformly model data, arbitrary metadata and their association. Second, that relational queries with a join mechanism augmented to permit matching of query result relations, instead of only atomic values, is an elegant way to uniformly query across data and metadata.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "1d6156b17ef02f510394f78c3c57b9a1d93af2b4",
            "title": "Making Designer Schemas with Colors",
            "abstract": "XML schema design has two opposing goals: elimination of update anomalies requires that the schema be as normalized as possible; yet higher query performance and simpler query expression are often obtained through the use of schemas that permit redundancy. In this paper, we show that the recently proposed MCT data model, which extends XML by adding colors, can be used to address this dichotomy effectively. Specifically, we formalize the intuition of anomaly avoidance in MCT using notions of node normal and edge normal forms, and the goal of efficient query processing using notions of association recoverability and direct recoverability. We develop algorithms for transforming design specifications given as ER diagrams into MCT schemas that are in a node or edge normal form and satisfy association or direct recoverability. Experimental results using a wide variety of ER diagrams validate the benefits of our design methodology.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3301260",
                    "name": "Nuwee Wiwatwattana"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "28f1d0814670999c32ee2c95817938718aa22048",
            "title": "Updates Through Views: A New Hope",
            "abstract": "Database views are extensively used to represent unmaterialized tables. Applications rarely distinguish between a materialized base table and a virtual view, thus, they may issue update requests on the views. Since views are virtual, update requests on them need to be translated to updates on the base tables. Existing literature has shown the difficulty of translating view updates in a side-effect free manner. To address this problem, we propose a novel approach for separating the data instance into a logical and a physical level. This separation allows us to achieve side-effect free translations of any kind of update on the view. Furthermore, deletes on a view can be translated without affecting the base tables. We describe the implementation of the framework and present our experimental results",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1751420",
                    "name": "Y. Kotidis"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2163752",
                    "name": "Yannis Velegrakis"
                }
            ]
        },
        {
            "paperId": "4d82a5741b03a0a134380400d42552c53c89ac83",
            "title": "Reasoning About Approximate Match Query Results",
            "abstract": "Join techniques deploying approximate match predicates are fundamental data cleaning operations. A variety of predicates have been utilized to quantify approximate match in such operations and some have been embedded in a declarative data cleaning framework. These techniques return pairs of tuples from both relations, tagged with a score, signifying the degree of similarity between the tuples in the pair according to the specific approximate match predicate. In this paper, we consider the problem of estimating various parameters on the output of declarative approximate join algorithms for planning purposes. Such algorithms are highly time consuming, so precise knowledge of the result size as well as its score distribution is a pressing concern. This knowledge aids decisions as to which operations are more promising for identifying highly similar tuples, which is a key operation for data cleaning. We propose solution strategies that fully comply with a declarative framework and analytically reason about the quality of the estimates we obtain as well as the performance of our strategies. We present the results of a detailed performance evaluation of all strategies proposed. Our experimental results validate our analytical expectations and shed additional light on the quality and performance of our estimation framework. Our study offers a set of simple, fully declarative techniques for this problem, which can be readily deployed in data cleaning systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144392463",
                    "name": "S. Guha"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "145235707",
                    "name": "Xiaohui Yu"
                }
            ]
        },
        {
            "paperId": "5a5d6b0a9a7036c5417a618925c8e03d786e7d74",
            "title": "Using SPIDER: an experience report",
            "abstract": "At AT&T Labs-Research, we have been developing a prototype system called SPIDER to efficiently support flexible string matching of attribute values in large databases. SPIDER has been used in AT&T, both as a key component of an operational portal for matching customer names and addresses, and for a variety of ad hoc data quality analyses. In this talk, we report on experiences with SPIDER.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "2440762",
                    "name": "A. Marathe"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "5f4abcc6526ca79fb0d5df14527a9a5ce1c12621",
            "title": "Integrating XML data sources using approximate joins",
            "abstract": "XML is widely recognized as the data interchange standard of tomorrow because of its ability to represent data from a variety of sources. Hence, XML is likely to be the format through which data from multiple sources is integrated. In this article, we study the problem of integrating XML data sources through correlations realized as join operations. A challenging aspect of this operation is the XML document structure. Two documents might convey approximately or exactly the same information but may be quite different in structure. Consequently, an approximate match in structure, in addition to content, has to be folded into the join operation. We quantify an approximate match in structure and content for pairs of XML documents using well defined notions of distance. We show how notions of distance that have metric properties can be incorporated in a framework for joins between XML data sources and introduce the idea of reference sets to facilitate this operation. Intuitively, a reference set consists of data elements used to project the data space. We characterize what constitutes a good choice of a reference set, and we propose sampling-based algorithms to identify them. We then instantiate our join framework using the tree edit distance between a pair of trees. We next turn our attention to utilizing well known index structures to improve the performance of approximate XML join operations. We present a methodology enabling adaptation of index structures for this problem, and we instantiate it in terms of the R-tree. We demonstrate the practical utility of our solutions using large collections of real and synthetic XML data sets, varying parameters of interest, and highlighting the performance benefits of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144392463",
                    "name": "S. Guha"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "144478228",
                    "name": "Ting Yu"
                }
            ]
        },
        {
            "paperId": "72c16b9622725a2c9b3395247c4499bd24bb72ea",
            "title": "Record linkage: similarity measures and algorithms",
            "abstract": "This tutorial provides a comprehensive and cohesive overview of the key research results in the area of record linkage methodologies and algorithms for identifying approximate duplicate records, and available tools for this purpose. It encompasses techniques introduced in several communities including databases, information retrieval, statistics and machine learning. It aims to identify similarities and differences across the techniques as well as their merits and limitations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "1770124",
                    "name": "Sunita Sarawagi"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "82d7e22ce63f66d5c5caf8200bee2fa7f62445b2",
            "title": "Circumventing Data Quality Problems Using Multiple Join Paths",
            "abstract": "We propose the Multiple Join Path (MJP) framework for obtaining high quality information by linking fields across multiple databases, when the underlying databases have poor quality data, which are characterized by violations of integrity constraints like keys and functional dependencies within and across databases. MJP associates quality scores with candidate answers by first scoring individual data paths between a pair of field values taking into account data quality with respect to specified integrity constraints, and then agglomerating scores across multiple data paths that serve as corroborating evidences for a candidate answer. We address the problem of finding the top-few (highest quality) answers in the MJP framework using novel techniques, and demonstrate the utility of our techniques using real data and our Virtual Integration Prototype testbed.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1751420",
                    "name": "Y. Kotidis"
                },
                {
                    "authorId": "144173226",
                    "name": "A. Marian"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "86f35940d8850dd1b76362304b878b6e6e66cdf7",
            "title": "The Bellman Data Quality Browser",
            "abstract": "Data quality is a serious concern in complex industrial-scale databases, which often have thousands of tables and tens of thousands of columns. Commonly encountered problems include missing data (null values), duplicates and default values in columns supposed to treated as keys, data inconsistencies (violation of functional dependencies), and poor quality join paths (lack of referential integrity). Compounding the data quality problems are incomplete and out-of-date metadata about the database and the processes used to populate the database. These problems make the task of analyzing data particularly challenging. To effectively address such problems, we have built the Bellman data quality browser at AT&T. Bellman profiles the database and computes concise statistical summaries of the contents of the database, to identify approximate keys, frequent values of a field (often default values), joinable fields with estimates of join sizes paths, and to understand database dynamics (changes in a database over time). In this talk, I'll describe the technology underlying Bellman and how it is used to help make sense of complex databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "92221c813e678e885905934872f4fd405aa51874",
            "title": "Performance and overhead of semantic cache management",
            "abstract": "The emergence of query-based online data services and e-commerce applications has prompted much recent research on data caching. This article studies semantic caching, a caching architecture for such applications, that caches the results of selection queries. The primary contribution of this article is to revisit the performance and overhead of semantic caching using a modern database server and modern hardware. Initially, the performance study focuses on simple workloads and demonstrates several benefits of semantic caching, including low overhead, insensitivity to the physical layout of the database, reduced network traffic, and the ability to answer some queries without contacting the server. With moderately complex workloads, careful coding of remainder queries is required to maintain efficient query processing at the server. Using very complex workloads, we demonstrate that semantic caching works well in a range of applications, especially in network-constrained environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50398858",
                    "name": "B. J\u00f3nsson"
                },
                {
                    "authorId": "2931740",
                    "name": "Mar\u00eda Arinbjarnar"
                },
                {
                    "authorId": "1918330",
                    "name": "Bjarnsteinn \u00de\u00f3rsson"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "be15028db5dc7f50cbca96d9e3d264802f385873",
            "title": "Rapid Identification of Column Heterogeneity",
            "abstract": "Data quality is a serious concern in every data management application, and a variety of quality measures have been proposed, e.g., accuracy, freshness and completeness, to capture common sources of data quality degradation. We identify and focus attention on a novel measure, column heterogeneity, that seeks to quantify the data quality problems that can arise when merging data from different sources. We identify desiderata that a column heterogeneity measure should intuitively satisfy, and describe our technique to quantify database column heterogeneity based on using a novel combination of cluster entropy and soft clustering. Finally, we present detailed experimental results, using diverse data sets of different types, to demonstrate that our approach provides a robust mechanism for identifying and quantifying database column heterogeneity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21526180",
                    "name": "B. Dai"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "1693070",
                    "name": "B. Ooi"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1747652",
                    "name": "Suresh Venkatasubramanian"
                }
            ]
        },
        {
            "paperId": "ca64d452fa6a811d549ca99e744c2eb8d117b346",
            "title": "Space- and time-efficient deterministic algorithms for biased quantiles over data streams",
            "abstract": "Skew is prevalent in data streams, and should be taken into account by algorithms that analyze the data. The problem of finding \"biased quantiles\"\u2014that is, approximate quantiles which must be more accurate for more extreme values\u2014is a framework for summarizing such skewed data on data streams. We present the first deterministic algorithms for answering biased quantiles queries accurately with small\u2014sublinear in the input size\u2014space and time bounds in one pass. The space bound is near-optimal, and the amortized update cost is close to constant, making it practical for handling high speed network data streams. We not only demonstrate theoretical properties of the algorithm, but also show it uses less space than existing methods in many practical settings, and is fast to maintain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "cb3e11ff1f5f03aa58e47e08640647ca133faab2",
            "title": "Meta-data indexing for XPath location steps",
            "abstract": "XML is the de facto standard for data representation and exchange over the Web. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and sensitivity. When querying such XML data, say using XPath, it is important to efficiently identify the data that meet specified constraints on the meta-data. For example, different users may be satisfied with different levels of quality guarantees, or may only have access to different parts of the XML data based on specified security policies. In this paper, we address the problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints, when the meta-data levels are specifically drawn from an ordered domain (e.g., accuracy in [0,1], recency using timestamps, multi-level security, etc.). More specifically, we develop a family of index structures, which we refer to as meta-data indexes, to address this problem. A meta-data index is easily instantiated using a multi-dimensional index structure, such as an R-tree, incorporating novel query and update algorithms. We show that the full meta-data index (FMI), based on associating each XML element with its meta-data level, has a very high update cost for modifying an element's meta-data level. We resolve this problem by designing the inheritance meta-data index (IMI), in which (i) actual meta-data levels are associated only with elements for which this value is explicitly specified, and (ii) inherited meta-data levels and inheritance source nodes are associated with non-leaf nodes of the index structure. We design efficient query (for all XPath axes) and update (of meta-data levels) algorithms for the IMI, and experimentally demonstrate the superiority of the IMI over the FMI using benchmark data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2690683",
                    "name": "SungRan Cho"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "d5b5db41929d791b4e911a5c7b0e29f60ee23253",
            "title": "Column Heterogeneity as a Measure of Data Quality",
            "abstract": "Data quality is a serious concern in every data management application, and a variety of quality measures have been proposed, including accuracy, freshness and completeness, to capture the common sources of data quality degradation. We identify and focus attention on a novel measure, column heterogeneity, that seeks to quantify the data quality problems that can arise when merging data from different sources. We identify desiderata that a column heterogeneity measure should intuitively satisfy, and discuss a promising direction of research to quantify database column heterogeneity based on using a novel combination of cluster entropy and soft clustering. Finally, we present a few preliminary experimental results, using diverse data sets of semantically different types, to demonstrate that this approach appears to provide a robust mechanism for identifying and quantifying database column heterogeneity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "21526180",
                    "name": "B. Dai"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "1693070",
                    "name": "B. Ooi"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1747652",
                    "name": "Suresh Venkatasubramanian"
                }
            ]
        },
        {
            "paperId": "ed15fcf8761c982c215907a4678b48d2232b0c2b",
            "title": "Keyword proximity search in XML trees",
            "abstract": "Recent works have shown the benefits of keyword proximity search in querying XML documents in addition to text documents. For example, given query keywords over Shakespeare's plays in XML, the user might be interested in knowing how the keywords cooccur. In this paper, we focus on XML trees and define XML keyword, proximity queries to return the (possibly heterogeneous) set of minimum connecting trees (MCTs) of the matches to the individual keywords in the query. We consider efficiently executing keyword proximity queries on labeled trees (XML) in various settings: 1) when the XML database has been preprocessed and 2) when no indices are available on the XML database. We perform a detailed experimental evaluation to study the benefits of our approach and show that our algorithms considerably outperform prior algorithms and other applicable approaches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1754970",
                    "name": "Vagelis Hristidis"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "1786049",
                    "name": "Y. Papakonstantinou"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "1e435e2f05ff27a18d88a8ccd649f75eab7533ab",
            "title": "XTreeNet: scalable overlay networks for XML content dissemination and querying (synopsis)",
            "abstract": "XML is becoming a ubiquitous format for information exchange on the Internet. To alleviate the problems of \"whom to ask\" and \"whom to tell\" when connecting XML information producers with consumers over the network, content-based querying and dissemination of information have been investigated in the literature. Our XTreeNet project unifies the publish/subscribe and query/response models with a single common XML aware overlay network for XML-based information producers and consumers. This integrated framework lends itself to a variety of applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2097132373",
                    "name": "W. Fenner"
                },
                {
                    "authorId": "145503699",
                    "name": "M. Rabinovich"
                },
                {
                    "authorId": "145922660",
                    "name": "K. Ramakrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "40600407",
                    "name": "Yin Zhang"
                }
            ]
        },
        {
            "paperId": "29360559cbb9d8d4012c94262f3a79ea259abe2f",
            "title": "MIX: A Meta-data Indexing System for XML",
            "abstract": "We present a system for efficient meta-data indexed querying of XML documents. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and security assessments. We address the meta-data indexing problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints. Our system, named MIX, incorporates query processing on all XPath axes suitably enhanced with meta-data features offering not only query answering but also dynamic maintenance of meta-data levels for XML documents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2690683",
                    "name": "SungRan Cho"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "8d99de9b0e4557c2648e4ab7a840def5c3fba3ab",
            "title": "Proceedings of the 2nd international workshop on Information quality in information systems",
            "abstract": "The problem of poor data quality stored in database-backed information systems is widespread in the governmental, commercial and industrial environments. Alarming situations with various information quality problems can not be ignored anymore and theoretical as well as pragmatic approaches are urgently needed to be proposed and validated. As a consequence, information quality is now becoming one of the hot topics of emerging interest in the academic and industrial communities.Many processes and applications (such as information system integration, information retrieval, and knowledge discovery from databases) require various forms of data preparation or repair with several data processing techniques, because the data input to the application-dedicated algorithms is assumed to conform to nice data distributions, containing no missing, inconsistent or incorrect values. This leaves a large gap between the available \"dirty\" data and the available machinery to process the data for application purposes.The Second Edition of the International Workshop IQIS 2005 (Information Quality in Information Systems) is held in Baltimore, MD, USA, on June 17, 2005. The workshop is sponsored by ACM and in conjunction with the Symposium on Principles of Database System (PODS) and the ACM SIGMOD International Conference on Management of Data. IQIS workshop focuses on database-centric issues in data quality (scalability, quality-aware query processing, applications like data integration). It intends to address methods, techniques of massive data processing and analysis, methodologies, new algorithmic approaches or frameworks for designing data quality metrics in order to understand and to explore data quality, to end data glitches (as data quality problems such as duplicates, errors, outliers, contradictions, inconsistencies, etc.) and to ensure both data and information quality of database-backed information systems.The 11 papers collected in this volume, out of 26 papers that were submitted (with 10 short papers and 16 research papers), are a significant sample of recent achievements in the various areas of information and data quality, ranging from quality models to record linkage and statistical techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403176402",
                    "name": "Laure Berti-\u00c9quille"
                },
                {
                    "authorId": "1693220",
                    "name": "C. Batini"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "92ab6e7da006afcec42104f75e3b380aa1653827",
            "title": "SPIDER: flexible matching in databases",
            "abstract": "We present a prototype system, SPIDER, developed at AT&T Labs-Research, which supports flexible string attribute value matching in large databases. We discuss the design principles on which SPIDER is based, describe the basic techniques encompassed by the tool and provide a description of the demo.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "2440762",
                    "name": "A. Marathe"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "a1ff389afc271e9c77538d4398e0828e961e77ce",
            "title": "Answering order-based queries over XML data",
            "abstract": "Order-based queries over XML data include XPath navigation axes such as following-sibling and following. In this paper, we present holistic algorithms that evaluate such order-based queries. An experimental comparison with previous approaches shows the performance benefits of our algorithms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3104475",
                    "name": "Zografoula Vagena"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1761528",
                    "name": "V. Tsotras"
                }
            ]
        },
        {
            "paperId": "bfc1fdbca05cdd97a20cd4eeaf0dd5ac4b1ffcc0",
            "title": "Effective computation of biased quantiles over data streams",
            "abstract": "Skew is prevalent in many data sources such as IP traffic streams. To continually summarize the distribution of such data, a high-biased set of quantiles (e.g., 50th, 90th and 99th percentiles) with finer error guarantees at higher ranks (e.g., errors of 5, 1 and 0.1 percent, respectively) is more useful than uniformly distributed quantiles (e.g., 25th, 50th and 75th percentiles) with uniform error guarantees. In this paper, we address the following two problems. First, can we compute quantiles with finer error guarantees for the higher ranks of the data distribution effectively using less space and computation time than computing all quantiles uniformly at the finest error? Second, if specific quantiles and their error bounds are requested a priori, can the necessary space usage and computation time be reduced? We answer both questions in the affirmative by formalizing them as the \"high-biased\" and the \"targeted\" quantiles problems, respectively, and presenting algorithms with provable guarantees, that perform significantly better than previously known solutions for these problems. We implemented our algorithms in the Gigascope data stream management system, and evaluated alternate approaches for maintaining the relevant summary structures. Our experimental results on real and synthetic IP data streams complement our theoretical analyses, and highlight the importance of lightweight, non-blocking implementations when maintaining summary structures over highspeed data streams.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "c1dd0115a69a9971cd204095e7b38f712c7fbf62",
            "title": "The threshold join algorithm for top-k queries in distributed sensor networks",
            "abstract": "In this paper we present the Threshold Join Algorithm (TJA), which is an efficient TOP-k query processing algorithm for distributed sensor networks. The objective of a top-k query is to find the k highest ranked answers to a user defined similarity function. The evaluation of such a query in a sensor network environment is associated with the transfer of data over an extremely expensive communication medium. TJA uses a non-uniform threshold on the queried attribute in order to minimize the number of tuples that have to be transferred towards the querying node. Additionally, TJA resolves queries in the network rather than in a centralized fashion, which minimizes even more the consumption of bandwidth and delay. Our preliminary experimental results, using our trace driven simulator, show that TJA is both practical and efficient.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403214054",
                    "name": "D. Zeinalipour-Yazti"
                },
                {
                    "authorId": "3104475",
                    "name": "Zografoula Vagena"
                },
                {
                    "authorId": "1736832",
                    "name": "D. Gunopulos"
                },
                {
                    "authorId": "1685532",
                    "name": "V. Kalogeraki"
                },
                {
                    "authorId": "1761528",
                    "name": "V. Tsotras"
                },
                {
                    "authorId": "1699100",
                    "name": "M. Vlachos"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "c8c194fe80b1d95e117c2a40a52bb21c82b39dc2",
            "title": "Adaptive processing of top-k queries in XML",
            "abstract": "The ability to compute top-k matches to XML queries is gaining importance due to the increasing number of large XML repositories. The efficiency of top-k query evaluation relies on using scores to prune irrelevant answers as early as possible in the evaluation process. In this context, evaluating the same query plan for all answers might be too rigid because, at any time in the evaluation, answers have gone through the same number and sequence of operations, which limits the speed at which scores grow. Therefore, adaptive query processing that permits different plans for different partial matches and maximizes the best scores is more appropriate. In this paper, we propose an architecture and adaptive algorithms for efficiently computing top-k matches to XML queries. Our techniques can be used to evaluate both exact and approximate matches where approximation is defined by relaxing XPath axes. In order to compute the scores of query answers, we extend the traditional tf*idf measure to account for document structure. We conduct extensive experiments on a variety of benchmark data and queries, and demonstrate the usefulness of the adaptive approach for computing top-k queries in XML.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144173226",
                    "name": "A. Marian"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "f0876f54d95b017e5508438a0590d944f1cc9780",
            "title": "Multiple aggregations over data streams",
            "abstract": "Monitoring aggregates on IP traffic data streams is a compelling application for data stream management systems. The need for exploratory IP traffic data analysis naturally leads to posing related aggregation queries on data streams, that differ only in the choice of grouping attributes. In this paper, we address this problem of efficiently computing multiple aggregations over high speed data streams, based on a two-level LFTA/HFTA DSMS architecture, inspired by Gigascope.Our first contribution is the insight that in such a scenario, additionally computing and maintaining fine-granularity aggregation queries (phantoms) at the LFTA has the benefit of supporting shared computation. Our second contribution is an investigation into the problem of identifying beneficial LFTA configurations of phantoms and user-queries. We formulate this problem as a cost optimization problem, which consists of two sub-optimization problems: how to choose phantoms and how to allocate space for them in the LFTA. We formally show the hardness of determining the optimal configuration, and propose cost greedy heuristics for these independent sub-problems based on detailed analyses. Our final contribution is a thorough experimental study, based on real IP traffic data, as well as synthetic data, to demonstrate the effectiveness of our techniques for identifying beneficial configurations.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118403501",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "1693070",
                    "name": "B. Ooi"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "03963368736aacb724b82900a01bf0313b609f5e",
            "title": "Holistic UDAFs at streaming speeds",
            "abstract": "Many algorithms have been proposed to approximate holistic aggregates, such as quantiles and heavy hitters, over data streams. However, little work has been done to explore what techniques are required to incorporate these algorithms in a data stream query processor, and to make them useful in practice.In this paper, we study the performance implications of using user-defined aggregate functions (UDAFs) to incorporate selection-based and sketch-based algorithms for holistic aggregates into a data stream management system's query processing architecture. We identify key performance bottlenecks and tradeoffs, and propose novel techniques to make these holistic UDAFs fast and space-efficient for use in high-speed data stream applications. We evaluate performance using generated and actual IP packet data, focusing on approximating quantiles and heavy hitters. The best of our current implementations can process streaming queries at OC48 speeds (2x 2.4Gbps).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "145633895",
                    "name": "T. Johnson"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                },
                {
                    "authorId": "1704170",
                    "name": "Oliver Spatscheck"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "3c43d0c9a1d7864e7d819179940d857f8633d4ce",
            "title": "A compressed accessibility map for XML",
            "abstract": "XML is the undisputed standard for data representation and exchange. As companies transact business over the Internet, letting authorized customers directly access, and even modify, XML data offers many advantages in terms of cost, accuracy, and timeliness. Given the complex business relationships between companies, and the sensitive nature of information, access must be provided selectively, using sophisticated access control specifications. Using the specification directly to determine if a user has access to an XML data item can be extremely inefficient. The alternative of fully materializing, for each data item, the users authorized to access it can be space-inefficient. In this article, we introduce a compressed accessibility map (CAM) as a space- and time-efficient solution to the access control problem for XML data. A CAM compactly identifies the XML data items to which a user has access, by exploiting structural locality of accessibility in tree-structured data. We present a CAM lookup algorithm for determining if a user has access to a data item that takes time proportional to the product of the depth of the item in the XML data and logarithm of the CAM size. We develop an algorithm for building an optimal size CAM that takes time linear in the size of the XML data set. While optimality cannot be preserved incrementally under data item updates, we provide an algorithm for incrementally maintaining near-optimality. Finally, we experimentally demonstrate the effectiveness of the CAM for multiple users on a variety of real and synthetic data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144478228",
                    "name": "Ting Yu"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                }
            ]
        },
        {
            "paperId": "58cfb95d06b6f4ccbf79ed1436c50187d1657252",
            "title": "Routing XML queries",
            "abstract": "In file-sharing P2P networks, a fundamental problem is that of identifying databases that are relevant to user queries. This problem is referred to as the location problem in P2P literature. We propose a scalable solution to the location problem in a data-sharing P2P network, consisting of a network of XML database nodes and XML router nodes, and make the following contributions. We develop the internal organization and routing protocols for the XML router nodes, to enable scalable XPath query and update processing, under the open and the agreement cooperation models between nodes. Since router nodes tend to be memory constrained, we facilitate a space/performance tradeoff by permitting aggregated routing states, and developing algorithms for generating and using such aggregated information. We experimentally demonstrate the scalability of our approach, and the performance of our query and update protocols, using a detailed simulation model, varying key design parameters.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145503699",
                    "name": "M. Rabinovich"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "73013131",
                    "name": "Tingbao Yu"
                }
            ]
        },
        {
            "paperId": "5d458a1bff91aa598fcc47711e5cfd7a6dfa559d",
            "title": "Diamond in the rough: finding Hierarchical Heavy Hitters in multi-dimensional data",
            "abstract": "Data items archived in data warehouses or those that arrive online as streams typically have attributes which take values from multiple hierarchies (e.g., time and geographic location; source and destination IP addresses). Providing an aggregate view of such data is important to summarize, visualize, and analyze. We develop the aggregate view based on certain hierarchically organized sets of large-valued regions (\"heavy hitters\"). Such Hierarchical Heavy Hitters (HHHs) were previously introduced as a crucial aggregation technique in one dimension. In order to analyze the wider range of data warehousing applications and realistic IP data streams, we generalize this problem to multiple dimensions.We identify and study two variants of HHHs for multi-dimensional data, namely the \"overlap\" and \"split\" cases, depending on how an aggregate computed for a child node in the multi-dimensional hierarchy is propagated to its parent element(s). For data warehousing applications, we present offline algorithms that take multiple passes over the data and produce the exact HHHs. For data stream applications, we present online algorithms that find approximate HHHs in one pass, with provable accuracy guarantees.We show experimentally, using real and synthetic data, that our proposed online algorithms yield outputs which are very similar (virtually identical, in many cases) to their offline counterparts. The lattice property of the product of hierarchical dimensions (\"diamond\") is crucially exploited in our online algorithms to track approximate HHHs using only a small, fixed number of statistics per candidate node, regardless of the number of dimensions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709589",
                    "name": "Graham Cormode"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "7f482af5eaa0c34a1113d7e9837887fa097a9745",
            "title": "Distributed evaluation of network directory queries",
            "abstract": "We describe novel efficient techniques for the distributed evaluation of hierarchical aggregate selection queries over LDAP directory data, distributed across multiple autonomous directory servers. Such queries are useful for emerging applications like the directory enabled networks initiative. Our techniques follow the LDAP approach of distributed query evaluation by referrals, where each relevant server computes answers locally, and the LDAP client coordinates between directory servers. We make a conceptual separation between the identification of relevant servers and the distributed computation of answers. We focus on the challenging task of generating an efficient plan for evaluating hierarchical aggregate selection queries, which involves correlating directory entries across multiple servers. The key features of our plan are: 1) the network traffic consists of query answers, and auxiliary messages that depend only on the number of servers and the size of the query (not on the data size), 2) the coordination effort at the client is independent of the data size, and 3) potentially expensive server-to-server communication and coordination is avoided. We complement our analysis with experiments that show the robustness and scalability of our techniques for highly distributed directory query processing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                }
            ]
        },
        {
            "paperId": "c17a1ffc1ee5ef8a21f509202b5e5dbe13f8b96a",
            "title": "Colorful XML: one hierarchy isn't enough",
            "abstract": "XML has a tree-structured data model, which is used to uniformly represent structured as well as semi-structured data, and also enable concise query specification in XQuery, via the use of its XPath (twig) patterns. This in turn can leverage the recently developed technology of structural join algorithms to evaluate the query efficiently. In this paper, we identify a fundamental tension in XML data modeling: (i) data represented as deep trees (which can make effective use of twig patterns) are often un-normalized, leading to update anomalies, while (ii) normalized data tends to be shallow, resulting in heavy use of expensive value-based joins in queries.Our solution to this data modeling problem is a novel multi-colored trees (MCT) logical data model, which is an evolutionary extension of the XML data model, and permits trees with multi-colored nodes to signify their participation in multiple hierarchies. This adds significant semantic structure to individual data nodes. We extend XQuery expressions to navigate between structurally related nodes, taking color into account, and also to create new colored trees as restructurings of an MCT database. While MCT serves as a significant evolutionary extension to XML as a logical data model, one of the key roles of XML is for information exchange. To enable exchange of MCT information, we develop algorithms for optimally serializing an MCT database as XML. We discuss alternative physical representations for MCT databases, using relational and native XML databases, and describe an implementation on top of the Timber native XML database. Experimental evaluation, using our prototype implementation, shows that not only are MCT queries/updates more succinct and easier to express than equivalent shallow tree XML queries, but they can also be significantly more efficient to evaluate than equivalent deep and shallow tree XML queries/updates.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1789318",
                    "name": "M. Scannapieco"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "3301260",
                    "name": "Nuwee Wiwatwattana"
                }
            ]
        },
        {
            "paperId": "0e6c65b8c366d6f925b8221e01aceddbf931ef19",
            "title": "TIMBER: a native system for querying XML",
            "abstract": "XML has become ubiquitous, and XML data has to be managed in databases. The current industry standard is to map XML data into relational tables and store this information in a relational database. Such mappings create both expressive power problems and performance problems.In the TIMBER [7] project we are exploring the issues involved in storing XML in native format. We believe that the key intellectual contribution of this system is a comprehensive set-at-a-time query processing ability in a native XML store, with all the standard components of relational query processing, including algebraic rewriting and a cost-based optimizer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2822695",
                    "name": "Stelios Paparizos"
                },
                {
                    "authorId": "51461409",
                    "name": "S. Al-Khalifa"
                },
                {
                    "authorId": "144030084",
                    "name": "Adriane P. Chapman"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "2183125",
                    "name": "Andrew Nierman"
                },
                {
                    "authorId": "49111633",
                    "name": "J. Patel"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "3301260",
                    "name": "Nuwee Wiwatwattana"
                },
                {
                    "authorId": "50118077",
                    "name": "Yuqing Wu"
                },
                {
                    "authorId": "71884697",
                    "name": "Cong Yu"
                }
            ]
        },
        {
            "paperId": "2ad85ec8ac6a37ab4111bf9354300e8579f86f49",
            "title": "LockX: a system for efficiently querying secure XML",
            "abstract": "1. MOTIVATION Companies are using the Web for information dissemination, sparking interest in models and efficient mechanisms for controlled access to information. In this context, securing XML documents is important. Much of the work on XML access control to date has studied models for the specification of XML access control policies, focusing on issues such as granularity of access and conflict resolution. However, there has been little work on enforcement of access control policies for queries. A naive two-step solution to secure query evaluation is to first compute query results, and then use access control policies to filter the results. Consider the XML database of an online-seller, which has information on books and customers. Assume that a specific user is allowed access to books and not to customer information. If only query results are filtered for accessibility, the XPath query:",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2690683",
                    "name": "SungRan Cho"
                },
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "338bb5e2ec2616380f156630c4b5be9b73e91212",
            "title": "PIX: a system for phrase matching in XML documents: a demonstration",
            "abstract": "We present a system that enables flexible and efficient phrase matching in XML documents. Since XML allows structured and unstructured information to be interleaved, phrase matching in XML raises new challenges. Our system, named PIX, permits phrase matching in XML documents that contain \u201cmixed content\u201d. A key feature of PIX is that users can specify which element and content to ignore when matching a phrase. PIX uses inverted indices and an efficient evaluation algorithm to compute the set of matches and returns answers where phrases, ignored tags and content are highlighted. In addition, query answers are sorted using a ranking function. PIX is implemented as an extension of GALAX, a full-fledged XQuery engine. The functionality of PIX is fully integrated into XQuery and permits a natural combination of XPath-based structure matching with phrase matching.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "35941003",
                    "name": "M. Fern\u00e1ndez"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2110165071",
                    "name": "Yu Xu"
                }
            ]
        },
        {
            "paperId": "3777986cf147797a52ae0134a7e90343b0ac48a7",
            "title": "Efficient Approximation of Correlated Sums on Data Streams",
            "abstract": "In many applications such as IP network management, data arrives in streams and queries over those streams need to be processed online using limited storage. Correlated-sum (CS) aggregates are a natural class of queries formed by composing basic aggregates on (x, y) pairs and are of the form SUM{g(y) : x /spl les/ f(AGG(x))}, where AGG(x) can be any basic aggregate and f(), g() are user-specified functions. CS-aggregates cannot be computed exactly in one pass through a data stream using limited storage; hence, we study the problem of computing approximate CS-aggregates. We guarantee a priori error bounds when AGG(x) can be computed in limited space (e.g., MIN, MAX, AVG), using two variants of Greenwald and Khanna's summary structure for the approximate computation of quantiles. Using real data sets, we experimentally demonstrate that an adaptation of the quantile summary structure uses much less space, and is significantly faster, than a more direct use of the quantile summary structure, for the same a posteriori error bounds. Finally, we prove that, when AGG(x) is a quantile (which cannot be computed over a data stream in limited space), the error of a CS-aggregate can be arbitrarily large.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2681364",
                    "name": "R. Ananthakrishna"
                },
                {
                    "authorId": "2892654",
                    "name": "Abhinandan Das"
                },
                {
                    "authorId": "143614516",
                    "name": "J. Gehrke"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "3b78454b4dc5982750bc6ffd5362833b0ab78885",
            "title": "Data stream query processing",
            "abstract": "Stream data are also generated naturally by (message-based) Web services, in which loosely coupled systems interact by exchanging high volumes of business data (e.g., purchase orders, retail transactions) tagged in XML (the lingua franca of Web services), forming continuous XML data streams. A central aspect of Web services is the ability to efficiently operate on these XML data streams executing queries (expressed in some XML query language) to continuously match, extract and transform parts of the XML data stream to drive legacy back-end business applications. The objective of this tutorial is to provide a comprehensive and cohesive overview of the key research results in the area of data stream query processing, both for SQL-like and XML query languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "3fe8840e3cd36b64423396086050c8ba45775b25",
            "title": "Secure evaluation of xml queries",
            "abstract": "The rapid emergence of XML as a standard for data exchange over the Web has led to considerable interest in the problem of securing XML documents. This thesis focuses on the simple, but useful, multi-level access control model, where a security level can be either specified at an XML element, or inherited from its parent. \nQuery evaluation engines need to ensure that user queries use and return only XML data the user is allowed to access. Secure query evaluation is possible by rewriting the query to use a recursive function that computes an element's security level. These added access control checks can considerably increase query evaluation time. Based on security information in the DTD, efficient algorithms that determine when the recursive check can be eliminated or simplified to a local check on the element's attributes are devised. This thesis experimentally evaluates the performance benefits of these techniques using a variety of XML data and queries. \nTo efficiently provide secure access to XML data, a family of index structures is developed, which it is referred to as security indices, obtained by enhancing R-tree indices with security levels for XML elements. The straightforward index that annotates each XML element with its (explicitly specified an inherited) security level has a very high update cost for modifying the security level of an XML element. An improved index makes explicit use of the multi-level access control model to reduce the cost of update an element's security level. Efficient algorithms for update are developed. The thesis shows experimentally that this reduced update cost can be achieved without increasing query access cost. \nFinally, a prototype LockX is developed, which implements the idea of secure optimization for specifying access control policies and securely querying XML documents. LockX demonstrates access control specification, enforcement, and explanation features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2690683",
                    "name": "SungRan Cho"
                }
            ]
        },
        {
            "paperId": "4e8646cf49c15a695b577d6e24deecdba17a6411",
            "title": "Approximate string joins",
            "abstract": "Summary form only given. String data is ubiquitous and is commonly used to correlate (or join) entities across autonomous, heterogeneous databases. The main challenge is to effectively deal with the noisy nature of string data, due to, for example, transcription errors, incomplete information, and multiple conventions for recording string value attributes. Commercial databases do not support approximate string joins directly, and it is a challenge to implement this functionality efficiently. The author presents techniques for performing approximate string joins, based n a variety of string similarity metrics, including variants of edit distance and cosine similarity. These techniques are scalable, and can be formulated to execute efficiently in a relational database management system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "667f5206d5afab49f830a24849bf14d5a6ad5564",
            "title": "Text joins for data cleansing and integration in an RDBMS",
            "abstract": "An organization's data records are often noisy because of transcription errors, incomplete information, lack of standard formats for textual data or combinations thereof. A fundamental task in a data cleaning system is matching textual attributes that refer to the same entity (e.g., organization name or address). This matching is effectively performed via the cosine similarity metric from the information retrieval field. For robustness and scalability, these \"text joins\" are best done inside an RDBMS, which is where the data is likely to reside. Unfortunately, computing an exact answer to a text join can be expensive. We propose an approximate, sampling-based text join execution strategy that can be robustly executed in a standard, unmodified RDBMS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1684012",
                    "name": "L. Gravano"
                },
                {
                    "authorId": "2942126",
                    "name": "Panagiotis G. Ipeirotis"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "78b729049a0135dc75a021ce5bbc127902253fde",
            "title": "Text joins in an RDBMS for web data integration",
            "abstract": "The integration of data produced and collected across autonomous, heterogeneous web services is an increasingly important and challenging problem. Due to the lack of global identifiers, the same entity (e.g., a product) might have different textual representations across databases. Textual data is also often noisy because of transcription errors, incomplete information, and lack of standard formats. A fundamental task during data integration is matching of strings that refer to the same entity. In this paper, we adopt the widely used and established cosine similarity metric from the information retrieval field in order to identify potential string matches across web sources. We then use this similarity metric to characterize this key aspect of data integration as a join between relations on textual attributes, where the similarity of matches exceeds a specified threshold. Computing an exact answer to the text join can be expensive. For query processing efficiency, we propose a sampling-based join approximation strategy for execution in a standard, unmodified relational database management system (RDBMS), since more and more web sites are powered by RDBMSs with a web-based front end. We implement the join inside an RDBMS, using SQL queries, for scalability and robustness reasons. Finally, we present a detailed performance evaluation of an implementation of our algorithm within a commercial RDBMS, using real-life data sets. Our experimental results demonstrate the efficiency and accuracy of our techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1684012",
                    "name": "L. Gravano"
                },
                {
                    "authorId": "2942126",
                    "name": "Panagiotis G. Ipeirotis"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "908f7db1b21067ee9f9442bccce4787d9ad5634f",
            "title": "Panel: querying networked databases",
            "abstract": "1. MOTIVATION A large number of useful databases are currently accessible over the Web and within corporate networks. In addition to being frequently updated, this collection of databases tends to be highly dynamic: new databases appear often, and databases (just like Web sites) also disappear. In this environment, the goal of providing flexible, timely and declarative query access over all these databases remains elusive.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "aec61cf9a776c18544ed5eefdf5cafb398df78b9",
            "title": "Index-based approximate XML joins",
            "abstract": "XML data integration tools are facing a variety of challenges for their efficient and effective operation. Among these is the requirement to handle a variety of inconsistencies or mistakes present in the data sets. We study the problem of integrating XML data sources through index assisted join operations, using notions of approximate match in the structure and content of XML documents as the join predicate. We show how a well known and widely deployed index structure, namely the R-tree, can be adopted to improve the performance of such operations. We propose novel search and join algorithms for R-trees adopted to index XML document collections. We also propose novel optimization objectives for R-tree construction, making R-trees better suited for this application.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144392463",
                    "name": "S. Guha"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2087140577",
                    "name": "T. Yu"
                }
            ]
        },
        {
            "paperId": "bfc5ecd4337a6718b98f321bfd956933e05df222",
            "title": "Approximate matching in XML",
            "abstract": "The success of XML lies in its ability to easily represent homogeneous data as well as heterogeneous data. In particular, features such as optionality (e.g., a book may or may not have an associated cdrom), repetition (e.g., a chapter may have zero or more sections), alternation (e.g., a book may have either editors or authors),and nesting (e.g., a section may have nested sections) permit considerable variability among XML data conforming to the same schema. When querying or correlating such heterogeneous XML data, exact matching is typically inadequate, resulting in either too few or too many matches. Approximate matching, along with ranking the results of this matching, in the same spirit as Information Retrieval (IR) approaches, is more appropriate.Flexible specification of approximate matching over structure and content, and efficient evaluation of such specifications, create new challenges and exciting opportunities for the database research and development communities. In this seminar, we survey the research in the database and IR communities on this subject, including language proposals for the flexible specification of approximate matching in XML, and optimized evaluation strategies for approximate matching.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "d561d3af0484de865a5d9b7f805224a23f03dce2",
            "title": "Iterative multi-tier management information modeling",
            "abstract": "The management information models currently used in the Internet have several limitations. Some of them contain errors, are missing important features, or are difficult to understand. Second, standards bodies keep reinventing the wheel, which confuses the terminology (hence customers) and wastes precious time. Third, finding a good balance between too abstract, and overly detailed models is a tough challenge, rarely achieved in practice. Last, the learning curve of existing data models is too steep. We propose to alleviate these problems by adopting a new process for designing and standardizing management information models. It is inspired by two techniques form software engineering: the iterative and incremental software development process, which addresses the shortcomings of the waterfall process usually adhered to by the IETF and DMTF; and multi-tier models, which capture different perspectives (e.g., analysis, design, and implementation) of the information model. Our main innovations are management-architecture-neutral universal information models (UIMs), sharing of conceptual models by different standard bodies, and specialization of the people involved in designing the different layers of the models. Our new process takes into account a number of constraints identified in real-life environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1402987132",
                    "name": "J. Martin-Flatin"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "3271498",
                    "name": "A. Westerinen"
                }
            ]
        },
        {
            "paperId": "e86d5e8c2fef08fada0f0a7362312108c5a502fe",
            "title": "XML publishing: look at siblings too!",
            "abstract": "In order to publish a nested XML document from flat relational data, multiple SQL queries are often needed. The efficiency of publishing relies on how fast these queries can be evaluated and their results shipped to the client. We illustrate novel optimization techniques that enable computation sharing between queries that construct sibling elements in the XML tree. Such queries typically share large common join expressions that can be exploited through appropriate rewritings. These rewritings are fundamental to XML publishing and provide considerable performance benefits without having to modify the relational engine.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "1751420",
                    "name": "Y. Kotidis"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "f996f5ed468126a655b3c49b8f3b83ba511dcddb",
            "title": "Ranked join indices",
            "abstract": "A plethora of data sources contain data entities that could be ordered according to a variety of attributes associated with the entities. Such orderings result effectively in a ranking of the entities according to the values in the attribute domain. Commonly, users correlate such sources for query processing purposes through join operations. In query processing, it is desirable to incorporate user preferences towards specific attributes or their values. A way to incorporate such preferences is by utilizing scoring functions that combine user preferences and attribute values and return a numerical score for each tuple in the join result. Then, a target query, which we refer to as top-k join query, seeks to identify the k tuples in the join result with the highest scores. We propose a novel technique, which we refer to as ranked join index, to efficiently answer top-k join queries for arbitrary, user specified, preferences and a large class of scoring functions. Our rank join index requires small space (compared to the entire join result) and provides guarantees for its performance. Moreover, our proposal provides a graceful tradeoff between its space requirements and worst case search performance. We supplement our analytical results with a thorough experimental evaluation using a variety of real and synthetic data sets, demonstrating that, in comparison to other viable approaches, our technique offers significant performance benefits.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1701195",
                    "name": "Panayiotis Tsaparas"
                },
                {
                    "authorId": "1725167",
                    "name": "Themis Palpanas"
                },
                {
                    "authorId": "1751420",
                    "name": "Y. Kotidis"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "f9df398e5e8e173111a27a12da5ee72f1722ad11",
            "title": "Navigation- vs. index-based XML multi-query processing",
            "abstract": "XML path queries form the basis of complex filtering of XML data. Most current XML path query processing techniques can be divided in two groups. Navigation-based algorithms compute results by analyzing an input document one tag at a time. In contrast, index-based algorithms take advantage of precomputed numbering schemes over the input XML document. We introduce a new index-based technique, index-filter, to answer multiple XML path queries. Index-filter uses indexes built over the document tags to avoid processing large portions of the input document that are guaranteed not to be part of any match. We analyze index-filter and compare it against Y-filter, a state-of-the-art navigation-based technique. We show that both techniques have their advantages, and we discuss the scenarios under which each technique is superior to the other one. In particular, we show that while most XML path query processing techniques work off SAX events, in some cases it pays off to preprocess the input document, augmenting it with auxiliary information that can be used to evaluate the queries faster. We present experimental results over real and synthetic XML documents that validate our claims.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2362833",
                    "name": "Nicolas Bruno"
                },
                {
                    "authorId": "1684012",
                    "name": "L. Gravano"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "101af12224e5a8b942e6724a7feff6c352826a94",
            "title": "Holistic twig joins: optimal XML pattern matching",
            "abstract": "XML employs a tree-structured data model, and, naturally, XML queries specify patterns of selection predicates on multiple elements related by a tree structure. Finding all occurrences of such a twig pattern in an XML database is a core operation for XML query processing. Prior work has typically decomposed the twig pattern into binary structural (parent-child and ancestor-descendant) relationships, and twig matching is achieved by: (i) using structural join algorithms to match the binary relationships against the XML database, and (ii) stitching together these basic matches. A limitation of this approach for matching twig patterns is that intermediate result sizes can get large, even when the input and output sizes are more manageable.In this paper, we propose a novel holistic twig join algorithm, TwigStack, for matching an XML query twig pattern. Our technique uses a chain of linked stacks to compactly represent partial results to root-to-leaf query paths, which are then composed to obtain matches for the twig pattern. When the twig pattern uses only ancestor-descendant relationships between elements, TwigStack is I/O and CPU optimal among all sequential algorithms that read the entire input: it is linear in the sum of sizes of the input lists and the final result list, but independent of the sizes of intermediate results. We then show how to use (a modification of) B-trees, along with TwigStack, to match query twig patterns in sub-linear time. Finally, we complement our analysis with experimental results on a range of real and synthetic data, and query twig patterns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2362833",
                    "name": "Nicolas Bruno"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "1c1644f5dae06546b7116473d4ff347b7bbf1c9e",
            "title": "A mapping schema and interface for XML stores",
            "abstract": "Most XML storage efforts have focused on mapping documents to relational databases. Mapping choices range from storing documents verbatim to shredding documents into relations in various ways. These choices are usually hard-coded into each storage system which makes sharing loading and querying utilities and exchanging information between different XML storage systems hard. To address these issues, we designed MXM and IMXM, a mapping schema and an interface API to define and query XML-to-relational mappings.A mapping is expressed as an instance of MXM. MXM is declarative, concise and captures most existing XML-to-relational mappings. Mappings can be expressed for documents for which no schema information is provided or documents that conform to either a DTD or an XML Schema. IMXM is an interface that allows querying of information contained in a MXM mapping. IMXM is designed as a library of functions which makes it easy to use inside any utility or application that needs to gain access to the XML-to-relational mapping. MXM is extensible and can incorporate new XML-to-relational mappings. We implemented a prototype to define mappings as instances of MXM and generate a repository of meta information on the XML and the relational data and the mapping choices. We implemented IMXM on top of this repository and used it for generating a relational schema and loading XML documents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "44020230313fd1c54d4b63555cd35c9eacb25410",
            "title": "Logical and physical support for heterogeneous data",
            "abstract": "Heterogeneity arises naturally in virtually all real-world data. This paper presents evolutionary extensions to a relational database system for supporting three classes of data heterogeneity: variational, structural and annotational heterogeneities. We define these classes and show the impact of these new features on data storage, data-access mechanisms, and the data-description language. Since XML is an important source of heterogeneity, we describe how the system automatically utilizes these new features when storing XML documents.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "35941003",
                    "name": "M. Fern\u00e1ndez"
                },
                {
                    "authorId": "38930390",
                    "name": "R. Greer"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "72e9899bf60d1cb5af9fe5767c5a19a901e13385",
            "title": "Approximate XML joins",
            "abstract": "XML is widely recognized as the data interchange standard for tomorrow, because of its ability to represent data from a wide variety sources. Hence, XML is likely to be the format through which data from multiple sources is integrated.In this paper we study the problem of integrating XML data sources through correlations realized as join operations. A challenging aspect of this operation is the XML document structure. Two documents might convey approximately or exactly the same information but may be quite different in structure. Consequently approximate match in structure, in addition to, content has to be folded in the join operation. We quantify approximate match in structure and content using well defined notions of distance. For structure, we propose computationally inexpensive lower and upper bounds for the tree edit distance metric between two trees. We then show how the tree edit distance, and other metrics that quantify distance between trees, can be incorporated in a join framework. We introduce the notion of reference sets to facilitate this operation. Intuitively, a reference set consists of data elements used to project the data space. We characterize what constitutes a good choice of a reference set and we propose sampling based algorithms to identify them. This gives rise to a variety of algorithmic approaches for the problem, which we formulate and analyze. We demonstrate the practical utility of our solutions using large collections of real and synthetic XML data sets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144392463",
                    "name": "S. Guha"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2087140577",
                    "name": "T. Yu"
                }
            ]
        },
        {
            "paperId": "7509931a7cafe9b0a61337d141d79d76eadaaeed",
            "title": "Fast algorithms for hierarchical range histogram construction",
            "abstract": "Data Warehousing and OLAP applications typically view data an having multiple logical dimensions (e.g., product, location) with natural hierarchies defined on each dimension. OLAP queries usually involve hierarchical selections on some of the dimensions, and often aggregate measure attributes (e.g., sales, volume). Accurately estimating the distribution of measure attributes, under hierarchical selections, is important in a variety of scenarios, including approximate query evaluation and cost-based optimization of queries.In this paper, we propose fast (near linear time) algorithms for the problem of approximating the distribution of measure attributes with hierarchies defined on them, using histograms. Our algorithms are based on dynamic programming and a novel notion of sparse intervals that we introduce, and are the first practical algorithms for this problem. They effectively trade space for construction time without compromising histogram accuracy. We complement our analytical contributions with an experimental evaluation using real data sets, demonstrating the superiority of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144392463",
                    "name": "S. Guha"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "a2967aec77ad5e2bc791903932b61a1c7e187b3b",
            "title": "Structural joins: a primitive for efficient XML query pattern matching",
            "abstract": "XML queries typically specify patterns of selection predicates on multiple elements that have some specified tree structured relationships. The primitive tree structured relationships are parent-child and ancestor-descendant, and finding all occurrences of these relationships in an XML database is a core operation for XML query processing. We develop two families of structural join algorithms for this task: tree-merge and stack-tree. The tree-merge algorithms are a natural extension of traditional merge joins and the multi-predicate merge joins, while the stack-tree algorithms have no counterpart in traditional relational join processing. We present experimental results on a range of data and queries using the TIMBER native XML query engine built on top of SHORE. We show that while, in some cases, tree-merge algorithms can have performance comparable to stack-tree algorithms, in many cases they are considerably worse. This behavior is explained by analytical results that demonstrate that, on sorted inputs, the stack-tree algorithms have worst-case I/O and CPU complexities linear in the sum of the sizes of inputs and output, while the tree-merge algorithms do not have the same guarantee.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51461409",
                    "name": "S. Al-Khalifa"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "49111633",
                    "name": "J. Patel"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "50118077",
                    "name": "Yuqing Wu"
                }
            ]
        },
        {
            "paperId": "b5c405718cc897c0226f78e3c268e4942a5bcae1",
            "title": "Efficient OLAP query processing in distributed data warehouses",
            "abstract": "The success of Internet applications has led to an explosive growth in the demand for bandwidth from ISPs. Managing an IP network includes complex data analysis that can often be expressed as OLAP queries. Current day OLAP tools assume the availability of the detailed data in a centralized warehouse. However, the inherently distributed nature of the data collection (e.g., flow-level traffic statistics are gathered at network routers) and the huge amount of data extracted at each collection point (of the order of several gigabytes per day for large IP networks) makes such an approach highly impractical. The natural solution to this problem is to maintain a distributed data warehouse, consisting of multiple local data warehouses (sites) adjacent to the collection points, together with a coordinator. In order for such a solution to make sense, we need a technology for distributed processing of complex OLAP queries. We have developed the Skalla system for this task. We conducted an experimental study of the Skalla evaluation scheme using TPC(R) data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2477303",
                    "name": "M. Akinde"
                },
                {
                    "authorId": "2272920683",
                    "name": "Michael H. B\u00f6hlen"
                },
                {
                    "authorId": "145633895",
                    "name": "T. Johnson"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "43fc0302e46e91ffcbae49a2aae1a32dd0e927d9",
            "title": "Using q-grams in a DBMS for Approximate String Processing.",
            "abstract": "String data is ubiquitous, and its management has taken on particular importance in the past few years. Approximate queries are very important on string data. This is due, for example, to the prevalence of typographical errors in data, and multiple conventions for recording attributes such as name and address. Commercial databases do not support approximate string queries directly, and it is a challenge to implement this functionality efficiently with user-defined functions (UDFs). In this paper, we develop a technique for building approximate string processing capabilities on top of commercial databases by exploiting facilities already available in them. At the core, our technique relies on generating short substrings of length q, called q-grams, and processing them using standard methods available in the DBMS. The proposed technique enables various approximate string processing methods in a DBMS, for example approximate (sub)string selections and joins, and can even be used with a variety of possible edit distance functions. The approximate string match predicate, with a suitable edit distance threshold, can be mapped into a vanilla relational expression and optimized by conventional relational optimizers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1684012",
                    "name": "L. Gravano"
                },
                {
                    "authorId": "2942126",
                    "name": "Panagiotis G. Ipeirotis"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                },
                {
                    "authorId": "152320428",
                    "name": "Lauri Pietarinen"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "5737a1f6fd8d928b88726ada916d7874afdfe0d7",
            "title": "Approximate String Joins in a Database (Almost) for Free",
            "abstract": "String data is ubiquitous, and its management has taken on particular importance in the past few years. Approximate queries are very important on string data especially for more complex queries involving joins. This is due, for example, to the prevalence of typographical errors in data, and multiple conventions for recording attributes such as name and address. Commercial databases do not support approximate string joins directly, and it is a challenge to implement this functionality efficiently with user-defined functions (UDFs). In this paper, we develop a technique for building approximate string join capabilities on top of commercial databases by exploiting facilities already available in them. At the core, our technique relies on matching short substrings of length , called -grams, and taking into account both positions of individual matches and the total number of such matches. Our approach applies to both approximate full string matching and approximate substring matching, with a variety of possible edit distance functions. The approximate string match predicate, with a suitable edit distance threshold, can be mapped into a vanilla relational expression and optimized by conventional relational optimizers. We demonstrate experimentally the benefits of our technique over the direct use of UDFs, using commercial database systems and real data. To study the I/O and CPU behavior of approximate string join algorithms with variations in edit distance and -gram length, we also describe detailed experiments based on a prototype implementation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1684012",
                    "name": "L. Gravano"
                },
                {
                    "authorId": "2942126",
                    "name": "Panagiotis G. Ipeirotis"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "575265124fe11196c1e2e77a5466d48a988024d6",
            "title": "On computing correlated aggregates over continual data streams",
            "abstract": "In many applications from telephone fraud detection to network management, data arrives in a stream, and there is a need to maintain a variety of statistical summary information about a large number of customers in an online fashion. At present, such applications maintain basic aggregates such as running extrema values (MIN, MAX), averages, standard deviations, etc., that can be computed over data streams with limited space in a straightforward way. However, many applications require knowledge of more complex aggregates relating different attributes, so-called correlated aggregates. As an example, one might be interested in computing the percentage of international phone calls that are longer than the average duration of a domestic phone call. Exact computation of this aggregate requires multiple passes over the data stream, which is infeasible.\nWe propose single-pass techniques for approximate computation of correlated aggregates over both landmark and sliding window views of a data stream of tuples, using a very limited amount of space. We consider both the case where the independent aggregate (average duration in the example above) is an extrema value and the case where it is an average value, with any standard aggregate as the dependent aggregate; these can be used as building blocks for more sophisticated aggregates. We present an extensive experimental study based on some real and a wide variety of synthetic data sets to demonstrate the accuracy of our techniques. We show that this effectiveness is explained by the fact that our techniques exploit monotonicity and convergence properties of aggregates over data streams.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143614516",
                    "name": "J. Gehrke"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "6cdb3e2e43ff5be37cf053fb018b9d4fa30e2a77",
            "title": "Minimization of tree pattern queries",
            "abstract": "Tree patterns forms a natural basis to query tree-structured data such as XML and LDAP. Since the efficiency of tree pattern matching against a tree-structured database depends on the size of the pattern, it is essential to identify and eliminate redundant nodes in the pattern and do so as quickly as possible. In this paper, we study tree pattern minimization both in the absence and in the presence of integrity constraints (ICs) on the underlying tree-structured database.\nWhen no ICs are considered, we call the process of minimizing a tree pattern, constraint-independent minimization. We develop a polynomial time algorithm called CIM for this purpose. CIM's efficiency stems from two key properties: (i) a node cannot be redundant unless its children are, and (ii) the order of elimination of redundant nodes is immaterial. When ICs are considered for minimization, we refer to it as constraint-dependent minimization. For tree-structured databases, required child/descendant and type co-occurrence ICs are very natural. Under such ICs, we show that the minimal equivalent query is unique. We show the surprising result that the algorithm obtained by first augmenting the tree pattern using ICS, and then applying CIM, always finds the unique minimal equivalent query; we refer to this algorithm as ACIM. While ACIM is also polynomial time, it can be expensive in practice because of its inherent non-locality. We then present a fast algorithm, CDM, that identifies and eliminates local redundancies due to ICs, based on propagating \u201cinformation labels\u201d up the tree pattern. CDM can be applied prior to ACIM for improving the minimization efficiency. We complement our analytical results with an experimental study that shows the effectiveness of our tree pattern minimization techniques.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1403657578",
                    "name": "S. Amer-Yahia"
                },
                {
                    "authorId": "2690683",
                    "name": "SungRan Cho"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "8c9259d24a76630009c440b08e2a535899aa180d",
            "title": "Counting twig matches in a tree",
            "abstract": "Describes efficient algorithms for accurately estimating the number of matches of a small node-labeled tree, i.e. a twig, in a large node-labeled tree, using a summary data structure. This problem is of interest for queries on XML and other hierarchical data, to provide query feedback and for cost-based query optimization. Our summary data structure scalably represents approximate frequency information about twiglets (i.e. small twigs) in the data tree. Given a twig query, the number of matches is estimated by creating a set of query twiglets, and combining two complementary approaches: set hashing, used to estimate the number of matches of each query twiglet, and maximal overlap, used to combine the query twiglet estimates into an estimate for the twig query. We propose several estimation algorithms that apply these approaches on query twiglets formed using variations on different twiglet decomposition techniques. We present an extensive experimental evaluation using several real XML data sets, with a variety of twig queries. Our results demonstrate that accurate and robust estimates can be achieved, even with limited space.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111630539",
                    "name": "Zhiyuan Chen"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "dfd790fd02afea1de60cba77a3b6b98640bea491",
            "title": "Two-dimensional substring indexing",
            "abstract": "As databases have expanded in scope to storing string data (XML documents, product catalogs), it has become increasingly important to search databases based on matching substrings, often on multiple, correlated dimensions. While string B-trees are I/O optimal in one dimension, no index structure with non-trivial query bounds is known for two-dimensional substring indexing. In this paper, we present a technique for two-dimensional substring indexing based on a reduction to the geometric problem of identifying common colors in two ranges containing colored points. We develop an I/O efficient algorithm for solving the common colors problem, and use it to obtain an I/O efficient (poly-logarithmic query time) algorithm for the two-dimensional substring indexing problem. Our techniques result in a family of secondary memory index structures that trade space for time, with no loss of accuracy. We show how our technique can be practically realized using a combination of string B-trees and R-trees.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1681278",
                    "name": "P. Ferragina"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                }
            ]
        },
        {
            "paperId": "6556cea7f91c1221dd2693aba0d3dd45d2e09e6d",
            "title": "Optimal histograms for hierarchical range queries (extended abstract)",
            "abstract": "Now there is tremendous interest in data warehousing and OLAP applications. OLAP applications typically view data as having multiple logical dimensions (e.g., product, location) with natural hierarchies de ned on each dimension, and analyze the behavior of various measure attributes (e.g., sales, volume) in terms of the dimensions. OLAP queries typically involve hierarchical selections on some of the dimensions (e.g., product is classi ed under the jeans product category, or location is in the north-east region), often aggregating measure attributes (see, e.g., [6]). Cost-based query optimization of such OLAP queries needs good estimates of the selectivity of hierarchical selections. Histograms capture attribute value distribution statistics in a space-e cient fashion. They have been designed to work well for numeric attribute value domains, and have long been used to support cost-based query optimization in databases [11, 9, 2, 4, 10, 5]. Histograms can be used to estimate the selectivity of OLAP queries by modeling the (hierarchical) conditions on a given dimension as a set of hierarchical ranges (i.e., two ranges are either disjoint or one is contained in the other), and using standard range selectivity estimation techniques (see, e.g., [10]). The quality of selectivity estimates obtained using a histogram depends on computing a good solution to the histogram construction problem, and there has been considerable recent e ort in this area (see, e.g., [10, 5]). However, while OLAP queries make extensive use of hierarchical selection conditions, previous works on computing good histograms, for the most part, consider only equality queries when computing the error incurred by a particular choice of histogram bucket boundaries. This mismatch between the nature of OLAP queries, and the class of queries considered when constructing histograms can result in poor selectivity estimates for OLAP queries. In this paper, we address this problem and focus on e ciently computing optimal histograms for the case of hierarchical range queries. W e make the following contributions:",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "144963537",
                    "name": "S. Muthukrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "80660b0a3ed986eccc144090b2fef3a37dd632f8",
            "title": "Directories: Managing Data for Networked Applications",
            "abstract": "Directories have recently emerged as an essential component of the network infrastructure, and are being used to store a wide variety of information to support network applications. These include address books for messaging applications, user preferences for configuration management, access control lists and certificates for security applications, and profiles and policies in the DEN (directory enabled networks) initiative. Network directory data is typically heterogeneous, highly distributed and replicated, requiring autonomy across multiple directory servers while allowing for conceptual unity, in a way that is not well supported by conventional relational or object-oriented databases. Efficient management of such data creates new challenges and exciting opportunities for the database community.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "dd20a4ba7df272adbbdbf3d92fb176358a0e7297",
            "title": "On effective multi-dimensional indexing for strings",
            "abstract": "As databases have expanded in scope from storing purely business data to include XML documents, product catalogs, e-mail messages, and directory data, it has become increasingly important to search databases based on wild-card string matching: prefix matching, for example, is more common (and useful) than exact matching, for such data. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the dimensions. Traditional multi-dimensional index structures, designed with (fixed length) numeric data in mind, are not suitable for matching unbounded length string data.\nIn this paper, we describe a general technique for adapting a multi-dimensional index structure for wild-card indexing of unbounded length string data. The key ideas are (a) a carefully developed mapping function from strings to rational numbers, (b) representing an unbounded length string in an index leaf page by a fixed length offset to an external key, and (c) storing multiple elided tries, one per dimension, in an index page to prune search during traversal of index pages. These basic ideas affect all index algorithms. In this paper, we present efficient algorithms for different types of string matching.\nWhile our technique is applicable to a wide range of multi-dimensional index structures, we instantiate our generic techniques by adapting the 2-dimensional R-tree to string data. We demonstrate the space effectiveness and time benefits of using the string R-tree both analytically and experimentally.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "20dd934a841f775ed18e0d2f8e887f363cbb9099",
            "title": "Querying network directories",
            "abstract": "Heirarchically structured directories have recently proliferated with the growth of the Internet, and are being used to store not only address books and contact information for people, but also personal profiles, network resource information, and network and service policies. These systems provide a means for managing scale and heterogeneity, while allowing for conceptual unity and autonomy across multiple directory servers in the network, in a way for superior to what conventional relational or object-oriented databases offer. Yet, in deployed systems today, much of the data is modeled in an ad hoc manner, and many of the more sophisticated \u201cqueries\u201d involve navigational access.\nIn this paper, we develop the core of a formal data model for network directories, and propose a sequence of efficiently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the efficiency of each algorithm in terms of its I/O complexity. Our data model and query languages share the flexibility and utility of the recent proposals for semi-structured data models, while at the same time effectively addressing the specific needs of network directory applications, which we demonstrate by means of a representative real-life example.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "1702212",
                    "name": "Tova Milo"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1778524",
                    "name": "D. Vista"
                }
            ]
        },
        {
            "paperId": "275621632943d88b5403b33c440a9b944819a903",
            "title": "Snakes and sandwiches: optimal clustering strategies for a data warehouse",
            "abstract": "Physical layout of data is a crucial determinant of performance in a data warehouse. The optimal clustering of data on disk, for minimizing expected I/O, depends on the query workload. In practice, we often have a reasonable sense of the likelihood of different classes of queries, e.g., 40% of the queries concern calls made from some specific telephone number in some month. In this paper, we address the problem of finding an optimal clustering of records of a fact table on disk, given an expected workload in the form of a probability distribution over query classes.\nAttributes in a data warehouse fact table typically have hierarchies defined on them (by means of auxiliary dimension tables). The product of the dimensional hierarchy levels forms a lattice and leads to a natural notion of query classes. Optimal clustering in this context is a combinatorially explosive problem with a huge search space (doubly exponential in number of hierarchy levels). We identify an important subclass of clustering strategies called lattice paths, and present a dynamic programming algorithm for finding the optimal lattice path clustering, in time linear in the lattice size. We additionally propose a technique called snaking, which when applied to a lattice path, always reduces its cost. For a representative class of star schemas, we show that for every workload, there is a snaked lattice path which is globally optimal. Further, we prove that the clustering obtained by applying snaking to the optimal lattice path is never much worse than the globally optimal snaked lattice path clustering. We complement our analyses and validate the practical utility of our techniques with experiments using TPC-D benchmark data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "3c7da5d87776db54d04978700f01f3c41156d3e2",
            "title": "What can Hierarchies do for Data Warehouses?",
            "abstract": "Data in a warehouse typically has multiple dimensions of interest, such as location, time, and product. It is well-recognized that these dimensions have hierarchies deened on them, such as \\store-city-state-region\" for location. The standard way to model such data is with a star/snowwake schema. However, current approaches do not give a rst-class status to dimensions. Consequently, a substantial class of interesting queries involving dimension hierarchies and their interaction with the fact tables are quite verbose to write, hard to read, and diicult to optimize. We propose the SQL(H) model and a natural extension to the SQL query language, that gives a rst-class status to dimensions, and we pin down its semantics. Our model permits structural and schematic heterogeneity in dimension hierarchies, situations often arising in practice that cannot be modeled satisfactorily using the star/snowwake approach. We show using examples that sophisticated queries involving dimension hierarchies and their interplay with aggregation can be expressed concisely in SQL(H). By comparison, expressing such queries in SQL would involve a union of numerous complex sequences of joins. Finally, we develop an eecient implementation strategy for computing SQL queries, based on an algorithm for hierarchical joins, and the use of dimension indexes. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "833c474964439246ecc4dcbe786b6df26ccbde16",
            "title": "Answering Queries Using Views.",
            "abstract": "We consider the problem of computing answers to queries by using materialized views. Aside from its potential in optimizing query evaluation, the problem also arises in applications such as Global Information Systems, Mobile Computing and maintaining physical data independence. We consider the problem of nding a rewriting of a query that uses the materialized views, the problem of nding minimal rewritings, and nding complete rewritings (i.e., rewritings that use only the views). We show that all the possible rewritings can be obtained by considering containment mappings from the views to the query, and that the problems we consider are NP-complete when both the query and the views are conjunctive and don't involve builtin comparison predicates. We show that the problem has two independent sources of complexity (the number of possible containment mappings, and the complexity of deciding which literals from the original query can be deleted). We describe a polynomial time algorithm for nding rewritings, and show that under certain conditions, it will nd the minimal rewriting. Finally, we analyze the complexity of the problems when the queries and views may be disjunctive and involve built-in comparison predicates.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50874323",
                    "name": "Alon Y. Levy"
                },
                {
                    "authorId": "1682406",
                    "name": "A. Mendelzon"
                },
                {
                    "authorId": "1714472",
                    "name": "Y. Sagiv"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "af5a145735128e1aa4782f8eb8981ff5e3292b2a",
            "title": "Revisiting the Hierarchical Data Model",
            "abstract": "SUMMARY Much of the data we deal with every day is organized hierarchically: file systems, library classification schemes and yellow page categories are salient examples. Business data too, benefits from a hierarchical organization, and indeed the hierarchical data model was quite prevalent thirty years ago. Due to the recently increased importance of X.500/LDAP directories, which are hierarchical, and the prevalence of aggregation hierarchies in datacubes, there is now renewed interest in the hierarchical organization of data. In this paper, we develop a framework for a modern hierarchical data model, substantially improved from the original version by taking advantage of the lessons learned in the relational database context. We argue that this new hierarchical data model has many benefits with respect",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2094211307",
                    "name": "Nonmembers"
                }
            ]
        },
        {
            "paperId": "bf1d7df019f8c5355758a65002dedc9690539819",
            "title": "Multi-Dimensional Substring Selectivity Estimation",
            "abstract": "With the explosion of the Internet, LDAP di-rectories and XML, there is an ever greater need to evaluate queries involving (sub)string matching. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the dimensions. E(cid:11)ective query optimization in this context requires good selectivity estimates. In this paper, we use multi-dimensionalcount-su(cid:14)x trees as the basic framework for sub-string selectivity estimation. Given the enormous size of these trees for large databases, we develop a space and time e(cid:14)cient probabilistic algorithm to construct multi-dimensional pruned count-su(cid:14)x trees directly. We then present two techniques to obtain good estimates for a given multi-dimensional sub-string matching query, using a pruned count-su(cid:14)x tree. The (cid:12)rst one, called GNO (for Greedy Non-Overlap), generalizes the greedy parsing suggested by Krishnan et al. [9] for one-dimensional substring selectivity estimation. The second one, called MO (for Maximal Overlap), uses all maximal multi-dimensional substrings of the query for estimation; these multi-dimensional substrings help to capture the correlation that may exist between strings (cid:3)",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1821764",
                    "name": "Olga Kapitskaia"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "c32df277282811c30a98042d4016ca2fb056607b",
            "title": "Substring selectivity estimation",
            "abstract": "With the explosion of the Internet, LDAP directories and XML, there is an ever greater need to evaluate queries involving (sub)string matching. Effective query optimization in this context requires good selectivity estimates. In this paper, we use pruned count-suffix trees as the basic framework for substring selectivity estimation. We present a novel technique to obtain a good estimate for a given substring matching query, called MO (for Maximal Overlap), that estimates the selectivity of a query based on all maximal substrings of the query in the pruned count-suffix tree. We show that MO is provably better than the (independence-based) substring selectivity estimation technique proposed by Krishnan et al. [6], called KVI, under the natural assumption that strings exhibit the so-called \u201cshort memory\u201d property. We complement our analysis with an experiment, using a real AT&T data set, that demonstrates that MO is substantially superior to KVI in the quality of the estimate. Finally, we develop and analyze two selectivity estimation algorithms, MOC and MOLC, based on MO and a constraint-based characterization of all possible completions of a given pruned count-suffix tree. We show that KVI, MO, MOC and MOLC illustrate an interesting tradeoff between estimation accuracy and computational efficiency. *This work was done when the author was at AT&T Labs-Research, Florham Park, NJ 07932, USA. +This work was done when the author was on sabbatical at AT&T Labs-Research, Florham Park, NJ 07932, USA. Permission to make digital or hard copies ol\u2019all or part ol\u2018this work for personal or classroom use is granted without fee provided that topics are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the lirst page. To copy otherwise, to republish, lo post on servers or to redistribute to lists. requires prior specific permission andior a fee. PODS \u2018W Philadelphia PA Copyright ACM 1999 1-58 113-062-7/99/05...$5.00",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1736021",
                    "name": "R. Ng"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "c777150ab228c2fca9149eff704dbd190b30cbc8",
            "title": "Hierarchical or relational? A case for a modern hierarchical data model",
            "abstract": "Much of the data we deal with every day is organized hierarchically: file systems, library classification schemes and yellow page categories are salient examples. Business data too, benefits from a hierarchical organization, and indeed the hierarchical data model was quite prevalent thirty years ago. Due to the recently increased importance of X.500/LDAP directories, which are hierarchical, and the prevalence of aggregation hierarchies in datacubes, there is now renewed interest in the hierarchical organization of data. We develop a framework for a modern hierarchical data model, substantially improved from the original version by taking advantage of the lessons learned in the relational database context. We argue that this new hierarchical data model has many benefits with respect to the ubiquitous flat relational data model. We argue also that this model is well-suited for representing XML, and for interchange of information across heterogeneous databases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "1708593",
                    "name": "L. Lakshmanan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "d340f6917d3872a5913a63e21439543edbf57f84",
            "title": "Using LDAP directory caches",
            "abstract": "Sophie Cluet* Olga Kapitskaia INRIA Rocquencourt AT&T Labs-Research Sophie.Cluet@inria.fr olga@research.att.com Divesh Srivastava AT&T Labs-Research divesh@research.att.com LDAP (Lightweight Directory Access Protocol) directories have recently proliferated with the growth of the Internet, and are being used in a wide variety of network-based applications to store data such as personal profiles, address books, and network and service policies. These systems provide a means for managing heterogeneity in a way far superior to what conventional relational or object-oriented databases can offer. To achieve fast performance for declarative query answering, it is desirable to use client caching based on semantic information (instead of individual directory entries). We formally consider the problem of reusing cached LDAP directory entries for answering declarative LDAP queries. A semantic LDAP directory cache contains directory entries, which are semantically described by a set of query templates. We show that, for conjunctive queries and LDAP directory caches with positive templates, the complexity of cache-answerability is NP-complete in the size of the query. For this case, we design a sound and complete algorithm for cache-answerability based on a suite of query transformations that capture the semantics of LDAP queries. We demonstrate the practicality of this algorithm for real applications with a performance evaluation, based on sample queries from a directory enabled application at AT&T Labs. When the query templates in the cache contain negation, we show that the complexity of cache-answerability of conjunctive LDAP queries is coNP complete in the size of the schema and query templates in the semantic description of the cache. Finally, we identify natural restrictions on the nature of the semantic descriptions for polynomial-time cache-answerability. *This work was done when the author was visiting AT&T Labs-Research, Florham Park, NJ 07932, USA. Permission to make digital or hard copies of all or part of this work thl personal or classroom USC is gralltcd without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the lirst page. To copy othetwise, to republish, to post on servers or to redistribute to lists. requires prior specific permission andior a fee. PODS \u201899 Philadelphia PA Copyright ACM 1999 I-581 13-062-7/99/05...$5.00",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1759150",
                    "name": "S. Cluet"
                },
                {
                    "authorId": "1821764",
                    "name": "Olga Kapitskaia"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "1cedb7d37a2a11c64fecb6a7cb87612f73d3808e",
            "title": "Focusing search in hierarchical structures with directory sets",
            "abstract": "Keyword-based searches on the World Wide Web are often of limited use, because they return too many uninteresting matches. We propose here a novel mechanism that permits the user to specify directory sets to restrict the space of documents searched, and, at the same time, increase the speed of the search. We view the Web as a single, huge hierarchy, reeected in the structure of the URLs. We refer to each sub-tree in this hierarchy as a directory, and group semantically related documents from multiple directories into a directory set. Starting from a collection of pre-deened directory sets, a user can dynamically generate new directory sets by using operators in a directory set algebra, and focus keyword-based searches to documents that belong to a (pre-deened or dynamically generated) directory set. We design algorithms for eeciently evaluating expressions in the directory set algebra, and describe a technique for tightly integrating directory sets into keyword-based search engines. Finally, we validate our ideas using a realistic case study for a sig-niicant fragment of the AT&T intranet, and 211 pre-deened directory sets. 1 Motivation The World Wide Web is a very large and rapidly growing repository of documents. The utility of the Web is considerably enhanced by the presence of search services (e.g., Alta Vista 1 , Yahoo 2) that locate documents matching user queries, and the explosion of search services (see, e.g., the search services listed under the \\Internet Search\" feature provided by Netscape Navigator and other Web browsers) is testimony to the increasing importance of the problem of searching for documents on the Web. There are two main approaches that have been used with considerable success on the Web: search engines and web guides. Search engines such as Alta Vista essentially use text indexing techniques to index a large fraction of the documents on the Web. Queries to these search engines are typically speciied by providing keywords, and the query answers are pointers (URLs) to matching documents. There are many advantages of this approach: several search engines do an eeective job of nding many possible matching documents, the index can be kept (reasonably) up-to-date in an automatic fashion, and the approach is scalable. The eeective-ness of these search engines is, however, limited by the fact that searching by keywords alone returns many matches that may not be of interest to the user, and the large number of matches makes it diicult to \u2026",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30422969",
                    "name": "G. Jacobson"
                },
                {
                    "authorId": "145846952",
                    "name": "B. Krishnamurthy"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                }
            ]
        },
        {
            "paperId": "98f2848be3f58ec8420ed15de37040fa7ad60980",
            "title": "Flexible list management in a directory",
            "abstract": "Lists of entities must often be speciied in many real-world applications such as customer lists, electronic distribution lists and access control lists. These lists are typically spec-iied through explicit enumeration, frequently aided by re-cursive expansion. In this paper, we discuss the declarative speciication and extraction of members of such lists as queries over a directory that maintains information both about individuals and about lists, and identify key features that the directory must support to manage lists in a exible manner. X.500 is the industry standard for modeling information about individuals in a directory, and LDAP is the proposed standard for accessing directory information. We have designed and built a system to represent and manage lists in the X.500 information model, and developed ee-ciently evaluable extensions to the LDAP query language for the location and expansion of lists. We describe the system architecture and the query evaluation algorithm of this system. Our system is deployed for use in the speciication and expansion of (organizational and personal) electronic messaging (e-mail, voice mail and FAX) distribution lists at AT&T Labs. 1 Motivation Lists of entities must often be speciied in many real-world applications such as prospective customer lists, lists of network resources, manufacturing parts lists, electronic distribution lists, and access control lists. These lists are typically speciied through explicit enumeration, frequently aided by recursive expansion: Prospective Customer Lists: Companies often use customer lists to market their products and services to targeted lists of prospective customers. For example, a pharmaceutical company may maintain a prospective customer list doctors, that includes addresses of doctors to whom it sends promotional mailings, and samples of new medicines. Electronic Distribution Lists: Distribution lists are the primary mechanism of sending an e-mail message to an intended list of recipients, without the sender having to know or enumerate explicitly their individual electronic mail addresses. For example, the distribution list dbworld maintained at the Computer Sciences Department at the University of Wisconsin{Madison includes most researchers in the world interested in databases. Access Control Lists: Secure systems often have lists of users with speciic access rights. For example, in the UNIX le system, read/write/execute access rights to individual documents associated with the HighFlier project of a corporation can be provided to members of the highflier access control list (or group), allowing for controlled sharing of information. The utility of these lists would be considerably enhanced if they could be used to specify \u2026",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "2111035923",
                    "name": "M. Jones"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1778524",
                    "name": "D. Vista"
                }
            ]
        },
        {
            "paperId": "9ca2f7851604ea2de6fe3f0a96b28eb1bed9a37a",
            "title": "Interaction of query evaluation and buffer management for information retrieval",
            "abstract": "The proliferation of the World Wide Web has brought information retrieval (IR) techniques to the forefront of search technology. To the average computer user, \u201csearching\u201d now means using IR-based systems for finding information on the WWW or in other document collections. IR query evaluation methods and workloads differ significantly from those found in database systems. In this paper, we focus on three such differences. First, due to the inherent fuzziness of the natural language used in IR queries and documents, an additional degree of flexibility is permitted in evaluating queries. Second, IR query evaluation algorithms tend to have access patterns that cause problems for traditional buffer replacement policies. Third, IR search is often an iterative process, in which a query is repeatedly refined and resubmitted by the user. Based on these differences, we develop two complementary techniques to improve the efficiency of IR queries: 1) Buffer-aware query evaluation, which alters the query evaluation process based on the current contents of buffers; and 2) Ranking-aware buffer replacement, which incorporates knowledge of the query processing strategy into replacement decisions. In a detailed performance study we show that using either of these techniques yields significant performance benefits and that in many cases, combining them produces even further improvements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50398858",
                    "name": "B. J\u00f3nsson"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "bcc525ce385cae3aa3354def9e7e0c889a4a4698",
            "title": "Selecting and Maintaining Materialized Views for Message Management",
            "abstract": "Electronic messaging has become one of the primary means for the dissemination, exchange and sharing of information. This is facilitated, especially within an organization, by the use of shared folders, which are supported by current electronic messaging systems. We demonstrate that considerable additional flexibility can be achieved by modeling the messaging system as a data warehouse, where each message is a tuple of attribute-value pairs, and each folder is a view on the set of all messages in the messaging system; both user mailboxes and current-day shared folders can be treated as specialized views. Supporting this paradigm in emerging messaging systems, which support thousands of users, makes it imperative to efficiently support a very large number of folders, each defined as a selection view: this is the key difference with conventional data warehouses. We identify two complementary problems concerning the design of such a messaging system. One of the most important tasks of the messaging system concerns the efficient incremental maintenance of eagerly maintained (materialized) folders. This problem for our model of folder definitions is a more general version of the classical point-location problem, and we design an I/O and CPU efficient algorithm for this problem, based on external segment trees and tries. A second important design decision that a messaging system needs to make is the choice of eagerly maintained folders. We present various special cases of the folder-selection problem in the context of messaging systems and present efficient exact/approximation algorithms, and complexity hardness results for them.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2055613584",
                    "name": "H. Gupta"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "310a3a30eb766ccf6e0a00c7862517b87542827c",
            "title": "Fast Computation of Sparse Datacubes",
            "abstract": "Datacube queries compute aggregates over database relations at a variety of granularities, and they constitute an important class of decision support queries. Real-world data is frequently sparse, and hence efficiently computing datacubes over large sparse relations is important. We show that current techniques for computing datacubes over sparse relations do not scale well with the number of CUBE BY attributes, especially when the relation is much larger than main memory. We propose a novel algorithm for the fast computation of datacubes over sparse relations, and demonstrate the efficiency of our algorithm using synthetic, benchmark and real-world data sets. When the relation fits in memory, our technique performs multiple in-memory sorts, and does not incur any I/O beyond the input of the relation and the output of the datacube itself. When the relation does not fit in memory, a divideand-conquer strategy divides the problem of computing the datacube into several simpler computations of sub-datacubes. Often, all but one of the sub-datacubes can be computed in memory and our in-memory solution applies. In that case, the total I/O overhead is linear in the number of CUBE BY attributes. We demonstrate with an implementation that the CPU cost of our algorithm is dominated by the I/O cost for sparse relations. \u2018The research of Kenneth A.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144812095",
                    "name": "K. A. Ross"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "3599c62ad98d78314276f73dbde43fac5d5a65b7",
            "title": "Semantic Data Caching and Replacement",
            "abstract": "We propose a semantic model for client-side caching and replacement in a client-server database system and compare this approach to page caching and tuple caching strategies. Our caching model is based on, and derives its advantages from, three key ideas. First, the client maintains a semantic description of the data in its cache,which allows for a compact specification, as a remainder query, of the tuples needed to answer a query that are not available in the cache. Second, usage information for replacement policies is maintained in an adaptive fashion for semantic regions, which are associated with collections of tuples. This avoids the high overheads of tuple caching and, unlike page caching, is insensitive to bad clustering. Third, maintaining a semantic description of cached data enables the use of sophisticated value functions that incorporate semantic notions of locality, not just LRU or MRU, for cache replacement. We validate these ideas with a detailed performance study that includes traditional workloads as well as a workload motivated by a mobile navigation application.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35612310",
                    "name": "Shaul Dar"
                },
                {
                    "authorId": "143666627",
                    "name": "M. Franklin"
                },
                {
                    "authorId": "50398858",
                    "name": "B. J\u00f3nsson"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2068027542",
                    "name": "Michael Tan"
                }
            ]
        },
        {
            "paperId": "437b328a1a62eeb85aaa87514a13010a4df315ee",
            "title": "Answering Queries with Aggregation Using Views",
            "abstract": "We present novel algorithms for the problem of using materialized views to compute answers to SQL queries with grouping and aggregation, in the presence of multiset tables. ln addition to its obvious potential in query optimization, this problem is important in many applications, such as data warehousing, very large transaction recording systems, global information systems and mobile computing, where access to local or cached materialized views may be cheaper than access to the underlying database. Our contributions are the following: First, we show that in the case where the query has grouping and aggregation but the views do not, a view is usable in answering a query only if there is an isomorphism between the view and a portion of the query. Second, when the views also have grouping and aggregation we identify conditions under which the aggregation information present in a view is sufficient to perform the aggregation computations requited in the query. The algorithms we describe for rewriting a query also consider the case in which the rewritten query may be a union of single-block queries. Our approach is a semantic one, in that it detects when the information existing in a view is sufficient to answer a query. In contrast, previous work performed syntactic transformations on the query such that the definition of the view would be a sub-part of the definition of the query. Consequently, these methods can only detect usages of views in limited cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "35612310",
                    "name": "Shaul Dar"
                },
                {
                    "authorId": "145531067",
                    "name": "H. V. Jagadish"
                },
                {
                    "authorId": "50874323",
                    "name": "Alon Y. Levy"
                }
            ]
        },
        {
            "paperId": "72becee18a4507ed78e9f8d20e0f0785ef6f130a",
            "title": "IDEA: interactive data exploration and analysis",
            "abstract": "The analysis of business data is often an ill-defined task characterized by large amounts of noisy data. Because of this, business data analysis must combine two kinds of intertwined tasks: exploration and analysis. Exploration is the process of finding the appropriate subset of data to analyze, and analysis is the process of measuring the data to provide the business answer. While there are many tools available both for exploration and for analysis, a single tool or set of tools may not provide full support for these intertwined tasks. We report here on a project that set out to understand a specific business data analysis problem and build an environment to support it. The results of this understanding are, first of all, a detailed list of requirements of this task; second, a set of capabilities that meet these requirements; and third, an implemented client-server solution that addresses many of these requirements and identifies others for future work. Our solution incorporates several novel perspectives on data analysis and combines a history mechanism with a graphical, re-usable representation of the analysis and exploration process. Our approach emphasizes using the database itself to represent as many of these functions as possible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1690361",
                    "name": "P. Selfridge"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2074125103",
                    "name": "Lynn O. Wilson"
                }
            ]
        },
        {
            "paperId": "8200b09a46903ad3b0ac329bf71a082aa6c20b47",
            "title": "Materialized view maintenance and integrity constraint checking: trading space for time",
            "abstract": "We investigate the problem of incremental maintenance of an SQL view in the face of database updates, and show that it is possible to reduce the total time cost of view maintenance by materializing (and maintaining) additional views. We formulate the problem of determining the optimal set of additional views to materialize as an optimization problem over the space of possible view sets (which includes the empty set). The optimization problem is harder than query optimization since it has to deal with multiple view sets, updates of multiple relations, and multiple ways of maintaining each view set for each updated relation.We develop a memoing solution for the problem; the solution can be implemented using the expression DAG representation used in rule-based optimizers such as Volcano. We demonstrate that global optimization cannot, in general, be achieved by locally optimizing each materialized subview, because common subexpressions between different materialized subviews can allow nonoptimal local plans to be combined into an optimal global plan. We identify conditions on materialized subviews in the expression DAG when local optimization is possible. Finally, we suggest heuristics that can be used to efficiently determine a useful set of additional views to materialize.Our results are particularly important for the efficient checking of assertions (complex integrity constraints) in the SQL-92 standard, since the incremental checking of such integrity constraints is known to be essentially equivalent to the view maintenance problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144812095",
                    "name": "K. A. Ross"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "143761034",
                    "name": "S. Sudarshan"
                }
            ]
        },
        {
            "paperId": "94afd68620f589746988bd424a5a23821a80e478",
            "title": "Chasing constrained tuple-generating dependencies",
            "abstract": "We investigate the implication problem for constrained tuple-generating dependencies (CTGDs), the extension of tuple-and equality-generating dependencies that permits expression of semantic relations (constraints) on variables. The implication problem is central to identifying redundant integrity constraints, checking integrity constraints on constraint databases, detecting the independence of queries and updates, and optimizing queries. We provide two chase procedures for the implication problem. The (cid:12)rst is cautious, generating tuples and constraints only when justi(cid:12)ed, whereas the second is speculative, generating tuples and constraints that have attached conditions about when they exist/hold. The cautious chase is more e(cid:14)cient, in some sense, but less powerful in demonstrating that a CTGD is implied. We demonstrate that, for constraint domains with Independence of Negative Constraints, the two chase procedures are equally powerful. The cautious chase is thus the chase of choice for such constraint domains, and can be used as a weak implication procedure for other constraint domains. We describe the conditions under which the chase procedures can be terminated early without weakening them. We develop a form of magic sets optimization for making the chase procedures for CTGDs goal-directed; this is the (cid:12)rst such use of magic sets in chase procedures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46512381",
                    "name": "Michael J. Maher"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "a97380c5c44691164f526df0f02f9a8d227c7453",
            "title": "Cost-based optimization for magic: algebra and implementation",
            "abstract": "Magic sets rewriting is a well-known optimization heuristic for complex decision-support queries. There can be many variants of this rewriting even for a single query, which differ greatly in execution performance. We propose cost-based techniques for selecting an efficient variant from the many choices.Our first contribution is a practical scheme that models magic sets rewriting as a special join method that can be added to any cost-based query optimizer. We derive cost formulas that allow an optimizer to choose the best variant of the rewriting and to decide whether it is beneficial. The order of complexity of the optimization process is preserved by limiting the search space in a reasonable manner. We have implemented this technique in IBM's DB2 C/S V2 database system. Our performance measurements demonstrate that the cost-based magic optimization technique performs well, and that without it, several poor decisions could be made.Our second contribution is a formal algebraic model of magic sets rewriting, based on an extension of the multiset relational algebra, which cleanly defines the search space and can be used in a rule-based optimizer. We introduce the multiset &theta;-semijoin operator, and derive equivalence rules involving this operator. We demonstrate that magic sets rewriting for non-recursive SQL queries can be modeled as a sequential composition of these equivalence rules.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1745794",
                    "name": "P. Seshadri"
                },
                {
                    "authorId": "1695576",
                    "name": "J. Hellerstein"
                },
                {
                    "authorId": "2886859",
                    "name": "H. Pirahesh"
                },
                {
                    "authorId": "152456070",
                    "name": "T. Y. Leung"
                },
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1682747",
                    "name": "Peter James Stuckey"
                },
                {
                    "authorId": "143761034",
                    "name": "S. Sudarshan"
                }
            ]
        },
        {
            "paperId": "d364d5317491d075fecc177e0b7063b7481504ae",
            "title": "A visual language for interactive data exploration and analysis",
            "abstract": "The analysis of large amounts of data to extract generalizations, exceptions, trends, and hidden relationships is a common activity in the business and scientific communities. While some kinds of \"knowledge\" can be extracted automatically with preselected algorithms or data mining techniques, others require an experienced human, often an expert in analysis, the business or scientific context, or both. We have found that such humans combine exploration, the search for a relevant subset or view of the data, with analysis, statistical or other techniques for measurement. We designed and implemented a visual language, IDEA, to assist the data analyst in these two intertwined tasks. The language is a convenient representation for data analysis and provides environmental support for keeping track of sequences of operations, reuse of the data analysis itself, and enforced semantics between operations and data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1690361",
                    "name": "P. Selfridge"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "a8450a2666891b0fa6be16d0fa320db25abe9bbb",
            "title": "The Information Manifold",
            "abstract": "We describe the Information Manifold (IM), a system for browsing and querying of multiple networked information sources. As a first contribution, the system demonstrates the viability of knowledge representation technology for retrieval and organization of information from disparate (structured and unstructured) information sources. Such an organization allows the user to pose high-level queries that use data from multiple information sources. As a second contribution, we describe novel query processing algorithms used to combine information from multiple sources. In particular, our algorithms are guaranteed to find exactly the set of information sources relevant to a query, and to completely exploit knowledge about local closed world information (Etzioni et al. 1994).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2076231991",
                    "name": "T. Kirk"
                },
                {
                    "authorId": "50874323",
                    "name": "Alon Y. Levy"
                },
                {
                    "authorId": "1714472",
                    "name": "Y. Sagiv"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "d8ee01208f909a6c53809c712aa0537472157db1",
            "title": "Space optimization in deductive databases",
            "abstract": "In the bottom-up evaluation of logic programs and recursively defined views on databases, all generated facts are usually assumed to be stored until the end of the evaluation. Discarding facts during the evaluation, however, can considerably improve the efficiency of the evaluation: the space needed to evaluate the program, the I/O costs, the costs of maintaining and accessing indices, and the cost of eliminating duplicates may all be reduced. Given an evaluation method that is sound, complete, and does not repeat derivation steps, we consider how facts can be discarded during the evaluation without compromising these properties. We show that every such space optimization method has certain components, the first to ensure soundness and completeness, the second to avoid redundancy (i.e., repetition of derivations), and the third to reduce \u201cfact lifetimes\u201d (i.e., the time period for which each fact must be retained during evaluation). We present new techniques based on providing bounds on the number of derivations and uses of facts, and using monotonicity constraints for each of the first two components, and provide novel synchronization techniques for the third component of a space optimization method. We describe how techniques for each of the three components can be combined in practice to obtain a space optimization method for a program. Our results are also of importance in applications such as sequence querying, and in active databases where triggers are defined over multiple \u201cevents.\u201d",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2066549925",
                    "name": "S. Sudarshan"
                },
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "5151034",
                    "name": "J. Naughton"
                }
            ]
        },
        {
            "paperId": "7e7edaf64f588c36f972baee4bf401731bef2344",
            "title": "Challenges for Global Information Systems",
            "abstract": "Currently, the Internet provides access to a very large number and wide variety of information sources (e.g., textual databases, sites containing technical reports, directory listings), and systems to access these sources (e.g., World Wide Web, Gopher, WAIS). The challenge is to provide easy, efficient, robust and secure access to this information and other kinds (e.g., relational and object oriented databases). This aim of this panel is to explore whether there are any new technical problems, relevant to the Database field, that need to be solved in order to realize such global information systems. In particular, we debate whether existing techniques from database systems (e.g., multidatabases and distributed databases) can be applied or straigtitforwardly extended to global information systems. Furthermore, we attempt to establish realistic goals for database technologies in global information systems. Some of the specific issues discussed are the following:",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1770962",
                    "name": "A. Halevy"
                },
                {
                    "authorId": "1688159",
                    "name": "A. Silberschatz"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2345443",
                    "name": "M. Zemankova"
                }
            ]
        },
        {
            "paperId": "d99f472c08a14ab135953a87312d36a9fdac7153",
            "title": "Towards Efficient Information Gathering Agents",
            "abstract": "Information gathering agents are required in many software agent applications to answer queries, posed by other agents, using a variety of available information sources. We formally consider the problem of designing information gathering agents, and make two important contributions. First, we examine the key issue of integrating knowledge from external sites into our knowledge base, and present an expressive language for this purpose. A noteworthy feature of our language is its ability to capture the knowledge that some external sites have complete information of a certain kind, using rich semantic constraints. Given a query on the knowledge base, it is important for the agent to first determine the set of external sites that contain information relevant to answering the query, and then access those sites. Our second contribution is to show that, given a query and the descriptions of the external sites in our language, it is possible to determine minimal subsets of sites that are needed to answer the query.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "50874323",
                    "name": "Alon Y. Levy"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "22d1d3c28ce75e50f18179c388e17044563d0ca4",
            "title": "Coral++: Adding Object-Orientation to a Logic Database Language",
            "abstract": "Coral++ is a database programming language that integrates Coral [23] with the C++ type system. The data model allows arbitrary C++ objects in database facts, and the declarative query language extends Coral with C++ expressions in rules. Coral++ also supports an imperative rule-based sub-language that is integrated with C++, providing support for updates. The design and implementation of Coral++ incorporates several important decisions: the data model is based on C++, and class definitions and method invocations are handled entirely by the C++ compiler; the notion of classes is kept orthogonal to the re*The work of Haghu Hamakrishnan, Divesh Srivastava and Praveen Seshadri was supported by a David and Lutile Packard Foundation Fellowship in Science and Engineering, a Presidential Young Investigator Award with matching grants from DEC, Tandem and Xerox, and NSF grant IHI-9011563. The addresses of the authors are Computer Sciences Department, University of Wisconsin, Madison, WI 53706, USA, and ATT and declarative Coral++ programs can be largely understood in terms of standard Horn clause logic with C++ method invocations treated as external functions. The implementation outline illustrates that extending an existing deductive system to incorporate object-oriented features in the data model is feasible, and is orthogonal to the techniques used for object storage and retrieval.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "1745794",
                    "name": "P. Seshadri"
                },
                {
                    "authorId": "143761034",
                    "name": "S. Sudarshan"
                }
            ]
        },
        {
            "paperId": "4c96cb6db3c0fa664ea8ac9d70a1d097b021cb56",
            "title": "Implementation of the CORAL deductive database system",
            "abstract": "CORAL is a deductive database system that supports a rich declarative language, provides a wide range of evaluation methods, and allows a combination of declarative and imperative programming. The data can be persistent on disk or can reside in main-memory. We describe the architecture and implementation of CORAL. There were two important goals in the design of the CORAL architecture: (1) to integrate the different evaluation strategies in a reasonable fashion, and (2) to allow users to influence the optimization techniques used so as to exploit the full power of the CORAL implementation. A CORAL declarative program can be organized as a collection of interacting modules and this modular structure is the key to satisfying both these goals. The high level module interface allows modules with different evaluation techniques to interact in a transparent fashion. Further, users can optionally tailor the execution of a program by selecting from among a wide range of control choices at the level of each module. CORAL also has an interface with C++, and users can program in a combination of declarative CORAL, and C++ extended with CORAL primitives. A high degree of extensibility is provided by allowing C++ programmers to use the class structure of C++ to enhance the CORAL implementation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "143761034",
                    "name": "S. Sudarshan"
                },
                {
                    "authorId": "1745794",
                    "name": "P. Seshadri"
                }
            ]
        },
        {
            "paperId": "8e97fe4378779b0d70735048de768e74fa19871e",
            "title": "Extending the Well-Founded and Valid Semantics for Aggregation",
            "abstract": "We present a very general technique for deening semantics for programs that use aggregation. We use the technique to extend the well-founded semantics and the valid semantics, both of which were designed to provide semantics for programs with negation, to handle programs that contain possibly recursive use of aggregation. The generalization is based on a simple but powerful idea of aggregation on three-valued multisets. The use of three-valued multisets makes our extended well-founded semantics, which we call aggregate-well-founded semantics, easier to understand and more intuitive, in our opinion, than the extension of well-founded models by Van Gelder 14].",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143761034",
                    "name": "S. Sudarshan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                }
            ]
        },
        {
            "paperId": "bdba8ce1250bbbed25bc5d00cbe7377737ef1303",
            "title": "The CORAL deductive database system",
            "abstract": "CORAL [4, 5] is a deductive database system that supports a powerful declarative query language. The language supports general Horn clause logic programs, extended with SQL-style groupiug, set-generation, aud negation. Programs can be organized into independently optimiied modules, and users can provide optimization hints in the form of high-level annotations. The system supports a wide variety of optimizw tion techniques. There is art interface to C++ that enables programs to be written in a combination of imperative and declarative styles; C++ code can be called from declarative programs, and vice versa. A notable feature of the CORAL system is that it is extensible. In particular, new data types can be defined, and new relation and index implementations can be added. An interface to the EXODUS storage manager [2] provides support for disk-resident data, transactions and crash-recovery.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "2058327078",
                    "name": "William G. Roth"
                },
                {
                    "authorId": "1745794",
                    "name": "P. Seshadri"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "143761034",
                    "name": "S. Sudarshan"
                }
            ]
        },
        {
            "paperId": "0633007655271a805e9ea6a5933a997e201fafa9",
            "title": "Pushing constraint selections",
            "abstract": "Abstract Bottom-up evaluation of a program-query pair in a constraint query language often computes only ground facts. Constraints do not contribute to answers, but are used only to prune derivations. The Magic Templates evaluation cannot utilize all the constraint information present in such program-query pairs while computing only ground facts. In general, constraint facts are computed, making the resulting evaluation more expensive. We describe an optimization that propagates constraints occuring in the program and the query, such that the rewritten program fully utilizes the constraint information present in the original program. Only constraint-relevant facts are computed, and if the evaluation of the original program computed only ground facts, so does the evaluation of the rewritten program. Our procedure can be combined with the Magic Templates transformation to propagate query binding information in addition to the constraint information. We show that it is always better to defer the application of Magic Templates.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                }
            ]
        },
        {
            "paperId": "3ef900850d3072c20b206682260ac4ab05979a03",
            "title": "The valid model semantics for logic programs",
            "abstract": "We present the valid model semantics, a new approach to providing semantics for logic programs with negation, set-terms and grouping. The valid model semantics is a three-valued semantics, and is defined in terms of a \u2018normal form\u2019 computation. The valid model semantics also gives meaning to the generation and use of non-ground facts (i.e., facts with variables) in a computation. The formulation of the semantics in terms of a normal form computation offers important insight not only into the valid model semantics, but also into other semantics proposed earlier. We show that the valid model semantics extends the well-founded semantics in a natural manner, and has several advantages over it. The well-founded semantics can also be undertood using a variant of the normal form computations that we use; the normal form computations used for valid semantics seem more natural than those used for well-founded semantics. We also show that the valid model semantics has several other desirable properties: it is founded ([SZ90]), it is contained in every regular model ([YY90]), and it is contained in every two-valued stable model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1722047",
                    "name": "C. Beeri"
                },
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "143761034",
                    "name": "S. Sudarshan"
                }
            ]
        },
        {
            "paperId": "4314b035018da02f810f0f2746d0c130ff6fef7e",
            "title": "Controlling the Search in Bottom-Up Evaluation",
            "abstract": "Bottom-up evaluation of queries on deductive databases has many advantages over an evaluation scheme such as Prolog. It is sound and complete with respect to the declarative semantics of least Herbrand models for positive Horn clause programs. In particular, it is able to avoid innnite loops by detecting repeated (possibly cyclic) subgoals. Further, in many database applications , it is more eecient than Prolog due to its set-orientedness. However , the completely set-oriented, breadth-rst search strategy of bottom-up evaluation has certain disadvantages. For example, to evaluate several classes of programs with negation (or aggregation), it is necessary to order the inferences; in essence, we must evaluate all answers to a negative sub-goal before making an inference that depends upon the negative subgoal. A completely breadth-rst search strategy ((14]) would have to maintain a lot of redundant subgoal dependency information to achieve this. We present a technique to order the use of generated subgoals, that is a hybrid between pure breadth-rst and pure depth-rst search. The technique , called Ordered Search, is able to maintain subgoal dependency information eeciently, while being able to detect repeated subgoals, and avoid innnite loops. Also, the technique avoids repeated computation and is complete for DATALOG. We demonstrate the power of Ordered Search through two applications. First, we show that it can be used to evaluate programs with left-to-right modularly stratiied negation and aggregation more ee-ciently than with any previously known bottom-up technique. Second, we illustrate its use for optimizing single-answer queries for linear programs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "143761034",
                    "name": "S. Sudarshan"
                }
            ]
        },
        {
            "paperId": "4d9659afad4f50591cb650fdd945f6a537c90321",
            "title": "CORAL - Control, Relations and Logic",
            "abstract": "CORAL is a modular declarative query language/programming language that supports general Horn clauses with complex terms, set-grouping, aggregation, negation, and relations with tuples that contain (universally quantified) variables. Support for persistent relations is provided by using the EXODUS storage manager. A unique feature of CORAL is that it provides a wide range of evaluation strategies and allows users to optionally tailor execution of a program through high-level annotations. A CORAL program is organized as a collection of modules, and this structure is used as the basis for expressing control choices. CORAL has an interface to C++, and uses the class structure of C++ to provide extensibility. FinaUy, CORAL supports a command sublanguage, in which statements are evaluated in a user-specified order. The statements can be queries, updates, production-system style rules, or any command that can be typed in at the CORAL",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "143761034",
                    "name": "S. Sudarshan"
                }
            ]
        },
        {
            "paperId": "9b84daaed8734fdda0c539e1c62e809a84899330",
            "title": "Query Restricted Bottom-Up Evaluation of Normal Logic Programs",
            "abstract": "Several program transformations|magic sets, envelopes, NRSU transformations and context transformations, among others|have been proposed for eeciently computing the answers to a query while taking advantage of the query constants. These transformations use sideways information passing strategies (sips) to restrict bottom-up evaluation to facts potentially relevant to the query. It is of interest to extend these transformations to all logic programs with negation, and identify classes of programs and sips for which these transformations preserve well-founded models with respect to the query. In a previous paper we identiied classes of programs and sips for which the magic sets transformation preserves well-founded models wrt the query. We continue this line of research to other transformations that use sips. We identify classes of programs and sips for which the context transformations and the envelopes transformations preserve well-founded models wrt the query. We also deene a new program transformation based on magic sets that preserves well-founded models with respect to the query for any choice of sips. Finally, we compare and contrast the performance of envelopes with our new program transformation using the Aditi deductive database system.",
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054999164",
                    "name": "D. Kemp"
                },
                {
                    "authorId": "1682747",
                    "name": "Peter James Stuckey"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        },
        {
            "paperId": "913681d40dcee03c3eba858770282b83f83b8cac",
            "title": "Magic Sets and Bottom-Up Evaluation of Well-Founded Models",
            "abstract": "We present a bottom-up operational procedure for computing well-founded models of allowed DATALOG programs with negation. This procedure provides a practical method of handling programs that involve unstratiied negation in a manner that may be mixed with other evaluation approaches, such as semi-naive evaluation. We also deene classes of programs and sips for which the magic sets transformation preserves well-founded models with respect to the query. The class of programs and sips we consider strictly subsume those already considered in the literature, and include stratiied programs (with any choice of sips), modularly stratiied programs (with left-to-right sips) and programs with three-valued well-founded models (with well-founded sips). For these programs and sips, our procedure for computing well-founded models is applicable to the magic programs, thus allowing increased eeciency by specializing a program for a query.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2054999164",
                    "name": "D. Kemp"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1682747",
                    "name": "Peter James Stuckey"
                }
            ]
        },
        {
            "paperId": "938f05788c40cecd6151cd90d60acf977a37ca99",
            "title": "Space optimization in the bottom-up evaluation of logic programs",
            "abstract": "In the bottom-up evaluation of a logic program, all generated facts are usually assumed to be stored until the end of the evaluation. Considerable gains can be achieved by instead discarding facts that are no longer required: the space needed to evaluate the program is reduced, 1/0 costs may be reduced, and the costs of maintaining and accessing indices, eliminating duplicates etc. are reduced. Thus, discarding facts early could achieve time as well as space improvements. Given an evaluation method that is sound, complete and does not repeat derivation steps, we consider how facts can be discarded during the evaluation without compromising these properties. Our first contribution is to show that such a space optimization technique has three distinct components. Informally, we must make all derivations that we can with each fact, detect all duplicate derivations of facts and try to order the computation so as to minimize the \u201clifespan\u201d of each fact. This separation enables us to use different methods for each of the components for different parts of the program. We present several methods for ensuring each of these corrponents. We also briefly describe how to obtain a complete space optimization technique by making a choice of techniques for each component and combining them. Our results apply to a significantly larger class of programs than those considered in [NR90]. *The work of the first three authors was supported in part by a David and Lucile Packard Foundation Fellowship in Science and Engineering, an IBM Faculty Development Award and NSF grant IRI-S804319. t The work of this author supported by NSF grant IRI-8909795. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. e 1991 ACM 0-89791 -425 -2/91 /0005 /0068 . ..$1 .50",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2066549925",
                    "name": "S. Sudarshan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "5151034",
                    "name": "J. Naughton"
                }
            ]
        },
        {
            "paperId": "45e1eaadb56d42c18d9969438c9f1b4c7a2ca199",
            "title": "Rule Ordering in Bottom-Up Fixpoint Evaluation of Logic Programs",
            "abstract": "Logic programs can be evaluated bottom-up by repeatedly applying all rules, in \"iterations\", until the fixpoint is reached. However, it is often desirable-and, in some cases, e.g. programs with stratified negation, it is even necessary to guarantee the semantics-to apply the rules in some order. We present two algorithms that apply rules in a specified order without repeating inferences. One of them (GSN) is capable of dealing with a wide range of rule orderings, but with a little more overhead than the well-known seminaive algorithm (which we call BSN). The other (PSN) handles a smaller class of rule orderings, but with no overheads beyond those in BSN. We also demonstrate that by choosing a good ordering, we can reduce the number of rule applications (and thus the number of joins). We present a theoretical analysis of rule orderings and identify orderings that minimize the number of rule applications (for all possible instances of the base relations) with respect to a class of orderings called fair orderings. We also show that though nonfair orderings may do a little better on some data sets, they can do much worse on others. The analysis is supplemented by performance results. >",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1709145",
                    "name": "R. Ramakrishnan"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                },
                {
                    "authorId": "2066549925",
                    "name": "S. Sudarshan"
                }
            ]
        },
        {
            "paperId": "d11dbc952b71f846fa6502e3de83dca40b5217fc",
            "title": "Explanation of the Tables",
            "abstract": "We present a robust solution to the following problem: given a table with multiple categorical dimension attributes and one binary outcome attribute, construct a summary that offers an interpretable explanation of the factors affecting the outcome attribute in terms of the dimension attribute value combinations. We refer to such a summary as an explanation table, which is a disjunction of overlapping patterns over the dimension attributes, where each pattern specifies a conjunction of attribute=value conditions. The Flashlight algorithm that we describe is based on sampling and includes optimizations related to computing the information content of a summary from a sample of the data. Using real data sets, we demonstrate the advantages of explanation tables compared to related approaches that can be adapted to solve our problem, and we show significant performance benefits of our approach.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2531258",
                    "name": "Kareem El Gebaly"
                },
                {
                    "authorId": "3270313",
                    "name": "Guoyao Feng"
                },
                {
                    "authorId": "2285679174",
                    "name": "Lukasz Golab"
                },
                {
                    "authorId": "144430493",
                    "name": "Flip Korn"
                },
                {
                    "authorId": "145860176",
                    "name": "D. Srivastava"
                }
            ]
        }
    ]
}