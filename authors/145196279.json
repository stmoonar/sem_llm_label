{
    "authorId": "145196279",
    "papers": [
        {
            "paperId": "001c35feef59c01394707b29fd95e4c7374b284d",
            "title": "Recommendation Unlearning via Influence Function",
            "abstract": "Recommendation unlearning is an emerging task to serve users for erasing unusable data (e.g., some historical behaviors) from a well-trained recommender model. Existing methods process unlearning requests by fully or partially retraining the model after removing the unusable data. However, these methods are impractical due to the high computation cost of full retraining and the highly possible performance damage of partial training. In this light, a desired recommendation unlearning method should obtain a similar model as full retraining in a more efficient manner, i.e., achieving complete, efficient and harmless unlearning. In this work, we propose a new Influence Function-based Recommendation Unlearning (IFRU) framework, which efficiently updates the model without retraining by estimating the influence of the unusable data on the model via the influence function. In the light that recent recommender models use historical data for both the constructions of the optimization loss and the computational graph (e.g., neighborhood aggregation), IFRU jointly estimates the direct influence of unusable data on optimization loss and the spillover influence on the computational graph to pursue complete unlearning. Furthermore, we propose an importance-based pruning algorithm to reduce the cost of the influence function. IFRU is harmless and applicable to mainstream differentiable models. Extensive experiments demonstrate that IFRU achieves more than 250 times acceleration compared to retraining-based methods with recommendation performance comparable to full retraining. Codes are avaiable at https://github.com/baiyimeng/IFRU.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2145957648",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2111297664",
                    "name": "ZhiYu Hu"
                },
                {
                    "authorId": "1456009564",
                    "name": "Yimeng Bai"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "1491035012",
                    "name": "Jiancan Wu"
                },
                {
                    "authorId": "145196279",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "2736abd76c8fd66614ed5d64cab2e6ae04871965",
            "title": "Ameli: Enhancing Multimodal Entity Linking with Fine-Grained Attributes",
            "abstract": "We propose attribute-aware multimodal entity linking, where the input consists of a mention described with a text paragraph and images, and the goal is to predict the corresponding target entity from a multimodal knowledge base (KB) where each entity is also accompanied by a text description, visual images, and a collection of attributes that present the meta-information of the entity in a structured format. To facilitate this research endeavor, we construct Ameli, encompassing a new multimodal entity linking benchmark dataset that contains 16,735 mentions described in text and associated with 30,472 images, and a multimodal knowledge base that covers 34,690 entities along with 177,873 entity images and 798,216 attributes. To establish baseline performance on Ameli, we experiment with several state-of-the-art architectures for multimodal entity linking and further propose a new approach that incorporates attributes of entities into disambiguation. Experimental results and extensive qualitative analysis demonstrate that extracting and understanding the attributes of mentions from their text descriptions and visual images play a vital role in multimodal entity linking. To the best of our knowledge, we are the first to integrate attributes in the multimodal entity linking task. The programs, model checkpoints, and the dataset are publicly available at https://github.com/VT-NLP/Ameli.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2166311346",
                    "name": "Barry Menglong Yao"
                },
                {
                    "authorId": "2181535101",
                    "name": "Yu Chen"
                },
                {
                    "authorId": "145196279",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2116423012",
                    "name": "Sijia Wang"
                },
                {
                    "authorId": "2123130842",
                    "name": "Minqian Liu"
                },
                {
                    "authorId": "2136442661",
                    "name": "Zhiyang Xu"
                },
                {
                    "authorId": "2112477373",
                    "name": "Licheng Yu"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                }
            ]
        },
        {
            "paperId": "46ac5734e8e5122d3c34df1b37cd68481bd4b9f2",
            "title": "Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality",
            "abstract": "Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning, leading to state-of-the-art models for various downstream multimodal tasks. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graph-structured semantic representations of images that contain objects, their attributes, and relations with other objects in a scene. In this work, we consider the scene graph parsed from text as a proxy for the image scene graph and propose a graph decomposition and augmentation framework along with a coarse-to-fine contrastive learning objective between images and text that aligns sentences of various complexities to the same image. Along with this, we propose novel negative mining techniques in the scene graph space for improving attribute binding and relation understanding. Through extensive experiments, we demonstrate the effectiveness of our approach that significantly improves attribute binding, relation understanding, systematic generalization, and productivity on multiple recently proposed benchmarks (For example, improvements upto $18\\%$ for systematic generalization, $16.5\\%$ for relation understanding over a strong baseline), while achieving similar or better performance than CLIP on various general multimodal tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2119151340",
                    "name": "Harman Singh"
                },
                {
                    "authorId": "9325940",
                    "name": "Pengchuan Zhang"
                },
                {
                    "authorId": "145196279",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "3308665",
                    "name": "Mengjiao MJ Wang"
                },
                {
                    "authorId": "22253126",
                    "name": "Wenhan Xiong"
                },
                {
                    "authorId": "3048577",
                    "name": "Jingfei Du"
                },
                {
                    "authorId": "2181535101",
                    "name": "Yu Chen"
                }
            ]
        },
        {
            "paperId": "656216cf5b5c206486a36dd875bf2a5abee7019c",
            "title": "Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning",
            "abstract": "Recently, self-supervised learning (SSL) was shown to be vulnerable to patch-based data poisoning backdoor attacks. It was shown that an adversary can poison a small part of the unlabeled data so that when a victim trains an SSL model on it, the final model will have a back-door that the adversary can exploit. This work aims to defend self-supervised learning against such attacks. We use a three-step defense pipeline, where we first train a model on the poisoned data. In the second step, our proposed defense algorithm (PatchSearch) uses the trained model to search the training data for poisoned samples and removes them from the training set. In the third step, a final model is trained on the cleaned-up training set. Our results show that PatchSearch is an effective defense. As an example, it improves a model's accuracy on images containing the trigger from 38.2% to 63.7% which is very close to the clean model's accuracy, 64.6%. More-over, we show that PatchSearch outperforms baselines and state-of-the-art defense approaches including those using additional clean, trusted data. Our code is available at https://github.com/UCDvision/PatchSearch",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1471776526",
                    "name": "Ajinkya Tejankar"
                },
                {
                    "authorId": "2095979",
                    "name": "Maziar Sanjabi"
                },
                {
                    "authorId": "145196279",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2116420716",
                    "name": "Sinong Wang"
                },
                {
                    "authorId": "22593971",
                    "name": "Hamed Firooz"
                },
                {
                    "authorId": "2367683",
                    "name": "H. Pirsiavash"
                },
                {
                    "authorId": "48327785",
                    "name": "L Tan"
                }
            ]
        },
        {
            "paperId": "b56916d0ab48b8a0cd58218641463f10c58ff65a",
            "title": "DualGNN: Dual Graph Neural Network for Multimedia Recommendation",
            "abstract": "One of the important factors affecting micro-video recommender systems is to model the multi-modal user preference on the micro-video. Despite the remarkable performance of prior arts, they are still limited by fusing the user preference derived from different modalities in a unified manner, ignoring the users tend to place different emphasis on different modalities. Furthermore, modality-missing is ubiquity and unavoidable in the micro-video recommendation, some modalities information of micro-videos are lacked in many cases, which negatively affects the multi-modal fusion operations. To overcome these disadvantages, we propose a novel framework for the micro-video recommendation, dubbed Dual Graph Neural Network (DualGNN), upon the user-microvideo bipartite and user co-occurrence graphs, which leverages the correlation between users to collaboratively mine the particular fusion pattern for each user. Specifically, we first introduce a single-modal representation learning module, which performs graph operations on the user-microvideo graph in each modality to capture single-modal user preferences on different modalities. And then, we devise a multi-modal representation learning module to explicitly model the user\u2019s attentions over different modalities and inductively learn the multi-modal user preference. Finally, we propose a prediction module to rank the potential micro-videos for users. Extensive experiments on two public datasets demonstrate the significant superiority of our DualGNN over state-of-the-arts methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145196279",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "1887997",
                    "name": "Yin-wei Wei"
                },
                {
                    "authorId": "2051685317",
                    "name": "Jianhua Yin"
                },
                {
                    "authorId": "2720873",
                    "name": "Jianlong Wu"
                },
                {
                    "authorId": "33977299",
                    "name": "Xuemeng Song"
                },
                {
                    "authorId": "143982887",
                    "name": "Liqiang Nie"
                },
                {
                    "authorId": "2156053235",
                    "name": "Min Zhang"
                }
            ]
        },
        {
            "paperId": "f328fb7431fdb37522b27cffee069bc1c7533dcc",
            "title": "A Causal View for Item-level Effect of Recommendation on User Preference",
            "abstract": "Recommender systems not only serve users but also affect user preferences through personalized recommendations. Recent researches investigate the effects of the entire recommender system on user preferences, i.e., system-level effects, and find that recommendations may lead to problems such as echo chambers and filter bubbles. To properly alleviate the problems, it is necessary to estimate the effects of recommending a specific item on user preferences, i.e., item-level effects. For example, by understanding whether recommending an item aggravates echo chambers, we can better decide whether to recommend it or not. This work designs a method to estimate the item-level effects from the causal perspective. We resort to causal graphs to characterize the average treatment effect of recommending an item on the preference of another item. The key to estimating the effects lies in mitigating the confounding bias of time and user features without the costly randomized control trials. Towards the goal, we estimate the causal effects from historical observations through a method with stratification and matching to address the two confounders, respectively. Nevertheless, directly implementing stratification and matching is intractable, which requires high computational cost due to the large sample size. We thus propose efficient approximations of stratification and matching to reduce the computation complexity. Extensive experimental results on two real-world datasets validate the effectiveness and efficiency of our method. We also show a simple example of using the item-level effects to provide insights for mitigating echo chambers.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113721360",
                    "name": "Wei Cai"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "145196279",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2209949178",
                    "name": "Tian Yang"
                },
                {
                    "authorId": "2145312301",
                    "name": "Zhenguang Liu"
                },
                {
                    "authorId": "1682914",
                    "name": "Congfu Xu"
                }
            ]
        },
        {
            "paperId": "0c5aaa417483181efd30e0207d7b5b545f322bbc",
            "title": "Deep Partial Multiplex Network Embedding",
            "abstract": "Network embedding is an effective technique to learn the low-dimensional representations of nodes in networks. Real-world networks are usually with multiplex or having multi-view representations from different relations. Recently, there has been increasing interest in network embedding on multiplex data. However, most existing multiplex approaches assume that the data is complete in all views. But in real applications, it is often the case that each view suffers from the missing of some data and therefore results in partial multiplex data. In this paper, we present a novel Deep Partial Multiplex Network Embedding approach to deal with incomplete data. In particular, the network embeddings are learned by simultaneously minimizing the deep reconstruction loss with the autoencoder neural network, enforcing the data consistency across views via common latent subspace learning, and preserving the data topological structure within the same network through graph Laplacian. We further prove the orthogonal invariant property of the learned embeddings and connect our approach with the binary embedding techniques. Experiments on four multiplex benchmarks demonstrate the superior performance of the proposed approach over several state-of-the-art methods on node classification, link prediction and clustering tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145196279",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "144145642",
                    "name": "Yi Fang"
                },
                {
                    "authorId": "101210026",
                    "name": "Anirudh Ravula"
                },
                {
                    "authorId": "2933399",
                    "name": "Ruining He"
                },
                {
                    "authorId": "48113851",
                    "name": "Bin Shen"
                },
                {
                    "authorId": "2109593402",
                    "name": "Jingang Wang"
                },
                {
                    "authorId": "38472218",
                    "name": "Xiaojun Quan"
                },
                {
                    "authorId": "1995922397",
                    "name": "Dongfang Liu"
                }
            ]
        },
        {
            "paperId": "28c7a45ae36d8bd501b7fe79d5b55473001a48e7",
            "title": "Improved Adaptive Algorithm for Scalable Active Learning with Weak Labeler",
            "abstract": "Active learning with strong and weak labelers considers a practical setting where we have access to both costly but accurate strong labelers and inaccurate but cheap predictions provided by weak labelers. We study this problem in the streaming setting, where decisions must be taken online . We design a novel algorithmic template, Weak Labeler Active Cover (WL-AC), that is able to robustly leverage the lower quality weak labelers to reduce the query complexity while retaining the desired level of accuracy. Prior active learning algorithms with access to weak labelers learn a difference classi\ufb01er which predicts where the weak labels differ from strong labelers; this requires the strong assumption of realizability of the difference classi\ufb01er (Zhang and Chaudhuri, 2015). WL-AC by-passes this realizability assumption and thus is applicable to many real-world scenarios such as random corrupted weak labels and high dimen-sional family of difference classi\ufb01ers ( e.g., deep neural nets). Moreover, WL-AC cleverly trades off evaluating the quality with full exploitation of weak labelers, which allows to convert any active learning strategy to one that can leverage weak labelers. We provide an instantiation of this template that achieves the optimal query complexity for any given weak labeler, without knowing its accuracy a-priori. Empirically, we propose an instantiation of the WL-AC template that can be ef\ufb01ciently implemented for large-scale models",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1556022677",
                    "name": "Yifang Chen"
                },
                {
                    "authorId": "2178963",
                    "name": "Karthik Abinav Sankararaman"
                },
                {
                    "authorId": "3254390",
                    "name": "A. Lazaric"
                },
                {
                    "authorId": "6234609",
                    "name": "Matteo Pirotta"
                },
                {
                    "authorId": "2840779",
                    "name": "Dmytro Karamshuk"
                },
                {
                    "authorId": "145196279",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "1396416724",
                    "name": "Karishma Mandyam"
                },
                {
                    "authorId": "2116420716",
                    "name": "Sinong Wang"
                },
                {
                    "authorId": "2087117615",
                    "name": "Han Fang"
                }
            ]
        },
        {
            "paperId": "44cb535698c55468ceffd7ff353a1fa200799038",
            "title": "Rethinking Missing Data: Aleatoric Uncertainty-Aware Recommendation",
            "abstract": "Historical interactions are the default choice for recommender model training, which typically exhibit high sparsity, i.e., most user-item pairs are unobserved missing data. A standard choice is treating the missing data as negative training samples and estimating interaction likelihood between user-item pairs along with the observed interactions. In this way, some potential interactions are inevitably mislabeled during training, which will hurt the model fidelity, hindering the model to recall the mislabeled items, especially the long-tail ones. In this work, we investigate the mislabeling issue from a new perspective of aleatoric uncertainty, which describes the inherent randomness of missing data. The randomness pushes us to go beyond merely the interaction likelihood and embrace aleatoric uncertainty modeling. Towards this end, we propose a new Aleatoric Uncertainty-aware Recommendation (AUR) framework that consists of a new uncertainty estimator along with a normal recommender model. According to the theory of aleatoric uncertainty, we derive a new recommendation objective to learn the estimator. As the chance of mislabeling reflects the potential of a pair, AUR makes recommendations according to the uncertainty, which is demonstrated to improve the recommendation performance of less popular items without sacrificing the overall performance. We instantiate AUR on three representative recommender models: Matrix Factorization (MF), LightGCN, and VAE from mainstream model architectures. Extensive results on four real-world datasets validate the effectiveness of AUR w.r.t. better recommendation results, especially on long-tail items.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2164713058",
                    "name": "Chenxu Wang"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2145957648",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "145196279",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2109907241",
                    "name": "Xu Hu"
                },
                {
                    "authorId": "7792071",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "77365b30336ac46d620d958dc4c108a159c02834",
            "title": "WebFormer: The Web-page Transformer for Structure Information Extraction",
            "abstract": "Structure information extraction refers to the task of extracting structured text fields from web pages, such as extracting a product offer from a shopping page including product title, description, brand and price. It is an important research topic which has been widely studied in document understanding and web search. Recent natural language models with sequence modeling have demonstrated state-of-the-art performance on web information extraction. However, effectively serializing tokens from unstructured web pages is challenging in practice due to a variety of web layout patterns. Limited work has focused on modeling the web layout for extracting the text fields. In this paper, we introduce WebFormer, a Web-page transFormer model for structure information extraction from web documents. First, we design HTML tokens for each DOM node in the HTML by embedding representations from their neighboring tokens through graph attention. Second, we construct rich attention patterns between HTML tokens and text tokens, which leverages the web layout for effective attention weight computation. We conduct an extensive set of experiments on SWDE and Common Crawl benchmarks. Experimental results demonstrate the superior performance of the proposed approach over several state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145196279",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "144145642",
                    "name": "Yi Fang"
                },
                {
                    "authorId": "101210026",
                    "name": "Anirudh Ravula"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "38472218",
                    "name": "Xiaojun Quan"
                },
                {
                    "authorId": "1995922397",
                    "name": "Dongfang Liu"
                }
            ]
        }
    ]
}