{
    "authorId": "51456850",
    "papers": [
        {
            "paperId": "029fddaf2cfb690ad4392cba62a7bfba7c52a717",
            "title": "WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for Wikipedia Categories",
            "abstract": "Our research focuses on solving the zero-shot text classification problem in NLP, with a particular emphasis on innovative self-training strategies. To achieve this objective, we propose a novel self-training strategy that uses labels rather than text for training, significantly reducing the model's training time. Specifically, we use categories from Wikipedia as our training set and leverage the SBERT pre-trained model to establish positive correlations between pairs of categories within the same text, facilitating associative training. For new test datasets, we have improved the original self-training approach, eliminating the need for prior training and testing data from each target dataset. Instead, we adopt Wikipedia as a unified training dataset to better approximate the zero-shot scenario. This modification allows for rapid fine-tuning and inference across different datasets, greatly reducing the time required for self-training. Our experimental results demonstrate that this method can adapt the model to the target dataset within minutes. Compared to other BERT-based transformer models, our approach significantly reduces the amount of training data by training only on labels, not the actual text, and greatly improves training efficiency by utilizing a unified training set. Additionally, our method achieves state-of-the-art results on both the Yahoo Topic and AG News datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2191235625",
                    "name": "Te-Yu Chi"
                },
                {
                    "authorId": "3036900",
                    "name": "Y. Tang"
                },
                {
                    "authorId": "2225932142",
                    "name": "Chia-Wen Lu"
                },
                {
                    "authorId": "51456850",
                    "name": "Qiuyue Zhang"
                },
                {
                    "authorId": "144293175",
                    "name": "J. Jang"
                }
            ]
        },
        {
            "paperId": "5ec605ac28a0af0ca1c929aff40827a5c232ece5",
            "title": "A Dynamic Attributes-driven Graph Attention Network Modeling on Behavioral Finance for Stock Prediction",
            "abstract": "Stock prediction is a challenging task due to multiple influencing factors and complex market dependencies. Traditional solutions are based on a single type of information. With the success of multi-source information in different fields, the combination of different types of information such as numerical and textual information has become a promising option. Although multi-source information provides rich multi-view information, how to mine and construct structured relationships from them is a difficult problem. Specifically, most existing methods usually extract features from commonly used multi-source information as predictive information sources, without further pre-constructing stock relationship graphs with dependencies using broader information. More importantly, they typically treat each stock as an isolated forecasting, or employ stock market correlations based on a fixed predefined graph structure, but current methods are not sensitive enough to aggregate the attribute features extracted from multi-source information and stock relationship graph, to obtain the dynamic update of market relations and relationship strength. The stock market is highly temporally, and the attributes of nodes are affected by the time perception of other attributes, which is not fully considered. To address these problems, we propose a novel dynamic attributes-driven graph attention networks incorporating sentiment (DGATS) information, transaction data, and text data. Inspired by behavioral finance, we separately extract sentiment information as a factor of technical indicators, and further realize the early fusion of technical indicators and textual data through Kronecker product-based tensor fusion. In particular, by LSTM and temporal attention network, the short-term and long-term transition features are gradually grasped from the local composition of the fused stock trading sequence. Furthermore, real-time intra-market dependencies and key attributes information are captured with graph networks, enabling dynamic updates of relationships and relationship strengths in predefined graphs. Experiments on the real datasets show that the architecture can outperform the previous methods in prediction performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51456850",
                    "name": "Qiuyue Zhang"
                },
                {
                    "authorId": "2108127508",
                    "name": "Yunfeng Zhang"
                },
                {
                    "authorId": "3476711",
                    "name": "Xunxiang Yao"
                },
                {
                    "authorId": "50341250",
                    "name": "Shilong Li"
                },
                {
                    "authorId": "3321883",
                    "name": "Caiming Zhang"
                },
                {
                    "authorId": "1762182",
                    "name": "Peide Liu"
                }
            ]
        },
        {
            "paperId": "4fc6929590ccddf6d79cef4e52fb08e1c5e72586",
            "title": "Fair ranking: a critical review, challenges, and future directions",
            "abstract": "Ranking, recommendation, and retrieval systems are widely used in online platforms and other societal systems, including e-commerce, media-streaming, admissions, gig platforms, and hiring. In the recent past, a large \u201cfair ranking\u201d research literature has been developed around making these systems fair to the individuals, providers, or content that are being ranked. Most of this literature defines fairness for a single instance of retrieval, or as a simple additive notion for multiple instances of retrievals over time. This work provides a critical overview of this literature, detailing the often context-specific concerns that such approaches miss: the gap between high ranking placements and true provider utility, spillovers and compounding effects over time, induced strategic incentives, and the effect of statistical uncertainty. We then provide a path forward for a more holistic and impact-oriented fair ranking research agenda, including methodological lessons from other fields and the role of the broader stakeholder community in overcoming data bottlenecks and designing effective regulatory environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51964560",
                    "name": "Gourab K. Patro"
                },
                {
                    "authorId": "74248595",
                    "name": "Lorenzo Porcaro"
                },
                {
                    "authorId": "2152051189",
                    "name": "Laura Mitchell"
                },
                {
                    "authorId": "51456850",
                    "name": "Qiuyue Zhang"
                },
                {
                    "authorId": "41154657",
                    "name": "Meike Zehlike"
                },
                {
                    "authorId": "2779427",
                    "name": "Nikhil Garg"
                }
            ]
        },
        {
            "paperId": "a9a3ca9243c2236ffd2213e479265e54feb9e5a8",
            "title": "Incorporating Transformers and Attention Networks for Stock Movement Prediction",
            "abstract": "Predicting stock movements is a valuable research field that can help investors earn more profits. As with time-series data, the stock market is time-dependent and the value of historical information may decrease over time. Accurate prediction can be achieved by mining valuable information with words on social platforms and further integrating it with actual stock market conditions. However, many methods still cannot effectively dig deep into hidden information, integrate text and stock prices, and ignore the temporal dependence. Therefore, to solve the above problems, we propose a transformer-based attention network framework that uses historical text and stock prices to capture the temporal dependence of financial data. Among them, the transformer model and attention mechanism are used for feature extraction of financial data, which has fewer applications in the financial field, and effective analysis of key information to achieve an accurate prediction. A large number of experiments have proved the effectiveness of our proposed method. The actual simulation experiment verifies that our model has practical application value.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47003183",
                    "name": "Ya-wei Li"
                },
                {
                    "authorId": "72923190",
                    "name": "Shuqi Lv"
                },
                {
                    "authorId": "2146037596",
                    "name": "Xinghua Liu"
                },
                {
                    "authorId": "51456850",
                    "name": "Qiuyue Zhang"
                }
            ]
        },
        {
            "paperId": "a879939152e4b3dbff23cc575e3a2ae0bd609dc0",
            "title": "A Fusion Model-Based Label Embedding and Self-Interaction Attention for Text Classification",
            "abstract": "Text classification is a pivotal task in NLP (Natural Language Processing), which has received widespread attention recently. Most of the existing methods leverage the power of deep learning to improve the performance of models. However, these models ignore the interaction information between all the sentences in a text when generating the current text representation, which results in a partial semantics loss. Labels play a central role in text classification. And the attention learned from text-label in the joint space of labels and words is not leveraged, leaving enough room for further improvement. In this paper, we propose a text classification method based on Self-Interaction attention mechanism and label embedding. Firstly, our method introduce BERT (Bidirectional Encoder Representation from Transformers) to extract text features. Then Self-Interaction attention mechanism is employed to obtain text representations containing more comprehensive semantics. Moreover, we focus on the embedding of labels and words in the joint space to achieve the dual-label embedding, which further leverages the attention learned from text-label. Finally, the texts are classified by the classifier according to the weighted labels representations. The experimental results show that our method outperforms other state-of-the-art methods in terms of classification accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49266059",
                    "name": "Yanru Dong"
                },
                {
                    "authorId": "2137323",
                    "name": "Peiyu Liu"
                },
                {
                    "authorId": "144702363",
                    "name": "Zhenfang Zhu"
                },
                {
                    "authorId": "2109217284",
                    "name": "Qicai Wang"
                },
                {
                    "authorId": "51456850",
                    "name": "Qiuyue Zhang"
                }
            ]
        },
        {
            "paperId": "6362df4b17f3571eea35117f3c3a10f0a4528a28",
            "title": "An Improved Approach for Text Sentiment Classification Based on a Deep Neural Network via a Sentiment Attention Mechanism",
            "abstract": "Text sentiment analysis is an important but challenging task. Remarkable success has been achieved along with the wide application of deep learning methods, but deep learning methods dealing with text sentiment classification tasks cannot fully exploit sentiment linguistic knowledge, which hinders the development of text sentiment analysis. In this paper, we propose a sentiment-feature-enhanced deep neural network (SDNN) to address the problem by integrating sentiment linguistic knowledge into a deep neural network via a sentiment attention mechanism. Specifically, first we introduce a novel sentiment attention mechanism to help select the crucial sentiment-word-relevant context words by leveraging the sentiment lexicon in an attention mechanism, which bridges the gap between traditional sentiment linguistic knowledge and current popular deep learning methods. Second, we develop an improved deep neural network to extract sequential correlation information and text local features by combining bidirectional gated recurrent units with a convolutional neural network, which further enhances the ability of comprehensive text representation learning. With this design, the SDNN model can generate a powerful semantic representation of text to improve the performance of text sentiment classification tasks. Extensive experiments were conducted to evaluate the effectiveness of the proposed SDNN model on two real-world datasets with a binary-sentiment-label and a multi-sentiment-label. The experimental results demonstrated that the SDNN achieved substantially better performance than the strong competitors for text sentiment classification tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108801196",
                    "name": "Wenkuan Li"
                },
                {
                    "authorId": "2137323",
                    "name": "Peiyu Liu"
                },
                {
                    "authorId": "51456850",
                    "name": "Qiuyue Zhang"
                },
                {
                    "authorId": "2110172122",
                    "name": "Wenfeng Liu"
                }
            ]
        },
        {
            "paperId": "92e2271a146742d758ffa2fb2987df92a44c4b13",
            "title": "A Text Abstraction Summary Model Based on BERT Word Embedding and Reinforcement Learning",
            "abstract": "As a core task of natural language processing and information retrieval, automatic text summarization is widely applied in many fields. There are two existing methods for text summarization task at present: abstractive and extractive. On this basis we propose a novel hybrid model of extractive-abstractive to combine BERT (Bidirectional Encoder Representations from Transformers) word embedding with reinforcement learning. Firstly, we convert the human-written abstractive summaries to the ground truth labels. Secondly, we use BERT word embedding as text representation and pre-train two sub-models respectively. Finally, the extraction network and the abstraction network are bridged by reinforcement learning. To verify the performance of the model, we compare it with the current popular automatic text summary model on the CNN/Daily Mail dataset, and use the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics as the evaluation method. Extensive experimental results show that the accuracy of the model is improved obviously.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109217284",
                    "name": "Qicai Wang"
                },
                {
                    "authorId": "2137323",
                    "name": "Peiyu Liu"
                },
                {
                    "authorId": "144702363",
                    "name": "Zhenfang Zhu"
                },
                {
                    "authorId": "2113971558",
                    "name": "Hongxia Yin"
                },
                {
                    "authorId": "51456850",
                    "name": "Qiuyue Zhang"
                },
                {
                    "authorId": "2108911919",
                    "name": "Lindong Zhang"
                }
            ]
        },
        {
            "paperId": "9f54246708921866e1338280a1440ca8439caac1",
            "title": "A Multi-Attention Network for Aspect-Level Sentiment Analysis",
            "abstract": "Aspect-level sentiment analysis (ASA) aims at determining the sentiment polarity of specific aspect term with a given sentence. Recent advances in attention mechanisms suggest that attention models are useful in ASA tasks and can help identify focus words. Or combining attention mechanisms with neural networks are also common methods. However, according to the latest research, they often fail to extract text representations efficiently and to achieve interaction between aspect terms and contexts. In order to solve the complete task of ASA, this paper proposes a Multi-Attention Network (MAN) model which adopts several attention networks. This model not only preprocesses data by Bidirectional Encoder Representations from Transformers (BERT), but a number of measures have been taken. First, the MAN model utilizes the partial Transformer after transformation to obtain hidden sequence information. Second, because words in different location have different effects on aspect terms, we introduce location encoding to analyze the impact on distance from ASA tasks, then we obtain the influence of different words with aspect terms through the bidirectional attention network. From the experimental results of three datasets, we could find that the proposed model could achieve consistently superior results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51456850",
                    "name": "Qiuyue Zhang"
                },
                {
                    "authorId": "2055871026",
                    "name": "Ran Lu"
                }
            ]
        },
        {
            "paperId": "d135ef877a8be8b9ba317c9a699747954144a826",
            "title": "Interactive Multi-Head Attention Networks for Aspect-Level Sentiment Classification",
            "abstract": "Aspect-level sentiment classification (ASC) has received much attention these years. With the successful application of attention networks in many fields, attention-based ASC has aroused great interest. However, most of the previous methods did not analyze the contribution of words well and the context\u2013aspect term interaction was not well implemented, which largely limit the efficacy of models. In this paper, we exploit a novel method that is efficient and mainly adopts Multi-head Attention (MHA) networks. First, the word embedding and aspect term embedding are pre-trained by Bidirectional Encoder Representations from Transformers (BERT). Second, we make full use of MHA and convolutional operation to obtain hidden states, which is superior to traditional neural networks. Then, the interaction between context and aspect term is further implemented through averaging pooling and MHA. We conduct extensive experiments on three benchmark datasets and the final results show that the Interactive Multi-head Attention Networks (IMAN) model consistently outperforms the state-of-the-art methods on ASC task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51456850",
                    "name": "Qiuyue Zhang"
                },
                {
                    "authorId": "2055871026",
                    "name": "Ran Lu"
                },
                {
                    "authorId": "2109217284",
                    "name": "Qicai Wang"
                },
                {
                    "authorId": "144702363",
                    "name": "Zhenfang Zhu"
                },
                {
                    "authorId": "2137323",
                    "name": "Peiyu Liu"
                }
            ]
        },
        {
            "paperId": "192f4dfe8a4b13e77d514e45f94ad6610ffbdd31",
            "title": "Music syntactic processing",
            "abstract": "Music Syntax refers to the principle of combining discrete key elements into hierarchical system. Organization of pitches based on tonality harmony is the most important music syntax which affects the expectation, the construction and the feedback on the music events of the audience from which aesthetic musical experience is finally produced. Lerdahl and Jackendoff proposed a musical tree-structure that parallel to linguistic syntax in their generative theory of tonal music (GTTM). And Rohrmeier further specified hierarchical generativerules of pitch syntax in the generative syntax model (GSM). Both the GTTM and the GSM put forward the analogy between music syntax and linguistic syntax. But what is obviously different from language processing is that the processing of music syntax of the audience reveals a cross-empiricalness effects. Whether in early studies of behavioristics or in recent studies of EPR, most evidences support that there is a syntax diagram guiding people\u2019s musical expectations without much association with their musical experiences among western audience, at least in the sense of simple linear syntax. Therefore, the acquisition of music syntax under the non-explicit conditions arouses researchers\u2019 thinking on music syntax processing mechanism. Current studies have not reached a consensus on processing of syntactic structure, and two antagonistic theories \u201cCognitivism\u201d and \u201cPhysicalism\u201d have generated. \u201cCognitivism\u201d put forward that since pitch syntax reflects abstract cognitive structural relations between sounds and meanings, syntactic integration requires the help of scheme drive of long-term memory, of which MUSACT and the shared syntactic integration resource hypothesis (SSIRH) are the representative theories. While \u201cPhysicalism\u201d considered that it only needs the help of short-term memory in perception drive because pitch syntax has a property of psychoacoustics derived from voice frequency. And the periodicity pitch model (PP) and its derivate, the auditory short-term memory (ASTM), are the representative theories. Both theories demonstrated their viewpoints respectively from the angle of theoretical model, behavioral studies and cognitive neuroscience.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2048499096",
                    "name": "Manping Xie"
                },
                {
                    "authorId": "49307385",
                    "name": "Yufang Yang"
                },
                {
                    "authorId": "51456850",
                    "name": "Qiuyue Zhang"
                }
            ]
        }
    ]
}