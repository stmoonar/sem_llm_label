{
    "authorId": "3475586",
    "papers": [
        {
            "paperId": "2de305c8940a5ac2fbd925d5a16aa95676cbba8f",
            "title": "Tree-of-Traversals: A Zero-Shot Reasoning Algorithm for Augmenting Black-box Language Models with Knowledge Graphs",
            "abstract": "Knowledge graphs (KGs) complement Large Language Models (LLMs) by providing reliable, structured, domain-specific, and up-to-date external knowledge. However, KGs and LLMs are often developed separately and must be integrated after training. We introduce Tree-of-Traversals, a novel zero-shot reasoning algorithm that enables augmentation of black-box LLMs with one or more KGs. The algorithm equips a LLM with actions for interfacing a KG and enables the LLM to perform tree search over possible thoughts and actions to find high confidence reasoning paths. We evaluate on two popular benchmark datasets. Our results show that Tree-of-Traversals significantly improves performance on question answering and KG question answering tasks. Code is available at \\url{https://github.com/amazon-science/tree-of-traversals}",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2083006949",
                    "name": "Elan Markowitz"
                },
                {
                    "authorId": "2266838160",
                    "name": "Anil Ramakrishna"
                },
                {
                    "authorId": "3475586",
                    "name": "J. Dhamala"
                },
                {
                    "authorId": "51997673",
                    "name": "Ninareh Mehrabi"
                },
                {
                    "authorId": "102648923",
                    "name": "Charith Peris"
                },
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                },
                {
                    "authorId": "2256646555",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                }
            ]
        },
        {
            "paperId": "8f22fb1ded314b43922d5cec71deab5fc8b63e06",
            "title": "MICo: Preventative Detoxification of Large Language Models through Inhibition Control",
            "abstract": "Large Language Models (LLMs) are powerful tools which have been both dominant and commonplace in the field of Artificial Intelligence. Yet, LLMs have a tendency to devolve into toxic degeneration, wherein otherwise safe and unproblematic models begin generating toxic content. For the sake of social responsibility and inspired by the biological mechanisms of inhibition control, we introduce the paradigm of Education for Societal Norms (ESN). By collecting and labeling examples as acceptable and unacceptable (in this case toxic and non-toxic), and including a corresponding acceptable rewrite with every unacceptable example, we introduce a new mechanism for LLM detoxification. We annotate a dataset of 2,850 entries and use it to fine-tune a model, which we call a Model with Inhibition Control (MICo). Evaluating this model on toxicity detection capability, rewrite detoxification, meaning preservation, and overall toxicity reduction, we discover significant improvements over the baseline model. In our experiments we show that overall toxicity of this model is more than 60% reduced, with over 75% reduction in severe toxicity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2313541602",
                    "name": "Roy Siegelmann"
                },
                {
                    "authorId": "51997673",
                    "name": "Ninareh Mehrabi"
                },
                {
                    "authorId": "3436466",
                    "name": "Palash Goyal"
                },
                {
                    "authorId": "38774604",
                    "name": "Prasoon Goyal"
                },
                {
                    "authorId": "2308474837",
                    "name": "Lisa Bauer"
                },
                {
                    "authorId": "3475586",
                    "name": "J. Dhamala"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                },
                {
                    "authorId": "2313538923",
                    "name": "Reza Ghanadan"
                }
            ]
        },
        {
            "paperId": "283c4236d56e970a20859b442e048c9f197d15c5",
            "title": "Tokenization Matters: Navigating Data-Scarce Tokenization for Gender Inclusive Language Technologies",
            "abstract": "Gender-inclusive NLP research has documented the harmful limitations of gender binary-centric large language models (LLM), such as the inability to correctly use gender-diverse English neopronouns (e.g., xe, zir, fae). While data scarcity is a known culprit, the precise mechanisms through which scarcity affects this behavior remain underexplored. We discover LLM misgendering is significantly influenced by Byte-Pair Encoding (BPE) tokenization, the tokenizer powering many popular LLMs. Unlike binary pronouns, BPE overfragments neopronouns, a direct consequence of data scarcity during tokenizer training. This disparate tokenization mirrors tokenizer limitations observed in multilingual and low-resource NLP, unlocking new misgendering mitigation strategies. We propose two techniques: (1) pronoun tokenization parity, a method to enforce consistent tokenization across gendered pronouns, and (2) utilizing pre-existing LLM pronoun knowledge to improve neopronoun proficiency. Our proposed methods outperform finetuning with standard BPE, improving neopronoun accuracy from 14.1% to 58.4%. Our paper is the first to link LLM misgendering to tokenization and deficient neopronoun grammar, indicating that LLMs unable to correctly treat neopronouns as pronouns are more prone to misgender.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51494507",
                    "name": "Anaelia Ovalle"
                },
                {
                    "authorId": "51997673",
                    "name": "Ninareh Mehrabi"
                },
                {
                    "authorId": "3436466",
                    "name": "Palash Goyal"
                },
                {
                    "authorId": "3475586",
                    "name": "J. Dhamala"
                },
                {
                    "authorId": "2256646555",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2261362897",
                    "name": "Richard Zemel"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "1826312",
                    "name": "Yuval Pinter"
                },
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                }
            ]
        },
        {
            "paperId": "a0b5d3025a1560274d392846d54d6c93ae518856",
            "title": "Resolving Ambiguities in Text-to-Image Generative Models",
            "abstract": "Natural language often contains ambiguities that can lead to misinterpretation and miscommunication. While humans can handle ambiguities effectively by asking clarifying questions and/or relying on contextual cues and common-sense knowledge, resolving ambiguities can be notoriously hard for machines. In this work, we study ambiguities that arise in text-to-image generative models. We curate the Text-to-image Ambiguity Benchmark (TAB) dataset to study different types of ambiguities in text-to-image generative models. We then propose the Text-to-ImagE Disambiguation (TIED) framework to disambiguate the prompts given to the text-to-image generative models by soliciting clarifications from the end user. Through automatic and human evaluations, we show the effectiveness of our framework in generating more faithful images aligned with end user intention in the presence of ambiguities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51997673",
                    "name": "Ninareh Mehrabi"
                },
                {
                    "authorId": "3436466",
                    "name": "Palash Goyal"
                },
                {
                    "authorId": "3363380",
                    "name": "Apurv Verma"
                },
                {
                    "authorId": "3475586",
                    "name": "J. Dhamala"
                },
                {
                    "authorId": "40574366",
                    "name": "Varun Kumar"
                },
                {
                    "authorId": "2192042848",
                    "name": "Qian Hu"
                },
                {
                    "authorId": "2782886",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "1804104",
                    "name": "R. Zemel"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                }
            ]
        },
        {
            "paperId": "af4e2bc2da111f2a9da9846a6d19d43ce5e63729",
            "title": "Incorporating Fairness in Large Scale NLU Systems",
            "abstract": "NLU models power several user facing experiences such as conversations agents and chat bots. Building NLU models typically consist of 3 stages: a) building or finetuning a pre-trained model b) distilling or fine-tuning the pre-trained model to build task specific models and, c) deploying the task-specific model to production. In this presentation, we will identify fairness considerations that can be incorporated in the aforementioned three stages in the life-cycle of NLU model building: (i) selection/building of a large scale language model, (ii) distillation/fine-tuning the large model into task specific model and, (iii) deployment of the task specific model. We will present select metrics that can be used to quantify fairness in NLU models and fairness enhancement techniques that can be deployed in each of these stages. Finally, we will share some recommendations to successfully implement fairness considerations when building an industrial scale NLU system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                },
                {
                    "authorId": "12620674",
                    "name": "Lisa Bauer"
                },
                {
                    "authorId": "2782886",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "3475586",
                    "name": "J. Dhamala"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "3436466",
                    "name": "Palash Goyal"
                },
                {
                    "authorId": "2192042848",
                    "name": "Qian Hu"
                },
                {
                    "authorId": "33882114",
                    "name": "Avni Khatri"
                },
                {
                    "authorId": "3262272",
                    "name": "Rohit Parimi"
                },
                {
                    "authorId": "102648923",
                    "name": "Charith Peris"
                },
                {
                    "authorId": "3363380",
                    "name": "Apurv Verma"
                },
                {
                    "authorId": "1804104",
                    "name": "R. Zemel"
                },
                {
                    "authorId": "2104644641",
                    "name": "Premkumar Natarajan"
                }
            ]
        },
        {
            "paperId": "d42625e5d8c0ba481fc58be28849c3231250aa0b",
            "title": "JAB: Joint Adversarial Prompting and Belief Augmentation",
            "abstract": "With the recent surge of language models in different applications, attention to safety and robustness of these models has gained significant importance. Here we introduce a joint framework in which we simultaneously probe and improve the robustness of a black-box target model via adversarial prompting and belief augmentation using iterative feedback loops. This framework utilizes an automated red teaming approach to probe the target model, along with a belief augmenter to generate instructions for the target model to improve its robustness to those adversarial probes. Importantly, the adversarial model and the belief generator leverage the feedback from past interactions to improve the effectiveness of the adversarial prompts and beliefs, respectively. In our experiments, we demonstrate that such a framework can reduce toxic content generation both in dynamic cases where an adversary directly interacts with a target model and static cases where we use a static benchmark dataset to evaluate our model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51997673",
                    "name": "Ninareh Mehrabi"
                },
                {
                    "authorId": "3436466",
                    "name": "Palash Goyal"
                },
                {
                    "authorId": "2266838160",
                    "name": "Anil Ramakrishna"
                },
                {
                    "authorId": "3475586",
                    "name": "J. Dhamala"
                },
                {
                    "authorId": "2267024420",
                    "name": "Shalini Ghosh"
                },
                {
                    "authorId": "2261362897",
                    "name": "Richard Zemel"
                },
                {
                    "authorId": "2256646555",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                }
            ]
        },
        {
            "paperId": "e46bc16bf3dc13aa49b93f25b3e0462ac869cfe4",
            "title": "\u201cI\u2019m fully who I am\u201d: Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation",
            "abstract": "Warning: This paper contains examples of gender non-affirmative language which could be offensive, upsetting, and/or triggering. Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of TGNB persons contributes to and persists within Open Language Generation (OLG). This social knowledge serves as a guide for evaluating popular large language models (LLMs) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. To do this, we introduce TANGO, a dataset of template-based real-world text curated from a TGNB-oriented community. We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on average. Our findings warrant further research on how TGNB harms manifest in LLMs and serve as a broader case study toward concretely grounding the design of gender-inclusive AI in community voices and interdisciplinary literature.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51494507",
                    "name": "Anaelia Ovalle"
                },
                {
                    "authorId": "3436466",
                    "name": "Palash Goyal"
                },
                {
                    "authorId": "3475586",
                    "name": "J. Dhamala"
                },
                {
                    "authorId": "20742874",
                    "name": "Zachary Jaggers"
                },
                {
                    "authorId": "2110821190",
                    "name": "Kai Wei Chang"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "1804104",
                    "name": "R. Zemel"
                },
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                }
            ]
        },
        {
            "paperId": "3c759e2f16bfde8d31189631e4893d3ac8ff05f2",
            "title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal",
            "abstract": "Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model\u2019s biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal\u2014modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT\u20132 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2058888124",
                    "name": "Umang Gupta"
                },
                {
                    "authorId": "3475586",
                    "name": "J. Dhamala"
                },
                {
                    "authorId": "40574366",
                    "name": "Varun Kumar"
                },
                {
                    "authorId": "3363380",
                    "name": "Apurv Verma"
                },
                {
                    "authorId": "100984698",
                    "name": "Yada Pruksachatkun"
                },
                {
                    "authorId": "2143841730",
                    "name": "Satyapriya Krishna"
                },
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                },
                {
                    "authorId": "2782886",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "1719898",
                    "name": "G. V. Steeg"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                }
            ]
        },
        {
            "paperId": "452cb1065b9d4ac3fb8f5b4466483dc45585e93d",
            "title": "Measuring Fairness of Text Classifiers via Prediction Sensitivity",
            "abstract": "With the rapid growth in language processing applications, fairness has emerged as an important consideration in data-driven solutions. Although various fairness definitions have been explored in the recent literature, there is lack of consensus on which metrics most accurately reflect the fairness of a system. In this work, we propose a new formulation \u2013 accumulated prediction sensitivity, which measures fairness in machine learning models based on the model\u2019s prediction sensitivity to perturbations in input features. The metric attempts to quantify the extent to which a single prediction depends on a protected attribute, where the protected attribute encodes the membership status of an individual in a protected group. We show that the metric can be theoretically linked with a specific notion of group fairness (statistical parity) and individual fairness. It also correlates well with humans\u2019 perception of fairness. We conduct experiments on two text classification datasets \u2013 Jigsaw Toxicity, and Bias in Bios, and evaluate the correlations between metrics and manual annotations on whether the model produced a fair outcome. We observe that the proposed fairness metric based on prediction sensitivity is statistically significantly more correlated with human annotation than the existing counterfactual fairness metric.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143841730",
                    "name": "Satyapriya Krishna"
                },
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                },
                {
                    "authorId": "3363380",
                    "name": "Apurv Verma"
                },
                {
                    "authorId": "3475586",
                    "name": "J. Dhamala"
                },
                {
                    "authorId": "100984698",
                    "name": "Yada Pruksachatkun"
                },
                {
                    "authorId": "2782886",
                    "name": "Kai-Wei Chang"
                }
            ]
        },
        {
            "paperId": "6c77e4cf3ae78913d5bcf3faf2101bfe28bd19b3",
            "title": "Is the Elephant Flying? Resolving Ambiguities in Text-to-Image Generative Models",
            "abstract": "Natural language often contains ambiguities that can lead to misinterpretation and miscommunication. While humans can handle ambiguities effectively by asking clarifying questions and/or relying on contextual cues and common-sense knowledge, resolving ambiguities can be notoriously hard for machines. In this work, we study ambiguities that arise in text-to-image generative models. We curate a benchmark dataset covering different types of ambiguities that occur in these systems. We then propose a framework to mitigate ambiguities in the prompts given to the systems by soliciting clarifications from the user. Through automatic and human evaluations, we show the effectiveness of our framework in generating more faithful images aligned with human intention in the presence of ambiguities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51997673",
                    "name": "Ninareh Mehrabi"
                },
                {
                    "authorId": "3436466",
                    "name": "Palash Goyal"
                },
                {
                    "authorId": "3363380",
                    "name": "Apurv Verma"
                },
                {
                    "authorId": "3475586",
                    "name": "J. Dhamala"
                },
                {
                    "authorId": "2256788347",
                    "name": "Varun Kumar"
                },
                {
                    "authorId": "2192042848",
                    "name": "Qian Hu"
                },
                {
                    "authorId": "2256646555",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "1804104",
                    "name": "R. Zemel"
                },
                {
                    "authorId": "143728483",
                    "name": "A. Galstyan"
                },
                {
                    "authorId": "2139538015",
                    "name": "Rahul Gupta"
                }
            ]
        }
    ]
}