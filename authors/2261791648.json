{
    "authorId": "2261791648",
    "papers": [
        {
            "paperId": "bf1d7ad51b1ff2c160fbd9f2ee3c49f9ec4de04d",
            "title": "Weakly Supervised Video Moment Retrieval via Location-irrelevant Proposal Learning",
            "abstract": "This paper deals with Video Moment Retrieval (VMR) in a weakly-supervised fashion, which aims to retrieve local video clips with only global video-level descriptions. Scrutinizing the recent advances in VMR, we find that the fully-supervised models achieve strong performance, but they are heavily relied on the precise temporal annotations. Weakly-supervised methods do not rely on temporal annotations, however, their performance is much weaker than the fully-supervised ones. To fill such gap, we propose to take advantage of a pretrained video-text model as hitchhiker to generate pseudo temporal labels. The pseudo temporal labels, together with the descriptive labels, are then utilized to guide the training of the proposed VMR model. The proposed Location-irrelevant Proposal Learning (LPL) model is based on a pretrained video-text model with cross-modal prompt learning, together with different strategies to generate reasonable proposals with various lengths. Despite the simplicity, we find that our method performs much better than the previous state-of-the-art methods on standard benchmarks, eg., +4.4% and +1.4% in mIoU on the Charades and ActivityNet-Caption datasets respectively, which benefits from training with fine-grained video-text pairs. Further experiments on two synthetic datasets with shuffled temporal location and longer video length demonstrate our model's robustness towards temporal localization bias as well as its strength in handling long video sequences.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261791648",
                    "name": "Wei Ji"
                },
                {
                    "authorId": "2301213442",
                    "name": "Ruiqi Shi"
                },
                {
                    "authorId": "2258676675",
                    "name": "Yinwei Wei"
                },
                {
                    "authorId": "2301269310",
                    "name": "Shanshan Zhao"
                },
                {
                    "authorId": "2301213794",
                    "name": "Roger Zimmermann"
                }
            ]
        },
        {
            "paperId": "06673e37f8d8a515d6fe26511dd1fbae69a9237d",
            "title": "Partial Annotation-based Video Moment Retrieval via Iterative Learning",
            "abstract": "Given a descriptive language query, Video Moment Retrieval (VMR) aims to seek the corresponding semantic-consistent moment clip in the video, which is represented as a pair of the start and end timestamps. Although current methods have achieved satisfying performance, training these models heavily relies on the fully-annotated VMR datasets. Nonetheless, precise video temporal annotations are extremely labor-intensive and ambiguous due to the diverse preferences of different annotators. Although there are several works trying to explore weakly supervised VMR tasks with scattered annotated frames as labels, there is still much room to improve in terms of accuracy. Therefore, we design a new setting of VMR where users can easily point to small segments of non-controversy video moments and our proposed method can automatically fill in the remaining parts based on the video and query semantics. To support this, we propose a new framework named Video Moment Retrieval via Iterative Learning (VMRIL). It treats the partial temporal region as the seed, then expands the pseudo label by iterative training. In order to restrict the expansion with reasonable boundaries, we utilize a pretrained video action localization model to provide coarse guidance of potential video segments. Compared with other VMR methods, our VMRIL achieves a trade-off between satisfying performance and annotation efficiency. Experimental results show that our proposed method can achieve the SOTA performance in the weakly supervised VMR setting, and are even comparable with some fully-supervised VMR methods but with much less annotation cost.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261791648",
                    "name": "Wei Ji"
                },
                {
                    "authorId": "2261748535",
                    "name": "Renjie Liang"
                },
                {
                    "authorId": "2261751914",
                    "name": "Lizi Liao"
                },
                {
                    "authorId": "2261750082",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "c84a35eb7da90b7ac54a9cbcfe04631423e825cd",
            "title": "Biased-Predicate Annotation Identification via Unbiased Visual Predicate Representation",
            "abstract": "Panoptic Scene Graph Generation (PSG) translates visual scenes to structured linguistic descriptions, i.e., mapping visual instances to subjects/objects, and their relationships to predicates. However, the annotators' preferences and semantic overlaps between predicates inevitably lead to the semantic mappings of multiple predicates to one relationship, i.e., biased-predicate annotations. As a result, with the contradictory mapping between visual and linguistics, PSG models are struggled to construct clear decision planes among predicates, so as to cause existing poor performances. Obviously, it is essential for the PSG task to tackle this multi-modal contradiction. Therefore, we propose a novel method that utilizes unbiased visual predicate representations for Biased-Annotation Identification (BAI) as a fundamental step for PSG/SGG tasks. Our BAI includes three main steps: predicate representation extraction, predicate representation debiasing, and biased-annotation identification. With flexible biased annotation processing methods, our BAI can act as a fundamental step of dataset debiasing. Experimental results demonstrate that our proposed BAI has achieved state-of-the-art performance, which promotes the performance of benchmark models to various degrees with ingenious biased annotation processing methods. Furthermore, our BAI shows great generalization and effectiveness on multiple datasets. Our codes are released at https://github.com/lili0415/BAI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156060734",
                    "name": "Li Li"
                },
                {
                    "authorId": "2261805541",
                    "name": "Chenwei Wang"
                },
                {
                    "authorId": "2142095326",
                    "name": "Youxuan Qin"
                },
                {
                    "authorId": "2261791648",
                    "name": "Wei Ji"
                },
                {
                    "authorId": "2261748535",
                    "name": "Renjie Liang"
                }
            ]
        },
        {
            "paperId": "d00e854123f11390ace1cae2d6ae0c96f5fb4270",
            "title": "Learning Style-Invariant Robust Representation for Generalizable Visual Instance Retrieval",
            "abstract": "Visual Instance Retrieval (VIR) is a hot research topic for its wide application in real world, such as object re-identification in smart city scenarios. However, due to the limited style diversity in source training data, most existing VIR models always fail to generalize well to unseen domain. How to improve the generalizability of VIR models has received increasing attention in most recent years. In this paper, we pay attention to the Single Domain Generalization (SDG) based VIR task, a more challenging but practical problem, where the model is only trained on single domain data and directly evaluated on unseen target domain without any fine-tuning or adaptations. In this case, the limited style variance in training data may cause the model learning incorrect reliance on the superficial style feature and reduce the generalizability of the model. To address this issue, we propose a novel Style-Invariant robust Representation Learning (SIRL) method for the challenging task, which mainly aims to first diversify the training data with style augmentation, and then enforce the model to learn style-invariant features. Specifically, we first design an adversarial style synthesis module which learns to synthesize diverse augmented samples with adversarially learned styles. Then, we devise an invariant feature learning module to minimize cross-domain feature inconsistency between source images and style-augmented images for capturing domain-invariant instance features. In this way, we can prevent the model from over-exploiting semantic content-independent cues (e.g., color) as shortcut features, thereby estimating the pairwise instance similarity more robustly. We integrate our SIRL method with SOTA VIR networks and evaluate its effectiveness on several public benchmark datasets. Extensive experiments clearly show that the SIRL method can substantially improve the generalizability of existing VIR networks in the challenging SDG-VIR setting.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261896694",
                    "name": "Tianyu Chang"
                },
                {
                    "authorId": "2238337235",
                    "name": "Xun Yang"
                },
                {
                    "authorId": "2261811512",
                    "name": "Xin Luo"
                },
                {
                    "authorId": "2261791648",
                    "name": "Wei Ji"
                },
                {
                    "authorId": "2146058717",
                    "name": "Meng Wang"
                }
            ]
        },
        {
            "paperId": "e46dea661e82d2c2468dbdb758aae3e4ba878277",
            "title": "Deep Multimodal Learning for Information Retrieval",
            "abstract": "Information retrieval (IR) is a fundamental technique that aims to acquire information from a collection of documents, web pages, or other sources. While traditional text-based IR has achieved great success, the under-utilization of varied data sources in different modalities (i.e., text, images, audio, and video) would hinder IR techniques from giving its full advancement and thus limits the applications in the real world. Within recent years, the rapid development of deep multimodal learning paves the way for advancing IR with multi-modality. Benefiting from a variety of data types and modalities, some latest prevailing techniques are invented to show great facilitation in multi-modal and IR learning, such as CLIP, ChatGPT, GPT4, etc. In the context of IR, deep multi-modal learning has shown the prominent potential to improve the performance of retrieval systems, by enabling them to better understand and process the diverse types of data that they encounter. Given the great potential shown by multimodal-empowered IR, there can be still unsolved challenges and open questions in the related directions. With this workshop, we aim to provide a platform for discussion about multi-modal IR among scholars, practitioners, and other interested parties.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2261791648",
                    "name": "Wei Ji"
                },
                {
                    "authorId": "1887997",
                    "name": "Yin-wei Wei"
                },
                {
                    "authorId": "2187311001",
                    "name": "Zhedong Zheng"
                },
                {
                    "authorId": "2261750082",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                }
            ]
        }
    ]
}