{
    "authorId": "2242962967",
    "papers": [
        {
            "paperId": "2902a67f9aebb115fc2b6cdf611910e72e896bdd",
            "title": "GOODAT: Towards Test-time Graph Out-of-Distribution Detection",
            "abstract": "Graph neural networks (GNNs) have found widespread application in modeling graph data across diverse domains. While GNNs excel in scenarios where the testing data shares the distribution of their training counterparts (in distribution, ID), they often exhibit incorrect predictions when confronted with samples from an unfamiliar distribution (out-of-distribution, OOD). To identify and reject OOD samples with GNNs, recent studies have explored graph OOD detection, often focusing on training a specific model or modifying the data on top of a well-trained GNN. Despite their effectiveness, these methods come with heavy training resources and costs, as they need to optimize the GNN-based models on training data. Moreover, their reliance on modifying the original GNNs and accessing training data further restricts their universality. To this end, this paper introduces a method to detect Graph Out-of-Distribution At Test-time (namely GOODAT), a data-centric, unsupervised, and plug-and-play solution that operates independently of training data and modifications of GNN architecture. With a lightweight graph masker, GOODAT can learn informative subgraphs from test samples, enabling the capture of distinct graph patterns between OOD and ID samples. To optimize the graph masker, we meticulously design three unsupervised objective functions based on the graph information bottleneck principle, motivating the masker to capture compact yet informative subgraphs for OOD detection. Comprehensive evaluations confirm that our GOODAT method outperforms state-of-the-art benchmarks across a variety of real-world datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2278588129",
                    "name": "Luzhi Wang"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2242962967",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "49336556",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2277958230",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "8b6dfbd42df82c49bf358d1a233184c6cb177011",
            "title": "ARC: A Generalist Graph Anomaly Detector with In-Context Learning",
            "abstract": "Graph anomaly detection (GAD), which aims to identify abnormal nodes that differ from the majority within a graph, has garnered significant attention. However, current GAD methods necessitate training specific to each dataset, resulting in high training costs, substantial data requirements, and limited generalizability when being applied to new datasets and domains. To address these limitations, this paper proposes ARC, a generalist GAD approach that enables a ``one-for-all'' GAD model to detect anomalies across various graph datasets on-the-fly. Equipped with in-context learning, ARC can directly extract dataset-specific patterns from the target dataset using few-shot normal samples at the inference stage, without the need for retraining or fine-tuning on the target dataset. ARC comprises three components that are well-crafted for capturing universal graph anomaly patterns: 1) smoothness-based feature Alignment module that unifies the features of different datasets into a common and anomaly-sensitive space; 2) ego-neighbor Residual graph encoder that learns abnormality-related node embeddings; and 3) cross-attentive in-Context anomaly scoring module that predicts node abnormality by leveraging few-shot normal samples. Extensive experiments on multiple benchmark datasets from various domains demonstrate the superior anomaly detection performance, efficiency, and generalizability of ARC.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2242962967",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "2291165055",
                    "name": "Shiyuan Li"
                },
                {
                    "authorId": "2149514422",
                    "name": "Yu Zheng"
                },
                {
                    "authorId": "2303729500",
                    "name": "Qingfeng Chen"
                },
                {
                    "authorId": "2302859288",
                    "name": "Chengqi Zhang"
                },
                {
                    "authorId": "2291081128",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "d212c555293b81f845b3c99af4e922b0fcdb4290",
            "title": "Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark",
            "abstract": "To build safe and reliable graph machine learning systems, unsupervised graph-level anomaly detection (GLAD) and unsupervised graph-level out-of-distribution (OOD) detection (GLOD) have received significant attention in recent years. Though those two lines of research indeed share the same objective, they have been studied independently in the community due to distinct evaluation setups, creating a gap that hinders the application and evaluation of methods from one to the other. To bridge the gap, in this work, we present a Unified Benchmark for unsupervised Graph-level OOD and anomaly Detection (our method), a comprehensive evaluation framework that unifies GLAD and GLOD under the concept of generalized graph-level OOD detection. Our benchmark encompasses 35 datasets spanning four practical anomaly and OOD detection scenarios, facilitating the comparison of 16 representative GLAD/GLOD methods. We conduct multi-dimensional analyses to explore the effectiveness, generalizability, robustness, and efficiency of existing methods, shedding light on their strengths and limitations. Furthermore, we provide an open-source codebase (https://github.com/UB-GOLD/UB-GOLD) of our method to foster reproducible research and outline potential directions for future investigations based on our insights.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2298205559",
                    "name": "Yili Wang"
                },
                {
                    "authorId": "2242962967",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "2298108098",
                    "name": "Xu Shen"
                },
                {
                    "authorId": "2308068445",
                    "name": "Chenyu Li"
                },
                {
                    "authorId": "2261590627",
                    "name": "Kaize Ding"
                },
                {
                    "authorId": "2171753488",
                    "name": "Rui Miao"
                },
                {
                    "authorId": "2303364512",
                    "name": "Ying Wang"
                },
                {
                    "authorId": "2298272389",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2298094103",
                    "name": "Xin Wang"
                }
            ]
        },
        {
            "paperId": "e360dc0ad430d91380055a06aecfb1a0811cf26b",
            "title": "Noise-Resilient Unsupervised Graph Representation Learning via Multi-Hop Feature Quality Estimation",
            "abstract": "Unsupervised graph representation learning (UGRL) based on graph neural networks (GNNs), has received increasing attention owing to its efficacy in handling graph-structured data. However, existing UGRL methods ideally assume that the node features are noise-free, which makes them fail to distinguish between useful information and noise when applied to real data with noisy features, thus affecting the quality of learned representations. This urges us to take node noisy features into account in real-world UGRL. With empirical analysis, we reveal that feature propagation, the essential operation in GNNs, acts as a\"double-edged sword\"in handling noisy features - it can both denoise and diffuse noise, leading to varying feature quality across nodes, even within the same node at different hops. Building on this insight, we propose a novel UGRL method based on Multi-hop feature Quality Estimation (MQE for short). Unlike most UGRL models that directly utilize propagation-based GNNs to generate representations, our approach aims to learn representations through estimating the quality of propagated features at different hops. Specifically, we introduce a Gaussian model that utilizes a learnable\"meta-representation\"as a condition to estimate the expectation and variance of multi-hop propagated features via neural networks. In this way, the\"meta representation\"captures the semantic and structural information underlying multiple propagated features but is naturally less susceptible to interference by noise, thereby serving as high-quality node representations beneficial for downstream tasks. Extensive experiments on multiple real-world datasets demonstrate that MQE in learning reliable node representations in scenarios with diverse types of feature noise.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2291165055",
                    "name": "Shiyuan Li"
                },
                {
                    "authorId": "2242962967",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "2303729500",
                    "name": "Qingfeng Chen"
                },
                {
                    "authorId": "2243967976",
                    "name": "Geoffrey I. Webb"
                },
                {
                    "authorId": "2291081128",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "89ba59ab6b086d86ce27df32652b7499498d70fb",
            "title": "Integrating Graphs With Large Language Models: Methods and Prospects",
            "abstract": "Large language models (LLMs) such as Generative Pre-trained Transformer 4 have emerged as frontrunners, showcasing unparalleled prowess in diverse applications including answering queries, code generation, and more. Parallelly, graph-structured data, intrinsic data types, are pervasive in real-world scenarios. Merging the capabilities of LLMs with graph-structured data has been a topic of keen interest. This article bifurcates such integrations into two predominant categories. The first leverages LLMs for graph learning, where LLMs can not only augment existing graph algorithms but also stand as prediction models for various graph tasks. Conversely, the second category underscores the pivotal role of graphs in advancing LLMs. Mirroring human cognition, we solve complex tasks by adopting graphs in either reasoning or collaboration. Integrating with such structures can significantly boost the performance of LLMs in various complicated tasks. We also discuss and propose open questions for integrating LLMs with graph-structured data for the future direction of the field.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2256999516",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "2242962967",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "2215634631",
                    "name": "San Murugesan"
                }
            ]
        },
        {
            "paperId": "911247dddb66d1f3ef3ba3235861bd707d15b9ad",
            "title": "Towards Data-centric Graph Machine Learning: Review and Outlook",
            "abstract": "Data-centric AI, with its primary focus on the collection, management, and utilization of data to drive AI models and applications, has attracted increasing attention in recent years. In this article, we conduct an in-depth and comprehensive review, offering a forward-looking outlook on the current efforts in data-centric AI pertaining to graph data-the fundamental data structure for representing and capturing intricate dependencies among massive and diverse real-life entities. We introduce a systematic framework, Data-centric Graph Machine Learning (DC-GML), that encompasses all stages of the graph data lifecycle, including graph data collection, exploration, improvement, exploitation, and maintenance. A thorough taxonomy of each stage is presented to answer three critical graph-centric questions: (1) how to enhance graph data availability and quality; (2) how to learn from graph data with limited-availability and low-quality; (3) how to build graph MLOps systems from the graph data-centric view. Lastly, we pinpoint the future prospects of the DC-GML domain, providing insights to navigate its advancements and applications.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1430762233",
                    "name": "Xin Zheng"
                },
                {
                    "authorId": "2242962967",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "2273941154",
                    "name": "Zhifeng Bao"
                },
                {
                    "authorId": "2273562188",
                    "name": "Meng Fang"
                },
                {
                    "authorId": "2243358879",
                    "name": "Xia Hu"
                },
                {
                    "authorId": "2243282363",
                    "name": "Alan Wee-Chung Liew"
                },
                {
                    "authorId": "2585415",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "db27a18d04e22ddc91ff74204e074c1aaa6d239b",
            "title": "Towards Self-Interpretable Graph-Level Anomaly Detection",
            "abstract": "Graph-level anomaly detection (GLAD) aims to identify graphs that exhibit notable dissimilarity compared to the majority in a collection. However, current works primarily focus on evaluating graph-level abnormality while failing to provide meaningful explanations for the predictions, which largely limits their reliability and application scope. In this paper, we investigate a new challenging problem, explainable GLAD, where the learning objective is to predict the abnormality of each graph sample with corresponding explanations, i.e., the vital subgraph that leads to the predictions. To address this challenging problem, we propose a Self-Interpretable Graph aNomaly dETection model (SIGNET for short) that detects anomalous graphs as well as generates informative explanations simultaneously. Specifically, we first introduce the multi-view subgraph information bottleneck (MSIB) framework, serving as the design basis of our self-interpretable GLAD approach. This way SIGNET is able to not only measure the abnormality of each graph based on cross-view mutual information but also provide informative graph rationales by extracting bottleneck subgraphs from the input graph and its dual hypergraph in a self-supervised way. Extensive experiments on 16 datasets demonstrate the anomaly detection capability and self-interpretability of SIGNET.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2242962967",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "2261590627",
                    "name": "Kaize Ding"
                },
                {
                    "authorId": "2262196971",
                    "name": "Qinghua Lu"
                },
                {
                    "authorId": "2203372264",
                    "name": "Fuyi Li"
                },
                {
                    "authorId": "2248789322",
                    "name": "Leo Yu Zhang"
                },
                {
                    "authorId": "2240550258",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "ebfa6bfa3251bd516d68e2854ef9258ff0653d02",
            "title": "PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection",
            "abstract": "Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies - reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreover, our method demonstrated robustness and effectiveness in five datasets. Notably, when validated on the ACM dataset, PREM achieved a 5% improvement in AUC, a 9-fold increase in training speed, and sharply reduce memory usage compared to the most efficient baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2260294610",
                    "name": "Junjun Pan"
                },
                {
                    "authorId": "2242962967",
                    "name": "Yixin Liu"
                },
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "2256999516",
                    "name": "Shirui Pan"
                }
            ]
        }
    ]
}