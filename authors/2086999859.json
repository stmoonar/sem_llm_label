{
    "authorId": "2086999859",
    "papers": [
        {
            "paperId": "1796063938d67283f7b726e14181ad6c1db591ec",
            "title": "Contrast then Memorize: Semantic Neighbor Retrieval-Enhanced Inductive Multimodal Knowledge Graph Completion",
            "abstract": "A large number of studies have emerged for Multimodal Knowledge Graph Completion (MKGC) to predict the missing links in MKGs. However, fewer studies have been proposed to study the inductive MKGC (IMKGC) involving emerging entities unseen during training. Existing inductive approaches focus on learning textual entity representations, which neglect rich semantic information in visual modality. Moreover, they focus on aggregating structural neighbors from existing KGs, which of emerging entities are usually limited. However, the semantic neighbors are decoupled from the topology linkage and usually imply the true target entity. In this paper, we propose the IMKGC task and a semantic neighbor retrieval-enhanced IMKGC framework CMR, where the contrast brings the helpful semantic neighbors close, and then the memorize supports semantic neighbor retrieval to enhance inference. Specifically, we first propose a unified cross-modal contrastive learning to simultaneously capture the textual-visual and textual-textual correlations of query-entity pairs in a unified representation space. The contrastive learning increases the similarity of positive query-entity pairs, therefore making the representations of helpful semantic neighbors close. Then, we explicitly memorize the knowledge representations to support the semantic neighbor retrieval. At test time, we retrieve the nearest semantic neighbors and interpolate them to the query-entity similarity distribution to augment the final prediction. Extensive experiments validate the effectiveness of CMR on three inductive MKGC datasets. Codes are available at https://github.com/OreOZhao/CMR.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2309833618",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "2301634520",
                    "name": "Xinying Qian"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "2279947272",
                    "name": "Xiangrui Cai"
                }
            ]
        },
        {
            "paperId": "52dd29bd1d968e5db8661aa0a9e1d3f5d6c8c827",
            "title": "MCIL: Multimodal Counterfactual Instance Learning for Low-resource Entity-based Multimodal Information Extraction",
            "abstract": "Multimodal information extraction (MIE) is a challenging task which aims to extract the structural information in free text coupled with the image for constructing the multimodal knowledge graph. The entity-based MIE tasks are based on the entity information to complete the specific tasks. However, the existing methods only investigated the entity-based MIE tasks under supervised learning with adequate labeled data. In the real-world scenario, collecting enough data and annotating the entity-based samples are time-consuming, and impractical. Therefore, we propose to investigate the entity-based MIE tasks under the low-resource settings. The conventional models are prone to overfitting on limited labeled data, which can result in poor performance. This is because the models tend to learn the bias existing in the limited samples, which can lead them to model the spurious correlations between multimodal features and task labels. To provide a more comprehensive understanding of the bias inherent in multimodal features of MIE samples, we decompose the features into image, entity, and context factors. Furthermore, we investigate the causal relationships between these factors and model performance, leveraging the structural causal model to delve into the correlations between the input features and output labels. Based on this, we propose the multimodal counterfactual instance learning framework to generate the counterfactual instances by the interventions on the limited observational samples. In the framework, we analyze the causal effect of the counterfactual instances and exploit it as a supervisory signal to maximize the effect for reducing the bias and improving the generalization of the model. Empirically, we evaluate the proposed method on the two public MIE benchmark datasets and the experimental results verify the effectiveness of it.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "2301654681",
                    "name": "Hongru Wang"
                },
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2168240441",
                    "name": "Xuhui Sui"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "8fdafdd43a224b0d9cfd8fb5bf9cae60a582067e",
            "title": "Bring Invariant to Variant: A Contrastive Prompt-based Framework for Temporal Knowledge Graph Forecasting",
            "abstract": "Temporal knowledge graph forecasting aims to reason over known facts to complete the missing links in the future. Existing methods are highly dependent on the structures of temporal knowledge graphs and commonly utilize recurrent or graph neural networks for forecasting. However, entities that are infrequently observed or have not been seen recently face challenges in learning effective knowledge representations due to insufficient structural contexts. To address the above disadvantages, in this paper, we propose a Contrastive Prompt-based framework with Entity background information for TKG forecasting, which we named CoPET. Specifically, to bring the time-invariant entity background information to time-variant structural information, we employ a dual encoder architecture consisting of a candidate encoder and a query encoder. A contrastive learning framework is used to encourage the query representation to be closer to the candidate representation. We further propose three kinds of trainable time-variant prompts aimed at capturing temporal structural information. Experiments on two datasets demonstrate that our method is effective and stays competitive in inference with limited structural information. Our code is available at https://github.com/qianxinying/CoPET.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2301634520",
                    "name": "Xinying Qian"
                },
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "cb5ddabab918e28ed36ab211f6c9ace6e476fb01",
            "title": "MELOV: Multimodal Entity Linking with Optimized Visual Features in Latent Space",
            "abstract": "Multimodal entity linking (MEL), which aligns ambiguous mentions within multimodal contexts to referent entities from multimodal knowledge bases, is essential for many natu-ral language processing applications. Previous MEL methods mainly focus on exploring complex multimodal interaction mechanisms to better capture coherence evidence between mentions and entities by mining complementary information. However, in real-world social media scenarios, vision modality often exhibits low quality, low value, or low relevance to the mention. Integrating such information directly will backfire, leading to a weakened consistency between mentions and their corresponding entities. In this paper, we propose a novel latent space vision feature optimization framework MELOV, which combines inter-modality and intra-modality optimizations to address these challenges. For the inter-modality optimization, we exploit the variational autoencoder to mine shared information and generate text-based visual features. For the intra-modality optimization, we consider the relationships between mentions and build graph convolutional network to aggregate the visual features of semantic similar neighbors. Extensive experiments on three benchmark datasets demonstrate the superiority of our proposed framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2168240441",
                    "name": "Xuhui Sui"
                },
                {
                    "authorId": "2309833618",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "2314896930",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "0941d750525d6dcea5c4a497ea8d80bd4e7bfba3",
            "title": "Incorporating Object-Level Visual Context for Multimodal Fine-Grained Entity Typing",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2273905684",
                    "name": "Wenbo Fan"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "2110260640",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2168240441",
                    "name": "Xuhui Sui"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "9823e7d0bc60968e0bc793593811092c8a0146ef",
            "title": "Selecting Key Views for Zero-Shot Entity Linking",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2168240441",
                    "name": "Xuhui Sui"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                },
                {
                    "authorId": "2273574757",
                    "name": "Wensheng Zhang"
                }
            ]
        },
        {
            "paperId": "a09ae984b1782a2206591c2b77ad13296dc790e3",
            "title": "Location Recommendation Based on Mobility Graph With Individual and Group Influences",
            "abstract": "With the rapid development of mobile technology, it is very convenient to share people\u2019s current locations by checking-in on Location-Based Social Networks (LBSNs). Using users\u2019 check-in histories to study mobility preferences and recommend new locations is a typical application to LBSNs. Most existing models explore reasonable representations for users and locations. However, a lack of behavioral mobility modeling would hamper a better understanding of users\u2019 mobility patterns. This paper proposes a location recommendation model to serve the personalized LBSNs application, called Spatio-temporal Individual mobility graph encoding network with Group Mobility Assistance (SIGMA). We design a spatio-temporal interaction enhanced graph neural network to encode the mobility graphs to represent individual mobility behaviors. Furthermore, we provide a novel stacked scoring approach to generate the recommendation score by combining the stacked individual mobility graphs with the group influences. We conduct extensive experiments on two real-world LBSNs data, Foursquare and Gowalla. The result demonstrates SIGMA outperforms ten state-of-the-art models and further confirms that both the individual and the group mobility behaviors play essential roles in the practical scenario of location recommendation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144224876",
                    "name": "Xuan Pan"
                },
                {
                    "authorId": "3127329",
                    "name": "Xiangrui Cai"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "37022396",
                    "name": "T. Baker"
                },
                {
                    "authorId": "11041265",
                    "name": "T. Gadekallu"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "8c739b074de8d664617cdcc56805202ca1596b6e",
            "title": "Improving Zero-Shot Entity Linking Candidate Generation with Ultra-Fine Entity Type Information",
            "abstract": "Entity linking, which aims at aligning ambiguous entity mentions to their referent entities in a knowledge base, plays a key role in multiple natural language processing tasks. Recently, zero-shot entity linking task has become a research hotspot, which links mentions to unseen entities to challenge the generalization ability. For this task, the training set and test set are from different domains, and thus entity linking models tend to be overfitting due to the tendency of memorizing the properties of entities that appear frequently in the training set. We argue that general ultra-fine-grained type information can help the linking models to learn contextual commonality and improve their generalization ability to tackle the overfitting problem. However, in the zero-shot entity linking setting, any type information is not available and entities are only identified by textual descriptions. Thus, we first extract the ultra-fine entity type information from the entity textual descriptions. Then, we propose a hierarchical multi-task model to improve the high-level zero-shot entity linking candidate generation task by utilizing the entity typing task as an auxiliary low-level task, which introduces extracted ultra-fine type information into the candidate generation task. Experimental results demonstrate the effectiveness of utilizing the ultra-fine entity type information and our proposed method achieves state-of-the-art performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2168240441",
                    "name": "Xuhui Sui"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "2007774059",
                    "name": "Xinde Wei"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "b14820cd3c051794c8db445dc21682cb6c198f8e",
            "title": "A Span-based Multimodal Variational Autoencoder for Semi-supervised Multimodal Named Entity Recognition",
            "abstract": "Multimodal named entity recognition (MNER) on social media is a challenging task which aims to extract named entities in free text and incorporate images to classify them into user-defined types. However, the annotation for named entities on social media demands a mount of human efforts. The existing semi-supervised named entity recognition methods focus on the text modal and are utilized to reduce labeling costs in traditional NER. However, the previous methods are not efficient for semi-supervised MNER. Because the MNER task is defined to combine the text information with image one and needs to consider the mismatch between the posted text and image. To fuse the text and image features for MNER effectively under semi-supervised setting, we propose a novel span-based multimodal variational autoencoder (SMVAE) model for semi-supervised MNER. The proposed method exploits modal-specific VAEs to model text and image latent features, and utilizes product-of-experts to acquire multimodal features. In our approach, the implicit relations between labels and multimodal features are modeled by multimodal VAE. Thus, the useful information of unlabeled data can be exploited in our method under semi-supervised setting. Experimental results on two benchmark datasets demonstrate that our approach not only outperforms baselines under supervised setting, but also improves MNER performance with less labeled data than existing semi-supervised methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "48379623",
                    "name": "Y. Zhang"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "1638101906",
                    "name": "Wenya Guo"
                },
                {
                    "authorId": "2176479216",
                    "name": "Guoqing Zhao"
                },
                {
                    "authorId": "2155786293",
                    "name": "Hongbin Wang"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        },
        {
            "paperId": "db433b75d947ca93bb99955d2e16083c9952e7a1",
            "title": "A Multi-Task Learning Framework for Chinese Medical Procedure Entity Normalization",
            "abstract": "Medical entity normalization is a fundamental task in medical natural language processing and clinical applications. The task aims to map medical mentions to standard entities in a given knowledge base. In this paper, we focus on Chinese medical procedure entity normalization. This task brings an extra multi-implication challenge that a mention may link to multiple standard entities. To perform the task, we propose a novel deep neural multi-task learning framework to jointly model implication number prediction and entity normalization. Our model utilizes the multi-head attention mechanism to provide mutual benefits between the two tasks. Experimental results show that our method achieves comparable performance compared with the baseline methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2168240441",
                    "name": "Xuhui Sui"
                },
                {
                    "authorId": "2086999859",
                    "name": "Kehui Song"
                },
                {
                    "authorId": "2109060719",
                    "name": "Baohang Zhou"
                },
                {
                    "authorId": "100996634",
                    "name": "Ying Zhang"
                },
                {
                    "authorId": "1721029",
                    "name": "Xiaojie Yuan"
                }
            ]
        }
    ]
}