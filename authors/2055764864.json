{
    "authorId": "2055764864",
    "papers": [
        {
            "paperId": "8120b21c146bd96d82add2b5af54f2673d9d437c",
            "title": "CTRL: Connect Collaborative and Language Model for CTR Prediction",
            "abstract": "Traditional click-through rate (CTR) prediction models convert the tabular data into one-hot vectors and leverage the collaborative relations among features for inferring the user's preference over items. This modeling paradigm discards essential semantic information. Though some works like P5 and CTR-BERT have explored the potential of using Pre-trained Language Models (PLMs) to extract semantic signals for CTR prediction, they are computationally expensive and suffer from low efficiency. Besides, the beneficial collaborative relations are not considered, hindering the recommendation performance. To solve these problems, in this paper, we propose a novel framework \\textbf{CTRL}, which is industrial-friendly and model-agnostic with superior inference efficiency. Specifically, the original tabular data is first converted into textual data. Both tabular data and converted textual data are regarded as two different modalities and are separately fed into the collaborative CTR model and pre-trained language model. A cross-modal knowledge alignment procedure is performed to fine-grained align and integrate the collaborative and semantic signals, and the lightweight collaborative model can be deployed online for efficient serving after fine-tuned with supervised signals. Experimental results on three public datasets show that CTRL outperforms the state-of-the-art (SOTA) CTR models significantly. Moreover, we further verify its effectiveness on a large-scale industrial recommender system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2181637944",
                    "name": "Xiangyang Li"
                },
                {
                    "authorId": "92633145",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2055764864",
                    "name": "Luyao Hou"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "db5db403116563cab60f82cbf1a0d745fb80e4aa",
            "title": "CTRL: Connect Tabular and Language Model for CTR Prediction",
            "abstract": "Traditional click-through rate (CTR) prediction models convert the tabular data into one-hot vectors and leverage the collaborative relations among features for inferring user\u2019s preference over items. This modeling paradigm discards the essential semantic information. Though some recent works like P5 and M6-Rec have explored the potential of using Pre-trained Language Models (PLMs) to extract semantic signals for CTR prediction, they are computationally expensive and suffer from low efficiency. Besides, the beneficial collaborative relations are not considered, hindering the recommendation performance. To solve these problems, in this paper, we propose a novel framework CTRL , which is industrial friendly and model-agnostic with high training and inference efficiency. Specifically, the original tabular data is first converted into textual data. Both tabular data and converted textual data are regarded as two different modalities and are separately fed into the collaborative CTR model and pre-trained language model. A cross-modal knowledge alignment procedure is performed to fine-grained align and integrate the collaborative and semantic signals, and the lightweight collaborative model can be deployed online for efficient serving after fine-tuned with supervised signals. Experimental results on three public datasets show that CTRL outperforms the SOTA CTR models significantly. Moreover, we further verify its effectiveness on a large-scale industrial recommender system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2181637944",
                    "name": "Xiangyang Li"
                },
                {
                    "authorId": "92633145",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "2055764864",
                    "name": "Luyao Hou"
                },
                {
                    "authorId": "2257180930",
                    "name": "Ruiming Tang"
                }
            ]
        },
        {
            "paperId": "624c0373019dabb9f5b69464c0f094013c2b101c",
            "title": "Adaptive Low-Precision Training for Embeddings in Click-Through Rate Prediction",
            "abstract": "Embedding tables are usually huge in click-through rate (CTR) prediction models. To train and deploy the CTR models efficiently and economically, it is necessary to compress their embedding tables. To this end, we formulate a novel quantization training paradigm to compress the embeddings from the training stage, termed low-precision training (LPT). Also, we provide theoretical analysis on its convergence. The results show that stochastic weight quantization has a faster convergence rate and a smaller convergence error than deterministic weight quantization in LPT. Further, to reduce accuracy degradation, we propose adaptive low-precision training (ALPT) which learns the step size (i.e., the quantization resolution). Experiments on two real-world datasets confirm our analysis and show that ALPT can significantly improve the prediction accuracy, especially at extremely low bit width. For the first time in CTR models, we successfully train 8-bit embeddings without sacrificing prediction accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2195780326",
                    "name": "Shiwei Li"
                },
                {
                    "authorId": "3339005",
                    "name": "Huifeng Guo"
                },
                {
                    "authorId": "2055764864",
                    "name": "Luyao Hou"
                },
                {
                    "authorId": "101586650",
                    "name": "Wei Zhang"
                },
                {
                    "authorId": "2109888596",
                    "name": "Xing Tang"
                },
                {
                    "authorId": "2824766",
                    "name": "Ruiming Tang"
                },
                {
                    "authorId": "144142354",
                    "name": "Rui Zhang"
                },
                {
                    "authorId": "145765726",
                    "name": "Rui Li"
                }
            ]
        }
    ]
}