{
    "authorId": "2150591081",
    "papers": [
        {
            "paperId": "303a86f5bf84d5767a19c888d7d3027b30d85b98",
            "title": "Attentive Modeling and Distillation for Out-of-Distribution Generalization of Federated Learning",
            "abstract": "Out-of-distribution issues lead to different optimization directions between clients, which weakens collaborative modeling in federated learning. Existing methods aims to decouple invariant features in the latent space to mitigate attribute bias. However, their performance is limited by suboptimal decoupling capabilities in complex latent spaces. To address this problem, this paper presents a method, termed FedAKD, that adaptively identifies meaningful visual regions in images to guide the model in learning causal features. It includes two main modules, where the attentive modeling module adaptively locates critical regions to mitigate the negative impact of irrelevant elements, which are considered significant contributors to distribution heterogeneity. The attention-guided representation learning module leverages attentive knowledge to guide the local model to pay more attention to important regions, which acts as a soft attention regularizer to mitigate the trade-off between capturing category-relevant information and irrelevant contextual information in images. Experiments were conducted on four datasets, including performance comparison, ablation study, and case study. The results demonstrate that FedAKD can effectively enhance attention to causal features, which leads to superior performance compared with the state-of-the-art methods. The source codes have been released at https://github.com/qizhuang-qz/FedAKD.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2303974934",
                    "name": "Weihao He"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2240864213",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "4301175198235888abfff048d5a430c832d9403b",
            "title": "Unifying Visual and Semantic Feature Spaces with Diffusion Models for Enhanced Cross-Modal Alignment",
            "abstract": "Image classification models often demonstrate unstable performance in real-world applications due to variations in image information, driven by differing visual perspectives of subject objects and lighting discrepancies. To mitigate these challenges, existing studies commonly incorporate additional modal information matching the visual data to regularize the model's learning process, enabling the extraction of high-quality visual features from complex image regions. Specifically, in the realm of multimodal learning, cross-modal alignment is recognized as an effective strategy, harmonizing different modal information by learning a domain-consistent latent feature space for visual and semantic features. However, this approach may face limitations due to the heterogeneity between multimodal information, such as differences in feature distribution and structure. To address this issue, we introduce a Multimodal Alignment and Reconstruction Network (MARNet), designed to enhance the model's resistance to visual noise. Importantly, MARNet includes a cross-modal diffusion reconstruction module for smoothly and stably blending information across different domains. Experiments conducted on two benchmark datasets, Vireo-Food172 and Ingredient-101, demonstrate that MARNet effectively improves the quality of image information extracted by the model. It is a plug-and-play framework that can be rapidly integrated into various image classification frameworks, boosting model performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2183591290",
                    "name": "Yuze Zheng"
                },
                {
                    "authorId": "2313697725",
                    "name": "Zixuan Li"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2313586899",
                    "name": "Jinxing Liu"
                },
                {
                    "authorId": "2226670159",
                    "name": "Yuqing Wang"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2240864213",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "82318b63f31b74ed44f136c99e5952d4aafb7928",
            "title": "Multimodal Conditioned Diffusion Model for Recommendation",
            "abstract": "Multimodal recommendation aims at to modeling the feature distributions of items by using their multi-modal information. Prior efforts typically focus on the denoising of the user-item graph with a degree-sensitive strategy, which may not well-handle the users' consistent preference across modalities. More importantly, it has been observed that existing methods may learn ill-posed item embeddings due to their focus on a specific auxiliary optimization task for multimodal representations rather than explicitly modeling them. This paper therefore presents a solution that takes the advantages of the explicit uncertainty injection ability of Diffusion Model (DM) for the modeling and fusion of multi-modal information. Specifically, we propose a novel Multimodal Conditioned Diffusion Model for Recommendation (MCDRec), which tailors DM with two technical modules to model the high-order multimodal knowledge. The first module is multimodal-conditioned representation diffusion (MRD), which integrates pre-extracted multimodal knowledge into the item representation modeling via a tailored DM. This smoothly bridges the insurmountable gap between the multi-modal content features and the collaborative signals. Secondly, with the diffusion-guided graph denoising (DGD) module, MCDRec may effectively denoise the user-item graph by filtering the occasional interactions in user historical behaviors. This is achieved with the power of DM in aligning the users' collaborative preferences with their shared items' content information. Extensive experiments compared to several SOTA baselines on two real-word datasets demonstrate the effectiveness of MCDRec. The specific visualization also reveals the potential of MRD to precisely handling the high-order representation correlations among the user embeddings and the multi-modal heterogeneous representations of items.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152191425",
                    "name": "Haokai Ma"
                },
                {
                    "authorId": "2301174391",
                    "name": "Yimeng Yang"
                },
                {
                    "authorId": "2278235051",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "2257007994",
                    "name": "Ruobing Xie"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "b93feee06377547e1b6c2c3062879504c532cebf",
            "title": "Scene Sketch-to-Image Synthesis Based on Multi-Object Control",
            "abstract": "Scene sketch-to-image synthesis is a challenging task, especially when the sketches contain multiple objects of different classes. Existing methods interfere between different classes of objects when generating images from scene sketches, making it difficult to synthesis images with accurate object classes. In this paper, we propose a scene sketch-to-image generation method based on multi-object control, which can generate high-quality and class-accurate images from scene sketches and text prompts. We propose a sampling strategy based on segmentation mask and independent denoising, which can accurately control the classes of foreground objects and make foreground objects and background more harmonized. Our method is based on a pre-trained diffusion model without additional training overhead. Experiments on SketchyCOCO and SketchyScene datasets demonstrate that our method\u2019s capacity to generate realistic complex images from scene sketches and text prompts.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216710974",
                    "name": "Zhenwei Cheng"
                },
                {
                    "authorId": "2292654467",
                    "name": "Lei Wu"
                },
                {
                    "authorId": "2274200802",
                    "name": "Changshuo Wang"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "c184104b396cd7fa8afd448f7238ecc7d5b8f8ea",
            "title": "LayoutDM: Precision Multi-Scale Diffusion for Layout-to-Image",
            "abstract": "In recent research literature, the layout to image domain has gained significant traction. Research based on GANs can generate complete images, but there are still issues with insufficient details and overall quality not being high. Diffusion models, on the other hand, challenges still exist in ensuring the quality of image details in complex scene regions, as well as ensuring the smooth expression of the overall semantic information. To address these challenges, we propose LayoutDM based on multi-scale diffusion, which employs the Parallel Sampling Module to enhance local precision and ensures global semantic coherence through the Semantic Coherence Module. Significantly, our approach generates within the visible space, progressively revealing more details and semantic information. Simultaneously, our method enhances layout part handling via parallel region-wise clip guidance, achieving strong zero-shot generation without direct training samples. Tests on COCO-stuff and VG datasets confirm that our approach achieves both fine-grained object generation and overall visual effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2282237274",
                    "name": "Mingzhe Yu"
                },
                {
                    "authorId": "2292654467",
                    "name": "Lei Wu"
                },
                {
                    "authorId": "2274200802",
                    "name": "Changshuo Wang"
                },
                {
                    "authorId": "2240864213",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "de74d99f5d22b957b213a934e42aedac2f900251",
            "title": "Cross-Training with Multi-View Knowledge Fusion for Heterogenous Federated Learning",
            "abstract": "Federated learning benefits from cross-training strategies, which enables models to train on data from distinct sources to improve the generalization capability. However, the data heterogeneity between sources may lead models to gradually forget previously acquired knowledge when undergoing cross-training to adapt to new tasks or data sources. We argue that integrating personalized and global knowledge to gather information from multiple perspectives could potentially improve performance. To achieve this goal, this paper presents a novel approach that enhances federated learning through a cross-training scheme incorporating multi-view information. Specifically, the proposed method, termed FedCT, includes three main modules, where the consistency-aware knowledge broadcasting module aims to optimize model assignment strategies, which enhances collaborative advantages between clients and achieves an efficient federated learning process. The multi-view knowledge-guided representation learning module leverages fused prototypical knowledge from both global and local views to enhance the preservation of local knowledge before and after model exchange, as well as to ensure consistency between local and global knowledge. The mixup-based feature augmentation module aggregates rich information to further increase the diversity of feature spaces, which enables the model to better discriminate complex samples. Extensive experiments were conducted on four datasets in terms of performance comparison, ablation study, in-depth analysis and case study. The results demonstrated that FedCT alleviates knowledge forgetting from both local and global views, which enables it outperform state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2286747782",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2240864213",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "2303974934",
                    "name": "Weihao He"
                },
                {
                    "authorId": "2303975653",
                    "name": "Ruohan Zhang"
                },
                {
                    "authorId": "2304090309",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2304291807",
                    "name": "Xin Qi"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "e9e18e5680279ef754771083ed5fd0e4882fcbf7",
            "title": "Negative Sampling in Recommendation: A Survey and Future Directions",
            "abstract": "Recommender systems aim to capture users' personalized preferences from the cast amount of user behaviors, making them pivotal in the era of information explosion. However, the presence of the dynamic preference, the\"information cocoons\", and the inherent feedback loops in recommendation make users interact with a limited number of items. Conventional recommendation algorithms typically focus on the positive historical behaviors, while neglecting the essential role of negative feedback in user interest understanding. As a promising but easy-to-ignored area, negative sampling is proficients in revealing the genuine negative aspect inherent in user behaviors, emerging as an inescapable procedure in recommendation. In this survey, we first discuss the role of negative sampling in recommendation and thoroughly analyze challenges that consistently impede its progress. Then, we conduct an extensive literature review on the existing negative sampling strategies in recommendation and classify them into five categories with their discrepant techniques. Finally, we detail the insights of the tailored negative sampling strategies in diverse recommendation scenarios and outline an overview of the prospective research directions toward which the community may engage and benefit.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152191425",
                    "name": "Haokai Ma"
                },
                {
                    "authorId": "2292022322",
                    "name": "Ruobing Xie"
                },
                {
                    "authorId": "2278235051",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "2320729117",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2312760581",
                    "name": "Xiaoyu Du"
                },
                {
                    "authorId": "2277239672",
                    "name": "Xingwu Sun"
                },
                {
                    "authorId": "2261082002",
                    "name": "Zhanhui Kang"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "1d2af52456308588c84a4e01e57e936442aa9fb3",
            "title": "Compositional Zero-Shot Artistic Font Synthesis",
            "abstract": "Recently, many researchers have made remarkable achievements in the field of artistic font synthesis, with impressive glyph style and effect style in the results. However, due to less exploration in style disentanglement, it is difficult for existing methods to envision a kind of unseen style (glyph-effect) compositions of artistic font, and thus can only learn the seen style compositions. To solve this problem, we propose a novel compositional zero-shot artistic font synthesis gan (CAFS-GAN), which allows the synthesis of unseen style compositions by exploring the visual independence and joint compatibility of encoding semantics between glyph and effect. Specifically, we propose two contrast-based style encoders to achieve style disentanglement due to glyph and effect intertwining in the image. Meanwhile, to preserve more glyph and effect detail, we propose a generator based on hierarchical dual styles AdaIN to reorganize content-styles representations from structure to texture gradually. Extensive experiments demonstrate the superiority of our model in generating high-quality artistic font images with unseen style compositions against other state-of-the-art methods. The source code and data is available at moonlight03.github.io/CAFS-GAN/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "47057383",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "50789878",
                    "name": "L. Wu"
                },
                {
                    "authorId": "115877739",
                    "name": "Changshuo Wang"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "2c39e3fba188a2529b8887612e77337514d63ffe",
            "title": "End-to-End Unsupervised Sketch to Image Generation",
            "abstract": "A freehand sketch is geometrically distorted and lacks colors, textures, and other visual details. This leads to challenges in freehand sketch-to-image generation tasks. Many existing works take a multistage strategy to generate images from sketches, and they depend on the shape of the input sketch badly and ignore the shape adjustment of the generated image. More importantly, the generated images are of low quality with distorted textures and colors, and shape deformation. In order to overcome the above challenges, we propose an end-to-end method to accomplish the freehand sketch-to-image task, and the proposed architecture is based on an unsupervised network. Our key insight is to propose a shape discriminator to provide shape constraints for the generated image. Besides, we introduce Image-Global Attention(IGA) and Focal Frequency Loss(FFL). IGA and FFL mainly focuses on the whole image and every patch of an image, respectively. We also extend a new dataset called NewGiraffe based on the giraffe class of SketchyCOCO, and our approach is validated on two datasets: Shoes and NewGiraffe. Through qualitative and quantitative results, we demonstrate our method\u2019s ability to generate realistic images from freehand sketches.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2216453553",
                    "name": "Xingming Lv"
                },
                {
                    "authorId": "50789878",
                    "name": "L. Wu"
                },
                {
                    "authorId": "2216710974",
                    "name": "Zhenwei Cheng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "385dd16d1859525e14020987e451acce1e5511d6",
            "title": "Cross-Modal Content Inference and Feature Enrichment for Cold-Start Recommendation",
            "abstract": "Multimedia recommendation aims to fuse the multi-modal information of items for feature enrichment to improve the recommendation performance. However, existing methods typically introduce multi-modal information based on collaborative information to improve the overall recommendation precision, while failing to explore its cold-start recommendation performance. Meanwhile, these above methods are only applicable when such multi-modal data is available. To address this problem, this paper proposes a recommendation framework, named Cross-modal Content Inference and Feature Enrichment Recommendation (CIERec), which exploits the multi-modal information to improve its cold-start recommendation performance. Specifically, CIERec first introduces image annotation as the privileged information to help guide the mapping of unified features from the visual space to the semantic space in the training phase. And then CIERec enriches the content representation with the fusion of collaborative, visual, and cross-modal inferred representations, so as to improve its cold-start recommendation performance. Experimental results on two real-world datasets show that the content representations learned by CIERec are able to achieve superior cold-start recommendation performance over existing visually-aware recommendation algorithms. More importantly, CIERec can consistently achieve significant improvements with different conventional visually-aware backbones, which verifies its universality and effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152191425",
                    "name": "Haokai Ma"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2143918656",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2183591290",
                    "name": "Yuze Zheng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        }
    ]
}