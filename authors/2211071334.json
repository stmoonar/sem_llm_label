{
    "authorId": "2211071334",
    "papers": [
        {
            "paperId": "4301175198235888abfff048d5a430c832d9403b",
            "title": "Unifying Visual and Semantic Feature Spaces with Diffusion Models for Enhanced Cross-Modal Alignment",
            "abstract": "Image classification models often demonstrate unstable performance in real-world applications due to variations in image information, driven by differing visual perspectives of subject objects and lighting discrepancies. To mitigate these challenges, existing studies commonly incorporate additional modal information matching the visual data to regularize the model's learning process, enabling the extraction of high-quality visual features from complex image regions. Specifically, in the realm of multimodal learning, cross-modal alignment is recognized as an effective strategy, harmonizing different modal information by learning a domain-consistent latent feature space for visual and semantic features. However, this approach may face limitations due to the heterogeneity between multimodal information, such as differences in feature distribution and structure. To address this issue, we introduce a Multimodal Alignment and Reconstruction Network (MARNet), designed to enhance the model's resistance to visual noise. Importantly, MARNet includes a cross-modal diffusion reconstruction module for smoothly and stably blending information across different domains. Experiments conducted on two benchmark datasets, Vireo-Food172 and Ingredient-101, demonstrate that MARNet effectively improves the quality of image information extracted by the model. It is a plug-and-play framework that can be rapidly integrated into various image classification frameworks, boosting model performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2183591290",
                    "name": "Yuze Zheng"
                },
                {
                    "authorId": "2313697725",
                    "name": "Zixuan Li"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2313586899",
                    "name": "Jinxing Liu"
                },
                {
                    "authorId": "2226670159",
                    "name": "Yuqing Wang"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2240864213",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "2fb1ae01a98fde6b360ef53b16a8e6812f3c2444",
            "title": "A Dual-branch Enhanced Multi-task Learning Network for Multimodal Sentiment Analysis",
            "abstract": "Multimodal sentiment analysis is a complex research problem. Firstly, current multimodal approaches fail to adequately consider the intricate multi-level correspondence between modalities and the unique contextual information within each modality; secondly, cross-modal fusion methods for inter-modal fusion somewhat weaken the mode-specific internal features, which is a limitation of the traditional single-branch model. To this end, we proposes a dual-branch enhanced multi-task learning network (DBEM), a new architecture that considers both the multiple dependencies of sequences and the heterogeneity of multimodal data, for better multimodal sentiment analysis. The global-local branch takes into account the intra-modal dependencies of different length time subsequences and aggregates global and local features to enrich the feature diversity. The cross-refine branch considers the difference in information density of different modalities and adopts coarse-to-fine fusion learning to model the inter-modal dependencies. Coarse-grained fusion achieves low-level feature reinforcement of audio and visual modalities, and fine-grained fusion improves the ability to integrate information complementarity between different levels of modalities. Finally, multi-task learning is carried out to improve the generalization and performance of the model based on the enhanced fusion features obtained from the dual-branch network. Compared with the single branch network (SBEM, variant of DBEM model) and SOTA methods, the experimental results on the two datasets CH-SIMS and CMU-MOSEI validate the effectiveness of the DBEM model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7936094",
                    "name": "Wenxiu Geng"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2219738952",
                    "name": "Yulong Bian"
                }
            ]
        },
        {
            "paperId": "385dd16d1859525e14020987e451acce1e5511d6",
            "title": "Cross-Modal Content Inference and Feature Enrichment for Cold-Start Recommendation",
            "abstract": "Multimedia recommendation aims to fuse the multi-modal information of items for feature enrichment to improve the recommendation performance. However, existing methods typically introduce multi-modal information based on collaborative information to improve the overall recommendation precision, while failing to explore its cold-start recommendation performance. Meanwhile, these above methods are only applicable when such multi-modal data is available. To address this problem, this paper proposes a recommendation framework, named Cross-modal Content Inference and Feature Enrichment Recommendation (CIERec), which exploits the multi-modal information to improve its cold-start recommendation performance. Specifically, CIERec first introduces image annotation as the privileged information to help guide the mapping of unified features from the visual space to the semantic space in the training phase. And then CIERec enriches the content representation with the fusion of collaborative, visual, and cross-modal inferred representations, so as to improve its cold-start recommendation performance. Experimental results on two real-world datasets show that the content representations learned by CIERec are able to achieve superior cold-start recommendation performance over existing visually-aware recommendation algorithms. More importantly, CIERec can consistently achieve significant improvements with different conventional visually-aware backbones, which verifies its universality and effectiveness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152191425",
                    "name": "Haokai Ma"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2143918656",
                    "name": "X. Dong"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2183591290",
                    "name": "Yuze Zheng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "7217fe2b9659fa01d180a135be88a759cfff5880",
            "title": "Multi-channel Attentive Weighting of Visual Frames for Multimodal Video Classification",
            "abstract": "Multimodal video classification aims to incorporate semantic information to regularize the visual representation learning of videos. Conventional methods typically focus on analyzing all information extracted from different modals rather than key information. However, they usually face the problem of handling the redundant video frames of little categorical information. To address this problem, this paper proposes a novel approach that employs multi-channel weighting of visual frames to mitigate the interference of redundant information. Specifically, the proposed algorithm, termed MCA-WF, includes two main modules, where the multi-channel attentive weighting of video frames (McAW) module performs the multi-granularity and multi-channel frame weighting mechanism based on visual self-attention, contrastive attention and cross-modal attention constraints to filter visual noise and redundant information. The visual frame selection (VFS) module explores the combination of multi-channel attention mechanisms to select the key visual information in the video. Experiments were conducted on MSR-VTT and ActivityNet Captions datasets in terms of performance comparison, ablation study, in-depth analysis, and case studies. The results verified that MCA-WF can notice the key information in the classification and effectively improve the ability of information complementation and integration between modals, which leads to better performance than the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2226670159",
                    "name": "Yuqing Wang"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "48211506",
                    "name": "Jinxing Liu"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "88b0df915085ff8c115393fe445b1ce8464194b1",
            "title": "Class-level Structural Relation Modeling and Smoothing for Visual Representation Learning",
            "abstract": "Representation learning for images has been advanced by recent progress in more complex neural models such as the Vision Transformers and new learning theories such as the structural causal models. However, these models mainly rely on the classification loss to implicitly regularize the class-level data distributions, and they may face difficulties when handling classes with diverse visual patterns. We argue that the incorporation of the structural information between data samples may improve this situation. To achieve this goal, this paper presents a framework termed Class-level Structural Relation Modeling and Smoothing for Visual Representation Learning (CSRMS), which includes the Class-level Relation Modelling, Class-aware Graph Sampling, and Relational Graph-Guided Representation Learning modules to model a relational graph of the entire dataset and perform class-aware smoothing and regularization operations to alleviate the issue of intra-class visual diversity and inter-class similarity. Specifically, the Class-level Relation Modelling module uses a clustering algorithm to learn the data distributions in the feature space and identify three types of class-level sample relations for the training set; Class-aware Graph Sampling module extends typical training batch construction process with three strategies to sample dataset-level sub-graphs; and Relational Graph-Guided Representation Learning module employs a graph convolution network with knowledge-guided smoothing operations to ease the projection from different visual patterns to the same class. Experiments demonstrate the effectiveness of structured knowledge modelling for enhanced representation learning and show that CSRMS can be incorporated with any state-of-the-art visual representation learning models for performance gains. The source codes and demos have been released at https://github.com/czt117/CSRMS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262218236",
                    "name": "Zitan Chen"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2262088191",
                    "name": "Xiao Cao"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2240864213",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "ce28e4f498d6e74952f5f8a3d3e110ee2548b6ae",
            "title": "Class-aware Convolution and Attentive Aggregation for Image Classification",
            "abstract": "Deep learning has been proven to be effective in image classification tasks. However, existing methods may face difficulties in distinguishing complex images due to the distraction caused by diverse image content. To overcome this challenge, we propose a class-aware convolution and attentive aggregation (CA-Net) framework that improves the effectiveness of representation learning and reduces the influence of irrelevant background. CA-Net includes three main modules: the discrete representation learning (DRL) module that uses a group learning method to learn discriminative representations, the class-aware score of discrete representation (CSDR) module that infers class-aware scores to generate weights for representation learners, and the class-aware representation fusion module(CRF) that aggregates class-aware representations using the class-aware scores as a guide. Our experimental results on three benchmarking datasets show that CA-Net improves the performance of state-of-the-art backbones and enhances feature extraction robustness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2262218236",
                    "name": "Zitan Chen"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2226670159",
                    "name": "Yuqing Wang"
                },
                {
                    "authorId": "2240864213",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "f738797a4a3c5907f11e35e914a192577dc06888",
            "title": "A Multi-View Co-Learning Method for Multimodal Sentiment Analysis",
            "abstract": "Existing works on multimodal sentiment analysis have focused on learning more discriminative unimodal sentiment information or improving multimodal fusion methods to enhance modal complementarity. However, practical results of these methods have been limited owing to the problems of insufficient intra-modal representation and inter-modal noise. To alleviate this problem, we propose a multi-view co-learning method (MVATF) for video sentiment analysis. First, we propose a multi-view features extraction module to capture more perspectives from a single modality. Second, we propose a two-level fusion sentiment enhancement strategy that uses hierarchical attentive learning fusion and a multi-task learning fusion module to achieve co-learning to effectively filter inter-modal noise for better multimodal sentiment fusion features. Experimental results on the CH-SIMS, CMU-MOSI and MOSEI datasets show that the proposed method outperforms the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "7936094",
                    "name": "Wenxiu Geng"
                },
                {
                    "authorId": "2219738952",
                    "name": "Yulong Bian"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                }
            ]
        },
        {
            "paperId": "394c0fe4d4ff4ce3c7e9ca913e7a8abe30b37299",
            "title": "Meta-Causal Feature Learning for Out-of-Distribution Generalization",
            "abstract": "Causal inference has become a powerful tool to handle the out-of-distribution (OOD) generalization problem, which aims to extract the invariant features. However, conventional methods apply causal learners from multiple data splits, which may incur biased representation learning from imbalanced data distributions and difficulty in invariant feature learning from heterogeneous sources. To address these issues, this paper presents a balanced meta-causal learner (BMCL), which includes a balanced task generation module (BTG) and a meta-causal feature learning module (MCFL). Specifically, the BTG module learns to generate balanced subsets by a self-learned partitioning algorithm with constraints on the proportions of sample classes and contexts. The MCFL module trains a meta-learner adapted to different distributions. Experiments conducted on NICO++ dataset verified that BMCL effectively identifies the class-invariant visual regions for classification and may serve as a general framework to improve the performance of the state-of-the-art methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Yuqing Wang"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2182292307",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "1492114275",
                    "name": "Jingyu Li"
                },
                {
                    "authorId": "2173892322",
                    "name": "Xuelong Li"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        },
        {
            "paperId": "43569403fade3cdbed6ed79c85f88f71233e15ae",
            "title": "Unsupervised Contrastive Masking for Visual Haze Classification",
            "abstract": "Haze classification has gained much attention recently as a cost-effective solution for air quality monitoring. Different from conventional image classification tasks, it requires the classifier to capture the haze patterns of different severity degrees. Existing efforts typically focus on the extraction of effective haze features, such as the dark channel and deep features. However, it is observed that the light-haze images are often mis-classified due to the presence of diverse background scenes. To address this issue, this paper presents an unsupervised contrastive masking (UCM) algorithm to segment the haze regions without any supervision, and develops a dual-channel model-agnostic framework, termed magnifier neural network (MagNet), to effectively use the segmented haze regions to enhance the learning of haze features by conventional deep learning models. Specifically, MagNet employs the haze regions to provide the pixel- and feature-level visual information via three strategies, including Input Augmentation, Network Constraint, and Feature Enhancement, which work as a soft-attention regularizer to alleviates the trade-off between capturing the global scene information and the local information in the haze regions. Experiments were conducted on two datasets in terms of performance comparison, parameter estimation, ablation studies, and case studies, and the results verified that UCM can accurately and rapidly segment the haze regions, and the proposed three strategies of MagNet consistently improve the performance of the state-of-the-art deep learning backbones.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1492114275",
                    "name": "Jingyu Li"
                },
                {
                    "authorId": "2152191425",
                    "name": "Haokai Ma"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2072540723",
                    "name": "Zhuang Qi"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                }
            ]
        },
        {
            "paperId": "b02d4af3960330d206c75186cc849b61b4f93781",
            "title": "A Visually-Aware Food Analysis System for Diet Management",
            "abstract": "This demo illustrates a visually-aware food analysis (VAFA) system for socially-engaged diet management. VAFA is able to receive multimedia inputs, such as the images of food with/without a description to record a user\u2019s daily diet. Such information will be passed to AI algorithms for food classification, ingredient recognition, and nutrition analysis, to produce a nutrition report for the user. Moreover, VAFA profiles the users\u2019 eating habits to make personalized recipe recommendation and identify the social communities with similar eating preferences. VAFA is empowered by state-of-the-art AI algorithms and a large-scale dataset with 300K users, 400K recipes, and over 10M user-recipe interactions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2183601740",
                    "name": "Hang Wu"
                },
                {
                    "authorId": "2145307865",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "2184029053",
                    "name": "Xuelong Li"
                },
                {
                    "authorId": "2152191425",
                    "name": "Haokai Ma"
                },
                {
                    "authorId": "2183591290",
                    "name": "Yuze Zheng"
                },
                {
                    "authorId": "2211071334",
                    "name": "Xiangxian Li"
                },
                {
                    "authorId": "2150591081",
                    "name": "Xiangxu Meng"
                },
                {
                    "authorId": "2113609032",
                    "name": "Lei Meng"
                }
            ]
        }
    ]
}