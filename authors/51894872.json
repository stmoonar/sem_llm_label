{
    "authorId": "51894872",
    "papers": [
        {
            "paperId": "767dcc48c7ad2c943f3c1a25c46b873e7b8b3bc8",
            "title": "Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages",
            "abstract": "The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and low-resource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, \u201chelp\u201d from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should notlimit NLP to a small fraction of the world\u2019s languages and instead strive to support as many languages as possible to bring the benefits of NLP technology to all languages and cultures. Code, data and models are available at https://github.com/cisnlp/Glot500.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51894641",
                    "name": "Ayyoob Imani"
                },
                {
                    "authorId": "152178958",
                    "name": "Peiqin Lin"
                },
                {
                    "authorId": "98623604",
                    "name": "Amir Hossein Kargaran"
                },
                {
                    "authorId": "51894872",
                    "name": "Silvia Severini"
                },
                {
                    "authorId": "33014152",
                    "name": "Masoud Jalili Sabet"
                },
                {
                    "authorId": "9529535",
                    "name": "Nora Kassner"
                },
                {
                    "authorId": "2217563675",
                    "name": "Chunlan Ma"
                },
                {
                    "authorId": "40360495",
                    "name": "Helmut Schmid"
                },
                {
                    "authorId": "1400227478",
                    "name": "Andr\u00e9 F. T. Martins"
                },
                {
                    "authorId": "1846431",
                    "name": "Fran\u00e7ois Yvon"
                },
                {
                    "authorId": "144418438",
                    "name": "Hinrich Sch\u00fctze"
                }
            ]
        },
        {
            "paperId": "f65e74350c3ee16e1ec422c0844ab69d358ed5d0",
            "title": "Multilingual Word Embeddings for Low-Resource Languages using Anchors and a Chain of Related Languages",
            "abstract": "Very low-resource languages, having only a few million tokens worth of data, are not well-supported by multilingual NLP approaches due to poor quality cross-lingual word representations. Recent work showed that good cross-lingual performance can be achieved if a source language is related to the low-resource target language. However, not all language pairs are related. In this paper, we propose to build multilingual word embeddings (MWEs) via a novel language chain-based approach, that incorporates intermediate related languages to bridge the gap between the distant source and target. We build MWEs one language at a time by starting from the resource rich source and sequentially adding each language in the chain till we reach the target. We extend a semi-joint bilingual approach to multiple languages in order to eliminate the main weakness of previous works, i.e., independently trained monolingual embeddings, by anchoring the target language around the multilingual space. We evaluate our method on bilingual lexicon induction for 4 language families, involving 4 very low-resource (<5M tokens) and 4 moderately low-resource (<50M) target languages, showing improved performance in both categories. Additionally, our analysis reveals the importance of good quality embeddings for intermediate languages as well as the importance of leveraging anchor points from all languages in the multilingual space.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2113189",
                    "name": "Viktor Hangya"
                },
                {
                    "authorId": "51894872",
                    "name": "Silvia Severini"
                },
                {
                    "authorId": "2267483979",
                    "name": "Radoslav Ralev"
                },
                {
                    "authorId": "2266753399",
                    "name": "Alexander Fraser"
                },
                {
                    "authorId": "144418438",
                    "name": "Hinrich Sch\u00fctze"
                }
            ]
        },
        {
            "paperId": "0e1e076323a0ed366ad34dbfb75b4ab6fa2ad100",
            "title": "Don\u2019t Forget Cheap Training Signals Before Building Unsupervised Bilingual Word Embeddings",
            "abstract": "Bilingual Word Embeddings (BWEs) are one of the cornerstones of cross-lingual transfer of NLP models. They can be built using only monolingual corpora without supervision leading to numerous works focusing on unsupervised BWEs. However, most of the current approaches to build unsupervised BWEs do not compare their results with methods based on easy-to-access cross-lingual signals. In this paper, we argue that such signals should always be considered when developing unsupervised BWE methods. The two approaches we find most effective are: 1) using identical words as seed lexicons (which unsupervised approaches incorrectly assume are not available for orthographically distinct language pairs) and 2) combining such lexicons with pairs extracted by matching romanized versions of words with an edit distance threshold. We experiment on thirteen non-Latin languages (and English) and show that such cheap signals work well and that they outperform using more complex unsupervised methods on distant language pairs such as Chinese, Japanese, Kannada, Tamil, and Thai. In addition, they are even competitive with the use of high-quality lexicons in supervised approaches. Our results show that these training signals should not be neglected when building BWEs, even for distant languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51894872",
                    "name": "Silvia Severini"
                },
                {
                    "authorId": "2113189",
                    "name": "Viktor Hangya"
                },
                {
                    "authorId": "33014152",
                    "name": "Masoud Jalili Sabet"
                },
                {
                    "authorId": "2277248",
                    "name": "Alexander M. Fraser"
                },
                {
                    "authorId": "144418438",
                    "name": "Hinrich Sch\u00fctze"
                }
            ]
        },
        {
            "paperId": "5ddd6dab271e047da0f4f60840e0d8287dceff07",
            "title": "Graph-Based Multilingual Label Propagation for Low-Resource Part-of-Speech Tagging",
            "abstract": "Part-of-Speech (POS) tagging is an important component of the NLP pipeline, but many low-resource languages lack labeled data for training. An established method for training a POS tagger in such a scenario is to create a labeled training set by transferring from high-resource languages. In this paper, we propose a novel method for transferring labels from multiple high-resource source to low-resource target languages. We formalize POS tag projection as graph-based label propagation. Given translations of a sentence in multiple languages, we create a graph with words as nodes and alignment links as edges by aligning words for all language pairs. We then propagate node labels from source to target using a Graph Neural Network augmented with transformer layers. We show that our propagation creates training sets that allow us to train POS taggers for a diverse set of languages. When combined with enhanced contextualized embeddings, our method achieves a new state-of-the-art for unsupervised POS tagging of low-resource languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51894641",
                    "name": "Ayyoob Imani"
                },
                {
                    "authorId": "51894872",
                    "name": "Silvia Severini"
                },
                {
                    "authorId": "33014152",
                    "name": "Masoud Jalili Sabet"
                },
                {
                    "authorId": "1846431",
                    "name": "Fran\u00e7ois Yvon"
                },
                {
                    "authorId": "144418438",
                    "name": "Hinrich Sch\u00fctze"
                }
            ]
        },
        {
            "paperId": "7c80547c8ac8e461c67ed99e8c74a26d5ff6691d",
            "title": "Towards a Broad Coverage Named Entity Resource: A Data-Efficient Approach for Many Diverse Languages",
            "abstract": "Parallel corpora are ideal for extracting a multilingual named entity (MNE) resource, i.e., a dataset of names translated into multiple languages. Prior work on extracting MNE datasets from parallel corpora required resources such as large monolingual corpora or word aligners that are unavailable or perform poorly for underresourced languages. We present CLC-BN, a new method for creating an MNE resource, and apply it to the Parallel Bible Corpus, a corpus of more than 1000 languages. CLC-BN learns a neural transliteration model from parallel-corpus statistics, without requiring any other bilingual resources, word aligners, or seed data. Experimental results show that CLC-BN clearly outperforms prior work. We release an MNE resource for 1340 languages and demonstrate its effectiveness in two downstream tasks: knowledge graph augmentation and bilingual lexicon induction.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51894872",
                    "name": "Silvia Severini"
                },
                {
                    "authorId": "51894641",
                    "name": "Ayyoob Imani"
                },
                {
                    "authorId": "35501453",
                    "name": "Philipp Dufter"
                },
                {
                    "authorId": "144418438",
                    "name": "Hinrich Sch\u00fctze"
                }
            ]
        },
        {
            "paperId": "f5d4609bfec6d33d2d267e5d5b40c7228cb46560",
            "title": "SilverAlign: MT-Based Silver Data Algorithm for Evaluating Word Alignment",
            "abstract": "Word alignments are essential for a variety of NLP tasks. Therefore, choosing the best approaches for their creation is crucial. However, the scarce availability of gold evaluation data makes the choice difficult. We propose SilverAlign, a new method to automatically create silver data for the evaluation of word aligners by exploiting machine translation and minimal pairs. We show that performance on our silver data correlates well with gold benchmarks for 9 language pairs, making our approach a valid resource for evaluation of different languages and domains when gold data is not available. This addresses the important scenario of missing gold data alignments for low-resource languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1999179692",
                    "name": "Abdullatif K\u00f6ksal"
                },
                {
                    "authorId": "51894872",
                    "name": "Silvia Severini"
                },
                {
                    "authorId": "144418438",
                    "name": "Hinrich Sch\u00fctze"
                }
            ]
        },
        {
            "paperId": "5bfb0cc16b871c75e32a6a9d54dd7db225260e04",
            "title": "CodeTrans: Towards Cracking the Language of Silicone's Code Through Self-Supervised Deep Learning and High Performance Computing",
            "abstract": "Currently, a growing number of mature natural language processing applications make people's life more convenient. Such applications are built by source code - the language in software engineering. However, the applications for understanding source code language to ease the software engineering process are under-researched. Simultaneously, the transformer model, especially its combination with transfer learning, has been proven to be a powerful technique for natural language processing tasks. These breakthroughs point out a promising direction for process source code and crack software engineering tasks. This paper describes CodeTrans - an encoder-decoder transformer model for tasks in the software engineering domain, that explores the effectiveness of encoder-decoder transformer models for six software engineering tasks, including thirteen sub-tasks. Moreover, we have investigated the effect of different training strategies, including single-task learning, transfer learning, multi-task learning, and multi-task learning with fine-tuning. CodeTrans outperforms the state-of-the-art models on all the tasks. To expedite future works in the software engineering domain, we have published our pre-trained models of CodeTrans. https://github.com/agemagician/CodeTrans",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145951466",
                    "name": "Ahmed Elnaggar"
                },
                {
                    "authorId": "2113832157",
                    "name": "Wei Ding"
                },
                {
                    "authorId": "145024664",
                    "name": "Llion Jones"
                },
                {
                    "authorId": "115481520",
                    "name": "Tom Gibbs"
                },
                {
                    "authorId": "2105629186",
                    "name": "Tamas B. Feh\u00e9r"
                },
                {
                    "authorId": "2064464398",
                    "name": "Christoph Angerer"
                },
                {
                    "authorId": "51894872",
                    "name": "Silvia Severini"
                },
                {
                    "authorId": "2522197",
                    "name": "F. Matthes"
                },
                {
                    "authorId": "49967661",
                    "name": "B. Rost"
                }
            ]
        },
        {
            "paperId": "1af470e01157887b2573d5691b6546968901f107",
            "title": "LMU Bilingual Dictionary Induction System with Word Surface Similarity Scores for BUCC 2020",
            "abstract": "The task of Bilingual Dictionary Induction (BDI) consists of generating translations for source language words which is important in the framework of machine translation (MT). The aim of the BUCC 2020 shared task is to perform BDI on various language pairs using comparable corpora. In this paper, we present our approach to the task of English-German and English-Russian language pairs. Our system relies on Bilingual Word Embeddings (BWEs) which are often used for BDI when only a small seed lexicon is available making them particularly effective in a low-resource setting. On the other hand, they perform well on high frequency words only. In order to improve the performance on rare words as well, we combine BWE based word similarity with word surface similarity methods, such as orthography In addition to the often used top-n translation method, we experiment with a margin based approach aiming for dynamic number of translations for each source word. We participate in both the open and closed tracks of the shared task and we show improved results of our method compared to simple vector similarity based approaches. Our system was ranked in the top-3 teams and achieved the best results for English-Russian.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51894872",
                    "name": "Silvia Severini"
                },
                {
                    "authorId": "2113189",
                    "name": "Viktor Hangya"
                },
                {
                    "authorId": "2277248",
                    "name": "Alexander M. Fraser"
                },
                {
                    "authorId": "144418438",
                    "name": "Hinrich Sch\u00fctze"
                }
            ]
        },
        {
            "paperId": "ceb1531d4aef57ad26a5d4989613613723ecd9d3",
            "title": "Combining Word Embeddings with Bilingual Orthography Embeddings for Bilingual Dictionary Induction",
            "abstract": "Bilingual dictionary induction (BDI) is the task of accurately translating words to the target language. It is of great importance in many low-resource scenarios where cross-lingual training data is not available. To perform BDI, bilingual word embeddings (BWEs) are often used due to their low bilingual training signal requirements. They achieve high performance, but problematic cases still remain, such as the translation of rare words or named entities, which often need to be transliterated. In this paper, we enrich BWE-based BDI with transliteration information by using Bilingual Orthography Embeddings (BOEs). BOEs represent source and target language transliteration word pairs with similar vectors. A key problem in our BDI setup is to decide which information source \u2013 BWEs (or semantics) vs. BOEs (or orthography) \u2013 is more reliable for a particular word pair. We propose a novel classification-based BDI system that uses BWEs, BOEs and a number of other features to make this decision. We test our system on English-Russian BDI and show improved performance. In addition, we show the effectiveness of our BOEs by successfully using them for transliteration mining based on cosine similarity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "51894872",
                    "name": "Silvia Severini"
                },
                {
                    "authorId": "2113189",
                    "name": "Viktor Hangya"
                },
                {
                    "authorId": "2277248",
                    "name": "Alexander M. Fraser"
                },
                {
                    "authorId": "144418438",
                    "name": "Hinrich Sch\u00fctze"
                }
            ]
        },
        {
            "paperId": "af78c45ee23f22d9b9dbfb358e9626133c8c5789",
            "title": "A Comparative Study of Models for Answer Sentence Selection",
            "abstract": "Answer Sentence Selection is one of the steps typically involved in Question Answering. Question Answering is considered a hard task for natural language processing systems, since full solutions would require both natural language understanding and inference abilities. In this paper, we explore how the state of the art in answer selection has improved recently, comparing two of the best proposed models for tackling the problem: the Crossattentive Convolutional Network and the BERT model. The experiments are carried out on two datasets, WikiQA and SelQA, both created for and used in open-domain question answering challenges. We also report on cross domain experiments with the two datasets.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48890086",
                    "name": "Federico Rossetto"
                },
                {
                    "authorId": "2042289369",
                    "name": "Alessio Gravina"
                },
                {
                    "authorId": "51894872",
                    "name": "Silvia Severini"
                },
                {
                    "authorId": "49666627",
                    "name": "Giuseppe Attardi"
                }
            ]
        }
    ]
}