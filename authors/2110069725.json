{
    "authorId": "2110069725",
    "papers": [
        {
            "paperId": "40f0390067b1db8fb391a4c8f280e7b302073f4d",
            "title": "Unraveling the Impact of Heterophilic Structures on Graph Positive-Unlabeled Learning",
            "abstract": "While Positive-Unlabeled (PU) learning is vital in many real-world scenarios, its application to graph data still remains under-explored. We unveil that a critical challenge for PU learning on graph lies on the edge heterophily, which directly violates the irreducibility assumption for Class-Prior Estimation (class prior is essential for building PU learning algorithms) and degenerates the latent label inference on unlabeled nodes during classifier training. In response to this challenge, we introduce a new method, named Graph PU Learning with Label Propagation Loss (GPL). Specifically, GPL considers learning from PU nodes along with an intermediate heterophily reduction, which helps mitigate the negative impact of the heterophilic structure. We formulate this procedure as a bilevel optimization that reduces heterophily in the inner loop and efficiently learns a classifier in the outer loop. Extensive experiments across a variety of datasets have shown that GPL significantly outperforms baseline methods, confirming its effectiveness and superiority.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2290064536",
                    "name": "Yuhao Wu"
                },
                {
                    "authorId": "2110069725",
                    "name": "Jiangchao Yao"
                },
                {
                    "authorId": "2257285217",
                    "name": "Bo Han"
                },
                {
                    "authorId": "2304113884",
                    "name": "Lina Yao"
                },
                {
                    "authorId": "2244770736",
                    "name": "Tongliang Liu"
                }
            ]
        },
        {
            "paperId": "47bd285cca2c6491391f49af4d979cb513a028ac",
            "title": "Decoupling the Class Label and the Target Concept in Machine Unlearning",
            "abstract": "Machine unlearning as an emerging research topic for data regulations, aims to adjust a trained model to approximate a retrained one that excludes a portion of training data. Previous studies showed that class-wise unlearning is successful in forgetting the knowledge of a target class, through gradient ascent on the forgetting data or fine-tuning with the remaining data. However, while these methods are useful, they are insufficient as the class label and the target concept are often considered to coincide. In this work, we decouple them by considering the label domain mismatch and investigate three problems beyond the conventional all matched forgetting, e.g., target mismatch, model mismatch, and data mismatch forgetting. We systematically analyze the new challenges in restrictively forgetting the target concept and also reveal crucial forgetting dynamics in the representation level to realize these tasks. Based on that, we propose a general framework, namely, TARget-aware Forgetting (TARF). It enables the additional tasks to actively forget the target concept while maintaining the rest part, by simultaneously conducting annealed gradient ascent on the forgetting data and selected gradient descent on the hard-to-affect remaining data. Empirically, various experiments under the newly introduced settings are conducted to demonstrate the effectiveness of our TARF.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143475848",
                    "name": "Jianing Zhu"
                },
                {
                    "authorId": "2301791087",
                    "name": "Bo Han"
                },
                {
                    "authorId": "2110069725",
                    "name": "Jiangchao Yao"
                },
                {
                    "authorId": "2157263249",
                    "name": "Jianliang Xu"
                },
                {
                    "authorId": "47537639",
                    "name": "Gang Niu"
                },
                {
                    "authorId": "67154907",
                    "name": "Masashi Sugiyama"
                }
            ]
        },
        {
            "paperId": "71f38b06243b35cfb1602d6ceb7079ce108e6819",
            "title": "Mitigating Label Noise on Graph via Topological Sample Selection",
            "abstract": "Despite the success of the carefully-annotated benchmarks, the effectiveness of existing graph neural networks (GNNs) can be considerably impaired in practice when the real-world graph data is noisily labeled. Previous explorations in sample selection have been demonstrated as an effective way for robust learning with noisy labels, however, the conventional studies focus on i.i.d data, and when moving to non-iid graph data and GNNs, two notable challenges remain: (1) nodes located near topological class boundaries are very informative for classification but cannot be successfully distinguished by the heuristic sample selection. (2) there is no available measure that considers the graph topological information to promote sample selection in a graph. To address this dilemma, we propose a $\\textit{Topological Sample Selection}$ (TSS) method that boosts the informative sample selection process in a graph by utilising topological information. We theoretically prove that our procedure minimizes an upper bound of the expected risk under target clean distribution, and experimentally show the superiority of our method compared with state-of-the-art baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2290064536",
                    "name": "Yuhao Wu"
                },
                {
                    "authorId": "2110069725",
                    "name": "Jiangchao Yao"
                },
                {
                    "authorId": "2077454998",
                    "name": "Xiaobo Xia"
                },
                {
                    "authorId": "2249841076",
                    "name": "Jun Yu"
                },
                {
                    "authorId": "2213453651",
                    "name": "Ruxing Wang"
                },
                {
                    "authorId": "2257285276",
                    "name": "Bo Han"
                },
                {
                    "authorId": "2244770736",
                    "name": "Tongliang Liu"
                }
            ]
        },
        {
            "paperId": "b9b879f12bdda97e2e1af38cd88966fe7ede8697",
            "title": "Knowledge-Enhanced Facial Expression Recognition with Emotional-to-Neutral Transformation",
            "abstract": "Existing facial expression recognition (FER) methods typically fine-tune a pre-trained visual encoder using discrete labels. However, this form of supervision limits to specify the emotional concept of different facial expressions. In this paper, we observe that the rich knowledge in text embeddings, generated by vision-language models, is a promising alternative for learning discriminative facial expression representations. Inspired by this, we propose a novel knowledge-enhanced FER method with an emotional-to-neutral transformation. Specifically, we formulate the FER problem as a process to match the similarity between a facial expression representation and text embeddings. Then, we transform the facial expression representation to a neutral representation by simulating the difference in text embeddings from textual facial expression to textual neutral. Finally, a self-contrast objective is introduced to pull the facial expression representation closer to the textual facial expression, while pushing it farther from the neutral representation. We conduct evaluation with diverse pre-trained visual encoders including ResNet-18 and Swin-T on four challenging facial expression datasets. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art FER methods. The code will be publicly available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2118386010",
                    "name": "Hangyu Li"
                },
                {
                    "authorId": "2321246120",
                    "name": "Yihan Xu"
                },
                {
                    "authorId": "2110069725",
                    "name": "Jiangchao Yao"
                },
                {
                    "authorId": "2284094963",
                    "name": "Nan Wang"
                },
                {
                    "authorId": "2239061108",
                    "name": "Xinbo Gao"
                },
                {
                    "authorId": "2321694991",
                    "name": "Bo Han"
                }
            ]
        },
        {
            "paperId": "c52772edbbbc408fe864716a74b08e4e076c0966",
            "title": "Causal Distillation for Alleviating Performance Heterogeneity in Recommender Systems",
            "abstract": "Recommendation performance usually exhibits a long-tail distribution over users \u2014 a small portion of head users enjoy much more accurate recommendation services than the others. We reveal two sources of this performance heterogeneity problem: the uneven distribution of historical interactions (a natural source); and the biased training of recommender models (a model source). As addressing this problem cannot sacrifice the overall performance, a wise choice is to eliminate the model bias while maintaining the natural heterogeneity. The key to debiased training lies in eliminating the effect of confounders that influence both the user's historical behaviors and the next behavior. The emerging causal recommendation methods achieve this by modeling the causal effect between user behaviors, however potentially neglect unobserved confounders (e.g., friend suggestions) that are hard to measure in practice. To address unobserved confounders, we resort to the front-door adjustment (FDA) in causal theory and propose a causal multi-teacher distillation framework (CausalD). FDA requires proper mediators in order to estimate the causal effects of historical behaviors on the next behavior. To achieve this, we equip CausalD with multiple heterogeneous recommendation models to model the mediator distribution. Then, the causal effect estimated by FDA is the expectation of recommendation prediction over the mediator distribution and the prior distribution of historical behaviors, which is technically achieved by multi-teacher ensemble. To pursue efficient inference, CausalD further distills multiple teachers into one student model to directly infer the causal effect for making recommendations. We instantiate CausalD on two representative models, DeepFM and DIN, and conduct extensive experiments on three real-world datasets, which validate the superiority of CausalD over state-of-the-art methods. Through in-depth analysis, we find that CausalD largely improves the performance of tail users, reduces the performance heterogeneity, and enhances the overall performance.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1739188006",
                    "name": "Shengyu Zhang"
                },
                {
                    "authorId": "2142708915",
                    "name": "Ziqi Jiang"
                },
                {
                    "authorId": "2110069725",
                    "name": "Jiangchao Yao"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2315590980",
                    "name": "Kun Kuang"
                },
                {
                    "authorId": "2187385241",
                    "name": "Zhou Zhao"
                },
                {
                    "authorId": "2222777391",
                    "name": "Shuo Li"
                },
                {
                    "authorId": "2145952806",
                    "name": "Hongxia Yang"
                },
                {
                    "authorId": "143779329",
                    "name": "Tat-seng Chua"
                },
                {
                    "authorId": "2110922423",
                    "name": "Fei Wu"
                }
            ]
        },
        {
            "paperId": "01210dddf3d2e449fcd69592ed2d78cf419d7361",
            "title": "Neural Atoms: Propagating Long-range Interaction in Molecular Graphs through Efficient Communication Channel",
            "abstract": "Graph Neural Networks (GNNs) have been widely adopted for drug discovery with molecular graphs. Nevertheless, current GNNs mainly excel in leveraging short-range interactions (SRI) but struggle to capture long-range interactions (LRI), both of which are crucial for determining molecular properties. To tackle this issue, we propose a method to abstract the collective information of atomic groups into a few $\\textit{Neural Atoms}$ by implicitly projecting the atoms of a molecular. Specifically, we explicitly exchange the information among neural atoms and project them back to the atoms' representations as an enhancement. With this mechanism, neural atoms establish the communication channels among distant nodes, effectively reducing the interaction scope of arbitrary node pairs into a single hop. To provide an inspection of our method from a physical perspective, we reveal its connection to the traditional LRI calculation method, Ewald Summation. The Neural Atom can enhance GNNs to capture LRI by approximating the potential LRI of the molecular. We conduct extensive experiments on four long-range graph benchmarks, covering graph-level and link-level tasks on molecular graphs. We achieve up to a 27.32% and 38.27% improvement in the 2D and 3D scenarios, respectively. Empirically, our method can be equipped with an arbitrary GNN to help capture LRI. Code and datasets are publicly available in https://github.com/tmlr-group/NeuralAtom.",
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "authors": [
                {
                    "authorId": "2144461756",
                    "name": "Xuan Li"
                },
                {
                    "authorId": "1768392566",
                    "name": "Zhanke Zhou"
                },
                {
                    "authorId": "2110069725",
                    "name": "Jiangchao Yao"
                },
                {
                    "authorId": "2264223036",
                    "name": "Yu Rong"
                },
                {
                    "authorId": "2265222808",
                    "name": "Lu Zhang"
                },
                {
                    "authorId": "2261906733",
                    "name": "Bo Han"
                }
            ]
        },
        {
            "paperId": "25ffa7c814856551843025632f092239152aa43e",
            "title": "Combating Exacerbated Heterogeneity for Robust Models in Federated Learning",
            "abstract": "Privacy and security concerns in real-world applications have led to the development of adversarially robust federated models. However, the straightforward combination between adversarial training and federated learning in one framework can lead to the undesired robustness deterioration. We discover that the attribution behind this phenomenon is that the generated adversarial data could exacerbate the data heterogeneity among local clients, making the wrapped federated learning perform poorly. To deal with this problem, we propose a novel framework called Slack Federated Adversarial Training (SFAT), assigning the client-wise slack during aggregation to combat the intensified heterogeneity. Theoretically, we analyze the convergence of the proposed method to properly relax the objective when combining federated learning and adversarial training. Experimentally, we verify the rationality and effectiveness of SFAT on various benchmarked and real-world datasets with different adversarial training and federated optimization methods. The code is publicly available at https://github.com/ZFancy/SFAT.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143475848",
                    "name": "Jianing Zhu"
                },
                {
                    "authorId": "2110069725",
                    "name": "Jiangchao Yao"
                },
                {
                    "authorId": "121698214",
                    "name": "Tongliang Liu"
                },
                {
                    "authorId": "3259992",
                    "name": "Quanming Yao"
                },
                {
                    "authorId": "2157263249",
                    "name": "Jianliang Xu"
                },
                {
                    "authorId": "2087238859",
                    "name": "Bo Han"
                }
            ]
        },
        {
            "paperId": "62446b08b2347e409ccf6e83e493f46351ce4b4f",
            "title": "Long-Range Neural Atom Learning for Molecular Graphs",
            "abstract": "Graph Neural Networks (GNNs) have been widely adopted for drug discovery with molecular graphs. Nevertheless, current GNNs are mainly good at leveraging short-range interactions (SRI) but struggle to capture long-range interactions (LRI), both of which are crucial for determining molecular properties. To tackle this issue, we propose a method that implicitly projects all original atoms into a few Neural Atoms , which abstracts the collective information of atomic groups within a molecule. Specifically, we explicitly exchange the information among neural atoms and project them back to the atoms\u2019 representations as an enhancement. With this mechanism, neural atoms establish the communication channels among distant nodes, effectively reducing the interaction scope of arbitrary node pairs into a single hop. To provide an inspection of our method from a physical perspective, we reveal its connection with the traditional LRI calculation method, Ewald Summation. We conduct extensive experiments on three long-range graph benchmarks, covering both graph-level and link-level tasks on molecular graphs. We empirically justify that our method can be equipped with an arbitrary GNN and help to capture LRI.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2144461756",
                    "name": "Xuan Li"
                },
                {
                    "authorId": "1768392566",
                    "name": "Zhanke Zhou"
                },
                {
                    "authorId": "2110069725",
                    "name": "Jiangchao Yao"
                },
                {
                    "authorId": "2264223036",
                    "name": "Yu Rong"
                },
                {
                    "authorId": "2265222808",
                    "name": "Lu Zhang"
                },
                {
                    "authorId": "2261906733",
                    "name": "Bo Han"
                }
            ]
        },
        {
            "paperId": "76210786f024577acf636777cd1f668eb5a4a710",
            "title": "Combating Bilateral Edge Noise for Robust Link Prediction",
            "abstract": "Although link prediction on graphs has achieved great success with the development of graph neural networks (GNNs), the potential robustness under the edge noise is still less investigated. To close this gap, we first conduct an empirical study to disclose that the edge noise bilaterally perturbs both input topology and target label, yielding severe performance degradation and representation collapse. To address this dilemma, we propose an information-theory-guided principle, Robust Graph Information Bottleneck (RGIB), to extract reliable supervision signals and avoid representation collapse. Different from the basic information bottleneck, RGIB further decouples and balances the mutual dependence among graph topology, target labels, and representation, building new learning objectives for robust representation against the bilateral noise. Two instantiations, RGIB-SSL and RGIB-REP, are explored to leverage the merits of different methodologies, i.e., self-supervised learning and data reparameterization, for implicit and explicit data denoising, respectively. Extensive experiments on six datasets and three GNNs with diverse noisy scenarios verify the effectiveness of our RGIB instantiations. The code is publicly available at: https://github.com/tmlr-group/RGIB.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1768392566",
                    "name": "Zhanke Zhou"
                },
                {
                    "authorId": "2110069725",
                    "name": "Jiangchao Yao"
                },
                {
                    "authorId": "2264395342",
                    "name": "Jiaxu Liu"
                },
                {
                    "authorId": "2265389387",
                    "name": "Xiawei Guo"
                },
                {
                    "authorId": "2264858085",
                    "name": "Quanming Yao"
                },
                {
                    "authorId": "2265252624",
                    "name": "Li He"
                },
                {
                    "authorId": "2268509495",
                    "name": "Liang Wang"
                },
                {
                    "authorId": "2273352644",
                    "name": "Bo Zheng"
                },
                {
                    "authorId": "2261906733",
                    "name": "Bo Han"
                }
            ]
        },
        {
            "paperId": "7d1c7810b8ef57a43cd3eb0be197bd270f1c8dc3",
            "title": "Diversified Outlier Exposure for Out-of-Distribution Detection via Informative Extrapolation",
            "abstract": "Out-of-distribution (OOD) detection is important for deploying reliable machine learning models on real-world applications. Recent advances in outlier exposure have shown promising results on OOD detection via fine-tuning model with informatively sampled auxiliary outliers. However, previous methods assume that the collected outliers can be sufficiently large and representative to cover the boundary between ID and OOD data, which might be impractical and challenging. In this work, we propose a novel framework, namely, Diversified Outlier Exposure (DivOE), for effective OOD detection via informative extrapolation based on the given auxiliary outliers. Specifically, DivOE introduces a new learning objective, which diversifies the auxiliary distribution by explicitly synthesizing more informative outliers for extrapolation during training. It leverages a multi-step optimization method to generate novel outliers beyond the original ones, which is compatible with many variants of outlier exposure. Extensive experiments and analyses have been conducted to characterize and demonstrate the effectiveness of the proposed DivOE. The code is publicly available at: https://github.com/tmlr-group/DivOE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2143475848",
                    "name": "Jianing Zhu"
                },
                {
                    "authorId": "2261357818",
                    "name": "Geng Yu"
                },
                {
                    "authorId": "2110069725",
                    "name": "Jiangchao Yao"
                },
                {
                    "authorId": "2244770736",
                    "name": "Tongliang Liu"
                },
                {
                    "authorId": "47537639",
                    "name": "Gang Niu"
                },
                {
                    "authorId": "67154907",
                    "name": "Masashi Sugiyama"
                },
                {
                    "authorId": "2257285276",
                    "name": "Bo Han"
                }
            ]
        }
    ]
}