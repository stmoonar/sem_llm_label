{
    "authorId": "2237192",
    "papers": [
        {
            "paperId": "0d028f69cdb23a4d408364b5ba0f621892a61d75",
            "title": "LLM-jp: A Cross-organizational Project for the Research and Development of Fully Open Japanese LLMs",
            "abstract": "This paper introduces LLM-jp, a cross-organizational project for the research and development of Japanese large language models (LLMs). LLM-jp aims to develop open-source and strong Japanese LLMs, and as of this writing, more than 1,500 participants from academia and industry are working together for this purpose. This paper presents the background of the establishment of LLM-jp, summaries of its activities, and technical reports on the LLMs developed by LLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2310227573",
                    "name": "LLM-jp Akiko Aizawa"
                },
                {
                    "authorId": "2307012219",
                    "name": "Eiji Aramaki"
                },
                {
                    "authorId": "2302436363",
                    "name": "Bowen Chen"
                },
                {
                    "authorId": "49412583",
                    "name": "Fei Cheng"
                },
                {
                    "authorId": "2310228227",
                    "name": "Hiroyuki Deguchi"
                },
                {
                    "authorId": "2306632898",
                    "name": "Rintaro Enomoto"
                },
                {
                    "authorId": "2298889885",
                    "name": "Kazuki Fujii"
                },
                {
                    "authorId": "2310230602",
                    "name": "Kensuke Fukumoto"
                },
                {
                    "authorId": "2275986420",
                    "name": "Takuya Fukushima"
                },
                {
                    "authorId": "2304716939",
                    "name": "Namgi Han"
                },
                {
                    "authorId": "2301581498",
                    "name": "Yuto Harada"
                },
                {
                    "authorId": "1760686",
                    "name": "Chikara Hashimoto"
                },
                {
                    "authorId": "114789302",
                    "name": "Tatsuya Hiraoka"
                },
                {
                    "authorId": "1661044591",
                    "name": "Shohei Hisada"
                },
                {
                    "authorId": "2310229439",
                    "name": "Sosuke Hosokawa"
                },
                {
                    "authorId": "2310229958",
                    "name": "Lu Jie"
                },
                {
                    "authorId": "2310230506",
                    "name": "Keisuke Kamata"
                },
                {
                    "authorId": "1894424",
                    "name": "T. Kanazawa"
                },
                {
                    "authorId": "3166546",
                    "name": "H. Kanezashi"
                },
                {
                    "authorId": "2310229458",
                    "name": "Hiroshi Kataoka"
                },
                {
                    "authorId": "2310230732",
                    "name": "Satoru Katsumata"
                },
                {
                    "authorId": "2306632595",
                    "name": "Daisuke Kawahara"
                },
                {
                    "authorId": "1992912155",
                    "name": "Seiya Kawano"
                },
                {
                    "authorId": "2293313908",
                    "name": "Atsushi Keyaki"
                },
                {
                    "authorId": "2310229349",
                    "name": "Keisuke Kiryu"
                },
                {
                    "authorId": "51195367",
                    "name": "Hirokazu Kiyomaru"
                },
                {
                    "authorId": "2057510100",
                    "name": "Takashi Kodama"
                },
                {
                    "authorId": "2310230965",
                    "name": "Takahiro Kubo"
                },
                {
                    "authorId": "1839750",
                    "name": "Yohei Kuga"
                },
                {
                    "authorId": "2304551550",
                    "name": "Ryoma Kumon"
                },
                {
                    "authorId": "2306632538",
                    "name": "Shuhei Kurita"
                },
                {
                    "authorId": "1795664",
                    "name": "S. Kurohashi"
                },
                {
                    "authorId": "2310389396",
                    "name": "Conglong Li"
                },
                {
                    "authorId": "2260760830",
                    "name": "Taiki Maekawa"
                },
                {
                    "authorId": "2221005697",
                    "name": "Hiroshi Matsuda"
                },
                {
                    "authorId": "2302320083",
                    "name": "Yusuke Miyao"
                },
                {
                    "authorId": "2310230618",
                    "name": "Kentaro Mizuki"
                },
                {
                    "authorId": "98889783",
                    "name": "Sakae Mizuki"
                },
                {
                    "authorId": "2606962",
                    "name": "Yugo Murawaki"
                },
                {
                    "authorId": "2310222763",
                    "name": "Ryo Nakamura"
                },
                {
                    "authorId": "2294513555",
                    "name": "Taishi Nakamura"
                },
                {
                    "authorId": "90721555",
                    "name": "Kouta Nakayama"
                },
                {
                    "authorId": "2310228071",
                    "name": "Tomoka Nakazato"
                },
                {
                    "authorId": "2301579122",
                    "name": "Takuro Niitsuma"
                },
                {
                    "authorId": "134196908",
                    "name": "Jiro Nishitoba"
                },
                {
                    "authorId": "2266470694",
                    "name": "Yusuke Oda"
                },
                {
                    "authorId": "2310230797",
                    "name": "Hayato Ogawa"
                },
                {
                    "authorId": "2310229150",
                    "name": "Takumi Okamoto"
                },
                {
                    "authorId": "2298897174",
                    "name": "Naoaki Okazaki"
                },
                {
                    "authorId": "50856622",
                    "name": "Yohei Oseki"
                },
                {
                    "authorId": "2310230447",
                    "name": "Shintaro Ozaki"
                },
                {
                    "authorId": "2310228915",
                    "name": "Koki Ryu"
                },
                {
                    "authorId": "2279628470",
                    "name": "Rafa\u0142 Rzepka"
                },
                {
                    "authorId": "2274484600",
                    "name": "Keisuke Sakaguchi"
                },
                {
                    "authorId": "31480878",
                    "name": "S. Sasaki"
                },
                {
                    "authorId": "2301583956",
                    "name": "Satoshi Sekine"
                },
                {
                    "authorId": "2310229875",
                    "name": "Kohei Suda"
                },
                {
                    "authorId": "2673984",
                    "name": "Saku Sugawara"
                },
                {
                    "authorId": "2310230616",
                    "name": "Issa Sugiura"
                },
                {
                    "authorId": "2310228516",
                    "name": "Hiroaki Sugiyama"
                },
                {
                    "authorId": "2302145923",
                    "name": "Hisami Suzuki"
                },
                {
                    "authorId": "2291636791",
                    "name": "Jun Suzuki"
                },
                {
                    "authorId": "2231831",
                    "name": "T. Suzumura"
                },
                {
                    "authorId": "2310230917",
                    "name": "Kensuke Tachibana"
                },
                {
                    "authorId": "2310230516",
                    "name": "Yu Takagi"
                },
                {
                    "authorId": "144664473",
                    "name": "Kyosuke Takami"
                },
                {
                    "authorId": "2310229733",
                    "name": "Koichi Takeda"
                },
                {
                    "authorId": "2052753881",
                    "name": "Masashi Takeshita"
                },
                {
                    "authorId": "2286067351",
                    "name": "Masahiro Tanaka"
                },
                {
                    "authorId": "47879281",
                    "name": "K. Taura"
                },
                {
                    "authorId": "153616317",
                    "name": "A. Tolmachev"
                },
                {
                    "authorId": "2057544990",
                    "name": "Nobuhiro Ueda"
                },
                {
                    "authorId": "2163526362",
                    "name": "Zhen Wan"
                },
                {
                    "authorId": "2084889",
                    "name": "Shuntaro Yada"
                },
                {
                    "authorId": "2290073801",
                    "name": "Sakiko Yahata"
                },
                {
                    "authorId": "2310795985",
                    "name": "Yuya Yamamoto"
                },
                {
                    "authorId": "2310229909",
                    "name": "Yusuke Yamauchi"
                },
                {
                    "authorId": "3486313",
                    "name": "Hitomi Yanaka"
                },
                {
                    "authorId": "2294362068",
                    "name": "Rio Yokota"
                },
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                }
            ]
        },
        {
            "paperId": "21b4777948797377deedf4a9f1f58ad13f6b8b5d",
            "title": "Overview of the Tenth Dialog System Technology Challenge: DSTC10",
            "abstract": "This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                },
                {
                    "authorId": "1725643",
                    "name": "Yun-Nung (Vivian) Chen"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                },
                {
                    "authorId": "2150275",
                    "name": "Satwik Kottur"
                },
                {
                    "authorId": "2887412",
                    "name": "Jinchao Li"
                },
                {
                    "authorId": "2127328167",
                    "name": "Behnam Hedayatnia"
                },
                {
                    "authorId": "29072828",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2066415714",
                    "name": "Zhengcong Fei"
                },
                {
                    "authorId": "2109965103",
                    "name": "Zekang Li"
                },
                {
                    "authorId": "27672597",
                    "name": "Jinchao Zhang"
                },
                {
                    "authorId": "2257374643",
                    "name": "Yang Feng"
                },
                {
                    "authorId": "2116575668",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2127354716",
                    "name": "Seokhwan Kim"
                },
                {
                    "authorId": "2152797401",
                    "name": "Yang Liu"
                },
                {
                    "authorId": "2152284471",
                    "name": "Di Jin"
                },
                {
                    "authorId": "1710287",
                    "name": "A. Papangelis"
                },
                {
                    "authorId": "145916630",
                    "name": "Karthik Gopalakrishnan"
                },
                {
                    "authorId": "1395813836",
                    "name": "Dilek Z. Hakkani-T\u00fcr"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "1979505",
                    "name": "A. Geramifard"
                },
                {
                    "authorId": "1765212",
                    "name": "Chiori Hori"
                },
                {
                    "authorId": "31017418",
                    "name": "Ankit Shah"
                },
                {
                    "authorId": "2111574800",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                },
                {
                    "authorId": "2319137716",
                    "name": "Jo\u00e3o Sedoc"
                },
                {
                    "authorId": "1405511901",
                    "name": "L. F. D\u2019Haro"
                },
                {
                    "authorId": "1694652",
                    "name": "Rafael E. Banchs"
                },
                {
                    "authorId": "1783635",
                    "name": "Alexander I. Rudnicky"
                }
            ]
        },
        {
            "paperId": "41baacf5dc0fda5c36fb63c48f2c91dbfeda3ba1",
            "title": "Rapport-Driven Virtual Agent: Rapport Building Dialogue Strategy for Improving User Experience at First Meeting",
            "abstract": "Rapport is known as a conversational aspect focusing on relationship building, which influences outcomes in collaborative tasks. This study aims to establish human-agent rapport through small talk by using a rapport-building strategy. We implemented this strategy for the virtual agents based on dialogue strategies by prompting a large language model (LLM). In particular, we utilized two dialogue strategies-predefined sequence and free-form-to guide the dialogue generation framework. We conducted analyses based on human evaluations, examining correlations between total turn, utterance characters, rapport score, and user experience variables: naturalness, satisfaction, interest, engagement, and usability. We investigated correlations between rapport score and naturalness, satisfaction, engagement, and conversation flow. Our experimental results also indicated that using free-form to prompt the rapport-building strategy performed the best in subjective scores.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2306780624",
                    "name": "Muhammad Yeza Baihaqi"
                },
                {
                    "authorId": "2306780637",
                    "name": "Angel Garc'ia Contreras"
                },
                {
                    "authorId": "1992912155",
                    "name": "Seiya Kawano"
                },
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                }
            ]
        },
        {
            "paperId": "55b7734598c8ee99953a532412e16bbd07051017",
            "title": "J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution",
            "abstract": "Understanding expressions that refer to the physical world is crucial for such human-assisting systems in the real world, as robots that must perform actions that are expected by users. In real-world reference resolution, a system must ground the verbal information that appears in user interactions to the visual information observed in egocentric views. To this end, we propose a multimodal reference resolution task and construct a Japanese Conversation dataset for Real-world Reference Resolution (J-CRe3). Our dataset contains egocentric video and dialogue audio of real-world conversations between two people acting as a master and an assistant robot at home. The dataset is annotated with crossmodal tags between phrases in the utterances and the object bounding boxes in the video frames. These tags include indirect reference relations, such as predicate-argument structures and bridging references as well as direct reference relations. We also constructed an experimental model and clarified the challenges in multimodal reference resolution tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2057544990",
                    "name": "Nobuhiro Ueda"
                },
                {
                    "authorId": "2293721872",
                    "name": "Hideko Habe"
                },
                {
                    "authorId": "2293710784",
                    "name": "Yoko Matsui"
                },
                {
                    "authorId": "19263795",
                    "name": "Akishige Yuguchi"
                },
                {
                    "authorId": "1992912155",
                    "name": "Seiya Kawano"
                },
                {
                    "authorId": "2293396251",
                    "name": "Yasutomo Kawanishi"
                },
                {
                    "authorId": "1795664",
                    "name": "S. Kurohashi"
                },
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                }
            ]
        },
        {
            "paperId": "79f48dd07c3f51a3308e6c842ef8dd965c144b35",
            "title": "Do as I Demand, Not as I Say: A Dataset for Developing a Reflective Life-Support Robot",
            "abstract": "Interactive robots that cooperate with humans must take appropriate actions in response to their requests. Unfortunately, such requests often have information gaps with their actual demands. However, robots are still expected to reason and act on what is required, depending on the situation. We call these reflective actions. To achieve such reflective actions for robots, we constructed a dataset that consists of the reflective actions of a domestic manipulation robot, in which the actions correspond to user utterances with their surroundings situations. By crowdsourcing, we defined several action scenarios that could be regarded as reflective. We recorded videos of situations described in the crowdsourcing scenarios, corresponding to the user situations just before the robot\u2019s reflective actions. We also annotated the videos of the user utterance transcriptions, objects, user poses, and user positions to investigate the contribution of such descriptive features to the reflective action decisions. Our experimental results indicated that even though our newly defined task is very challenging, it can be solved if the system has a concrete understanding of the situation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2111425352",
                    "name": "Shohei Tanaka"
                },
                {
                    "authorId": "2278110291",
                    "name": "Konosuke Yamasaki"
                },
                {
                    "authorId": "19263795",
                    "name": "Akishige Yuguchi"
                },
                {
                    "authorId": "1992912155",
                    "name": "Seiya Kawano"
                },
                {
                    "authorId": "2185313614",
                    "name": "Satoshi Nakamura"
                },
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                }
            ]
        },
        {
            "paperId": "9b3ea2cf5ce93f70def1d373f64c580b844b2bb3",
            "title": "A Gaze-grounded Visual Question Answering Dataset for Clarifying Ambiguous Japanese Questions",
            "abstract": "Situated conversations, which refer to visual information as visual question answering (VQA), often contain ambiguities caused by reliance on directive information. This problem is exacerbated because some languages, such as Japanese, often omit subjective or objective terms. Such ambiguities in questions are often clarified by the contexts in conversational situations, such as joint attention with a user or user gaze information. In this study, we propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous questions using gaze information by focusing on a clarification process complemented by gaze information. We also propose a method that utilizes gaze target estimation results to improve the accuracy of GazeVQA tasks. Our experimental results showed that the proposed method improved the performance in some cases of a VQA system on GazeVQA and identified some typical problems of GazeVQA tasks that need to be improved.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2293396183",
                    "name": "Shun Inadumi"
                },
                {
                    "authorId": "1992912155",
                    "name": "Seiya Kawano"
                },
                {
                    "authorId": "19263795",
                    "name": "Akishige Yuguchi"
                },
                {
                    "authorId": "2293396251",
                    "name": "Yasutomo Kawanishi"
                },
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                }
            ]
        },
        {
            "paperId": "2ef41c8d42959c13ef4752c63bdee07416872236",
            "title": "What\u2019s New? Identifying the Unfolding of New Events in a Narrative",
            "abstract": "Narratives include a rich source of events unfolding over time and context. Automatic understanding of these events provides a summarised comprehension of the narrative for further computation (such as reasoning). In this paper, we study the Information Status (IS) of the events and propose a novel challenging task: the automatic identification of new events in a narrative. We define an event as a triplet of subject, predicate, and object. The event is categorized as new with respect to the discourse context and whether it can be inferred through commonsense reasoning. We annotated a publicly available corpus of narratives with the new events at sentence level using human annotators. We present the annotation protocol and study the quality of the annotation and the difficulty of the task. We publish the annotated dataset, annotation materials, and machine learning baseline models for the task of new event extraction for narrative understanding.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2101322593",
                    "name": "Seyed Mahed Mousavi"
                },
                {
                    "authorId": "2111425352",
                    "name": "Shohei Tanaka"
                },
                {
                    "authorId": "2038117205",
                    "name": "G. Roccabruna"
                },
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                },
                {
                    "authorId": "2148246160",
                    "name": "Satoshi Nakamura"
                },
                {
                    "authorId": "1719162",
                    "name": "G. Riccardi"
                }
            ]
        },
        {
            "paperId": "61488048b1f326f7198a191d9f4958e2527a9dd7",
            "title": "End-to-end dialogue structure parsing on multi-floor dialogue based on multi-task learning",
            "abstract": "A multi-floor dialogue consists of multiple sets of dialogue participants, each conversing within their own floor. In the multi-floor dialogue, at least one multi-communicating member who is a participant of multiple floors and coordinates each to achieve a shared dialogue goal. The structure of such dialogues can be complex, involving intentional structure and relations that are within or across floors. In this study, We proposed a neural dialogue structure parser with an attention mechanism that applies multi-task learning to automatically identify the dialogue structure of multi-floor dialogues in a collaborative robot navigation domain. Furthermore, we propose to use dialogue response prediction as an auxiliary objective of the multi-floor dialogue structure parser to enhance the consistency of the multi-floor dialogue structure parsing. Our experimental results show that our proposed model improved the dialogue structure parsing performance more than conventional models in multi-floor dialogue.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1992912155",
                    "name": "Seiya Kawano"
                },
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                },
                {
                    "authorId": "144518646",
                    "name": "D. Traum"
                },
                {
                    "authorId": "2148246160",
                    "name": "Satoshi Nakamura"
                }
            ]
        },
        {
            "paperId": "b7f676060a8faa90b2ae839029499b758112cb97",
            "title": "Analysis of Style-Shifting on Social Media: Using Neural Language Model Conditioned by Social Meanings",
            "abstract": ",",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1992912155",
                    "name": "Seiya Kawano"
                },
                {
                    "authorId": "32013892",
                    "name": "Shota Kanezaki"
                },
                {
                    "authorId": "2273684715",
                    "name": "Angel Fernando Garcia Contreras"
                },
                {
                    "authorId": "19263795",
                    "name": "Akishige Yuguchi"
                },
                {
                    "authorId": "2740358",
                    "name": "Marie Katsurai"
                },
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                }
            ]
        },
        {
            "paperId": "bac3eceef3a511d1a74479b8ed6d4cc86510d8eb",
            "title": "Operative Action Captioning for Estimating System Actions",
            "abstract": "Human-assistive systems, such as robots, need to correctly understand the surrounding situation based on obser-vations and output the required support actions for humans. Language is one of the important channels to communicate with humans, and robots are required to have the ability to express their understanding and action-planning results. In this study, we propose a new task of operative action captioning that estimates and verbalizes the actions to be taken by the system in a human-assisting domain. We constructed a system that outputs a verbal description of a possible operative action that changes the current state to the given target state. We collected a dataset consisting of two images as observations, which express the current state and the state changed by actions and a caption that describes the actions that change the current state to the target state, by crowdsourcing in daily life situations. Then we constructed a system that estimates an operative action by a caption. Since the operative action's caption is expected to contain some state-changing actions, we use scene graph prediction as an auxiliary task because the events written in the scene graphs correspond to the state changes. Experimental results showed that our system successfully described the operative actions that should be conducted between the current and target states. The auxiliary tasks that predict the scene graphs improved the quality of the estimation results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116473094",
                    "name": "Taiki Nakamura"
                },
                {
                    "authorId": "1992912155",
                    "name": "Seiya Kawano"
                },
                {
                    "authorId": "19263795",
                    "name": "Akishige Yuguchi"
                },
                {
                    "authorId": "1770200",
                    "name": "Yasutomo Kawanishi"
                },
                {
                    "authorId": "2237192",
                    "name": "Koichiro Yoshino"
                }
            ]
        }
    ]
}