{
    "authorId": "2082239112",
    "papers": [
        {
            "paperId": "36f7bc27c9a37eb337c35df4ae86f148e13d4e9a",
            "title": "Understanding In-Context Learning via Supportive Pretraining Data",
            "abstract": "In-context learning (ICL) improves language models\u2019 performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining data that supports ICL. We observe that a continued pretraining on this small subset significantly improves the model\u2019s ICL ability, by up to 18%. We then compare the supportive subset constrastively with random subsets of pretraining data and discover: (1) The supportive pretraining data to ICL do not have a higher domain relevance to downstream tasks. (2) The supportive pretraining data have a higher mass of rarely occurring, long-tail tokens. (3) The supportive pretraining data are challenging examples where the information gain from long-range context is below average, indicating learning to incorporate difficult long-range context encourages ICL. Our work takes a first step towards understanding ICL via analyzing instance-level pretraining data. Our insights have a potential to enhance the ICL ability of language models by actively guiding the construction of pretraining data in the future.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40500540",
                    "name": "Xiaochuang Han"
                },
                {
                    "authorId": "2082239112",
                    "name": "Daniel Simig"
                },
                {
                    "authorId": "39980906",
                    "name": "Todor Mihaylov"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "1709797",
                    "name": "Asli Celikyilmaz"
                },
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                }
            ]
        },
        {
            "paperId": "412e266cddfd87c79087a88ba1e4d11b89a45a13",
            "title": "MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers",
            "abstract": "Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49297123",
                    "name": "L. Yu"
                },
                {
                    "authorId": "2082239112",
                    "name": "Daniel Simig"
                },
                {
                    "authorId": "2191722001",
                    "name": "Colin Flaherty"
                },
                {
                    "authorId": "2201435",
                    "name": "Armen Aghajanyan"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "35084211",
                    "name": "M. Lewis"
                }
            ]
        },
        {
            "paperId": "638b08154fbb71fd34db2aae6cb40045577fe0de",
            "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication",
            "abstract": "Progress in machine learning has been driven in large part by massive increases in data. However, large web-scale datasets such as LAION are largely uncurated beyond searches for exact duplicates, potentially leaving much redundancy. Here, we introduce SemDeDup, a method which leverages embeddings from pre-trained models to identify and remove semantic duplicates: data pairs which are semantically similar, but not exactly identical. Removing semantic duplicates preserves performance and speeds up learning. Analyzing a subset of LAION, we show that SemDeDup can remove 50% of the data with minimal performance loss, effectively halving training time. Moreover, performance increases out of distribution. Also, analyzing language models trained on C4, a partially curated dataset, we show that SemDeDup improves over prior approaches while providing efficiency gains. SemDeDup provides an example of how simple ways of leveraging quality embeddings can be used to make models learn faster with less data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2176785233",
                    "name": "Amro Abbas"
                },
                {
                    "authorId": "2551387",
                    "name": "Kushal Tirumala"
                },
                {
                    "authorId": "2082239112",
                    "name": "Daniel Simig"
                },
                {
                    "authorId": "25769960",
                    "name": "S. Ganguli"
                },
                {
                    "authorId": "4690624",
                    "name": "Ari S. Morcos"
                }
            ]
        },
        {
            "paperId": "bbf1718167778347a2aa11a84975a69edff6bbd5",
            "title": "Evaluating end-to-end entity linking on domain-specific knowledge bases: Learning about ancient technologies from museum collections",
            "abstract": "To study social, economic, and historical questions, researchers in the social sciences and humanities have started to use increasingly large unstructured textual datasets. While recent advances in NLP provide many tools to efficiently process such data, most existing approaches rely on generic solutions whose performance and suitability for domain-specific tasks is not well understood. This work presents an attempt to bridge this domain gap by exploring the use of modern Entity Linking approaches for the enrichment of museum collection data. We collect a dataset comprising of more than 1700 texts annotated with 7,510 mention-entity pairs, evaluate some off-the-shelf solutions in detail using this dataset and finally fine-tune a recent end-to-end EL model on this data. We show that our fine-tuned model significantly outperforms other approaches currently available in this domain and present a proof-of-concept use case of this model. We release our dataset and our best model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1506090435",
                    "name": "Sebasti\u00e1n Cadavid-S\u00e1nchez"
                },
                {
                    "authorId": "2218882005",
                    "name": "Khalil Kacem"
                },
                {
                    "authorId": "2218764071",
                    "name": "Rafael Aparecido Martins Frade"
                },
                {
                    "authorId": "2218882001",
                    "name": "Johannes Boehm"
                },
                {
                    "authorId": "21571110",
                    "name": "Thomas Chaney"
                },
                {
                    "authorId": "1795365",
                    "name": "D. Lashkari"
                },
                {
                    "authorId": "2082239112",
                    "name": "Daniel Simig"
                }
            ]
        },
        {
            "paperId": "13a0d8bb38f739990c8cd65a44061c6534f17221",
            "title": "OPT: Open Pre-trained Transformer Language Models",
            "abstract": "Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2108244542",
                    "name": "Susan Zhang"
                },
                {
                    "authorId": "3849208",
                    "name": "Stephen Roller"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "2347956",
                    "name": "Mikel Artetxe"
                },
                {
                    "authorId": "2108267192",
                    "name": "Moya Chen"
                },
                {
                    "authorId": "1782969",
                    "name": "Shuohui Chen"
                },
                {
                    "authorId": "2065332326",
                    "name": "Christopher Dewan"
                },
                {
                    "authorId": "2138579860",
                    "name": "Mona T. Diab"
                },
                {
                    "authorId": "2116235416",
                    "name": "Xian Li"
                },
                {
                    "authorId": "143724481",
                    "name": "Xi Victoria Lin"
                },
                {
                    "authorId": "39980906",
                    "name": "Todor Mihaylov"
                },
                {
                    "authorId": "40511414",
                    "name": "Myle Ott"
                },
                {
                    "authorId": "88728159",
                    "name": "Sam Shleifer"
                },
                {
                    "authorId": "35752280",
                    "name": "Kurt Shuster"
                },
                {
                    "authorId": "2082239112",
                    "name": "Daniel Simig"
                },
                {
                    "authorId": "2146367061",
                    "name": "Punit Singh Koura"
                },
                {
                    "authorId": "5382923",
                    "name": "Anjali Sridhar"
                },
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                }
            ]
        },
        {
            "paperId": "2ec864b0aaf935b3ead513ee4fca86499a8e9c70",
            "title": "Text Characterization Toolkit",
            "abstract": "In NLP, models are usually evaluated by reporting single-number performance scores on a number of readily available benchmarks, without much deeper analysis. Here, we argue that - especially given the well-known fact that benchmarks often contain biases, artefacts, and spurious correlations - deeper results analysis should become the de-facto standard when presenting new models or benchmarks. We present a tool that researchers can use to study properties of the dataset and the influence of those properties on their models' behaviour. Our Text Characterization Toolkit includes both an easy-to-use annotation tool, as well as off-the-shelf scripts that can be used for specific analyses. We also present use-cases from three different domains: we use the tool to predict what are difficult examples for given well-known trained models and identify (potentially harmful) biases and heuristics that are present in a dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2082239112",
                    "name": "Daniel Simig"
                },
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "74461595",
                    "name": "Verna Dankers"
                },
                {
                    "authorId": "40068904",
                    "name": "Peter Henderson"
                },
                {
                    "authorId": "2049136",
                    "name": "Khuyagbaatar Batsuren"
                },
                {
                    "authorId": "3449411",
                    "name": "Dieuwke Hupkes"
                },
                {
                    "authorId": "2138579860",
                    "name": "Mona T. Diab"
                }
            ]
        },
        {
            "paperId": "840f4d6ad8b852918a6d85020ce789bfa8c56ef9",
            "title": "Open Vocabulary Extreme Classification Using Generative Models",
            "abstract": "The extreme multi-label classification (XMC) task aims at tagging content with a subset of labels from an extremely large label set. The label vocabulary is typically defined in advance by domain experts and assumed to capture all necessary tags. However in real world scenarios this label set, although large, is often incomplete and experts frequently need to refine it. To develop systems that simplify this process, we introduce the task of open vocabulary XMC (OXMC): given a piece of content, predict a set of labels, some of which may be outside of the known tag set. Hence, in addition to not having training data for some labels\u2013as is the case in zero-shot classification\u2013models need to invent some labels on-thefly. We propose GROOV, a fine-tuned seq2seq model for OXMC that generates the set of labels as a flat sequence and is trained using a novel loss independent of predicted label order. We show the efficacy of the approach, experimenting with popular XMC datasets for which GROOV is able to predict meaningful labels outside the given vocabulary while performing on par with state-of-the-art solutions for known labels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2082239112",
                    "name": "Daniel Simig"
                },
                {
                    "authorId": "40052301",
                    "name": "F. Petroni"
                },
                {
                    "authorId": "1878989",
                    "name": "Pouya Yanki"
                },
                {
                    "authorId": "34910890",
                    "name": "Kashyap Popat"
                },
                {
                    "authorId": "2164990923",
                    "name": "Christina Du"
                },
                {
                    "authorId": "48662861",
                    "name": "Sebastian Riedel"
                },
                {
                    "authorId": "69910001",
                    "name": "Majid Yazdani"
                }
            ]
        },
        {
            "paperId": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
            "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "abstract": "Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2053620465",
                    "name": "S. Iyer"
                },
                {
                    "authorId": "143724481",
                    "name": "Xi Victoria Lin"
                },
                {
                    "authorId": "10721120",
                    "name": "Ramakanth Pasunuru"
                },
                {
                    "authorId": "39980906",
                    "name": "Todor Mihaylov"
                },
                {
                    "authorId": "2082239112",
                    "name": "Daniel Simig"
                },
                {
                    "authorId": "2162840444",
                    "name": "Ping Yu"
                },
                {
                    "authorId": "35752280",
                    "name": "Kurt Shuster"
                },
                {
                    "authorId": "2118914337",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "2154975456",
                    "name": "Qing Liu"
                },
                {
                    "authorId": "2146367061",
                    "name": "Punit Singh Koura"
                },
                {
                    "authorId": "2116235416",
                    "name": "Xian Li"
                },
                {
                    "authorId": "2146367747",
                    "name": "Brian O'Horo"
                },
                {
                    "authorId": "2064737506",
                    "name": "Gabriel Pereyra"
                },
                {
                    "authorId": "2155451431",
                    "name": "Jeff Wang"
                },
                {
                    "authorId": "2065332326",
                    "name": "Christopher Dewan"
                },
                {
                    "authorId": "1709797",
                    "name": "Asli Celikyilmaz"
                },
                {
                    "authorId": "2137813791",
                    "name": "Luke S. Zettlemoyer"
                },
                {
                    "authorId": "1759422",
                    "name": "Veselin Stoyanov"
                }
            ]
        },
        {
            "paperId": "4724ebee34ca2cd0a19c3a1ddb83d6d870dd7904",
            "title": "Few-shot Learning with Multilingual Generative Language Models",
            "abstract": "Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "143724481",
                    "name": "Xi Victoria Lin"
                },
                {
                    "authorId": "39980906",
                    "name": "Todor Mihaylov"
                },
                {
                    "authorId": "2347956",
                    "name": "Mikel Artetxe"
                },
                {
                    "authorId": "1785372925",
                    "name": "Tianlu Wang"
                },
                {
                    "authorId": "1782969",
                    "name": "Shuohui Chen"
                },
                {
                    "authorId": "2082239112",
                    "name": "Daniel Simig"
                },
                {
                    "authorId": "40511414",
                    "name": "Myle Ott"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "2116473",
                    "name": "Shruti Bhosale"
                },
                {
                    "authorId": "3048577",
                    "name": "Jingfei Du"
                },
                {
                    "authorId": "10721120",
                    "name": "Ramakanth Pasunuru"
                },
                {
                    "authorId": "88728159",
                    "name": "Sam Shleifer"
                },
                {
                    "authorId": "2146367061",
                    "name": "Punit Singh Koura"
                },
                {
                    "authorId": "113810201",
                    "name": "Vishrav Chaudhary"
                },
                {
                    "authorId": "2146367747",
                    "name": "Brian O'Horo"
                },
                {
                    "authorId": "2155451431",
                    "name": "Jeff Wang"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "1714932",
                    "name": "Zornitsa Kozareva"
                },
                {
                    "authorId": "2138579860",
                    "name": "Mona T. Diab"
                },
                {
                    "authorId": "1389924486",
                    "name": "Ves Stoyanov"
                },
                {
                    "authorId": "2116235416",
                    "name": "Xian Li"
                }
            ]
        }
    ]
}