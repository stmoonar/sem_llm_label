{
    "authorId": "1477273148",
    "papers": [
        {
            "paperId": "38be57ad43ce2cefb7483702271dff1fbff2b844",
            "title": "Enhancing medical vision-language contrastive learning via inter-matching relation modelling",
            "abstract": "Medical image representations can be learned through medical vision-language contrastive learning (mVLCL) where medical imaging reports are used as weak supervision through image-text alignment. These learned image representations can be transferred to and benefit various downstream medical vision tasks such as disease classification and segmentation. Recent mVLCL methods attempt to align image sub-regions and the report keywords as local-matchings. However, these methods aggregate all local-matchings via simple pooling operations while ignoring the inherent relations between them. These methods therefore fail to reason between local-matchings that are semantically related, e.g., local-matchings that correspond to the disease word and the location word (semantic-relations), and also fail to differentiate such clinically important local-matchings from others that correspond to less meaningful words, e.g., conjunction words (importance-relations). Hence, we propose a mVLCL method that models the inter-matching relations between local-matchings via a relation-enhanced contrastive learning framework (RECLF). In RECLF, we introduce a semantic-relation reasoning module (SRM) and an importance-relation reasoning module (IRM) to enable more fine-grained report supervision for image representation learning. We evaluated our method using four public benchmark datasets on four downstream tasks, including segmentation, zero-shot classification, supervised classification, and cross-modal retrieval. Our results demonstrated the superiority of our RECLF over the state-of-the-art mVLCL methods with consistent improvements across single-modal and cross-modal tasks. These results suggest that our RECLF, by modelling the inter-matching relations, can learn improved medical image representations with better generalization capabilities.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2209861054",
                    "name": "Mingjian Li"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2110883",
                    "name": "M. Fulham"
                },
                {
                    "authorId": "2251669007",
                    "name": "David Dagan Feng"
                },
                {
                    "authorId": "49117537",
                    "name": "Lei Bi"
                },
                {
                    "authorId": "2110005686",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "5044cd6fa7f2ff4d2978cd956554ad38b52607f2",
            "title": "Correlation-aware Coarse-to-fine MLPs for Deformable Medical Image Registration",
            "abstract": "Deformable image registration is a fundamental step for medical image analysis. Recently, transformers have been used for registration and outperformed Convolutional Neural Networks (CNNs). Transformers can capture long-range dependence among image features, which have been shown beneficial for registration. However, due to the high computation/memory loads of self-attention, transformers are typically used at downsampled feature resolutions and cannot capture fine-grained long-range dependence at the full image resolution. This limits deformable registration as it necessitates precise dense correspondence between each image pixel. Multi-layer Perceptrons (MLPs) without self-attention are efficient in computation/memory usage, enabling the feasibility of capturing fine-grained long-range dependence at full resolution. Nevertheless, MLPs have not been extensively explored for image registration and are lacking the consideration of inductive bias crucial for medical registration tasks. In this study, we propose the first correlation-aware MLP-based registration network (CorrMLP) for deformable medical image registration. Our CorrMLP introduces a correlation-aware multi-window MLP block in a novel coarse-to-fine registration architecture, which captures fine-grained multi-range dependence to perform correlation-aware coarse-to-fine registration. Extensive experiments with seven public medical datasets show that our CorrMLP outperforms state-of-the-art deformable registration methods.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "49117537",
                    "name": "Lei Bi"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "87b2f50d4d09655185834c98d19db67a06e9fe03",
            "title": "3DPX: Progressive 2D-to-3D Oral Image Reconstruction with Hybrid MLP-CNN Networks",
            "abstract": "Panoramic X-ray (PX) is a prevalent modality in dental practice for its wide availability and low cost. However, as a 2D projection image, PX does not contain 3D anatomical information, and therefore has limited use in dental applications that can benefit from 3D information, e.g., tooth angular misa-lignment detection and classification. Reconstructing 3D structures directly from 2D PX has recently been explored to address limitations with existing methods primarily reliant on Convolutional Neural Networks (CNNs) for direct 2D-to-3D mapping. These methods, however, are unable to correctly infer depth-axis spatial information. In addition, they are limited by the in-trinsic locality of convolution operations, as the convolution kernels only capture the information of immediate neighborhood pixels. In this study, we propose a progressive hybrid Multilayer Perceptron (MLP)-CNN pyra-mid network (3DPX) for 2D-to-3D oral PX reconstruction. We introduce a progressive reconstruction strategy, where 3D images are progressively re-constructed in the 3DPX with guidance imposed on the intermediate recon-struction result at each pyramid level. Further, motivated by the recent ad-vancement of MLPs that show promise in capturing fine-grained long-range dependency, our 3DPX integrates MLPs and CNNs to improve the semantic understanding during reconstruction. Extensive experiments on two large datasets involving 464 studies demonstrate that our 3DPX outperforms state-of-the-art 2D-to-3D oral reconstruction methods, including standalone MLP and transformers, in reconstruction quality, and also im-proves the performance of downstream angular misalignment classification tasks.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2314773871",
                    "name": "Xiaoshuang Li"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2281291244",
                    "name": "Zimo Huang"
                },
                {
                    "authorId": "2280306478",
                    "name": "Lei Bi"
                },
                {
                    "authorId": "2281306199",
                    "name": "Eduardo Delamare"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "2241372738",
                    "name": "Bin Sheng"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "9d8bd7f38a90054635a976f7ced071780c432e4f",
            "title": "3DPX: Single Panoramic X-ray Analysis Guided by 3D Oral Structure Reconstruction",
            "abstract": "Panoramic X-ray (PX) is a prevalent modality in dentistry practice owing to its wide availability and low cost. However, as a 2D projection of a 3D structure, PX suffers from anatomical information loss and PX diagnosis is limited compared to that with 3D imaging modalities. 2D-to-3D reconstruction methods have been explored for the ability to synthesize the absent 3D anatomical information from 2D PX for use in PX image analysis. However, there are challenges in leveraging such 3D synthesized reconstructions. First, inferring 3D depth from 2D images remains a challenging task with limited accuracy. The second challenge is the joint analysis of 2D PX with its 3D synthesized counterpart, with the aim to maximize the 2D-3D synergy while minimizing the errors arising from the synthesized image. In this study, we propose a new method termed 3DPX - PX image analysis guided by 2D-to-3D reconstruction, to overcome these challenges. 3DPX consists of (i) a novel progressive reconstruction network to improve 2D-to-3D reconstruction and, (ii) a contrastive-guided bidirectional multimodality alignment module for 3D-guided 2D PX classification and segmentation tasks. The reconstruction network progressively reconstructs 3D images with knowledge imposed on the intermediate reconstructions at multiple pyramid levels and incorporates Multilayer Perceptrons to improve semantic understanding. The downstream networks leverage the reconstructed images as 3D anatomical guidance to the PX analysis through feature alignment, which increases the 2D-3D synergy with bidirectional feature projection and decease the impact of potential errors with contrastive guidance. Extensive experiments on two oral datasets involving 464 studies demonstrate that 3DPX outperforms the state-of-the-art methods in various tasks including 2D-to-3D reconstruction, PX classification and lesion segmentation.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2314773871",
                    "name": "Xiaoshuang Li"
                },
                {
                    "authorId": "2281291244",
                    "name": "Zimo Huang"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2281306199",
                    "name": "Eduardo Delamare"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "2280306478",
                    "name": "Lei Bi"
                },
                {
                    "authorId": "2323370873",
                    "name": "Bin Sheng"
                },
                {
                    "authorId": "2323703676",
                    "name": "Lingyong Jiang"
                },
                {
                    "authorId": "2323376308",
                    "name": "Bo Li"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "b27152219dcd97b3c6b4ffa84b135adff0af983a",
            "title": "Dynamic Traceback Learning for Medical Report Generation",
            "abstract": "Automated medical report generation has the potential to significantly reduce the workload associated with the time-consuming process of medical reporting. Recent generative representation learning methods have shown promise in integrating vision and language modalities for medical report generation. However, when trained end-to-end and applied directly to medical image-to-text generation, they face two significant challenges: i) difficulty in accurately capturing subtle yet crucial pathological details, and ii) reliance on both visual and textual inputs during inference, leading to performance degradation in zero-shot inference when only images are available. To address these challenges, this study proposes a novel multi-modal dynamic traceback learning framework (DTrace). Specifically, we introduce a traceback mechanism to supervise the semantic validity of generated content and a dynamic learning strategy to adapt to various proportions of image and text input, enabling text generation without strong reliance on the input from both modalities during inference. The learning of cross-modal knowledge is enhanced by supervising the model to recover masked semantic information from a complementary counterpart. Extensive experiments conducted on two benchmark datasets, IU-Xray and MIMIC-CXR, demonstrate that the proposed DTrace framework outperforms state-of-the-art methods for medical report generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281398403",
                    "name": "Shuchang Ye"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2209861054",
                    "name": "Mingjian Li"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "1394609613",
                    "name": "Usman Naseem"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "da8103f9c09f20db5433b917e0a3adc9e0556cdc",
            "title": "SGSeg: Enabling Text-free Inference in Language-guided Segmentation of Chest X-rays via Self-guidance",
            "abstract": "Segmentation of infected areas in chest X-rays is pivotal for facilitating the accurate delineation of pulmonary structures and pathological anomalies. Recently, multi-modal language-guided image segmentation methods have emerged as a promising solution for chest X-rays where the clinical text reports, depicting the assessment of the images, are used as guidance. Nevertheless, existing language-guided methods require clinical reports alongside the images, and hence, they are not applicable for use in image segmentation in a decision support context, but rather limited to retrospective image analysis after clinical reporting has been completed. In this study, we propose a self-guided segmentation framework (SGSeg) that leverages language guidance for training (multi-modal) while enabling text-free inference (uni-modal), which is the first that enables text-free inference in language-guided segmentation. We exploit the critical location information of both pulmonary and pathological structures depicted in the text reports and introduce a novel localization-enhanced report generation (LERG) module to generate clinical reports for self-guidance. Our LERG integrates an object detector and a location-based attention aggregator, weakly-supervised by a location-aware pseudo-label extraction module. Extensive experiments on a well-benchmarked QaTa-COV19 dataset demonstrate that our SGSeg achieved superior performance than existing uni-modal segmentation methods and closely matched the state-of-the-art performance of multi-modal language-guided segmentation methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281398403",
                    "name": "Shuchang Ye"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2209861054",
                    "name": "Mingjian Li"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "5932c79a0970ae7ba68172ae87893a960ecb246d",
            "title": "DeepMSS: Deep Multi-Modality Segmentation-to-Survival Learning for Survival Outcome Prediction from PET/CT Images",
            "abstract": "\u2014Survival prediction is a major concern for cancer management. Deep survival models based on deep learning have been widely adopted to perform end-to-end survival prediction from medical images. Recent deep survival models achieved promising performance by jointly performing tumor segmentation with survival prediction, where the models were guided to extract tumor-related information through Multi-Task Learning (MTL). However, existing deep survival models have difficulties in exploring out-of-tumor prognostic information (e.g., local lymph node metastasis and adjacent tissue invasions). In addition, existing deep survival models are underdeveloped in utilizing multi-modality images (e.g., PET/CT). Empirically-designed strategies (e.g., early/late fusion) were commonly adopted to fuse multi-modality information via fixed pre-designed networks. In this study, we propose a Deep Multi-modality Segmentation-to-Survival model (DeepMSS) for survival prediction from PET/CT. Instead of adopting MTL, we propose a novel Segmentation-to-Survival Learning (SSL) strategy, where our DeepMSS is trained for tumor segmentation and survival prediction sequentially. This strategy enables the DeepMSS to initially focus on tumor regions and gradually expand its focus to include other prognosis-related regions. We also propose a data-driven strategy to fuse multi-modality image information, which realizes automatic optimization of fusion strategies based on training data during training and also improves the adaptability of DeepMSS to different training targets (i.e., survival prediction or tumor segmentation). Our DeepMSS is also capable of incorporating conventional radiomics features as an enhancement, where handcrafted features can be extracted from the DeepMSS-segmented tumor regions and cooperatively integrated into the DeepMSS\u2019s training and inference. Extensive experiments with two large clinical datasets (1,380 patients from nine medical centers) demonstrate that our DeepMSS outperforms state-of-the-art survival prediction methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "4852453",
                    "name": "B. Gu"
                },
                {
                    "authorId": "2110883",
                    "name": "M. Fulham"
                },
                {
                    "authorId": "4731432",
                    "name": "Shaoli Song"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "49117537",
                    "name": "Lei Bi"
                },
                {
                    "authorId": "2110005686",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "6f5beb87e5da0df8fd1ddf9fdd5c0a35f200b295",
            "title": "Semantic-Driven Global-Local Cooperative Contrastive Learning for Medical Report Generation",
            "abstract": "As the demand for radiology continues to increase, the shortage of specialized professionals becomes a challenging issue. Such shortage highlights the need to utilize advancements in artificial intelligence (AI) to automatically generate draft medical reports from radiology images. Recently, the application of contrastive learning has been leveraged in image-to-text generation because it allows the model to learn meaningful representations in latent space by contrasting similar and dissimilar image-text pairs. However, existing approaches to applying contrastive learning in medical report generation are limited by the following: 1) they are performed as an independent pretraining step, which hinders the cooperation between contrastive learning and the subsequent report generation step; 2) these methods are contingent on pairing images with their corresponding reports, thus establishing similarity based solely on this association. Such contingency inadvertently overlooks the situation where unpaired reports could also be relevant to a given image, thereby failing to accurately capture and understand the semantic relationships within the data; and 3) existing contrastive learning in medical report generation only utilizes the global representation, which cannot capture subtle but crucial local visual information. To address these limitations, we propose a Semantic-Driven Global-Local Cooperative Contrastive Learning Network (SGLCCNet), which integrates contrastive learning into the training process of report generation, enriched with semantic information extracted from reports and enhanced by the inclusion of local feature exploration. Extensive experiments on the IU-Xray dataset demonstrate that our method achieved the state-of-the-art. Further, we demonstrate how each of our proposed steps adds to the overall performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2281398403",
                    "name": "Shuchang Ye"
                },
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2281343537",
                    "name": "David Dagan Feng"
                },
                {
                    "authorId": "2280928438",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "867a2412fa03e4d8f8c88295b36b9833a8b547f9",
            "title": "Full-resolution MLPs Empower Medical Dense Prediction",
            "abstract": "Dense prediction is a fundamental requirement for many medical vision tasks such as medical image restoration, registration, and segmentation. The most popular vision model, Convolutional Neural Networks (CNNs), has reached bottlenecks due to the intrinsic locality of convolution operations. Recently, transformers have been widely adopted for dense prediction for their capability to capture long-range visual dependence. However, due to the high computational complexity and large memory consumption of self-attention operations, transformers are usually used at downsampled feature resolutions. Such usage cannot effectively leverage the tissue-level textural information available only at the full image resolution. This textural information is crucial for medical dense prediction as it can differentiate the subtle human anatomy in medical images. In this study, we hypothesize that Multi-layer Perceptrons (MLPs) are superior alternatives to transformers in medical dense prediction where tissue-level details dominate the performance, as MLPs enable long-range dependence at the full image resolution. To validate our hypothesis, we develop a full-resolution hierarchical MLP framework that uses MLPs beginning from the full image resolution. We evaluate this framework with various MLP blocks on a wide range of medical dense prediction tasks including restoration, registration, and segmentation. Extensive experiments on six public well-benchmarked datasets show that, by simply using MLPs at full resolution, our framework outperforms its CNN and transformer counterparts and achieves state-of-the-art performance on various medical dense prediction tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "2261387272",
                    "name": "Yuxin Xue"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "49117537",
                    "name": "Lei Bi"
                },
                {
                    "authorId": "2110005686",
                    "name": "Jinman Kim"
                }
            ]
        },
        {
            "paperId": "90a109068318d7c9d12e3ee887dfc846671f7ea9",
            "title": "AdaMSS: Adaptive Multi-Modality Segmentation-to-Survival Learning for Survival Outcome Prediction from PET/CT Images",
            "abstract": "Survival prediction is a major concern for cancer management. Deep survival models based on deep learning have been widely adopted to perform end-to-end survival prediction from medical images. Recent deep survival models achieved promising performance by jointly performing tumor segmentation with survival prediction, where the models were guided to extract tumor-related information through Multi-Task Learning (MTL). However, these deep survival models have difficulties in exploring out-of-tumor prognostic information. In addition, existing deep survival models are unable to effectively leverage multi-modality images. Empirically-designed fusion strategies were commonly adopted to fuse multi-modality information via task-specific manually-designed networks, thus limiting the adaptability to different scenarios. In this study, we propose an Adaptive Multi-modality Segmentation-to-Survival model (AdaMSS) for survival prediction from PET/CT images. Instead of adopting MTL, we propose a novel Segmentation-to-Survival Learning (SSL) strategy, where our AdaMSS is trained for tumor segmentation and survival prediction sequentially in two stages. This strategy enables the AdaMSS to focus on tumor regions in the first stage and gradually expand its focus to include other prognosis-related regions in the second stage. We also propose a data-driven strategy to fuse multi-modality information, which realizes adaptive optimization of fusion strategies based on training data during training. With the SSL and data-driven fusion strategies, our AdaMSS is designed as an adaptive model that can self-adapt its focus regions and fusion strategy for different training stages. Extensive experiments with two large clinical datasets show that our AdaMSS outperforms state-of-the-art survival prediction methods.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1477273148",
                    "name": "Mingyuan Meng"
                },
                {
                    "authorId": "4852453",
                    "name": "B. Gu"
                },
                {
                    "authorId": "2110883",
                    "name": "M. Fulham"
                },
                {
                    "authorId": "4731432",
                    "name": "Shaoli Song"
                },
                {
                    "authorId": "2064896467",
                    "name": "Da-wei Feng"
                },
                {
                    "authorId": "49117537",
                    "name": "Lei Bi"
                },
                {
                    "authorId": "2110005686",
                    "name": "Jinman Kim"
                }
            ]
        }
    ]
}