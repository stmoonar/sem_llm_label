{
    "authorId": "27083576",
    "papers": [
        {
            "paperId": "1b6a050b00c6b2bd75f1ee5ecc28d5f102446d3a",
            "title": "Offensive AI: Enhancing Directory Brute-forcing Attack with the Use of Language Models",
            "abstract": "Web Vulnerability Assessment and Penetration Testing (Web VAPT) is a comprehensive cybersecurity process that uncovers a range of vulnerabilities which, if exploited, could compromise the integrity of web applications. In a VAPT, it is common to perform a \\textit{Directory brute-forcing Attack}, aiming at the identification of accessible directories of a target website. Current commercial solutions are inefficient as they are based on brute-forcing strategies that use wordlists, resulting in enormous quantities of trials for a small amount of success. Offensive AI is a recent paradigm that integrates AI-based technologies in cyber attacks. In this work, we explore whether AI can enhance the directory enumeration process and propose a novel Language Model-based framework. Our experiments -- conducted in a testbed consisting of 1 million URLs from different web application domains (universities, hospitals, government, companies) -- demonstrate the superiority of the LM-based attack, with an average performance increase of 969%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2297770506",
                    "name": "Alberto Castagnaro"
                },
                {
                    "authorId": "2163151382",
                    "name": "Mauro Conti"
                },
                {
                    "authorId": "27083576",
                    "name": "Luca Pajola"
                }
            ]
        },
        {
            "paperId": "d737191136cf3418c341071e3bd379e073583181",
            "title": "\"All of Me\": Mining Users' Attributes from their Public Spotify Playlists",
            "abstract": "In the age of digital music streaming, playlists on platforms like Spotify have become an integral part of individuals' musical experiences. People create and publicly share their own playlists to express their musical tastes, promote the discovery of their favorite artists, and foster social connections. In this work, we aim to address the question: can we infer users' private attributes from their public Spotify playlists? To this end, we conducted an online survey involving 739 Spotify users, resulting in a dataset of 10,286 publicly shared playlists comprising over 200,000 unique songs and 55,000 artists. Then, we utilize statistical analyses and machine learning algorithms to build accurate predictive models for users' attributes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1943185469",
                    "name": "Pier Paolo Tricomi"
                },
                {
                    "authorId": "27083576",
                    "name": "Luca Pajola"
                },
                {
                    "authorId": "32246134",
                    "name": "Luca Pasa"
                },
                {
                    "authorId": "2163151382",
                    "name": "Mauro Conti"
                }
            ]
        },
        {
            "paperId": "f23151a50b06f745a320a9213d04d70c3de3c551",
            "title": "Can LLMs Understand Computer Networks? Towards a Virtual System Administrator",
            "abstract": "Recent advancements in Artificial Intelligence, and particularly Large Language Models (LLMs), offer promising prospects for aiding system administrators in managing the complexity of modern networks. However, despite this potential, a significant gap exists in the literature regarding the extent to which LLMs can understand computer networks. Without empirical evidence, system administrators might rely on these models without assurance of their efficacy in performing network-related tasks accurately. In this paper, we are the first to conduct an exhaustive study on LLMs' comprehension of computer networks. We formulate several research questions to determine whether LLMs can provide correct answers when supplied with a network topology and questions on it. To assess them, we developed a thorough framework for evaluating LLMs' capabilities in various network-related tasks. We evaluate our framework on multiple computer networks employing proprietary (e.g., GPT4) and open-source (e.g., Llama2) models. Our findings in general purpose LLMs using a zero-shot scenario demonstrate promising results, with the best model achieving an average accuracy of 79.3%. Proprietary LLMs achieve noteworthy results in small and medium networks, while challenges persist in comprehending complex network topologies, particularly for open-source models. Moreover, we provide insight into how prompt engineering can enhance the accuracy of some tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2047405916",
                    "name": "Denis Donadel"
                },
                {
                    "authorId": "2212329016",
                    "name": "Francesco Marchiori"
                },
                {
                    "authorId": "27083576",
                    "name": "Luca Pajola"
                },
                {
                    "authorId": "2238206673",
                    "name": "Mauro Conti"
                }
            ]
        },
        {
            "paperId": "1041c775e9c46d11c095cda09d682168f33e88a4",
            "title": "Social Honeypot for Humans: Luring People through Self-managed Instagram Pages",
            "abstract": "Social Honeypots are tools deployed in Online Social Networks (OSN) to attract malevolent activities performed by spammers and bots. To this end, their content is designed to be of maximum interest to malicious users. However, by choosing an appropriate content topic, this attractive mechanism could be extended to any OSN users, rather than only luring malicious actors. As a result, honeypots can be used to attract individuals interested in a wide range of topics, from sports and hobbies to more sensitive subjects like political views and conspiracies. With all these individuals gathered in one place, honeypot owners can conduct many analyses, from social to marketing studies. In this work, we introduce a novel concept of social honeypot for attracting OSN users interested in a generic target topic. We propose a framework based on fully-automated content generation strategies and engagement plans to mimic legit Instagram pages. To validate our framework, we created 21 self-managed social honeypots (i.e., pages) on Instagram, covering three topics, four content generation strategies, and three engaging plans. In nine weeks, our honeypots gathered a total of 753 followers, 5387 comments, and 15739 likes. These results demonstrate the validity of our approach, and through statistical analysis, we examine the characteristics of effective social honeypots.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2182460960",
                    "name": "Sara Bardi"
                },
                {
                    "authorId": "145746490",
                    "name": "M. Conti"
                },
                {
                    "authorId": "27083576",
                    "name": "Luca Pajola"
                },
                {
                    "authorId": "1943185469",
                    "name": "Pier Paolo Tricomi"
                }
            ]
        },
        {
            "paperId": "68bef6825f33d6ea65be6d7cc8c5ae5243a7d811",
            "title": "Invisible Threats: Backdoor Attack in OCR Systems",
            "abstract": "Optical Character Recognition (OCR) is a widely used tool to extract text from scanned documents. Today, the state-of-the-art is achieved by exploiting deep neural networks. However, the cost of this performance is paid at the price of system vulnerability. For instance, in backdoor attacks, attackers compromise the training phase by inserting a backdoor in the victim's model that will be activated at testing time by specific patterns while leaving the overall model performance intact. This work proposes a backdoor attack for OCR resulting in the injection of non-readable characters from malicious input images. This simple but effective attack exposes the state-of-the-art OCR weakness, making the extracted text correct to human eyes but simultaneously unusable for the NLP application that uses OCR as a preprocessing step. Experimental results show that the attacked models successfully output non-readable characters for around 90% of the poisoned instances without harming their performance for the remaining instances.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2257348388",
                    "name": "Mauro Conti"
                },
                {
                    "authorId": "2257348285",
                    "name": "Nicola Farronato"
                },
                {
                    "authorId": "88073197",
                    "name": "Stefanos Koffas"
                },
                {
                    "authorId": "27083576",
                    "name": "Luca Pajola"
                },
                {
                    "authorId": "1686538",
                    "name": "S. Picek"
                }
            ]
        },
        {
            "paperId": "85f99102eff38ab5eb56c9128dcf8ad582e23f91",
            "title": "Your Attack Is Too DUMB: Formalizing Attacker Scenarios for Adversarial Transferability",
            "abstract": "Evasion attacks are a threat to machine learning models, where adversaries attempt to affect classifiers by injecting malicious samples. An alarming side-effect of evasion attacks is their ability to transfer among different models: this property is called transferability. Therefore, an attacker can produce adversarial samples on a custom model (surrogate) to conduct the attack on a victim\u2019s organization later. Although literature widely discusses how adversaries can transfer their attacks, their experimental settings are limited and far from reality. For instance, many experiments consider both attacker and defender sharing the same dataset, balance level (i.e., how the ground truth is distributed), and model architecture. In this work, we propose the DUMB attacker model. This framework allows analyzing if evasion attacks fail to transfer when the training conditions of surrogate and victim models differ. DUMB considers the following conditions: Dataset soUrces, Model architecture, and the Balance of the ground truth. We then propose a novel testbed to evaluate many state-of-the-art evasion attacks with DUMB; the testbed consists of three computer vision tasks with two distinct datasets each, four types of balance levels, and three model architectures. Our analysis, which generated 13K tests over 14 distinct attacks, led to numerous novel findings in the scope of transferable attacks with surrogate models. In particular, mismatches between attackers and victims in terms of dataset source, balance levels, and model architecture lead to non-negligible loss of attack performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2092593204",
                    "name": "Marco Alecci"
                },
                {
                    "authorId": "145746490",
                    "name": "M. Conti"
                },
                {
                    "authorId": "2212329016",
                    "name": "Francesco Marchiori"
                },
                {
                    "authorId": "1411739102",
                    "name": "L. Martinelli"
                },
                {
                    "authorId": "27083576",
                    "name": "Luca Pajola"
                }
            ]
        },
        {
            "paperId": "90b67d69725cbf5a337a8042cfe6d1c27a0c0f78",
            "title": "Boosting Big Brother: Attacking Search Engines with Encodings",
            "abstract": "Search engines are vulnerable to attacks against indexing and searching via text encoding manipulation. By imperceptibly perturbing text using uncommon encoded representations, adversaries can control results across search engines for specific search queries. We demonstrate that this attack is successful against two major commercial search engines - Google and Bing - and one open source search engine - Elasticsearch. We further demonstrate that this attack is successful against LLM chat search including Bing\u2019s GPT-4 chatbot and Google\u2019s Bard chatbot. We also present a variant of the attack targeting text summarization and plagiarism detection models, two ML tasks closely tied to search. We provide a set of defenses against these techniques and warn that adversaries can leverage these attacks to launch disinformation campaigns against unsuspecting users, motivating the need for search engine maintainers to patch deployed systems.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2136215568",
                    "name": "Nicholas Boucher"
                },
                {
                    "authorId": "27083576",
                    "name": "Luca Pajola"
                },
                {
                    "authorId": "47473421",
                    "name": "Ilia Shumailov"
                },
                {
                    "authorId": "40171469",
                    "name": "Ross Anderson"
                },
                {
                    "authorId": "145746490",
                    "name": "M. Conti"
                }
            ]
        },
        {
            "paperId": "b3549bc2db77115cf4fab5996136dc3de30a8a06",
            "title": "The Impact of Covid-19 on Online Discussions: the Case Study of the Sanctioned Suicide Forum",
            "abstract": "The COVID-19 pandemic has been at the center of the lives of many of us for at least a couple of years, during which periods of isolation and lockdowns were common. How all that affected our mental well-being, especially the ones\u2019 who were already in distress? To investigate the matter we analyse the online discussions on Sanctioned Suicide, a forum where users discuss suicide-related topics freely. We collected discussions starting from March 2018 (before pandemic) up to July 2022, for a total of 53K threads with 700K comments and 16K users. We investigate the impact of COVID-19 on the discussions in the forum. The data show that covid, while being present in the discussions, especially during the first lockdown, has not been the main reason why new users registered to the forum. However, covid appears to be indirectly connected to other causes of distress for the users, i.e. anxiety for the economy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2215463479",
                    "name": "Elisa Sartori"
                },
                {
                    "authorId": "27083576",
                    "name": "Luca Pajola"
                },
                {
                    "authorId": "134000266",
                    "name": "Giovanni Da San Martino"
                },
                {
                    "authorId": "2060071228",
                    "name": "M. Conti"
                }
            ]
        },
        {
            "paperId": "da5ffa6b7855385c6bf79ad258d54cf61e218cf3",
            "title": "A Novel Review Helpfulness Measure Based on the User-Review-Item Paradigm",
            "abstract": "Review platforms are viral online services where users share and read opinions about products (e.g., a smartphone) or experiences (e.g., a meal at a restaurant). Other users may be influenced by such opinions when deciding what to buy. The usability of review platforms is currently limited by the massive number of opinions on many products. Therefore, showing only the most helpful reviews for each product is in the best interest of both users and the platform (e.g., Amazon). The current state of the art is far from accurate in predicting how helpful a review is. First, most existing works lack compelling comparisons as many studies are conducted on datasets that are not publicly available. As a consequence, new studies are not always built on top of prior baselines. Second, most existing research focuses only on features derived from the review text, ignoring other fundamental aspects of the review platforms (e.g., the other reviews of a product, the order in which they were submitted). In this article, we first carefully review the most relevant works in the area published during the last 20 years. We then propose the User-Review-Item (URI) paradigm, a novel abstraction for modeling the problem that moves the focus of the feature engineering from the review to the platform level. We empirically validate the URI paradigm on a dataset of products from six Amazon categories with 270 trained models: on average, classifiers gain +4% in F1-score when considering the whole review platform context. In our experiments, we further emphasize some problems with the helpfulness prediction task: (1) the users\u2019 writing style changes over time (i.e., concept drift), (2) past models do not generalize well across different review categories, and (3) past methods to generate the ground truth produced unreliable helpfulness scores, affecting the model evaluation phase.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "27083576",
                    "name": "Luca Pajola"
                },
                {
                    "authorId": "16049481",
                    "name": "Dongkai Chen"
                },
                {
                    "authorId": "2060071228",
                    "name": "M. Conti"
                },
                {
                    "authorId": "1728462",
                    "name": "V. S. Subrahmanian"
                }
            ]
        },
        {
            "paperId": "0752bf2c18f8314ead06c3ab02ff544891ac0e5e",
            "title": "You Can\u2019t Hide Behind Your Headset: User Profiling in Augmented and Virtual Reality",
            "abstract": "Augmented and Virtual Reality (AR and VR), collectively known as Extended Reality (XR), are increasingly gaining traction thanks to their technical advancement and the need for remote connections, recently accentuated by the pandemic. Remote surgery, telerobotics, and virtual offices are only some examples of their successes. As users interact with XR, they generate extensive behavioral data usually leveraged for measuring human activity, which could be used for profiling users\u2019 identities or personal information (e.g., gender). However, several factors affect the efficiency of profiling, such as the technology employed, the action taken, the mental workload, the presence of bias, and the sensors available. To date, no study has considered all of these factors together and in their entirety, limiting the current understanding of XR profiling. In this work, we provide a comprehensive study on user profiling in virtual technologies (i.e., AR, VR). Specifically, we employ machine learning on behavioral data (i.e., head, controllers, and eye data) to identify users and infer their individual attributes (i.e., age, gender). Toward this end, we propose a general framework that can potentially infer any personal information from any virtual scenarios. We test our framework on eleven generic actions (e.g., walking, searching, pointing) involving low and high mental loads, derived from two distinct use cases: an AR everyday application (34 participants) and VR robot teleoperation (35 participants). Our framework limits the burden of creating technology- and action-dependent algorithms, also reducing the experimental bias evidenced in previous work, providing a simple (yet effective) baseline for future works. We identified users up to 97% F1-score in VR and 80% in AR. Gender and Age inference was also facilitated in VR, reaching up to 82% and 90% F1-score, respectively. Through an in-depth analysis of sensors\u2019 impact, we found VR profiling resulting more effective than AR mainly because of the eye sensors\u2019 presence.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1943185469",
                    "name": "Pier Paolo Tricomi"
                },
                {
                    "authorId": "1686954462",
                    "name": "Federica Nenna"
                },
                {
                    "authorId": "27083576",
                    "name": "Luca Pajola"
                },
                {
                    "authorId": "145746490",
                    "name": "M. Conti"
                },
                {
                    "authorId": "2204603741",
                    "name": "Luciano Gamberi"
                }
            ]
        }
    ]
}