{
    "authorId": "2814303",
    "papers": [
        {
            "paperId": "0dd88453703f0008019a9a55a364064f0e8aa5d0",
            "title": "PV2TEA: Patching Visual Modality to Textual-Established Information Extraction",
            "abstract": "Information extraction, e.g., attribute value extraction, has been extensively studied and formulated based only on text. However, many attributes can benefit from image-based extraction, like color, shape, pattern, among others. The visual modality has long been underutilized, mainly due to multimodal annotation difficulty. In this paper, we aim to patch the visual modality to the textual-established attribute information extractor. The cross-modality integration faces several unique challenges: (C1) images and textual descriptions are loosely paired intra-sample and inter-samples; (C2) images usually contain rich backgrounds that can mislead the prediction; (C3) weakly supervised labels from textual-established extractors are biased for multimodal training. We present PV2TEA, an encoder-decoder architecture equipped with three bias reduction schemes: (S1) Augmented label-smoothed contrast to improve the cross-modality alignment for loosely-paired image and text; (S2) Attention-pruning that adaptively distinguishes the visual foreground; (S3) Two-level neighborhood regularization that mitigates the label textual bias via reliability estimation. Empirical results on real-world e-Commerce datasets demonstrate up to 11.74% absolute (20.97% relatively) F1 increase over unimodal baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112821580",
                    "name": "Hejie Cui"
                },
                {
                    "authorId": "10035476",
                    "name": "Rongmei Lin"
                },
                {
                    "authorId": "2814303",
                    "name": "Nasser Zalmout"
                },
                {
                    "authorId": "2047145237",
                    "name": "Chenwei Zhang"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "1390553618",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2157096355",
                    "name": "Xian Li"
                }
            ]
        },
        {
            "paperId": "7b49d8ef1593dabfe37bbd5e40a6f53f0cbe8709",
            "title": "Prototype-Representations for Training Data Filtering in Weakly-Supervised Information Extraction",
            "abstract": "The availability of high quality training data is still a bottleneck for the practical utilization of information extraction models, despite the breakthroughs in zero and few-shot learning techniques. This is further exacerbated for industry applications, where new tasks, domains, and specific use cases keep arising, which makes it impractical to depend on manually annotated data. Therefore, weak and distant supervision emerged as popular approaches to bootstrap training, utilizing labeling functions to guide the annotation process. Weakly-supervised annotation of training data is fast and efficient, however, it results in many irrelevant and out-of-context matches. This is a challenging problem that can degrade the performance in downstream models, or require a manual data cleaning step that can incur significant overhead. In this paper we present a prototype-based filtering approach, that can be utilized to denoise weakly supervised training data. The system is very simple, unsupervised, scalable, and requires little manual intervention, yet results in significant precision gains. We apply the technique in the task of attribute value extraction in e-commerce websites, and achieve up to 9% gain in precision for the downstream models, with a minimal drop in recall.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2814303",
                    "name": "Nasser Zalmout"
                },
                {
                    "authorId": "2116235416",
                    "name": "Xian Li"
                }
            ]
        },
        {
            "paperId": "f2396e95d152fb212d2f18102bac5c5453519bc2",
            "title": "Ask-and-Verify: Span Candidate Generation and Verification for Attribute Value Extraction",
            "abstract": "The product attribute value extraction (AVE) task aims to capture key factual information from product profiles, and is useful for several downstream applications in e-Commerce platforms. Previous contributions usually formulate this task using sequence labeling or reading comprehension architectures. However, sequence labeling models tend to be conservative in their predictions resulting in a high false negative rate. Existing reading comprehension formulations, on the other hand, can over-generate attribute values which hinders precision. In the present work we address these limitations with a new end-to-end pipeline framework called Ask-and-Verify. Given a product and an attribute query, the Ask step detects the top-K span candidates ( i.e. , possible attribute values) from the product profiles, then the Verify step filters out false positive candidates. We evaluate Ask-and-Verify model on Amazon\u2019s product pages and AliExpress public dataset, and present a comparative analysis as well as a detailed ablation study. Despite its simplicity, we show that Ask-and-Verify outperforms recent state-of-the-art models by up to 3.1% F1 absolute improvement points, while also scaling to thousands of attributes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144506371",
                    "name": "Yifan Ding"
                },
                {
                    "authorId": "145461906",
                    "name": "Yan Liang"
                },
                {
                    "authorId": "2814303",
                    "name": "Nasser Zalmout"
                },
                {
                    "authorId": "2157096328",
                    "name": "Xian Li"
                },
                {
                    "authorId": "4985351",
                    "name": "Christan Earl Grant"
                },
                {
                    "authorId": "1714166",
                    "name": "Tim Weninger"
                }
            ]
        },
        {
            "paperId": "18f230688bf37a56598474f96e081c2327f9a554",
            "title": "All You Need to Know to Build a Product Knowledge Graph",
            "abstract": "Knowledge graphs have been pivotal in supporting downstream applications like search, recommendation, and question answering, among others. Therefore, knowledge graphs have naturally become key enabling technologies in e-Commerce platforms. Developing a high coverage product knowledge graph is more challenging than generic knowledge graphs. The highly specific and complex domain, the sparsity of training data, along with the dynamic taxonomies and product types, can constrain the resulting knowledge graphs. In this tutorial we present best practices and ML innovations in industry towards building a scalable product knowledge graph. Contributions in this domain benefit from the general literature in areas including information extraction and data mining, tailored to address the specific characteristics of e-Commerce platforms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2814303",
                    "name": "Nasser Zalmout"
                },
                {
                    "authorId": "2047145237",
                    "name": "Chenwei Zhang"
                },
                {
                    "authorId": "2116235540",
                    "name": "Xian Li"
                },
                {
                    "authorId": "2116799460",
                    "name": "Yan Liang"
                },
                {
                    "authorId": "2143917898",
                    "name": "Xin Dong"
                }
            ]
        },
        {
            "paperId": "5841743b05a5cbab84c81194620eeb0321493cd2",
            "title": "End-to-End Conversational Search for Online Shopping with Utterance Transfer",
            "abstract": "Successful conversational search systems can present natural, adaptive and interactive shopping experience for online shopping customers. However, building such systems from scratch faces real word challenges from both imperfect product schema/knowledge and lack of training dialog data. In this work we first propose ConvSearch, an end-to-end conversational search system that deeply combines the dialog system with search. It leverages the text profile to retrieve products, which is more robust against imperfect product schema/knowledge compared with using product attributes alone. We then address the lack of data challenges by proposing an utterance transfer approach that generates dialogue utterances by using existing dialog from other domains, and leveraging the search behavior data from e-commerce retailer. With utterance transfer, we introduce a new conversational search dataset for online shopping. Experiments show that our utterance transfer method can significantly improve the availability of training dialogue data without crowd-sourcing, and the conversational search system significantly outperformed the best tested baseline.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2149551771",
                    "name": "Liqiang Xiao"
                },
                {
                    "authorId": "65743795",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "2143918523",
                    "name": "Xin Dong"
                },
                {
                    "authorId": "1401946168",
                    "name": "Pascual Mart\u00ednez-G\u00f3mez"
                },
                {
                    "authorId": "2814303",
                    "name": "Nasser Zalmout"
                },
                {
                    "authorId": "2154941685",
                    "name": "Wei Chen"
                },
                {
                    "authorId": "2153707145",
                    "name": "Tong Zhao"
                },
                {
                    "authorId": "144111414",
                    "name": "Hao He"
                },
                {
                    "authorId": "35692109",
                    "name": "Yaohui Jin"
                }
            ]
        },
        {
            "paperId": "72a7105465aa8d84161eef602fcbe43b0e9f1b35",
            "title": "PAM: Understanding Product Images in Cross Product Category Attribute Extraction",
            "abstract": "Understanding product attributes plays an important role in improving online shopping experience for customers and serves asan integral part for constructing a product knowledge graph. Most existing methods focus on attribute extraction from text description or utilize visual information from product images such as shape and color. Compared to the inputs considered in prior works, a product image in fact contains more information, represented by a rich mixture of words and visual clues with a layout carefully designed to impress customers. This work proposes a more inclusive framework that fully utilizes these different modalities for attribute extraction.Inspired by recent works in visual question answering, we use a transformer based sequence to sequence model to fuse representations of product text, Optical Character Recognition (OCR) tokens and visual objects detected in the product image. The framework is further extended with the capability to extract attribute value across multiple product categories with a single model, by training the decoder to predict both product category and attribute value and conditioning its output on product category. The model provides a unified attribute extraction solution desirable at an e-commerce platform that offers numerous product categories with a diverse body of product attributes. We evaluated the model on two product attributes, one with many possible values and one with a small set of possible values, over 14 product categories and found the model could achieve 15% gain on the Recall and 10% gain on the F1 score compared to existing methods using text-only features.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10035476",
                    "name": "Rongmei Lin"
                },
                {
                    "authorId": "2111080293",
                    "name": "Xiang He"
                },
                {
                    "authorId": "2109020722",
                    "name": "J. Feng"
                },
                {
                    "authorId": "2814303",
                    "name": "Nasser Zalmout"
                },
                {
                    "authorId": "2116799460",
                    "name": "Yan Liang"
                },
                {
                    "authorId": "2068239517",
                    "name": "Li Xiong"
                },
                {
                    "authorId": "2143917898",
                    "name": "Xin Dong"
                }
            ]
        },
        {
            "paperId": "b2eec2dc3c870b2d3f37564579c45434be128383",
            "title": "AdaTag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding",
            "abstract": "Automatic extraction of product attribute values is an important enabling technology in e-Commerce platforms. This task is usually modeled using sequence labeling architectures, with several extensions to handle multi-attribute extraction. One line of previous work constructs attribute-specific models, through separate decoders or entirely separate models. However, this approach constrains knowledge sharing across different attributes. Other contributions use a single multi-attribute model, with different techniques to embed attribute information. But sharing the entire network parameters across all attributes can limit the model\u2019s capacity to capture attribute-specific characteristics. In this paper we present AdaTag, which uses adaptive decoding to handle extraction. We parameterize the decoder with pretrained attribute embeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module. This allows for separate, but semantically correlated, decoders to be generated on the fly for different attributes. This approach facilitates knowledge sharing, while maintaining the specificity of each attribute. Our experiments on a real-world e-Commerce dataset show marked improvements over previous methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49781448",
                    "name": "Jun Yan"
                },
                {
                    "authorId": "2814303",
                    "name": "Nasser Zalmout"
                },
                {
                    "authorId": "145461906",
                    "name": "Yan Liang"
                },
                {
                    "authorId": "4985351",
                    "name": "Christan Earl Grant"
                },
                {
                    "authorId": "145201124",
                    "name": "Xiang Ren"
                },
                {
                    "authorId": "2143917898",
                    "name": "Xin Dong"
                }
            ]
        },
        {
            "paperId": "77f7cf34e1914c5c8892e813d71239fc6e13c666",
            "title": "Morphological Analysis and Disambiguation for Gulf Arabic: The Interplay between Resources and Methods",
            "abstract": "In this paper we present the first full morphological analysis and disambiguation system for Gulf Arabic. We use an existing state-of-the-art morphological disambiguation system to investigate the effects of different data sizes and different combinations of morphological analyzers for Modern Standard Arabic, Egyptian Arabic, and Gulf Arabic. We find that in very low settings, morphological analyzers help boost the performance of the full morphological disambiguation task. However, as the size of resources increase, the value of the morphological analyzers decreases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48040328",
                    "name": "Salam Khalifa"
                },
                {
                    "authorId": "2814303",
                    "name": "Nasser Zalmout"
                },
                {
                    "authorId": "1696645",
                    "name": "Nizar Habash"
                }
            ]
        },
        {
            "paperId": "995ec006ac98a697ea38bd4eea8c1f3170a8adb4",
            "title": "CAMeL Tools: An Open Source Python Toolkit for Arabic Natural Language Processing",
            "abstract": "We present CAMeL Tools, a collection of open-source tools for Arabic natural language processing in Python. CAMeL Tools currently provides utilities for pre-processing, morphological modeling, Dialect Identification, Named Entity Recognition and Sentiment Analysis. In this paper, we describe the design of CAMeL Tools and the functionalities it provides.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "40010893",
                    "name": "Ossama Obeid"
                },
                {
                    "authorId": "2814303",
                    "name": "Nasser Zalmout"
                },
                {
                    "authorId": "48040328",
                    "name": "Salam Khalifa"
                },
                {
                    "authorId": "3405525",
                    "name": "Dima Taji"
                },
                {
                    "authorId": "2420100",
                    "name": "Mai Oudah"
                },
                {
                    "authorId": "66589548",
                    "name": "Bashar Alhafni"
                },
                {
                    "authorId": "46915471",
                    "name": "Go Inoue"
                },
                {
                    "authorId": "41127087",
                    "name": "Fadhl Eryani"
                },
                {
                    "authorId": "36785672",
                    "name": "Alexander Erdmann"
                },
                {
                    "authorId": "1696645",
                    "name": "Nizar Habash"
                }
            ]
        },
        {
            "paperId": "fb620d79c220ffe2903c940e0e71049533c9d4c5",
            "title": "Utilizing Subword Entities in Character-Level Sequence-to-Sequence Lemmatization Models",
            "abstract": "In this paper we present a character-level sequence-to-sequence lemmatization model, utilizing several subword features in multiple configurations. In addition to generic n-gram embeddings (using FastText), we experiment with concatenative (stems) and templatic (roots and patterns) morphological subwords. We present several architectures that embed these features directly at the encoder side, or learn them jointly at the decoder side with a multitask learning architecture. The results indicate that using the generic n-gram embeddings (through FastText) outperform the other linguistically-driven subwords. We use Modern Standard Arabic and Egyptian Arabic as test cases, with up to 22% and 13% relative error reduction, respectively, from a strong baseline. An error analysis shows that our best system is even able to handle word/lemma pairs that are both unseen in the training data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2814303",
                    "name": "Nasser Zalmout"
                },
                {
                    "authorId": "1696645",
                    "name": "Nizar Habash"
                }
            ]
        }
    ]
}