{
    "authorId": "1943275076",
    "papers": [
        {
            "paperId": "b59527b089c31feafac422cd559396dc3c2f0b37",
            "title": "MM-EMOG: Multi-Label Emotion Graph Representation for Mental Health Classification on Social Media",
            "abstract": "More than 80% of people who commit suicide disclose their intention to do so on social media. The main information we can use in social media is user-generated posts, since personal information is not always available. Identifying all possible emotions in a single textual post is crucial to detecting the user\u2019s mental state; however, human emotions are very complex, and a single text instance likely expresses multiple emotions. This paper proposes a new multi-label emotion graph representation for social media post-based mental health classification. We first construct a word\u2013document graph tensor to describe emotion-based contextual representation using emotion lexicons. Then, it is trained by multi-label emotions and conducts a graph propagation for harmonising heterogeneous emotional information, and is applied to a textual graph mental health classification. We perform extensive experiments on three publicly available social media mental health classification datasets, and the results show clear improvements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2051060951",
                    "name": "R. Cabral"
                },
                {
                    "authorId": "2046142",
                    "name": "S. Han"
                },
                {
                    "authorId": "144179461",
                    "name": "Josiah Poon"
                },
                {
                    "authorId": "1943275076",
                    "name": "G. Nenadic"
                }
            ]
        },
        {
            "paperId": "fb022333ebbd1ccf8ca38f70ac0956080b09be82",
            "title": "Exploring the Consistency, Quality and Challenges in Manual and Automated Coding of Free-text Diagnoses from Hospital Outpatient Letters",
            "abstract": "Coding of unstructured clinical free-text to produce interoperable structured data is essential to improve direct care, support clinical communication and to enable clinical research.However, manual clinical coding is difficult and time consuming, which motivates the development and use of natural language processing for automated coding. This work evaluates the quality and consistency of both manual and automated clinical coding of diagnoses from hospital outpatient letters. Using 100 randomly selected letters, two human clinicians performed coding of diagnosis lists to SNOMED CT. Automated coding was also performed using IMO's Concept Tagger. A gold standard was constructed by a panel of clinicians from a subset of the annotated diagnoses. This was used to evaluate the quality and consistency of both manual and automated coding via (1) a distance-based metric, treating SNOMED CT as a graph, and (2) a qualitative metric agreed upon by the panel of clinicians. Correlation between the two metrics was also evaluated. Comparing human and computer-generated codes to the gold standard, the results indicate that humans slightly out-performed automated coding, while both performed notably better when there was only a single diagnosis contained in the free-text description. Automated coding was considered acceptable by the panel of clinicians in approximately 90% of cases.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2264380048",
                    "name": "Warren Del-Pinto"
                },
                {
                    "authorId": "1944043",
                    "name": "G. Demetriou"
                },
                {
                    "authorId": "35458796",
                    "name": "M. Jani"
                },
                {
                    "authorId": "2267375381",
                    "name": "Rikesh Patel"
                },
                {
                    "authorId": "2267342616",
                    "name": "Leanne Gray"
                },
                {
                    "authorId": "2049019554",
                    "name": "Alexander Bulcock"
                },
                {
                    "authorId": "2267341668",
                    "name": "Niels Peek"
                },
                {
                    "authorId": "2156722007",
                    "name": "A. Kanter"
                },
                {
                    "authorId": "2267341752",
                    "name": "William G Dixon"
                },
                {
                    "authorId": "1943275076",
                    "name": "G. Nenadic"
                }
            ]
        },
        {
            "paperId": "a0e2a6d1a74d62c12f6b60f5e777aa7bc2434472",
            "title": "A Text Mining Model for Answering Checklist Questions Automatically from Parasitology Literature",
            "abstract": "Complete reporting of Experimental Meta-data (EM) is necessary for reproducing and understanding biomedical experiments and results. Experimental Metadata Reporting Checklist Questions (EMR-CLQs) have been designed and used by journals as guidelines to capture EM and evaluate the quality of the reporting. Automatically answering EMR-CLQs is necessary to check completeness and clarity of EM, which can be useful for the peer-review process. Moreover, automatically extracting the EMR-CLQs answers can be used to search the relevant literature for the meta-data analysis process in an efficient way. This paper shows the possibility of answering different types of EMR-CLQs automatically by understanding the structure of both EMR-CLQs and the biomedical article. A text mining model (rule-based approach) based on the information extraction techniques and the structure of the biomedical articles and the EMR-CLQs, is proposed as a first model in the biomedical reproducibility domain to answer EMR-CLQs automatically. The model was used to answer five EMR-CLQs of two different types automatically; Main and Attribute questions. We evaluated the feasibility of the model against gold-standard data of 58 full-text articles annotated by domain experts. The results are showing the possibility of answering the EMR-CLQs automatically with a mean f-measure of 75% and 73% for development and testing datasets, respectively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1388009478",
                    "name": "Haifa Alrdahi"
                },
                {
                    "authorId": "116930644",
                    "name": "U. Sattler"
                },
                {
                    "authorId": "1943275076",
                    "name": "G. Nenadic"
                }
            ]
        }
    ]
}