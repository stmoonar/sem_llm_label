{
    "authorId": "2786889",
    "papers": [
        {
            "paperId": "110627219174919ab2c18126cca9af751b8cc1ba",
            "title": "Segmentation and tracking of vegetable plants by exploiting vegetable shape feature for precision spray of agricultural robots",
            "abstract": "For robotic precision spray application in vegetable farms, simultaneous accurate instance segmentation and robust tracking of plants are of great importance and a prerequisite for the following spray action. With onboard cameras, agricultural robots can apply Multiple Object Tracking and Segmentation (MOTS) methods, for instance, segmentation and tracking of plants. By assigning a unique identification for each vegetable, it ensures the robot to spray each vegetable exactly once, while traversing along the farm rows. Conventional MOTS methods, which are mostly designed for tracking pedestrians or vehicles, usually extract their color and texture features for associating different targets in consecutive images. However, vegetable plants of the same species normally show similar color and texture, which leads to degraded performance when conventional MOTS methods are used. To solve the challenging problem of associating vegetables with similar color and texture in consecutive images, in this paper, a novel MOTS method that exploits contour and blob features is proposed, for instance, segmentation and tracking of multiple vegetable plants. The method takes advantage of the fact that different plants normally possess different shape contours and blob properties. With images captured on top of them, these features of the same plant show little difference in consecutively captured images. Comprehensive experiments including ablation studies are conducted, which prove its superior performance over two state\u2010of\u2010the\u2010art MOTS methods. Compared with the conventional MOTS methods, the proposed method is able to re\u2010identify objects which have gone out of the camera field of view and re\u2010appear again using the proposed data association strategy, which is important to ensure each vegetable be sprayed only once when the robot travels back and forth. Although the method is tested on lettuce farm, it can be applied to other similar vegetables, such as broccoli and canola. Both the code and the dataset of this paper are publicly released for the benefit of the community: https://github.com/NanH5837/LettuceMOTS.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2056375114",
                    "name": "Nan Hu"
                },
                {
                    "authorId": "34345924",
                    "name": "Daobilige Su"
                },
                {
                    "authorId": "2186520629",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2144705246",
                    "name": "Xuechang Wang"
                },
                {
                    "authorId": "40062930",
                    "name": "H. Zhong"
                },
                {
                    "authorId": "2220579510",
                    "name": "Zimeng Wang"
                },
                {
                    "authorId": "2786889",
                    "name": "Yongliang Qiao"
                },
                {
                    "authorId": "2211452239",
                    "name": "Yu Tan"
                }
            ]
        },
        {
            "paperId": "4885a3f1bb3aceea208df1b92af5025ea72158df",
            "title": "Contrastive Graph Similarity Networks",
            "abstract": "Graph similarity learning is a significant and fundamental issue in the theory and analysis of graphs, which has been applied in a variety of fields, including object tracking, recommender systems, similarity search, and so on. Recent methods for graph similarity learning that utilize deep learning typically share two deficiencies: (1) they leverage graph neural networks as backbones for learning graph representations but have not well captured the complex information inside data, and (2) they employ a cross-graph attention mechanism for graph similarity learning, which is computationally expensive. Taking these limitations into consideration, a method for graph similarity learning is devised in this study, namely, Contrastive Graph Similarity Network (CGSim). To enhance graph similarity learning, CGSim makes use of the complementary information of two input graphs and captures pairwise relations in a contrastive learning framework. By developing a dual contrastive learning module with a node-graph matching and a graph-graph matching mechanism, our method significantly reduces the quadratic time complexity for cross-graph interaction modeling to linear time complexity. Jointly learning in an end-to-end framework, the graph representation embedding module and the well-designed contrastive learning module can be beneficial to one another. A comprehensive series of experiments indicate that CGSim outperforms state-of-the-art baselines on six datasets and significantly reduces the computational cost, which demonstrates our CGSim model\u2019s superiority over other baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "82527705",
                    "name": "Luzhi Wang"
                },
                {
                    "authorId": "26956796",
                    "name": "Yizhen Zheng"
                },
                {
                    "authorId": "1860892",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2203372264",
                    "name": "Fuyi Li"
                },
                {
                    "authorId": "2786889",
                    "name": "Yongliang Qiao"
                },
                {
                    "authorId": "2153326034",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "464be5f95b4704d91484ae05f55bd201ffc0baad",
            "title": "The Research Progress of Vision-Based Artificial Intelligence in Smart Pig Farming",
            "abstract": "Pork accounts for an important proportion of livestock products. For pig farming, a lot of manpower, material resources and time are required to monitor pig health and welfare. As the number of pigs in farming increases, the continued use of traditional monitoring methods may cause stress and harm to pigs and farmers and affect pig health and welfare as well as farming economic output. In addition, the application of artificial intelligence has become a core part of smart pig farming. The precision pig farming system uses sensors such as cameras and radio frequency identification to monitor biometric information such as pig sound and pig behavior in real-time and convert them into key indicators of pig health and welfare. By analyzing the key indicators, problems in pig health and welfare can be detected early, and timely intervention and treatment can be provided, which helps to improve the production and economic efficiency of pig farming. This paper studies more than 150 papers on precision pig farming and summarizes and evaluates the application of artificial intelligence technologies to pig detection, tracking, behavior recognition and sound recognition. Finally, we summarize and discuss the opportunities and challenges of precision pig farming.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Shunli Wang"
                },
                {
                    "authorId": "2158160448",
                    "name": "Honghua Jiang"
                },
                {
                    "authorId": "2786889",
                    "name": "Yongliang Qiao"
                },
                {
                    "authorId": "2142580863",
                    "name": "Shuzhen Jiang"
                },
                {
                    "authorId": "2183735430",
                    "name": "Huaiqin Lin"
                },
                {
                    "authorId": "2183716876",
                    "name": "Qian Sun"
                }
            ]
        },
        {
            "paperId": "d13912fc63a3fa3e97a11d89449cdde8e12367f8",
            "title": "One-Shot Learning-Based Animal Video Segmentation",
            "abstract": "Deep learning-based video segmentation methods can offer a good performance after being trained on the large-scale pixel labeled datasets. However, a pixel-wise manual labeling of animal images is challenging and time consuming due to irregular contours and motion blur. To achieve desirable tradeoffs between the accuracy and speed, a novel one-shot learning-based approach is proposed in this article to segment animal video with only one labeled frame. The proposed approach consists of the following three main modules: guidance frame selection utilizes \u201cBubbleNet\u201d to choose one frame for manual labeling, which can leverage the fine-tuning effects of the only labeled frame; Xception-based fully convolutional network localizes dense prediction using depthwise separable convolutions based on one single labeled frame; and postprocessing is used to remove outliers and sharpen object contours, which consists of two submodules\u2014test time augmentation and conditional random field. Extensive experiments have been conducted on the DAVIS 2016 animal dataset. Our proposed video segmentation approach achieved mean intersection-over-union score of 89.5% on the DAVIS 2016 animal dataset with less run time, and outperformed the state-of-art methods (OSVOS and OSMN). The proposed one-shot learning-based approach achieves real-time and automatic segmentation of animals with only one labeled video frame. This can be potentially used further as a baseline for intelligent perception-based monitoring of animals and other domain-specific applications.11The source code, datasets, and pre-trained weights for this work are publicly [Online]. Available: https://github.com/tengfeixue-victor/One-Shot-Animal-Video-Segmentation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2141573247",
                    "name": "Tengfei Xue"
                },
                {
                    "authorId": "2786889",
                    "name": "Yongliang Qiao"
                },
                {
                    "authorId": "2143672423",
                    "name": "He Kong"
                },
                {
                    "authorId": "34345924",
                    "name": "Daobilige Su"
                },
                {
                    "authorId": "2585415",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2130564239",
                    "name": "Khalid Rafique"
                },
                {
                    "authorId": "1800730",
                    "name": "S. Sukkarieh"
                }
            ]
        },
        {
            "paperId": "944af9311d93d39642eb54075e006460f007a0ed",
            "title": "Data Augmentation for Deep Learning based Cattle Segmentation in Precision Livestock Farming",
            "abstract": "Accurate segmentation of cattle is a prerequisite for feature extraction and estimation. Convolutional neural networks (CNN) based approaches that train models on the largescale labeled datasets have achieved high levels of segmentation performance. However, pixel-wise manual labeling of a cattle image is challenging and time consuming due to the irregularity of the cattle contour. In this regard, data augmentation for deep learning based cattle segmentation is required. Our proposed data augmentation approach uses random image cropping and patching to expand the number of training images and their corresponding labels, then, a state-of-the-art deep neural net is trained to segment cattle images. Here we apply these techniques to images of cattle in a feedlot environment. Our data augmentation-based approach segmented cattle from a complex background with 99.5% mean Accuracy (mAcc) and 97.3% mean Intersection of Unions (mIoU), improving current techniques including a combination of random flipping, rotation and color jitter.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2786889",
                    "name": "Yongliang Qiao"
                },
                {
                    "authorId": "34345924",
                    "name": "Daobilige Su"
                },
                {
                    "authorId": "2143672423",
                    "name": "He Kong"
                },
                {
                    "authorId": "1800730",
                    "name": "S. Sukkarieh"
                },
                {
                    "authorId": "144231177",
                    "name": "S. Lomax"
                },
                {
                    "authorId": "3764837",
                    "name": "C. Clark"
                }
            ]
        },
        {
            "paperId": "a255ea609e80c47a7e4fadd07d277ed6896d8f62",
            "title": "BiLSTM-based Individual Cattle Identification for Automated Precision Livestock Farming",
            "abstract": "Individual cattle identification plays an important role for automation in precision livestock management. Existing methods for cattle identification require radio frequency and visual ear tags, all of which are prone to loss or damage. In this work, we propose a deep learning-based framework to identify beef cattle using image sequences, unifying merits of both Convolutional Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM) network methods. A CNN (Inception-V3) was used to extract features from a video dataset taken of the rear-view of cattle, after which extracted features were fed to a BiLSTM layer to capture spatial-temporal information enabling the identification of each individual animal. A total of 363 rear-view videos of 50 cattle were collected for our dataset. The proposed method achieved 91% identification accuracy using a 30-frame video length, improving that of Inception-V3 use or LSTM. Additionally, increasing video sequence length to 30-frames enhanced identification performance. Our approach can use spatial-temporal features to identify cattle, and enables automated identification for precision livestock farming.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2786889",
                    "name": "Yongliang Qiao"
                },
                {
                    "authorId": "34345924",
                    "name": "Daobilige Su"
                },
                {
                    "authorId": "2143672423",
                    "name": "He Kong"
                },
                {
                    "authorId": "1800730",
                    "name": "S. Sukkarieh"
                },
                {
                    "authorId": "144231177",
                    "name": "S. Lomax"
                },
                {
                    "authorId": "3764837",
                    "name": "C. Clark"
                }
            ]
        },
        {
            "paperId": "d5df164f166189a192d550392225e6ab6b46b289",
            "title": "ConvNet and LSH-Based Visual Localization Using Localized Sequence Matching",
            "abstract": "Convolutional Network (ConvNet), with its strong image representation ability, has achieved significant progress in the computer vision and robotic fields. In this paper, we propose a visual localization approach based on place recognition that combines the powerful ConvNet features and localized image sequence matching. The image distance matrix is constructed based on the cosine distance of extracted ConvNet features, and then a sequence search technique is applied on this distance matrix for the final visual recognition. To speed up the computational efficiency, the locality sensitive hashing (LSH) method is applied to achieve real-time performances with minimal accuracy degradation. We present extensive experiments on four real world data sets to evaluate each of the specific challenges in visual recognition. A comprehensive performance comparison of different ConvNet layers (each defining a level of features) considering both appearance and illumination changes is conducted. Compared with the traditional approaches based on hand-crafted features and single image matching, the proposed method shows good performances even in the presence of appearance and illumination changes.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2786889",
                    "name": "Yongliang Qiao"
                },
                {
                    "authorId": "2496072",
                    "name": "C. Cappelle"
                },
                {
                    "authorId": "1712857",
                    "name": "Y. Ruichek"
                },
                {
                    "authorId": "2122900909",
                    "name": "Tao Yang"
                }
            ]
        },
        {
            "paperId": "60347f2e8b0160899391aef66369112f090c485d",
            "title": "Corrigendum to \"Visual Localization by Place Recognition Based on Multifeature (D-\u03bbLBP++HOG)\"",
            "abstract": "In the article titled \u201cVisual Localization by Place Recognition Based on Multifeature (D-\u03bbLBP++HOG)\u201d [1], Dr. Cindy Cappelle and Dr. Yassine Ruichek were missing from the authors\u2019 list. They participated in the article writing, the data collection (data acquired with the UTBM experimental vehicles), the improvement of the proposed method, the formatting of experimental results and figures/schemas, and the analysis of the results. Additionally, the first affiliation was incorrect. The corrected authors\u2019 list and affiliations are shown above.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2786889",
                    "name": "Yongliang Qiao"
                },
                {
                    "authorId": "2496072",
                    "name": "C. Cappelle"
                },
                {
                    "authorId": "1712857",
                    "name": "Y. Ruichek"
                },
                {
                    "authorId": "2156119661",
                    "name": "Zhao Zhang"
                }
            ]
        },
        {
            "paperId": "5d172ecc456ce562eeba5c640d5dd7a81569f8b2",
            "title": "Visual Localization by Place Recognition Based on Multifeature (D-\u03bbLBP++HOG)",
            "abstract": "Visual localization is widely used in the autonomous navigation system and Advanced Driver Assistance Systems (ADAS). This paper presents a visual localization method based on multifeature fusion and disparity information using stereo images. We integrate disparity information into complete center-symmetric local binary patterns (CSLBP) to obtain a robust global image description (D-CSLBP). In order to represent the scene in depth, multifeature fusion of D-CSLBP and HOG features provides valuable information and permits decreasing the effect of some typical problems in place recognition such as perceptual aliasing. It improves visual recognition performance by taking advantage of depth, texture, and shape information. In addition, for real-time visual localization, local sensitive hashing method (LSH) was used to compress the high-dimensional multifeature into binary vectors. It can thus speed up the process of image matching. To show its effectiveness, the proposed method is tested and evaluated using real datasets acquired in outdoor environments. Given the obtained results, our approach allows more effective visual localization compared with the state-of-the-art method FAB-MAP.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2786889",
                    "name": "Yongliang Qiao"
                },
                {
                    "authorId": "2156119661",
                    "name": "Zhao Zhang"
                }
            ]
        },
        {
            "paperId": "6ec312df1a2fb01fbce08ae257e28c3af4caa59a",
            "title": "Place recognition based visual localization in changing environments",
            "abstract": "In many applications, it is crucial that a robot or vehicle localizes itself within the world especially for autonomous navigation and driving. The goal of this thesis is to improve place recognition performance for visual localization in changing environment. The approach is as follows: in off-line phase, geo-referenced images of each location are acquired, features are extracted and saved. While in the on-line phase, the vehicle localizes itself by identifying a previously-visited location through image or sequence retrieving. However, visual localization is challenging due to drastic appearance and illumination changes caused by weather conditions or seasonal changing. This thesis addresses the challenge of improving place recognition techniques through strengthen the ability of place describing and recognizing. Several approaches are proposed in this thesis:1) Multi-feature combination of CSLBP (extracted from gray-scale image and disparity map) and HOG features is used for visual localization. By taking the advantages of depth, texture and shape information, visual recognition performance can be improved. In addition, local sensitive hashing method (LSH) is used to speed up the process of place recognition;2) Visual localization across seasons is proposed based on sequence matching and feature combination of GIST and CSLBP. Matching places by considering sequences and feature combination denotes high robustness to extreme perceptual changes;3) All-environment visual localization is proposed based on automatic learned Convolutional Network (ConvNet) features and localized sequence matching. To speed up the computational efficiency, LSH is taken to achieve real-time visual localization with minimal accuracy degradation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2786889",
                    "name": "Yongliang Qiao"
                }
            ]
        }
    ]
}