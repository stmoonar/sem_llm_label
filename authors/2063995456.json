{
    "authorId": "2063995456",
    "papers": [
        {
            "paperId": "3dabd7acc3cab6235c828efaca933b5fd7f14026",
            "title": "PRoDeliberation: Parallel Robust Deliberation for End-to-End Spoken Language Understanding",
            "abstract": "Spoken Language Understanding (SLU) is a critical component of voice assistants; it consists of converting speech to semantic parses for task execution. Previous works have explored end-to-end models to improve the quality and robustness of SLU models with Deliberation, however these models have remained autoregressive, resulting in higher latencies. In this work we introduce PRoDeliberation, a novel method leveraging a Connectionist Temporal Classification-based decoding strategy as well as a denoising objective to train robust non-autoregressive deliberation models. We show that PRoDeliberation achieves the latency reduction of parallel decoding (2-10x improvement over autoregressive models) while retaining the ability to correct Automatic Speech Recognition (ASR) mistranscriptions of autoregressive deliberation systems. We further show that the design of the denoising training allows PRoDeliberation to overcome the limitations of small ASR devices, and we provide analysis on the necessity of each component of the system.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2241614580",
                    "name": "Trang Le"
                },
                {
                    "authorId": "2178345339",
                    "name": "Daniel Lazar"
                },
                {
                    "authorId": "2241496784",
                    "name": "Suyoun Kim"
                },
                {
                    "authorId": "2284264485",
                    "name": "Shan Jiang"
                },
                {
                    "authorId": "2284109865",
                    "name": "Duc Le"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "2075676826",
                    "name": "Aleksandr Livshits"
                },
                {
                    "authorId": "2284079232",
                    "name": "Ahmed Aly"
                },
                {
                    "authorId": "1519979046",
                    "name": "Akshat Shrivastava"
                }
            ]
        },
        {
            "paperId": "5a5f74204ed577bd52e0b1158ee5ab1153958c24",
            "title": "CoDi: Conversational Distillation for Grounded Question Answering",
            "abstract": "Distilling conversational skills into Small Language Models (SLMs) with approximately 1 billion parameters presents significant challenges. Firstly, SLMs have limited capacity in their model parameters to learn extensive knowledge compared to larger models. Secondly, high-quality conversational datasets are often scarce, small, and domain-specific. Addressing these challenges, we introduce a novel data distillation framework named CoDi (short for Conversational Distillation, pronounced\"Cody\"), allowing us to synthesize large-scale, assistant-style datasets in a steerable and diverse manner. Specifically, while our framework is task agnostic at its core, we explore and evaluate the potential of CoDi on the task of conversational grounded reasoning for question answering. This is a typical on-device scenario for specialist SLMs, allowing for open-domain model responses, without requiring the model to\"memorize\"world knowledge in its limited weights. Our evaluations show that SLMs trained with CoDi-synthesized data achieve performance comparable to models trained on human-annotated data in standard metrics. Additionally, when using our framework to generate larger datasets from web data, our models surpass larger, instruction-tuned models in zero-shot conversational grounded reasoning tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2287921432",
                    "name": "Patrick Huber"
                },
                {
                    "authorId": "2287921908",
                    "name": "Arash Einolghozati"
                },
                {
                    "authorId": "2304748670",
                    "name": "Rylan Conway"
                },
                {
                    "authorId": "2249757531",
                    "name": "Kanika Narang"
                },
                {
                    "authorId": "2316670006",
                    "name": "Matt Smith"
                },
                {
                    "authorId": "2316562238",
                    "name": "Waqar Nayyar"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "2284079232",
                    "name": "Ahmed Aly"
                },
                {
                    "authorId": "1519979046",
                    "name": "Akshat Shrivastava"
                }
            ]
        },
        {
            "paperId": "723a6340ee641e190b22bb47455d05e3b4237179",
            "title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling",
            "abstract": "Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We have made the code publicly available at https://github.com/facebookresearch/FnCTOD",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109964198",
                    "name": "Zekun Li"
                },
                {
                    "authorId": "2284639678",
                    "name": "Zhiyu Zoey Chen"
                },
                {
                    "authorId": "2284594951",
                    "name": "Mike Ross"
                },
                {
                    "authorId": "2287921432",
                    "name": "Patrick Huber"
                },
                {
                    "authorId": "2256132624",
                    "name": "Seungwhan Moon"
                },
                {
                    "authorId": "2146396528",
                    "name": "Zhaojiang Lin"
                },
                {
                    "authorId": "2215596266",
                    "name": "Xin Luna Dong"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "2258077708",
                    "name": "Xifeng Yan"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                }
            ]
        },
        {
            "paperId": "97f7a0c4be425f0019f7fca603d3dfd522da025a",
            "title": "PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs",
            "abstract": "On-device training is currently the most common approach for training machine learning (ML) models on private, distributed user data. Despite this, on-device training has several drawbacks: (1) most user devices are too small to train large models on-device, (2) on-device training is communication- and computation-intensive, and (3) on-device training can be difficult to debug and deploy. To address these problems, we propose Private Evolution-Text (PrE-Text), a method for generating differentially private (DP) synthetic textual data. First, we show that across multiple datasets, training small models (models that fit on user devices) with PrE-Text synthetic data outperforms small models trained on-device under practical privacy regimes ($\\epsilon=1.29$, $\\epsilon=7.58$). We achieve these results while using 9$\\times$ fewer rounds, 6$\\times$ less client computation per round, and 100$\\times$ less communication per round. Second, finetuning large models on PrE-Text's DP synthetic data improves large language model (LLM) performance on private data across the same range of privacy budgets. Altogether, these results suggest that training on DP synthetic data can be a better option than training a model on-device on private distributed data. Code is available at https://github.com/houcharlie/PrE-Text.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1443436264",
                    "name": "Charlie Hou"
                },
                {
                    "authorId": "1519979046",
                    "name": "Akshat Shrivastava"
                },
                {
                    "authorId": "2304748832",
                    "name": "Hongyuan Zhan"
                },
                {
                    "authorId": "2304748670",
                    "name": "Rylan Conway"
                },
                {
                    "authorId": "2241614580",
                    "name": "Trang Le"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "2288256947",
                    "name": "Giulia Fanti"
                },
                {
                    "authorId": "2178345339",
                    "name": "Daniel Lazar"
                }
            ]
        },
        {
            "paperId": "2c676ecdc954ec24e1907c76accb1e8ac06deec0",
            "title": "Data-Efficiency with a Single GPU: An Exploration of Transfer Methods for Small Language Models",
            "abstract": "Multi-task learning (MTL), instruction tuning, and prompting have recently been shown to improve the generalizability of large language models to new tasks. However, the benefits of such methods are less well-documented in smaller language models, with some studies finding contradictory results. In this work, we explore and isolate the effects of (i) model size, (ii) general purpose MTL, (iii) in-domain MTL, (iv) instruction tuning, and (v) few-shot fine-tuning for models with fewer than 500 million parameters. Our experiments in the zero-shot setting demonstrate that models gain 31% relative improvement, on average, from general purpose MTL, with an additional 37.6% relative gain from in-domain MTL. Contradictory to prior works on large models, we find that instruction tuning provides a modest 2% performance improvement for small models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2044198106",
                    "name": "Alon Albalak"
                },
                {
                    "authorId": "1519979046",
                    "name": "Akshat Shrivastava"
                },
                {
                    "authorId": "21669342",
                    "name": "Chinnadhurai Sankar"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "2187300449",
                    "name": "Mike Ross"
                }
            ]
        },
        {
            "paperId": "9358952fad5a109160ea43cf09a7a821518e4be1",
            "title": "Stop: A Dataset for Spoken Task Oriented Semantic Parsing",
            "abstract": "End-to-end spoken language understanding (SLU) predicts intent directly from audio using a single model. It promises to improve the performance of assistant systems by leveraging acoustic information lost in the intermediate textual representation and preventing cascading errors from Automatic Speech Recognition (ASR). Further, having one unified model has efficiency advantages when deploying assistant systems on-device. However, the limited number of public audio datasets with semantic parse labels hinders the research progress in this area. In this paper, we release the Spoken Task-Oriented semantic Parsing (STOP) dataset 1, the largest and most complex SLU dataset publicly available. Additionally, we define low-resource splits to establish a benchmark for improving SLU when limited labeled data is available. Furthermore, in addition to the human-recorded audio, we are releasing a TTS-generated versions to benchmark the performance for low-resource and domain adaptation of end-to-end SLU systems.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "52021958",
                    "name": "Paden Tomasello"
                },
                {
                    "authorId": "2057582426",
                    "name": "Po-chun Hsu"
                },
                {
                    "authorId": "1519979046",
                    "name": "Akshat Shrivastava"
                },
                {
                    "authorId": "2178345339",
                    "name": "Daniel Lazar"
                },
                {
                    "authorId": "145267619",
                    "name": "Duc Le"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "35309245",
                    "name": "A. Elkahky"
                },
                {
                    "authorId": "1805998294",
                    "name": "Jade Copet"
                },
                {
                    "authorId": "2957796",
                    "name": "Wei-Ning Hsu"
                },
                {
                    "authorId": "2178345845",
                    "name": "Yossef Mordechay"
                },
                {
                    "authorId": "108040932",
                    "name": "Robin Algayres"
                },
                {
                    "authorId": "2116225477",
                    "name": "Tu Nguyen"
                },
                {
                    "authorId": "90049550",
                    "name": "Emmanuel Dupoux"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "40360972",
                    "name": "Abdel-rahman Mohamed"
                }
            ]
        },
        {
            "paperId": "fae272cedc2c050f56fe19e0cfaf2c6b01039e5f",
            "title": "RetroNLU: Retrieval Augmented Task-Oriented Semantic Parsing",
            "abstract": "While large pre-trained language models accumulate a lot of knowledge in their parameters, it has been demonstrated that augmenting it with non-parametric retrieval-based memory has a number of benefits ranging from improved accuracy to data efficiency for knowledge-focused tasks such as question answering. In this work, we apply retrieval-based modeling ideas to the challenging complex task of multi-domain task-oriented semantic parsing for conversational assistants. Our technique, RetroNLU, extends a sequence-to-sequence model architecture with a retrieval component, which is used to retrieve existing similar samples and present them as an additional context to the model. In particular, we analyze two settings, where we augment an input with (a) retrieved nearest neighbor utterances (utterance-nn), and (b) ground-truth semantic parses of nearest neighbor utterances (semparse-nn). Our technique outperforms the baseline method by 1.5% absolute macro-F1, especially at the low resource setting, matching the baseline model accuracy with only 40% of the complete data.Furthermore, we analyse the quality, model sensitivity, and performance of the nearest neighbor retrieval component\u2019s for semantic parses of varied utterance complexity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46346053",
                    "name": "Vivek Gupta"
                },
                {
                    "authorId": "1519979046",
                    "name": "Akshat Shrivastava"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "2201435",
                    "name": "Armen Aghajanyan"
                },
                {
                    "authorId": "144375814",
                    "name": "Denis Savenkov"
                }
            ]
        },
        {
            "paperId": "38626eab2e3a386d4558addb8f980344d76d2e37",
            "title": "Lattice-Based Improvements for Voice Triggering Using Graph Neural Networks",
            "abstract": "Voice-triggered smart assistants often rely on detection of a trigger-phrase before they start listening for the user request. Mitigation of false triggers is an important aspect of building a privacy-centric non-intrusive smart assistant. In this paper, we address the task of false trigger mitigation (FTM) using a novel approach based on analyzing automatic speech recognition (ASR) lattices using graph neural networks (GNN). The proposed approach uses the fact that decoding lattice of a falsely triggered audio exhibits uncertainties in terms of many alternative paths and unexpected words on the lattice arcs as compared to the lattice of a correctly triggered audio. A pure trigger-phrase detector model doesn\u2019t fully utilize the intent of the user speech whereas by using the complete decoding lattice of user audio, we can effectively mitigate speech not intended for the smart assistant. We deploy two variants of GNNs in this paper based on 1) graph convolution layers and 2) self-attention mechanism respectively. Our experiments demonstrate that GNNs are highly accurate in FTM task by mitigating ~87% of false triggers at 99% true positive rate (TPR). Furthermore, the proposed models are fast to train and efficient in parameter requirements.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "3309670",
                    "name": "Pranay Dighe"
                },
                {
                    "authorId": "145073156",
                    "name": "Saurabh N. Adya"
                },
                {
                    "authorId": "2120126445",
                    "name": "Nuoyu Li"
                },
                {
                    "authorId": "1731106",
                    "name": "Srikanth Vishnubhotla"
                },
                {
                    "authorId": "100925254",
                    "name": "D. Naik"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "2146277365",
                    "name": "Ying Ma"
                },
                {
                    "authorId": "50419262",
                    "name": "S. Pulman"
                },
                {
                    "authorId": "2118122645",
                    "name": "Jason D. Williams"
                }
            ]
        },
        {
            "paperId": "1a643c6a43341b0480e031193bd57b83a8cb0d47",
            "title": "Active Learning for Domain Classification in a Commercial Spoken Personal Assistant",
            "abstract": "We describe a method for selecting relevant new training data for the LSTM-based domain selection component of our personal assistant system. Adding more annotated training data for any ML system typically improves accuracy, but only if it provides examples not already adequately covered in the existing data. However, obtaining, selecting, and labeling relevant data is expensive. This work presents a simple technique that automatically identifies new helpful examples suitable for human annotation. Our experimental results show that the proposed method, compared with random-selection and entropy-based methods, leads to higher accuracy improvements given a fixed annotation budget. Although developed and tested in the setting of a commercial intelligent assistant, the technique is of wider applicability.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1683647",
                    "name": "X. Chen"
                },
                {
                    "authorId": "2063995456",
                    "name": "Adithya Sagar"
                },
                {
                    "authorId": "2055513699",
                    "name": "Justine T. Kao"
                },
                {
                    "authorId": "2180924499",
                    "name": "Tony Y. Li"
                },
                {
                    "authorId": "32192159",
                    "name": "Christopher Klein"
                },
                {
                    "authorId": "50419262",
                    "name": "S. Pulman"
                },
                {
                    "authorId": "2054554224",
                    "name": "Ashish Garg"
                },
                {
                    "authorId": "47271859",
                    "name": "J. Williams"
                }
            ]
        }
    ]
}