{
    "authorId": "2116265843",
    "papers": [
        {
            "paperId": "06df406805b59dcafb59214b99a77f9d2dad8b76",
            "title": "Large Language Models for Recommendation: Past, Present, and Future",
            "abstract": "Large language models (LLMs) have significantly influenced recommender systems, spurring interest across academia and industry in leveraging LLMs for recommendation tasks. This includes using LLMs for generative item retrieval and ranking, and developing versatile LLMs for various recommendation tasks, potentially leading to a paradigm shift in the field of recommender systems. This tutorial aims to demystify the Large Language Model for Recommendation (LLM4Rec) by reviewing its evolution and delving into cutting-edge research. We will explore how LLMs enhance recommender systems in terms of architecture, learning paradigms, and functionalities such as conversational abilities, generalization, planning, and content generation. The tutorial will shed light on the challenges and open problems in this burgeoning field, including trustworthiness, efficiency, online training, and evaluation of LLM4Rec. We will conclude by summarizing key learnings from existing studies and outlining potential avenues for future research, with the goal of equipping the audience with a comprehensive understanding of LLM4Rec and inspiring further exploration in this transformative domain.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188063534",
                    "name": "Keqin Bao"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2192203811",
                    "name": "Xinyu Lin"
                },
                {
                    "authorId": "2305605922",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "203574cdbb7ae35a44c40abeed414093a58b74bf",
            "title": "Prospect Personalized Recommendation on Large Language Model-based Agent Platform",
            "abstract": "The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A preliminary study involving several cases of Rec4Agentverse validates its significant potential for application. Lastly, we discuss potential issues and promising directions for future research.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2188063534",
                    "name": "Keqin Bao"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2145957648",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2153422494",
                    "name": "Wentao Shi"
                },
                {
                    "authorId": "2288043800",
                    "name": "Wanhong Xu"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "2add25de304fbf12b75f3209532bf685972f29b9",
            "title": "Debias Can be Unreliable: Mitigating Bias Issue in Evaluating Debiasing Recommendation",
            "abstract": "Recent work has improved recommendation models remarkably by equipping them with debiasing methods. Due to the unavailability of fully-exposed datasets, most existing approaches resort to randomly-exposed datasets as a proxy for evaluating debiased models, employing traditional evaluation scheme to represent the recommendation performance. However, in this study, we reveal that traditional evaluation scheme is not suitable for randomly-exposed datasets, leading to inconsistency between the Recall performance obtained using randomly-exposed datasets and that obtained using fully-exposed datasets. Such inconsistency indicates the potential unreliability of experiment conclusions on previous debiasing techniques and calls for unbiased Recall evaluation using randomly-exposed datasets. To bridge the gap, we propose the Unbiased Recall Evaluation (URE) scheme, which adjusts the utilization of randomly-exposed datasets to unbiasedly estimate the true Recall performance on fully-exposed datasets. We provide theoretical evidence to demonstrate the rationality of URE and perform extensive experiments on real-world datasets to validate its soundness.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2320452720",
                    "name": "Chengbing Wang"
                },
                {
                    "authorId": "2153422494",
                    "name": "Wentao Shi"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2230045941",
                    "name": "Hang Pan"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "3a2cd1ac9a71912a37b6fb513e0ec69a3a74176f",
            "title": "Large Language Models for Recommendation: Progresses and Future Directions",
            "abstract": "Large language models (LLMs) have significantly influenced recommender systems. Both academia and industry have shown growing interest in developing LLMs for recommendation purposes, an approach commonly referred to as LLM4Rec. This involves efforts such as utilizing LLMs for generative item retrieval and ranking, along with the potential for creating universal LLMs for varied recommendation tasks, signaling a possible paradigm shift in recommender systems. This tutorial is designed to review the progression of LLM4Rec and provide an in-depth analysis of the prevailing studies. We will discuss how LLMs advance recommender systems in model architecture, learning paradigms, and capabilities like conversation, generalization, planning, and content generation. Additionally, the tutorial will highlight open problems and challenges in this nascent field, addressing concerns related to trustworthiness, efficiency, online training, and recommendation data modeling. Concluding with a summary of the takeaways from previous research, the tutorial will suggest avenues for future investigations. Our aim is to help the audience grasp the developments in LLM4Rec, as well as to spark inspiration for further research. By doing so, we expect to contribute to the growth and success of LLM4Rec, possibly leading to a fundamental change in recommender paradigms.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2188063534",
                    "name": "Keqin Bao"
                },
                {
                    "authorId": "2145957648",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2275173839",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2239071206",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "415e25e8e3c8c10255abbb704d545f9d02c54c76",
            "title": "GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation",
            "abstract": "Large language models have seen widespread adoption in math problem-solving. However, in geometry problems that usually require visual aids for better understanding, even the most advanced multi-modal models currently still face challenges in effectively using image information. High-quality data is crucial for enhancing the geometric capabilities of multi-modal models, yet existing open-source datasets and related efforts are either too challenging for direct model learning or suffer from misalignment between text and images. To overcome this issue, we introduce a novel pipeline that leverages GPT-4 and GPT-4V to generate relatively basic geometry problems with aligned text and images, facilitating model learning. We have produced a dataset of 4.9K geometry problems and combined it with 19K open-source data to form our GeoGPT4V dataset. Experimental results demonstrate that the GeoGPT4V dataset significantly improves the geometry performance of various models on the MathVista and MathVision benchmarks. The code is available at https://github.com/Lanyu0303/GeoGPT4V_Project",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2307075749",
                    "name": "Shihao Cai"
                },
                {
                    "authorId": "2188063534",
                    "name": "Keqin Bao"
                },
                {
                    "authorId": "2307434316",
                    "name": "Hangyu Guo"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2303303180",
                    "name": "Jun Song"
                },
                {
                    "authorId": "2303257086",
                    "name": "Bo Zheng"
                }
            ]
        },
        {
            "paperId": "435d1fd081c2ee3f9ade8081be6bdda87385a458",
            "title": "Fair Recommendations with Limited Sensitive Attributes: A Distributionally Robust Optimization Approach",
            "abstract": "As recommender systems are indispensable in various domains such as job searching and e-commerce, providing equitable recommendations to users with different sensitive attributes becomes an imperative requirement. Prior approaches for enhancing fairness in recommender systems presume the availability of all sensitive attributes, which can be difficult to obtain due to privacy concerns or inadequate means of capturing these attributes. In practice, the efficacy of these approaches is limited, pushing us to investigate ways of promoting fairness with limited sensitive attribute information. Toward this goal, it is important to reconstruct missing sensitive attributes. Nevertheless, reconstruction errors are inevitable due to the complexity of real-world sensitive attribute reconstruction problems and legal regulations. Thus, we pursue fair learning methods that are robust to reconstruction errors. To this end, we propose Distributionally Robust Fair Optimization (DRFO), which minimizes the worst-case unfairness over all potential probability distributions of missing sensitive attributes instead of the reconstructed one to account for the impact of the reconstruction errors. We provide theoretical and empirical evidence to demonstrate that our method can effectively ensure fairness in recommender systems when only limited sensitive attributes are accessible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2276424502",
                    "name": "Tianhao Shi"
                },
                {
                    "authorId": "2145957648",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2275173839",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2239071206",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "4efd7cd724802d7ac3ccef972309691466d4637a",
            "title": "Decoding Matters: Addressing Amplification Bias and Homogeneity Issue for LLM-based Recommendation",
            "abstract": "Adapting Large Language Models (LLMs) for recommendation requires careful consideration of the decoding process, given the inherent differences between generating items and natural language. Existing approaches often directly apply LLMs' original decoding methods. However, we find these methods encounter significant challenges: 1) amplification bias -- where standard length normalization inflates scores for items containing tokens with generation probabilities close to 1 (termed ghost tokens), and 2) homogeneity issue -- generating multiple similar or repetitive items for a user. To tackle these challenges, we introduce a new decoding approach named Debiasing-Diversifying Decoding (D3). D3 disables length normalization for ghost tokens to alleviate amplification bias, and it incorporates a text-free assistant model to encourage tokens less frequently generated by LLMs for counteracting recommendation homogeneity. Extensive experiments on real-world datasets demonstrate the method's effectiveness in enhancing accuracy and diversity.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2188063534",
                    "name": "Keqin Bao"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2145957648",
                    "name": "Yang Zhang"
                },
                {
                    "authorId": "2307916329",
                    "name": "Xinyue Huo"
                },
                {
                    "authorId": "2307971167",
                    "name": "Chong Chen"
                },
                {
                    "authorId": "2275173839",
                    "name": "Fuli Feng"
                }
            ]
        },
        {
            "paperId": "6aa5177b38969f69a5c1b17b2846e6e90bd684e8",
            "title": "Learnable Item Tokenization for Generative Recommendation",
            "abstract": "Utilizing powerful Large Language Models (LLMs) for generative recommendation has attracted much attention. Nevertheless, a crucial challenge is transforming recommendation data into the language space of LLMs through effective item tokenization. Current approaches, such as ID, textual, and codebook-based identifiers, exhibit shortcomings in encoding semantic information, incorporating collaborative signals, or handling code assignment bias. To address these limitations, we propose LETTER (a LEarnable Tokenizer for generaTivE Recommendation), which integrates hierarchical semantics, collaborative signals, and code assignment diversity to satisfy the essential requirements of identifiers. LETTER incorporates Residual Quantized VAE for semantic regularization, a contrastive alignment loss for collaborative regularization, and a diversity loss to mitigate code assignment bias. We instantiate LETTER on two models and propose a ranking-guided generation loss to augment their ranking ability theoretically. Experiments on three datasets validate the superiority of LETTER, advancing the state-of-the-art in the field of LLM-based generative recommendation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2301153513",
                    "name": "Honghui Bao"
                },
                {
                    "authorId": "2192203811",
                    "name": "Xinyu Lin"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2257099189",
                    "name": "Yongqi Li"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2241348826",
                    "name": "See-kiong Ng"
                },
                {
                    "authorId": "2279753672",
                    "name": "Tat-Seng Chua"
                }
            ]
        },
        {
            "paperId": "81571f64e0b4b75fedae6a4279290e59e4143976",
            "title": "Item-side Fairness of Large Language Model-based Recommendation System",
            "abstract": "Recommendation systems for Web content distribution intricately connect to the information access and exposure opportunities for vulnerable populations. The emergence of Large Language Models-based Recommendation System (LRS) may introduce additional societal challenges to recommendation systems due to the inherent biases in Large Language Models (LLMs). From the perspective of item-side fairness, there remains a lack of comprehensive investigation into the item-side fairness of LRS given the unique characteristics of LRS compared to conventional recommendation systems. To bridge this gap, this study examines the property of LRS with respect to item-side fairness and reveals the influencing factors of both historical users' interactions and inherent semantic biases of LLMs, shedding light on the need to extend conventional item-side fairness methods for LRS. Towards this goal, we develop a concise and effective framework called IFairLRS to enhance the item-side fairness of an LRS. IFairLRS covers the main stages of building an LRS with specifically adapted strategies to calibrate the recommendations of LRS. We utilize IFairLRS to fine-tune LLaMA, a representative LLM, on MovieLens and Steam datasets, and observe significant item-side fairness improvements. The code can be found in https://github.com/JiangM-C/IFairLRS.git.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2225076028",
                    "name": "Meng Jiang"
                },
                {
                    "authorId": "2188063534",
                    "name": "Keqin Bao"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2261748725",
                    "name": "Zhengyi Yang"
                },
                {
                    "authorId": "2163400298",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2239071206",
                    "name": "Xiangnan He"
                }
            ]
        },
        {
            "paperId": "c13f295e1cfbe487b78f3e60ba0f2d7bc5bfa172",
            "title": "Diffusion Models for Generative Outfit Recommendation",
            "abstract": "Outfit Recommendation (OR) in the fashion domain has evolved through two stages: Pre-defined Outfit Recommendation and Personalized Outfit Composition. However, both stages are constrained by existing fashion products, limiting their effectiveness in addressing users' diverse fashion needs. Recently, the advent of AI-generated content provides the opportunity for OR to transcend these limitations, showcasing the potential for personalized outfit generation and recommendation. To this end, we introduce a novel task called Generative Outfit Recommendation (GOR), aiming to generate a set of fashion images and compose them into a visually compatible outfit tailored to specific users. The key objectives of GOR lie in the high fidelity, compatibility, and personalization of generated outfits. To achieve these, we propose a generative outfit recommender model named DiFashion, which empowers exceptional diffusion models to accomplish the parallel generation of multiple fashion images. To ensure three objectives, we design three kinds of conditions to guide the parallel generation process and adopt Classifier-Free-Guidance to enhance the alignment between the generated images and conditions. We apply DiFashion on both personalized Fill-In-The-Blank and GOR tasks and conduct extensive experiments on iFashion and Polyvore-U datasets. The quantitative and human-involved qualitative evaluation demonstrate the superiority of DiFashion over competitive baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2279662165",
                    "name": "Yiyan Xu"
                },
                {
                    "authorId": "2117833732",
                    "name": "Wenjie Wang"
                },
                {
                    "authorId": "2280911299",
                    "name": "Fuli Feng"
                },
                {
                    "authorId": "2288019705",
                    "name": "Yunshan Ma"
                },
                {
                    "authorId": "2116265843",
                    "name": "Jizhi Zhang"
                },
                {
                    "authorId": "2239071206",
                    "name": "Xiangnan He"
                }
            ]
        }
    ]
}