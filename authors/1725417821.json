{
    "authorId": "1725417821",
    "papers": [
        {
            "paperId": "8f07183bf588f551b701fad6bfecf231f3ab78bc",
            "title": "AraDiCE: Benchmarks for Dialectal and Cultural Capabilities in LLMs",
            "abstract": "Arabic, with its rich diversity of dialects, remains significantly underrepresented in Large Language Models, particularly in dialectal variations. We address this gap by introducing seven synthetic datasets in dialects alongside Modern Standard Arabic (MSA), created using Machine Translation (MT) combined with human post-editing. We present AraDiCE, a benchmark for Arabic Dialect and Cultural Evaluation. We evaluate LLMs on dialect comprehension and generation, focusing specifically on low-resource Arabic dialects. Additionally, we introduce the first-ever fine-grained benchmark designed to evaluate cultural awareness across the Gulf, Egypt, and Levant regions, providing a novel dimension to LLM evaluation. Our findings demonstrate that while Arabic-specific models like Jais and AceGPT outperform multilingual models on dialectal tasks, significant challenges persist in dialect identification, generation, and translation. This work contributes ~45K post-edited samples, a cultural benchmark, and highlights the importance of tailored training to improve LLM performance in capturing the nuances of diverse Arabic dialects and cultural contexts. We will release the dialectal translation models and benchmarks curated in this study.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2171367840",
                    "name": "Basel Mousi"
                },
                {
                    "authorId": "145938140",
                    "name": "Nadir Durrani"
                },
                {
                    "authorId": "2311384605",
                    "name": "Fatema Ahmad"
                },
                {
                    "authorId": "151116677",
                    "name": "Md. Arid Hasan"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "2305614243",
                    "name": "Tameem Kabbani"
                },
                {
                    "authorId": "6415321",
                    "name": "Fahim Dalvi"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "2257274157",
                    "name": "Firoj Alam"
                }
            ]
        },
        {
            "paperId": "a33cbc6d75b0dc2a36a359b4e6e6ff085690191f",
            "title": "NativQA: Multilingual Culturally-Aligned Natural Query for LLMs",
            "abstract": "Natural Question Answering (QA) datasets play a crucial role in evaluating the capabilities of large language models (LLMs), ensuring their effectiveness in real-world applications. Despite the numerous QA datasets that have been developed, there is a notable lack of region-specific datasets generated by native users in their own languages. This gap hinders the effective benchmarking of LLMs for regional and cultural specificities. Furthermore, it also limits the development of fine-tuned models. In this study, we propose a scalable, language-independent framework, NativQA, to seamlessly construct culturally and regionally aligned QA datasets in native languages, for LLM evaluation and tuning. We demonstrate the efficacy of the proposed framework by designing a multilingual natural QA dataset, \\mnqa, consisting of ~64k manually annotated QA pairs in seven languages, ranging from high to extremely low resource, based on queries from native speakers from 9 regions covering 18 topics. We benchmark open- and closed-source LLMs with the MultiNativQA dataset. We also showcase the framework efficacy in constructing fine-tuning data especially for low-resource and dialectally-rich languages. We made both the framework NativQA and MultiNativQA dataset publicly available for the community (https://nativqa.gitlab.io).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "151116677",
                    "name": "Md. Arid Hasan"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "2311384605",
                    "name": "Fatema Ahmad"
                },
                {
                    "authorId": "12352363",
                    "name": "Sahinur Rahman Laskar"
                },
                {
                    "authorId": "2311437171",
                    "name": "Sunaya Upadhyay"
                },
                {
                    "authorId": "2155270590",
                    "name": "Vrunda N. Sukhadia"
                },
                {
                    "authorId": "38330039",
                    "name": "Mucahid Kutlu"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "2257274157",
                    "name": "Firoj Alam"
                }
            ]
        },
        {
            "paperId": "ab49160c764ac3ade945e16c24b53d518398553a",
            "title": "Children's Speech Recognition through Discrete Token Enhancement",
            "abstract": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2155270590",
                    "name": "Vrunda N. Sukhadia"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                }
            ]
        },
        {
            "paperId": "cc874c256015b65400449ca93ae079728987c8bb",
            "title": "Speech Representation Analysis Based on Inter- and Intra-Model Similarities",
            "abstract": "Self-supervised models have revolutionized speech processing, achieving new levels of performance in a wide variety of tasks with limited resources. However, the inner workings of these models are still opaque. In this paper, we aim to analyze the encoded contextual representation of these foundation models based on their inter- and intramodel similarity, independent of any external annotation and task-specific constraint. We examine different SSL models varying their training paradigm \u2013 Contrastive (Wav2Vec2.0) and Predictive models (HuBERT); and model sizes (base and large). We explore these models on different levels of localization/distributivity of information including (i) individual neurons; (ii) layer representation; (iii) attention weights and (iv) compare the representations with their finetuned counterparts. Our results highlight that these models converge to similar representation subspaces but not to similar neuron-localized concepts1. We made the code publicly available for facilitating further research, we publicly released our code2.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2320577657",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "2261359986",
                    "name": "Ahmed Ali"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                }
            ]
        },
        {
            "paperId": "f9566cab417a1ca917a59e72cf198ef5d92b187b",
            "title": "Beyond Orthography: Automatic Recovery of Short Vowels and Dialectal Sounds in Arabic",
            "abstract": "This paper presents a novel Dialectal Sound and Vowelization Recovery framework, designed to recognize borrowed and dialectal sounds within phonologically diverse and dialect-rich languages, that extends beyond its standard orthographic sound sets. The proposed framework utilized a quantized sequence of input with(out) continuous pretrained self-supervised representation. We show the efficacy of the pipeline using limited data for Arabic, a dialect-rich language containing more than 22 major dialects. Phonetically correct transcribed speech resources for dialectal Arabic are scarce. Therefore, we introduce ArabVoice15, a first-of-its-kind, curated test set featuring 5 hours of dialectal speech across 15 Arab countries, with phonetically accurate transcriptions, including borrowed and dialect-specific sounds. We described in detail the annotation guideline along with the analysis of the dialectal confusion pairs. Our extensive evaluation includes both subjective -- human perception tests and objective measures. Our empirical results, reported with three test sets, show that with only one and half hours of training data, our model improve character error rate by ~ 7\\% in ArabVoice15 compared to the baseline.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "2261359986",
                    "name": "Ahmed Ali"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                }
            ]
        },
        {
            "paperId": "3627dc4fd36a5f4511ea8de9e07c553efb6e0bce",
            "title": "MyVoice: Arabic Speech Resource Collaboration Platform",
            "abstract": "We introduce MyVoice, a crowdsourcing platform designed to collect Arabic speech to enhance dialectal speech technologies. This platform offers an opportunity to design large dialectal speech datasets; and makes them publicly available. MyVoice allows contributors to select city/country-level fine-grained dialect and record the displayed utterances. Users can switch roles between contributors and annotators. The platform incorporates a quality assurance system that filters out low-quality and spurious recordings before sending them for validation. During the validation phase, contributors can assess the quality of recordings, annotate them, and provide feedback which is then reviewed by administrators. Furthermore, the platform offers flexibility to admin roles to add new data or tasks beyond dialectal speech and word collection, which are displayed to contributors. Thus, enabling collaborative efforts in gathering diverse and large Arabic speech data.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2218145245",
                    "name": "Yousseif Elshahawy"
                },
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "145907529",
                    "name": "Ahmed Ali"
                }
            ]
        },
        {
            "paperId": "7e52ffa0ecdaa1b8eb6dc0757de532894bfd24b1",
            "title": "Automatic Pronunciation Assessment - A Review",
            "abstract": "Pronunciation assessment and its application in computer-aided pronunciation training (CAPT) have seen impressive progress in recent years. With the rapid growth in language processing and deep learning over the past few years, there is a need for an updated review. In this paper, we review methods employed in pronunciation assessment for both phonemic and prosodic. We categorize the main challenges observed in prominent research trends, and highlight existing limitations, and available resources. This is followed by a discussion of the remaining challenges and possible directions for future work.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "2261359986",
                    "name": "Ahmed Ali"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                }
            ]
        },
        {
            "paperId": "98b2463a9b3312776c84ecc2a2b0c6bb8d351634",
            "title": "QVoice: Arabic Speech Pronunciation Learning Application",
            "abstract": "This paper introduces a novel Arabic pronunciation learning application QVoice, powered with end-to-end mispronunciation detection and feedback generator module. The application is designed to support non-native Arabic speakers in enhancing their pronunciation skills, while also helping native speakers mitigate any potential influence from regional dialects on their Modern Standard Arabic (MSA) pronunciation. QVoice employs various learning cues to aid learners in comprehending meaning, drawing connections with their existing knowledge of English language, and offers detailed feedback for pronunciation correction, along with contextual examples showcasing word usage. The learning cues featured in QVoice encompass a wide range of meaningful information, such as visualizations of phrases/words and their translations, as well as phonetic transcriptions and transliterations. QVoice provides pronunciation feedback at the character level and assesses performance at the word level.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "1972477667",
                    "name": "Fouad Khnaisser"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "2099539780",
                    "name": "S. Afzal"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                }
            ]
        },
        {
            "paperId": "9f87c8e27a10d71500314e7e21853f5a23efce59",
            "title": "LAraBench: Benchmarking Arabic AI with Large Language Models",
            "abstract": "Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13b-chat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing ~296K data points, ~46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperformed LLMs in zero-shot learning, with a few exceptions. Notably, larger computational models with few-shot learning techniques managed to reduce these performance gaps. Our findings provide valuable insights into the applicability of LLMs for Arabic NLP and speech processing tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1683403",
                    "name": "Ahmed Abdelali"
                },
                {
                    "authorId": "143779235",
                    "name": "Hamdy Mubarak"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "2905745",
                    "name": "Maram Hasanain"
                },
                {
                    "authorId": "2171367840",
                    "name": "Basel Mousi"
                },
                {
                    "authorId": "2466162",
                    "name": "Sabri Boughorbel"
                },
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "2177436744",
                    "name": "Daniel Izham"
                },
                {
                    "authorId": "6415321",
                    "name": "Fahim Dalvi"
                },
                {
                    "authorId": "2762811",
                    "name": "Majd Hawasly"
                },
                {
                    "authorId": "2218353460",
                    "name": "Nizi Nazar"
                },
                {
                    "authorId": "2218145245",
                    "name": "Yousseif Elshahawy"
                },
                {
                    "authorId": "2109102523",
                    "name": "Ahmed M. Ali"
                },
                {
                    "authorId": "145938140",
                    "name": "Nadir Durrani"
                },
                {
                    "authorId": "1398136050",
                    "name": "Natasa Milic-Frayling"
                },
                {
                    "authorId": "37784060",
                    "name": "Firoj Alam"
                }
            ]
        },
        {
            "paperId": "a136c9bec3c3b942d5040cc88a09f814a4f51114",
            "title": "The complementary roles of non-verbal cues for Robust Pronunciation Assessment",
            "abstract": "Research on pronunciation assessment systems focuses on utilizing phonetic and phonological aspects of non-native (L2) speech, often neglecting the rich layer of information hidden within the non-verbal cues. In this study, we proposed a novel pronunciation assessment framework, IntraVerbalPA. % The framework innovatively incorporates both fine-grained frame- and abstract utterance-level non-verbal cues, alongside the conventional speech and phoneme representations. Additionally, we introduce ''Goodness of phonemic-duration'' metric to effectively model duration distribution within the framework. Our results validate the effectiveness of the proposed IntraVerbalPA framework and its individual components, yielding performance that either matches or outperforms existing research works.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2189476655",
                    "name": "Yassine El Kheir"
                },
                {
                    "authorId": "1725417821",
                    "name": "Shammur A. Chowdhury"
                },
                {
                    "authorId": "145907529",
                    "name": "Ahmed Ali"
                }
            ]
        }
    ]
}