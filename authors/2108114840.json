{
    "authorId": "2108114840",
    "papers": [
        {
            "paperId": "7b3a430caf62d8c63457c586b39f078a3bca2bd7",
            "title": "Unveiling Implicit Deceptive Patterns in Multi-Modal Fake News via Neuro-Symbolic Reasoning",
            "abstract": "In the current Internet landscape, the rampant spread of fake news, particularly in the form of multi-modal content, poses a great social threat. While automatic multi-modal fake news detection methods have shown promising results, the lack of explainability remains a significant challenge. Existing approaches provide superficial explainability by displaying learned important components or views from well-trained networks, but they often fail to uncover the implicit deceptive patterns that reveal how fake news is fabricated. To address this limitation, we begin by predefining three typical deceptive patterns, namely image manipulation, cross-modal inconsistency, and image repurposing, which shed light on the mechanisms underlying fake news fabrication. Then, we propose a novel Neuro-Symbolic Latent Model called NSLM, that not only derives accurate judgments on the veracity of news but also uncovers the implicit deceptive patterns as explanations. Specifically, the existence of each deceptive pattern is expressed as a two-valued learnable latent variable, which is acquired through amortized variational inference and weak supervision based on symbolic logic rules. Additionally, we devise pseudo-siamese networks to capture distinct deceptive patterns effectively. Experimental results on two real-world datasets demonstrate that our NSLM achieves the best performance in fake news detection while providing insightful explanations of deceptive patterns.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2278788263",
                    "name": "Yiqi Dong"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2278775372",
                    "name": "Youzhu Jin"
                },
                {
                    "authorId": "2173872399",
                    "name": "Meng Ge"
                },
                {
                    "authorId": "2278797506",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2288101014",
                    "name": "Di Jin"
                }
            ]
        },
        {
            "paperId": "9ad6c4aeb2ce9bc72ed3c839deeb65d2879cafd7",
            "title": "AIMDiT: Modality Augmentation and Interaction via Multimodal Dimension Transformation for Emotion Recognition in Conversations",
            "abstract": "Emotion Recognition in Conversations (ERC) is a popular task in natural language processing, which aims to recognize the emotional state of the speaker in conversations. While current research primarily emphasizes contextual modeling, there exists a dearth of investigation into effective multimodal fusion methods. We propose a novel framework called AIMDiT to solve the problem of multimodal fusion of deep features. Specifically, we design a Modality Augmentation Network which performs rich representation learning through dimension transformation of different modalities and parameter-efficient inception block. On the other hand, the Modality Interaction Network performs interaction fusion of extracted inter-modal features and intra-modal features. Experiments conducted using our AIMDiT framework on the public benchmark dataset MELD reveal 2.34% and 2.87% improvements in terms of the Acc-7 and w-F1 metrics compared to the state-of-the-art (SOTA) models.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2309205750",
                    "name": "Sheng Wu"
                },
                {
                    "authorId": "2108354640",
                    "name": "Jiaxing Liu"
                },
                {
                    "authorId": "2277569329",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                }
            ]
        },
        {
            "paperId": "a4fe48146630df3a37c3b9437cc6f551db4aa909",
            "title": "Progressive Residual Extraction based Pre-training for Speech Representation Learning",
            "abstract": "Self-supervised learning (SSL) has garnered significant attention in speech processing, excelling in linguistic tasks such as speech recognition. However, jointly improving the performance of pre-trained models on various downstream tasks, each requiring different speech information, poses significant challenges. To this purpose, we propose a progressive residual extraction based self-supervised learning method, named ProgRE. Specifically, we introduce two lightweight and specialized task modules into an encoder-style SSL backbone to enhance its ability to extract pitch variation and speaker information from speech. Furthermore, to prevent the interference of reinforced pitch variation and speaker information with irrelevant content information learning, we residually remove the information extracted by these two modules from the main branch. The main branch is then trained using HuBERT's speech masking prediction to ensure the performance of the Transformer's deep-layer features on content tasks. In this way, we can progressively extract pitch variation, speaker, and content representations from the input speech. Finally, we can combine multiple representations with diverse speech information using different layer weights to obtain task-specific representations for various downstream tasks. Experimental results indicate that our proposed method achieves joint performance improvements on various tasks, such as speaker identification, speech recognition, emotion recognition, speech enhancement, and voice conversion, compared to excellent SSL methods such as wav2vec2.0, HuBERT, and WavLM.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2276154427",
                    "name": "Tianrui Wang"
                },
                {
                    "authorId": "2319828763",
                    "name": "Jin Li"
                },
                {
                    "authorId": "2116609277",
                    "name": "Ziyang Ma"
                },
                {
                    "authorId": "2275198079",
                    "name": "Rui Cao"
                },
                {
                    "authorId": "2276453166",
                    "name": "Xie Chen"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "47589289",
                    "name": "Meng Ge"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2319424769",
                    "name": "Yuguang Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                },
                {
                    "authorId": "4577369",
                    "name": "N. Tashi"
                }
            ]
        },
        {
            "paperId": "efd754649f9283fad98aea8c9ce375fd5dd64db0",
            "title": "A Prompt-Based Hierarchical Pipeline for Cross-Domain Slot Filling",
            "abstract": "In task-oriented dialogue systems, slot filling aims to identify the semantic slot types of each token in user utterances. Due to the lack of sufficient supervised data in many scenarios, it is necessary to transfer relevant knowledge by using cross-domain slot filling. Previous studies rely on manually additional meta-information to build the relationships among similar slots across domains, yet not fully utilizing the knowledge learned by language models in the pre-training stage. In this study, we propose a prompt-based hierarchical pipeline (PHP) with three innovations. First, we design a hierarchical pipeline to separately model domain-independent syntactic structures and domain-specific semantic structures, i.e., span detection and slot prediction. Second, we improve the prompt paradigm with discriminative structure to fully utilize pre-trained language models, which reformulates downstream tasks into pre-trained tasks. Finally, we polish our template and verbalizer to effectively utilize task-specific prior knowledge, adding some meta-information and updating their additional trainable parameters. We conducted extensive experiments on three datasets to evaluate our method, and experimental results show that our method significantly outperforms the previous state-of-the-art results.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2185358961",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "2226454143",
                    "name": "Yuhang Li"
                },
                {
                    "authorId": "10785308",
                    "name": "Yuke Si"
                },
                {
                    "authorId": "2277569329",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                }
            ]
        },
        {
            "paperId": "2235f0df7efa6571007c33c3a5f3ea4286be1b9a",
            "title": "Local-Global Defense against Unsupervised Adversarial Attacks on Graphs",
            "abstract": "Unsupervised pre-training algorithms for graph representation learning are vulnerable to adversarial attacks, such as first-order perturbations on graphs, which will have an impact on particular downstream applications. Designing an effective representation learning strategy against white-box attacks remains a crucial open topic. Prior research attempts to improve representation robustness by maximizing mutual information between the representation and the perturbed graph, which is sub-optimal because it does not adapt its defense techniques to the severity of the attack. To address this issue, we propose an unsupervised defense method that combines local and global defense to improve the robustness of representation. Note that we put forward the Perturbed Edges Harmfulness (PEH) metric to determine the riskiness of the attack. Thus, when the edges are attacked, the model can automatically identify the risk of attack. We present a method of attention-based protection against high-risk attacks that penalizes attention coefficients of perturbed edges to encoders. Extensive experiments demonstrate that our strategies can enhance the robustness of representation against various adversarial attacks on three benchmark graphs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2152284414",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2222464835",
                    "name": "Bingdao Feng"
                },
                {
                    "authorId": "2148931772",
                    "name": "Siqi Guo"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2170344171",
                    "name": "Jianguo Wei"
                },
                {
                    "authorId": "2118453036",
                    "name": "Zhen Wang"
                }
            ]
        },
        {
            "paperId": "4898001845da3cfb689ca30dbbf749f125aead0a",
            "title": "Speech and Noise Dual-Stream Spectrogram Refine Network With Speech Distortion Loss For Robust Speech Recognition",
            "abstract": "In recent years, the joint training of speech enhancement front-end and automatic speech recognition (ASR) back-end has been widely used to improve the robustness of ASR systems. Traditional joint training methods only use enhanced speech as input for the backend. However, it is difficult for speech enhancement systems to directly separate speech from input due to the diverse types of noise with different intensities. Furthermore, speech distortion and residual noise are often observed in enhanced speech, and the distortion of speech and noise is different. Most existing methods focus on fusing enhanced and noisy features to address this issue. In this paper, we propose a dual-stream spectrogram refine network to simultaneously refine the speech and noise and decouple the noise from the noisy input. Our proposed method can achieve better performance with a relative 8.6% CER reduction.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2115608152",
                    "name": "Haoyu Lu"
                },
                {
                    "authorId": "2155791551",
                    "name": "Nan Li"
                },
                {
                    "authorId": "2027447617",
                    "name": "Tongtong Song"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "144699460",
                    "name": "J. Dang"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "1776581",
                    "name": "Shiliang Zhang"
                }
            ]
        },
        {
            "paperId": "7303e0faadbf441a5ecffdedf68b66147beae90f",
            "title": "A Generalized Deep Markov Random Fields Framework for Fake News Detection",
            "abstract": "Recently, the wanton dissemination of fake news on social media has adversely affected our lives, rendering automatic fake news detection a pressing issue. Current methods are often fully supervised and typically employ deep neural networks (DNN) to learn implicit relevance from labeled data, ignoring explicitly shared properties (e.g., inflammatory expressions) across fake news. To address this limitation, we propose a graph-theoretic framework, called Generalized Deep Markov Random Fields Framework (GDMRFF), that inherits the capability of deep learning while at the same time exploiting the correlations among the news articles (including labeled and unlabeled data). Specifically, we first leverage a DNN-based module to learn implicit relations, which we then reveal as the unary function of MRF. Pairwise functions with refining effects to encapsulate human insights are designed to capture the explicit association among all samples. Meanwhile, an event removal module is introduced to remove event impact on pairwise functions. Note that we train GDMRFF with the semi-supervised setting, which decreases the reliance on labeled data while maximizing the potential of unlabeled data. We further develop an Ambiguity Learning Guided MRF (ALGM) model as a concretization of GDMRFF. Experiments show that ALGM outperforms the compared methods significantly on two datasets, especially when labeled data is limited.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "12877447",
                    "name": "Yiqi Dong"
                },
                {
                    "authorId": "40137924",
                    "name": "Dongxiao He"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2222317038",
                    "name": "Yawen Li"
                },
                {
                    "authorId": "2217865116",
                    "name": "Xiaowen Su"
                },
                {
                    "authorId": "2221795887",
                    "name": "Di Jin"
                }
            ]
        },
        {
            "paperId": "d7af953e4ede906dd677257a902d22b5ead13cdb",
            "title": "Improving Zero-shot Cross-domain Slot Filling via Transformer-based Slot Semantics Fusion",
            "abstract": "Slot filling is an essential component in task-oriented dialogue systems. Due to the scarcity of annotated data, zero-shot slot filling has been studied to transfer knowledge from source domains to a target domain. Previous methods adopt slot descriptions or questions as slot semantics, where they utilize slot descriptions to calculate similarity scores, or reformat the task as a question-answering problem. However, these meth-ods do not fully exploit the token-level dependency between the slot semantics and utterances. In this study, we propose a T ransformer-based S lot semantics fusion method for S lot F illing (TSSF). We first adopt two encoders with shared weights to obtain the representations of utterances and slot semantics. Then, we design a transformer-based fusion module for effectively integrating slot semantics into utterances. Experimental results on the public benchmark SNIPS show that our model significantly outperforms the state-of-the-art model by 6.09% in terms of slot F1.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110538629",
                    "name": "Yuhang Li"
                },
                {
                    "authorId": "2185358961",
                    "name": "Xiao Wei"
                },
                {
                    "authorId": "10785308",
                    "name": "Yuke Si"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "144699460",
                    "name": "J. Dang"
                }
            ]
        },
        {
            "paperId": "e14fa05cf95d0a54ac8ff296eef50e5d608392c0",
            "title": "Multi-Modal Sarcasm Detection Based on Cross-Modal Composition of Inscribed Entity Relations",
            "abstract": "Sarcasm, a linguistic technique employed to express emotions opposite to their literal meaning, has garnered significant attention from researchers due to the rise of social media. Detecting sarcasm in a multi-modal context has become a focal point in recent studies. However, existing research primarily relies on identifying inconsistencies between text semantics and image semantics, often lacking a deep understanding of images. Consequently, capturing inconsistencies between images and texts poses a challenge in many cases. In this paper, we propose the Entity-Relational Graph Convolutional Network (ERGCN) as a solution to detect sarcasm by examining the relationship between entities within images. Our approach involves extracting entities and text descriptions from each image, which provides valuable entity information. Subsequently, we employ external knowledge to construct a cross-modal graph for each text and image pair, emphasizing the presence of internal contradictory information. Finally, we utilize the graph convolutional network to identify inconsistent information across modalities and successfully detect sarcasm. Experimental results demonstrate that our model achieves state-of-the-art performance on a widely used multimodal Twitter dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2275738704",
                    "name": "Lingshan Li"
                },
                {
                    "authorId": "2221795887",
                    "name": "Di Jin"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2275686405",
                    "name": "Fengyu Guo"
                },
                {
                    "authorId": "2127116223",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "2248034991",
                    "name": "Jianwu Dang"
                }
            ]
        },
        {
            "paperId": "e21aee869748bccf00e75f114e0f751c934fc0d4",
            "title": "Local and Global Context Modeling with Relation Matching Task for Dialog Act Recognition",
            "abstract": "In dialog act recognition (DAR) of an utterance in a conversation, the prior studies have focused either on the global context using the whole utterances in the dialog, or the local context using the neighbouring utterance flow in the dialog. However, their methods attempt to deal with all types of dialogs indiscriminately. In this study, we propose a model to extract the local context information by an inter-utterance relation matching task (RMT), and a DAR framework to incorporate the local context information into a hierarchical network to fulfil both local and global context modeling. Extensive evaluations were conducted on a Mandarin dialog corpus and two benchmark English corpora. It is found that the different dialog types possess different window lengths for RMT, which is related to the length of subtopics in a given type of dialog. According to ablation experiments, the global information contributed more to the DAR in the hierarchical framework, while the contribution ratio of the local to the global context information was larger than 0.1. The results demonstrated that the proposed RMT and DAR framework significantly improved the DAR performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "10785308",
                    "name": "Yuke Si"
                },
                {
                    "authorId": "2152821178",
                    "name": "Yan Zhang"
                },
                {
                    "authorId": "2226454143",
                    "name": "Yuhang Li"
                },
                {
                    "authorId": "2108114840",
                    "name": "Xiaobao Wang"
                },
                {
                    "authorId": "2111541663",
                    "name": "Longbiao Wang"
                },
                {
                    "authorId": "144699460",
                    "name": "J. Dang"
                },
                {
                    "authorId": "2457835",
                    "name": "E. Chng"
                },
                {
                    "authorId": "1711271",
                    "name": "Haizhou Li"
                }
            ]
        }
    ]
}