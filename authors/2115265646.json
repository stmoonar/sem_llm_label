{
    "authorId": "2115265646",
    "papers": [
        {
            "paperId": "07c4c40bd5650cdfc35fb29f314a6f6b737828f0",
            "title": "DeFiGuard: A Price Manipulation Detection Service in DeFi using Graph Neural Networks",
            "abstract": "The prosperity of Decentralized Finance (DeFi) unveils underlying risks, with reported losses surpassing 3.2 billion USD between 2018 and 2022 due to vulnerabilities in Decentralized Applications (DApps). One significant threat is the Price Manipulation Attack (PMA) that alters asset prices during transaction execution. As a result, PMA accounts for over 50 million USD in losses. To address the urgent need for efficient PMA detection, this paper introduces a novel detection service, DeFiGuard, using Graph Neural Networks (GNNs). In this paper, we propose cash flow graphs with four distinct features, which capture the trading behaviors from transactions. Moreover, DeFiGuard integrates transaction parsing, graph construction, model training, and PMA detection. Evaluations on a dataset of 208 PMA and 2,080 non-PMA transactions show that DeFiGuard with GNN models outperforms the baseline in Accuracy, TPR, FPR, and AUC-ROC. The results of ablation studies suggest that the combination of the four proposed node features enhances DeFiGuard's efficacy. Moreover, DeFiGuard classifies transactions within 0.892 to 5.317 seconds, which provides sufficient time for the victims (DApps and users) to take action to rescue their vulnerable funds. In conclusion, this research offers a significant step towards safeguarding the DeFi landscape from PMAs using GNNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1903012606",
                    "name": "Dabao Wang"
                },
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2294143864",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2307037007",
                    "name": "Lei Wu"
                },
                {
                    "authorId": "2307023629",
                    "name": "Yajin Zhou"
                },
                {
                    "authorId": "2308423078",
                    "name": "Helei Cui"
                }
            ]
        },
        {
            "paperId": "4a339e9e6b8879f785e9af607517c17b31a06154",
            "title": "Gradient Transformation: Towards Efficient and Model-Agnostic Unlearning for Dynamic Graph Neural Networks",
            "abstract": "Graph unlearning has emerged as an essential tool for safeguarding user privacy and mitigating the negative impacts of undesirable data. Meanwhile, the advent of dynamic graph neural networks (DGNNs) marks a significant advancement due to their superior capability in learning from dynamic graphs, which encapsulate spatial-temporal variations in diverse real-world applications (e.g., traffic forecasting). With the increasing prevalence of DGNNs, it becomes imperative to investigate the implementation of dynamic graph unlearning. However, current graph unlearning methodologies are designed for GNNs operating on static graphs and exhibit limitations including their serving in a pre-processing manner and impractical resource demands. Furthermore, the adaptation of these methods to DGNNs presents non-trivial challenges, owing to the distinctive nature of dynamic graphs. To this end, we propose an effective, efficient, model-agnostic, and post-processing method to implement DGNN unlearning. Specifically, we first define the unlearning requests and formulate dynamic graph unlearning in the context of continuous-time dynamic graphs. After conducting a role analysis on the unlearning data, the remaining data, and the target DGNN model, we propose a method called Gradient Transformation and a loss function to map the unlearning request to the desired parameter update. Evaluations on six real-world datasets and state-of-the-art DGNN backbones demonstrate its effectiveness (e.g., limited performance drop even obvious improvement) and efficiency (e.g., at most 7.23$\\times$ speed-up) outperformance, and potential advantages in handling future unlearning requests (e.g., at most 32.59$\\times$ speed-up).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2112166824",
                    "name": "Xiangwen Yang"
                },
                {
                    "authorId": "2274650211",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2302859288",
                    "name": "Chengqi Zhang"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "0d00c6dde034253d3d9284b89c38484c6cd666cb",
            "title": "Resisting DNN-Based Website Fingerprinting Attacks Enhanced by Adversarial Training",
            "abstract": "Deep neural network (DNN) based website fingerprinting (WF) attacks pose a severe threat to the privacy of Tor users. To overcome this challenge, adversarial perturbation based WF defenses have been recently proposed to fool the classifiers of attackers, through purposefully perturbing the user\u2019s traffic traces. Unfortunately, these defenses significantly deteriorate once the WF attacks are enhanced with adversarial training (AT). AT endows the WF attacks with more powerful website recognition capability, through learning the perturbed traffic traces generated by attackers. To resist the WF attacks enhanced by AT, we develop a black-box WF defense, called Acup3. First, Acup3 leverages many-to- one website imitation to make the traffic traces associated with different websites look more like each other, increasing the difficulty of website classification. Second, Acup3 generates trace-agnostic perturbations without accessing traffic traces, making it suitable for practical deployment. Third, Acup3 employs perturbation variation to diversify the traffic traces of different users visiting the same website, making the knowledge learnt from AT less helpful for WF attacks. Therefore, Acup3 is more robust against AT. Experiments demonstrate Acup3 markedly surpasses four representative WF defenses (e.g., Mockingbird and AWA) in defense capability and bandwidth overhead. Facing the state-of-the-art (SOTA) attack Var-CNN enhanced with AT, Acup3 depresses its attack success rate (ASR) from 98% to 24.29% with only 13.95% bandwidth overhead. Compared to the SOTA defense AWA, Acup3 causes a 24.5% larger decrement in ASR of WF attacks, and achieves a more than 100 times faster speed of perturbation generation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": null,
                    "name": "Litao Qiao"
                },
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "116530081",
                    "name": "Shuijun Yin"
                },
                {
                    "authorId": "2153614368",
                    "name": "Heng Li"
                },
                {
                    "authorId": "2114109412",
                    "name": "Wei Yuan"
                },
                {
                    "authorId": "2241567710",
                    "name": "Xiapu Luo"
                }
            ]
        },
        {
            "paperId": "3beedc05ca956bd39dc84173f3f5b4660535e814",
            "title": "Demystifying Uneven Vulnerability of Link Stealing Attacks against Graph Neural Networks",
            "abstract": "While graph neural networks (GNNs) dominate the state-of-the-art for exploring graphs in real-world applications, they have been shown to be vulnerable to a growing number of privacy attacks. For instance, link stealing is a well-known membership inference attack (MIA) on edges that infers the presence of an edge in a GNN\u2019s training graph. Recent studies on independent and identically distributed data (e.g., images) have empirically demonstrated that individuals from different groups suffer from different levels of privacy risks to MIAs, i.e., uneven vulnerability. However, theoretical evidence for such uneven vulnerability is missing. In this paper, we first present theoretical evidence of the uneven vulnerability of GNNs to link stealing attacks, which lays the foundation for demystifying such uneven risks among different groups of edges. We further demonstrate a group-based attack paradigm to expose the practical privacy harm to GNN users derived from the uneven vulnerability of edges. Finally, we empirically validate the existence of obvious uneven vulnerability on ten real-world datasets (e.g., about 25% AUC difference between different groups in the Credit graph). Compared with existing methods, the outperformance of our group-based attack paradigm confirms that cus-tomising different strategies for different groups results in more effective privacy attacks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2117010331",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2112166824",
                    "name": "Xiangwen Yang"
                },
                {
                    "authorId": "2837434",
                    "name": "Minhui Xue"
                },
                {
                    "authorId": "2153326034",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                }
            ]
        },
        {
            "paperId": "93b5f8111957a5b2ba36e1b4377d49b0309f3ae3",
            "title": "GraphGuard: Detecting and Counteracting Training Data Misuse in Graph Neural Networks",
            "abstract": "The emergence of Graph Neural Networks (GNNs) in graph data analysis and their deployment on Machine Learning as a Service platforms have raised critical concerns about data misuse during model training. This situation is further exacerbated due to the lack of transparency in local training processes, potentially leading to the unauthorized accumulation of large volumes of graph data, thereby infringing on the intellectual property rights of data owners. Existing methodologies often address either data misuse detection or mitigation, and are primarily designed for local GNN models rather than cloud-based MLaaS platforms. These limitations call for an effective and comprehensive solution that detects and mitigates data misuse without requiring exact training data while respecting the proprietary nature of such data. This paper introduces a pioneering approach called GraphGuard, to tackle these challenges. We propose a training-data-free method that not only detects graph data misuse but also mitigates its impact via targeted unlearning, all without relying on the original training data. Our innovative misuse detection technique employs membership inference with radioactive data, enhancing the distinguishability between member and non-member data distributions. For mitigation, we utilize synthetic graphs that emulate the characteristics previously learned by the target model, enabling effective unlearning even in the absence of exact graph data. We conduct comprehensive experiments utilizing four real-world graph datasets to demonstrate the efficacy of GraphGuard in both detection and unlearning. We show that GraphGuard attains a near-perfect detection rate of approximately 100% across these datasets with various GNN models. In addition, it performs unlearning by eliminating the impact of the unlearned graph with a marginal decrease in accuracy (less than 5%).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2112166824",
                    "name": "Xiangwen Yang"
                },
                {
                    "authorId": "2274079627",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2273925304",
                    "name": "Minhui Xue"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2274650211",
                    "name": "Xingliang Yuan"
                }
            ]
        },
        {
            "paperId": "c8c865abf67950ab6a8c7ea042861fb05ae97d27",
            "title": "Securing Graph Neural Networks in MLaaS: A Comprehensive Realization of Query-based Integrity Verification",
            "abstract": "The deployment of Graph Neural Networks (GNNs) within Machine Learning as a Service (MLaaS) has opened up new attack surfaces and an escalation in security concerns regarding model-centric attacks. These attacks can directly manipulate the GNN model parameters during serving, causing incorrect predictions and posing substantial threats to essential GNN applications. Traditional integrity verification methods falter in this context due to the limitations imposed by MLaaS and the distinct characteristics of GNN models.In this research, we introduce a groundbreaking approach to protect GNN models in MLaaS from model-centric attacks. Our approach includes a comprehensive verification schema for GNN\u2019s integrity, taking into account both transductive and inductive GNNs, and accommodating varying pre-deployment knowledge of the models. We propose a query-based verification technique, fortified with innovative node fingerprint generation algorithms. To deal with advanced attackers who know our mechanisms in advance, we introduce randomized fingerprint nodes within our design. The experimental evaluation demonstrates that our method can detect five representative adversarial model-centric attacks, displaying 2 to 4 times greater efficiency compared to baselines.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2274650211",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2274079627",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2274083090",
                    "name": "Qi Li"
                },
                {
                    "authorId": "2273925304",
                    "name": "Minhui Xue"
                },
                {
                    "authorId": "2274067913",
                    "name": "Shirui Pan"
                }
            ]
        },
        {
            "paperId": "d3b408c0c4a71b54cc794c723167dbe2ca7aff70",
            "title": "Black-box Adversarial Example Attack towards FCG Based Android Malware Detection under Incomplete Feature Information",
            "abstract": "The function call graph (FCG) based Android malware detection methods have recently attracted increasing attention due to their promising performance. However, these methods are susceptible to adversarial examples (AEs). In this paper, we design a novel black-box AE attack towards the FCG based malware detection system, called BagAmmo. To mislead its target system, BagAmmo purposefully perturbs the FCG feature of malware through inserting\"never-executed\"function calls into malware code. The main challenges are two-fold. First, the malware functionality should not be changed by adversarial perturbation. Second, the information of the target system (e.g., the graph feature granularity and the output probabilities) is absent. To preserve malware functionality, BagAmmo employs the try-catch trap to insert function calls to perturb the FCG of malware. Without the knowledge about feature granularity and output probabilities, BagAmmo adopts the architecture of generative adversarial network (GAN), and leverages a multi-population co-evolution algorithm (i.e., Apoem) to generate the desired perturbation. Every population in Apoem represents a possible feature granularity, and the real feature granularity can be achieved when Apoem converges. Through extensive experiments on over 44k Android apps and 32 target models, we evaluate the effectiveness, efficiency and resilience of BagAmmo. BagAmmo achieves an average attack success rate of over 99.9% on MaMaDroid, APIGraph and GCN, and still performs well in the scenario of concept drift and data imbalance. Moreover, BagAmmo outperforms the state-of-the-art attack SRL in attack success rate.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2153614368",
                    "name": "Heng Li"
                },
                {
                    "authorId": "2168183425",
                    "name": "Zhang Cheng"
                },
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2156968112",
                    "name": "Liheng Yuan"
                },
                {
                    "authorId": "3399452",
                    "name": "Cuiying Gao"
                },
                {
                    "authorId": "2114109412",
                    "name": "Wei Yuan"
                },
                {
                    "authorId": "2131805425",
                    "name": "Xi Luo"
                }
            ]
        },
        {
            "paperId": "21913eb287f8fc33db8f6274fd2a07072c4e11eb",
            "title": "Trustworthy Graph Neural Networks: Aspects, Methods, and Trends",
            "abstract": "Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications such as recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects, such as vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterized by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehensively summarize existing efforts for trustworthy GNNs from six aspects, including robustness, explainability, privacy, fairness, accountability, and environmental well-being. In addition, we highlight the intricate cross-aspect relations between the above six aspects of trustworthy GNNs. Finally, we present a thorough overview of trending directions for facilitating the research and industrialization of trustworthy GNNs.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2156713249",
                    "name": "He Zhang"
                },
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "2153326034",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "8163721",
                    "name": "Hanghang Tong"
                },
                {
                    "authorId": "2112496348",
                    "name": "Jian Pei"
                }
            ]
        },
        {
            "paperId": "f04af8802e1219b6c0dc056ea0527ebb89e1d52f",
            "title": "Leia: A Lightweight Cryptographic Neural Network Inference System at the Edge",
            "abstract": "The advances in machine learning have revealed its great potential for emerging mobile applications such as face recognition and voice assistant. Models trained via a Neural Network (NN) can offer accurate and efficient inference services for mobile users. Unfortunately, the current deployment of such service encounters privacy concerns. Directly offloading the model to the mobile device violates model privacy of the model owner, while feeding user input to the service compromises user privacy. To address this issue, we propose Leia, a lightweight cryptographic NN inference system at the edge. Leia is designed from two mobile-friendly perspectives. First, it leverages the paradigm of edge computing wherein the inference procedure keeps the model closer to the mobile user to foster low latency service. Specifically, Leia\u2019s architecture consists of two non-colluding edge services to obliviously perform NN inference on the encoded user data and model. Second, Leia\u2019s realization makes the judicious use of potentially constrained computational and communication resources in edge devices. We adapt the Binarized Neural Network (BNN), a trending flavor of NN with low inference overhead, and purely choose the lightweight secret sharing techniques to realize secure blocks of BNN. We implement Leia and deploy it on Raspberry Pi. Empirical evaluations on benchmark and medical datasets via various models demonstrate the practicality of Leia.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2110661801",
                    "name": "Xiaoning Liu"
                },
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                },
                {
                    "authorId": "143824407",
                    "name": "X. Yi"
                }
            ]
        },
        {
            "paperId": "244d8f4fc47b94efb976016acf51f3ea5852034f",
            "title": "Towards Extracting Graph Neural Network Models via Prediction Queries (Student Abstract)",
            "abstract": "Graph data has been widely used to represent data from various domain, e.g., social networks, recommendation system. With great power, the GNN models, usually as valuable properties of their owners, also become attractive targets of the adversary who covets to steal them. While existing works show that simple deep neural networks can be reproduced by so-called Model Extraction Attacks, how to extract a GNN model has not been explored. In this paper, we exploit the threat of model extraction attacks against GNN models. Unlike ordinary attacks which obtain model information via only the input-output query pairs, we utilize both the node queries and the graph structure to extract the GNNs. Furthermore, we consider the stealthiness of the attack and propose to generate legitimate queries so the extraction can be applied discreetly. We implement our attack by leveraging the responses of these queries, as well as other accessible knowledge, e.g., neighbor connectives of the queried nodes. By evaluating over three real-world datasets, our attack is shown to effectively produce a surrogate model with more than 80% equivalent predictions as the target model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2115265646",
                    "name": "Bang Wu"
                },
                {
                    "authorId": "2585415",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "3032058",
                    "name": "Xingliang Yuan"
                }
            ]
        }
    ]
}