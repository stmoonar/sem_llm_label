{
    "authorId": "1723161",
    "papers": [
        {
            "paperId": "03f487649f57177a33a61c79ad2dd74d8022f247",
            "title": "Cross-Lingual Link Discovery for Under-Resourced Languages",
            "abstract": "In this paper, we provide an overview of current technologies for cross-lingual link discovery, and we discuss challenges, experiences and prospects of their application to under-resourced languages. We rst introduce the goals of cross-lingual linking and associated technologies, and in particular, the role that the Linked Data paradigm (Bizer et al., 2011) applied to language data can play in this context. We de ne under-resourced languages with a speci c focus on languages actively used on the internet, i.e., languages with a digitally versatile speaker community, but limited support in terms of language technology. We argue that languages for which considerable amounts of textual data and (at least) a bilingual word list are available, techniques for cross-lingual linking can be readily applied, and that these enable the implementation of downstream applications for under-resourced languages via the localisation and adaptation of existing technologies and resources.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "35041118",
                    "name": "M. Rosner"
                },
                {
                    "authorId": "35183492",
                    "name": "Sina Ahmadi"
                },
                {
                    "authorId": "3202565",
                    "name": "E. Apostol"
                },
                {
                    "authorId": "1409496551",
                    "name": "Julia Bosque-Gil"
                },
                {
                    "authorId": "1723161",
                    "name": "C. Chiarcos"
                },
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "2598237",
                    "name": "K. Gkirtzou"
                },
                {
                    "authorId": "143708147",
                    "name": "J. Gracia"
                },
                {
                    "authorId": "2640975",
                    "name": "Dagmar Gromann"
                },
                {
                    "authorId": "2601200",
                    "name": "Chaya Liebeskind"
                },
                {
                    "authorId": "118562165",
                    "name": "G. Ole\u0161kevi\u010dien\u0117"
                },
                {
                    "authorId": "1753940",
                    "name": "Gilles S\u00e9rasset"
                },
                {
                    "authorId": "39812162",
                    "name": "Ciprian-Octavian Truic\u0103"
                }
            ]
        },
        {
            "paperId": "0d4259c8f8cddc734fea39287ffa43591794756a",
            "title": "ISO-based Annotated Multilingual Parallel Corpus for Discourse Markers",
            "abstract": "Discourse markers carry information about the discourse structure and organization, and also signal local dependencies or epistemological stance of speaker. They provide instructions on how to interpret the discourse, and their study is paramount to understand the mechanism underlying discourse organization. This paper presents a new language resource, an ISO-based annotated multilingual parallel corpus for discourse markers. The corpus comprises nine languages, Bulgarian, Lithuanian, German, European Portuguese, Hebrew, Romanian, Polish, and Macedonian, with English as a pivot language. In order to represent the meaning of the discourse markers, we propose an annotation scheme of discourse relations from ISO 24617-8 with a plug-in to ISO 24617-2 for communicative functions. We describe an experiment in which we applied the annotation scheme to assess its validity. The results reveal that, although some extensions are required to cover all the multilingual data, it provides a proper representation of discourse markers value. Additionally, we report some relevant contrastive phenomena concerning discourse markers interpretation and role in discourse. This first step will allow us to develop deep learning methods to identify and extract discourse relations and communicative functions, and to represent that information as Linguistic Linked Open Data (LLOD).",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2092315310",
                    "name": "P. Silvano"
                },
                {
                    "authorId": "2680139",
                    "name": "M. Damova"
                },
                {
                    "authorId": "118562165",
                    "name": "G. Ole\u0161kevi\u010dien\u0117"
                },
                {
                    "authorId": "2601200",
                    "name": "Chaya Liebeskind"
                },
                {
                    "authorId": "1723161",
                    "name": "C. Chiarcos"
                },
                {
                    "authorId": "3034066",
                    "name": "D. Trajanov"
                },
                {
                    "authorId": "39812162",
                    "name": "Ciprian-Octavian Truic\u0103"
                },
                {
                    "authorId": "3202565",
                    "name": "E. Apostol"
                },
                {
                    "authorId": "69416609",
                    "name": "A. B\u0105czkowska"
                }
            ]
        },
        {
            "paperId": "0f7de30aedaf838b83d7d740a9cbe8619c7ae32b",
            "title": "Querying a Dozen Corpora and a Thousand Years with Fintan",
            "abstract": "Large-scale diachronic corpus studies covering longer time periods are difficult if more than one corpus are to be consulted and, as a result, different formats and annotation schemas need to be processed and queried in a uniform, comparable and replicable manner. We describes the application of the Flexible Integrated Transformation and Annotation eNgineering (Fintan) platform for studying word order in German using syntactically annotated corpora that represent its entire written history. Focusing on nominal dative and accusative arguments, this study hints at two major phases in the development of scrambling in modern German. Against more recent assumptions, it supports the traditional view that word order flexibility decreased over time, but it also indicates that this was a relatively sharp transition in Early New High German. The successful case study demonstrates the potential of Fintan and the underlying LLOD technology for historical linguistics, linguistic typology and corpus linguistics. The technological contribution of this paper is to demonstrate the applicability of Fintan for querying across heterogeneously annotated corpora, as previously, it had only been applied for transformation tasks. With its focus on quantitative analysis, Fintan is a natural complement for existing multi-layer technologies that focus on query and exploration.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1723161",
                    "name": "C. Chiarcos"
                },
                {
                    "authorId": "3448712",
                    "name": "Christian F\u00e4th"
                },
                {
                    "authorId": "144190007",
                    "name": "Maxim Ionov"
                }
            ]
        },
        {
            "paperId": "110904dee1ec18f051e64832a36b85c901d44aeb",
            "title": "When linguistics meets web technologies. Recent advances in modelling linguistic linked data",
            "abstract": "This article provides a comprehensive and up-to-date survey of models and vocabularies for creating linguistic linked data (LLD) focusing on the latest developments in the area and both building upon and complementing previous works covering similar territory. The article begins with an overview of some recent trends which have had a significant impact on linked data models and vocabularies. Next, we give a general overview of existing vocabularies and models for different categories of LLD resource. After which we look at some of the latest developments in community standards and initiatives including descriptions of recent work on the OntoLex-Lemon model, a survey of recent initiatives in linguistic annotation and LLD, and a discussion of the LLD metadata vocabularies META-SHARE and lime. In the next part of the paper, we focus on the influence of projects on LLD models and vocabularies, starting with a general survey of relevant projects, before dedicating individual sections to a number of recent projects and their impact on LLD vocabularies and models. Finally, in the conclusion, we look ahead at some future challenges for LLD models and vocabularies. The appendix to the paper consists of a brief introduction to the OntoLex-Lemon model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "104486550",
                    "name": "Anas Fahad Khan"
                },
                {
                    "authorId": "1723161",
                    "name": "C. Chiarcos"
                },
                {
                    "authorId": "72836788",
                    "name": "T. Declerck"
                },
                {
                    "authorId": "2717024",
                    "name": "Daniela G\u00eefu"
                },
                {
                    "authorId": "2111355960",
                    "name": "Elena Gonz\u00e1lez-Blanco Garc\u00eda"
                },
                {
                    "authorId": "143708147",
                    "name": "J. Gracia"
                },
                {
                    "authorId": "144190007",
                    "name": "Maxim Ionov"
                },
                {
                    "authorId": "2884435",
                    "name": "Penny Labropoulou"
                },
                {
                    "authorId": "2043236",
                    "name": "Francesco Mambrini"
                },
                {
                    "authorId": "1689974",
                    "name": "John P. McCrae"
                },
                {
                    "authorId": "1405070418",
                    "name": "\u00c9milie Pag\u00e9-Perron"
                },
                {
                    "authorId": "3271120",
                    "name": "M. Passarotti"
                },
                {
                    "authorId": "2067418969",
                    "name": "Salvador Ros Mu\u00f1oz"
                },
                {
                    "authorId": "39812162",
                    "name": "Ciprian-Octavian Truic\u0103"
                }
            ]
        },
        {
            "paperId": "167fa157be5aa31dafb27cd4b106cfbb1e1b436b",
            "title": "A Survey of Guidelines and Best Practices for the Generation, Interlinking, Publication, and Validation of Linguistic Linked Data",
            "abstract": "This article discusses a survey carried out within the NexusLinguarum COST Action which aimed to give an overview of existing guidelines (GLs) and best practices (BPs) in linguistic linked data. In particular it focused on four core tasks in the production/publication of linked data: generation, interlinking, publication, and validation. We discuss the importance of GLs and BPs for LLD before describing the survey and its results in full. Finally we offer a number of directions for future work in order to address the findings of the survey.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145769397",
                    "name": "Fahad Khan"
                },
                {
                    "authorId": "1723161",
                    "name": "C. Chiarcos"
                },
                {
                    "authorId": "1743895",
                    "name": "Thierry Declerck"
                },
                {
                    "authorId": "1738661501",
                    "name": "Maria Pia di Buono"
                },
                {
                    "authorId": "1819564",
                    "name": "Milan Dojchinovski"
                },
                {
                    "authorId": "49458747",
                    "name": "Jorge Gracia"
                },
                {
                    "authorId": "118562165",
                    "name": "G. Ole\u0161kevi\u010dien\u0117"
                },
                {
                    "authorId": "2717024",
                    "name": "Daniela G\u00eefu"
                }
            ]
        },
        {
            "paperId": "331401000458579495bc596bbafc038d81867d94",
            "title": "Unifying Morphology Resources with OntoLex-Morph. A Case Study in German",
            "abstract": "The OntoLex vocabulary has become a widely used community standard for machine-readable lexical resources on the web. The primary motivation to use OntoLex in favor of tool- or application-specific formalisms is to facilitate interoperability and information integration across different resources. One of its extension that is currently being developed is a module for representing morphology, OntoLex-Morph. In this paper, we show how OntoLex-Morph can be used for the encoding and integration of different types of morphological resources on a unified basis. With German as the example, we demonstrate it for (a) a full-form dictionary with inflection information (Unimorph), (b) a dictionary of base forms and their derivations (UDer), (c) a dictionary of compounds (from GermaNet), and (d) lexicon and inflection rules of a finite-state parser/generator (SMOR/Morphisto). These data are converted to OntoLex-Morph, their linguistic information is consolidated and corresponding lexical entries are linked with each other.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1723161",
                    "name": "C. Chiarcos"
                },
                {
                    "authorId": "3448712",
                    "name": "Christian F\u00e4th"
                },
                {
                    "authorId": "144190007",
                    "name": "Maxim Ionov"
                }
            ]
        },
        {
            "paperId": "7ceae25adc6897c86570a9cb3d1a7b25a17a98d5",
            "title": "A Cheap and Dirty Cross-Lingual Linking Service in the Cloud",
            "abstract": "In this paper, we describe the application of Linguistic Linked Open Data (LLOD) technology for dynamic cross-lingual querying on demand. Whereas most related research is focusing on providing a static linking, i.e., cross-lingual inference, and then storing the resulting links, we demonstrate the application of the federation capabilities of SPARQL to perform lexical linking on the fly. In the end, we provide a baseline functionality that uses the connection of two web services \u2013 a SPARQL end point for multilingual lexical data and another SPARQL end point for querying an English language knowledge graph \u2013 in order to perform querying an English language knowledge graph using foreign language labels. We argue that, for low-resource languages where substantial native knowledge graphs are lacking, this functionality can be used to lower the language barrier by allowing to formulate cross-linguistically applicable queries mediated by a multilingual dictionary.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1723161",
                    "name": "C. Chiarcos"
                },
                {
                    "authorId": "1753940",
                    "name": "Gilles S\u00e9rasset"
                }
            ]
        },
        {
            "paperId": "7e686a1cf4a07eb2368cad8af23e632efbca1ac9",
            "title": "Spicy Salmon: Converting between 50+ Annotation Formats with Fintan, Pepper, Salt and Powla",
            "abstract": "Heterogeneity of formats, models and annotations has always been a primary hindrance for exploiting the ever increasing amount of existing linguistic resources for real world applications in and beyond NLP. Fintan - the Flexible INtegrated Transformation and Annotation eNgineering platform introduced in 2020 is designed to rapidly convert, combine and manipulate language resources both in and outside the Semantic Web by transforming it into segmented RDF representations which can be processed in parallel on a multithreaded environment and integrating it with ontologies and taxonomies. Fintan has recently been extended with a set of additional modules increasing the amount of supported non-RDF formats and the interoperability with existing non-JAVA conversion tools, and parts of this work are demonstrated in this paper. In particular, we focus on a novel recipe for resource transformation in which Fintan works in tandem with the Pepper toolset to allow computational linguists to transform their data between over 50 linguistic corpus formats with a graphical workflow manager.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3448712",
                    "name": "Christian F\u00e4th"
                },
                {
                    "authorId": "1723161",
                    "name": "C. Chiarcos"
                }
            ]
        },
        {
            "paperId": "c74e0d60da84aadeef64f965ec08431991524b30",
            "title": "Modelling Frequency, Attestation, and Corpus-Based Information with OntoLex-FrAC",
            "abstract": "OntoLex-Lemon has become a de facto standard for lexical resources in the web of data. This paper provides the first overall description of the emerging OntoLex module for Frequency, Attestations, and Corpus-Based Information (OntoLex-FrAC) that is intended to complement OntoLex-Lemon with the necessary vocabulary to represent major types of information found in or automatically derived from corpora, for applications in both language technology and the language sciences.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1723161",
                    "name": "C. Chiarcos"
                },
                {
                    "authorId": "3202565",
                    "name": "E. Apostol"
                },
                {
                    "authorId": "2715062",
                    "name": "Besim Kabashi"
                },
                {
                    "authorId": "39812162",
                    "name": "Ciprian-Octavian Truic\u0103"
                }
            ]
        },
        {
            "paperId": "cc671e5e4ff92d82fd06b869c2ae7d78da0e6fe9",
            "title": "Inducing Discourse Marker Inventories from Lexical Knowledge Graphs",
            "abstract": "Discourse marker inventories are important tools for the development of both discourse parsers and corpora with discourse annotations. In this paper we explore the potential of massively multilingual lexical knowledge graphs to induce multilingual discourse marker lexicons using concept propagation methods as previously developed in the context of translation inference across dictionaries. Given one or multiple source languages with discourse marker inventories that discourse relations as senses of potential discourse markers, as well as a large number of bilingual dictionaries that link them \u2013 directly or indirectly \u2013 with the target language, we specifically study to what extent discourse marker induction can benefit from the integration of information from different sources, the impact of sense granularity and what limiting factors may need to be considered. Our study uses discourse marker inventories from nine European languages normalized against the discourse relation inventory of the Penn Discourse Treebank (PDTB), as well as three collections of machine-readable dictionaries with different characteristics, so that the interplay of a large number of factors can be studied.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1723161",
                    "name": "C. Chiarcos"
                }
            ]
        }
    ]
}