{
    "authorId": "2257034822",
    "papers": [
        {
            "paperId": "fc080333b3a7e39d20dd362f2f9805014d531aaa",
            "title": "LitSearch: A Retrieval Benchmark for Scientific Literature Search",
            "abstract": "Literature search questions, such as\"where can I find research on the evaluation of consistency in generated summaries?\"pose significant challenges for modern search engines and retrieval systems. These questions often require a deep understanding of research concepts and the ability to reason over entire articles. In this work, we introduce LitSearch, a retrieval benchmark comprising 597 realistic literature search queries about recent ML and NLP papers. LitSearch is constructed using a combination of (1) questions generated by GPT-4 based on paragraphs containing inline citations from research papers and (2) questions about recently published papers, manually written by their authors. All LitSearch questions were manually examined or edited by experts to ensure high quality. We extensively benchmark state-of-the-art retrieval models and also evaluate two LLM-based reranking pipelines. We find a significant performance gap between BM25 and state-of-the-art dense retrievers, with a 24.8% difference in absolute recall@5. The LLM-based reranking strategies further improve the best-performing dense retriever by 4.4%. Additionally, commercial search engines and research tools like Google Search perform poorly on LitSearch, lagging behind the best dense retriever by 32 points. Taken together, these results show that LitSearch is an informative new testbed for retrieval systems while catering to a real-world use case.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2218438150",
                    "name": "Anirudh Ajith"
                },
                {
                    "authorId": "67284811",
                    "name": "Mengzhou Xia"
                },
                {
                    "authorId": "2284703052",
                    "name": "Alexis Chevalier"
                },
                {
                    "authorId": "2257034822",
                    "name": "Tanya Goyal"
                },
                {
                    "authorId": "2311929494",
                    "name": "Danqi Chen"
                },
                {
                    "authorId": "4800645",
                    "name": "Tianyu Gao"
                }
            ]
        },
        {
            "paperId": "6f217d984f36499d88ab8a3d89572171552e6f3f",
            "title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "abstract": "As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these ``LLM evaluators'', particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2150468823",
                    "name": "Zhiyuan Zeng"
                },
                {
                    "authorId": "2257230025",
                    "name": "Jiatong Yu"
                },
                {
                    "authorId": "4800645",
                    "name": "Tianyu Gao"
                },
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "2257034822",
                    "name": "Tanya Goyal"
                },
                {
                    "authorId": "50536468",
                    "name": "Danqi Chen"
                }
            ]
        }
    ]
}