{
    "authorId": "2138604826",
    "papers": [
        {
            "paperId": "49f20646bc93d5cf36ecfc43d38db0968403e478",
            "title": "Recent Approaches and Trends in Approximate Nearest Neighbor Search, with Remarks on Benchmarking",
            "abstract": "Nearest neighbor search is a computational primitive whose ef\ufb01ciency is paramount to many applications. As such, the literature recently blossomed with many works focusing on improving its effectiveness in an approximate setting. In this overview paper, we review recent advances of the state of the art and discuss some trends. Given the practical relevance of the problem, new approaches need to be thoroughly benchmarked. We therefore review some recent benchmarking efforts and provide advice on the benchmarking pipeline",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138604826",
                    "name": "Martin Aum\u00fcller"
                },
                {
                    "authorId": "2739224",
                    "name": "Matteo Ceccarello"
                }
            ]
        },
        {
            "paperId": "558cddf324adb1970089530b4e5cbde9b9c57667",
            "title": "Representing Sparse Vectors with Differential Privacy, Low Error, Optimal Space, and Fast Access",
            "abstract": "\nRepresenting a sparse histogram, or more generally a sparse vector, is a fundamental task in differential privacy. \nAn ideal solution would use space close to information-theoretical lower bounds, have an error distribution that depends optimally on the desired privacy level, and allow fast random access to entries in the vector. \nHowever, existing approaches have only achieved two of these three goals. \n\u00a0 \nIn this paper we introduce the Approximate Laplace Projection (ALP) mechanism for approximating k-sparse vectors. This mechanism is shown to simultaneously have information-theoretically optimal space (up to constant factors), fast access to vector entries, and error of the same magnitude as the Laplace-mechanism applied to dense vectors. \nA key new technique is a unary representation of small integers, which we show to be robust against ''randomized response'' noise. This representation is combined with hashing, in the spirit of Bloom filters, to obtain a space-efficient, differentially private representation. \n \nOur theoretical performance bounds are complemented by simulations which show that the constant factors on the main performance parameters are quite small, suggesting practicality of the technique. \n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "31134809",
                    "name": "C. Lebeda"
                },
                {
                    "authorId": "2138604826",
                    "name": "Martin Aum\u00fcller"
                },
                {
                    "authorId": "1801719",
                    "name": "R. Pagh"
                }
            ]
        },
        {
            "paperId": "ba818901c0e2108a7e5dda901e21668d21b050f2",
            "title": "Sampling near neighbors in search for fairness",
            "abstract": "Similarity search is a fundamental algorithmic primitive, widely used in many computer science disciplines. Given a set of points S and a radius parameter r > 0, the r-near neighbor (r-NN) problem asks for a data structure that, given any query point q, returns a point p within distance at most r from q. In this paper, we study the r-NN problem in the light of individual fairness and providing equal opportunities: all points that are within distance r from the query should have the same probability to be returned. The problem is of special interest in high dimensions, where Locality Sensitive Hashing (LSH), the theoretically leading approach to similarity search, does not provide any fairness guarantee. In this work, we show that LSH-based algorithms can be made fair, without a significant loss in efficiency. We propose several efficient data structures for the exact and approximate variants of the fair NN problem. Our approach works more generally for sampling uniformly from a sub-collection of sets of a given collection and can be used in a few other applications. We also carried out an experimental evaluation that highlights the inherent unfairness of existing NN data structures.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138604826",
                    "name": "Martin Aum\u00fcller"
                },
                {
                    "authorId": "1388677326",
                    "name": "Sariel Har-Peled"
                },
                {
                    "authorId": "145700959",
                    "name": "S. Mahabadi"
                },
                {
                    "authorId": "1801719",
                    "name": "R. Pagh"
                },
                {
                    "authorId": "35007525",
                    "name": "Francesco Silvestri"
                }
            ]
        },
        {
            "paperId": "fc50d3d9f2838e0c87068c3839e7d18ea6a7c7ad",
            "title": "Results of the NeurIPS'21 Challenge on Billion-Scale Approximate Nearest Neighbor Search",
            "abstract": "Despite the broad range of algorithms for Approximate Nearest Neighbor Search, most empirical evaluations of algorithms have focused on smaller datasets, typically of 1 million points~\\citep{Benchmark}. However, deploying recent advances in embedding based techniques for search, recommendation and ranking at scale require ANNS indices at billion, trillion or larger scale. Barring a few recent papers, there is limited consensus on which algorithms are effective at this scale vis-\\`a-vis their hardware cost. This competition compares ANNS algorithms at billion-scale by hardware cost, accuracy and performance. We set up an open source evaluation framework and leaderboards for both standardized and specialized hardware. The competition involves three tracks. The standard hardware track T1 evaluates algorithms on an Azure VM with limited DRAM, often the bottleneck in serving billion-scale indices, where the embedding data can be hundreds of GigaBytes in size. It uses FAISS~\\citep{Faiss17} as the baseline. The standard hardware track T2 additional allows inexpensive SSDs in addition to the limited DRAM and uses DiskANN~\\citep{DiskANN19} as the baseline. The specialized hardware track T3 allows any hardware configuration, and again uses FAISS as the baseline. We compiled six diverse billion-scale datasets, four newly released for this competition, that span a variety of modalities, data types, dimensions, deep learning models, distance functions and sources. The outcome of the competition was ranked leaderboards of algorithms in each track based on recall at a query throughput threshold. Additionally, for track T3, separate leaderboards were created based on recall as well as cost-normalized and power-normalized query throughput.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2492972",
                    "name": "H. Simhadri"
                },
                {
                    "authorId": "2107433300",
                    "name": "G. Williams"
                },
                {
                    "authorId": "2138604826",
                    "name": "Martin Aum\u00fcller"
                },
                {
                    "authorId": "3271933",
                    "name": "Matthijs Douze"
                },
                {
                    "authorId": "143743802",
                    "name": "Artem Babenko"
                },
                {
                    "authorId": "35921582",
                    "name": "Dmitry Baranchuk"
                },
                {
                    "authorId": "1819450790",
                    "name": "Qi Chen"
                },
                {
                    "authorId": "26360550",
                    "name": "Lucas Hosseini"
                },
                {
                    "authorId": "1826157",
                    "name": "Ravishankar Krishnaswamy"
                },
                {
                    "authorId": "2064586974",
                    "name": "Gopal Srinivasa"
                },
                {
                    "authorId": "71513226",
                    "name": "Suhas Jayaram Subramanya"
                },
                {
                    "authorId": "2109534192",
                    "name": "Jingdong Wang"
                }
            ]
        },
        {
            "paperId": "f82fa87c8e619e5823459ad420aff224b32a6d91",
            "title": "Fair near neighbor search via sampling",
            "abstract": "Similarity search is a fundamental algorithmic primitive, widely used in many computer science disciplines. Given a set of points S and a radius parameter r > 0, the rnear neighbor (r-NN) problem asks for a data structure that, given any query point q, returns a point p within distance at most r from q. In this paper, we study the r-NN problem in the light of individual fairness and providing equal opportunities: all points that are within distance r from the query should have the same probability to be returned. In the low-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS 2014). Locality sensitive hashing (LSH), the theoretically strongest approach to similarity search in high dimensions, does not provide such a fairness guarantee.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138604826",
                    "name": "Martin Aum\u00fcller"
                },
                {
                    "authorId": "1388677326",
                    "name": "Sariel Har-Peled"
                },
                {
                    "authorId": "145700959",
                    "name": "S. Mahabadi"
                },
                {
                    "authorId": "1801719",
                    "name": "R. Pagh"
                },
                {
                    "authorId": "35007525",
                    "name": "Francesco Silvestri"
                }
            ]
        },
        {
            "paperId": "866c41925524a5954a1b030e8f4aa59b6ef86720",
            "title": "Algorithm Engineering for High-Dimensional Similarity Search Problems (Invited Talk)",
            "abstract": "Similarity search problems in high-dimensional data arise in many areas of computer science such as data bases, image analysis, machine learning, and natural language processing. One of the most prominent problems is finding the k nearest neighbors of a data point q \u2208 \u211d^d in a large set of data points S \u2282 \u211d^d, under same distance measure such as Euclidean distance. In contrast to lower dimensional settings, we do not know of worst-case efficient data structures for such search problems in high-dimensional data, i.e., data structures that are faster than a linear scan through the data set. However, there is a rich body of (often heuristic) approaches that solve nearest neighbor search problems much faster than such a scan on many real-world data sets. As a necessity, the term solve means that these approaches give approximate results that are close to the true k-nearest neighbors. In this talk, we survey recent approaches to nearest neighbor search and related problems. \nThe talk consists of three parts: (1) What makes nearest neighbor search difficult? (2) How do current state-of-the-art algorithms work? (3) What are recent advances regarding similarity search on GPUs, in distributed settings, or in external memory?",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138604826",
                    "name": "Martin Aum\u00fcller"
                }
            ]
        },
        {
            "paperId": "906f7a85c047c1380f802997bad58f3b1e471159",
            "title": "Reproducibility Companion Paper: Visual Sentiment Analysis for Review Images with Item-Oriented and User-Oriented CNN",
            "abstract": "We revisit our contributions on visual sentiment analysis for online review images published at ACM Multimedia 2017, where we develop item-oriented and user-oriented convolutional neural networks that better capture the interaction of image features with specific expressions of users or items. In this work, we outline the experimental claims as well as describe the procedures to reproduce the results therein. In addition, we provide artifacts including data sets and code to replicate the experiments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "39603708",
                    "name": "Quoc-Tuan Truong"
                },
                {
                    "authorId": "3309003",
                    "name": "Hady W. Lauw"
                },
                {
                    "authorId": "2138604826",
                    "name": "Martin Aum\u00fcller"
                },
                {
                    "authorId": "2939508",
                    "name": "Naoko Nitta"
                }
            ]
        },
        {
            "paperId": "41f9f3c2808114772c6b5753f97eb8e5853df6b6",
            "title": "Benchmarking Nearest Neighbor Search: Influence of Local Intrinsic Dimensionality and Result Diversity in Real-World Datasets",
            "abstract": "This paper reconsiders common benchmarking approaches to nearest neighbor search. It is shown that the concept of local intrinsic dimensionality (LID) allows to choose query sets of a wide range of difficulty for real-world datasets. Moreover, the effect of different LID distributions on the running time performance of implementations is empirically studied. To this end, different visualization concepts are introduced that allow to get a more fine-grained overview of the inner workings of nearest neighbor search principles. The paper closes with remarks about the diversity of datasets commonly used for nearest neighbor search benchmarking. It is shown that such real-world datasets are not diverse: results on a single dataset predict results on all other datasets well.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138604826",
                    "name": "Martin Aum\u00fcller"
                },
                {
                    "authorId": "2739224",
                    "name": "Matteo Ceccarello"
                }
            ]
        },
        {
            "paperId": "a31099a90f02394ef3400054cbf8372ff7ebef30",
            "title": "PUFFINN: Parameterless and Universally Fast FInding of Nearest Neighbors",
            "abstract": "We present PUFFINN, a parameterless LSH-based index for solving the $k$-nearest neighbor problem with probabilistic guarantees. By parameterless we mean that the user is only required to specify the amount of memory the index is supposed to use and the result quality that should be achieved. The index combines several heuristic ideas known in the literature. By small adaptions to the query algorithm, we make heuristics rigorous. We perform experiments on real-world and synthetic inputs to evaluate implementation choices and show that the implementation satisfies the quality guarantees while being competitive with other state-of-the-art approaches to nearest neighbor search. \nWe describe a novel synthetic data set that is difficult to solve for almost all existing nearest neighbor search approaches, and for which PUFFINN significantly outperform previous methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138604826",
                    "name": "Martin Aum\u00fcller"
                },
                {
                    "authorId": "3042305",
                    "name": "Tobias Christiani"
                },
                {
                    "authorId": "1801719",
                    "name": "R. Pagh"
                },
                {
                    "authorId": "150237048",
                    "name": "Michael Vesterli"
                }
            ]
        },
        {
            "paperId": "f1c792178f8b47ddb9aa3e9194de96a8c04d35b2",
            "title": "Fair Near Neighbor Search: Independent Range Sampling in High Dimensions",
            "abstract": "Similarity search is a fundamental algorithmic primitive, widely used in many computer science disciplines. There are several variants of the similarity search problem, and one of the most relevant is the r-near neighbor (r-NN) problem: given a radius r>0 and a set of points S, construct a data structure that, for any given query point q, returns a point p within distance at most r from q. In this paper, we study the r-NN problem in the light of fairness. We consider fairness in the sense of equal opportunity: all points that are within distance r from the query should have the same probability to be returned. In the low-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS 2014). Locality sensitive hashing (LSH), the theoretically strongest approach to similarity search in high dimensions, does not provide such a fairness guarantee. To address this, we propose efficient data structures for r-NN where all points in S that are near q have the same probability to be selected and returned by the query. Specifically, we first propose a black-box approach that, given any LSH scheme, constructs a data structure for uniformly sampling points in the neighborhood of a query. Then, we develop a data structure for fair similarity search under inner product that requires nearly-linear space and exploits locality sensitive filters. The paper concludes with an experimental evaluation that highlights (un)fairness in a recommendation setting on real-world datasets and discusses the inherent unfairness introduced by solving other variants of the problem.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2138604826",
                    "name": "Martin Aum\u00fcller"
                },
                {
                    "authorId": "1801719",
                    "name": "R. Pagh"
                },
                {
                    "authorId": "35007525",
                    "name": "Francesco Silvestri"
                }
            ]
        }
    ]
}