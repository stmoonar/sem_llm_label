{
    "authorId": "1741931",
    "papers": [
        {
            "paperId": "16815b2795509b66b30a9a43b52f4027b2de6982",
            "title": "Efficient Microscopic Image Instance Segmentation for Food Crystal Quality Control",
            "abstract": "This paper is directed towards the food crystal quality control area for manufacturing, focusing on efficiently predicting food crystal counts and size distributions. Previously, manufacturers used the manual counting method on microscopic images of food liquid products, which requires substantial human effort and suffers from inconsistency issues. Food crystal segmentation is a challenging problem due to the diverse shapes of crystals and their surrounding hard mimics. To address this challenge, we propose an efficient instance segmentation method based on object detection. Experimental results show that the predicted crystal counting accuracy of our method is comparable with existing segmentation methods, while being five times faster. Based on our experiments, we also define objective criteria for separating hard mimics and food crystals, which could benefit manual annotation tasks on similar dataset.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187003736",
                    "name": "Xiaoyu Ji"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                },
                {
                    "authorId": "145479245",
                    "name": "A. Shakouri"
                },
                {
                    "authorId": "2264470129",
                    "name": "Fengqing Zhu"
                }
            ]
        },
        {
            "paperId": "60a6754d3906bbc7af5e8e3bffea56bd05990638",
            "title": "GRIB: Combining Global Reception and Inductive Bias For Human Segmentation and Matting",
            "abstract": "Human video segmentation and matting are challenging computer vision tasks, with many applications such as background replacement or background editing. Numerous methods have been proposed for human segmentation and matting in either portrait or first-person view videos. In this paper, we propose a real-time network that performs first-person view hand and manipulated object segmentation as well as second-person view human video matting. We introduce a global reception inductive bias block in the network\u2019s encoder that aggregates the pixel features at short, medium, and long ranges. Furthermore, we propose a multi-target optimization method that fully leverages segmentation and matting labels to accelerate training. Our model outperforms existing real-time methods by achieving 93.9% mIoU on HP-Portrait, 95.1% mIoU on VideoMatte as well as 72.7% mIoU on EgoHOS datasets and achieves faster runtime.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1657323157",
                    "name": "Yezhi Shen"
                },
                {
                    "authorId": "48721645",
                    "name": "Weichen Xu"
                },
                {
                    "authorId": "2114096337",
                    "name": "Qian Lin"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                },
                {
                    "authorId": "2264470129",
                    "name": "Fengqing Zhu"
                }
            ]
        },
        {
            "paperId": "25a2100d60ba9b1907f1a4bf84b0483acae2dcab",
            "title": "Ink Drop Displacement Model-Based Direct Binary Search",
            "abstract": "A novel statistical ink drop displacement (IDD) printer model for the direct binary search (DBS) halftoning algorithm is proposed. It is intended primarily for pagewide inkjet printers that exhibit dot displacement errors. The tabular approach in the literature predicts the gray value of a printed pixel based on the halftone pattern in some neighborhood of that pixel. However, memory retrieval time and the complexity of memory requirements hamper its feasibility in printers that have a very large number of nozzles and produce ink drops that affect a large neighborhood. To avoid this problem, our IDD model embodies dot displacements by moving each perceived ink drop in the image from its nominal location to its actual location, rather than manipulating the average gray values. This enables DBS to directly compute the appearance of the final printout without retrieving values from a table. In so doing, the memory issue is eliminated and the computation efficiency is enhanced. The deterministic cost function of DBS is replaced by the expectation over the ensemble of the displacements for the proposed model such that the statistical behavior of the ink drops is accounted for. Experimental results show significant improvement in the quality of the printed image over the original DBS. Besides, the image quality obtained by the proposed approach appears to be slightly better than that obtained by the tabular approach.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2117696148",
                    "name": "Yafei Mao"
                },
                {
                    "authorId": "2142184230",
                    "name": "Utpal Sarkar"
                },
                {
                    "authorId": "2127260840",
                    "name": "Isabel Borrell"
                },
                {
                    "authorId": "1897497",
                    "name": "L. Abello"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                }
            ]
        },
        {
            "paperId": "96c760b2eafed484b62735fee5ccf459d15d7b66",
            "title": "Classifier Guided Domain Adaptation for VR Facial Expression Tracking",
            "abstract": "Immersive Virtual Reality (VR) experience from headsets requires high-quality virtual facial expression animation. Existing approaches for facial expression animation require ground truth labels for facial expression prediction. However, current headset devices can only capture partially occluded facial images from the wearer due to the placement of the embedded cameras. To reduce the cost and effort of human annotation to collect ground truth labels of facial action units, synthetic data are widely used. In this paper, we focus on the problem of adapting the model trained on synthetic data to real data. We introduce a novel classifier-guided multi-branch domain adaptation model. Based on this model, we propose a new two-branch backpropagation design. The proposed model quantitatively outperforms existing domain adaptation methods on the synthetic data. Apart from quantitative evaluations, we conducted a subjective study to evaluate the quality of visual expression tracking on real data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187003736",
                    "name": "Xiaoyu Ji"
                },
                {
                    "authorId": "2109721249",
                    "name": "Justin Yang"
                },
                {
                    "authorId": "39791510",
                    "name": "Jishang Wei"
                },
                {
                    "authorId": "2187081003",
                    "name": "Yvonne Huang"
                },
                {
                    "authorId": "2236065498",
                    "name": "Shibo Zhang"
                },
                {
                    "authorId": "2114096337",
                    "name": "Qian Lin"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                },
                {
                    "authorId": "2833390",
                    "name": "F. Zhu"
                }
            ]
        },
        {
            "paperId": "b8434e0cc25a56b9c1fdb0274751d2073635459f",
            "title": "Efficient Joint Video Denoising and Super-Resolution",
            "abstract": "Denoising and super-resolution are two important tasks for video enhancement. Despite recent progress for each task, there are very few works that target both tasks simultaneously. In this paper, we propose an efficient noise-robust video super-resolution method that is trained end-to-end for an input video containing observable noises. We investigate current approaches to address this joint denoising and super-resolution task and compare them to our proposed method. Experimental results show that our method achieves competitive reconstruction performance with existing solutions on various datasets while maintaining a low computation cost and a small model size which prove the effectiveness of our joint model design and training. Our code is available at \"https://github.com/Eventhyn/EVDSRNet.\".",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2235916421",
                    "name": "Yuning Huang"
                },
                {
                    "authorId": "2198886350",
                    "name": "Tianqi Wang"
                },
                {
                    "authorId": "2114096337",
                    "name": "Qian Lin"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                },
                {
                    "authorId": "49042243",
                    "name": "Fengqing M Zhu"
                }
            ]
        },
        {
            "paperId": "c017276ec222010839bd09d080415ed816c7da8b",
            "title": "A Visual Quality Assessment Method for Raster Images in Scanned Document",
            "abstract": "Image quality assessment (IQA) is an active research area in the field of image processing. Most prior works targeted the visual quality of natural images captured by cameras. In this paper, we shift the focus towards the visual quality of scanned documents, especially raster image areas. Different from many existing works that aim to estimate a visual quality score, we propose a machine learning based classification method to determine whether the visual quality of a scanned raster image at a given resolution setting is acceptable. We conduct a psychophysical study to determine the acceptability of different image resolutions based on human subject ratings and use them as the ground truth to train our machine learning model. However, this dataset is imbalanced as most images were rated as visually acceptable. To address the data imbalance problem, we introduce several noise models to simulate the degradation of image quality during the scanning process. Our results show that by including augmented data in training, we can significantly improve the performance of the classifier to determine whether the visual quality of raster images in a scanned document is acceptable or not for a given resolution setting.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2109721249",
                    "name": "Justin Yang"
                },
                {
                    "authorId": "47712094",
                    "name": "P\u00e9ter Bauer"
                },
                {
                    "authorId": "48036391",
                    "name": "Todd Harris"
                },
                {
                    "authorId": "2036045",
                    "name": "Changhyung Lee"
                },
                {
                    "authorId": "2208826886",
                    "name": "Hyeon Seok Seo"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                },
                {
                    "authorId": "2833390",
                    "name": "F. Zhu"
                }
            ]
        },
        {
            "paperId": "decf76f359e7d622d5585b0cd5301a90f95efeb7",
            "title": "Exploiting Temporal Information in Real-time Portrait Video Segmentation",
            "abstract": "Portrait video segmentation has been widely used in applications such as online conferencing and content creation. However, it is challenging for mobile devices with limited computation resources to achieve accurate and temporal consistent real-time portrait segmentation. In this work, we propose a segmentation method based on the classic encoder-decoder architecture with a lightweight model design. To facilitate the efficient use of temporal guidance, our method takes an RGB-M input where M is a guidance portrait mask concatenated to the RGB input. Furthermore, we leverage the temporal guidance to enable model inference on the adaptive portrait region of interest (ROI). We introduce a two-stage training strategy to compensate for the limited data variety of portrait video datasets. Our method is evaluated on portrait videos including different types of daily activities, and outperforms existing portrait segmentation methods in terms of segmentation accuracy. Without introducing significant delay, our method is suitable for applications requiring real-time processing.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "48721645",
                    "name": "Weichen Xu"
                },
                {
                    "authorId": "1657323157",
                    "name": "Yezhi Shen"
                },
                {
                    "authorId": "2114096337",
                    "name": "Qian Lin"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                },
                {
                    "authorId": "2264470129",
                    "name": "Fengqing Zhu"
                }
            ]
        },
        {
            "paperId": "e3d9a61884eec130af75f8ea184b90f61decfaa2",
            "title": "Real-Time End-to-End Portrait and In-Hand Object Segmentation with Background Fusion",
            "abstract": "Portrait segmentation has become increasingly important for video conferencing software. However, existing methods fail to produce consistent and accurate segmentation of in-hand objects due to the weak connection in pixel distribution between humans and the in-hand objects. In this paper, we propose a fast and lightweight end-to-end model that performs portrait and in-hand object segmentation simultaneously. Our design takes spatial and temporal information into account by using a model containing a Convolution-Transformer hybrid encoder and a recurrent decoder, followed by a background fusion refinement module to produce accurate and consistent segmentation results. Our experimental results show improvements both visually and quantitatively compared to existing real-time video portrait segmentation with respect to accuracy, consistency, and speed.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1657323157",
                    "name": "Yezhi Shen"
                },
                {
                    "authorId": "48721645",
                    "name": "Weichen Xu"
                },
                {
                    "authorId": "2114096337",
                    "name": "Qian Lin"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                },
                {
                    "authorId": "2833390",
                    "name": "F. Zhu"
                }
            ]
        },
        {
            "paperId": "f46a4ce3ce5d94e7784690a2c8cdc8c108bf77d1",
            "title": "VR Facial Expression Tracking Using Locally Linear Embedding",
            "abstract": "Immersive virtual reality (VR) experiences have enabled the seamlessly transition of users to a completely new scene, which creates an authentic sense of presence within the virtual environment. By using VR headsets or head-mounted displays (HMD), users can further interact with the virtual surroundings and engage into a variety of activities that can deliver realistic and lifelike feeling. In order to perform better VR experience, the high-quality virtual facial animation tracking is indispensable, which aims to detect the real facial expression of users wearing HMD and reenact the detected facial expression onto virtual avatars to simulate the same facial animation in virtual environment. However, it is labor intensive to obtain and annotate the real facial expression as it requires Facial Action Coding System (FACS) experts to annotate each facial AU intensity. In addition, the mounted cameras embedded in the HMD can only capture partial facial images due to the occlusion of the HMD. Therefore, synthetic facial data are widely used for training and the performance greatly depends on the quality of generated data. In this paper, we focus on estimating facial expression in terms of facial action unit (AU) intensity. To obtain higher quality of synthetic facial data, we developed a model with domain adversarial training to generate domain invariant features representations and then utilized local neighbors in high-dimension feature space to perform facial AU intensity estimation on real people data. We evaluate our method on self-collected synthetic avatar data with labels and real facial expression data without labels. The results show that our method can obtain better tracking performance both quantitatively and qualitatively.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109721249",
                    "name": "Justin Yang"
                },
                {
                    "authorId": "2187003736",
                    "name": "Xiaoyu Ji"
                },
                {
                    "authorId": "39791510",
                    "name": "Jishang Wei"
                },
                {
                    "authorId": "2187081003",
                    "name": "Yvonne Huang"
                },
                {
                    "authorId": "2236065498",
                    "name": "Shibo Zhang"
                },
                {
                    "authorId": "2114096337",
                    "name": "Qian Lin"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                },
                {
                    "authorId": "2833390",
                    "name": "F. Zhu"
                }
            ]
        },
        {
            "paperId": "f725c9a78c74c47db78c757503aca0d62679cfef",
            "title": "An Ensemble Method with Edge Awareness for Abnormally Shaped Nuclei Segmentation",
            "abstract": "Abnormalities in biological cell nuclei shapes are correlated with cell cycle stages, disease states, and various external stimuli. There have been many deep learning approaches that are being used for nuclei segmentation and analysis. In recent years, transformers have performed better than CNN methods on many computer vision tasks. One problem with many deep learning nuclei segmentation methods is acquiring large amounts of annotated nuclei data, which is generally expensive to obtain. In this paper, we propose a Transformer and CNN hybrid ensemble processing method with edge awareness for accurately segmenting abnormally shaped nuclei. We call this method Hybrid Edge Mask R-CNN (HER-CNN), which uses Mask R-CNNs with the ResNet and the Swin-Transformer to segment abnormally shaped nuclei. We add an edge awareness loss to the mask prediction step of the Mask R-CNN to better distinguish the edge difference between the abnormally shaped nuclei and typical oval nuclei. We describe an ensemble processing strategy to combine or fuse individual segmentations from the CNN and the Transformer. We introduce the use of synthetic ground truth image generation to supplement the annotated training images due to the limited amount of data. Our proposed method is compared with other segmentation methods for segmenting abnormally shaped nuclei. We also include ablation studies to show the effectiveness of the edge awareness loss and the use of synthetic ground truth images.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2112895827",
                    "name": "Yue Han"
                },
                {
                    "authorId": "2113651354",
                    "name": "Yang Lei"
                },
                {
                    "authorId": "95614728",
                    "name": "V. Shkolnikov"
                },
                {
                    "authorId": "1380316837",
                    "name": "Daisy Xin"
                },
                {
                    "authorId": "2202949619",
                    "name": "Alicia Auduong"
                },
                {
                    "authorId": "2095722548",
                    "name": "Steven Barcelo"
                },
                {
                    "authorId": "1741931",
                    "name": "J. Allebach"
                },
                {
                    "authorId": "1741483",
                    "name": "E. Delp"
                }
            ]
        }
    ]
}