{
    "authorId": "83331194",
    "papers": [
        {
            "paperId": "021c4955947fe6f7836dba8e5d63bb35d0917059",
            "title": "AbDiffuser: Full-Atom Generation of In-Vitro Functioning Antibodies",
            "abstract": "We introduce AbDiffuser, an equivariant and physics-informed diffusion model for the joint generation of antibody 3D structures and sequences. AbDiffuser is built on top of a new representation of protein structure, relies on a novel architecture for aligned proteins, and utilizes strong diffusion priors to improve the denoising process. Our approach improves protein diffusion by taking advantage of domain knowledge and physics-based constraints; handles sequence-length changes; and reduces memory complexity by an order of magnitude, enabling backbone and side chain generation. We validate AbDiffuser in silico and in vitro. Numerical experiments showcase the ability of AbDiffuser to generate antibodies that closely track the sequence and structural properties of a reference set. Laboratory experiments confirm that all 16 HER2 antibodies discovered were expressed at high levels and that 57.1% of the selected designs were tight binders.",
            "fieldsOfStudy": [
                "Biology",
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "1995092493",
                    "name": "Karolis Martinkus"
                },
                {
                    "authorId": "12304318",
                    "name": "J. Ludwiczak"
                },
                {
                    "authorId": "2111049203",
                    "name": "Kyunghyun Cho"
                },
                {
                    "authorId": "1409471958",
                    "name": "Weishao Lian"
                },
                {
                    "authorId": "1398403530",
                    "name": "J. Lafrance-Vanasse"
                },
                {
                    "authorId": "6585148",
                    "name": "I. Hotzel"
                },
                {
                    "authorId": "2214607",
                    "name": "A. Rajpal"
                },
                {
                    "authorId": "83331194",
                    "name": "Y. Wu"
                },
                {
                    "authorId": "2067157865",
                    "name": "Richard Bonneau"
                },
                {
                    "authorId": "3135439",
                    "name": "V. Gligorijevi\u0107"
                },
                {
                    "authorId": "1966031",
                    "name": "Andreas Loukas"
                }
            ]
        },
        {
            "paperId": "212afcc24025a1fc10533e63355a743aed9be59f",
            "title": "A Novel Lightweight Attention-Discarding Transformer for High-Resolution SAR Image Classification",
            "abstract": "Vision transformer (ViT) has been introduced in high-resolution synthetic aperture radar (HR SAR) image classification due to its excellent global feature extraction ability. However, small samples of SAR images make it difficult to fit the ViT with excessive trainable parameters, which easily results in over-fitting in training. Meanwhile, poor capability in capturing local features of ViT limits its accuracy in SAR image classification. To solve these problems, this letter proposes a new lightweight attention-discarding transformer (LAD Transformer) for the classification of HR SAR images. In the proposed model, the backbone of the advanced Swin transformer is used to model global information and extract hierarchical features. Moreover, the vital feature extraction part of the LAD transformer completely discards the self-attention mechanism and extracts local features of SAR images by introducing lighter group convolution and channel shuffle (GC-CS) block. In addition, to address the estimation shift caused by consecutive batch normalization (BN) layers, a new composite normalization method consisting of BN and layer normalization (BLN) in GC-CS block is proposed. The experiments show that the proposed network has fewer parameters and higher classification accuracy on two real HR SAR data.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2175968439",
                    "name": "Xingxian Liu"
                },
                {
                    "authorId": "83331194",
                    "name": "Y. Wu"
                },
                {
                    "authorId": "2110048334",
                    "name": "Xin Hu"
                },
                {
                    "authorId": "2109657047",
                    "name": "Zhikang Li"
                },
                {
                    "authorId": "50652019",
                    "name": "Ming Li"
                }
            ]
        },
        {
            "paperId": "421124db6a7901fc1933bf984c1e5ff5fdc27225",
            "title": "Embedding Based Retrieval in Friend Recommendation",
            "abstract": "Friend recommendation systems in online social and professional networks such as Snapchat helps users find friends and build connections, leading to better user engagement and retention. Traditional friend recommendation systems take advantage of the principle of locality and use graph traversal to retrieve friend candidates, e.g. Friends-of-Friends (FoF). While this approach has been adopted and shown efficacy in companies with large online networks such as Linkedin and Facebook, it suffers several challenges: (i) discrete graph traversal offers limited reach in cold-start settings, (ii) it is expensive and infeasible in realtime settings beyond 1 or 2 hop requests owing to latency constraints, and (iii) it cannot well-capture the complexity of graph topology or connection strengths, forcing one to resort to other mechanisms to rank and find top-K candidates. In this paper, we proposed a new Embedding Based Retrieval (EBR) system for retrieving friend candidates, which complements the traditional FoF retrieval by retrieving candidates beyond 2-hop, and providing a natural way to rank FoF candidates. Through online A/B test, we observe statistically significant improvements in the number of friendships made with EBR as an additional retrieval source in both low- and high-density network markets. Our contributions in this work include deploying a novel retrieval system to a large-scale friend recommendation system at Snapchat, generating embeddings for billions of users using Graph Neural Networks, and building EBR infrastructure in production to support Snapchat scale.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2223893005",
                    "name": "Jiahui Shi"
                },
                {
                    "authorId": "2226063688",
                    "name": "Vivek Chaurasiya"
                },
                {
                    "authorId": "152891495",
                    "name": "Yozen Liu"
                },
                {
                    "authorId": "2223762996",
                    "name": "Shubham Vij"
                },
                {
                    "authorId": "83331194",
                    "name": "Y. Wu"
                },
                {
                    "authorId": "2892275",
                    "name": "Satya Kanduri"
                },
                {
                    "authorId": "2153429147",
                    "name": "Neil Shah"
                },
                {
                    "authorId": "2223884564",
                    "name": "Peicheng Yu"
                },
                {
                    "authorId": "2223768936",
                    "name": "Nik Srivastava"
                },
                {
                    "authorId": "2117206743",
                    "name": "Lei Shi"
                },
                {
                    "authorId": "2058606101",
                    "name": "Ganesh Venkataraman"
                },
                {
                    "authorId": "28584977",
                    "name": "Junliang Yu"
                }
            ]
        },
        {
            "paperId": "54fda958a3853c544f4176d53c71236c0b86ad4e",
            "title": "Investigating Vision Foundational Models for Tactile Representation Learning",
            "abstract": "Tactile representation learning (TRL) equips robots with the ability to leverage touch information, boosting performance in tasks such as environment perception and object manipulation. However, the heterogeneity of tactile sensors results in many sensor- and task-specific learning approaches. This limits the efficacy of existing tactile datasets, and the subsequent generalisability of any learning outcome. In this work, we investigate the applicability of vision foundational models to sensor-agnostic TRL, via a simple yet effective transformation technique to feed the heterogeneous sensor readouts into the model. Our approach recasts TRL as a computer vision (CV) problem, which permits the application of various CV techniques for tackling TRL-specific challenges. We evaluate our approach on multiple benchmark tasks, using datasets collected from four different tactile sensors. Empirically, we demonstrate significant improvements in task performance, model robustness, as well as cross-sensor and cross-task knowledge transferability with limited data requirements.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2187929217",
                    "name": "Ben Zandonati"
                },
                {
                    "authorId": "3368222",
                    "name": "Ruohan Wang"
                },
                {
                    "authorId": "117423648",
                    "name": "Ruihan Gao"
                },
                {
                    "authorId": "83331194",
                    "name": "Y. Wu"
                }
            ]
        },
        {
            "paperId": "5b1f6d191b41df23c55600ac509277c96a985ee2",
            "title": "HDTFF-Net: Hierarchical Deep Texture Features Fusion Network for High-Resolution Remote Sensing Scene Classification",
            "abstract": "Fusing features from different feature descriptors or different convolutional layers can improve the understanding of scene and enhance the classification accuracy. In this article, we propose a hierarchical deep texture feature fusion network, abbreviated as HDTFF-Net, aiming to improve the classification accuracy of high-resolution remote sensing scene classification. The proposed HDTFF-Net can effectively combine the shallow texture information from manual features and the deep texture information by convolutional neural networks (CNNs). First, for deeply excavating the multiscale and multidirectional shallow texture features in images, an improved Wavelet feature extraction module and a Gabor feature extraction module are designed by fully fusing the structural features into the backbone neural network. Then, to make the output texture features more discriminative and interpretative, we incorporate the above texture feature extraction modules into traditional CNNs (Tra-CNNs), and design two improved deep networks, namely Wave-CNN and Gabor-CNN. Finally, according to the Dempster-Shafer evidence theory, the designed Wave-CNN and Gabor-CNN are fused with the Tra-CNN by a decision-level fusion strategy, which can effectively capture the deep texture features by different feature descriptors and improve the classification performance. Experiments on high-resolution remote sensing images demonstrate the effectiveness of the proposed HDTFF-Net, and verify that it can greatly improve the classification performance.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38311874",
                    "name": "Wanying Song"
                },
                {
                    "authorId": "2199946682",
                    "name": "Yifan Cong"
                },
                {
                    "authorId": "2145398716",
                    "name": "Shiru Zhang"
                },
                {
                    "authorId": "83331194",
                    "name": "Y. Wu"
                },
                {
                    "authorId": "32482521",
                    "name": "Peng Zhang"
                }
            ]
        },
        {
            "paperId": "65a9341924ce648c34f7d1dfe1db1eb7ac1fb714",
            "title": "Forest Volume Estimation Method Based on Allometric Growth Model and Multisource Remote Sensing Data",
            "abstract": "The accurate forest volume is crucial for forest management, but rapid, large-scale, and high-accuracy estimation is still challenging. We proposed a method of coupling allometric growth model and multisource data for forest volume estimation (CAMFVe). First, the diameter at breast height (DBH) estimation model is constructed by terrestrial laser scanning (TLS) and airborne laser scanning (ALS) to obtain more accurate measured volume. Second, the spectral attributes of Landsat and structural attributes of ALS are extracted and upscaled onto the 30-m plot scale, and the optimal attributes for volume estimation are selected. Third, the model of CAMFVe is constructed and applied to obtain the volume of study area. Finally, the applicability of CAMFVe is evaluated under four forest growth environments (different canopy closure and slope categories), and the accuracy is compared with multiple linear regression (MLR), random forest (RF), and support vector machine (SVM). The results show the following. First, the DBH estimation model by TLS and ALS improves the DBH calculation accuracy of ALS with a 2.058 cm reduction in RMSE. Second, the mean of canopy height (Hmean) and enhanced vegetation index (EVI) are identified as the optimal structural and spectral attributes, respectively. Third, the model constructed by Hmean and EVI consistently achieves higher accuracy for most forest growth environments, and the addition of spectral attribute improves volume estimation accuracy with a 10.152% reduction in RMSE compared with the Hmean-based model. Fourth, compared with MLR, RF, and SVM, CAMFVe offers higher accuracy, requires fewer parameters, and is simpler and more efficient. Our proposed method, based on allometric growth model and utilizing vegetation index instead of DBH, provides a solution for large-scale and high-accuracy volume estimation by combining spaceborne light detection and ranging and optical satellite images.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "83331194",
                    "name": "Y. Wu"
                },
                {
                    "authorId": "46458752",
                    "name": "Zhihui Mao"
                },
                {
                    "authorId": "2238899217",
                    "name": "Lijie Guo"
                },
                {
                    "authorId": "2238921874",
                    "name": "Chenrui Li"
                },
                {
                    "authorId": "2239056987",
                    "name": "Lei Deng"
                }
            ]
        },
        {
            "paperId": "a738c9a318e94311fbca5511216cfaf2508635b9",
            "title": "DBH Estimation for Individual Tree: Two-Dimensional Images or Three-Dimensional Point Clouds?",
            "abstract": "Accurate forest parameters are crucial for ecological protection, forest resource management and sustainable development. The rapid development of remote sensing can retrieve parameters such as the leaf area index, cluster index, diameter at breast height (DBH) and tree height at different scales (e.g., plots and stands). Although some LiDAR satellites such as GEDI and ICESAT-2 can measure the average tree height in a certain area, there is still a lack of effective means for obtaining individual tree parameters using high-resolution satellite data, especially DBH. The objective of this study is to explore the capability of 2D image-based features (texture and spectrum) in estimating the DBH of individual tree. Firstly, we acquired unmanned aerial vehicle (UAV) LiDAR point cloud data and UAV RGB imagery, from which digital aerial photography (DAP) point cloud data were generated using the structure-from-motion (SfM) method. Next, we performed individual tree segmentation and extracted the individual tree crown boundaries using the DAP and LiDAR point cloud data, respectively. Subsequently, the eight 2D image-based textural and spectral metrics and 3D point-cloud-based metrics (tree height and crown diameters) were extracted from the tree crown boundaries of each tree. Then, the correlation coefficients between each metric and the reference DBH were calculated. Finally, the capabilities of these metrics and different models, including multiple linear regression (MLR), random forest (RF) and support vector machine (SVM), in the DBH estimation were quantitatively evaluated and compared. The results showed that: (1) The 2D image-based textural metrics had the strongest correlation with the DBH. Among them, the highest correlation coefficient of \u22120.582 was observed between dissimilarity, variance and DBH. When using textural metrics alone, the estimated DBH accuracy was the highest, with a RMSE of only 0.032 and RMSE% of 16.879% using the MLR model; (2) Simply feeding multi-features, such as textural, spectral and structural metrics, into the machine learning models could not have led to optimal results in individual tree DBH estimations; on the contrary, it could even reduce the accuracy. In general, this study indicated that the 2D image-based textural metrics have great potential in individual tree DBH estimations, which could help improve the capability to efficiently and meticulously monitor and manage forests on a large scale.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "46458752",
                    "name": "Zhihui Mao"
                },
                {
                    "authorId": "2107137998",
                    "name": "Zhuo Lu"
                },
                {
                    "authorId": "83331194",
                    "name": "Y. Wu"
                },
                {
                    "authorId": "2088582008",
                    "name": "L. Deng"
                }
            ]
        },
        {
            "paperId": "a775d9eb2dac5d51d11e15fe2a632ecea89e5115",
            "title": "A Feature-Level Point Cloud Fusion Method for Timber Volume of Forest Stands Estimation",
            "abstract": "Accurate diameter at breast height (DBH) and tree height (H) information can be acquired through terrestrial laser scanning (TLS) and airborne LiDAR scanner (ALS) point cloud, respectively. To utilize these two features simultaneously but avoid the difficulties of point cloud fusion, such as technical complexity and time-consuming and laborious efforts, a feature-level point cloud fusion method (FFATTe) is proposed in this paper. Firstly, the TLS and ALS point cloud data in a plot are georeferenced by differential global navigation and positioning system (DGNSS) technology. Secondly, point cloud processing and feature extraction are performed for the georeferenced TLS and ALS to form feature datasets, respectively. Thirdly, the feature-level fusion of LiDAR data from different data sources is realized through spatial join according to the tree trunk location obtained from TLS and ALS, that is, the tally can be implemented at a plot. Finally, the individual tree parameters are optimized based on the tally results and fed into the binary volume model to estimate the total volume (TVS) in a large area (whole study area). The results show that the georeferenced ALS and TLS point cloud data using DGNSS RTK/PPK technology can achieve coarse registration (mean distance \u2248 40 cm), which meets the accuracy requirements for feature-level point cloud fusion. By feature-level fusion of the two point cloud data, the tally can be achieved quickly and accurately in the plot. The proposed FFATTe method achieves high accuracy (with error of 3.09%) due to its advantages of combining different LiDAR data from different sources in a simple way, and it has strong operability when acquiring TVS over large areas.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2512709",
                    "name": "Lijie Guo"
                },
                {
                    "authorId": "83331194",
                    "name": "Y. Wu"
                },
                {
                    "authorId": "2114191882",
                    "name": "Lei Deng"
                },
                {
                    "authorId": "2191033064",
                    "name": "Peng Hou"
                },
                {
                    "authorId": "2055272158",
                    "name": "Jun Zhai"
                },
                {
                    "authorId": "2144283806",
                    "name": "Yan Chen"
                }
            ]
        },
        {
            "paperId": "c487978755db616c3407ff3b02706db32d8be8cf",
            "title": "Approach for Monitoring Spatiotemporal Changes in Fractional Vegetation Cover Through Unmanned Aerial System-Guided-Satellite Survey: A Case Study in Mining Area",
            "abstract": "Fractional vegetation cover (FVC) is a vital indicator for monitoring regional vegetation and ecology. Although satellite remote sensing is used to monitor long-term changes in regional FVC, its applications are limited by the spatial resolution. Moreover, for unmanned aerial systems (UASs), obtaining long-term and large-scale images is difficult, and the efficiency of the synergy between UAS and satellite data for long-term FVC monitoring is limited. This article considered a mining area with extreme changes in vegetation as an example and proposed an efficient approach called multiple spatiotemporal-scale FVC prediction (MSFP) for long-term FVC monitoring in the region, which is based on the synergy of high spatial-resolution UAS data with high temporal-resolution Landsat data. First, we used the UAS imagery of several typical mining areas in Qianxi County of China collected in 2021, from which the vegetation information was extracted. Second, the 2-D Gaussian sampling was applied to aggregate, that is, to join/connect them into Landsat pixels. The vegetation index (VI) calculated from contemporary Landsat imagery was further used with the aggregated FVC of each satellite pixel. Finally, the VIs from the satellite imagery for different years were calibrated. The analysis demonstrated that: first, the proposed MSFP yielded improved the coefficient of determination (by 0.437) and decreased root-mean-square error (by 0.200) than the traditional dimidiate pixel method based on satellite imagery; second, the UAS imagery for few typical areas was used to predict the FVC of the large-scale area, thereby providing fine-scale vegetation information; third, the MSFP achieved high accuracy and long-term FVC monitoring by interyear calibration of VI calculated from Landsat data. This article paves the way toward accurate long-term monitoring of regional FVC. The demonstrated methodological framework is simple and operable, thereby opening the prospects for its applications in other environments.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2142611971",
                    "name": "Shuang Wu"
                },
                {
                    "authorId": "2088582008",
                    "name": "L. Deng"
                },
                {
                    "authorId": "2055272158",
                    "name": "Jun Zhai"
                },
                {
                    "authorId": "2107137998",
                    "name": "Zhuo Lu"
                },
                {
                    "authorId": "83331194",
                    "name": "Y. Wu"
                },
                {
                    "authorId": "2144283806",
                    "name": "Yan Chen"
                },
                {
                    "authorId": "2512709",
                    "name": "Lijie Guo"
                },
                {
                    "authorId": "47179178",
                    "name": "H. Gao"
                }
            ]
        },
        {
            "paperId": "d42f538db8e391f5fb740fd702881522a4ac59d9",
            "title": "GLF-Net: A Semantic Segmentation Model Fusing Global and Local Features for High-Resolution Remote Sensing Images",
            "abstract": "Semantic segmentation of high-resolution remote sensing images holds paramount importance in the field of remote sensing. To better excavate and fully fuse the features in high-resolution remote sensing images, this paper introduces a novel Global and Local Feature Fusion Network, abbreviated as GLF-Net, by incorporating the extensive contextual information and refined fine-grained features. The proposed GLF-Net, devised as an encoder\u2013decoder network, employs the powerful ResNet50 as its baseline model. It incorporates two pivotal components within the encoder phase: a Covariance Attention Module (CAM) and a Local Fine-Grained Extraction Module (LFM). And an additional wavelet self-attention module (WST) is integrated into the decoder stage. The CAM effectively extracts the features of different scales from various stages of the ResNet and then encodes them with graph convolutions. In this way, the proposed GLF-Net model can well capture the global contextual information with both universality and consistency. Additionally, the local feature extraction module refines the feature map by encoding the semantic and spatial information, thereby capturing the local fine-grained features in images. Furthermore, the WST maximizes the synergy between the high-frequency and the low-frequency information, facilitating the fusion of global and local features for better performance in semantic segmentation. The effectiveness of the proposed GLF-Net model is validated through experiments conducted on the ISPRS Potsdam and Vaihingen datasets. The results verify that it can greatly improve segmentation accuracy.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38311874",
                    "name": "Wanying Song"
                },
                {
                    "authorId": "2244608334",
                    "name": "Xinwei Zhou"
                },
                {
                    "authorId": "2145398716",
                    "name": "Shiru Zhang"
                },
                {
                    "authorId": "83331194",
                    "name": "Y. Wu"
                },
                {
                    "authorId": "32482521",
                    "name": "Peng Zhang"
                }
            ]
        }
    ]
}