{
    "authorId": "145199659",
    "papers": [
        {
            "paperId": "83e3b3b19bbe92a85265bcee6634738206662f03",
            "title": "Universal Proposition Bank 2.0",
            "abstract": "Semantic role labeling (SRL) represents the meaning of a sentence in the form of predicate-argument structures. Such shallow semantic analysis is helpful in a wide range of downstream NLP tasks and real-world applications. As treebanks enabled the development of powerful syntactic parsers, the accurate predicate-argument analysis demands training data in the form of propbanks. Unfortunately, most languages simply do not have corresponding propbanks due to the high cost required to construct such resources. To overcome such challenges, Universal Proposition Bank 1.0 (UP1.0) was released in 2017, with high-quality propbank data generated via a two-stage method exploiting monolingual SRL and multilingual parallel data. In this paper, we introduce Universal Proposition Bank 2.0 (UP2.0), with significant enhancements over UP1.0: (1) propbanks with higher quality by using a state-of-the-art monolingual SRL and improved auto-generation of annotations; (2) expanded language coverage (from 7 to 9 languages); (3) span annotation for the decoupling of syntactic analysis; and (4) Gold data for a subset of the languages. We also share our experimental results that confirm the significant quality improvements of the generated propbanks. In addition, we present a comprehensive experimental evaluation on how different implementation choices impact the quality of the resulting data. We release these resources to the research community and hope to encourage more research on cross-lingual SRL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144377686",
                    "name": "Ishan Jindal"
                },
                {
                    "authorId": "13836343",
                    "name": "Alexandre Rademaker"
                },
                {
                    "authorId": "2185433526",
                    "name": "Micha\u0142 Ulewicz"
                },
                {
                    "authorId": "2180951336",
                    "name": "Linh H. Ha"
                },
                {
                    "authorId": "145199659",
                    "name": "Huyen Nguyen"
                },
                {
                    "authorId": "1399212475",
                    "name": "Khoi-Nguyen Tran"
                },
                {
                    "authorId": "2115718238",
                    "name": "Huaiyu Zhu"
                },
                {
                    "authorId": "1718694",
                    "name": "Yunyao Li"
                }
            ]
        },
        {
            "paperId": "efb0bf81663c2e60136e01f30bbb751a0ba0b217",
            "title": "SimpleBooks: Long-term dependency book dataset with simplified English vocabulary for word-level language modeling",
            "abstract": "With language modeling becoming the popular base task for unsupervised representation learning in Natural Language Processing, it is important to come up with new architectures and techniques for faster and better training of language models. However, due to a peculiarity of languages -- the larger the dataset, the higher the average number of times a word appears in that dataset -- datasets of different sizes have very different properties. Architectures performing well on small datasets might not perform well on larger ones. For example, LSTM models perform well on WikiText-2 but poorly on WikiText-103, while Transformer models perform well on WikiText-103 but not on WikiText-2. For setups like architectural search, this is a challenge since it is prohibitively costly to run a search on the full dataset but it is not indicative to experiment on smaller ones. In this paper, we introduce SimpleBooks, a small dataset with the average word frequency as high as that of much larger ones. Created from 1,573 Gutenberg books with the highest ratio of word-level book length to vocabulary size, SimpleBooks contains 92M word-level tokens, on par with WikiText-103 (103M tokens), but has the vocabulary of 98K, a third of WikiText-103's. SimpleBooks can be downloaded from this https URL.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145199659",
                    "name": "Huyen Nguyen"
                }
            ]
        }
    ]
}