{
    "authorId": "2180198448",
    "papers": [
        {
            "paperId": "66dfa576c8ddb084479791f6c4e23b08530dfcaf",
            "title": "Boosted Off-Policy Learning",
            "abstract": "We propose the first boosting algorithm for off-policy learning from logged bandit feedback. Unlike existing boosting methods for supervised learning, our algorithm directly optimizes an estimate of the policy's expected reward. We analyze this algorithm and prove that the excess empirical risk decreases (possibly exponentially fast) with each round of boosting, provided a ''weak'' learning condition is satisfied by the base learner. We further show how to reduce the base learner to supervised learning, which opens up a broad range of readily available base learners with practical benefits, such as decision trees. Experiments indicate that our algorithm inherits many desirable properties of tree-based boosting algorithms (e.g., robustness to feature scaling and hyperparameter tuning), and that it can outperform off-policy learning with deep neural networks as well as methods that simply regress on the observed rewards.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "2085850",
                    "name": "Ben London"
                },
                {
                    "authorId": "2180198448",
                    "name": "Levi Lu"
                },
                {
                    "authorId": "2138911",
                    "name": "Ted Sandler"
                },
                {
                    "authorId": "1680188",
                    "name": "T. Joachims"
                }
            ]
        }
    ]
}