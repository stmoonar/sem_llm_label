{
    "authorId": "2268675415",
    "papers": [
        {
            "paperId": "06b5a815283b6124249c87fc1b99801d1b68ad08",
            "title": "GLEMOS: Benchmark for Instantaneous Graph Learning Model Selection",
            "abstract": "The choice of a graph learning (GL) model (i.e., a GL algorithm and its hyperparameter settings) has a significant impact on the performance of downstream tasks. However, selecting the right GL model becomes increasingly difficult and time consuming as more and more GL models are developed. Accordingly, it is of great significance and practical value to equip users of GL with the ability to perform a near-instantaneous selection of an effective GL model without manual intervention. Despite the recent attempts to tackle this important problem, there has been no comprehensive benchmark environment to evaluate the performance of GL model selection methods. To bridge this gap, we present GLEMOS in this work, a comprehensive benchmark for instantaneous GL model selection that makes the following contributions. (i) GLEMOS provides extensive benchmark data for fundamental GL tasks, i.e., link prediction and node classification, including the performances of 366 models on 457 graphs on these tasks. (ii) GLEMOS designs multiple evaluation settings, and assesses how effectively representative model selection techniques perform in these different settings. (iii) GLEMOS is designed to be easily extended with new models, new graphs, and new performance records. (iv) Based on the experimental results, we discuss the limitations of existing approaches and highlight future research directions. To promote research on this significant problem, we make the benchmark data and code publicly available at https://github.com/facebookresearch/glemos.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268675415",
                    "name": "Namyong Park"
                },
                {
                    "authorId": "2288277670",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "2289802183",
                    "name": "Xing Wang"
                },
                {
                    "authorId": "2288274423",
                    "name": "Antoine Simoulin"
                },
                {
                    "authorId": "2280742578",
                    "name": "Nesreen K. Ahmed"
                },
                {
                    "authorId": "2263543517",
                    "name": "Christos Faloutsos"
                }
            ]
        },
        {
            "paperId": "27cc05d2ad2f48123db8fb6b38690862b34ac75c",
            "title": "Large Generative Graph Models",
            "abstract": "Large Generative Models (LGMs) such as GPT, Stable Diffusion, Sora, and Suno are trained on a huge amount of language corpus, images, videos, and audio that are extremely diverse from numerous domains. This training paradigm over diverse well-curated data lies at the heart of generating creative and sensible content. However, all previous graph generative models (e.g., GraphRNN, MDVAE, MoFlow, GDSS, and DiGress) have been trained only on one dataset each time, which cannot replicate the revolutionary success achieved by LGMs in other fields. To remedy this crucial gap, we propose a new class of graph generative model called Large Graph Generative Model (LGGM) that is trained on a large corpus of graphs (over 5000 graphs) from 13 different domains. We empirically demonstrate that the pre-trained LGGM has superior zero-shot generative capability to existing graph generative models. Furthermore, our pre-trained LGGM can be easily fine-tuned with graphs from target domains and demonstrate even better performance than those directly trained from scratch, behaving as a solid starting point for real-world customization. Inspired by Stable Diffusion, we further equip LGGM with the capability to generate graphs given text prompts (Text-to-Graph), such as the description of the network name and domain (i.e.,\"The power-1138-bus graph represents a network of buses in a power distribution system.\"), and network statistics (i.e.,\"The graph has a low average degree, suitable for modeling social media interactions.\"). This Text-to-Graph capability integrates the extensive world knowledge in the underlying language model, offering users fine-grained control of the generated graphs. We release the code, the model checkpoint, and the datasets at https://lggm-lg.github.io/.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2284900711",
                    "name": "Yu Wang"
                },
                {
                    "authorId": "2238208116",
                    "name": "Ryan Rossi"
                },
                {
                    "authorId": "2268675415",
                    "name": "Namyong Park"
                },
                {
                    "authorId": "2305588553",
                    "name": "Huiyuan Chen"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                },
                {
                    "authorId": "30440868",
                    "name": "Puja Trivedi"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                },
                {
                    "authorId": "2290558635",
                    "name": "Tyler Derr"
                }
            ]
        },
        {
            "paperId": "cb63d7d3e1fdd53b26090c6949a8c373d1798f01",
            "title": "Forward Learning of Graph Neural Networks",
            "abstract": "Graph neural networks (GNNs) have achieved remarkable success across a wide range of applications, such as recommendation, drug discovery, and question answering. Behind the success of GNNs lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks (NNs). However, despite its effectiveness, BP imposes several constraints, which are not only biologically implausible, but also limit the scalability, parallelism, and flexibility in learning NNs. Examples of such constraints include storage of neural activities computed in the forward pass for use in the subsequent backward pass, and the dependence of parameter updates on non-local signals. To address these limitations, the forward-forward algorithm (FF) was recently proposed as an alternative to BP in the image classification domain, which trains NNs by performing two forward passes over positive and negative data. Inspired by this advance, we propose ForwardGNN in this work, a new forward learning procedure for GNNs, which avoids the constraints imposed by BP via an effective layer-wise local forward training. ForwardGNN extends the original FF to deal with graph data and GNNs, and makes it possible to operate without generating negative inputs (hence no longer forward-forward). Further, ForwardGNN enables each layer to learn from both the bottom-up and top-down signals without relying on the backpropagation of errors. Extensive experiments on real-world datasets show the effectiveness and generality of the proposed forward graph learning framework. We release our code at https://github.com/facebookresearch/forwardgnn.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2268675415",
                    "name": "Namyong Park"
                },
                {
                    "authorId": "2289802183",
                    "name": "Xing Wang"
                },
                {
                    "authorId": "2288274423",
                    "name": "Antoine Simoulin"
                },
                {
                    "authorId": "2292339664",
                    "name": "Shuai Yang"
                },
                {
                    "authorId": "2292059387",
                    "name": "Grey Yang"
                },
                {
                    "authorId": "2288277670",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "30440868",
                    "name": "Puja Trivedi"
                },
                {
                    "authorId": "2280742578",
                    "name": "Nesreen K. Ahmed"
                }
            ]
        },
        {
            "paperId": "84557580980d28af40581143c62a856988d64eef",
            "title": "Leveraging Graph Diffusion Models for Network Refinement Tasks",
            "abstract": "Most real-world networks are noisy and incomplete samples from an unknown target distribution. Refining them by correcting corruptions or inferring unobserved regions typically improves downstream performance. Inspired by the impressive generative capabilities that have been used to correct corruptions in images, and the similarities between\"in-painting\"and filling in missing nodes and edges conditioned on the observed graph, we propose a novel graph generative framework, SGDM, which is based on subgraph diffusion. Our framework not only improves the scalability and fidelity of graph diffusion models, but also leverages the reverse process to perform novel, conditional generation tasks. In particular, through extensive empirical analysis and a set of novel metrics, we demonstrate that our proposed model effectively supports the following refinement tasks for partially observable networks: T1: denoising extraneous subgraphs, T2: expanding existing subgraphs and T3: performing\"style\"transfer by regenerating a particular subgraph to match the characteristics of a different node or subgraph.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "30440868",
                    "name": "Puja Trivedi"
                },
                {
                    "authorId": "2066337266",
                    "name": "Ryan A. Rossi"
                },
                {
                    "authorId": "2268674629",
                    "name": "David Arbour"
                },
                {
                    "authorId": "1500399016",
                    "name": "Tong Yu"
                },
                {
                    "authorId": "2462276",
                    "name": "Franck Dernoncourt"
                },
                {
                    "authorId": "2261424174",
                    "name": "Sungchul Kim"
                },
                {
                    "authorId": "1793409",
                    "name": "Nedim Lipka"
                },
                {
                    "authorId": "2268675415",
                    "name": "Namyong Park"
                },
                {
                    "authorId": "144741751",
                    "name": "Nesreen K. Ahmed"
                },
                {
                    "authorId": "2479152",
                    "name": "Danai Koutra"
                }
            ]
        }
    ]
}