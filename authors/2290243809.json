{
    "authorId": "2290243809",
    "papers": [
        {
            "paperId": "69da6cdd316388e5908c4aadf6d1d630d8f63546",
            "title": "Are Large Language Models a Good Replacement of Taxonomies?",
            "abstract": "\n Large language models (LLMs) demonstrate an impressive ability to internalize knowledge and answer natural language questions. Although previous studies validate that LLMs perform well on general knowledge while presenting poor performance on long-tail nuanced knowledge, the community is still doubtful about whether the traditional knowledge graphs should be replaced by LLMs. In this paper, we ask\n if the schema of knowledge graph (i.e., taxonomy) is made obsolete by LLMs.\n Intuitively, LLMs should perform well on common taxonomies and at taxonomy levels that are common to people. Unfortunately, there lacks a comprehensive benchmark that evaluates the LLMs over a wide range of taxonomies from common to specialized domains and at levels from root to leaf so that we can draw a confident conclusion. To narrow the research gap, we constructed a novel taxonomy hierarchical structure discovery benchmark named TaxoGlimpse to evaluate the performance of LLMs over taxonomies. TaxoGlimpse covers ten representative taxonomies from common to specialized domains with in-depth experiments of different levels of entities in this taxonomy from root to leaf. Our comprehensive experiments of eighteen LLMs under three prompting settings validate that LLMs perform miserably poorly in handling specialized taxonomies and leaf-level entities. Specifically, the QA accuracy of the best LLM drops by up to 30% as we go from common to specialized domains and from root to leaf levels of taxonomies.\n",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2214733444",
                    "name": "Yushi Sun"
                },
                {
                    "authorId": "2305482447",
                    "name": "Hao Xin"
                },
                {
                    "authorId": "2305756456",
                    "name": "Kai Sun"
                },
                {
                    "authorId": "2290243809",
                    "name": "Y. Xu"
                },
                {
                    "authorId": "2305569900",
                    "name": "Xiao Yang"
                },
                {
                    "authorId": "2283964663",
                    "name": "Xin Luna Dong"
                },
                {
                    "authorId": "2305482335",
                    "name": "Nan Tang"
                },
                {
                    "authorId": "2304515425",
                    "name": "Lei Chen"
                }
            ]
        },
        {
            "paperId": "6d9690ab7674d70a3d8e41870186acba7325485b",
            "title": "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM",
            "abstract": "Vision-extended LLMs have made significant strides in Visual Question Answering (VQA). Despite these advancements, VLLMs still encounter substantial difficulties in handling queries involving long-tail entities, with a tendency to produce erroneous or hallucinated responses. In this work, we introduce a novel evaluative benchmark named \\textbf{SnapNTell}, specifically tailored for entity-centric VQA. This task aims to test the models' capabilities in identifying entities and providing detailed, entity-specific knowledge. We have developed the \\textbf{SnapNTell Dataset}, distinct from traditional VQA datasets: (1) It encompasses a wide range of categorized entities, each represented by images and explicitly named in the answers; (2) It features QA pairs that require extensive knowledge for accurate responses. The dataset is organized into 22 major categories, containing 7,568 unique entities in total. For each entity, we curated 10 illustrative images and crafted 10 knowledge-intensive QA pairs. To address this novel task, we devised a scalable, efficient, and transparent retrieval-augmented multimodal LLM. Our approach markedly outperforms existing methods on the SnapNTell dataset, achieving a 66.5\\% improvement in the BELURT score. We will soon make the dataset and the source code publicly accessible.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2267397829",
                    "name": "Jielin Qiu"
                },
                {
                    "authorId": "2111680936",
                    "name": "Andrea Madotto"
                },
                {
                    "authorId": "2146396528",
                    "name": "Zhaojiang Lin"
                },
                {
                    "authorId": "34963487",
                    "name": "Paul A. Crook"
                },
                {
                    "authorId": "2290243809",
                    "name": "Y. Xu"
                },
                {
                    "authorId": "2215596266",
                    "name": "Xin Luna Dong"
                },
                {
                    "authorId": "2290179598",
                    "name": "Christos Faloutsos"
                },
                {
                    "authorId": "2290642943",
                    "name": "Lei Li"
                },
                {
                    "authorId": "3057557",
                    "name": "Babak Damavandi"
                },
                {
                    "authorId": "2256132624",
                    "name": "Seungwhan Moon"
                }
            ]
        },
        {
            "paperId": "cd37ddd21b590d2cbe9301680dd244b601510f71",
            "title": "NTIRE 2024 Challenge on Short-form UGC Video Quality Assessment: Methods and Results",
            "abstract": "This paper reviews the NTIRE 2024 Challenge on Short-form UGC Video Quality Assessment (S-UGC VQA), where various excellent solutions are submitted and evaluated on the collected dataset KVQ from popular short-form video platform, i.e., Kuaishou/Kwai Platform. The KVQ database is divided into three parts, including 2926 videos for training, 420 videos for validation, and 854 videos for testing. The purpose is to build new benchmarks and advance the development of S-UGC VQA. The competition had 200 participants and 13 teams submitted valid solutions for the final testing phase. The proposed solutions achieved state-of-the-art performances for S-UGC VQA. The project can be found at https://github.com/lixinustc/KVQ-Challenge-CVPR-NTIRE2024.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "2164776612",
                    "name": "Xin Li"
                },
                {
                    "authorId": "2283840498",
                    "name": "Kun Yuan"
                },
                {
                    "authorId": "2283840733",
                    "name": "Yajing Pei"
                },
                {
                    "authorId": "2249693284",
                    "name": "Yiting Lu"
                },
                {
                    "authorId": "2238906044",
                    "name": "Ming Sun"
                },
                {
                    "authorId": "2283882455",
                    "name": "Chao Zhou"
                },
                {
                    "authorId": "2285224248",
                    "name": "Zhibo Chen"
                },
                {
                    "authorId": "1732855",
                    "name": "R. Timofte"
                },
                {
                    "authorId": "2266378162",
                    "name": "Wei Sun"
                },
                {
                    "authorId": "120155330",
                    "name": "Haoning Wu"
                },
                {
                    "authorId": "2116459218",
                    "name": "Zicheng Zhang"
                },
                {
                    "authorId": "2297749404",
                    "name": "Jun Jia"
                },
                {
                    "authorId": "2297144496",
                    "name": "Zhichao Zhang"
                },
                {
                    "authorId": "2249721406",
                    "name": "Lin-Li Cao"
                },
                {
                    "authorId": "2297153390",
                    "name": "Qiubo Chen"
                },
                {
                    "authorId": "2246414",
                    "name": "Xiongkuo Min"
                },
                {
                    "authorId": "2266768297",
                    "name": "Weisi Lin"
                },
                {
                    "authorId": "2266393212",
                    "name": "Guangtao Zhai"
                },
                {
                    "authorId": "2297014697",
                    "name": "Jianhui Sun"
                },
                {
                    "authorId": "2297008901",
                    "name": "Tianyi Wang"
                },
                {
                    "authorId": "2261071905",
                    "name": "Lei Li"
                },
                {
                    "authorId": "2296995521",
                    "name": "Han Kong"
                },
                {
                    "authorId": "2108595857",
                    "name": "Wenxuan Wang"
                },
                {
                    "authorId": "2295363497",
                    "name": "Bing Li"
                },
                {
                    "authorId": "2153561173",
                    "name": "Cheng Luo"
                },
                {
                    "authorId": "2157846393",
                    "name": "Haiqiang Wang"
                },
                {
                    "authorId": "2295898772",
                    "name": "Xiang-Zhong Chen"
                },
                {
                    "authorId": "2281892218",
                    "name": "Wenhui Meng"
                },
                {
                    "authorId": "2268292588",
                    "name": "Xiang Pan"
                },
                {
                    "authorId": "2282140640",
                    "name": "Huiying Shi"
                },
                {
                    "authorId": "2115313367",
                    "name": "Han Zhu"
                },
                {
                    "authorId": "1718111",
                    "name": "Xiaozhong Xu"
                },
                {
                    "authorId": "2276488753",
                    "name": "Lei Sun"
                },
                {
                    "authorId": "2257641974",
                    "name": "Zhenzhong Chen"
                },
                {
                    "authorId": "2267312030",
                    "name": "Sha Liu"
                },
                {
                    "authorId": "2112395233",
                    "name": "Fan-zhi Kong"
                },
                {
                    "authorId": "2296272069",
                    "name": "Hao Fan"
                },
                {
                    "authorId": "2290243809",
                    "name": "Y. Xu"
                },
                {
                    "authorId": "2297141821",
                    "name": "Haoran Xu"
                },
                {
                    "authorId": "2265618355",
                    "name": "Meng-Zhao Yang"
                },
                {
                    "authorId": "2280186807",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2269433934",
                    "name": "Jiaze Li"
                },
                {
                    "authorId": "2274370341",
                    "name": "Shijie Wen"
                },
                {
                    "authorId": "2297135177",
                    "name": "Mai Xu"
                },
                {
                    "authorId": "2229001412",
                    "name": "Da Li"
                },
                {
                    "authorId": "2279334643",
                    "name": "Shunyu Yao"
                },
                {
                    "authorId": "40868422",
                    "name": "Jiazhi Du"
                },
                {
                    "authorId": "2279210262",
                    "name": "Wangmeng Zuo"
                },
                {
                    "authorId": "2292212516",
                    "name": "Zhibo Li"
                },
                {
                    "authorId": "2115303319",
                    "name": "Shuaiqi He"
                },
                {
                    "authorId": "2296993063",
                    "name": "Anlong Ming"
                },
                {
                    "authorId": "2265162193",
                    "name": "Hui Fu"
                },
                {
                    "authorId": "2296939599",
                    "name": "Hua-Min Ma"
                },
                {
                    "authorId": "2284506528",
                    "name": "Yong Wu"
                },
                {
                    "authorId": "2296995534",
                    "name": "Fie Xue"
                },
                {
                    "authorId": "2284279845",
                    "name": "Guozhi Zhao"
                },
                {
                    "authorId": "2269874815",
                    "name": "Li-Fen Du"
                },
                {
                    "authorId": "2214539996",
                    "name": "Jie Guo"
                },
                {
                    "authorId": "2297201323",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "2253031211",
                    "name": "Hu Zheng"
                },
                {
                    "authorId": "2270520779",
                    "name": "Junhao Chen"
                },
                {
                    "authorId": "2294470297",
                    "name": "Yue Liu"
                },
                {
                    "authorId": "2298212671",
                    "name": "Dulan Zhou"
                },
                {
                    "authorId": "2117100857",
                    "name": "Kele Xu"
                },
                {
                    "authorId": "2296729429",
                    "name": "Qisheng Xu"
                },
                {
                    "authorId": "2294307310",
                    "name": "Tao Sun"
                },
                {
                    "authorId": "2274756067",
                    "name": "Zhi-Guo Ding"
                },
                {
                    "authorId": "2135923596",
                    "name": "Yuhan Hu"
                }
            ]
        },
        {
            "paperId": "ec1bec009e68a4df478aaf11e3615e5587768990",
            "title": "CRAG - Comprehensive RAG Benchmark",
            "abstract": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)'s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2305569900",
                    "name": "Xiao Yang"
                },
                {
                    "authorId": "2305756456",
                    "name": "Kai Sun"
                },
                {
                    "authorId": "2305482447",
                    "name": "Hao Xin"
                },
                {
                    "authorId": "2214733444",
                    "name": "Yushi Sun"
                },
                {
                    "authorId": "2305481658",
                    "name": "Nikita Bhalla"
                },
                {
                    "authorId": "2306139906",
                    "name": "Xiangsen Chen"
                },
                {
                    "authorId": "2305482313",
                    "name": "Sajal Choudhary"
                },
                {
                    "authorId": "2305481573",
                    "name": "Rongze Daniel Gui"
                },
                {
                    "authorId": "2305570216",
                    "name": "Ziran Will Jiang"
                },
                {
                    "authorId": "2269119211",
                    "name": "Ziyu Jiang"
                },
                {
                    "authorId": "2305625674",
                    "name": "Lingkun Kong"
                },
                {
                    "authorId": "2305481275",
                    "name": "Brian Moran"
                },
                {
                    "authorId": "2305531076",
                    "name": "Jiaqi Wang"
                },
                {
                    "authorId": "2290243809",
                    "name": "Y. Xu"
                },
                {
                    "authorId": "2305481525",
                    "name": "An Yan"
                },
                {
                    "authorId": "2218987196",
                    "name": "Chenyu Yang"
                },
                {
                    "authorId": "2305481568",
                    "name": "Eting Yuan"
                },
                {
                    "authorId": "47291370",
                    "name": "Hanwen Zha"
                },
                {
                    "authorId": "2305482335",
                    "name": "Nan Tang"
                },
                {
                    "authorId": "2282352887",
                    "name": "Lei Chen"
                },
                {
                    "authorId": "2305481622",
                    "name": "Nicolas Scheffer"
                },
                {
                    "authorId": "2282384339",
                    "name": "Yue Liu"
                },
                {
                    "authorId": "2305649695",
                    "name": "Nirav Shah"
                },
                {
                    "authorId": "2305481558",
                    "name": "Rakesh Wanga"
                },
                {
                    "authorId": "2247977368",
                    "name": "Anuj Kumar"
                },
                {
                    "authorId": "2072801764",
                    "name": "Wen-tau Yih"
                },
                {
                    "authorId": "2283964663",
                    "name": "Xin Luna Dong"
                }
            ]
        }
    ]
}