{
    "authorId": "145668048",
    "papers": [
        {
            "paperId": "0b5970fd876b52b676f878079b52e7289dc56025",
            "title": "DeepFixCX: Explainable privacy\u2010preserving image compression for medical image analysis",
            "abstract": "Explanations of a model's biases or predictions are essential to medical image analysis. Yet, explainable machine learning approaches for medical image analysis are challenged by needs to preserve privacy of patient data, and by current trends in deep learning to use unsustainably large models and large datasets. We propose DeepFixCX for explainable and privacy\u2010preserving medical image compression that is nimble and performant. We contribute a review of the field and a conceptual framework for simultaneous privacy and explainability via tools of compression. DeepFixCX compresses images without learning by removing or obscuring spatial and edge information. DeepFixCX is ante\u2010hoc explainable and gives privatized post hoc explanations of spatial and edge bias without accessing the original image. DeepFixCX privatizes images to prevent image reconstruction and mitigate patient re\u2010identification. DeepFixCX is nimble. Compression can occur on a laptop CPU or GPU to compress and privatize 1700 images per second of size 320\u2009\u00d7\u2009320. DeepFixCX enables use of low memory MLP classifiers for vision data; permitting small performance loss gives end\u2010to\u2010end MLP performance over 70\u00d7 faster and batch size over 100\u00d7 larger. DeepFixCX consistently improves predictive classification performance of a Deep Neural Network (DNN) by 0.02 AUC ROC on Glaucoma and Cervix Type detection datasets, and can improve multi\u2010label chest x\u2010ray classification performance in seven of 10 tested settings. In all three datasets, compression to less than 5% of original number of pixels gives matching or improved performance. Our main novelty is to define an explainability versus privacy problem and address it with lossy compression.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152888952",
                    "name": "Alex Gaudio"
                },
                {
                    "authorId": "1772588",
                    "name": "A. Smailagic"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "2186979527",
                    "name": "Shreshta Mohan"
                },
                {
                    "authorId": "1998967224",
                    "name": "Elvin Johnson"
                },
                {
                    "authorId": "2210047886",
                    "name": "Yuhao Liu"
                },
                {
                    "authorId": "145668048",
                    "name": "P. Costa"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                }
            ]
        },
        {
            "paperId": "275ffc5cd59930d7a594a12513ae1077dfcb0588",
            "title": "CoNIC Challenge: Pushing the Frontiers of Nuclear Detection, Segmentation, Classification and Counting",
            "abstract": "Nuclear detection, segmentation and morphometric profiling are essential in helping us further understand the relationship between histology and patient outcome. To drive innovation in this area, we setup a community-wide challenge using the largest available dataset of its kind to assess nuclear segmentation and cellular composition. Our challenge, named CoNIC, stimulated the development of reproducible algorithms for cellular recognition with real-time result inspection on public leaderboards. We conducted an extensive post-challenge analysis based on the top-performing models using 1,658 whole-slide images of colon tissue. With around 700 million detected nuclei per model, associated features were used for dysplasia grading and survival analysis, where we demonstrated that the challenge's improvement over the previous state-of-the-art led to significant boosts in downstream performance. Our findings also suggest that eosinophils and neutrophils play an important role in the tumour microevironment. We release challenge models and WSI-level results to foster the development of further methods for biomarker discovery.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "1393638783",
                    "name": "S. Graham"
                },
                {
                    "authorId": "50826887",
                    "name": "Q. Vu"
                },
                {
                    "authorId": "9965999",
                    "name": "M. Jahanifar"
                },
                {
                    "authorId": "49233880",
                    "name": "Martin Weigert"
                },
                {
                    "authorId": "152732433",
                    "name": "Uwe Schmidt"
                },
                {
                    "authorId": "2183427913",
                    "name": "Wenhua Zhang"
                },
                {
                    "authorId": "1807508122",
                    "name": "Jun Zhang"
                },
                {
                    "authorId": "2115255766",
                    "name": "Sen Yang"
                },
                {
                    "authorId": "31103193",
                    "name": "Jin-Peng Xiang"
                },
                {
                    "authorId": "72541440",
                    "name": "Xiyue Wang"
                },
                {
                    "authorId": "51985449",
                    "name": "J. L. Rumberger"
                },
                {
                    "authorId": "2057204440",
                    "name": "Elias Baumann"
                },
                {
                    "authorId": "152610896",
                    "name": "Peter Hirsch"
                },
                {
                    "authorId": "2277921",
                    "name": "Lihao Liu"
                },
                {
                    "authorId": "2114045523",
                    "name": "Chenyang Hong"
                },
                {
                    "authorId": "1388720262",
                    "name": "Angelica I. Avil\u00e9s-Rivero"
                },
                {
                    "authorId": "2110542269",
                    "name": "Ayushi Jain"
                },
                {
                    "authorId": "2157865704",
                    "name": "Heeyoung Ahn"
                },
                {
                    "authorId": "8791980",
                    "name": "Yiyu Hong"
                },
                {
                    "authorId": "2157430683",
                    "name": "Hussam Azzuni"
                },
                {
                    "authorId": "2153555130",
                    "name": "Min Xu"
                },
                {
                    "authorId": "145628052",
                    "name": "Mohammad Yaqub"
                },
                {
                    "authorId": "121154974",
                    "name": "Marie-Claire Blache"
                },
                {
                    "authorId": "2211436493",
                    "name": "Benoit Pi'egu"
                },
                {
                    "authorId": "5907498",
                    "name": "Bertrand Vernay"
                },
                {
                    "authorId": "82023161",
                    "name": "Tim Scherr"
                },
                {
                    "authorId": "2094840132",
                    "name": "Moritz Bohland"
                },
                {
                    "authorId": "37305634",
                    "name": "K. Loffler"
                },
                {
                    "authorId": "2140292264",
                    "name": "Jiachen Li"
                },
                {
                    "authorId": "1698221",
                    "name": "W. Ying"
                },
                {
                    "authorId": "98243864",
                    "name": "Chixin Wang"
                },
                {
                    "authorId": "49619199",
                    "name": "Dagmar Kainmueller"
                },
                {
                    "authorId": "29398330",
                    "name": "C. Schonlieb"
                },
                {
                    "authorId": "2108062313",
                    "name": "Shuolin Liu"
                },
                {
                    "authorId": "11697193",
                    "name": "D. Talsania"
                },
                {
                    "authorId": "2211435924",
                    "name": "Yughender Meda"
                },
                {
                    "authorId": "144279439",
                    "name": "P. Mishra"
                },
                {
                    "authorId": "98576663",
                    "name": "Muhammad Ridzuan"
                },
                {
                    "authorId": "2113828112",
                    "name": "Oliver Neumann"
                },
                {
                    "authorId": "2145621830",
                    "name": "Marcel P. Schilling"
                },
                {
                    "authorId": "1915293",
                    "name": "M. Reischl"
                },
                {
                    "authorId": "2351970",
                    "name": "R. Mikut"
                },
                {
                    "authorId": "2156922533",
                    "name": "Banban Huang"
                },
                {
                    "authorId": "1388791128",
                    "name": "Hsiang-Chin Chien"
                },
                {
                    "authorId": "2144564716",
                    "name": "Ching-Ping Wang"
                },
                {
                    "authorId": "2155116402",
                    "name": "Chia-Yen Lee"
                },
                {
                    "authorId": "37553559",
                    "name": "Hongfei Lin"
                },
                {
                    "authorId": "3168855",
                    "name": "Zaiyi Liu"
                },
                {
                    "authorId": "2151989660",
                    "name": "Xipeng Pan"
                },
                {
                    "authorId": "2118642307",
                    "name": "Chu Han"
                },
                {
                    "authorId": "2151016643",
                    "name": "Jijun Cheng"
                },
                {
                    "authorId": "144138090",
                    "name": "Muhammad Dawood"
                },
                {
                    "authorId": "31152614",
                    "name": "Srijay Deshpande"
                },
                {
                    "authorId": "51915873",
                    "name": "R. M. S. Bashir"
                },
                {
                    "authorId": "112987024",
                    "name": "A. Shephard"
                },
                {
                    "authorId": "145668048",
                    "name": "P. Costa"
                },
                {
                    "authorId": "2137315473",
                    "name": "Jo\u00e3o D. Nunes"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                },
                {
                    "authorId": "2075457734",
                    "name": "J. S. Cardoso"
                },
                {
                    "authorId": "1668049982",
                    "name": "S. HrishikeshP."
                },
                {
                    "authorId": "1667997430",
                    "name": "Densen Puthussery"
                },
                {
                    "authorId": "2204466001",
                    "name": "G. DevikaR"
                },
                {
                    "authorId": "1668035436",
                    "name": "V. JijiC."
                },
                {
                    "authorId": "48379629",
                    "name": "Yehui Zhang"
                },
                {
                    "authorId": "2041274523",
                    "name": "Zijie Fang"
                },
                {
                    "authorId": "1491968902",
                    "name": "Zhifan Lin"
                },
                {
                    "authorId": "2162638271",
                    "name": "Yongbing Zhang"
                },
                {
                    "authorId": "12622477",
                    "name": "Chun-xin Lin"
                },
                {
                    "authorId": "2107948309",
                    "name": "Liukun Zhang"
                },
                {
                    "authorId": "2066668142",
                    "name": "Lijian Mao"
                },
                {
                    "authorId": "2144152796",
                    "name": "Min Wu"
                },
                {
                    "authorId": "1678392238",
                    "name": "Vicky Vo"
                },
                {
                    "authorId": "2155236116",
                    "name": "Soo-Hyung Kim"
                },
                {
                    "authorId": "2152477133",
                    "name": "T. Lee"
                },
                {
                    "authorId": "2156115660",
                    "name": "Satoshi Kondo"
                },
                {
                    "authorId": "2162266292",
                    "name": "Satoshi Kasai"
                },
                {
                    "authorId": "2211435560",
                    "name": "Pranay Dumbhare"
                },
                {
                    "authorId": "2211435570",
                    "name": "V. Phuse"
                },
                {
                    "authorId": "152291069",
                    "name": "Yash Dubey"
                },
                {
                    "authorId": "41159348",
                    "name": "A. Jamthikar"
                },
                {
                    "authorId": "2123471573",
                    "name": "T. Vuong"
                },
                {
                    "authorId": "2089681",
                    "name": "J. T. Kwak"
                },
                {
                    "authorId": "2112915613",
                    "name": "D. Ziaei"
                },
                {
                    "authorId": "2150178247",
                    "name": "Hyun Jung"
                },
                {
                    "authorId": "2113700840",
                    "name": "Tianyi Miao"
                },
                {
                    "authorId": "47723656",
                    "name": "D. Snead"
                },
                {
                    "authorId": "31752042",
                    "name": "S. Raza"
                },
                {
                    "authorId": "36805137",
                    "name": "F. Minhas"
                },
                {
                    "authorId": "1580315694",
                    "name": "N. Rajpoot"
                }
            ]
        },
        {
            "paperId": "e00b40a7edac5d5a3b557e26727c62ce98855c26",
            "title": "ExplainFix: Explainable spatially fixed deep networks",
            "abstract": "Is there an initialization for deep networks that requires no learning? ExplainFix adopts two design principles: the \u201cfixed filters\u201d principle that all spatial filter weights of convolutional neural networks can be fixed at initialization and never learned, and the \u201cnimbleness\u201d principle that only few network parameters suffice. We contribute (a) visual model\u2010based explanations, (b) speed and accuracy gains, and (c) novel tools for deep convolutional neural networks. ExplainFix gives key insights that spatially fixed networks should have a steered initialization, that spatial convolution layers tend to prioritize low frequencies, and that most network parameters are not necessary in spatially fixed models. ExplainFix models have up to \u00d7100 fewer spatial filter kernels than fully learned models and matching or improved accuracy. Our extensive empirical analysis confirms that ExplainFix guarantees nimbler models (train up to 17% faster with channel pruning), matching or improved predictive performance (spanning 13 distinct baseline models, four architectures and two medical image datasets), improved robustness to larger learning rate, and robustness to varying model size. We are first to demonstrate that all spatial filters in state\u2010of\u2010the\u2010art convolutional deep networks can be fixed at initialization, not learned.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "152888952",
                    "name": "Alex Gaudio"
                },
                {
                    "authorId": "1702392",
                    "name": "C. Faloutsos"
                },
                {
                    "authorId": "1772588",
                    "name": "A. Smailagic"
                },
                {
                    "authorId": "145668048",
                    "name": "P. Costa"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                }
            ]
        },
        {
            "paperId": "143092d693c235b5b8de2b85beb7c0f725bdc3c1",
            "title": "Colon Nuclei Instance Segmentation using a Probabilistic Two-Stage Detector",
            "abstract": "Cancer is one of the leading causes of death in the developed world. Cancer diagnosis is performed through the microscopic analysis of a sample of suspicious tissue. This process is time consuming and error prone, but Deep Learning models could be helpful for pathologists during cancer diagnosis. We propose to change the CenterNet2 object detection model to also perform instance segmentation, which we call SegCenterNet2. We train SegCenterNet2 in the CoNIC challenge dataset and show that it performs better than Mask R-CNN in the competition metrics.",
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145668048",
                    "name": "P. Costa"
                },
                {
                    "authorId": "2115962703",
                    "name": "Y. Fu"
                },
                {
                    "authorId": "2137315473",
                    "name": "Jo\u00e3o D. Nunes"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                },
                {
                    "authorId": "2075457734",
                    "name": "J. S. Cardoso"
                }
            ]
        },
        {
            "paperId": "61d7e95b1cb2427eac4fb6a76753f4ea8b504e88",
            "title": "Epistemic and Heteroscedastic Uncertainty Estimation in Retinal Blood Vessel Segmentation",
            "abstract": "Current state-of-the-art medical image segmentation methods require high quality datasets to obtain good performance. However, medical specialists often disagree on diagnosis, hence, datasets contain contradictory annotations. This, in turn, leads to difficulties in the optimization process of Deep Learning models and hinder performance. We propose a method to estimate uncertainty in Convolutional Neural Network (CNN) segmentation models, that makes the training of CNNs more robust to contradictory annotations. In this work, we model two types of uncertainty, heteroscedastic and epistemic, without adding any additional supervisory signal other than the ground-truth segmentation mask. As expected, the uncertainty is higher closer to vessel boundaries, and on top of thinner and less visible vessels where it is more likely for medical specialists to disagree. Therefore, our method is more suitable to learn from datasets created with heterogeneous annotators. We show that there is a correlation between the uncertainty estimated by our method and the disagreement in the segmentation provided by two different medical specialists. Furthermore, by explicitly modeling the uncertainty, the Intersection over Union of the segmentation network improves 5.7 percentage points.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145668048",
                    "name": "P. Costa"
                },
                {
                    "authorId": "1772588",
                    "name": "A. Smailagic"
                },
                {
                    "authorId": "3698192",
                    "name": "Jaime S. Cardoso"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                }
            ]
        },
        {
            "paperId": "2f418aeb50ed60b4af499c71665c0df848d65024",
            "title": "O\u2010MedAL: Online active deep learning for medical image analysis",
            "abstract": "Active learning (AL) methods create an optimized labeled training set from unlabeled data. We introduce a novel online active deep learning method for medical image analysis. We extend our MedAL AL framework to present new results in this paper. A novel sampling method queries the unlabeled examples that maximize the average distance to all training set examples. Our online method enhances performance of its underlying baseline deep network. These novelties contribute to significant performance improvements, including improving the model's underlying deep network accuracy by 6.30%, using only 25% of the labeled dataset to achieve baseline accuracy, reducing backpropagated images during training by as much as 67%, and demonstrating robustness to class imbalance in binary and multiclass tasks.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "1772588",
                    "name": "A. Smailagic"
                },
                {
                    "authorId": "145668048",
                    "name": "P. Costa"
                },
                {
                    "authorId": "152888952",
                    "name": "Alex Gaudio"
                },
                {
                    "authorId": "51436490",
                    "name": "Kartik Khandelwal"
                },
                {
                    "authorId": "2521288",
                    "name": "Mostafa Mirshekari"
                },
                {
                    "authorId": "10217959",
                    "name": "Jonathon Fagert"
                },
                {
                    "authorId": "29951387",
                    "name": "Devesh Walawalkar"
                },
                {
                    "authorId": "8182534",
                    "name": "Susu Xu"
                },
                {
                    "authorId": "2161526",
                    "name": "A. Galdran"
                },
                {
                    "authorId": "144617913",
                    "name": "Pei Zhang"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                },
                {
                    "authorId": "1832917",
                    "name": "H. Noh"
                }
            ]
        },
        {
            "paperId": "78bf1fe419386e323041ea52490925dafd88068b",
            "title": "EyeWeS: Weakly Supervised Pre-Trained Convolutional Neural Networks for Diabetic Retinopathy Detection",
            "abstract": "Diabetic Retinopathy (DR) is one of the leading causes of preventable blindness in the developed world. With the increasing number of diabetic patients there is a growing need of an automated system for DR detection. We propose Eye WeS, a method that not only detects DR in eye fundus images but also pinpoints the regions of the image that contain lesions, while being trained with image labels only. We show that it is possible to convert any pre-trained convolutional neural network into a weakly-supervised model while increasing their performance and efficiency. EyeWeS improved the results of Inception V3 from 94.9% Area Under the Receiver Operating Curve (AUC) to 95.8% AUC while maintaining only approximately 5% of the Inception V3's number of parameters. The same model is able to achieve 97.1% AUC in a cross-dataset experiment.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145668048",
                    "name": "P. Costa"
                },
                {
                    "authorId": "152926097",
                    "name": "Teresa Ara\u00fajo"
                },
                {
                    "authorId": "115550376",
                    "name": "Guilherme Aresta"
                },
                {
                    "authorId": "2161526",
                    "name": "A. Galdran"
                },
                {
                    "authorId": "144731836",
                    "name": "A. Mendon\u00e7a"
                },
                {
                    "authorId": "1772588",
                    "name": "A. Smailagic"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                }
            ]
        },
        {
            "paperId": "92bcac4b4394d930233508d28cfedab0e3f45145",
            "title": "Real-Time Informative Laryngoscopic Frame Classification with Pre-Trained Convolutional Neural Networks",
            "abstract": "Visual exploration of the larynx represents a relevant technique for the early diagnosis of laryngeal disorders. However, visualizing an endoscopy for finding abnormalities is a time-consuming process, and for this reason much research has been dedicated to the automatic analysis of endoscopic video data. In this work we address the particular task of discriminating among informative laryngoscopic frames and those that carry insufficient diagnostic information. In the latter case, the goal is also to determine the reason for this lack of information. To this end, we analyze the possibility of training three different state-of-the-art Convolutional Neural Networks, but initializing their weights from configurations that have been previously optimized for solving natural image classification problems. Our findings show that the simplest of these three architectures not only is the most accurate (outperforming previously proposed techniques), but also the fastest and most efficient, with the lowest inference time and minimal memory requirements, enabling real-time application and deployment in portable devices.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2161526",
                    "name": "A. Galdran"
                },
                {
                    "authorId": "145668048",
                    "name": "P. Costa"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                }
            ]
        },
        {
            "paperId": "d576e9514156b44864666e135c0d018c9e7082bf",
            "title": "Learning to Segment the Lung Volume from CT Scans Based on Semi-Automatic Ground-Truth",
            "abstract": "Lung volume segmentation is a key step in the design of Computer-Aided Diagnosis systems for automated lung pathology analysis. However, isolating the lung from CT volumes can be a challenging process due to considerable deformations and the potential presence of pathologies. Convolutional Neural Networks (CNN) are effective tools for modeling the spatial relationship between lung voxels. Unfortunately, they typically require large quantities of annotated data, and manually delineating the lung from volumetric CT scans can be a cumbersome process. We propose to train a 3D CNN to solve this task based on semi-automatically generated annotations. For this, we introduce an extension of the well-known V-Net architecture that can handle higher-dimensional input data. Even if the training set labels are noisy and contain errors, our experiments show that it is possible to learn to accurately segment the lung relying on them. Numerical comparisons on an external test set containing lung segmentations provided by a medical expert demonstrate that the proposed model generalizes well to new data, reaching an average 98.7% Dice coefficient. The proposed approach results in a superior performance with respect to the standard V-Net model, particularly on the lung boundary.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144959555",
                    "name": "Patrick Sousa"
                },
                {
                    "authorId": "2161526",
                    "name": "A. Galdran"
                },
                {
                    "authorId": "145668048",
                    "name": "P. Costa"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                }
            ]
        },
        {
            "paperId": "e08ceb59cb67ebfc31ab6b90872b699e32f103ec",
            "title": "Uncertainty-Aware Artery/Vein Classification on Retinal Images",
            "abstract": "The automatic differentiation of retinal vessels into arteries and veins (A/V) is a highly relevant task within the field of retinal image analysis. However, due to limitations of retinal image acquisition devices, specialists can find it impossible to label certain vessels in eye fundus images. In this paper, we introduce a method that takes into account such uncertainty by design. For this, we formulate the A/V classification task as a four-class segmentation problem, and a Convolutional Neural Network is trained to classify pixels into background, A/V, or uncertain classes. The resulting technique can directly provide pixelwise uncertainty estimates. In addition, instead of depending on a previously available vessel segmentation, the method automatically segments the vessel tree. Experimental results show a performance comparable or superior to several recent A/V classification approaches. In addition, the proposed technique also attains state-of-the-art performance when evaluated for the task of vessel segmentation, generalizing to data that was not used during training, even with considerable differences in terms of appearance and resolution.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2161526",
                    "name": "A. Galdran"
                },
                {
                    "authorId": "31540263",
                    "name": "Maria In\u00eas Meyer"
                },
                {
                    "authorId": "145668048",
                    "name": "P. Costa"
                },
                {
                    "authorId": "144731836",
                    "name": "A. Mendon\u00e7a"
                },
                {
                    "authorId": "36054719",
                    "name": "A. Campilho"
                }
            ]
        }
    ]
}