{
    "authorId": "144287919",
    "papers": [
        {
            "paperId": "0f008e07d601e8f21d1df5db3d36e85484840083",
            "title": "GameQA: Gamified Mobile App Platform for Building Multiple-Domain Question-Answering Datasets",
            "abstract": "The methods used to create many of the well-known Question-Answering (QA) datasets are hard to replicate for low-resource languages. A commonality amongst these methods is hiring annotators to source answers from the internet by querying a single answer source, such as Wikipedia. Applying these methods for low-resource languages can be problematic since there is no single large answer source for these languages. Consequently, this can result in a high ratio of unanswered questions, since the amount of information in any single source is limited. To address this problem, we developed a novel crowd-sourcing platform to gather multiple-domain QA data for low-resource languages. Our platform, which consists of a mobile app and a web API, gamifies the data collection process. We successfully released the app for Icelandic (a low-resource language with about 350,000 native speakers) to build a dataset which rivals large QA datasets for high-resource languages both in terms of size and ratio of answered questions. We have made the platform open source with instructions on how to localize and deploy it to gather data for other low-resource languages.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2215628899",
                    "name": "Njall Skarphedinsson"
                },
                {
                    "authorId": "2215615463",
                    "name": "Breki Gudmundsson"
                },
                {
                    "authorId": "2215622263",
                    "name": "Steinar Smari"
                },
                {
                    "authorId": "2372164",
                    "name": "M. L\u00e1rusd\u00f3ttir"
                },
                {
                    "authorId": "40631719",
                    "name": "H. Einarsson"
                },
                {
                    "authorId": "153675592",
                    "name": "Abuzar Khan"
                },
                {
                    "authorId": "144287919",
                    "name": "Eric Nyberg"
                },
                {
                    "authorId": "1713580",
                    "name": "H. Loftsson"
                }
            ]
        },
        {
            "paperId": "3a30217c4115777fb30c182c97cc77d34d065556",
            "title": "InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers",
            "abstract": "We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which -- as we found out -- produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available. https://github.com/searchivarius/inpars_light/",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3308561",
                    "name": "Leonid Boytsov"
                },
                {
                    "authorId": "1570831201",
                    "name": "Preksha Patel"
                },
                {
                    "authorId": "40882083",
                    "name": "Vivek Sourabh"
                },
                {
                    "authorId": "2199743435",
                    "name": "Riddhi Nisar"
                },
                {
                    "authorId": "2147299388",
                    "name": "Sayan Kundu"
                },
                {
                    "authorId": "2055935016",
                    "name": "R. Ramanathan"
                },
                {
                    "authorId": "144287919",
                    "name": "Eric Nyberg"
                }
            ]
        },
        {
            "paperId": "61354e45bca908ad08f24e44bd507b4e1c958e6f",
            "title": "Chain-of-Skills: A Configurable Model for Open-Domain Question Answering",
            "abstract": "The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transfer- ability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "22244290",
                    "name": "Kaixin Ma"
                },
                {
                    "authorId": "47413820",
                    "name": "Hao Cheng"
                },
                {
                    "authorId": "49891156",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "46522098",
                    "name": "Xiaodong Liu"
                },
                {
                    "authorId": "144287919",
                    "name": "Eric Nyberg"
                },
                {
                    "authorId": "48441311",
                    "name": "Jianfeng Gao"
                }
            ]
        },
        {
            "paperId": "0a40a405a1ea603530b8e933e620a0a0da4cb72e",
            "title": "Learn-to-Race Challenge 2022: Benchmarking Safe Learning and Cross-domain Generalisation in Autonomous Racing",
            "abstract": "We present the results of our autonomous racing virtual challenge, based on the newly-released Learn-to-Race (L2R) simulation framework, which seeks to encourage interdisciplinary research in autonomous driving and to help advance the state of the art on a realistic benchmark. Analogous to racing being used to test cutting-edge vehicles, we envision autonomous racing to serve as a particularly challenging proving ground for autonomous agents as: (i) they need to make sub-second, safety-critical decisions in a complex, fast-changing environment; and (ii) both perception and control must be robust to distribution shifts, novel road features, and unseen obstacles. Thus, the main goal of the challenge is to evaluate the joint safety, performance, and generalisation capabilities of reinforcement learning agents on multi-modal perception, through a two-stage process. In the first stage of the challenge, we evaluate an autonomous agent's ability to drive as fast as possible, while adhering to safety constraints. In the second stage, we additionally require the agent to adapt to an unseen racetrack through safe exploration. In this paper, we describe the new L2R Task 2.0 benchmark, with refined metrics and baseline approaches. We also provide an overview of deployment, evaluation, and rankings for the inaugural instance of the L2R Autonomous Racing Virtual Challenge (supported by Carnegie Mellon University, Arrival Ltd., AICrowd, Amazon Web Services, and Honda Research), which officially used the new L2R Task 2.0 benchmark and received over 20,100 views, 437 active participants, 46 teams, and 733 model submissions -- from 88+ unique institutions, in 58+ different countries. Finally, we release leaderboard results from the challenge and provide description of the two top-ranking approaches in cross-domain model transfer, across multiple sensor configurations and simulated races.",
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "authors": [
                {
                    "authorId": "26253744",
                    "name": "Jonathan M Francis"
                },
                {
                    "authorId": "12515120",
                    "name": "Bingqing Chen"
                },
                {
                    "authorId": "10730666",
                    "name": "Siddha Ganju"
                },
                {
                    "authorId": "2153777657",
                    "name": "Sidharth Kathpal"
                },
                {
                    "authorId": "2060864646",
                    "name": "Jyotish Poonganam"
                },
                {
                    "authorId": "2164337930",
                    "name": "Ayush Shivani"
                },
                {
                    "authorId": "34828513",
                    "name": "Sahika Genc"
                },
                {
                    "authorId": "2059103187",
                    "name": "Ivan Zhukov"
                },
                {
                    "authorId": "2056773939",
                    "name": "Max Kumskoy"
                },
                {
                    "authorId": "3219011",
                    "name": "Anirudh Koul"
                },
                {
                    "authorId": "143904954",
                    "name": "Jean Oh"
                },
                {
                    "authorId": "144287919",
                    "name": "Eric Nyberg"
                }
            ]
        },
        {
            "paperId": "1b9625e620b75c1b82c771e59209b19cdd5b191b",
            "title": "Knowledge-driven Scene Priors for Semantic Audio-Visual Embodied Navigation",
            "abstract": "Generalisation to unseen contexts remains a challenge for embodied navigation agents. In the context of semantic audio-visual navigation (SAVi) tasks, the notion of generalisation should include both generalising to unseen indoor visual scenes as well as generalising to unheard sounding objects. However, previous SAVi task definitions do not include evaluation conditions on truly novel sounding objects, resorting instead to evaluating agents on unheard sound clips of known objects; meanwhile, previous SAVi methods do not include explicit mechanisms for incorporating domain knowledge about object and region semantics. These weaknesses limit the development and assessment of models' abilities to generalise their learned experience. In this work, we introduce the use of knowledge-driven scene priors in the semantic audio-visual embodied navigation task: we combine semantic information from our novel knowledge graph that encodes object-region relations, spatial knowledge from dual Graph Encoder Networks, and background knowledge from a series of pre-training tasks -- all within a reinforcement learning framework for audio-visual navigation. We also define a new audio-visual navigation sub-task, where agents are evaluated on novel sounding objects, as opposed to unheard clips of known objects. We show improvements over strong baselines in generalisation to unseen regions and novel sounding objects, within the Habitat-Matterport3D simulation environment, under the SoundSpaces task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1394711029",
                    "name": "Gyan Tatiya"
                },
                {
                    "authorId": "26253744",
                    "name": "Jonathan M Francis"
                },
                {
                    "authorId": "2271468",
                    "name": "L. Bondi"
                },
                {
                    "authorId": "30596850",
                    "name": "Ingrid Navarro"
                },
                {
                    "authorId": "144287919",
                    "name": "Eric Nyberg"
                },
                {
                    "authorId": "1715858",
                    "name": "Jivko Sinapov"
                },
                {
                    "authorId": "143904954",
                    "name": "Jean Oh"
                }
            ]
        },
        {
            "paperId": "37ba5946527629cf538a4675476f824da99dfe35",
            "title": "Table Retrieval May Not Necessitate Table-specific Model Design",
            "abstract": "Tables are an important form of structured data for both human and machine readers alike, providing answers to questions that cannot, or cannot easily, be found in texts. Recent work has designed special models and training paradigms for table-related tasks such as table-based question answering and table retrieval. Though effective, they add complexity in both modeling and data acquisition compared to generic text solutions and obscure which elements are truly beneficial. In this work, we focus on the task of table retrieval, and ask: \u201cis table-specific model design necessary for table retrieval, or can a simpler text-based model be effectively used to achieve a similar result?\u2019\u2019 First, we perform an analysis on a table-based portion of the Natural Questions dataset (NQ-table), and find that structure plays a negligible role in more than 70% of the cases. Based on this, we experiment with a general Dense Passage Retriever (DPR) based on text and a specialized Dense Table Retriever (DTR) that uses table-specific model designs. We find that DPR performs well without any table-specific design and training, and even achieves superior results compared to DTR when fine-tuned on properly linearized tables. We then experiment with three modules to explicitly encode table structures, namely auxiliary row/column embeddings, hard attention masks, and soft relation-based attention biases. However, none of these yielded significant improvements, suggesting that table-specific model design may not be necessary for table retrieval.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "1390877035",
                    "name": "Zhiruo Wang"
                },
                {
                    "authorId": "2669515",
                    "name": "Zhengbao Jiang"
                },
                {
                    "authorId": "144287919",
                    "name": "Eric Nyberg"
                },
                {
                    "authorId": "1700325",
                    "name": "Graham Neubig"
                }
            ]
        },
        {
            "paperId": "3f848e8620a1f1ccef544e46b483ceff5cbc7f2a",
            "title": "Understanding Performance of Long-Document Ranking Models through Comprehensive Evaluation and Leaderboarding",
            "abstract": "We evaluated 20+ Transformer models for ranking of long documents (including recent LongP models trained with FlashAttention) and compared them with a simple FirstP baseline, which applies the same model to the truncated input (at most 512 tokens). We used MS MARCO Documents v1 as a primary training set and evaluated both the zero-shot transferred and fine-tuned models. On MS MARCO, TREC DLs, and Robust04 no long-document model outperformed FirstP by more than 5% in NDCG and MRR (when averaged over all test sets). We conjectured this was not due to models' inability to process long context, but due to a positional bias of relevant passages, whose distribution was skewed towards the beginning of documents. We found direct evidence of this bias in some test sets, which motivated us to create MS MARCO FarRelevant (based on MS MARCO Passages) where the relevant passages were not present among the first 512 tokens. Unlike standard collections where we saw both little benefit from incorporating longer contexts and limited variability in model performance (within a few %), experiments on MS MARCO FarRelevant uncovered dramatic differences among models. The FirstP models performed roughly at the random-baseline level in both zero-shot and fine-tuning scenarios. Simple aggregation models including MaxP and PARADE Attention had good zero-shot accuracy, but benefited little from fine-tuning. Most other models had poor zero-shot performance (sometimes at a random baseline level), but outstripped MaxP by as much as 13-28% after fine-tuning. Thus, the positional bias not only diminishes benefits of processing longer document contexts, but also leads to model overfitting to positional bias and performing poorly in a zero-shot setting when the distribution of relevant passages changes substantially. We make our software and data available.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3308561",
                    "name": "Leonid Boytsov"
                },
                {
                    "authorId": "2143608808",
                    "name": "Tianyi Lin"
                },
                {
                    "authorId": "1664751156",
                    "name": "Fangwei Gao"
                },
                {
                    "authorId": "2175039230",
                    "name": "Yutian Zhao"
                },
                {
                    "authorId": "2174997095",
                    "name": "Jeffrey Huang"
                },
                {
                    "authorId": "144287919",
                    "name": "Eric Nyberg"
                }
            ]
        },
        {
            "paperId": "413bfdd01291d3bc013cdc9cb91066a3c8a1d0e0",
            "title": "PRO-CS : An Instance-Based Prompt Composition Technique for Code-Switched Tasks",
            "abstract": "Code-switched (CS) data is ubiquitous in today\u2019s globalized world, but the dearth of annotated datasets in code-switching poses a significant challenge for learning diverse tasks across different language pairs. Parameter-efficient prompt-tuning approaches conditioned on frozen language models have shown promise for transfer learning in limited-resource setups. In this paper, we propose a novel instance-based prompt composition technique, PRO-CS, for CS tasks that combine language and task knowledge. We compare our approach with prompt-tuning and fine-tuning for code-switched tasks on 10 datasets across 4 language pairs. Our model outperforms the prompt-tuning approach by significant margins across all datasets and outperforms or remains at par with fine-tuning by using just 0.18% of total parameters. We also achieve competitive results when compared with the fine-tuned model in the low-resource cross-lingual and cross-task setting, indicating the effectiveness of our approach to incorporate new code-switched tasks.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "67152985",
                    "name": "Srijan Bansal"
                },
                {
                    "authorId": "51260651",
                    "name": "Suraj Tripathi"
                },
                {
                    "authorId": "2114357424",
                    "name": "Sumit Agarwal"
                },
                {
                    "authorId": "1706595",
                    "name": "T. Mitamura"
                },
                {
                    "authorId": "144287919",
                    "name": "Eric Nyberg"
                }
            ]
        },
        {
            "paperId": "529c6d06d8215561c080797ddaa3e2058142637b",
            "title": "Coalescing Global and Local Information for Procedural Text Understanding",
            "abstract": "Procedural text understanding is a challenging language reasoning task that requires models to track entity states across the development of a narrative. We identify three core aspects required for modeling this task, namely the local and global view of the inputs, as well as the global view of outputs. Prior methods have considered a subset of these aspects, which leads to either low precision or low recall. In this paper, we propose a new model Coalescing Global and Local Information (CGLI), which builds entity- and timestep-aware input representations (local input) considering the whole context (global input), and we jointly model the entity states with a structured prediction objective (global output). Thus, CGLI simultaneously optimizes for both precision and recall. Moreover, we extend CGLI with additional output layers and integrate it into a story reasoning framework. Extensive experiments on a popular procedural text understanding dataset show that our model achieves state-of-the-art results, while experiments on a story reasoning benchmark show the positive impact of our model on downstream reasoning.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "22244290",
                    "name": "Kaixin Ma"
                },
                {
                    "authorId": "2125822063",
                    "name": "Filip Ilievski"
                },
                {
                    "authorId": "26253744",
                    "name": "Jonathan M Francis"
                },
                {
                    "authorId": "144287919",
                    "name": "Eric Nyberg"
                },
                {
                    "authorId": "49930888",
                    "name": "A. Oltramari"
                }
            ]
        },
        {
            "paperId": "6716b7ec2e5ccb8efbcbc22c9a640a6ae523a860",
            "title": "R3 : Refined Retriever-Reader pipeline for Multidoc2dial",
            "abstract": "In this paper, we present our submission to the DialDoc shared task based on the MultiDoc2Dial dataset. MultiDoc2Dial is a conversational question answering dataset that grounds dialogues in multiple documents. The task involves grounding a user\u2019s query in a document followed by generating an appropriate response. We propose several improvements over the baseline\u2019s retriever-reader architecture to aid in modeling goal-oriented dialogues grounded in multiple documents. Our proposed approach employs sparse representations for passage retrieval, a passage re-ranker, the fusion-in-decoder architecture for generation, and a curriculum learning training paradigm. Our approach shows a 12 point improvement in BLEU score compared to the baseline RAG model.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "67152985",
                    "name": "Srijan Bansal"
                },
                {
                    "authorId": "51260651",
                    "name": "Suraj Tripathi"
                },
                {
                    "authorId": "2114357424",
                    "name": "Sumit Agarwal"
                },
                {
                    "authorId": "2165225404",
                    "name": "Sireesh Gururaja"
                },
                {
                    "authorId": "2123019049",
                    "name": "Aditya Srikanth Veerubhotla"
                },
                {
                    "authorId": "36662010",
                    "name": "Ritam Dutt"
                },
                {
                    "authorId": "1706595",
                    "name": "T. Mitamura"
                },
                {
                    "authorId": "144287919",
                    "name": "Eric Nyberg"
                }
            ]
        }
    ]
}