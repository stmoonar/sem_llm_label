{
    "authorId": "153034701",
    "papers": [
        {
            "paperId": "f9b419d106c42c3036ffd4cb7527d8883bfe09d3",
            "title": "Reminiscences on Influential Papers",
            "abstract": "This issue's contributors all work at the intersection of machine learning and databases. Coincidentally, three of them picked papers that received the VLDB 10-Year Best Paper award, and one of them picked a paper that, in my opinion, should receive the CIDR Test-of-Time award at some point. Their write-ups highlight very well why these four papers still continue to leave a mark on us personally and the real-world data systems. Enjoy reading!\n While I will keep inviting members of the data management community, and neighboring communities, to contribute to this column, I also welcome unsolicited contributions. Please contact me if you are interested.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "144812095",
                    "name": "K. A. Ross"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1721062",
                    "name": "Nick Koudas"
                },
                {
                    "authorId": "1696519",
                    "name": "R. Rastogi"
                },
                {
                    "authorId": "1786532",
                    "name": "D. Rosenkrantz"
                },
                {
                    "authorId": "144823472",
                    "name": "P. Scheuermann"
                },
                {
                    "authorId": "144823759",
                    "name": "Dan Suciu"
                }
            ]
        },
        {
            "paperId": "fd0b6af0dce760f2d65a641e04ced8fd1e39697c",
            "title": "A Multi-Level Operation Method for Improving the Resilience of Power Systems under Extreme Weather through Preventive Control and a Virtual Oscillator",
            "abstract": "This paper proposes a multi-level operation method designed to enhance the resilience of power systems under extreme weather conditions by utilizing preventive control and virtual oscillator (VO) technology. Firstly, a novel model for predicting time intervals between successive failures of the power system during extreme weather is introduced. Based on this, this paper proposes a preventive control method considering the system ramping and transmission constraints prior to failures so as to ensure the normal electricity demand within the system. Further, a VO-based adaptive frequency control strategy is designed to accelerate the regulation speed and eliminate the frequency deviation. Finally, the control performance is comprehensively compared under different experimental conditions. The results verify that the method accurately predicted the time of the line fault occurrence, with a maximum error not exceeding 3 min compared to the actual occurrence; also, the virtual oscillator control (VOC) strategy outperformed traditional droop control in frequency stabilization, achieving stability within 2 s compared to the droop control\u2019s continued fluctuations beyond 20 s. These results highlight VOC\u2019s superior effectiveness in frequency stability and control in power systems.",
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "authors": [
                {
                    "authorId": "2286243287",
                    "name": "Chenghao Li"
                },
                {
                    "authorId": "2193057351",
                    "name": "Di Zhang"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "98707409",
                    "name": "Chunsun Tian"
                },
                {
                    "authorId": "2292346687",
                    "name": "Longjie Xie"
                },
                {
                    "authorId": "2291204137",
                    "name": "Chenxia Wang"
                },
                {
                    "authorId": "2072874634",
                    "name": "Zhou Fang"
                },
                {
                    "authorId": "2291635402",
                    "name": "Li Li"
                },
                {
                    "authorId": "2291337521",
                    "name": "Guanyu Zhang"
                }
            ]
        },
        {
            "paperId": "9a73d00559c301a1dbb24bfbb4f5f4156ae23496",
            "title": "TaxoCom: Topic Taxonomy Completion with Hierarchical Discovery of Novel Topic Clusters",
            "abstract": "Topic taxonomies, which represent the latent topic (or category) structure of document collections, provide valuable knowledge of contents in many applications such as web search and information filtering. Recently, several unsupervised methods have been developed to automatically construct the topic taxonomy from a text corpus, but it is challenging to generate the desired taxonomy without any prior knowledge. In this paper, we study how to leverage the partial (or incomplete) information about the topic structure as guidance to find out the complete topic taxonomy. We propose a novel framework for topic taxonomy completion, named TaxoCom, which recursively expands the topic taxonomy by discovering novel sub-topic clusters of terms and documents. To effectively identify novel topics within a hierarchical topic structure, TaxoCom devises its embedding and clustering techniques to be closely-linked with each other: (i) locally discriminative embedding optimizes the text embedding space to be discriminative among known (i.e., given) sub-topics, and (ii) novelty adaptive clustering assigns terms into either one of the known sub-topics or novel sub-topics. Our comprehensive experiments on two real-world datasets demonstrate that TaxoCom not only generates the high-quality topic taxonomy in terms of term coherency and topic coverage but also outperforms all other baselines for a downstream task.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3067773",
                    "name": "Dongha Lee"
                },
                {
                    "authorId": "3363642",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "71965979",
                    "name": "SeongKu Kang"
                },
                {
                    "authorId": "3396235",
                    "name": "Susik Yoon"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "1723357",
                    "name": "Hwanjo Yu"
                }
            ]
        },
        {
            "paperId": "d33296698859424b532355c198eeed98e76cf5bc",
            "title": "Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification",
            "abstract": "Large-scale multi-label text classification (LMTC) aims to associate a document with its relevant labels from a large candidate set. Most existing LMTC approaches rely on massive human-annotated training data, which are often costly to obtain and suffer from a long-tailed label distribution (i.e., many labels occur only a few times in the training set). In this paper, we study LMTC under the zero-shot setting, which does not require any annotated documents with labels and only relies on label surface names and descriptions. To train a classifier that calculates the similarity score between a document and a label, we propose a novel metadata-induced contrastive learning (MICoL) method. Different from previous text-based contrastive learning techniques, MICoL exploits document metadata (e.g., authors, venues, and references of research papers), which are widely available on the Web, to derive similar document\u2013document pairs. Experimental results on two large-scale datasets show that: (1) MICoL significantly outperforms strong zero-shot text classification and contrastive learning baselines; (2) MICoL is on par with the state-of-the-art supervised metadata-aware LMTC method trained on 10K\u2013200K labeled documents; and (3) MICoL tends to predict more infrequent labels than supervised methods, thus alleviates the deteriorated performance on long-tailed labels.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49891156",
                    "name": "Yu Zhang"
                },
                {
                    "authorId": "3303634",
                    "name": "Zhihong Shen"
                },
                {
                    "authorId": "153248481",
                    "name": "Chieh-Han Wu"
                },
                {
                    "authorId": "2064542611",
                    "name": "Boya Xie"
                },
                {
                    "authorId": "7182462",
                    "name": "Junheng Hao"
                },
                {
                    "authorId": "2155353485",
                    "name": "Yexin Wang"
                },
                {
                    "authorId": "1748169",
                    "name": "Kuansan Wang"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "039ce73659332c12168de439e3f79e7039b636af",
            "title": "RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System",
            "abstract": "We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "4428136",
                    "name": "Haoyang Wen"
                },
                {
                    "authorId": "2117032681",
                    "name": "Ying Lin"
                },
                {
                    "authorId": "145242558",
                    "name": "T. Lai"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "2109154767",
                    "name": "Sha Li"
                },
                {
                    "authorId": "48030192",
                    "name": "Xudong Lin"
                },
                {
                    "authorId": "2108536188",
                    "name": "Ben Zhou"
                },
                {
                    "authorId": "2118482058",
                    "name": "Manling Li"
                },
                {
                    "authorId": "34269118",
                    "name": "Haoyu Wang"
                },
                {
                    "authorId": "2111112132",
                    "name": "Hongming Zhang"
                },
                {
                    "authorId": "3099583",
                    "name": "Xiaodong Yu"
                },
                {
                    "authorId": "2101316346",
                    "name": "Alexander Dong"
                },
                {
                    "authorId": "2108330537",
                    "name": "Zhenhailong Wang"
                },
                {
                    "authorId": "51135899",
                    "name": "Y. Fung"
                },
                {
                    "authorId": "51234098",
                    "name": "Piyush Mishra"
                },
                {
                    "authorId": "1904906987",
                    "name": "Qing Lyu"
                },
                {
                    "authorId": "35399640",
                    "name": "D\u00eddac Sur\u00eds"
                },
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "1783500",
                    "name": "S. Brown"
                },
                {
                    "authorId": "145755155",
                    "name": "Martha Palmer"
                },
                {
                    "authorId": "1763608",
                    "name": "Chris Callison-Burch"
                },
                {
                    "authorId": "1856025",
                    "name": "Carl Vondrick"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "144590225",
                    "name": "D. Roth"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                },
                {
                    "authorId": "144016781",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "15e100120f080b9ef4230b4cbb8e107b76e2b839",
            "title": "TaxoClass: Hierarchical Multi-Label Text Classification Using Only Class Names",
            "abstract": "Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a taxonomic class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its \u201ccore classes\u201d, and then check core classes\u2019 ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document\u2019s core classes and utilizes confident core classes to train a taxonomy-enhanced classifier, and (3) generalizes the classifier via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only class names, outperforming the best previous method by 25%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3363642",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "46336131",
                    "name": "Wenda Qiu"
                },
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "2884976",
                    "name": "Jingbo Shang"
                },
                {
                    "authorId": "145201124",
                    "name": "Xiang Ren"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "16c94186fda2162facb4e2478a415ac8c5b29f0d",
            "title": "Who Should Go First? A Self-Supervised Concept Sorting Model for Improving Taxonomy Expansion",
            "abstract": "Taxonomies have been widely used in various machine learning and text mining systems to organize knowledge and facilitate downstream tasks. One critical challenge is that, as data and business scope grow in real applications, existing taxonomies need to be expanded to incorporate new concepts. Previous works on taxonomy expansion process the new concepts independently and simultaneously, ignoring the potential relationships among them and the appropriate order of inserting operations. However, in reality, the new concepts tend to be mutually correlated and form local hypernym-hyponym structures. In such a scenario, ignoring the dependencies of new concepts and the order of insertion may trigger error propagation. For example, existing taxonomy expansion systems may insert hyponyms to existing taxonomies before their hypernym, leading to sub-optimal expanded taxonomies. To complement existing taxonomy expansion systems, we propose TaxoOrder, a novel self-supervised framework that simultaneously discovers the local hypernym-hyponym structure among new concepts and decides the order of insertion. TaxoOrder can be directly plugged into any taxonomy expansion system and improve the quality of expanded taxonomies. Experiments on the real-world dataset validate the effectiveness of TaxoOrder to enhance taxonomy expansion systems, leading to better-resulting taxonomies with comparison to baselines under various evaluation metrics.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "19214393",
                    "name": "Xiangchen Song"
                },
                {
                    "authorId": "3363642",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "47540245",
                    "name": "Jieyu Zhang"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "19537be34dbadbcaa4fffcf028a8ada5095b1b5c",
            "title": "COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining",
            "abstract": "We present a self-supervised learning framework, COCO-LM, that pretrains Language Models by COrrecting and COntrasting corrupted text sequences. Following ELECTRA-style pretraining, COCO-LM employs an auxiliary language model to corrupt text sequences, upon which it constructs two new tasks for pretraining the main model. The first token-level task, Corrective Language Modeling, is to detect and correct tokens replaced by the auxiliary model, in order to better capture token-level semantics. The second sequence-level task, Sequence Contrastive Learning, is to align text sequences originated from the same source input while ensuring uniformity in the representation space. Experiments on GLUE and SQuAD demonstrate that COCO-LM not only outperforms recent state-of-the-art pretrained models in accuracy, but also improves pretraining efficiency. It achieves the MNLI accuracy of ELECTRA with 50% of its pretraining GPU hours. With the same pretraining steps of standard base/large-sized models, COCO-LM outperforms the previous best models by 1+ GLUE average points.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "145391513",
                    "name": "Yu Meng"
                },
                {
                    "authorId": "144628574",
                    "name": "Chenyan Xiong"
                },
                {
                    "authorId": "34765717",
                    "name": "Payal Bajaj"
                },
                {
                    "authorId": "40070335",
                    "name": "Saurabh Tiwary"
                },
                {
                    "authorId": "144609235",
                    "name": "Paul N. Bennett"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "50706785",
                    "name": "Xia Song"
                }
            ]
        },
        {
            "paperId": "27794bca3b7327aff29e2593e8b989b6a5af678b",
            "title": "Training ELECTRA Augmented with Multi-word Selection",
            "abstract": "Pre-trained text encoders such as BERT and its variants have recently achieved state-of-the-art performances on many NLP tasks. While being effective, these pre-training methods typically demand massive computation resources. To accelerate pre-training, ELECTRA trains a discriminator that predicts whether each input token is replaced by a generator. However, this new task, as a binary classification, is less semantically informative. In this study, we present a new text encoder pre-training method that improves ELECTRA based on multi-task learning. Specifically, we train the discriminator to simultaneously detect replaced tokens and select original tokens from candidate sets. We further develop two techniques to effectively combine all pre-training tasks: (1) using attention-based networks for task-specific heads, and (2) sharing bottom layers of the generator and the discriminator. Extensive experiments on GLUE and SQuAD datasets demonstrate both the effectiveness and the efficiency of our proposed method.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3363642",
                    "name": "Jiaming Shen"
                },
                {
                    "authorId": "2746747",
                    "name": "Jialu Liu"
                },
                {
                    "authorId": "2115346248",
                    "name": "Tianqi Liu"
                },
                {
                    "authorId": "82737548",
                    "name": "Cong Yu"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                }
            ]
        },
        {
            "paperId": "2a15f541a8216782a7579f01f56762448e48b582",
            "title": "Document-Level Event Argument Extraction by Conditional Generation",
            "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model\u2019s trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2109154767",
                    "name": "Sha Li"
                },
                {
                    "authorId": "2113323573",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "153034701",
                    "name": "Jiawei Han"
                }
            ]
        }
    ]
}