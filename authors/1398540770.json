{
    "authorId": "1398540770",
    "papers": [
        {
            "paperId": "10efe7785373604dc2d8a355fceb6130f77915a9",
            "title": "Beyond Incompatibility: Trade-offs between Mutually Exclusive Fairness Criteria in Machine Learning and Law",
            "abstract": "Fair and trustworthy AI is becoming ever more important in both machine learning and legal domains. One important consequence is that decision makers must seek to guarantee a 'fair', i.e., non-discriminatory, algorithmic decision procedure. However, there are several competing notions of algorithmic fairness that have been shown to be mutually incompatible under realistic factual assumptions. This concerns, for example, the widely used fairness measures of 'calibration within groups' and 'balance for the positive/negative class'. In this paper, we present a novel algorithm (FAir Interpolation Method: FAIM) for continuously interpolating between these three fairness criteria. Thus, an initially unfair prediction can be remedied to, at least partially, meet a desired, weighted combination of the respective fairness conditions. We demonstrate the effectiveness of our algorithm when applied to synthetic data, the COMPAS data set, and a new, real-world data set from the e-commerce sector. Finally, we discuss to what extent FAIM can be harnessed to comply with conflicting legal obligations. The analysis suggests that it may operationalize duties in traditional legal fields, such as credit scoring and criminal justice proceedings, but also for the latest AI regulations put forth in the EU, like the Digital Markets Act and the recently enacted AI Act.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41154657",
                    "name": "Meike Zehlike"
                },
                {
                    "authorId": "2311693522",
                    "name": "Alex Loosley"
                },
                {
                    "authorId": "2198666318",
                    "name": "H\u00e5kan Jonsson"
                },
                {
                    "authorId": "1398540770",
                    "name": "Emil Wiedemann"
                },
                {
                    "authorId": "2311691285",
                    "name": "Philipp Hacker"
                }
            ]
        },
        {
            "paperId": "ce6778e1171fc2c229e99e61c4b8ff089efc1a79",
            "title": "Towards a Flexible Framework for Algorithmic Fairness",
            "abstract": "Increasingly, scholars seek to integrate legal and technological insights to combat bias in AI systems. In recent years, many different definitions for ensuring non-discrimination in algorithmic decision systems have been put forward. In this paper, we first briefly describe the EU law framework covering cases of algorithmic discrimination. Second, we present an algorithm that harnesses optimal transport to provide a flexible framework to interpolate between different fairness definitions. Third, we show that important normative and legal challenges remain for the implementation of algorithmic fairness interventions in real-world scenarios. Overall, the paper seeks to contribute to the quest for flexible technical frameworks that can be adapted to varying legal and normative fairness constraints.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "49095886",
                    "name": "P. Hacker"
                },
                {
                    "authorId": "1398540770",
                    "name": "Emil Wiedemann"
                },
                {
                    "authorId": "41154657",
                    "name": "Meike Zehlike"
                }
            ]
        },
        {
            "paperId": "d014c61d1862f5bcc190563b774a8a3712505c7a",
            "title": "A continuous framework for fairness",
            "abstract": "Increasingly, discrimination by algorithms is perceived as a societal and legal problem. As a response, a number of criteria for implementing algorithmic fairness in machine learning have been developed in the literature. This paper proposes the Continuous Fairness Algorithm (CFA$\\theta$) which enables a continuous interpolation between different fairness definitions. More specifically, we make three main contributions to the existing literature. First, our approach allows the decision maker to continuously vary between concepts of individual and group fairness. As a consequence, the algorithm enables the decision maker to adopt intermediate \"worldviews\" on the degree of discrimination encoded in algorithmic processes, adding nuance to the extreme cases of \"we're all equal\" (WAE) and \"what you see is what you get\" (WYSIWYG) proposed so far in the literature. Second, we use optimal transport theory, and specifically the concept of the barycenter, to maximize decision maker utility under the chosen fairness constraints. Third, the algorithm is able to handle cases of intersectionality, i.e., of multi-dimensional discrimination of certain groups on grounds of several criteria. We discuss three main examples (college admissions; credit application; insurance contracts) and map out the policy implications of our approach. The explicit formalization of the trade-off between individual and group fairness allows this post-processing approach to be tailored to different situational contexts in which one or the other fairness criterion may take precedence.",
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "authors": [
                {
                    "authorId": "49095886",
                    "name": "P. Hacker"
                },
                {
                    "authorId": "1398540770",
                    "name": "Emil Wiedemann"
                }
            ]
        }
    ]
}