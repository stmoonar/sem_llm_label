{
    "authorId": "153188991",
    "papers": [
        {
            "paperId": "38be7643bcad936739550a1802220eb53ca9b1df",
            "title": "Simple Token-Level Confidence Improves Caption Correctness",
            "abstract": "The ability to judge whether a caption correctly describes an image is a critical part of vision-language understanding. However, state-of-the-art models often misinterpret the correctness of fine-grained details, leading to errors in outputs such as hallucinating objects in generated captions or poor compositional reasoning. In this work, we explore Token-Level Confidence, or TLC, as a simple yet surprisingly effective method to assess caption correctness. Specifically, we fine-tune a vision-language model on image captioning, input an image and proposed caption to the model, and aggregate either algebraic or learned token confidences over words or sequences to estimate image-caption consistency. Compared to sequence-level scores from pretrained models, TLC with algebraic confidence measures achieves a relative improvement in accuracy by 10% on verb understanding in SVO-Probes and more than doubles image and group scores for compositional reasoning in Winoground. When training data are available, a learned confidence estimator provides further improved performance, reducing object hallucination rates in MS COCO Captions by a relative 30% over the original model and setting a new state-of-the-art.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "52013156",
                    "name": "Suzanne Petryk"
                },
                {
                    "authorId": "153188991",
                    "name": "Spencer Whitehead"
                },
                {
                    "authorId": "2119114885",
                    "name": "Joseph Gonzalez"
                },
                {
                    "authorId": "1753210",
                    "name": "Trevor Darrell"
                },
                {
                    "authorId": "34721166",
                    "name": "Anna Rohrbach"
                },
                {
                    "authorId": "34849128",
                    "name": "Marcus Rohrbach"
                }
            ]
        },
        {
            "paperId": "7470a1702c8c86e6f28d32cfa315381150102f5b",
            "title": "Segment Anything",
            "abstract": "We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive \u2013 often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at segment-anything.com to foster research into foundation models for computer vision. We recommend reading the full paper at: arxiv.org/abs/2304.02643.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "2064802835",
                    "name": "A. Kirillov"
                },
                {
                    "authorId": "13131689",
                    "name": "Eric Mintun"
                },
                {
                    "authorId": "2065647966",
                    "name": "Nikhila Ravi"
                },
                {
                    "authorId": "2053590350",
                    "name": "Hanzi Mao"
                },
                {
                    "authorId": "2213549340",
                    "name": "Chloe Rolland"
                },
                {
                    "authorId": "47029037",
                    "name": "Laura Gustafson"
                },
                {
                    "authorId": "15727192",
                    "name": "Tete Xiao"
                },
                {
                    "authorId": "153188991",
                    "name": "Spencer Whitehead"
                },
                {
                    "authorId": "39668247",
                    "name": "A. Berg"
                },
                {
                    "authorId": "3317278",
                    "name": "Wan-Yen Lo"
                },
                {
                    "authorId": "3127283",
                    "name": "Piotr Doll\u00e1r"
                },
                {
                    "authorId": "2983898",
                    "name": "Ross B. Girshick"
                }
            ]
        },
        {
            "paperId": "e2e10d70f249db27088fb2d37cf47272d06d6136",
            "title": "Improving Selective Visual Question Answering by Learning from Your Peers",
            "abstract": "Despite advances in Visual Question Answering (VQA), the ability of models to assess their own correctness remains under-explored. Recent work has shown that VQA models, out-of-the-box, can have difficulties abstaining from answering when they are wrong. The option to abstain, also called Selective Prediction, is highly relevant when deploying systems to users who must trust the system's output (e.g., VQA assistants for users with visual impairments). For such scenarios, abstention can be especially important as users may provide out-of-distribution (OOD) or adversarial inputs that make incorrect answers more likely. In this work, we explore Selective VQA in both in-distribution (ID) and OOD scenarios, where models are presented with mixtures of ID and OOD data. The goal is to maximize the number of questions answered while minimizing the risk of error on those questions. We propose a simple yet effective Learning from Your Peers (LYP) approach for training multimodal selection functions for making abstention decisions. Our approach uses predictions from models trained on distinct subsets of the training data as targets for optimizing a Selective VQA model. It does not require additional manual labels or held-out data and provides a signal for identifying examples that are easy/difficult to generalize to. In our extensive evaluations, we show this benefits a number of models across different architectures and scales. Overall, for ID, we reach 32.92% in the selective prediction metric coverage at 1 % risk of error $(\\mathcal{C} {@} 1\\%)$ which doubles the previous best coverage of 15.79% on this task. For mixed ID/OOD, using models' softmax confidences for abstention decisions performs very poorly, answering <5% of questions at 1 % risk of error even when faced with only 10% OOD examples, but a learned selection function with LYP can increase that to 25.38% $\\mathcal{C}$@1%.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41020827",
                    "name": "Corentin Dancette"
                },
                {
                    "authorId": "153188991",
                    "name": "Spencer Whitehead"
                },
                {
                    "authorId": "2041015203",
                    "name": "Rishabh Maheshwary"
                },
                {
                    "authorId": "8137017",
                    "name": "Ramakrishna Vedantam"
                },
                {
                    "authorId": "1770312",
                    "name": "Stefan Scherer"
                },
                {
                    "authorId": "1639441927",
                    "name": "Xinlei Chen"
                },
                {
                    "authorId": "51021910",
                    "name": "M. Cord"
                },
                {
                    "authorId": "34849128",
                    "name": "Marcus Rohrbach"
                }
            ]
        },
        {
            "paperId": "ef17ffdd75a1b0347add2b91f90d937277138e90",
            "title": "Reliable Visual Question Answering: Abstain Rather Than Answer Incorrectly",
            "abstract": "Machine learning has advanced dramatically, narrowing the accuracy gap to humans in multimodal tasks like visual question answering (VQA). However, while humans can say\"I don't know\"when they are uncertain (i.e., abstain from answering a question), such ability has been largely neglected in multimodal research, despite the importance of this problem to the usage of VQA in real settings. In this work, we promote a problem formulation for reliable VQA, where we prefer abstention over providing an incorrect answer. We first enable abstention capabilities for several VQA models, and analyze both their coverage, the portion of questions answered, and risk, the error on that portion. For that, we explore several abstention approaches. We find that although the best performing models achieve over 70% accuracy on the VQA v2 dataset, introducing the option to abstain by directly using a model's softmax scores limits them to answering less than 7.5% of the questions to achieve a low risk of error (i.e., 1%). This motivates us to utilize a multimodal selection function to directly estimate the correctness of the predicted answers, which we show can increase the coverage by, for example, 2.3x from 6.8% to 15.6% at 1% risk. While it is important to analyze both coverage and risk, these metrics have a trade-off which makes comparing VQA models challenging. To address this, we also propose an Effective Reliability metric for VQA that places a larger cost on incorrect answers compared to abstentions. This new problem formulation, metric, and analysis for VQA provide the groundwork for building effective and reliable VQA models that have the self-awareness to abstain if and only if they don't know the answer.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153188991",
                    "name": "Spencer Whitehead"
                },
                {
                    "authorId": "52013156",
                    "name": "Suzanne Petryk"
                },
                {
                    "authorId": "2163739291",
                    "name": "Vedaad Shakib"
                },
                {
                    "authorId": "49988044",
                    "name": "Joseph E. Gonzalez"
                },
                {
                    "authorId": "1753210",
                    "name": "Trevor Darrell"
                },
                {
                    "authorId": "34721166",
                    "name": "Anna Rohrbach"
                },
                {
                    "authorId": "34849128",
                    "name": "Marcus Rohrbach"
                }
            ]
        },
        {
            "paperId": "6be64445935dcdf4053a6e78b623b80a314d9bbc",
            "title": "Separating Skills and Concepts for Novel Visual Question Answering",
            "abstract": "Generalization to out-of-distribution data has been a problem for Visual Question Answering (VQA) models. To measure generalization to novel questions, we propose to separate them into \"skills\" and \"concepts\". \"Skills\" are visual tasks, such as counting or attribute recognition, and are applied to \"concepts\" mentioned in the question, such as objects and people. VQA methods should be able to compose skills and concepts in novel ways, regardless of whether the specific composition has been seen in training, yet we demonstrate that existing models have much to improve upon towards handling new compositions. We present a novel method for learning to compose skills and concepts that separates these two factors implicitly within a model by learning grounded concept representations and disentangling the encoding of skills from that of concepts. We enforce these properties with a novel contrastive learning procedure that does not rely on external annotations and can be learned from unlabeled image-question pairs. Experiments demonstrate the effectiveness of our approach for improving compositional and grounding performance.1",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153188991",
                    "name": "Spencer Whitehead"
                },
                {
                    "authorId": "47987329",
                    "name": "Hui Wu"
                },
                {
                    "authorId": "144016781",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "1723233",
                    "name": "R. Feris"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                },
                {
                    "authorId": "2115030869",
                    "name": "Uiuc MIT-IBM"
                }
            ]
        },
        {
            "paperId": "04f7834936bf8f455f804c4d84b52fcffc6784ee",
            "title": "Cross-media Structured Common Space for Multimedia Event Extraction",
            "abstract": "We introduce a new task, MultiMedia Event Extraction, which aims to extract events and their arguments from multimedia documents. We develop the first benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments. We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to uni-modal state-of-the-art methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to state-of-the-art multimedia unstructured representations, we achieve 8.3% and 5.0% absolute F-score gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4% more event mentions than traditional text-only methods.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3361240",
                    "name": "Manling Li"
                },
                {
                    "authorId": "2778637",
                    "name": "Alireza Zareian"
                },
                {
                    "authorId": "145653969",
                    "name": "Qi Zeng"
                },
                {
                    "authorId": "153188991",
                    "name": "Spencer Whitehead"
                },
                {
                    "authorId": "152347526",
                    "name": "Di Lu"
                },
                {
                    "authorId": "2113323573",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                }
            ]
        },
        {
            "paperId": "1904c5389a70a905019d5429f09bc7f669bdc898",
            "title": "Learning from Lexical Perturbations for Consistent Visual Question Answering",
            "abstract": "Existing Visual Question Answering (VQA) models are often fragile and sensitive to input variations. In this paper, we propose a novel approach to address this issue based on modular networks, which creates two questions related by linguistic perturbations and regularizes the visual reasoning process between them to be consistent during training. We show that our framework markedly improves consistency and generalization ability, demonstrating the value of controlled linguistic perturbations as a useful and currently underutilized training and regularization tool for VQA models. We also present VQA Perturbed Pairings (VQA P2), a new, low-cost benchmark and augmentation pipeline to create controllable linguistic variations of VQA questions. Our benchmark uniquely draws from large-scale linguistic resources, avoiding human annotation effort while maintaining data quality compared to generative approaches. We benchmark existing VQA models using VQA P2 and provide robustness analysis on each type of linguistic variation.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "153188991",
                    "name": "Spencer Whitehead"
                },
                {
                    "authorId": "47987329",
                    "name": "Hui Wu"
                },
                {
                    "authorId": "51135899",
                    "name": "Y. Fung"
                },
                {
                    "authorId": "2113323573",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "1723233",
                    "name": "R. Feris"
                },
                {
                    "authorId": "2903226",
                    "name": "Kate Saenko"
                }
            ]
        },
        {
            "paperId": "51c8975d88aa66781300e8ca88272ab3112445c0",
            "title": "GAIA: A Fine-grained Multimedia Knowledge Extraction System",
            "abstract": "We present the first comprehensive, open source multimedia knowledge extraction system that takes a massive stream of unstructured, heterogeneous multimedia data from various sources and languages as input, and creates a coherent, structured knowledge base, indexing entities, relations, and events, following a rich, fine-grained ontology. Our system, GAIA, enables seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos. GAIA achieves top performance at the recent NIST TAC SM-KBP2019 evaluation. The system is publicly available at GitHub and DockerHub, with a narrated video that documents the system.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "3361240",
                    "name": "Manling Li"
                },
                {
                    "authorId": "2778637",
                    "name": "Alireza Zareian"
                },
                {
                    "authorId": "2117032681",
                    "name": "Ying Lin"
                },
                {
                    "authorId": "34741133",
                    "name": "Xiaoman Pan"
                },
                {
                    "authorId": "153188991",
                    "name": "Spencer Whitehead"
                },
                {
                    "authorId": "2108342501",
                    "name": "Brian Chen"
                },
                {
                    "authorId": "1993581583",
                    "name": "Bo Wu"
                },
                {
                    "authorId": "2113323573",
                    "name": "Heng Ji"
                },
                {
                    "authorId": "9546964",
                    "name": "Shih-Fu Chang"
                },
                {
                    "authorId": "1817166",
                    "name": "Clare R. Voss"
                },
                {
                    "authorId": "1413386261",
                    "name": "Dan Napierski"
                },
                {
                    "authorId": "2052513135",
                    "name": "Marjorie Freedman"
                }
            ]
        },
        {
            "paperId": "bfd384dfb550fc5d9b4ce506c28c8626cbcebc76",
            "title": "Global Attention for Name Tagging",
            "abstract": "Many name tagging approaches use local contextual information with much success, but can fail when the local context is ambiguous or limited. We present a new framework to improve name tagging by utilizing local, document-level, and corpus-level contextual information. For each word, we retrieve document-level context from other sentences within the same document and corpus-level context from sentences in other documents. We propose a model that learns to incorporate document-level and corpus-level contextual information alongside local contextual information via document-level and corpus-level attentions, which dynamically weight their respective contextual information and determines the influence of this information through gating mechanisms. Experiments on benchmark datasets show the effectiveness of our approach, which achieves state-of-the-art results for Dutch, German, and Spanish on the CoNLL-2002 and CoNLL-2003 datasets. We will make our code and pre-trained models publicly available for research purposes.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "38629264",
                    "name": "Boliang Zhang"
                },
                {
                    "authorId": "153188991",
                    "name": "Spencer Whitehead"
                },
                {
                    "authorId": "34170717",
                    "name": "Lifu Huang"
                },
                {
                    "authorId": "144016781",
                    "name": "Heng Ji"
                }
            ]
        },
        {
            "paperId": "127a6b196914ea14dd1551aaaa3dddea54c8952d",
            "title": "A Deep Reinforcement Learning based Approach to Learning Transferable Proof Guidance Strategies",
            "abstract": "Traditional first-order logic (FOL) reasoning systems usually rely on manual heuristics for proof guidance. We propose TRAIL: a system that learns to perform proof guidance using reinforcement learning. A key design principle of our system is that it is general enough to allow transfer to problems in different domains that do not share the same vocabulary of the training set. To do so, we developed a novel representation of the internal state of a prover in terms of clauses and inference actions. We also propose a novel neural-based attention mechanism to learn interactions between clauses. We demonstrate that this approach enables the system to generalize from training to test data across domains with different vocabularies, suggesting that the neural architecture in TRAIL is well suited for representing and processing of logical formalisms. We also show that TRAIL's learned strategies provide a comparable performance to an established heuristics-based theorem prover.",
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "authors": [
                {
                    "authorId": "41036307",
                    "name": "M. Crouse"
                },
                {
                    "authorId": "153188991",
                    "name": "Spencer Whitehead"
                },
                {
                    "authorId": "145749443",
                    "name": "I. Abdelaziz"
                },
                {
                    "authorId": "2542374",
                    "name": "B. Makni"
                },
                {
                    "authorId": "2470518",
                    "name": "Cristina Cornelio"
                },
                {
                    "authorId": "2223082",
                    "name": "Pavan Kapanipathi"
                },
                {
                    "authorId": "1403257577",
                    "name": "Edwin Pell"
                },
                {
                    "authorId": "145993352",
                    "name": "Kavitha Srinivas"
                },
                {
                    "authorId": "1861119",
                    "name": "Veronika Thost"
                },
                {
                    "authorId": "2582677",
                    "name": "Michael Witbrock"
                },
                {
                    "authorId": "2297836",
                    "name": "Achille Fokoue"
                }
            ]
        }
    ]
}